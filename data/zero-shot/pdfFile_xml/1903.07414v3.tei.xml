<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Lightweight Optical Flow CNN - Revisiting Data Fidelity and Regularization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tak-Wai</forename><surname>Hui</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Fellow, IEEE</roleName><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Senior Member, IEEE</roleName><forename type="first">Chen</forename><surname>Change Loy</surname></persName>
						</author>
						<title level="a" type="main">A Lightweight Optical Flow CNN - Revisiting Data Fidelity and Regularization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T02:16+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Convolutional neural network</term>
					<term>cost volume</term>
					<term>deep learning</term>
					<term>optical flow</term>
					<term>regularization</term>
					<term>spatial pyramid</term>
					<term>and warping</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Over four decades, the majority addresses the problem of optical flow estimation using variational methods. With the advance of machine learning, some recent works have attempted to address the problem using convolutional neural network (CNN) and have showed promising results. FlowNet2 [1], the state-of-the-art CNN, requires over 160M parameters to achieve accurate flow estimation. Our LiteFlowNet2 outperforms FlowNet2 on Sintel and KITTI benchmarks, while being 25.3 times smaller in the model size and 3.1 times faster in the running speed. LiteFlowNet2 is built on the foundation laid by conventional methods and resembles the corresponding roles as data fidelity and regularization in variational methods. We compute optical flow in a spatial-pyramid formulation as SPyNet [2] but through a novel lightweight cascaded flow inference. It provides high flow estimation accuracy through early correction with seamless incorporation of descriptor matching. Flow regularization is used to ameliorate the issue of outliers and vague flow boundaries through feature-driven local convolutions. Our network also owns an effective structure for pyramidal feature extraction and embraces feature warping rather than image warping as practiced in FlowNet2 and SPyNet. Comparing to LiteFlowNet [3], LiteFlowNet2 improves the optical flow accuracy on Sintel Clean by 23.3%, Sintel Final by 12.8%, KITTI 2012 by 19.6%, and KITTI 2015 by 18.8%, while being 2.2 times faster. Our network protocol and trained models are made publicly available on https://github.com/twhui/LiteFlowNet2. Fig. 1: Examples demonstrate the effectiveness of the proposed components in LiteFlowNet for i) feature warping, ii) cascaded flow inference, and iii) flow regularization. Enabled components are indicated with bold black fonts.</p><p>image. The model, as a result, comprises over 160M parameters and has a slow runtime, which could be formidable in many applications. Another work, SPyNet [2], uses a spatial pyramid network with only 1.2M parameters by adopting image warping in each pyramid level. Nonetheless, its performance can only match that of FlowNet but not FlowNet2.</p><p>FlowNet2 <ref type="bibr" target="#b0">[1]</ref> and SPyNet [2] showed the potential of solving the optical flow problem by using CNNs. Our earlier work, LiteFlowNet <ref type="bibr" target="#b12">[13]</ref>, is inspired by their successes, but we further drill down some of the key elements of solving the flow problem by adopting data fidelity and regularization in classical variational methods to CNN more closely. In this work, we provide more details on the correspondences between conventional methods and arXiv:1903.07414v3 [cs.CV]  </p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>O PTICAL FLOW, which refers to the point correspondence across a pair of images, is induced by the spatial motion at any image position. Due to the well-known aperture problem, optical flow cannot be directly measured. The partial observability of optical flow is the major reason that makes it a challenging problem. The optical flow problem has attracted many attentions since the seminal works by Horn and Schunck <ref type="bibr" target="#b3">[4]</ref>, and Lucas and Kanade <ref type="bibr" target="#b4">[5]</ref> about four decades ago. Most of the approaches estimate optical flow relying on an energy minimization method in a coarse-to-fine framework <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>. Optical flow is refined iteratively using a numerical approach from the coarsest level towards the finest level by warping one of the images in the image pair towards the other using the flow estimate from the coarser level. The warping technique is theoretically justified to minimize the energy functional <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>. On the other hand, normal flow which is directly measurable is more ready for motion estimation <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>.</p><p>FlowNet <ref type="bibr" target="#b11">[12]</ref> and FlowNet2 <ref type="bibr" target="#b0">[1]</ref>, are the pioneering works using convolutional neural network (CNN) for optical flow estimation. Their performances especially the successor are approaching to the state-of-the-art energy minimization approaches, while the speed is several orders of magnitude faster. To push the envelop of accuracy, FlowNet2 is designed as a cascade of variants of FlowNet, i.e., FlowNetC and FlowNetS. Each network in the cascade refines the preceding flow field by contributing on the flow adjustment between the first image and the warped second , NetC generates two pyramids of high-level features (F k (I 1 ) in pink and F k (I 2 ) in red, k ? <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b2">3]</ref>). NetE yields multi-scale flow fields such that each of them is generated by a cascaded flow inference module M :S (in blue color, including a descriptor matching unit M and a sub-pixel refinement unit S) and a regularization module R (in green color). Flow inference and regularization modules correspond to data fidelity and regularization terms in conventional energy minimization methods, respectively. optical flow CNNs. We also present LiteFlowNet2 that has a better flow accuracy and a faster runtime by optimizing the network architecture and training protocols of LiteFlowNet.</p><p>In the following, we first discuss the motivations, namely i) data fidelity, ii) image warping, and iii) regularization, from classical variational methods on the design of LiteFlowNet. Then, we highlight the more specific differences between our design and the state-of-the-art optical flow CNNs.</p><p>Data Fidelity. Point correspondence across two images is generally constrained by the classical brightness constancy <ref type="bibr" target="#b3">[4]</ref>. Gradient <ref type="bibr" target="#b5">[6]</ref> and higher-order brightness constancy <ref type="bibr" target="#b6">[7]</ref> assumptions are also widely used in the literature. The above constancy assumptions are collectively known as data fidelity and are often combined to form a hybrid data term <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b13">[14]</ref>. Although different matching quantities are proved to be useful in solving the optical flow problem, finding a correct proportion of their contributions in the hybrid data term is non-trivial and requires a highly engineered data fusion model <ref type="bibr" target="#b14">[15]</ref>. An improper mixture of the brightness and gradient terms can severely affect the performance <ref type="bibr" target="#b13">[14]</ref>. To avoid the aforementioned difficulties, feature descriptors that are not explicitly defined are learned in variational setting <ref type="bibr" target="#b15">[16]</ref>. We use a CNN to train a pyramidal feature descriptor (i.e., a feature encoder) <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b11">[12]</ref> which resembles data fidelity in variational methods and is prepared for establishing robust point correspondence later. Specifically, a given image pair is transformed from the spatial domain to the learned feature space in the form of two pyramids of multi-scale high-dimensional feature maps.</p><p>Image Warping. It is proved that image warping effectively minimizes an energy functional by using a numerical method in a coarse-to-fine framework <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>. Intuitively, at each iteration the numerical solver displaces every pixel value of the second image in the image pair according to the constraints imposed in the functional so that the warped image has a visual appearance close to the first image. Image warping is practiced in FlowNet2 <ref type="bibr" target="#b0">[1]</ref> and SPyNet <ref type="bibr" target="#b1">[2]</ref> between cascaded networks and pyramid levels, respectively. However, warping an image and then generating the feature maps of the warped image as the above CNN-based methods are two ordered steps. We find that the two steps can be reduced to a single one by directly warping the feature maps of the second image, which have been provided by the feature encoder. This one-step feature warping (f-warp) process reduces the more discriminative feature-space distance instead of the RGB-space distance between the two images. This makes LiteFlowNet more powerful and efficient in addressing the optical flow problem. To this end, we use the spatial transformer <ref type="bibr" target="#b16">[17]</ref> for the warping.</p><p>Regularization. Merely using data fidelity for flow estimation is an ill-posed problem <ref type="bibr" target="#b3">[4]</ref>. One example is the one-to-many point correspondences in homogeneous regions of an image pair. With the co-occurrence between motion boundaries and intensity edges, flow estimate is often smoothed by an anisotropic image-driven regularization <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b17">[18]</ref>. However, the image-driven strategies are prone to over-segmentation artifacts in the textured image regions since image edges do not necessarily correspond to flow edges. More advanced methods overcome the previous shortcomings through the use of an anisotropic image-and flow-driven regularization <ref type="bibr" target="#b15">[16]</ref> and a complementary regularizer <ref type="bibr" target="#b18">[19]</ref>. With the motivation to establish robust point correspondence in the learned feature space, we generalize the use of regularization from the spatial space to the feature space. This allow the flow field to be regularized by a feature-driven local convolution (f-lconv) at each pyramid level. The kernels of such a local convolution are adaptive to the pyramidal features from the encoder, flow estimate, and occlusion probability map. This makes the flow regularization to be both flow-and image-aware. We name it as the feature-driven local convolution layer in order to distinguish it from the local convolution (lconv) layer of which filter weights are locally fixed in conventional CNNs <ref type="bibr" target="#b19">[20]</ref>. We use the feature-driven convolution <ref type="bibr" target="#b20">[21]</ref> in our framework to regularize flow fields.</p><p>Our Design. The proposed network, dubbed LiteFlowNet <ref type="bibr" target="#b12">[13]</ref>, consists of a multi-scale feature encoder and a multi-scale flow decoder as shown in <ref type="figure" target="#fig_0">Figure 2</ref>. The encoder maps a given image pair, respectively, into two pyramids of multi-scale high-dimensional features <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b11">[12]</ref>. The decoder then estimates optical flow in a coarse-to-fine framework <ref type="bibr" target="#b1">[2]</ref>. Specifically, the decoder infers a flow field by selecting and using the features of the same resolution from the encoder at each pyramid level. This design leads to a TABLE 1: A comparison of the major components used in the state-of-the-art optical flow CNNs. (Notes: <ref type="bibr" target="#b0">1</ref> We use the convention that flow field at level 1 has the same spatial resolution as the given image pair. <ref type="bibr" target="#b1">2</ref> Flow inference from levels 7 to 3 is performed in each of the stacking networks except the fusion network. Flow fields resulting from FlowNet2-CSS and FlowNet2-SD are upsampled by a factor 4 (i.e., from level 3 to level 1) and then used as the inputs to the fusion network. <ref type="bibr" target="#b2">3</ref> The authors excluded the use of residual connections in the publicly released model.)</p><p>FlowNetS <ref type="bibr" target="#b11">[12]</ref> FlowNetC <ref type="bibr" target="#b11">[12]</ref> FlowNet2 <ref type="bibr" target="#b0">[1]</ref> SPyNet <ref type="bibr" target="#b1">[2]</ref> PWC-Net <ref type="bibr" target="#b21">[22]</ref> LiteFlowNet <ref type="bibr">[</ref> lighter and a more efficient network compared to FlowNet <ref type="bibr" target="#b11">[12]</ref> and FlowNet2 <ref type="bibr" target="#b0">[1]</ref> that adopt U-Net architecture <ref type="bibr" target="#b22">[23]</ref> for flow inference. SPyNet <ref type="bibr" target="#b1">[2]</ref> uses a spatial pyramid network to infer a flow field at each pyramid level from the corresponding image pair in the image pyramid. On the contrary, our network separates the processes of feature extraction and flow estimation into encoder and decoder, respectively. This helps us to better pinpoint the bottleneck of accuracy and model size. Particularly, our decoder uses a pair of feature maps from the encoder for flow inference instead of using a pair of images. At each pyramid level, we introduce a novel cascaded flow inference. Each of them has a f-warp layer to displace the feature maps of the second image towards the first image using the flow estimate from the previous level rather than image warping as practiced in FlowNet2 <ref type="bibr" target="#b0">[1]</ref> and SPyNet <ref type="bibr" target="#b1">[2]</ref>. Flow residue is computed to reduce the feature-space distance between the images. This design is advantageous to the conventional design of using a single network for flow inference. First, the cascade progressively improves flow accuracy thus allowing an early correction of the estimate without passing more errors to the next pyramid level. Second, this design allows seamless integration with descriptor matching. We assign a matching network to the first inference. Consequently, pixel-accuracy flow field can be generated first and then it is refined to sub-pixel accuracy in the subsequent inference network. Since at each pyramid level the feature-space distance between the images has been reduced by the f-warp, a short searching range rather than a long searching range <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b11">[12]</ref> is used to establish a cost volume. Besides, matching can be performed at sampled positions to aggregate a sparse cost volume. This effectively reduces the computational burden raised by the explicit matching. After the cascaded flow inference, the flow field is further regularized by a f-lconv layer.</p><p>The effectiveness of the aforementioned designs are depicted in <ref type="figure">Figure 1</ref>. In summary, our contributions are in four aspects:</p><p>1) We present a study to bridge the correspondences between the well-established principles in conventional methods for optical flow estimation and optical flow CNNs. 2) More details of our earlier work LiteFlowNet <ref type="bibr" target="#b12">[13]</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>The problem of optical flow estimation has been widely studied in the literature since 1980s. A detailed review is beyond the scope of this work. Here, we briefly review some of the major approaches, namely variational, machine learning, and CNN-based methods. Variational Methods. Since the pioneering work by Horn and Schunck <ref type="bibr" target="#b3">[4]</ref>, variational methods have dominated in the literature. Brox et al. address illumination changes by combining the brightness and gradient constancy assumptions <ref type="bibr" target="#b5">[6]</ref>. Brox et al. integrate rich descriptors into a variational formulation <ref type="bibr" target="#b7">[8]</ref>. In DeepFlow <ref type="bibr" target="#b23">[24]</ref>, Weinzaepfel et al. propose to correlate multi-scale patches and incorporate this as the matching term in a functional. In PatchMatch Filter <ref type="bibr" target="#b24">[25]</ref>, Lu et al. establish dense correspondence using the superpixel-based PatchMatch <ref type="bibr" target="#b25">[26]</ref>. <ref type="bibr">Revaud et al. propose</ref> EpicFlow that uses externally matched flows as the initialization and then performs interpolation <ref type="bibr" target="#b26">[27]</ref>. Zimmer et al. design the complementary regularization that exploits directional information from the constraints imposed in data term <ref type="bibr" target="#b18">[19]</ref>. Our network that infers optical flow and performs flow regularization is inspired by data fidelity and regularization in variational methods.  <ref type="bibr" target="#b31">[32]</ref>. Given a set of sparse matches, Wulff et al. propose to regress them to a dense flow field using a set of basis flow fields (PCA-Flow) <ref type="bibr" target="#b32">[33]</ref>. It can be shown that the parameterized model <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b32">[33]</ref> is related to the flow inference in CNNs. CNN-Based Methods. A comparison of the major components used in the state-of-the-art optical flow CNNs is summarized in <ref type="table">Table 1</ref>. In FlowNet <ref type="bibr" target="#b11">[12]</ref>, Dosovitskiy et al. use an optional post-processing step that involves energy minimization to reduce smoothing effect across flow boundaries. This process is not end-to-end trainable. On the contrary, we present an end-to-end approach that performs in-network flow regularization using a flconv layer, which plays a similar role as the regularization term in variational methods.  <ref type="bibr" target="#b35">[36]</ref>. DeepFlow <ref type="bibr" target="#b23">[24]</ref> that involves convolution and pooling operations is however not a CNN, since the "filter weights" are non-trainable image patches. It uses correlation according to the terminology used in FlowNet.</p><p>A notable concurrent work to LiteFlowNet is PWC-Net <ref type="bibr" target="#b21">[22]</ref>, which is about 18 times smaller than FlowNet2 <ref type="bibr" target="#b0">[1]</ref>. Lite-FlowNet <ref type="bibr" target="#b12">[13]</ref>, a more lightweight CNN, is about 30 times smaller than FlowNet2. Both of the works use the coarse-to-fine flow inference, feature warping, and cost volume for optical flow estimation, and are presented in CVPR 2018. However, there a number of distinctions between them. First, LiteFlowNet incorporates a cascaded flow inference to estimate residual flow at each pyramid level. Specifically, the pixel-level flow estimate that is generated by the cost-volume flow decoder is refined to the sub-pixel level. Second, flow fields resulting from the cascaded flow inference are further regularized by feature-driven local convolutions. Third, densely connected layers and feed-forwarding of feature maps from the previous level are not used in each pyramid level of the decoder. Fourth, LiteFlowNet is also benefited from the use of stage-wise training (more details in Section 6.1) to improve the optical flow accuracy and reduce the training time. These differences make LiteFlowNet to be more efficient in terms of the number of model parameters for solving the optical problem and therefore it attains a smaller model size than PWC-Net.</p><p>An alternative approach for establishing dense correspondence is to match image patches. Zagoruyko et al. introduce to use CNNfeature matching <ref type="bibr" target="#b36">[37]</ref>. G?ney et al. use feature representation and formulate optical flow estimation in MRF <ref type="bibr" target="#b37">[38]</ref>. Bailer et al. <ref type="bibr" target="#b38">[39]</ref> use multi-scale features and then perform feature matching as Flow Fields <ref type="bibr" target="#b39">[40]</ref>. Although pixel-wise matching can establish accurate point correspondence, the computational cost is too high for practical use (several seconds on a GPU). As a tradeoff, Dosovitskiy et al. <ref type="bibr" target="#b11">[12]</ref> and Ilg et al. <ref type="bibr" target="#b0">[1]</ref> perform feature matching only at a reduced spatial resolution. On the contrary, we reduce the computational burden of feature matching by using a short-ranged matching of warped CNN features and a sub-pixel refinement at every pyramid level. We further reduce the computation cost by constructing sparse cost volumes at high levels.</p><p>Jaderberg et al. propose a spatial transformer that allows spatial manipulation of feature maps within the network <ref type="bibr" target="#b16">[17]</ref>. We use the spatial transformer for the f-warp. Specifically, given a high-dimensional feature map as the input, each feature vector 1 is individually displaced to a new location by the f-warp layer in accordance with the displacement vector at the corresponding position in the computed flow field. In comparison to FlowNet2 <ref type="bibr" target="#b0">[1]</ref> and SPyNet <ref type="bibr" target="#b1">[2]</ref>, the spatial transformation is limited to images, LiteFlowNet is a more generic warping network that warps highlevel CNN features. Brabandere et al. propose a network to predict new frame(s) within a given video <ref type="bibr" target="#b20">[21]</ref>. The filters are generated dynamically conditioned on an input. We are inspired by flow regularization in variational methods <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref> and use the feature-driven convolution from Brabandere et al. in our framework to regularize flow fields.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">LITEFLOWNET</head><p>Two lightweight sub-networks that are specialized in pyramidal feature extraction and optical flow estimation constitute Lite-FlowNet. <ref type="figure" target="#fig_0">Figure 2</ref> shows an overview of its network architecture. Since the spatial dimension of feature maps is contracting in the feature encoder and that of flow fields is expanding in the flow decoder, we name the two sub-networks as NetC and NetE respectively. NetC transforms a given image pair respectively into two pyramids of multi-scale high-dimensional features. NetE consists of cascaded flow inference and regularization modules. It estimate flow fields from low to high spatial resolutions. Pyramidal Feature Extraction. As shown in <ref type="figure" target="#fig_0">Figure 2</ref>, NetC is a two-stream sub-network in which the filter weights are shared across the two streams. Each of them functions as a pyramidal feature descriptor that transforms a given image I to a pyramid of multi-scale high-dimensional features {F k (I)} from the highest spatial resolution (k = 1) to the lowest spatial resolution (k = L). The pyramidal features are generated by stride-1 and stride-s convolutions with the reduction of spatial resolution by a factor of s down the inverted pyramid. In the following, we omit the subscript k that indicates the level of pyramid for brevity. We use F i to represent the extracted CNN features for I i . When we discuss the operations in a pyramid level, the same operations are applicable to other levels.</p><p>We use the design principle that high-resolution feature maps require a large receptive field for convolutional processing. For every decrement of two pyramid levels, we assign a smaller receptive field than the previous level. Suppose a 6-level feature encoder is used, the sizes of receptive field are set to 7, 7, 5, 5, 3, and 3 for levels 6 to 1, respectively. Since the size of receptive field across convolution layers can be accumulated, we improve the computational efficiency by replacing a large-kernel convolution layer with multiple small-kernel convolution layers. Except a 7?7 kernel is used at the first convolution layer in NetC, 3 ? 3 kernels are used for the subsequent layers and the numbers of convolution layers are set to 3, 2, 2, 1, and 1 for levels 5 to 1, respectively. More details about the network architecture can be found in Appendix. Feature Warping. We denote x as a point in the image domain ? ? R 2 . At each pyramid level, a flow field u, i.e., a function u : ? ? R 2 , is inferred from the features F 1 and F 2 of images I 1 and I 2 . Flow inference becomes more challenging if I 1 and I 2 are captured far away from each other because a correspondence needs to be searched in a large area. With the motivation of image warping used in conventional methods <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref> and recent CNNs <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref> for addressing large-displacement flow, we propose to reduce the feature-space distance between F 1 and F 2 by feature warping (f-warp) prior to recovering the flow field. Specifically, F 2 is warped towards F 1 by f-warp via a flow estimate u, i.e., In M , f-warp transforms the high-level feature F 2 to F 2 using the upscaled (by a factor of 2) flow estimate 2u ?2 from the previous pyramid level. In S, F 2 is warped by the flow estimate u m resulting from M . Residual flow ?u m is inferred from the cost volume V . ?u s is used to correct u m due to the pixel-level cost aggregation. In comparison to the residual flow ?u m , more flow adjustment can be found on flow boundaries in ?u s .</p><formula xml:id="formula_0">F 2 (x) F 2 (x + u) ? F 1 (x)</formula><p>. This allows our network to infer residual flow ?u between F 1 and warped F 2 (i.e., F 2 ) that has smaller flow magnitude but not the complete flow field u that is more difficult to infer (more details in Section 3.1). Unlike conventional methods, f-warp is performed on high-level CNN features but not on images. This makes our network more powerful and efficient in addressing the optical flow problem. To allow end-to-end training, F is interpolated to F for any sub-pixel displacement u as follows:</p><formula xml:id="formula_1">F(x) = x i s ?N (xs) F(x i s ) 1 ? x s ? x i s 1 ? y s ? y i s ,<label>(1)</label></formula><p>where x s = x + u = (x s , y s ) denotes the source coordinates in the input feature map F that defines the sample point, x = (x, y) denotes the target coordinates of the regular grid in the interpolated feature map F, and N (x s ) denotes the four pixel neighbors of x s . The above bilinear interpolation allows backpropagation as its gradients can be efficiently computed <ref type="bibr" target="#b16">[17]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Cascaded Flow Inference</head><p>At each pyramid level of NetE, flow field inference is performed in a two-step procedure. An overview of the working mechanism is illustrated in <ref type="figure" target="#fig_2">Figure 3</ref>. First, the pixel-by-pixel matching of highlevel feature vectors across a given image pair yields a coarse flow estimate. Second, a subsequent refinement on the coarse flow further improves it to sub-pixel accuracy. The use of such a cascaded flow inference is novel in the literature.</p><p>First Flow Inference -Descriptor Matching. Point correspondence between I 1 and I 2 is established through computing the correlation (i.e., dot product) of high-level feature vectors in individual pyramidal features F 1 and F 2 as follows <ref type="bibr" target="#b11">[12]</ref>:</p><formula xml:id="formula_2">c(x, d) = F 1 (x) ? F 2 (x + d)/N,<label>(2)</label></formula><p>where c is the matching cost between point x in F 1 and point x + d in F 2 , d ? Z 2 (an 2-D integer set) is the displacement vector from x, and N is the length of the feature vector. The xand y-components of d are bounded by ?D and D ? Z + (an 1-D positive integer set). A cost volume V is built by aggregating all the matching costs c(</p><formula xml:id="formula_3">x, d) into a 3D grid. At pyramid level k, the dimension of V is H 2 k?1 ? W 2 k?1 ? (2D + 1) for an image pair of size H ? W .</formula><p>Unlike the conventional construction of cost volume <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b11">[12]</ref>, we reduce the computational burden raised in three ways: 1) Multi-Scale Short Searching Range: Matching of feature vectors between F 1 and F 2 is performed within a short searching range at every pyramid level instead of using a long searching range only at a high-resolution pyramid level. 2) Feature Warping: We reduce the feature-space distance between F 1 and F 2 prior to constructing the cost volume.</p><p>To this end, F 2 is warped towards F 1 by a f-warp layer using the flow estimate from the previous level. 3) Sparse Cost Volume: We perform feature matching only at the sampled positions in the pyramid levels with high spatial resolution. The sparse cost volume is interpolated in the spatial dimension to fill the missed matching costs for the unsampled positions.</p><p>The first two techniques effectively reduce the searching space needed, while the third technique reduces the frequency of matching per pyramid level. This in turn causes a speed-up in constructing the cost volume.</p><p>In the descriptor matching unit M , the residual flow ?u m between F 1 and warped F 2 , i.e., F 2 (x) = F 2 (x + su ?s ), is inferred from the constructed cost volume V as illustrated in <ref type="figure" target="#fig_2">Figure 3</ref>. A complete flow field u m is computed as follows:</p><formula xml:id="formula_4">u m = M V (F 1 , F 2 ; D) ?um +su ?s ,<label>(3)</label></formula><p>where flow field u from a preceding level needs to be upsampled in spatial resolution (denoted by "?s") and magnitude (multiplied by a scalar s) to su ?s for matching the resolution of the pyramidal features in the current level. For consecutive levels, we use s = 2.</p><p>Second Flow Inference -Sub-Pixel Refinement. Since the cost volume in the descriptor matching unit is aggregated by measuring pixel-by-pixel correlation, flow estimate u m resulting from the previous inference is only up to pixel-level accuracy. We introduce the second flow inference in the wake of descriptor matching as shown in <ref type="figure" target="#fig_2">Figure 3</ref>. It aims to refine the pixel-level flow field u m resulting from the descriptor matching unit to sub-pixel accuracy. This prevents erroneous flows being amplified by upsampling and passing to the next pyramid level. Specifically, F 2 is warped to a new F 2 using the current flow estimate u m . For correcting u m , the sub-pixel refinement unit S yields a more accurate flow field u s by minimizing the feature-space distance between F 1 and F 2 through computing a residual flow ?u s as follows:</p><formula xml:id="formula_5">u s = S F 1 , F 2 , u m ?us +u m .<label>(4)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Flow Regularization</head><p>Cascaded flow inference resembles the role of data fidelity in conventional minimization methods. However using data term alone, vague flow boundaries and other undesired artifacts can exist in flow fields <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>. To tackle this problem, featuredriven local convolution (f-lconv) is used to regularize each flow field resulting from the cascaded flow inference. The operation of f-lconv to a flow field is well-governed by the Laplacian formulation of diffusion of pixel values <ref type="bibr" target="#b40">[41]</ref> (see Section 4.2 for more details). In contrast to the local convolution (lconv) used in conventional CNNs <ref type="bibr" target="#b19">[20]</ref>, the f-lconv is more generalized. Not only a distinct filter is used for each position of a flow field but the filter is adaptively constructed to regularize each flow vector with a weighted average of flow vectors from nearby pixels. Consider a general case, a vector-valued feature F that has to be regularized has a spatial dimension H ? W and C channels. Define G = {g} as a set of filters used in a f-lconv layer. The operation of a f-lconv filter g x,y,c with size ? ? ? to F at position (x, y) and channel c is formulated as follow:</p><formula xml:id="formula_6">F r (x, y, c) = (xi,yi)?N (x,y) g x,y,c (x i , y i )F(x + x i , y + y i , c),<label>(5)</label></formula><p>where F r (x, y, c) is the scalar output and N (x, y) denotes the neighborhood containing ? ? ? pixels centered at position (x, y).</p><p>To regularize a flow field, f-lconv filters need to be specialized. It should behave as an averaging filter if the variation of flow vectors over a patch is supposed to be smooth. It should also not over-smooth flow vectors across flow boundary. To this end, we design a CNN unit R D to generate a feature-driven variation metric D with dimension H ? W ? ? ? ? ? C 2 . It predicts the local flow variation over a patch with size ? ? ? at all positions in a flow field using pyramidal feature F 1 , flow field u s from the cascaded flow inference, and occlusion probability map 3 O as follows:</p><formula xml:id="formula_7">D = R D (F 1 , u s , O).<label>(6)</label></formula><p>With the introduction of feature-driven variation metric D, each 2. For the case of flow field, the dimension of D is H ? W ? ? ? ? ? 2 as a flow field has 2 channels. But for the purpose of a lightweight implementation, both channels of a flow field is regularized equally, i.e., C = 1.</p><p>3. We use L 2 brightness error ||I 2 (x + u) ? I 1 (x)|| 2 between the warped second image and the first image as the occlusion probability map.</p><p>filter g of f-lconv is constructed as follows:</p><formula xml:id="formula_8">g x,y,c (x i , y i ) = exp(?D(x, y, x i , y i , c) 2 ) (xj ,yj )?N (x,y) exp(?D(x, y, x j , y j , c) 2 )</formula><p>. <ref type="formula">(7)</ref> We intend to use the negative tail of the exponential function to constrain the values of f-lconv filters in [0, 1] as the rapid-growing positive tail makes the training of the f-lconv more difficult.</p><p>Here, we provide a mechanism to perform f-lconv efficiently. For a C-channel input F, we use C tensors? 1 , ...,? C to store f-lconv filter set G. As illustrated in <ref type="figure" target="#fig_3">Figure 4</ref>, each f-lconv filter g x,y,c is folded into a 3D column vector with length w 2 and then packed into the (x, y)-entry of a 3D tensor? c with size H ?W ? w 2 . The same folding and packing operations are also applied to each patch in each channel of F. This results in C tensors F 1 , ...,F C for F. In this way, Equation <ref type="formula" target="#formula_6">(5)</ref> is reformulated to:</p><formula xml:id="formula_9">F r (c) =? c F c ,<label>(8)</label></formula><p>where " " denotes element-wise dot product between the corresponding column vectors of the tensors. With the abuse of notation, F r (c) denotes the xy-slice at channel c in the regularized C-channel feature F r . The operation of element-wise dot product reduces the dimension of tensors on the right-hand side from</p><formula xml:id="formula_10">H ? W ? ? 2 to H ? W .</formula><p>To summarize, u s resulting from the cascaded flow inference is adaptively regularized by the flow regularization module R using a set of f-lconv filters G as follows:</p><formula xml:id="formula_11">u r = R(u s ; G).<label>(9)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">CORRESPONDENCES BETWEEN OPTICAL FLOW CNNS AND VARIATIONAL METHODS</head><p>We first provide a brief review for estimating optical flow using variational methods. In the next two sub-sections, we will bridge the correspondences between optical flow CNNs and classical variational methods.</p><p>Consider an image sequence I(x, t) : R 3 ? R with x = (x, y) ? ? over a rectangular spatial domain ? ? R 2 and a temporal dimension t. The optical flow field u : ? ? R 2 that is induced by the spatial motion of the scene and/or the camera itself corresponds to the displacement vector field between images I 1 (at t = 1) and I 2 (at t = 2). The flow field can be estimated by minimizing an energy functional E of the general form <ref type="bibr" target="#b18">[19]</ref>:</p><formula xml:id="formula_12">E(u) = E dat (u) + ?E reg (?u) = ? e data (u) + ?e reg (?u) dx,<label>(10)</label></formula><p>where e dat and e reg represent the data and regularization costs respectively, and ? &gt; 0 is the smoothness weight.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Data Term</head><p>Point correspondence across a pair of images is imposed in the data term of Eq. (10) as a combination of several matching quantities {D i } as follows <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b18">[19]</ref>:</p><formula xml:id="formula_13">E dat (u) = ? ? i D i (I 1 , I 2 )dx,<label>(11)</label></formula><p>where ? i is the weighting factor for D i . Two popular matching quantities are image brightness constancy assumption ? |I 2 (x + u) ? I 1 (x)| 2 <ref type="bibr" target="#b3">[4]</ref> and gradient constancy assump-</p><formula xml:id="formula_14">tion ? |?I 2 (x + u) ? ?I 1 (x)| 2 [6]</formula><p>, where ? is a robust penalty function. Other higher-order constancy data terms are also widely used <ref type="bibr" target="#b6">[7]</ref>. The contributions of different matching quantities need to be compromised by using appropriate weighting factors <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>. It is also necessary to maintain differentiability of both data and regularization (Section 4.2) terms because Eq. (10) needs to be solved using the Euler-Lagrange equation.</p><p>In comparison to conventional methods, state-of-the-art optical flow networks do not explicitly define those matching quantities {D i }. Back2Basics <ref type="bibr" target="#b41">[42]</ref> uses a photometric loss that is computed as the difference between the first image and the warped second image. SPyNet <ref type="bibr" target="#b1">[2]</ref> uses a pair of images from the image pyramids to generate a flow field at the corresponding pyramid level. PWC-Net <ref type="bibr" target="#b21">[22]</ref> and LiteFlowNet <ref type="bibr" target="#b12">[13]</ref> use a learnable feature encoder instead. In more details, we train NetC of LiteFlowNet as a CNN-based pyramidal feature descriptor F(I) : R 2 ? R N that transforms a given image pair (I 1 , I 2 ) respectively into two pyramids of multi-scale high-dimensional features. With the introduction of feature descriptor, the cascaded flow inference in NetE that has been presented in Section 3.1 is trained to solve for the minimization of the difference between the high-level features F 2 of I 2 and F 1 of I 1 by computing the dense correspondence between them. In other words, feature encoders that are used in LiteFlowNet and other optical flow CNNs <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b21">[22]</ref> resemble the role of data term in variational methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Regularization Term</head><p>Flow field that is merely computed by data fidelity is fragile to outliers. Energy functional is often augmented to enforce dependency between neighboring flow vectors <ref type="bibr" target="#b3">[4]</ref>. Regularization of a vector field can be viewed as diffusion of pixel values <ref type="bibr" target="#b40">[41]</ref>. By applying the Euler-Lagrange equation to Eq. (10), the regularization component is given by:</p><formula xml:id="formula_15">div (? ?u E reg (?u)) = div(D?u),<label>(12)</label></formula><p>where D is a 2 ? 2 diffusion tensor. The above divergence formulation can also be rewritten into an oriented Laplacian form as follows:</p><formula xml:id="formula_16">div(D?u) = trace(TH i ), i = 1, 2,<label>(13)</label></formula><p>where H i is the Hessian matrix of the i-th vector component of the flow field and T is a 2 ? 2 tensor. The solution of Eq. (13) is given by:</p><formula xml:id="formula_17">u = K(T) * u ,<label>(14)</label></formula><p>where " * " denotes a convolution and K is a 2D oriented Gaussian kernel (the exact structure of K depends on D used in E reg ) and u is the intermediate flow field generated from the data term <ref type="bibr" target="#b42">[43]</ref>. In other words, enforcing smoothness constraint on the flow field is equivalent to applying a convolution with a 2D oriented Gaussian kernel to the intermediate flow field generated by the data term. Unlike the smoothing kernel in Eq. (14) that requires engineered regularizing structure, we use a feature-driven local convolution (f-lconv) filters G = {g} to regularize each flow vector differently in the flow field by adapting f-lconv kernel to the pyramidal feature F resulting from the encoder, intermediate flow field u from the data term, and occlusion probability map O. Our feature-driven flow regularization is defined as follows: . This concludes that our feature-driven regularization resembles the role of regularization term in variational methods. In Back2Basics <ref type="bibr" target="#b41">[42]</ref>, flow regularization is enforced by a piecewise smoothness function in the training loss instead.</p><formula xml:id="formula_18">u = g (F 1 , u , O) * u .<label>(15)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">RELATIONSHIP BETWEEN OPTICAL FLOW CNNS AND BASIS REPRESENTATION</head><p>The parameterized models of image motion <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b32">[33]</ref> use a linear combination of basis vectors {m i ? R 2hw } to approximate an image motion u within an image patch with size h ? w as follows:</p><formula xml:id="formula_19">u vec = C i=1 a i m i ,<label>(16)</label></formula><p>where u vec ? R 2hw is the vectorized flow field of u by packing all the xand y-components of u into a single vector and {a i } i=1,2,...,C are the flow coefficients to be estimated. The above basis representation is related to flow inferences in LiteFlowNet <ref type="bibr" target="#b12">[13]</ref> and other optical flow CNNs <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b21">[22]</ref>. At a pyramid level, the penultimate and last layers of the descriptor matching and sub-pixel flow refinement units in LiteFlowNet can be represented by the following:</p><formula xml:id="formula_20">F N ?1 = ? W N ?1 * F N ?2 + b N ?1 , (17.1) ?u = W N * F N ?1 + b N ,<label>(17.2)</label></formula><p>where " * " denotes a convolutional operator and N is the total number of convolution layers used. Furthermore, W i and F i represent the convolution filters and feature maps that are used and generated at the i-th layer, respectively. A trainable bias b i is added to each feature map after the convolutional operation. We denote a set of bias scalars as b i . Each convolution layer is followed by an activation function (?) for non-linear mapping unless otherwise specified. Suppose F N ?1 in Eq. (17.2) is a Cchannel vector-valued feature map, the equation can be re-written into the expanded form as follows:</p><formula xml:id="formula_21">?u = C i=1 W i * F i + b i ,<label>(18)</label></formula><p>where W = {W i }, b = {b i }, and F i is the i-th channel of F (superscripts N and N ? 1 are removed for brevity).</p><p>Similarities. Suppose the residual flow ?u in Eq. <ref type="formula" target="#formula_1">(18)</ref> is the flow field that we need to estimate even though it is not the full flow, the vectorized W i * F i + b i resembles a i m i in Eq.  <ref type="figure" target="#fig_4">Figure 5</ref> provides an example of the visualization of the learned filters (i.e., flow bases in conventional basis representation) at level 6 and level 3 in the sub-pixel refinement unit of LiteFlowNet2.</p><p>Differences. The dimension of CNN filters is usually small (a few pixels width) while the dimension of basis fields (before vectorization) is same as image patches under consideration. The dimension of CNN feature maps is proportional to that of the given images (depending on the pyramid level under consideration) while flow coefficients are scalars. Furthermore, a flow vector is constructed by a convolution between CNN filters and feature patches centered at the corresponding position in the feature maps as the flow vector while each vectorized flow patch is a linear combination of basis vectors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">LiteFlowNet</head><p>Network Details. In LiteFlowNet, NetC is a 6-level feature encoder and NetE is a flow decoder for generating flow fields from levels 6 to 2 in a coarse-to-fine manner. Flow field at level 2 is upsampled by a bilinear interpolation to the same resolution at level 1 as the given image pair. We set the maximum searching radius for constructing cost volumes to 3 and 6 pixels for levels 6 to 4 and levels 3 to 2, respectively. Matching is performed at every position across two pyramidal features to form a cost volume, except for levels 3 to 2 that it is performed at a regularly sampled grid (using a stride of 2) to form a sparse cost volume. All convolution layers use 3 ? 3 filters, except the first layer in NetC uses 7 ? 7 filters, each last layer in descriptor matching M , subpixel refinement S, and flow regularization R uses 5 ? 5 filters for levels 4 to 3 and 7 ? 7 filters for level 2. Each convolution layer is followed by a leaky rectified linear unit layer, except f-lconv and the last layers in M , S and R networks. More network details can be found in Appendix. Training Details. In conventional training methods <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b21">[22]</ref>, all parts of network are trained by the same number of iterations. On the contrary, we pre-train LiteFlowNet on FlyingChairs dataset <ref type="bibr" target="#b11">[12]</ref>  2) Better performance: Although the network stages that are lately added are relatively trained by a smaller number of iterations, stage-wise training promotes lower training losses on the overall network. This is possible because filter weights in the succeeding stage are wellinitialized from the previously trained stage rather than randomly assigned. The average end-point error (AEE) of LiteFlowNet2 is improved from 4.66 to 4.11 on KITTI 2012 <ref type="bibr" target="#b33">[34]</ref> and is significantly improved from 12.42 to 11.31 on KITTI 2015 <ref type="bibr" target="#b34">[35]</ref>. For benchmarking on Fly-ingChairs, AEEs are 1.68 (vs 1.70) and 1.60 (vs 1.61) on the training and validation sets, respectively. The results are significantly improved on KITTI but are similar on FlyingChairs. This indicates that stage-wise training is effective in alleviating the over-fitting issue as well.</p><p>Learning rates are initially set to 1e-4, 5e-5, and 4e-5 for levels 6 to 4, 3, and 2 respectively. We reduce it by a factor of 2 starting at 120K, 160K, 200K, and 240K iterations. We use the same batch size of 8, data set resolution (randomly cropped: 448 ? 320), loss weights (levels 6 to 2: 0.32, 0.08, 0.02, 0.01, 0.005), training loss (L 2 flow error), Adam optimization (? = 0.5, weight decay = 4e-4), data augmentation (including noise injection), scaled groundtruth flow (by a factor of 1 20 ) as FlowNet2 <ref type="bibr" target="#b0">[1]</ref>. Furthermore, we use a training loss for every inferred flow field.</p><p>After pre-training LiteFlowNet on FlyingChairs (Chairs) <ref type="bibr" target="#b11">[12]</ref>, it is trained on a more challenging data set, Things3D 4 <ref type="bibr" target="#b43">[44]</ref> according to the training schedule (Chairs ? Things3D) as FlowNet2 <ref type="bibr" target="#b0">[1]</ref>. It is trained for 500K iterations. Batch size is reduced to 4 and dataset resolution is increased to 768 ? 384. Learning rate is set to 3e-6 and is reduced by half starting at 200K iterations for every increment of 100K iterations. No stagewise training is used for subsequent fine-tunings. We denote LiteFlowNet-pre and LiteFlowNet as the networks pre-trained on Chairs and fine-tuned on Things3D, respectively.</p><p>After training on Things3D, we use the generalized Charbonnier function ?(x) = (x 2 + 2 ) q ( 2 = 0.01 and q = 0.2) as the robust training loss for further fine-tuning on subsequent datasets. The flow accuracy of LiteFlowNet2 is improved (KITTI 2012 <ref type="bibr" target="#b33">[34]</ref>: 3.73 vs 3.42 and KITTI 2015 <ref type="bibr" target="#b34">[35]</ref>: 9.80 vs 8.97) when the robust loss with a higher learning rate 1e-5 is used for fine-tuning on Things3D. However, the results on the testing sets of Sintel <ref type="bibr" target="#b44">[45]</ref> and KITTI are not much different from the case without using the robust loss. Therefore, we choose to use L 2 loss when fine-tuning on Things3D. The fine-tuning details on the respective training sets of Sintel and KITTI will be presented in Section 6.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">LiteFlowNet2</head><p>We analyze the flow accuracy in terms of AEE and the computation time at each pyramid level of LiteFlowNet <ref type="bibr" target="#b12">[13]</ref> trained on Chairs ? Things3D. The results are summarized in <ref type="table" target="#tab_5">Table 2</ref>. An example of multi-scale flow fields on Sintel Clean training set is also provided in <ref type="figure">Figure 6</ref>. We optimize the network architecture of LiteFlowNet with the following motivations and evolve our earlier model to a faster and more accurate model -LiteFlowNet2. Pyramid Level. As summarized in <ref type="table" target="#tab_5">Table 2</ref>, the computation time increases exponentially with the resolution of flow field. In 4. We excluded a small amount of training data in Things3D undergoing extremely large flow displacement as advised by the authors (https://github. com/lmb-freiburg/flownet2/issues).  <ref type="figure">Fig. 6</ref>: An example of coarse-to-fine flow fields generated from LiteFlowNet <ref type="bibr" target="#b12">[13]</ref> trained on Chairs ? Things3D. Each of the flow fields is upsampled to the same resolution as the ground truth by bilinear interpolation prior to computing AEE. particular, the improvement in flow accuracy is not significant when comparing the AEE at level 3 to that at level 2. On the contrary, about 60% of the total computation time spent on the flow decoder at level 2. In LiteFlowNet2, we improve the computational efficiency by reducing the number of pyramid levels in NetE from five (levels 6 to 2) to four (levels 6 to 3). Network Depth. By limiting the pyramid level of NetE up to level 3 in LiteFlowNet <ref type="bibr" target="#b12">[13]</ref>, the flow accuracy is decreased as revealed in <ref type="table" target="#tab_5">Table 2</ref>. In order to compensate the loss, we add two convolution layers (with 128 and 96 output channels) between the 128-and 64-channel convolution layers to each flow decoder in the cascaded flow inference of NetE. We will show in Section 6.3 that LiteFlowNet2 has a higher flow accuracy than LiteFlowNet. Pseudo Flow Inference and Regularization. We also address the inefficient computation at level 2 by introducing a simplified flow inference (without descriptor matching) and regularization at this level for the model fine-tuned on KITTI. The pseudo network is constructed as follows: First, we remove all the layers before the last layer in the original flow inference and regularization, respectively. Then, we replace the removed layers by feed-forwarding the upsampled features respectively from the layer prior to the last layer in the flow inference and regularization at level 3. Using the pseudo network, the runtime at level 2 is greatly reduced from 52.51ms to 8.95ms. We have experienced that the pseudo network can improve the flow accuracy on KITTI testing set (evaluation will be provided in Section 6.3) but there is no significant improvement on Sintel testing set. Since the latter is a synthetic dataset, a flow CNN is more easily to be trained for fitting the non-realistic scene. However, the variability in real-world data such as lighting and object textures is more challenging. Therefore, using one more flow inference and regularization is beneficial to the refinement of preceding flow estimate on KITTI.</p><p>Training Details. We pre-train LiteFlowNet2 (LiteFlowNet2pre) using the same stage-wise training protocol as Lite-FlowNet <ref type="bibr" target="#b12">[13]</ref> except for a few minor differences. Learning rate is set to 6e-5 instead of 5e-5. At the output of last flow regularization in NetE, the flow field is further upsampled to the same resolution as the image pair and we introduce an additional training loss with a loss weight 6.25e-4. <ref type="table" target="#tab_6">Table 3</ref> summarizes the results of  The fine-tuning protocol for the respective training set of Sintel and KITTI is the same as LiteFlowNet unless otherwise specified. The improvements due to the better finetuning protocol will be presented in Section 6.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Results</head><p>We evaluate LiteFlowNet and LiteFlowNet2 against the stateof-the-art methods on the public optical flow benchmarks including FlyingChairs (Chairs) <ref type="bibr" target="#b11">[12]</ref>, Sintel Clean and Final <ref type="bibr" target="#b44">[45]</ref>, KITTI 2012 <ref type="bibr" target="#b33">[34]</ref>, and KITTI 2015 <ref type="bibr" target="#b34">[35]</ref>. Average end-point error (AEE) and specialized percentage error are reported. FlyingChairs. We first compare the intermediate results of several well-performing networks trained on Chairs alone in <ref type="table" target="#tab_7">Table 4</ref>. LiteFlowNet-pre outperforms the compared networks. No intermediate result is available for FlowNet2 <ref type="bibr" target="#b0">[1]</ref> as each stacking network is trained on the Chairs ? Things3D schedule individually. Since FlowNetC, FlowNetS (variants of FlowNet <ref type="bibr" target="#b11">[12]</ref>), and SPyNet <ref type="bibr" target="#b1">[2]</ref> have fewer parameters than FlowNet2 and the latter two models do not perform feature matching, we construct a small-size counterpart LiteFlowNetX-pre for a fair comparison by removing the matching part and shrinking the model sizes of NetC and NetE by about 4 and 5 times, respectively. Despite LiteFlowNetX-pre is 43 and 1.33 times smaller than FlowNetC and SPyNet, respectively, it outperforms these networks and is on par with FlowNetC that uses explicit feature matching. As shown in <ref type="table" target="#tab_8">Table 5</ref>, LiteFlowNet2-pre which is also trained on Chairs is on par with LiteFlowNet on Sintel and outperforms LiteFlowNet on KITTI. MPI Sintel. The results are summarized in <ref type="table" target="#tab_8">Table 5</ref>. LiteFlowNetX-pre outperforms FlowNetS <ref type="bibr" target="#b11">[12]</ref>, FlowNetC <ref type="bibr" target="#b11">[12]</ref>, and SPyNet <ref type="bibr" target="#b1">[2]</ref> that are trained on Chairs on all cases. Lite-FlowNet, trained on the Chairs ? Things3D schedule, performs A comparison on the performance of the state-of-the-art optical flow methods in terns of AEE. The values in parentheses are the results of the networks on the data they were trained on, and hence are not directly comparable to the others. Out-Noc: Percentage of erroneous pixels defined as end-point error (EPE) &gt;3 pixels in non-occluded areas. Fl-all: Percentage of outliers averaged over all pixels. Inliers are defined as EPE &lt;3 pixels or &lt;5%. The best number for each category is highlighted in bold and the second best is underlined. (Notes: <ref type="bibr" target="#b0">1</ref> The values are reported from <ref type="bibr" target="#b0">[1]</ref>. <ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4</ref> The values are computed using the trained models provided by the authors. <ref type="bibr" target="#b2">3</ref> Large discrepancy exists as the authors mistakenly evaluated the results on the disparity dataset. <ref type="bibr" target="#b3">4</ref> Up-to-date dataset is used. <ref type="bibr" target="#b5">6</ref> Trained on Driving and Monkaa <ref type="bibr" target="#b43">[44]</ref>. <ref type="bibr" target="#b6">7</ref> Results are reported from the arXiv paper <ref type="bibr" target="#b2">[3]</ref>.) better than LiteFlowNet-pre as expected. LiteFlowNet also outperforms SPyNet, FlowNet2-S <ref type="bibr" target="#b0">[1]</ref>, and FlowNet2-C <ref type="bibr" target="#b0">[1]</ref>. It is on par with PWC-Net <ref type="bibr" target="#b21">[22]</ref>. With the improved architecture and training protocol, LiteFlowNet2 outperforms its predecessor LiteFlowNet and PWC-Net. We also fine-tuned LiteFlowNet (LiteFlowNet-ft) on a mixture of Sintel Clean and Final training data using the generalized Charbonnier loss with the settings 2 = 0.01 and q = 0.2. We randomly crop 768 ? 384 patches and use a batch size of 4. No noise augmentation is performed but we introduce image mirroring <ref type="bibr" target="#b21">[22]</ref> to improve the diversity of the training set. Learning rate is set to 5e-5 and the training schedule is similar to the training on Things3D except it is trained for 600K and is re-trained with a reduced learning rate for a reduced number of iterations. LiteFlowNet-ft outperforms FlowNet2-ft-sintel <ref type="bibr" target="#b0">[1]</ref> and EpicFlow <ref type="bibr" target="#b26">[27]</ref> on Sintel Final testing set. It is on par with PWC-Net-ft <ref type="bibr" target="#b21">[22]</ref>. Despite DC Flow <ref type="bibr" target="#b46">[47]</ref> (a hybrid method consists of CNN and post-processing) performs better than LiteFlowNet, its GPU runtime requires several seconds that makes it formidable in many applications. For fine-tuning LiteFlowNet2 (LiteFlowNet2ft), we further improve the diversity of the training set by using a mixture of Sintel and KITTI data for a batch size of 4 containing two image pairs from each of the training sets. Unlike PWC-Net+ <ref type="bibr" target="#b47">[48]</ref>, our mixture does not contains HD1K dataset <ref type="bibr" target="#b48">[49]</ref> as we have experienced that there is no significant improvement after including it. LiteFlowNet2-ft outperforms all the compared methods and is on par with PWC-Net-ft+ on Sintel Clean and KITTI. LiteFlowNet consistently performs better than LiteFlowNet-pre especially on KITTI 2015 as shown in <ref type="table" target="#tab_8">Table 5</ref>. It also outperforms SPyNet <ref type="bibr" target="#b1">[2]</ref>, FlowNet2-S <ref type="bibr" target="#b0">[1]</ref>, and FlowNet2-C <ref type="bibr" target="#b0">[1]</ref>. LiteFlowNet2, the successor of LiteFlowNet, outperforms FlowNet2 <ref type="bibr" target="#b0">[1]</ref>, LiteFlowNet, and PWC-Net <ref type="bibr" target="#b21">[22]</ref> as well. We also fine-tuned LiteFlowNet (LiteFlowNet-ft) and LiteFlowNet2 (LiteFlowNet2-ft) on a mixture of KITTI 2012 and KITTI 2015 training data using the same augmentation and training schedule as the case of Sintel except that we reduced the amount of augmentation for spatial motion <ref type="bibr" target="#b21">[22]</ref> to fit the driving scene. The height of each image in KITTI dataset is less than that of Sintel about 100 pixels. We randomly crop 896?320 patches to maintain a similar patch area as Sintel and use a batch size of 4. We have experienced that training on KITTI is more Image overlay Ground truth FlowNet2 <ref type="bibr" target="#b0">[1]</ref> PWC-Net 1 <ref type="bibr" target="#b21">[22]</ref> LiteFlowNet <ref type="bibr" target="#b12">[13]</ref> LiteFlowNet2</p><p>First image SPyNet-ft <ref type="bibr" target="#b1">[2]</ref> FlowNet2-ft <ref type="bibr" target="#b0">[1]</ref> PWC-Net+ <ref type="bibr" target="#b47">[48]</ref> LiteFlowNet-ft <ref type="bibr" target="#b12">[13]</ref> LiteFlowNet2-ft  <ref type="table" target="#tab_11">Table 6</ref> summarizes the improvements in terms of AEE under different network and training configurations. After fine-tuning, LiteFlowNet and LiteFlowNet2 generalize well to real-world data. LiteFlowNet-ft outperforms all the compared conventional and hybrid methods by a large extent. It also outperforms FlowNet2-ft-kitti <ref type="bibr" target="#b0">[1]</ref> and PWC-Net-ft <ref type="bibr" target="#b21">[22]</ref>. With the improved architecture and training protocol, LiteFlowNet2-ft outperforms LiteFlowNet, PWC-Net-ft, and PWC-Net+ <ref type="bibr" target="#b47">[48]</ref>. <ref type="figure">Figure 8</ref> shows some examples of flow fields on the training and testing sets KITTI 2012 and KITTI 2012. As in the case for Sintel, LiteFlowNet(-ft), and LiteFlowNet2(-ft) perform the best among the compared methods. Even though LiteFlowNet and LiteFlowNet2 perform pyramidal descriptor matching in a limited searching range, it yields reliable large-displacement flow fields for real-world data due to the feature warping (f-warp) layer introduced. An ablation study of different components in LiteFlowNet will be presented in Section 6.5.</p><p>LiteFlowNet-CVPR18 <ref type="bibr" target="#b12">[13]</ref> vs LiteFlowNet-arXiv <ref type="bibr" target="#b2">[3]</ref>. In the arXiv version of LiteFlowNet, we excluded a small amount of training data in Things3D undergoing extremely large flow displacement as it is rare to exist in real-world data.   <ref type="bibr" target="#b47">[48]</ref> reports the I/O requirement using Caffe and regards the previous improper usage <ref type="bibr" target="#b21">[22]</ref> as an I/O bug.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Runtime and Number of Parameters</head><p>We measure runtime on a machine equipped with an Intel Xeon E5 2.2GHz and an NVIDIA GTX 1080. Timings are averaged over 100 runs for a Sintel image pair with size 1024 ? 436. For a fair comparison, we also exclude the reading and writing time as PWC-Net(+) <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b47">[48]</ref>. As summarized in <ref type="table" target="#tab_12">Table 7</ref>,</p><p>? LiteFlowNet requires 30.3x fewer parameters than FlowNet2 <ref type="bibr" target="#b0">[1]</ref> and is 1.4x faster in the runtime. It requires 1.6x fewer parameters (? 3.4M) than PWC-Net+.</p><p>? LiteFlowNetX, a small-model variant of LiteFlowNet, which has no descriptor matching requires 43.5x fewer parameters than FlowNetC <ref type="bibr" target="#b11">[12]</ref> and has a comparable runtime. It has 1.3x fewer parameters than SPyNet <ref type="bibr" target="#b1">[2]</ref>.</p><p>? LiteFlowNet2 requires 25.3x fewer parameters than FlowNet2 while being 3.1x faster. It is 2.2 times faster than LiteFlowNet. In comparison to PWC-Net+, Lite-FlowNet2 requires 1.4x fewer parameters (? 2.3M). Its processing frequency can reach up to 25 flow fields per second and is similar to PWC-Net+.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5">Ablation Study</head><p>We investigate the role of each component in LiteFlowNet trained on Chairs (i.e., LiteFlowNet-pre) by evaluating the performance of different variants with some of the components disabled unless otherwise stated. The AEE results are summarized in <ref type="table" target="#tab_13">Table 8</ref> and examples of flow fields are illustrated in <ref type="figure" target="#fig_8">Figure 9</ref>. Feature Warping. We consider two variants of LiteFlowNet-pre (WM and WMS) and compare them to the counterparts with feature warping disabled (M and MS). Flow fields from M and MS are more vague. Large degradation in AEE is noticed especially for KITTI 2012 (? 33%) and KITTI 2015 (? 25%). With feature warping (f-warp), pyramidal features that are used as inputs to flow inference are closer in appearance to each other. This facilitates flow estimation in subsequent levels by computing residual flows.</p><p>Descriptor Matching. We evaluate WSR without descriptor matching for which the flow inference part is made as deep as that in the unamended LiteFlowNet-pre (ALL). No noticeable difference between the flow fields from WSR and ALL. Since the maximum displacement of the example flow field is not very large (only 14.7 pixels), accurate flow field can still be yielded from WSR. For evaluation covering a wide range of flow displacement (especially large-displacement benchmark, KITTI), degradation in AEE is noticed for WSR. This suggests that descriptor matching is useful in addressing large-displacement flow.</p><p>Sub-Pixel Refinement. The flow field generated from WMS is more crisp and contains more fine details than that generated from WM with sub-pixel refinement disabled. Less small-magnitude flow artifacts (represented by light color on the background) are observed. Besides, WMS achieves smaller AEE. Since descriptor matching establishes pixel-by-pixel correspondence, sub-pixel refinement is necessary to yield detail-preserving flow fields.</p><p>Regularization. In comparison WMS with regularization disabled to ALL, undesired artifacts exist in homogeneous regions (represented by very dim color on the background) of the flow field generated from WMS. Flow bleeding and vague flow boundaries are observed. Degradation in AEE is also noticed. This suggests that the proposed feature-driven local convolution (f-lconv) plays the vital role to smooth flow field and maintain crisp flow boundaries as regularization term in conventional variational methods.</p><p>Searching Range. We compare three variants of LiteFlowNet2 trained on Chairs using different cost-volume settings as shown in <ref type="table" target="#tab_14">Table 9</ref>. On the whole, a larger searching range leads to a lower AEE. The improvement is more significant on large-displacement benchmark, KITTI. Our design that uses a larger searching range together with a sparse cost volume in a high-resolution pyramid level not only improves flow accuracy but also promotes a more    efficient computation. We choose the second cost-volume setting for our final models due to the fastest computation time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSION</head><p>We have developed a lightweight and effective CNN for addressing the classical problem of optical flow estimation through adopting data fidelity and regularization from variational methods. LiteFlowNet uses pyramidal feature extraction, feature warping, multi-scale cascaded flow inference, and flow regularization to break the de facto rule of accurate flow network requiring large model size. To address large-displacement and detail-preserving flows, it exploits a multi-scale short-range matching to generate a pixel-level flow field and further improves the estimate to subpixel accuracy in each cascaded flow inference. To result crisp flow boundaries, each flow field is adaptively regularized through the feature-driven local convolution. can be deployed to many real-time applications such as video processing, motion segmentation, action recognition, SLAM, 3D reconstruction, and more.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX</head><p>LiteFlowNet2 consists of two compact sub-networks, namely NetC and NetE. NetC is a two-steam sub-network in which the two streams share the same set of filters. The input to NetC is an image pair (I 1 , I 2 ). The network architectures of the 6-level NetC and NetE at pyramid level 5 are provided in <ref type="table" target="#tab_6">Table 10 and Tables 11 to  13</ref>, respectively. We use suffixes "M", "S" and "R" to highlight the layers that are used in descriptor matching, sub-pixel refinement, and flow regularization modules in NetE, respectively. The name of convolution layer is replaced from "conv" to "flow" to highlight when the output is a flow field.  The network details of the descriptor matching unit (M) in NetE at pyramid level 5. "upconv", "f-warp", "corr", and "loss" denote the fractionally strided convolution (so-called deconvolution), feature warping, correlation, and the layer where training loss is applied, respectively. Furthermore, "conv5a' and "conv5b" denote the high-dimensional features of images I 1 and I 2 generated from NetC at pyramid level 5.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 :</head><label>2</label><figDesc>The network structure of LiteFlowNet. For the ease of representation, only a design of 3-level pyramid is shown. Given an image pair (I 1 and I 2 )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Machine Learning Methods. Black et al. propose to represent complex image motion as a linear combination of the learned basis vectors [28]. Roth et al. formulates the prior probability of flow field as Field-of-Experts model [29] that captures higher order spatial statistics [30]. Sun et al. study the probabilistic model of brightness inconstancy in a high-order random field framework [16]. Nir et al. represent image motion using the overparameterization model [31]. Rosenbaum et al. model the local statistics of optical flow using Gaussian mixtures</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 :</head><label>3</label><figDesc>A cascaded flow inference module M :S in NetE. It consists of a descriptor matching unit M and a sub-pixel refinement unit S.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 :</head><label>4</label><figDesc>Folding and packing of f-lconv filters {g}. The (x, y)-entry of a 3D tensor? c (cube on the right) with size H ? W ? ? 2 is a 3D column vector with length w 2 . It corresponds to the unfolded flconv filter g x,y,c (plane on the right) with size ? ?? to be applied at position (x, y) and channel c in the vector-valued feature F.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 :</head><label>5</label><figDesc>A visualization of the learned filters with sizes 3 ? 3 and 5 ? 5 for the horizontal flow component at level 6 (top 2 rows) and level 3 (bottom 2 rows) in the sub-pixel refinement unit of LiteFlowNet2, respectively. The flow regularization module R in NetE that performs the above feature-driven flow smoothing operation has been presented in Section 3.2. By replacing the intermediate flow field u to flow field u s generated from the cascaded flow inference, Eq. (15) corresponds to Eq. (5)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc><ref type="bibr" target="#b15">(16)</ref>. The number of channels of F N ?1 in Eq.(17.2) corresponds to the number of basis vectors in Eq. (16). In particular, the filters W and feature maps F in Eq. (17.2) correspond to the basis vectors {m i } and flow coefficients {a i } in Eq. (16), respectively. The computation of feature maps (i.e., flow coefficients in conventional basis representation) for CNN flow inference is governed by the N ? 1 convolution layers prior to Eq. (17.2).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Final</head><label></label><figDesc>testing sets. On the other hand, when LiteFlowNet2 is finetuned on the same training set as LiteFlowNet (i.e., containing Sintel training set only), AEEs are increased from 3.48 to 3.83 and 4.69 to 5.06 on Sintel Clean and Final testing sets, respectively. Nevertheless, it still outperforms LiteFlowNet. We also train LiteFlowNet using the new fine-tuning protocol. AEE is decreased from 4.54 to 4.01 and 5.38 to 5.21 on the testing sets of Sintel Clean and Final, respectively. Some examples of flow fields on the training and testing sets of Sintel are provided in Figure 7. Since LiteFlowNet(-ft) and LiteFlowNet2(-ft) have flow regularization, sharper flow boundaries and lesser artifacts can be observed in the resulting flow fields.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 7 :</head><label>7</label><figDesc>Examples of flow fields from different methods on Sintel training sets (clean pass: first to second rows, final pass: third to fourth rows) and testing sets (clean pass: fifth row, final pass: last row). Fine details are well preserved and less artifacts can be observed in the flow fields of LiteFlowNet2 and LiteFlowNet2-ft. For the best visual comparison, it is recommended to enlarge the figure electronically. (Note: 1 At the time of submission, the authors [22] only release the trained model of PWC-Net that uses a larger feature encoder (overall footprint: 9.37M vs 8.75M) and has a slower runtime (41.12ms vs 39.63ms) trained on Chairs ? Things3D.) challenging than Sintel not only because the training set of KITTI 2012 and KITTI 2015 contains just less than 400 image pairs but also the flow labels are sparse. The insufficient number of per-pixel flow labels greatly affect the performance of the flow network. When fine-tuning LiteFlowNet2 on KITTI, we upsample the constructed flow fields by a factor of 2 in each pyramid level. This effectively increases the number of per-pixel flow labels available.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 9 :</head><label>9</label><figDesc>Examples of flow fields from different variants of LiteFlowNet trained on Chairs with some of the components disabled. LiteFlowNet is denoted as "All". W = Feature Warping, M = Descriptor Matching, S = Sub-Pixel Refinement, R = Regularization.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>] on Sintel and KITTI benchmarks, while being 25.3 times smaller in the model size and 3.1 times faster in the runtime. The optical flow processing frequency of LiteFlowNet2 reaches up to 25 flow fields per second for an image pair in Sintel dataset with size 1024 ? 436 on a NVIDIA GTX 1080 GPU. Our network protocol and trained models are made publicly available on https://github.com/twhui/LiteFlowNet2.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>are</cell></row><row><cell>presented.</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">3) LiteFlowNet2, another lightweight convolutional net-</cell></row><row><cell cols="4">work, is evolved from LiteFlowNet [13] to better address</cell></row><row><cell cols="4">the problem of optical flow estimation by improving flow</cell></row><row><cell cols="2">accuracy and computation time.</cell><cell></cell><cell></cell></row><row><cell>4) LiteFlowNet2</cell><cell>outperforms</cell><cell>the</cell><cell>state-of-the-art</cell></row><row><cell>FlowNet2 [1</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>In FlowNet2 [1], Ilg et al. introduce a huge network cascade (over 160M parameters) that consists of variants of FlowNet (FlowNetS and FlowNetC). The cascade improves flow accuracy with an expense of model size and computational complexity. A compact network termed SPyNet [2] from Ranjan et al. uses a spatial pyramid network. It warps the second image toward the first one using the estimated flow field from the previous level. But the accuracy is below FlowNet2 (KITTI 2012 [</figDesc><table><row><cell>34]:</cell></row><row><cell>4.1 vs 1.8 measured in AEE, KITTI 2015 [35]: 35.07% vs 11.48%</cell></row><row><cell>measured in Fl-all). On the contrary, LiteFlowNet infers a flow</cell></row><row><cell>field at each pyramid level from the corresponding feature pair</cell></row><row><cell>in the encoder and uses feature warping. LiteFlowNetX, a small-</cell></row><row><cell>sized variant of our network, outperforms SPyNet while being 1.33</cell></row><row><cell>times smaller in the model size. Zweig et al. present a network to</cell></row><row><cell>interpolate third-party sparse flows but requiring off-the-shelf edge</cell></row><row><cell>detector</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>using stage-wise training protocol as follows: First, NetC and M 6 :S 6 of NetE are trained for 300K iterations. Second, R 6 together with the trained network in step 1 are trained for 300K iterations. Third, for levels k ? [5, 2], M k :S k followed by R k is added into the trained network each time. The new network cascade is trained for 240K iterations, except the last-level network is trained for 300K iterations. The new filter weights at level k are initialized from the previous level k ? 1.</figDesc><table><row><cell>The advantages of stage-</cell></row><row><cell>wise training over the conventional training are:</cell></row><row><cell>1) Shorter training time: Since the network stages are gradu-</cell></row><row><cell>ally added in the cascade, the stages that are lately added</cell></row><row><cell>are relatively trained by a smaller number of iterations</cell></row></table><note>than the early added stages. Furthermore, the runtime of the cascade consisting lesser network stages is faster than the more complete network. The overall network, there- fore, requires lesser training time than the conventional training method. The training of LiteFlowNet2 (including training and validation phases) requires 5.5 days instead of 8 days on an NVIDIA TITAN X.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE 2 :</head><label>2</label><figDesc>AEE and runtime (for Sintel) measured at different components (NetC: a feature encoder, NetE: a multi-scale flow decoder) and pyramid levels of LiteFlowNet<ref type="bibr" target="#b12">[13]</ref> trained on Things3D. The percentage change is relative to the previous level.</figDesc><table><row><cell></cell><cell>NetC</cell><cell></cell><cell></cell><cell>NetE</cell><cell></cell><cell></cell></row><row><cell>Level</cell><cell>-</cell><cell>6</cell><cell>5</cell><cell>4</cell><cell>3</cell><cell>2</cell></row><row><cell>Sintel Clean</cell><cell>-</cell><cell>5.41</cell><cell>3.85</cell><cell>3.03</cell><cell>2.65</cell><cell>2.48</cell></row><row><cell></cell><cell>-</cell><cell>-</cell><cell cols="3">-28.8% -21.3% -12.5%</cell><cell>-6.4%</cell></row><row><cell>KITTI 2012</cell><cell>-</cell><cell>8.58</cell><cell>6.04</cell><cell>4.67</cell><cell>4.18</cell><cell>4.00</cell></row><row><cell></cell><cell>-</cell><cell>-</cell><cell cols="3">-29.6% -22.7% -10.5%</cell><cell>-4.4%</cell></row><row><cell cols="2">Runtime (ms) 14.36</cell><cell>1.69</cell><cell>2.03</cell><cell>4.88</cell><cell>13.06</cell><cell>52.51</cell></row><row><cell></cell><cell>-</cell><cell>-</cell><cell cols="4">+20.1% +140% +168% +302%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE 3 :</head><label>3</label><figDesc>AEE of LiteFlowNet2 trained on Chairs using different training protocols against LiteFlowNet<ref type="bibr" target="#b12">[13]</ref>.</figDesc><table><row><cell></cell><cell cols="4">Sintel Clean Sintel Final KITTI12 KITTI15</cell></row><row><cell>LiteFlowNet [13]</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>learning rate: 5e-5</cell><cell>2.94</cell><cell>4.28</cell><cell>4.73</cell><cell>11.75</cell></row><row><cell>LiteFlowNet2</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>learning rate: 5e-5</cell><cell>2.84</cell><cell>4.16</cell><cell>4.29</cell><cell>12.01</cell></row><row><cell>learning rate: 6e-5</cell><cell>2.80</cell><cell>4.14</cell><cell>4.25</cell><cell>11.76</cell></row><row><cell>+ extra training loss</cell><cell>2.78</cell><cell>4.14</cell><cell>4.11</cell><cell>11.31</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE 4 :</head><label>4</label><figDesc>AEE on the Chairs testing set. Models are trained on the Chairs training set.</figDesc><table><row><cell cols="5">FlowNetS FlowNetC SPyNet LiteFlowNetX-pre LiteFlowNet-pre</cell></row><row><cell>2.71</cell><cell>2.19</cell><cell>2.63</cell><cell>2.25</cell><cell>1.57</cell></row><row><cell cols="5">LiteFlowNet and LiteFlowNet2 under different training protocols.</cell></row><row><cell cols="5">Using the same training protocol as LiteFlowNet, LiteFlowNet2</cell></row><row><cell cols="5">outperforms LiteFlowNet on Sintel and KITTI 2012. If the learn-</cell></row><row><cell cols="5">ing rate of LiteFlowNet2 is increased to 6e-5, it is on par with</cell></row><row><cell cols="5">LiteFlowNet on KITTI 2015. Using the learning rate of 6e-5</cell></row><row><cell cols="5">and extra training loss, LiteFlowNet2 outperforms LiteFlowNet on</cell></row><row><cell cols="2">KITTI 2012 and 2015.</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE 5 :</head><label>5</label><figDesc></figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>TABLE 6</head><label>6</label><figDesc>image mirroring during data augmentation as<ref type="bibr" target="#b21">[22]</ref>. AEE on the testing set can be improved from 4.86 to 4.54 for the Clean pass and 6.09 to 5.38 for the Final pass. For fine-tuning on KITTI, we further reduced the amount of augmentation for spatial motion as<ref type="bibr" target="#b21">[22]</ref>. On the respective testing set, AEE can be improved from 1.7 to 1.6 on KITTI 2012 and Fl-all can be improved from 10.24% to 9.38% on KITTI 2015. Examples of flow fields from different methods on the KITTI 2012 and 2015 training sets (2012: first to second rows, 2015: third to fourth row) and testing sets (2012: fifth row, 2015: last row). For the best visual comparison, it is recommended to enlarge the figure electronically. (Note: 1 At the time of submission, the authors [22] only release the trained model of PWC-Net that uses a larger feature encoder (overall footprint: 9.37M vs 8.75M) and has a slower runtime (41.12ms vs 39.63ms) trained on Chairs ? Things3D.) 1238 ? 374, 1241 ? 376, and 1242 ? 375. In order to fulfill the requirement, all the images and flow fields are cropped to 1224 ? 370 before generating LMDB files. A recent work</figDesc><table><row><cell cols="5">: AEE of LiteFlowNet2 fine-tuned on KITTI under</cell></row><row><cell cols="5">different configurations. Out-Noc (or Out-All): Percentage of er-</cell></row><row><cell cols="5">roneous pixels in non-occluded areas (or in total). Fl-bg (or Fl-fg):</cell></row><row><cell cols="5">Percentage of optical flow outliers averaged only over background</cell></row><row><cell cols="5">(or foreground) regions. (Note: 1 Comparing to LiteFlowNet [13],</cell></row><row><cell cols="5">LiteFlowNet2 uses a simplified (pseudo) network structure for</cell></row><row><cell cols="4">flow inference and regularization at level 2 on KITTI.)</cell><cell></cell></row><row><cell></cell><cell>[13]</cell><cell></cell><cell>LiteFlowNet2</cell><cell></cell></row><row><cell>Flow levels up to level 3</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Flow levels up to (pseudo) 1 level 2</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Double GT resolution at each level</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>KITTI 2012: Train</cell><cell>(1.05)</cell><cell>(1.07)</cell><cell>(1.00)</cell><cell>(0.95)</cell></row><row><cell>Test (Out-Noc)</cell><cell>3.27%</cell><cell>3.07%</cell><cell>2.72%</cell><cell>2.63%</cell></row><row><cell>Test (Out-all)</cell><cell>7.27%</cell><cell>6.92%</cell><cell>6.30%</cell><cell>6.16%</cell></row><row><cell>Test (Avg-all)</cell><cell>1.6</cell><cell>1.5</cell><cell>1.4</cell><cell>1.4</cell></row><row><cell>KITTI 2015: Train</cell><cell>(1.62)</cell><cell>(1.61)</cell><cell>(1.47)</cell><cell>(1.33)</cell></row><row><cell>Train (Fl-all)</cell><cell cols="4">(5.58%) (5.57%) (4.80%) (4.32%)</cell></row><row><cell>Test (Fl-bg)</cell><cell>9.66%</cell><cell>8.72%</cell><cell>7.85%</cell><cell>7.62%</cell></row><row><cell>Test (Fl-fg)</cell><cell>7.99%</cell><cell>8.20%</cell><cell>7.20%</cell><cell>7.64%</cell></row><row><cell>Test (Fl-all)</cell><cell>9.38%</cell><cell>8.63%</cell><cell>7.74%</cell><cell>7.62%</cell></row></table><note>An I/O requirement on LMDB generation. We use the modified Caffe package [12] to train and test our optical flow networks. The LMDB script requires all image pairs and flow fields to have the same spatial dimension. We knew the I/O requirement as early as our previous CVPR 2018 work (LiteFlowNet) [13]. There are five types of spatial dimensions in the combined training sets of KITTI 2012 and KITTI 2015, namely 1224 ? 370, 1226 ? 370,</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>TABLE 7 :</head><label>7</label><figDesc>Number of training parameters and runtime. The model for which the runtime is in parentheses is measured using Torch, and hence are not directly comparable to the others using Caffe. (Note:<ref type="bibr" target="#b0">1</ref> The runtime is longer when comparing to the value provided by the authors<ref type="bibr" target="#b47">[48]</ref> because it was measured by a faster NVIDIA TITAN Xp GPU than ours.)</figDesc><table><row><cell></cell><cell>Shallow</cell><cell></cell><cell></cell><cell></cell><cell>Deep</cell><cell></cell><cell></cell></row><row><cell>Model</cell><cell cols="7">FlowNetC [12] SPyNet [2] FlowNet2 [1] PWC-Net+ [48] LiteFlowNetX [13] LiteFlowNet [13] LiteFlowNet2</cell></row><row><cell>Number of learnable layers</cell><cell>26</cell><cell>35</cell><cell>115</cell><cell>59</cell><cell>69</cell><cell>94</cell><cell>91</cell></row><row><cell>Number of parameters (M)</cell><cell>39.16</cell><cell>1.20</cell><cell>162.49</cell><cell>8.75</cell><cell>0.90</cell><cell>5.37</cell><cell>6.42</cell></row><row><cell>Runtime (ms)</cell><cell>31.51</cell><cell>(129.83)</cell><cell>121.49</cell><cell>39.63 1</cell><cell>35.10</cell><cell>88.53</cell><cell>39.69</cell></row><row><cell>Frame/second (fps)</cell><cell>31</cell><cell>(8)</cell><cell>8</cell><cell>25</cell><cell>28</cell><cell>12</cell><cell>25</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>TABLE 8 :</head><label>8</label><figDesc>AEE of different variants of LiteFlowNet trained onChairs dataset with some of the components disabled.16.19 14.52 13.20 12.32 11.58    </figDesc><table><row><cell>Variants</cell><cell>M</cell><cell>MS</cell><cell>WM</cell><cell cols="2">WSR WMS</cell><cell>ALL</cell></row><row><cell>Feature Warping</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Descriptor Matching</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Sub-pixel Refinement</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Regularization</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>FlyingChairs (train)</cell><cell>3.75</cell><cell>2.70</cell><cell>2.98</cell><cell>1.63</cell><cell>1.82</cell><cell>1.57</cell></row><row><cell>Sintel clean (train)</cell><cell>4.70</cell><cell>4.17</cell><cell>3.54</cell><cell>3.19</cell><cell>2.90</cell><cell>2.78</cell></row><row><cell>Sintel final (train)</cell><cell>5.69</cell><cell>5.30</cell><cell>4.81</cell><cell>4.63</cell><cell>4.45</cell><cell>4.17</cell></row><row><cell>KITTI 2012 (train)</cell><cell>9.22</cell><cell>8.01</cell><cell>6.17</cell><cell>5.03</cell><cell>4.83</cell><cell>4.56</cell></row><row><cell>KITTI 2015 (train)</cell><cell>18.24</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>TABLE 9 :</head><label>9</label><figDesc>AEE and runtime of LiteFlowNet2 trained on Chairs under different cost-volume settings. The value in parentheses represents the setting at level 3.</figDesc><table><row><cell>Searching Range (pixels)</cell><cell>3</cell><cell>3 (6)</cell><cell>4</cell></row><row><cell>Stride</cell><cell>1</cell><cell>1 (2)</cell><cell>1</cell></row><row><cell>Levels</cell><cell cols="3">6 to 3 6 to 4 (3) 6 to 3</cell></row><row><cell>Sintel Clean (train)</cell><cell>2.73</cell><cell>2.78</cell><cell>2.71</cell></row><row><cell>Sintel Final (train)</cell><cell>4.14</cell><cell>4.14</cell><cell>4.14</cell></row><row><cell>KITTI 2012 (train)</cell><cell>4.26</cell><cell>4.11</cell><cell>4.20</cell></row><row><cell>KITTI 2015 (train)</cell><cell>11.72</cell><cell>11.31</cell><cell>11.12</cell></row><row><cell>Runtime (ms)</cell><cell>41.33</cell><cell>39.69</cell><cell>44.33</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head></head><label></label><figDesc>The evolution of LiteFlowNet creates LiteFlowNet2, which runs 2.2 times faster and attains a better flow accuracy. LiteFlowNet2 outperforms the state-of-theart FlowNet2 [1] on Sintel and KITTI benchmarks while being 3.1 times faster in the runtime and 25.3 times smaller in the model size. It also outperforms PWC-Net+ [48] on KITTI 2012 and 2015, and is on par with PWC-Net+ on Sintel Clean and Final while being 1.4 times smaller in the model size. With its lightweight, accurate, and fast flow computation, LiteFlowNet2</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>TABLE 10 :</head><label>10</label><figDesc>The network details of NetC. "# Ch. In / Out" means the number of input or output channels of the feature maps. "conv" denotes convolution.</figDesc><table><row><cell cols="4">Layer name Kernel Stride # Ch. In / Out</cell><cell>Input</cell></row><row><cell>conv1</cell><cell>7 ? 7</cell><cell>1</cell><cell>3 / 32</cell><cell>I1 or I2</cell></row><row><cell>conv2_1</cell><cell>3 ? 3</cell><cell>2</cell><cell>32 / 32</cell><cell>conv1</cell></row><row><cell>conv2_2</cell><cell>3 ? 3</cell><cell>1</cell><cell>32 / 32</cell><cell>conv2_1</cell></row><row><cell>conv2_3</cell><cell>3 ? 3</cell><cell>1</cell><cell>32 / 32</cell><cell>conv2_2</cell></row><row><cell>conv3_1</cell><cell>3 ? 3</cell><cell>2</cell><cell>32 / 64</cell><cell>conv2_3</cell></row><row><cell>conv3_2</cell><cell>3 ? 3</cell><cell>1</cell><cell>64 / 64</cell><cell>conv3_1</cell></row><row><cell>conv4_1</cell><cell>3 ? 3</cell><cell>2</cell><cell>64 / 96</cell><cell>conv3_2</cell></row><row><cell>conv4_2</cell><cell>3 ? 3</cell><cell>1</cell><cell>96 / 96</cell><cell>conv4_1</cell></row><row><cell>conv5</cell><cell>3 ? 3</cell><cell>2</cell><cell>96 / 128</cell><cell>conv4_2</cell></row><row><cell>conv6</cell><cell>3 ? 3</cell><cell>2</cell><cell>128 / 192</cell><cell>conv5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>TABLE 11 :</head><label>11</label><figDesc></figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19"><head>TABLE 12 :</head><label>12</label><figDesc>Network details of sub-pixel refinement module (S) in NetE at pyramid level 5.</figDesc><table><row><cell>Layer name</cell><cell>Kernel</cell><cell>Stride</cell><cell># Ch. In / Out</cell><cell>Input</cell></row><row><cell>f-warp5_S</cell><cell>-</cell><cell>-</cell><cell>128, 2 / 128</cell><cell>conv5b, flow5_C</cell></row><row><cell>conv5_1_S</cell><cell>3?3</cell><cell>1</cell><cell>258 / 128</cell><cell>concat(conv5a, f-warp5_S, flow5_C)</cell></row><row><cell>conv5_2_S</cell><cell>3 ? 3</cell><cell>1</cell><cell>128 / 128</cell><cell>conv5_1_S</cell></row><row><cell>conv5_3_S</cell><cell>3 ? 3</cell><cell>1</cell><cell>128 / 96</cell><cell>conv5_2_S</cell></row><row><cell>conv5_4_S</cell><cell>3 ? 3</cell><cell>1</cell><cell>96 / 64</cell><cell>conv5_3_S</cell></row><row><cell>conv5_5_S</cell><cell>3 ? 3</cell><cell>1</cell><cell>64 / 32</cell><cell>conv5_4_S</cell></row><row><cell>conv5_6_S</cell><cell>3 ? 3</cell><cell>1</cell><cell>32 / 2</cell><cell>conv5_5_S</cell></row><row><cell cols="3">flow5_S, loss5_S element-wise sum</cell><cell>2, 2 / 2</cell><cell>flow5_C, conv5_4_S</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">. We can also use the f-warp layer to displace each channel differently when multiple flow fields are supplied. The usage, however, is beyond the scope of this work.</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="bibr" target="#b12">13</ref><p>: Network details of flow regularization module (R) in NetE at pyramid level 5. "warp", "norm", "softmax", and "f-lcon" denote the image warping, L2 norm of the RGB brightness difference between the two input images, normalized exponential operation over each 1 ? 1 ? (# Ch. In) column in the 3-D tensor, and feature-driven local convolution, respectively. Furthermore, "conv_dist" highlight the output of the convolution layer is used as the feature-driven distance metric D. "im5a" and "im5b" denote the down-sized images of I 1 and I 2 at pyramidal level 5. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Layer name</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">FlowNet2.0: Evolution of optical flow estimation with deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Saikia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Keuper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="page" from="2462" to="2470" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Optical flow estimation using a spatial pyramid network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4161" to="4170" />
		</imprint>
	</monogr>
	<note type="report_type">CVPR</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">LiteFlowNet: A lightweight convolutional neural network for optical flow estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-W</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.07036</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Determining optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">K P</forename><surname>Horn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">G</forename><surname>Schunck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Arifical Intelligence</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="185" to="203" />
			<date type="published" when="1981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">An iterative image registration technique with an application to stereo vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">D</forename><surname>Lucas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCAI</title>
		<imprint>
			<biblScope unit="page" from="674" to="679" />
			<date type="published" when="1981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">High accuracy optical flow estimation based on a theory for warping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bruhn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Papenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weickert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ECCV</title>
		<imprint>
			<biblScope unit="page" from="25" to="36" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Highly accurate optic flow computation with theoretically justified warping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Papenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bruhn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Didas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weickert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="141" to="158" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Large displacement optical flow: Descriptor matching in variational motion estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mailk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="500" to="513" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Determining motion directly from normal flows upon the use of a spherical eye platform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-W</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chung</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2267" to="2274" />
		</imprint>
	</monogr>
	<note type="report_type">CVPR</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Determining shape and motion from non-overlapping multicamera rig: A direct approach using normal flows</title>
	</analytic>
	<monogr>
		<title level="j">CVIU</title>
		<imprint>
			<biblScope unit="volume">117</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="947" to="964" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Determining shape and motion from monocular camera: A direct approach using normal flows</title>
	</analytic>
	<monogr>
		<title level="j">PR</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="422" to="437" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>H?usser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Haz?rba?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Golkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Van Der Smagt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FlowNet: Learning optical flow with convolutional networks</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2758" to="2766" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">LiteFlowNet: A lightweight convolutional neural network for optical flow estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-W</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="page" from="8981" to="8989" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Motion detail preserving optical flow estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Matsushita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1744" to="1757" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Optical flow via locally adaptive fusion of complementary data costs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<biblScope unit="page" from="2373" to="2381" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Learning optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="83" to="97" />
		</imprint>
	</monogr>
	<note type="report_type">ECCV</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2017" to="2025" />
		</imprint>
	</monogr>
	<note>Spatial transformer networks,&quot; NIPS</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Werlberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Trobin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
		<title level="m">Anisotropic Huber-L 1 optical flow</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
	<note type="report_type">BMVC</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Optic flow in harmony</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zimmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bruhn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weickert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">93</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="368" to="388" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">DeepFace: Closing the gap to human-level performance in face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Taigman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="page" from="1701" to="1708" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Dynamic filter networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">D</forename><surname>Brabandere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">PWC-Net: CNNs for optical flow using pyramid, warping, and cost volume</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="page" from="8934" to="8943" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">U-Net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
	<note type="report_type">MICCAI</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">DeepFlow: Large displacement optical flow with deep matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Revaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="500" to="513" />
		</imprint>
	</monogr>
	<note type="report_type">ICCV</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">PatchMatch Filter: Efficient edge-aware filtering meets randomized search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">N</forename><surname>Do</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="page" from="1854" to="1861" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Patchmatch: A randomized correspondence algorithm for structural image editing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Finkelstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">B</forename><surname>Goldman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIGGRAGH</title>
		<imprint>
			<biblScope unit="page" from="83" to="97" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">EpicFlow: Edge-preserving interpolation of correspondences for optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Revaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1164" to="1172" />
		</imprint>
	</monogr>
	<note type="report_type">CVPR</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning parameterized models of image motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yacoobt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">D</forename><surname>Jepsont</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Fleets</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="page" from="674" to="679" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Fields of experts: A framework for learning image priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="860" to="867" />
		</imprint>
	</monogr>
	<note type="report_type">CVPR</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">On the spatial statistics of optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Black</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="42" to="49" />
		</imprint>
	</monogr>
	<note type="report_type">ICCV</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Over-parameterized variational optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Nir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Bruckstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kimmel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">76</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="205" to="216" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning the local statistics of optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rosenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zoran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<biblScope unit="page" from="2373" to="2381" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Efficient sparse-to-dense optical flow estimation using a learned basis and layers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wulff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="page" from="120" to="130" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Are we ready for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="3354" to="3361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Object scene flow for autonomous vehicles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Menze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3061" to="3070" />
		</imprint>
	</monogr>
	<note type="report_type">CVPR</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Interponet, a brain inspired neural network for optical flow dense interpolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zweig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6363" to="6372" />
		</imprint>
	</monogr>
	<note type="report_type">CVPR</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Learning to compare image patches via convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4353" to="4361" />
		</imprint>
	</monogr>
	<note type="report_type">CVPR</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Deep discrete flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Gney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ACCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">CNN-based patch matching for optical flow with thresholded hinge embedding loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bailer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Varanasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Stricker</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3250" to="3259" />
		</imprint>
	</monogr>
	<note type="report_type">CVPR</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Flow Fields: Dense correspondence fields for highly accurate large displacement optical flow estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bailer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Taetz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Stricker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<biblScope unit="page" from="4015" to="4023" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Vector-valued image regularization with PDEs: A common framework for different applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tschumperl?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Deriche</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="506" to="517" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Back to Basics: Unsupervised learning of optical flow via brightness constancy and motion smoothness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Harley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">G</forename><surname>Derpanis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ECCVW</publisher>
			<biblScope unit="page" from="3" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Bilateral filtering-based optical flow estimation with occlusion detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sawhney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Isnardi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ECCV</title>
		<imprint>
			<biblScope unit="page" from="211" to="224" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">A large dataset to train convolutional networks for disparity, optical flow, and scene flow estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Husser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="page" from="4040" to="4048" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">A naturalistic open source movie for optical flow evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Butler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wulff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">B</forename><surname>Stanley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ECCV</title>
		<imprint>
			<biblScope unit="page" from="611" to="625" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">A quantitative analysis of current practices in optical flow estimation and the principles behind them</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">106</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="115" to="137" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Accurate optical flow via direct cost volume processings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ranftl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1289" to="1297" />
		</imprint>
	</monogr>
	<note type="report_type">CVPR</note>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Models matter, so does training: An empirical study of CNNs for optical flow estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">TPAMI</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">The HCI benchmark suite: Stereo and flow ground truth with uncertainties for urban autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kondermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Honauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Krispin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Andrulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gussefeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rahimimoghaddam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hofmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR Workshops</title>
		<imprint>
			<biblScope unit="page" from="19" to="28" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
