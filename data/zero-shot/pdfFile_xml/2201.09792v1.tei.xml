<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">P A A Y N ?</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asher</forename><surname>Trockman</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Zico</forename><surname>Kolter</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Bosch Center for AI</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">P A A Y N ?</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>Preprint</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T16:41+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>A Although convolutional networks have been the dominant architecture for vision tasks for many years, recent experiments have shown that Transformer-based models, most notably the Vision Transformer (ViT), may exceed their performance in some settings. However, due to the quadratic runtime of the self-attention layers in Transformers, ViTs require the use of patch embeddings, which group together small regions of the image into single input features, in order to be applied to larger image sizes. This raises a question: Is the performance of ViTs due to the inherently-more-powerful Transformer architecture, or is it at least partly due to using patches as the input representation? In this paper, we present some evidence for the latter: specifically, we propose the ConvMixer, an extremely simple model that is similar in spirit to the ViT and the even-more-basic MLP-Mixer in that it operates directly on patches as input, separates the mixing of spatial and channel dimensions, and maintains equal size and resolution throughout the network. In contrast, however, the ConvMixer uses only standard convolutions to achieve the mixing steps. Despite its simplicity, we show that the ConvMixer outperforms the ViT, MLP-Mixer, and some of their variants for similar parameter counts and data set sizes, in addition to outperforming classical vision models such as the ResNet. Our code is available at httpsFigure 2: ConvMixer uses "tensor layout" patch embeddings to preserve locality, and then applies d copies of a simple fully-convolutional block consisting of large-kernel depthwise convolution followed by pointwise convolution, before finishing with global pooling and a simple linear classifier.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>For many years, convolutional neural networks have been the dominant architecture for deep learning systems applied to computer vision tasks. But recently, architectures based upon Transformer models, e.g., the so-called Vision Transformer architecture <ref type="bibr">(Dosovitskiy et al., 2020)</ref>, have demonstrated compelling performance in many of these tasks, often outperforming classical convolutional architectures, especially for large data sets. An understandable assumption, then, is that it is only a matter of time before Transformers become the dominant architecture for vision domains, just as they have for language processing. In order to apply Transformers to images, however, the representation had to be changed: because the computational cost of the self-attention layers used in Transformers would scale quadratically with the number of pixels per image if applied naively at the per-pixel level, the compromise was to first split the image into multiple "patches", linearly embed them, and then apply the transformer directly to this collection of patches.</p><p>In this work, we explore the question of whether, fundamentally, the strong performance of vision transformers may result more from this patch-based representation than from the Transformer architecture itself. We develop a very simple convolutional architecture which we dub the "ConvMixer" due to its similarity to the recently-proposed MLP-Mixer <ref type="bibr" target="#b11">(Tolstikhin et al., 2021)</ref>. This architecture is similar to the Vision Transformer (and MLP-Mixer) in many respects: it directly operates on patches, it maintains an equal-resolution-and-size representation throughout all layers, it does no downsampling of the representation at successive layers, and it separates "channel-wise mixing" from the "spatial mixing" of information. But unlike the Vision Transformer and MLP-Mixer, our architecture does all these operations via only standard convolutions.</p><p>The chief result we show in this paper is that this ConvMixer architecture, despite its extreme simplicity (it can be implemented in ? 6 lines of dense PyTorch code), outperforms both "standard" computer vision models such as ResNets of similar parameter counts and some corresponding Vision Transformer and MLP-Mixer variants, even with a slate of additions intended to make those architectures more performant on smaller data sets. Importantly, this is despite the fact that we did not design our experiments to maximize accuracy nor speed, in contrast to the models we compared against. Our results suggest that, at least to some extent, the patch representation itself may be a critical component to the "superior" performance of newer architectures like Vision Transformers. While these results are naturally just a snapshot, and more experiments are required to exactly disentangle the effect of patch embeddings from other factors, we believe that this provides a strong "convolutionalbut-patch-based" baseline to compare against for more advanced architectures in the future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">A S M : C M</head><p>Our model, dubbed ConvMixer, consists of a patch embedding layer followed by repeated applications of a simple fully-convolutional block. We maintain the spatial structure of the patch embeddings, as illustrated in <ref type="figure">Fig. 2</ref>. Patch embeddings with patch size p and embedding dimension h can be implemented as convolution with c in input channels, h output channels, kernel size p, and stride p:</p><formula xml:id="formula_0">z 0 = BN (?{Conv c in ?h (X, stride=p, kernel_size=p)})<label>(1)</label></formula><p>The ConvMixer block itself consists of depthwise convolution (i.e., grouped convolution with groups equal to the number of channels, h) followed by pointwise (i.e., kernel size 1 ? 1) convolution. As we will explain in Sec. 3, ConvMixers work best with unusually large kernel sizes for the depthwise convolution. Each of the convolutions is followed by an activation and post-activation BatchNorm:</p><formula xml:id="formula_1">z l = BN (?{ConvDepthwise(z l?1 )}) + z l?1 (2) z l+1 = BN (?{ConvPointwise(z l )})<label>(3)</label></formula><p>After many applications of this block, we perform global pooling to get a feature vector of size h, which we pass to a softmax classifier. See <ref type="figure" target="#fig_2">Fig. 3</ref> for an implementation of ConvMixer in PyTorch. Design parameters. An instantiation of ConvMixer depends on four parameters: (1) the "width" or hidden dimension h (i.e., the dimension of the patch embeddings), (2) the depth d, or the number of repetitions of the ConvMixer layer, (3) the patch size p which controls the internal resolution of the model, (4) the kernel size k of the depthwise convolutional layer. We name ConvMixers after their hidden dimension and depth, like ConvMixer-h/d. We refer to the original input size n divided by the patch size p as the internal resolution; note, however, that ConvMixers support variable-sized inputs. Motivation. Our architecture is based on the idea of mixing, as in <ref type="bibr" target="#b11">Tolstikhin et al. (2021)</ref>. In particular, we chose depthwise convolution to mix spatial locations and pointwise convolution to mix channel locations. A key idea from previous work is that MLPs and self-attention can mix distant spatial locations, i.e., they can have an arbitrarily large receptive field. Consequently, we used convolutions with an unusually large kernel size to mix distant spatial locations.</p><p>While self-attention and MLPs are theoretically more flexible, allowing for large receptive fields and content-aware behavior, the inductive bias of convolution is well-suited to vision tasks and leads to high data efficiency. By using such a standard operation, we also get a glimpse into the effect of the patch representation itself in contrast to the conventional pyramid-shaped, progressivelydownsampling design of convolutional networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">E</head><p>Training setup. We primarily evaluate ConvMixers on ImageNet-1k classification without any pretraining or additional data. We added ConvMixer to the timm framework <ref type="bibr" target="#b17">(Wightman, 2019)</ref> and trained it with nearly-standard settings: we used RandAugment <ref type="bibr">(Cubuk et al., 2020</ref><ref type="bibr">), mixup (Zhang et al., 2017</ref><ref type="bibr">), CutMix (Yun et al., 2019</ref>, random erasing <ref type="bibr">(Zhong et al., 2020)</ref>, and gradient norm clipping in addition to default timm augmentation. We used the AdamW <ref type="bibr" target="#b6">(Loshchilov &amp; Hutter, 2018)</ref> optimizer and a simple triangular learning rate schedule. Due to limited compute, we did absolutely no hyperparameter tuning on ImageNet and trained for fewer epochs than competitors. Consequently, our models could be over-or under-regularized, and the accuracies we report likely underestimate the capabilities of our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results.</head><p>A ConvMixer-1536/20 with 52M parameters can achieve 81.4% top-1 accuracy on Ima-geNet, and a ConvMixer-768/32 with 21M parameters 80.2% (see <ref type="table" target="#tab_0">Table 1</ref>). Wider ConvMixers seem to converge in fewer epochs, but are memory-and compute-hungry. They also work best with large kernel sizes: ConvMixer-1536/20 lost ? 1% accuracy when reducing the kernel size from k = 9 to k = 3 (we discuss kernel sizes more in Appendix A &amp; B). ConvMixers with smaller patches are substantially better in our experiments, similarly to <ref type="bibr" target="#b10">Sandler et al. (2019)</ref>; we believe larger patches require deeper ConvMixers. With everything held equal except increasing the patch size from 7 to 14, ConvMixer-1536/20 achieves 78.9% top-1 accuracy but is around 4? faster. We trained one model with ReLU to demonstrate that GELU <ref type="bibr" target="#b2">(Hendrycks &amp; Gimpel, 2016)</ref>, which is popular in recent isotropic models, isn't necessary. Comparisons. Our model and ImageNet1k-only training setup closely resemble that of recent patchbased models like DeiT <ref type="bibr" target="#b12">(Touvron et al., 2020)</ref>. Due to ConvMixer's simplicity, we focus on comparing to only the most basic isotropic patch-based architectures adapted to the ImageNet-1k setting, namely DeiT and ResMLP. Attempting a fair comparison with a standard baseline, we trained ResNets using exactly the same parameters as ConvMixers; while this choice of parameters is suboptimal <ref type="bibr">(Wightman et al., 2021)</ref>, it is likely also suboptimal for ConvMixers, since we did no hyperparameter tuning.</p><p>Looking at <ref type="table" target="#tab_0">Table 1</ref> and <ref type="figure" target="#fig_1">Fig. 1</ref>, ConvMixers achieve competitive accuracies for a given parameter budget: ConvMixer-1536/20 outperforms both ResNet-152 and ResMLP-B24 despite having substantially fewer parameters and is competitive with DeiT-B. ConvMixer-768/32 uses just a third of the parameters of ResNet-152, but is similarly accurate. Note that unlike ConvMixer, the DeiT and ResMLP results involved hyperparameter tuning, and when substantial resources are dedicated to tuning ResNets, including training for twice as many epochs, they only outperform an equivalently-sized ConvMixer by ? 0.2% <ref type="bibr">(Wightman et al., 2021)</ref>. However, ConvMixers are substantially slower at inference than the competitors, likely due to their smaller patch size; hyperparameter tuning and optimizations could narrow this gap. For more discussion and comparisons, see <ref type="table" target="#tab_3">Table 2</ref> and Appendix A.</p><p>Hyperparameters. For experiments presented in the main text, we used only one set of "common sense" parameters for the regularization methods. Recently, we adapted parameters from the A1 procedure in <ref type="bibr">Wightman et al. (2021)</ref>, published after our work, which were better than our initial guess, e.g., giving +0.8% for ConvMixer-1536/20, or 82.2% top-1 accuracy (see Appendix A). CIFAR-10 Experiments. We also performed smaller-scale experiments on CIFAR-10, where Con-vMixers achieve over 96% accuracy with as few as 0.7M parameters, demonstrating the data efficiency of the convolutional inductive bias. Details of these experiments are presented in Appendix B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">R W</head><p>Isotropic architectures. Vision transformers have inspired a new paradigm of "isotropic" architectures, i.e., those with equal size and shape throughout the network, which use patch embeddings for the first layer. These models look similar to repeated transformer-encoder blocks <ref type="bibr" target="#b15">(Vaswani et al., 2017)</ref> with different operations replacing the self-attention and MLP operations. For example, MLP-Mixer <ref type="bibr" target="#b11">(Tolstikhin et al., 2021)</ref> replaces them both with MLPs applied across different dimensions (i.e., spatial and channel location mixing); ResMLP <ref type="bibr" target="#b13">(Touvron et al., 2021a)</ref> is a data-efficient variation on this theme. CycleMLP (Chen et al., 2021), gMLP <ref type="bibr" target="#b4">(Liu et al., 2021a)</ref>, and vision permutator <ref type="bibr" target="#b3">(Hou et al., 2021)</ref>, replace one or both blocks with various novel operations. These are all quite performant, which is typically attributed to the novel choice of operations. In contrast, Melas-Kyriazi (2021) proposed an MLP-based isotropic vision model, and also hypothesized patch embeddings could be behind its performance. ResMLP tried replacing its linear interaction layer with (small-kernel) convolution and achieved good performance, but kept its MLP-based cross-channel layer and did not explore convolutions further. As our investigation of ConvMixers suggests, these works may conflate the effect of the new operations (like self-attention and MLPs) with the effect of the use of patch embeddings and the resulting isotropic architecture.</p><p>A study predating vision transformers investigate isotropic (or "isometric") MobileNets <ref type="bibr" target="#b10">(Sandler et al., 2019)</ref>, and even implements patch embeddings under another name. Their architecture simply repeats an isotropic MobileNetv3 block. They identify a tradeoff between patch size and accuracy that matches our experience, and train similarly performant models (see Appendix A, <ref type="table" target="#tab_3">Table 2</ref>). However, their block is substantially more complex than ours; simplicity and motivation sets our work apart. Patches aren't all you need. Several papers have increased vision transformer performance by replacing standard patch embeddings with a different stem: Xiao et al. <ref type="formula" target="#formula_0">(2021)</ref>   <ref type="bibr" target="#b1">Guo et al., 2021)</ref>, or include downsampling to be more like traditional pyramid-shaped convolutional networks . Conversely, self-attention or attention-like operations can supplement or replace convolution in ResNet-style models <ref type="bibr" target="#b9">(Bello et al., 2019;</ref><ref type="bibr" target="#b9">Ramachandran et al., 2019;</ref><ref type="bibr">Bello, 2021)</ref>. While all of these attempts have been successful in one way or another, they are orthogonal to this work, which aims to emphasize the effect of the architecture common to most ViTs by showcasing it with a less-expressive operation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">C</head><p>We presented ConvMixers, an extremely simple class of models that independently mixes the spatial and channel locations of patch embeddings using only standard convolutions. We also highlighted that using large kernel sizes, inspired by the large receptive fields of ViTs and MLP-Mixers, provides a substantial performance boost. While neither our model nor our experiments were designed to maximize accuracy or speed, i.e., we did not search for good hyperparameters, ConvMixers outperform the Vision Transformer and MLP-Mixer, and are competitive with ResNets, DeiTs, and ResMLPs.</p><p>We provided evidence that the increasingly common "isotropic" architecture with a simple patch embedding stem is itself a powerful template for deep learning. Patch embeddings allow all the downsampling to happen at once, immediately decreasing the internal resolution and thus increasing the effective receptive field size, making it easier to mix distant spatial information. Our title, while an exaggeration, points out that attention isn't the only export from language processing into computer vision: tokenizing inputs, i.e., using patch embeddings, is also a powerful and important takeaway.</p><p>While our model is not state-of-the-art, we find its simple patch-mixing design to be compelling. We hope that ConvMixers can serve as a baseline for future patch-based architectures with novel operations, or that they can provide a basic template for new conceptually simple and performant models.  Experiment overview. We did not design our experiments to maximize accuracy: We chose "common sense" parameters for timm and its augmentation settings, found that it worked well for a ConvMixer-1024/12, and stuck with them for the proceeding experiments. We admit this is not an optimal strategy, however, we were aware from our early experiments on CIFAR-10 that results seemed robust to various small changes. We did not have access to sufficient compute to attempt to tune hyperparameters for each model: e.g., larger ConvMixers could probably benefit from more regularization than we chose, and smaller ones from less regularization. Keeping the parameters the same across ConvMixer instances seemed more reasonable than guessing for each.</p><p>However, to some extent, we changed the number of epochs per model: for earlier experiments, we merely wanted a "proof of concept", and used only 90-100 epochs. Once we saw potential, we increased this to 150 epochs and trained some larger models, namely ConvMixer-1024/20 with p = 14 patches and ConvMixer-1536/20 with p = 7 patches. Then, believing that we should explore deeper-but-less-wide ConvMixers, and knowing from CIFAR-10 that the deeper models converged more slowly, we trained ConvMixer-768/32s with p = 14 and p = 7 for 300 epochs. Of course, training time was a consideration: ConvMixer-1536/20 took about 9 days to train (on 10? RTX8000s) 150 epochs, and ConvMixer-768/32 is over twice as fast, making 300 epochs more feasible.</p><p>If anything, we believe that in the worst case, the lack of parameter tuning in our experiments resulted in underestimating the accuracies of ConvMixers. Further, due to our limited compute and the fact that large models (particularly ConvMixers) are expensive to train on large data sets, we generally trained our models for fewer epochs than competition like DeiT and ResMLP (see <ref type="table" target="#tab_3">Table 2</ref>).</p><p>In this revision, we have added some additional results (denoted with a in <ref type="table" target="#tab_3">Table 2</ref>) using hyperparameters loosely based on the precisely-crafted "A1 training procedure" from <ref type="bibr">Wightman et al. (2021)</ref>.</p><p>In particular, we adjusted parameters for RandAug, Mixup, CutMix, Random Erasing, and weight decay to match those in the procedure. Importantly, we still only trained for 150 epochs, rather than the 600 epochs used in <ref type="bibr">Wightman et al. (2021)</ref>, and we did not use binary cross-entropy loss nor repeated augmentation. While we do not think optimal hyperparameters for ResNet would also be optimal for ConvMixer, these settings are significantly better than the ones we initially chose. This further highlights the capabilities of ConvMixers, and we are optimistic that further tuning could lead to still-better performance. Throughout the paper, we still refer to ConvMixers trained using our initial "one shot" selection of hyperparameters. A note on throughput. We measured throughput using batches of 64 images in half precision on a single RTX8000 GPU, averaged over 20 such batches. In particular, we measured CUDA execution time rather than "wall-clock" time. We noticed discrepancies in the relative throughputs of models, e.g., <ref type="bibr" target="#b12">Touvron et al. (2020)</ref> reports that ResNet-152 is 2? faster than DeiT-B, but our measurements show that the two models have nearly the same throughput. We therefore speculate that our throughputs may underestimate the performance of ResNets and ConvMixers relative to the transformers. The difference may be due to using RTX8000 rather than V100 GPUs, or other low-level differences. Our throughputs were similar for batch sizes 32 and 128. ResNets. As a simple baseline to which to compare ConvMixers, we trained three standard ResNets using exactly the same training setup and parameters as ConvMixer-1536/20. Despite having fewer parameters and being architecturally much simpler, ConvMixers substantially outperform these ResNets in terms of accuracy. A possible confounding factor is that ConvMixers use GELU, which may boost performance, while ResNets use ReLU. In an attempt to rule out this confound, we used ReLU in a later ConvMixer-768/32 experiment and found that it still achieved competitive accuracy. We also note that the choice of ReLU vs. GELU was not important on CIFAR-10 experiments (see <ref type="table" target="#tab_6">Table 3</ref>). However, ConvMixers do have substantially less throughput. DeiTs. We believe that DeiT is the most reasonable comparison in terms of vision transformers: It only adds additional regularization, as opposed to architectural additions in the case of CaiT <ref type="bibr" target="#b14">(Touvron et al., 2021b)</ref>, and is then essentially a "vanilla" ViT modulo the distillation token (we don't consider distilled architectures A confounding factor is the difference in patch size between DeiT and ConvMixer; DeiT uses p = 16 while ConvMixer uses p = 7. This means DeiT is substantially faster. However, ConvMixers using larger patches are not as competitive. While we were not able to train DeiTs with larger patch sizes, it is possible that they would outperform ConvMixers on the parameter count vs. accuracy curve; however, we tested their throughput for p = 7, and they are even slower than ConvMixers. Given the difference between convolution and self-attention, we are not sure it is salient to control for patch size differences.</p><p>DeiTs were subject to more hyperparameter tuning than ConvMixers, as well as longer training times. They also used stochastic depth while we did not, which can in some cases contribute percent differences in model accuracy <ref type="bibr" target="#b13">(Touvron et al., 2021a)</ref>. It is therefore possible that further hyperparameter tuning and more epochs for ConvMixers could close the gap between the two architectures for large patches, e.g., p = 16. ResMLPs. Similarly to DeiT for ViT, we believe that ResMLP is the most relevant MLP-Mixer variant to compare against. Unlike DeiT, we can compare against instances of ResMLP with similar patch size: ResMLP-B24/8 has p = 8 patches, and underperforms ConvMixer-1536/20 by 0.37%, despite having over twice the number of parameters; it also has similarly low throughput. ConvMixer-768/32 also outperforms ResMLP-S12/8 for millions fewer parameters, but 3? less throughput.</p><p>ResMLP did not significantly improve in terms of accuracy for halving the patch size from 16 to 8, which shows that smaller patches do not always lead to better accuracy for a fixed architecture and regularization strategy (e.g., training a p = 8 DeiT may be challenging). Swin Transformers. While we intend to focus on the most basic isotropic, patch-based architectures for fair comparisons with ConvMixer, it is also interesting to compare to a more complicated model that is closer to state-of-the-art. For a similar parameter budget, ConvMixer is around 1.2-1.6% less accurate than the Swin Transformer, while also being 4-6? slower. However, considering we did not attempt to tune or optimize our model in any way, we find it surprising that an exceedingly simple patch-based model that uses only plain convolution does not lag too far behind Swin Transformer. Isotropic MobileNets. These models are closest in design to ours, despite using a repeating block that is substantially more complex than the ConvMixer one. Despite this, for a similar number of parameters, we can get similar performance. Notably, isotropic MobileNets seem to suffer less from larger patch sizes than ConvMixers, which makes us optimistic that sufficient parameter tuning could lead to more performant large-patch ConvMixers. Other models. We included ViT and MLP-Mixer instances in our table, though they are not competitive with ConvMixer, DeiT, or ResMLP, even though MLP-Mixer has comparable regularization to ConvMixer. That is, ConvMixer seems to outperform MLP-Mixer and ViT, while being closer to complexity to them in terms of design and training regime than the other competitors, DeiT and ResMLP. Kernel size. While we found some evidence that larger kernels are better on CIFAR-10, we wanted to see if this finding transferred to ImageNet. Consequently, we trained our best-performing model, ConvMixer-1536/20, with kernel size k = 3 rather than k = 9. This resulted in a decrease of 0.94% top-1 accuracy, which we believe is quite significant relative to the mere 2.2M additional parameters. However, k = 3 is substantially faster than k = 9 for spatial-domain convolution; we speculate that low-level optimizations could close the performance gap to some extent, e.g., by using implicit instead of explicit padding. Since large-kernel convolutions throughout a model are unconventional, there has likely been low demand for such optimizations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B E CIFAR-10</head><p>Residual connections. We experimented with leaving out one, the other, or both residual connections before settling on the current configuration, and consequently chose to leave out the second residual connection. Our baseline model without the connection achieves 95.88% accuracy, while including the connection reduces it to 94.78%. Surprisingly, we see only a 0.31% decrease in accuracy for removing all residual connections. We acknowledge that these findings for residual connections may not generalize to deeper ConvMixers trained on larger data sets.  Normalization. Our model is conceptually similar to the vision transformer and MLP-Mixer, both of which use LayerNorm instead of BatchNorm. We attempted to use LayerNorm instead, and saw a decrease in performance of around 1% as well as slower convergence (see <ref type="table" target="#tab_6">Table 3</ref>). However, this was for a relatively shallow model, and we cannot guarantee that LayerNorm would not hinder ImageNet-scale models to an even larger degree. We note that the authors of ResMLP also saw a relatively small increase in accuracy for replacing LayerNorm with BatchNorm, but for a largerscale experiment <ref type="bibr" target="#b13">(Touvron et al., 2021a)</ref>. We conclude that BatchNorm is no more crucial to our architecture than other regularizations or parameter settings (e.g., kernel size).</p><p>Having settled on an architecture, we proceeded to adjust its parameters h, d, p, k as well as weight decay on CIFAR-10 experiments. (Initially, we took the unconventional approach of excluding weight decay since we were already using strong regularization in the form of RandAug and mixup.) We acknowledge that tuning our architecture on CIFAR-10 does not necessarily generalize to performance on larger data sets, and that this is a limitation of our study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 R</head><p>ConvMixers are quite performant on CIFAR-10, easily achieving &gt; 91% accuracy for as little as 100, 000 parameters, or &gt; 96% accuracy for only 887, 000 parameters (see <ref type="table">Table 4</ref>). With additional refinements e.g., a more expressive classifier or bottlenecks, we think that ConvMixer could be even more competitive. For all experiments, we trained for 200 epochs on CIFAR-10 with RandAug, mixup, cutmix, random erasing, gradient norm clipping, and the standard augmentations in timm.</p><p>We remove some of these augmentations in <ref type="table" target="#tab_6">Table 3</ref>, finding that RandAug and random scaling ("default" in timm) are very important, each accounting for over 3% of the accuracy. Scaling ConvMixer. We adjusted the hidden dimension h and the depth d, finding that deeper networks take longer to converge while wider networks converge faster. That said, increasing the width or the depth is an effective way to increase accuracy; a doubling of depth incurs less compute than a doubling of width. The number of parameters in a ConvMixer is given exactly by: #params = h[d(k 2 + h + 6) + c in p 2 + n classes + 3] + n classes ,</p><p>including affine scaling parameters in BatchNorm layers, convolutional kernels, and the classifier.</p><p>Kernel size. We initially hypothesized that large kernels would be important for ConvMixers, as they would allow the mixing of distant spatial information similarly to unconstrained MLPs or selfattention layers. We tried to investigate the effect of kernel size on CIFAR-10: we fixed the model to be a ConvMixer-256/8, and increased the kernel size by 2s from 3 to 15.</p><p>Using a kernel size of 3, the ConvMixer only achieves 93.61% accuracy. Simply increasing it to 5 gives an additional 1.50% accuracy, and further to 7 an additional 0.61%. The gains afterwards are relatively marginal, with kernel size 15 giving an additional 0.28% accuracy. It could be that with more training iterations or more regularization, the effect of larger kernels would be more pronounced. Nonetheless, we concluded that ConvMixers benefit from larger-than-usual kernels, and thus used kernel sizes 7 or 9 in most of our later experiments.</p><p>It is conventional wisdom that large-kernel convolutions can be "decomposed" into stacked smallkernel convolutions with activations between them, and it is therefore standard practice to use k = 3 convolutions, stacking more of them to increase the receptive field size with additional benefits from nonlinearities. This raises a question: is the benefit of larger kernels in ConvMixer actually better than simply increasing the depth with small kernels? First, we note that deeper networks are generally harder to train, so by increasing the kernel size independently of the depth, we may recover some of the benefits of depth without making it harder for signals to "propagate back" through the network. To test this, we trained a ConvMixer-256/10 with k = 3 (698K parameters) in the same setting as a ConvMixer-256/8 with k = 9 (707K parameters), i.e., we increased depth in a smallkernel model to roughly match the parameters of a large-kernel model. The ConvMixer-256/10 achieved 94.29% accuracy (1.5% less), which provides more evidence for the importance of larger kernels in ConvMixers. Next, instead of fixing the parameter budget, we tripled the depth (using the intuition that 3 stacked k = 3 convolutions have the receptive field of a k = 9 convolution), giving a ConvMixer-256/24 with 1670K parameters, and got 95.16% accuracy, i.e., still less. Patch size. CIFAR-10 inputs are so small that we initially only used p = 1, i.e., the patch embedding layer does little more than compute h linear combinations of the input image. Using p = 2, we see a reduction in accuracy of about 0.80%; this is a worthy tradeoff in terms of training and inference time. Further increasing the patch size leads to rapid decreases in accuracy, with only 92.61% for p = 4.</p><p>Since the "internal resolution" is decreased by a factor of p when increasing the patch size, we assumed that larger kernels would be less important for larger p. We investigated this by again increasing the kernel size from 3 to 11 for ConvMixer-256/8 with p = 2: however, this time, the improvement going from 3 to 5 is only 1.13%, and larger kernels than 5 provide only marginal benefit. Weight decay. We did many of our initial experiments with minimal weight decay. However, this was not optimal: by tuning weight decay, we can get an additional 0.15% of accuracy for no cost. Consequently, we used weight decay (without tuning) for our larger-scale experiments on ImageNet.</p><p>Tiny ConvMixers trained on CIFAR-10. <ref type="figure">Figure 4</ref>: Patch embedding weights for a ConvMixer-1024/20 with patch size 14 (see <ref type="table" target="#tab_3">Table 2</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C W V</head><p>Figure 5: Patch embedding weights for a ConvMixer-768/32 with patch size 7 (see <ref type="table" target="#tab_3">Table 2</ref>).  <ref type="table" target="#tab_0">Table 1</ref>).</p><p>In <ref type="figure">Figure 4</ref> and 5, we visualize the (complete) weights of the patch embedding layers of a ConvMixer-1536/20 with p = 14 and a ConvMixer-768/32 with p = 7, respectively. Much like <ref type="bibr" target="#b10">Sandler et al. (2019)</ref>, the layer consists of Gabor-like filters as well as "colorful globs" or rough edge detectors. The filters seem to be more structured than those learned by MLP-Mixer <ref type="bibr" target="#b11">(Tolstikhin et al., 2021)</ref>; also unlike MLP-Mixer, the weights look much the same going from p = 14 to p = 7: the latter simply looks like a downsampled version of the former. It is unclear, then, why we see such a drop in accuracy for larger patches. However, some of the filters essentially look like noise, maybe suggesting a need for more regularization or longer training, or even more data. Ultimately, we cannot read too much into the learned representations here.</p><p>In <ref type="figure" target="#fig_3">Figure 6</ref>, we plot the hidden convolutional kernels for successive layers of a ConvMixer. Initially, the kernels seem to be relatively small, but make use of their allowed full size in later layers; there is a clear hierarchy of features as one would expect from a standard convolutional architecture. Interestingly, <ref type="bibr" target="#b13">Touvron et al. (2021a)</ref> saw a similar effect for ResMLP, where earlier layers look like small-kernel convolution, while later layers were more diffuse, despite these layers being representated by an unconstrained matrix multiplication rather than convolution. nn.AdaptiveAvgPool2d <ref type="figure" target="#fig_1">((1,1)</ref>), return S(A(C(3,h,p,p)),*[S(R(A(C(h,h,k,groups=h,padding=k//2))),A(C(h,h,1))) for i in range(d)],AdaptiveAvgPool2d(1),Flatten(),Linear(h,n)) ? <ref type="figure">Figure 8</ref>: An implementation of our model in less than 280 characters, in case you happen to know of any means of disseminating information that could benefit from such a length. All you need to do to run this is from torch.nn import *.</p><p>This section presents an expanded (but still quite compact) version of the terse ConvMixer implementation that we presented in the paper. The code is given in <ref type="figure" target="#fig_4">Figure 7</ref>. We also present an even more terse implementation in <ref type="figure">Figure 8</ref>, which to the best of our knowledge is the first model that achieves the elusive dual goals of 80%+ ImageNet top-1 accuracy while also fitting into a tweet.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Accuracy vs. parameters, trained and evaluated on ImageNet-1k.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Implementation of ConvMixer in PyTorch; see Appendix D for more implementations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 :</head><label>6</label><figDesc>Random subsets of 64 depthwise convolutional kernels from progressively deeper layers of ConvMixer-1536/20 (see</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 :</head><label>7</label><figDesc>A more readable PyTorch<ref type="bibr" target="#b8">(Paszke et al., 2019)</ref> implementation of ConvMixer, where h = dim, d = depth, p = patch_size, k = kernel_size.1 def ConvMixer(h,d,k,p,n): 2 S,C,A=Sequential,Conv2d,lambda x:S(x,GELU(),BatchNorm2d(h)) 3 R=type('',(S,),{'forward':lambda s,x:s[0](x)+x}) 4</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1</head><label>1</label><figDesc></figDesc><table><row><cell>Network</cell><cell>Patch Size</cell><cell>Kernel Size</cell><cell># Params (?10 6 )</cell><cell>Throughput (img/sec)</cell><cell>Act. Fn.</cell><cell># Epochs</cell><cell>ImNet top-1 (%)</cell></row><row><cell>ConvMixer-1536/20</cell><cell>7</cell><cell>9</cell><cell>51.6</cell><cell>134</cell><cell>G</cell><cell>150</cell><cell>81.37</cell></row><row><cell>ConvMixer-768/32</cell><cell>7</cell><cell>7</cell><cell>21.1</cell><cell>206</cell><cell>R</cell><cell>300</cell><cell>80.16</cell></row><row><cell>ResNet-152</cell><cell>-</cell><cell>3</cell><cell>60.2</cell><cell>828</cell><cell>R</cell><cell>150</cell><cell>79.64</cell></row><row><cell>DeiT-B</cell><cell>16</cell><cell>-</cell><cell>86</cell><cell>792</cell><cell>G</cell><cell>300</cell><cell>81.8</cell></row><row><cell>ResMLP-B24/8</cell><cell>8</cell><cell>-</cell><cell>129</cell><cell>181</cell><cell>G</cell><cell>400</cell><cell>81.0</cell></row></table><note>Current "Most Interesting" ConvMixer Configurations vs. Other Simple Models: Models trained and evaluated on 224 ? 224 ImageNet-1k only. See more in Appendix A.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>and Yuan et al. (2021a) use a standard convolutional stem, while Yuan et al.(2021b)  repeatedly combines nearby patch embeddings. However, this conflates the effect of using patch embeddings with the effect of adding convolution or similar inductive biases e.g., locality. We attempt to focus on the use of patches.</figDesc><table /><note>CNNs meet ViTs. Many efforts have been made to incorporate features of convolutional networks into vision transformers and vice versa. Self-attention can emulate convolution (Cordonnier et al., 2019) and can be initialized or regularized to be like it (d'Ascoli et al., 2021); other works simply add convolution operations to transformers (Dai et al., 2021;</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>We are optimistic that a deeper ConvMixer with larger patches could reach a desirable tradeoff between accuracy, parameters, and throughput after longer training and more regularization and hyperparameter tuning, similarly to howWightman et al. (2021)  enhanced ResNet performance through carefully-designed training regimens. Low-level optimization of large-kernel depthwise convolution could substantially increase throughput, and small enhancements to our architecture like the addition of bottlenecks or a more expressive classifier could trade simplicity for performance.Due to its large internal resolution and isotropic design, ConvMixer may be especially well-suited for semantic segmentation, and it would be useful to run experiments on this task with a ConvMixer-like model and on other tasks such as object detection. More experiments could be designed to more clearly extricate the effect of patch embeddings from other architectural choices. In particular, for a more in-depth comparison to ViTs and MLP-Mixers, which excel when trained on very large data sets, it is important to investigate the performance of ConvMixers in the regime of large-scale pre-training. Modeling long-range interactions without attention. arXiv preprint arXiv:2102.08602, 2021.Irwan Bello, Barret Zoph, Ashish Vaswani, Jonathon Shlens, and Quoc V Le. Attention augmented convolutional networks. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 3286-3295, 2019. Shoufa Chen, Enze Xie, Chongjian Ge, Ding Liang, and Ping Luo. Cyclemlp: A mlp-like architecture for dense prediction. arXiv preprint arXiv:2107.10224, 2021. Jean-Baptiste Cordonnier, Andreas Loukas, and Martin Jaggi. On the relationship between selfattention and convolutional layers. arXiv preprint arXiv:1911.03584, 2019. Ekin D Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V Le. Randaugment: Practical automated data augmentation with a reduced search space. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops, pp. 702-703, 2020. Zihang Dai, Hanxiao Liu, Quoc V Le, and Mingxing Tan. Coatnet: Marrying convolution and attention for all data sizes. arXiv preprint arXiv:2106.04803, 2021. Hugo Touvron, and Herv? J?gou. Resnet strikes back: An improved training procedure in timm, 2021. Tete Xiao, Mannat Singh, Eric Mintun, Trevor Darrell, Piotr Doll?r, and Ross Girshick. Early convolutions help transformers see better. arXiv preprint arXiv:2106.14881, 2021. Kun Yuan, Shaopeng Guo, Ziwei Liu, Aojun Zhou, Fengwei Yu, and Wei Wu. Incorporating convolution designs into visual transformers. arXiv preprint arXiv:2103.11816, 2021a. Li Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi, Zihang Jiang, Francis EH Tay, Jiashi Feng, and Shuicheng Yan. Tokens-to-token vit: Training vision transformers from scratch on imagenet. arXiv preprint arXiv:2101.11986, 2021b.</figDesc><table><row><cell>Network</cell><cell>Patch Size</cell><cell>Kernel Size</cell><cell># Params (?10 6 )</cell><cell>Throughput (img/sec)</cell><cell>Act. Fn.</cell><cell># Epochs</cell><cell>ImNet top-1 (%)</cell></row><row><cell>ConvMixer-1536/20</cell><cell>7</cell><cell>9</cell><cell>51.6</cell><cell>134</cell><cell>G</cell><cell>150</cell><cell>82.20</cell></row><row><cell>ConvMixer-1536/20 ?</cell><cell>7</cell><cell>9</cell><cell>51.6</cell><cell>134</cell><cell>G</cell><cell>150</cell><cell>81.37</cell></row><row><cell>ConvMixer-1536/20</cell><cell>7</cell><cell>3</cell><cell>49.4</cell><cell>246</cell><cell>G</cell><cell>150</cell><cell>81.60</cell></row><row><cell>ConvMixer-1536/20</cell><cell>7</cell><cell>3</cell><cell>49.4</cell><cell>246</cell><cell>G</cell><cell>150</cell><cell>80.43</cell></row><row><cell>ConvMixer-1536/20</cell><cell>14</cell><cell>9</cell><cell>52.3</cell><cell>538</cell><cell>G</cell><cell>150</cell><cell>78.92</cell></row><row><cell>ConvMixer-1536/24</cell><cell>14</cell><cell>9</cell><cell>62.3</cell><cell>447</cell><cell>G</cell><cell>150</cell><cell>80.21</cell></row><row><cell>ConvMixer-768/32 ?</cell><cell>7</cell><cell>7</cell><cell>21.1</cell><cell>206</cell><cell>R</cell><cell>300</cell><cell>80.16</cell></row><row><cell>ConvMixer-1024/16</cell><cell>7</cell><cell>9</cell><cell>19.4</cell><cell>244</cell><cell>G</cell><cell>100</cell><cell>79.45</cell></row><row><cell>ConvMixer-1024/12</cell><cell>7</cell><cell>8</cell><cell>14.6</cell><cell>358</cell><cell>G</cell><cell>90</cell><cell>77.75</cell></row><row><cell>ConvMixer-512/16</cell><cell>7</cell><cell>8</cell><cell>5.4</cell><cell>599</cell><cell>G</cell><cell>90</cell><cell>73.76</cell></row><row><cell>ConvMixer-512/12 ?</cell><cell>7</cell><cell>8</cell><cell>4.2</cell><cell>798</cell><cell>G</cell><cell>90</cell><cell>72.59</cell></row><row><cell>ConvMixer-768/32</cell><cell>14</cell><cell>3</cell><cell>20.2</cell><cell>1235</cell><cell>R</cell><cell>300</cell><cell>74.93</cell></row><row><cell>ConvMixer-1024/20 ?</cell><cell>14</cell><cell>9</cell><cell>24.4</cell><cell>750</cell><cell>G</cell><cell>150</cell><cell>76.94</cell></row><row><cell>ResNet-152 ?</cell><cell>-</cell><cell>3</cell><cell>60.2</cell><cell>828</cell><cell>R</cell><cell>150</cell><cell>79.64</cell></row><row><cell>ResNet-101 ?</cell><cell>-</cell><cell>3</cell><cell>44.6</cell><cell>1187</cell><cell>R</cell><cell>150</cell><cell>78.33</cell></row><row><cell>ResNet-50</cell><cell>-</cell><cell>3</cell><cell>25.6</cell><cell>1739</cell><cell>R</cell><cell>150</cell><cell>76.32</cell></row><row><cell>DeiT-B  ?</cell><cell>7</cell><cell>-</cell><cell>86.7</cell><cell>83</cell><cell>G</cell><cell>-</cell><cell>-</cell></row><row><cell>DeiT-S  ?</cell><cell>7</cell><cell>-</cell><cell>22.1</cell><cell>174</cell><cell>G</cell><cell>-</cell><cell>-</cell></row><row><cell>DeiT-Ti  ?</cell><cell>7</cell><cell>-</cell><cell>5.7</cell><cell>336</cell><cell>G</cell><cell>-</cell><cell>-</cell></row><row><cell>DeiT-B ?</cell><cell>16</cell><cell>-</cell><cell>86</cell><cell>792</cell><cell>G</cell><cell>300</cell><cell>81.8</cell></row><row><cell>DeiT-S ?</cell><cell>16</cell><cell>-</cell><cell>22</cell><cell>1610</cell><cell>G</cell><cell>300</cell><cell>79.8</cell></row><row><cell>DeiT-Ti ?</cell><cell>16</cell><cell>-</cell><cell>5.7</cell><cell>2603</cell><cell>G</cell><cell>300</cell><cell>72.2</cell></row><row><cell>ResMLP-S12/8 ?</cell><cell>8</cell><cell>-</cell><cell>22.1</cell><cell>872</cell><cell>G</cell><cell>400</cell><cell>79.1</cell></row><row><cell>ResMLP-B24/8 ?</cell><cell>8</cell><cell>-</cell><cell>129</cell><cell>181</cell><cell>G</cell><cell>400</cell><cell>81.0</cell></row><row><cell>ResMLP-B24</cell><cell>16</cell><cell>-</cell><cell>116</cell><cell>1597</cell><cell>G</cell><cell>400</cell><cell>81.0</cell></row><row><cell>Swin-S ?</cell><cell>4</cell><cell>-</cell><cell>50</cell><cell>576</cell><cell>G</cell><cell>300</cell><cell>83.0</cell></row><row><cell>Swin-T ?</cell><cell>4</cell><cell>-</cell><cell>29</cell><cell>878</cell><cell>G</cell><cell>300</cell><cell>81.3</cell></row><row><cell>ViT-B/16 ?</cell><cell>16</cell><cell>-</cell><cell>86</cell><cell>789</cell><cell>G</cell><cell>300</cell><cell>77.9</cell></row><row><cell>Mixer-B/16 ?</cell><cell>16</cell><cell>-</cell><cell>59</cell><cell>1025</cell><cell>G</cell><cell>300</cell><cell>76.44</cell></row><row><cell>Isotropic MobileNetv3 ?</cell><cell>8</cell><cell>3</cell><cell>20</cell><cell>-</cell><cell>R</cell><cell>-</cell><cell>80.6</cell></row><row><cell>Isotropic MobileNetv3 ?</cell><cell>16</cell><cell>3</cell><cell>20</cell><cell>-</cell><cell>R</cell><cell>-</cell><cell>77.6</cell></row></table><note>Future work.R Irwan Bello. Lambdanetworks:St?phane d'Ascoli, Hugo Touvron, Matthew Leavitt, Ari Morcos, Giulio Biroli, and Levent Sagun. Convit: Improving vision transformers with soft convolutional inductive biases. arXiv preprint arXiv:2103.10697, 2021. Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An Ross Wightman,Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk Chun, Junsuk Choe, and Youngjoon Yoo.Cutmix: Regularization strategy to train strong classifiers with localizable features. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 6023-6032, 2019. Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. mixup: Beyond empirical risk minimization. arXiv preprint arXiv:1710.09412, 2017. Zhun Zhong, Liang Zheng, Guoliang Kang, Shaozi Li, and Yi Yang. Random erasing data aug- mentation. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pp. 13001-13008, 2020.A C Comparison with other simple models trained on ImageNet-1k only with input size 224.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Throughputs measured on an RTX8000 GPU using batch size 64 and fp16.</figDesc><table><row><cell>ConvMixers and</cell></row></table><note>? Throughput tested, but not trained. Activations: ReLU, GELU. Using new, better regularization hyperparameters based on Wightman et al. (2021)'s A1 procedure.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>). In terms of a fixed parameter budget, ConvMixers generally outperform DeiTs. For example, ConvMixer-1536/20 is only 0.43% less accurate than DeiT-B despite having over 30M fewer parameters; ConvMixer-768/32 is 0.36% more accurate than DeiT-S despite having 0.9M fewer parameters; and ConvMixer-512/16 is 0.39% more accurate than DeiT-Ti for nearly the same number of parameters. Admittedly, none of the ConvMixers are very competitive in terms of throughput, with the closest being the ConvMixer-512/16 which is 4? slower than DeiT-Ti.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 :</head><label>3</label><figDesc>Small ablation study of training a ConvMixer-256/8 on CIFAR-10.</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<idno type="arXiv">arXiv:2010.11929</idno>
		<title level="m">is worth 16x16 words: Transformers for image recognition at scale</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianyuan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yehui</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunjing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cmt</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.06263</idno>
		<title level="m">Convolutional neural networks meet vision transformers</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Gaussian error linear units (gelus)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.08415</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Vision permutator: A permutable mlp-like architecture for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qibin</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>So</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.08050</idno>
		<title level="m">Pay attention to mlps</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baining</forename><surname>Guo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Fixing weight decay regularization in adam</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Do you even need attention? a stack of feed-forward layers does surprisingly well on imagenet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Melas-Kyriazi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alykhan</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sasank</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benoit</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>H. Wallach, H. Larochelle, A. Beygelzimer, F. d&apos; Alch?-Buc, E. Fox, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="8024" to="8035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Stand-alone self-attention in vision models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prajit</forename><surname>Ramachandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irwan</forename><surname>Bello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anselm</forename><surname>Levskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.05909</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Non-discriminative data or weak model? on the relative importance of data and model resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Baccash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrey</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision Workshops</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Mlp-mixer: An all-mlp architecture for vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Tolstikhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jessica</forename><surname>Yung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Keysers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Lucic</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.01601</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Training data-efficient image transformers &amp; distillation through attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matth?s</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.12877</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alaaeldin</forename><surname>El-Nouby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Verbeek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.03404</idno>
		<title level="m">Resmlp: Feedforward networks for image classification with data-efficient training</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.17239</idno>
		<title level="m">Alexandre Sablayrolles, Gabriel Synnaeve, and Herv? J?gou. Going deeper with image transformers</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Pyramid vision transformer: A versatile backbone for dense prediction without convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enze</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng-Ping</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaitao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ding</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.12122</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Pytorch image models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Wightman</surname></persName>
		</author>
		<ptr target="https://github.com/rwightman/pytorch-image-models" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
