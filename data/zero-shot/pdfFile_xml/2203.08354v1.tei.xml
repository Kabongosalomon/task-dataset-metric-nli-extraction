<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Represent, Compare, and Learn: A Similarity-Aware Framework for Class-Agnostic Counting</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Shi</surname></persName>
							<email>minshi@hust.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Artificial Intelligence and Automation</orgName>
								<orgName type="institution">Huazhong University of Science and Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Lu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Artificial Intelligence and Automation</orgName>
								<orgName type="institution">Huazhong University of Science and Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Feng</surname></persName>
							<email>chenfeng@hust.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Artificial Intelligence and Automation</orgName>
								<orgName type="institution">Huazhong University of Science and Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengxin</forename><surname>Liu</surname></persName>
							<email>cxliu@hust.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Artificial Intelligence and Automation</orgName>
								<orgName type="institution">Huazhong University of Science and Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiguo</forename><surname>Cao</surname></persName>
							<email>zgcao@hust.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Artificial Intelligence and Automation</orgName>
								<orgName type="institution">Huazhong University of Science and Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Represent, Compare, and Learn: A Similarity-Aware Framework for Class-Agnostic Counting</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T21:25+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Class-agnostic counting (CAC) aims to count all instances in a query image given few exemplars. A standard pipeline is to extract visual features from exemplars and match them with query images to infer object counts. Two essential components in this pipeline are feature representation and similarity metric. Existing methods either adopt a pretrained network to represent features or learn a new one, while applying a naive similarity metric with fixed inner product. We find this paradigm leads to noisy similarity matching and hence harms counting performance. In this work, we propose a similarity-aware CAC framework that jointly learns representation and similarity metric. We first instantiate our framework with a naive baseline called Bilinear Matching Network (BMNet), whose key component is a learnable bilinear similarity metric. To further embody the core of our framework, we extend BMNet to BMNet+ that models similarity from three aspects: 1) representing the instances via their self-similarity to enhance feature robustness against intra-class variations; 2) comparing the similarity dynamically to focus on the key patterns of each exemplar; 3) learning from a supervision signal to impose explicit constraints on matching results. Extensive experiments on a recent CAC dataset FSC147 show that our models significantly outperform state-of-the-art CAC approaches. In addition, we also validate the cross-dataset generality of BMNet and BMNet+ on a car counting dataset CARPK. Code is at tiny.one/BMNet</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Object counting aims to infer the number of objects from an image. Most existing methods focus on a specific category, e.g., crowd <ref type="bibr" target="#b41">[42]</ref>, animal <ref type="bibr" target="#b1">[2]</ref>, or car <ref type="bibr" target="#b25">[26]</ref>, while requiring numerous training data to learn a good model. In contrast, given only one exemplar of a novel category, e.g., a car, even a child can easily capture its visual properties and  <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b39">40]</ref>, which counts objects of arbitrary categories given only few exemplars, is proposed to reduce the reliance on training data. CAC points out a promising direction for object counting, i.e., from learning to count objects to learning the way to count.</p><p>Generally, existing CAC methods <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b39">40]</ref> work in an extract-and-match pipeline. They first extract visual features from exemplars and match these features with those of query images. Similarity matching results are then used as intermediate representations to infer object counts. Intuitively, two factors play critical roles: feature representation and similarity metric. Existing methods either use a learnable <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b39">40]</ref> or a fixed feature extractor <ref type="bibr" target="#b28">[29]</ref>, but apply a similarity metric with some pre-defined rules, e.g., inner product <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b39">40]</ref>. We find this can yield unsatisfactory matching results. <ref type="figure" target="#fig_0">From Fig 1,</ref> by examining a recent model FamNet <ref type="bibr" target="#b28">[29]</ref>, we observe obvious noise on background and weak responses on target positions. The resulting density map may be erroneous given such ambiguity.</p><p>In this work, we present a generic similarity-aware framework for CAC, which jointly learns representation and similarity metric in an end-to-end manner. Our goal is to seek better similarity modeling that can generalize well to novel categories. First, we instantiate a bilinear matching network (BMNet), which extends the fixed inner product to a learnable bilinear similarity metric and also allows learnable representation through back-propagation. Unlike fixed inner product, the bilinear similarity metric captures flexible interactions among feature channels to measure similarity. Then, we extend BMNet to BMNet+ to embody the core motivation of our framework from three aspects: representing instances via self-similarity, comparing the similarity dynamically, and learning with explicit, similarity-aware supervision. In particular, we apply self-attention <ref type="bibr" target="#b42">[43]</ref> to represent self-similarity among features to mitigate intraclass variations. It augments the feature of each instance with information from other intra-class instances such that complementary clues like scales or viewpoints can be offered. The dynamic similarity metric applies a feature selection module to the exemplars to find key patterns and hence embraces both dynamism and selectivity. Then, inspired by metric learning <ref type="bibr" target="#b24">[25]</ref>, the similarity loss imposes an explicit supervision on the intermediate similarity map to pull the exemplar and the target close but to push the exemplar and background away.</p><p>Experiments on the public benchmark FSC147 <ref type="bibr" target="#b28">[29]</ref> show that our method outperforms the previous best approaches by large margins, with a relative improvement of +33.72% and +33.79% on the validation and test sets in terms of mean absolute error. According to <ref type="figure" target="#fig_0">Fig. 1</ref>, our method outputs better intermediate similarity results and presents generality over different categories. The ablation study validates the three main components within BMNet+. And we further show the cross-dataset generality of our models on a car counting dataset CARPK <ref type="bibr" target="#b12">[13]</ref>.</p><p>Our contributions are two-fold:</p><p>? A generic CAC framework that includes the existing pipeline and also generalizes it with joint representation learning and similarity learning;</p><p>? BMNet and BMNet+: two CAC models instantiated from our framework, which models packed similarity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Class-Specific Object Counting</head><p>According to how the counting problem is formulated, existing methods can be categorized into counting by detection <ref type="bibr" target="#b9">[10]</ref>, regression <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b43">44]</ref>, classification <ref type="bibr" target="#b16">[17]</ref>, and localization <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b30">31]</ref>. The most-studied regression-based approaches formulate counting as a dense prediction <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23]</ref> task, which learns to predict density maps <ref type="bibr" target="#b14">[15]</ref>. Under this paradigm, most methods focus on designing network architectures <ref type="bibr" target="#b43">[44]</ref>, multi-scale strategies <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b38">39]</ref>, or new loss functions and learning targets <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b35">36]</ref>. Recently, new paradigms are developed such as reinforcement learning <ref type="bibr" target="#b17">[18]</ref> and counting by localization <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b30">31]</ref>. The key difference between class-specific counting and class-agnostic one lies in that the latter requires a more generic representation and a more discriminative similarity metric.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Class-Agnostic Counting</head><p>Lu et al. <ref type="bibr" target="#b20">[21]</ref> first address CAC and propose a general matching network. One convolutional neural network is shared to extract feature maps for both query images and exemplars. These features are then concatenated to regress the object count. Considering that direct regression from concatenated features may cause overfitting, recent methods start to model similarity explicitly. CFOCNet <ref type="bibr" target="#b39">[40]</ref> uses the feature map of exemplar as a 2D kernel to convolve over the query feature map, following the spirit of Siamese network in object tracking <ref type="bibr" target="#b3">[4]</ref>. They also design a multi-scale matching framework to improve robustness. FamNet <ref type="bibr" target="#b28">[29]</ref> also adopts siamese way to model similarity and further proposes test-time adaptation given test exemplars. To alleviate the shortage of training data, Ranjan et al. <ref type="bibr" target="#b28">[29]</ref> propose the first and only CAC dataset FSC147 that covers challenges like occlusion and scale variation. The above methods report promising results for CAC. However, they typically focus on multi-scale strategy, data amplification, or test-time adaptation, but neglect a fundamental problem -similarity modeling. In this work, we show the importance of similarity modeling and also present a generic framework that jointly learns both representation and similarity metric.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Metric Learning</head><p>Metric learning aims to embed data into a space where similar samples are pulled close and dissimilar ones are pushed away <ref type="bibr" target="#b24">[25]</ref>. The similarity is measured in a fixed <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b15">16]</ref> or learned <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b36">37]</ref> manner. One common way constrains the similarity between features in a pair <ref type="bibr" target="#b10">[11]</ref> or triplet <ref type="bibr" target="#b37">[38]</ref>. Another way adds constraints based on signalto-noise ratios <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b40">41]</ref>, where the similarity between positive sample pairs is considered as the signal, and the similarity between negative pairs as the noise. The idea is to strengthen the signal and weaken the noise. We repurpose this idea into CAC, i.e., pulling close the features between the exemplar and target instances, while pushing away the features between exemplars and background patches. We further design a similarity loss based on this idea to supervise the similarity matching results.  <ref type="figure">Figure 2</ref>. The pipeline of BMNet and BMNet+. BMNet follows the extract-then-match paradigm but learns representation and similarity metric jointly in an end-to-end manner. BMNet+ is an improved version whose differences from BMNet are highlighted in colored blocks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Our framework</head><p>Counter  <ref type="figure">Figure 3</ref>. The comparison between the previous framework and ours. Ours can learn both representation and similarity metric jointly with more flexibility and generality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">A Similarity-Aware Framework for Class-Agnostic Counting</head><p>This section presents our framework for CAC, which jointly learns representation and similarity metric in an endto-end manner ( <ref type="figure">Fig. 3</ref>). We first instantiate this framework with a naive baseline, termed Bilinear Matching Network (BMNet), and then propose an extended BMNet+ to exemplify our idea on how to represent, dynamize, and learn similarity for both representation and similarity metric. The detailed pipeline of our methods is in <ref type="figure">Fig. 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Bilinear Matching Network</head><p>Differing from previous CAC methods, BMNet allows simultaneous optimization of representation and similarity metric. The core of BMNet is the bilinear similarity metric that captures flexible interactions among feature channels to model similarity.</p><p>Given a query image X and an exemplar Z of arbitrary category c, CAC aims to count all the instances of category c within X. Without loss of generality, we use one exemplar to explain our pipeline (we will also note how to operate with multiple exemplars). Feature Extractor. The feature extractor consists of layers of convolutional operations that map the input into dchannel features. For the query X, it outputs a downsampled feature map F (X) ? R d?hx?wx . For the exemplar Z, the output feature map is further processed with global average pooling to form a feature vector F (Z) ? R d . Learning Bilinear Similarity Metric. Previous methods apply fixed inner product to compute the similarity between two feature vectors. We argue that such fixed one-to-one interactions may be insufficient in modeling class-agnostic similarity. Inspired by neural similarity learning <ref type="bibr" target="#b18">[19]</ref> and bilinear models <ref type="bibr" target="#b27">[28]</ref>, we propose to extend the original inner product to a learnable bilinear similarity, which establishes flexible connections between two vectors. Specifically, let F ij (X) ? R d be the channel feature at spatial position (i, j). By redefining x ij = F ij (X) and z = F (Z), the similarity map S can be obtained by</p><formula xml:id="formula_0">S ij (x, z) = (P x ij + b x ) T (Qz + b z ) ,<label>(1)</label></formula><p>where P, Q ? R d?d are learnable matrices, and b x , b z ? R d?1 are learnable biases. The initial bilinear metric is in the form x T W z. We decompose W into P, Q specific to the query image and the exemplar, respectively. In practice, we find that this can yield better performance (refer to supplementary material for more details).</p><p>Given n exemplars, one can use Eq. 1 repetitively to compute n similarity maps, and then output their averaged similarity as the final similarity map S. Counter. The counter receives the channel-wise concatenation of the query feature map F (X) and the similarity map S, and then predicts a density map D pr . The final count is the integral of D pr . In practice, the counter consists of convolutional and bilinear upsampling layers. Supervision Signal. We adopt a conventional ? 2 loss as the counting loss L count :</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Channel Attention Weight</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Exemplar Index</head><formula xml:id="formula_1">L count = ||D pr (X, Z) ? D gt (X, Z)|| 2 2 ,<label>(2)</label></formula><p>where D gt denotes the ground truth density map.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Learning Dynamic Similarity Metric</head><p>The bilinear similarity in Sec. 3.1 increases flexibility to model similarity. However, the learned similarity metric stays fixed once trained and treats all categories equally during inference. Considering that humans may learn to recognize a category based on category-specific patterns, e.g., if told something is furry with four legs and pointy ears, one may suppose it to be a cat. We therefore think it is better to develop a dynamic similarity metric that can adaptively learn to focus on the key patterns of exemplars. Inspired by this intuition, we integrate a feature selection module over the exemplars to generate an exemplar-specific metric. Specifically, we regard each channel in Qz+b z as a pattern. Similar to SENet <ref type="bibr" target="#b13">[14]</ref>, we learn the dynamic channel attention weight a conditioned on Qz + b z such that similarity S can be computed by</p><formula xml:id="formula_2">S ij (x, z) = [(P x ij + b x )] T [a ? (Qz + b x )] ,<label>(3)</label></formula><p>where ? denotes the Hadamard product.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Similarity Map</head><p>Object Mask 'Signal' in the similarity map 'Noise' in the similarity map <ref type="figure">Figure 5</ref>. An illustration on how to compute the similarity loss.</p><p>We consider the similarity between exemplars and target instances as signals (positive labels), while similarity between exemplars and background as noises (negative labels).</p><p>We exemplify the learned dynamic attention weights in <ref type="figure" target="#fig_2">Fig. 4</ref>. For exemplars of the same category (cf. <ref type="figure" target="#fig_2">Fig. 4(a)</ref>), the generated dynamic attention weights turn similar. Similar phenomena can be observed given two visually close categories (cf. <ref type="figure" target="#fig_2">Fig. 4(b)</ref>). This validates our intuition that the dynamic similarity metric learns to focus on similar visual patterns for similar categories. In contrast, given two visually different categories (cf. <ref type="figure" target="#fig_2">Fig. 4(c)</ref>), our method learns to extract different key patterns with clear distinction. Note that whatever the cases are, there exist common patterns between different categories. This accords with the way we humans recognize objects: first use general visual clues like shapes and colors, then focus on category-specific details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Supervising the Similarity Map</head><p>Both existing CAC methods and our baseline BMNet only use the counting loss as supervision during training. In practice, we find that direct supervision on similarity matching results can help to guide similarity modeling. To this end, we start by posing a fundamental question: what makes an ideal similarity metric for CAC? In our opinion, it should output high similarity between the two features of the same category and low one for differing categories. This accords with the idea of metric learning <ref type="bibr" target="#b24">[25]</ref>.</p><p>Here we present a simple way to achieve this. Suppose the size of S is 1/r of that of X, i.e., each position in similarity map corresponds to a r ? r block within query image. For each position in S, we assign a positive label if its corresponding r ? r block contains more than one target, and assign a negative one if it contains no target. We then derive the similarity loss L sim with signal-to-noise ratio:</p><formula xml:id="formula_3">L sim = ? log i?pos exp (S i ) i?pos exp (S i ) + j?neg exp (S j )</formula><p>.</p><p>Here pos, neg denotes positive and negative positions in S.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Image Feature Exemplar Feature</head><p>Refined Image Feature Refined Exemplar Feature flatten Self-Attention reshape <ref type="figure">Figure 6</ref>. Self-similarity module.</p><p>With the counting loss L count and the similarity loss L sim , the final training loss can be written as</p><formula xml:id="formula_5">L = L count (D pr , D gt ) + ? ? L sim (S) ,<label>(5)</label></formula><p>where ? balances the two component loss items.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Self-Similarity Module</head><p>The core of our framework also includes improving the representation suitable for similarity matching. Here we present a feasible way to address this. As in <ref type="figure" target="#fig_4">Fig. 7</ref>, in reality, instances of the same category often appear with different attributes like poses and scales. Such intra-class variations impose great challenges on similarity matching. Accordingly, we propose to augment each instance feature with complementary information from other instances of the same category but with different attributes.</p><p>Technically, we first collect the exemplar feature F (Z) and each feature vector F ij (X) from the query feature map into a feature set. Then each vector in the feature set is updated via a self-attention mechanism <ref type="bibr" target="#b42">[43]</ref>  <ref type="figure">(Fig. 6)</ref>. The updated features are added back to the original ones with a learnable ratio ?. The resulting feature set is then re-split and re-shaped to obtain the final F (Z) and F (X).</p><p>We remark that, <ref type="bibr" target="#b39">[40]</ref> also applies self-attention over the feature maps similar to our work; hence the self-similarity module does not constitute our contribution. However, here we attempt to explain how self-attention works in our task. We start by visualizing the self-attention maps given the query points as in <ref type="figure" target="#fig_4">Fig. 7</ref>. It can be observed that each query point mainly focuses on instances of the same category. This differs from self-attention in object detection <ref type="bibr" target="#b4">[5]</ref> where the query point mainly focuses on a single instance. This indicates that, the self-similarity module in CAC tends to aggregate same-category information and hence enhances representations with robustness towards intra-class variations. Scale Embedding. Inspired by the positional embedding in Transformer <ref type="bibr" target="#b34">[35]</ref>, we wonder if we could similarly embed the scale information of the exemplars to improve the representation. Note that two factors cause the exemplars to  lose scale information in our method: one is the resizing of exemplars and the other is the pooling operation during feature extracting. To compensate for this loss, we propose to augment the exemplar's feature with its corresponding scale embedding. We discretize the scale space into l total levels. Each scale level is assigned with a d-dimensional embedding vector, yielding an embedding set whose cardinality equals l total . Given an exemplar Z and query image X, we first derive Z's scale level l(Z) by</p><formula xml:id="formula_6">l(Z) = min l total ? 1, ? h Z 2h X + w Z 2w X ? l total ? ,<label>(6)</label></formula><p>where h Z , w Z , h X , w X denote images' heights and widths. Then the scale embedding vector of level l(Z) is retrieved and added back into the original feature. The scale embedding set is randomly initialized and learned during training, and stays fixed during inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Implementation Details</head><p>For a fair comparison, we apply the same pre-processing to query images and the feature extractor as in FamNet <ref type="bibr" target="#b28">[29]</ref>.</p><p>Data Pre-processing. We resize the query image while keeping its aspect ratio so that the length of its sides is limited within <ref type="bibr">[384,</ref><ref type="bibr">1584]</ref>. Exemplars are resized to 128 ? 128 before fed into the feature extractor. No data augmentation is applied. During training, the size of all query images within a mini-batch is kept the same by zero-padding.</p><p>Network Architecture. The feature backbone consists of the first 4 blocks of ResNet-50 <ref type="bibr" target="#b11">[12]</ref>, which outputs the feature maps of 1024 channels. For each query image, the number of channels are reduced to 256 using 1 ? 1 convolution. For each exemplar, the feature maps are first processed with global average pooling and then linearly mapped to obtain a 256D feature vector. The counter consists of a few convolution and bilinear upsampling layers to regress a density map of the same size as the query image. When computing channel attention weight a in BMNet+, we apply Training Details. Our model is trained end-to-end. The backbone is initialized via SwAV <ref type="bibr" target="#b5">[6]</ref>. Other parameters are randomly initialized. We apply AdamW <ref type="bibr" target="#b19">[20]</ref> as the optimizer with a batch size of 8. The model is trained for 300 epochs with a fixed learning rate of 1e-5. The weight ? of similarity loss in Eq. 5 is set to 5e-6 so that all the loss items are of the same order of magnitude. The total number of scale levels in Eq. 6 is empirically set to 20. We use PyTorch <ref type="bibr" target="#b26">[27]</ref> as our experimental platform. Note that the BMNet+ consumes less then 12GB memory on a single GPU during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>Here we first showcase the advantage of our models over the state-of-the-art methods. We then validate each component in BMNet+. Next, we analyze the influence of exemplar numbers and discuss how to integrate features before feeding them to the counter. Finally, we show the crossdataset generality of our method on a car counting dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Comparison With State of the Arts</head><p>The FSC147 Dataset. FSC147 <ref type="bibr" target="#b28">[29]</ref> is the first large-scale dataset for class-agnostic counting. It includes 6, 135 images from 147 categories varying from animals, kitchen utensils, to vehicles. Given one query image, three instances of the same category are randomly chosen as the exemplars. To validate methods' generality, the categories in training, validation, and test sets have no overlap. All experiments are done on FSC147 unless otherwise specified. Comparing Methods. We mainly compare our models with two available CAC methods: GMN (General Matching Network <ref type="bibr" target="#b20">[21]</ref>) and FamNet (Few-shot adaptation and matching Network <ref type="bibr" target="#b28">[29]</ref>). Since FamNet executes finetuning during testing, we denote the fine-tuned version by FamNet+. The other compared methods apply no finetuning. Regarding our models, we validate two variants: 1) the baseline BMNet and 2) BMNet+ that implements all core components, i.e., self-similarity module, dynamic similarity metric, and direct supervision on similarity map. For more comparisons, we also test CFOCNet <ref type="bibr" target="#b39">[40]</ref> that applies self-attention similar to our work. We reproduce CFOCNet as its code is unavailable and keep the same exemplar preprocessing and training configuration as in our methods. We denote this by CFOCNet*. Note that the main comparisons are concentrated on the public state-of-the-art FamNet.</p><p>Quantitative Results. As shown in <ref type="table" target="#tab_1">Table 1</ref>, BMNet exhibits advantage over all of the compared methods with fixed similarity metrics (FamNet, GMN, and CFOCNet). Compared with FamNet, BMNet achieves a relative improvement of 21.63% w.r.t. validation MAE and 25.93% w.r.t. test MAE. Note that BMNet is already a strong baseline over FamNet, which indicates that BMNet is capable of characterizing a novel category without any concerned prior information. One can also observe that BMNet+ reduces the validation MAE by 18.23% and the test MAE by 12.51% compared with BMNet, which validates the effectiveness of our proposed components.</p><p>Qualitative Analysis. As shown in <ref type="figure" target="#fig_6">Fig. 8</ref>, both BMNet and BMNet+ output accurate density maps in whether dense or sparse scenes. Specifically, when counting hot air balloon (the 1st row), FamNet+ and BMNet mistake the tower for counting target, while BMNet+ offers comparatively better discrimination between target and background. In case of strawberries that exhibit large intra-class variation (the 2nd row), FamNet fails while our methods do not. This validates the effectiveness of our bilinear similarity metric (BMNet) and self-similarity in BMNet+. Refer to supplementary materials for more visualizations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Ablation Study on BMNet+</head><p>Here we justify the effectiveness of each component in BMNet+. We start by testing the supervision on similarity map, because it directly affects the learning of selfsimilarity and dynamic similarity metric.</p><p>Supervision of the Similarity Map. By comparing B1 and B2 in <ref type="table">Table 2</ref>, we can observe that direct supervision on the similarity map brings a relative improvement of 3.88% and 10.41% on the validation and test set w.r.t. MAE, respectively. This indicates that the similarity loss can help to learn a generic similarity metric.</p><p>Self-Similarity With Scale Embedding. Comparing B2 with B4 in <ref type="table">Table 2</ref>, we can observe that applying selfsimilarity module and scale embedding improves the validation MAE by 0.97. However, the performance on the test set shows a converse phenomenon (cf. B7 vs. B9). A plausible explanation is that additional parameters in the feature extractor lead to an over-fitting problem. Note that, the comparisons of B3 vs. B4 and B8 vs. B9 show that scale embedding generally improves representations. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Inputs</head><p>Ground Truth FamNet+ BMNet BMNet+ Dynamic Similarity Metric. The inclusion of dynamic similarity metric further brings a relative improvement of 9.28% on validation MAE and 11.29% on test MAE (cf. B4 vs. B5 and B9 vs. B10). In Sec. 3.2, we exemplify that dynamic similarity metric focuses on the exemplar-specific patterns to match similarity. Quantitative results here further demonstrate that the dynamic pattern selection mechanism can improve the naive bilinear similarity metric.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Number of Exemplars per Task</head><p>Here we investigate the impact of the number n of exemplars (randomly chosen) per task. Since the given maximum number of exemplars per query image is 3 in FSC147, we experiment with n = 1, 2, 3 and report their results in <ref type="table">Table 3</ref>. It is foreseeable that more exemplars yield better results as in <ref type="table">Table 3</ref>. Note that even our method with one single exemplar surpasses the other methods with three exemplars (cf. ods may get more vulnerable to intra-class variations with fewer exemplars, but we find that self-similarity module offers obvious improvement within our method in this sce- nario. Refer to supplementary materials for more details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">How to Integrate Features for the Counter?</head><p>Here we discuss possible ways to integrate the features before feeding them to the counter. Given the exemplar feature z, the query feature x, and the similarity map S, we investigate 4 ways of feature combination as in <ref type="table">Table 4</ref>, where "+" denotes channel-wise concatenation. According to the results, only using the similarity map to count objects yields the worst performance (the 1st row) , while leveraging raw features of exemplars and query images can improve the counting performance (the 3rd and 4th rows). However, excluding the similarity map makes the supervision on the similarity metric impossible (the 2nd row). In addition, concatenating the features of exemplars brings marginal improvements but with increased computation overheads (the 3rd row). Therefore, to leverage the information within similarity map while also maintaining a moderate computational cost, we suggest the combination of similarity map and query features as the default representation. In supplementary materials, we also show that the query features may encode generic semantic information to help correct the mistakes within the similarity map.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Cross-Dataset Generalization</head><p>Following FamNet <ref type="bibr" target="#b28">[29]</ref>, we test our models's generality on a car counting dataset CARPK <ref type="bibr" target="#b12">[13]</ref>. CARPK contains 1, 448 images of parking lots in a bird view, which differs significantly from the images in FSC147. We exclude the "car" category within FSC147 to ensure that training and test categories have no overlap.</p><p>The results are reported in <ref type="table">Table 5</ref>. We first focus on the models without fine-tuning on the CARPK dataset. It can be observed that our models exhibit strong generality.  <ref type="table">Table 5</ref>. Generalization performance on the CARPK dataset.</p><p>All models are pretrained on the FSC147 dataset. "fine-tuned" denotes whether the pretrained models are further fine-tuned on the CARPK dataset.</p><p>Compared with FamNet, BMNet and BMNet+ obtain a relative performance gain of 49.3% and 63.8% on MAE, respectively. Moreover, BMNet and BMNet+ still retain their advantages when compared with FamNet in the fine-tuning scenario, which demonstrates that our designs are orthogonal to fine-tuning. In addition, the improvements of FamNet and BMNet after fine-tuning indicate the benefit of introducing task-specific information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions and Limitations</head><p>In this work, we show that similarity modeling matters for CAC. In particular, we propose a similarity-aware framework for CAC where the feature representation and similarity metric are jointly learned in an end-to-end manner. Then we instantiate our framework with a naive BMNet that learns bilinear similarity. We also show how to extend the BMNet with the idea of exploiting self-similarity among features, learning dynamic similarity metric, and imposing explicit supervision on the similarity map. Both our BMNet and the extended BMNet+ achieve state-of-the-art performance on the large-scale dataset FSC147 and car counting dataset CARPK. Limitations. Technically, we mainly focus on designing a better similarity metric, while how to obtain better feature representation is not well addressed: 1) the function of selfsimilarity module is intuitive, and <ref type="table">Table 2</ref> shows the selfsimilarity may hurt the performance on the test set; 2) how to integrate rich representation along with similarity map is also not addressed well in this work. Maybe transformerbased tracking <ref type="bibr" target="#b7">[8]</ref> can help. In addition, since our goal is to present a generic framework, some designs in our instantiated models include some heuristics, which could be further studied in detail.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Visualizations of intermediate similarity maps in class-agnostic counting. Compared with the state-of-the-art Fam-Net [29], our model (BMNet+) generates high-fidelity results. count cars in new scenes. Recently, CAC (Class Agnostic Counting)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Visualization of channel attention weights for exemplars from the same and different categories. We visualize the attention weights (each vertical line) for exemplars from (a) the same category apple, (b) visually similar categories apple vs. strawberry, and (c) dissimilar ones apple vs. stamp. For (b) and (c), the red short line splits the samples into two categories. By focusing on the horizontal lines, we can observe that, channel attention weights for exemplars of the same or similar categories (cf. (a) and (b)) shows more consistency than those for different categories (cf. (c), especially in red boxes). Better zoom in for details. More visualizations can be found in supplementary material.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 .</head><label>7</label><figDesc>Exemplified attention maps using self-similarity map.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 .</head><label>8</label><figDesc>Qualitative results on the FSC147 dataset. The samples on the left exhibit significant intra-class variations such as scale, pose, and illumination condition. The red boxes indicate exemplars. Counting values are shown at the top-right corner. Our BMNet and BMNet+ can predict accurate density maps in both dense and sparse scenes. Best viewed by zooming in.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Comparison with state of the art on the FSC147 dataset. Best performance is in boldface.</figDesc><table><row><cell>Methods</cell><cell cols="4">Val MAE Val MSE Test MAE Test MSE</cell></row><row><cell>GMN [21]</cell><cell>29.66</cell><cell>89.81</cell><cell>26.52</cell><cell>124.57</cell></row><row><cell>FamNet [29]</cell><cell>24.32</cell><cell>70.94</cell><cell>22.56</cell><cell>101.54</cell></row><row><cell>FamNet+ [29]</cell><cell>23.75</cell><cell>69.07</cell><cell>22.08</cell><cell>99.54</cell></row><row><cell cols="2">CFOCNet* [40] 21.19</cell><cell>61.41</cell><cell>22.10</cell><cell>112.71</cell></row><row><cell>BMNet (Ours)</cell><cell>19.06</cell><cell>67.95</cell><cell>16.71</cell><cell>103.31</cell></row><row><cell cols="2">BMNet+ (Ours) 15.74</cell><cell>58.53</cell><cell>14.62</cell><cell>91.83</cell></row><row><cell cols="5">a Linear(128)-ReLU-Linear(256)-Tanh structure, where the</cell></row><row><cell cols="5">number in the bracket denotes the output dimension. Refer</cell></row><row><cell cols="4">to supplementary material for more details.</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 )Table 2 .</head><label>12</label><figDesc>. This indicates that our method seeing only one exemplar could still capture information to describe the corresponding category. Besides, CAC meth-Ablation study on self-similarity (SS), dynamic similarity metric (DSM), similarity loss (SL), and scale embedding (SE).</figDesc><table><row><cell>No.</cell><cell cols="4">SL SS SE DSM</cell><cell>Val MAE</cell><cell>Val MSE</cell></row><row><cell>B1</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>19.06</cell><cell>67.95</cell></row><row><cell>B2</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>18.32</cell><cell>64.01</cell></row><row><cell>B3</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>17.44</cell><cell>67.07</cell></row><row><cell>B4</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>17.35</cell><cell>60.28</cell></row><row><cell>B5</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>15.74</cell><cell>58.53</cell></row><row><cell>No.</cell><cell cols="6">SL SS SE DSM Test MAE Test MSE</cell></row><row><cell>B6</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>16.71</cell><cell>103.31</cell></row><row><cell>B7</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>14.97</cell><cell>92.88</cell></row><row><cell>B8</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>16.53</cell><cell>103.69</cell></row><row><cell>B9</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>16.48</cell><cell>96.85</cell></row><row><cell>B10</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>14.62</cell><cell>91.83</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .Table 4 .</head><label>34</label><figDesc>Impact of the number of exemplars. Ways to integrate features for the counter. x and z stand for the features of query and exemplar, respectively, S for similarity map, and "+" for channel-wise concatenation.</figDesc><table><row><cell>n</cell><cell>Val MAE</cell><cell cols="2">Val MSE</cell><cell cols="2">Test MAE</cell><cell>Test MSE</cell></row><row><cell>1</cell><cell>17.89</cell><cell>61.12</cell><cell></cell><cell></cell><cell>16.89</cell><cell>96.65</cell></row><row><cell>2</cell><cell>16.03</cell><cell>58.65</cell><cell></cell><cell></cell><cell>16.16</cell><cell>97.18</cell></row><row><cell>3</cell><cell>15.74</cell><cell>58.53</cell><cell></cell><cell></cell><cell>14.62</cell><cell>91.83</cell></row><row><cell cols="6">Combination Val MAE Val MSE Test MAE Test MSE</cell></row><row><cell>S</cell><cell></cell><cell>21.36</cell><cell cols="2">69.05</cell><cell>18.76</cell><cell>92.44</cell></row><row><cell>x + z</cell><cell></cell><cell>19.27</cell><cell cols="2">66.75</cell><cell>18.24</cell><cell>84.39</cell></row><row><cell cols="2">x + z + S</cell><cell>18.71</cell><cell cols="2">61.88</cell><cell>18.71</cell><cell>88.23</cell></row><row><cell cols="2">x + S (default)</cell><cell>19.06</cell><cell cols="2">67.95</cell><cell>16.53</cell><cell>103.31</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Localization in the crowd with topological constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shahira</forename><surname>Abousamra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh</forename><surname>Hoai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><surname>Samaras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Chen</surname></persName>
		</author>
		<idno>2021. 2</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI Conf. Artificial Intell</title>
		<meeting>AAAI Conf. Artificial Intell</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Counting in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Arteta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comput. Vis</title>
		<meeting>Eur. Conf. Comput. Vis</meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="483" to="498" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Faster image template matching in the sum of the absolute value of differences measure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikhail</forename><forename type="middle">J</forename><surname>Atallah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="659" to="663" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Fully-convolutional siamese networks for object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Bertinetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Valmadre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jo?o</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comput. Vis. Workshop</title>
		<meeting>Eur. Conf. Comput. Vis. Workshop</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="850" to="865" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">End-toend object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comput. Vis</title>
		<meeting>Eur. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="213" to="229" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Unsupervised learning of visual features by contrasting cluster assignments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc</title>
		<meeting>null</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="9912" to="9924" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Privacy preserving crowd monitoring: Counting people without people models or tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-Sheng John</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recogn</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recogn</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Transformer tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawen</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyun</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recogn</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recogn</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="8122" to="8131" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Decoupled two-stage crowd counting and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haipeng</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiguo</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="2862" to="2875" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Bernt Schiele, and Pietro Perona. Pedestrian detection: An evaluation of the state of the art</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Wojek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="743" to="761" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Dimensionality reduction by learning an invariant mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recogn</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recogn</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1735" to="1742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recogn</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recogn</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Drone-based object counting by spatially regularized regional proposal network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng-Ru</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Liang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Winston</forename><forename type="middle">H</forename><surname>Hsu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis</title>
		<meeting>IEEE Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="4165" to="4173" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Albanie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enhua</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning To Count Objects in Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc</title>
		<meeting>null</meeting>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2010" />
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="1324" to="1332" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Fast template matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Lewis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Vision Interface</title>
		<imprint>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Counting Objects by Blockwise Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haipeng</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Xian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiguo</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst. Video Technol</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Weighing counts: Sequential crowd counting by reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongwei</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haipeng</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiguo</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comput. Vis</title>
		<meeting>Eur. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="164" to="181" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Neural similarity learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiyang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Rehg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc</title>
		<meeting>null</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Learn. Repr</title>
		<meeting>Int. Conf. Learn. Repr</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Class-agnostic counting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erika</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weidi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Asi. Conf. Comput. Vis</title>
		<editor>C. V. Jawahar, Hongdong Li, Greg Mori, and Konrad Schindler</editor>
		<meeting>Asi. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="669" to="684" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Indices matter: Learning to index for deep image matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutong</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songcen</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis</title>
		<meeting>IEEE Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3265" to="3274" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Index networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutong</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songcen</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="242" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Bayesian loss for crowd count estimation with point supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaopeng</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihong</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis</title>
		<meeting>IEEE Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6142" to="6151" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A metric learning reality check</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Musgrave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ser-Nam</forename><surname>Lim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comput. Vis</title>
		<meeting>Eur. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Towards perspective-free object counting with deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>O?oro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-</forename><surname>Rubio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><forename type="middle">J</forename><surname>L?pez-Sastre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comput. Vis</title>
		<meeting>Eur. Conf. Comput. Vis</meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="615" to="629" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">PyTorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Others</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc</title>
		<meeting>null</meeting>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8026" to="8037" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Bilinear classifiers for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamed</forename><surname>Pirsiavash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charless</forename><surname>Fowlkes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Adv. Neural Inf</title>
		<meeting>Adv. Neural Inf</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">22</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning to count everything</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Viresh</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Udbhav</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thu</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh</forename><surname>Hoai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recogn</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recogn</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="3393" to="3402" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Improved deep metric learning with multiclass n-pair loss objective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihyuk</forename><surname>Sohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc</title>
		<meeting>null</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">29</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Rethinking counting and localization in crowds: A purely point-based framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingyu</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengkai</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yabiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengjie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jilin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feiyue</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Wu</surname></persName>
		</author>
		<idno>abs/2107.12746</idno>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">To choose or to fuse? scale selection for crowd counting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingyu</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yabiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengjie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jilin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiayi</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI Conf. Artificial Intell</title>
		<meeting>AAAI Conf. Artificial Intell</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Circle loss: A unified perspective of pair similarity optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changmao</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongdao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recogn</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recogn</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6397" to="6406" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Representation learning with contrastive predictive coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A?ron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yazhe</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<idno>abs/1807.03748</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc</title>
		<meeting>null</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Distribution Matching for Crowd Counting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huidong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><surname>Samaras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh</forename><forename type="middle">Hoai</forename><surname>Nguyen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc</title>
		<meeting>null</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Cosface: Large margin cosine loss for deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yitong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dihong</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingchao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recogn</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recogn</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5265" to="5274" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Distance metric learning for large margin nearest neighbor classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Kilian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Saul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc</title>
		<meeting>null</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">From open set to closed set: Counting objects by spatial divide-and-conquer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haipeng</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengxin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiguo</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis</title>
		<meeting>IEEE Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8361" to="8370" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Class-agnostic few-shot object counting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuo-Diao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hung-Ting</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Winston</forename><forename type="middle">H</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Chin</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Winter Conf. Appl. Comput. Vis</title>
		<meeting>Winter Conf. Appl. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="869" to="877" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Signal-to-noise ratio: A robust distance metric for deep metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tongtong</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihong</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinan</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Binghui</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recogn</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recogn</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4810" to="4819" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Cross-Scene Crowd Counting via Deep Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaokang</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recogn</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recogn</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Dimitris Metaxas, and Augustus Odena. Self-attention generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Single-image crowd counting via multi-column convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingying</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Desen</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siqin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenghua</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recogn</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recogn</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="589" to="597" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
