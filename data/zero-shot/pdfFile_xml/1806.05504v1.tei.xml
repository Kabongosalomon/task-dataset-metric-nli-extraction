<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Entity Commonsense Representation for Neural Abstractive Summarization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reinald</forename><forename type="middle">Kim</forename><surname>Amplayo</surname></persName>
							<email>rktamplayo@yonsei.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="institution">Yonsei University</orgName>
								<address>
									<settlement>Seoul</settlement>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seonjae</forename><surname>Lim</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Yonsei University</orgName>
								<address>
									<settlement>Seoul</settlement>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seung-Won</forename><surname>Hwang</surname></persName>
							<email>seungwonh@yonsei.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="institution">Yonsei University</orgName>
								<address>
									<settlement>Seoul</settlement>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Entity Commonsense Representation for Neural Abstractive Summarization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T09:58+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>A major proportion of a text summary includes important entities found in the original text. These entities build up the topic of the summary. Moreover, they hold commonsense information once they are linked to a knowledge base. Based on these observations, this paper investigates the usage of linked entities to guide the decoder of a neural text summarizer to generate concise and better summaries. To this end, we leverage on an off-the-shelf entity linking system (ELS) to extract linked entities and propose Entity2Topic (E2T), a module easily attachable to a sequence-to-sequence model that transforms a list of entities into a vector representation of the topic of the summary. Current available ELS's are still not sufficiently effective, possibly introducing unresolved ambiguities and irrelevant entities. We resolve the imperfections of the ELS by (a) encoding entities with selective disambiguation, and (b) pooling entity vectors using firm attention. By applying E2T to a simple sequenceto-sequence model with attention mechanism as base model, we see significant improvements of the performance in the Gigaword (sentence to title) and CNN (long document to multi-sentence highlights) summarization datasets by at least 2 ROUGE points.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Text summarization is a task to generate a shorter and concise version of a text while preserving the meaning of the original text. The task can be divided into two subtask based on the approach: extractive and abstractive summarization. Extractive summarization is a task to create summaries by pulling out snippets of text form the original text and combining them to form a summary. Abstractive summarization asks to generate summaries from scratch without the restriction to use * Amplayo and Lim are co-first authors with equal contribution. Names are arranged alphabetically. the available words from the original text. Due to the limitations of extractive summarization on incoherent texts and unnatural methodology <ref type="bibr" target="#b26">(Yao et al., 2017)</ref>, the research trend has shifted towards abstractive summarization. Sequence-to-sequence models  with attention mechanism <ref type="bibr" target="#b0">(Bahdanau et al., 2014)</ref> have found great success in generating abstractive summaries, both from a single sentence <ref type="bibr" target="#b4">(Chopra et al., 2016)</ref> and from a long document with multiple sentences <ref type="bibr" target="#b2">(Chen et al., 2016)</ref>. However, when generating summaries, it is necessary to determine the main topic and to sift out unnecessary information that can be omitted. Sequenceto-sequence models have the tendency to include all the information, relevant or not, that are found in the original text. This may result to unconcise summaries that concentrates wrongly on irrelevant topics. The problem is especially severe when summarizing longer texts.</p><p>In this paper, we propose to use entities found in the original text to infer the summary topic, miti-gating the aforementioned problem. Specifically, we leverage on linked entities extracted by employing a readily available entity linking system. The importance of using linked entities in summarization is intuitive and can be explained by looking at <ref type="figure" target="#fig_0">Figure 1</ref> as an example. First (O1 in the <ref type="bibr">Figure)</ref>, aside from auxiliary words to construct a sentence, a summary is mainly composed of linked entities extracted from the original text. Second (O2), we can depict the main topic of the summary as a probability distribution of relevant entities from the list of entities. Finally (O3), we can leverage on entity commonsense learned from a separate large knowledge base such as Wikipedia.</p><p>To this end, we present a method to effectively apply linked entities in sequence-tosequence models, called Entity2Topic (E2T).</p><p>E2T is a module that can be easily attached to any sequence-to-sequence based summarization model. The module encodes the entities extracted from the original text by an entity linking system (ELS), constructs a vector representing the topic of the summary to be generated, and informs the decoder about the constructed topic vector. Due to the imperfections of current ELS's, the extracted linked entities may be too ambiguous and coarse to be considered relevant to the summary. We solve this issue by using entity encoders with selective disambiguation and by constructing topic vectors using firm attention.</p><p>We experiment on two datasets, Gigaword and CNN, with varying lengths. We show that applying our module to a sequence-to-sequence model with attention mechanism significantly increases its performance on both datasets. Moreover, when compared with the state-of-the-art models for each dataset, the model obtains a comparable performance on the Gigaword dataset where the texts are short, and outperforms all competing models on the CNN dataset where the texts are longer. Furthermore, we provide analysis on how our model effectively uses the extracted linked entities to produce concise and better summaries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Usefulness of linked entities in summarization</head><p>In the next subsections, we present detailed arguments with empirical and previously examined evidences on the observations and possible issues when using linked entities extracted by an entity linking system (ELS) for generating abstractive summaries. For this purpose, we use the development sets of the Gigaword dataset provided in <ref type="bibr">(Rush et al., 2015)</ref> and of the CNN dataset provided in <ref type="bibr" target="#b7">(Hermann et al., 2015)</ref> as the experimental data for quantitative evidence and refer the readers to <ref type="figure" target="#fig_0">Figure 1</ref> as the running example.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Observations</head><p>As discussed in Section 1, we find three observations that show the usefulness of linked entities for abstractive summarization. First, summaries are mainly composed of linked entities extracted from the original text. In the example, it can be seen that the summary contains four words that refer to different entities. In fact, all noun phrases in the summary mention at least one linked entity. In our experimental data, we extract linked entities from the original text and compare them to the noun phrases found in the summary. We report that 77.1% and 75.1% of the noun phrases on the Gigaword and CNN datasets, respectively, contain at least one linked entity, which confirms our observation.</p><p>Second, linked entities can be used to represent the topic of the summary, defined as a multinomial distribution over entities, as graphically shown in the example, where the probabilities refer to the relevance of the entities. Entities have been previously used to represent topics <ref type="bibr" target="#b15">(Newman et al., 2006)</ref>, as they can be utilized as a controlled vocabulary of the main topics in a document <ref type="bibr" target="#b9">(Hulpus et al., 2013)</ref>. In the example, we see that the entity "Jae Seo" is the most relevant because it is the subject of the summary, while the entity "South Korean" is less relevant because it is less important when constructing the summary.</p><p>Third, we can make use of the entity commonsense that can be learned as a continuous vector representation from a separate larger corpus <ref type="bibr" target="#b16">(Ni et al., 2016;</ref><ref type="bibr" target="#b25">Yamada et al., 2017)</ref>. In the example, if we know that the entities "Los Angeles Dodgers" and "New York Mets" are American baseball teams and "Jae Seo" is a baseball player associated with the teams, then we can use this information to generate more coherent summaries. We find that 76.0% of the extracted linked entities are covered by the pre-trained vectors 1 in our experimental data, proving our third observation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Possible issues</head><p>Despite its usefulness, linked entities extracted from ELS's have issues because of low precision rates <ref type="bibr" target="#b6">(Hasibi et al., 2016)</ref> and design challenges in training datasets <ref type="bibr" target="#b12">(Ling et al., 2015)</ref>. These issues can be summarized into two parts: ambiguity and coarseness.</p><p>First, the extracted entities may be ambiguous. In the example, the entity "South Korean" is ambiguous because it can refer to both the South Korean person and the South Korean language, among others 2 . In our experimental data, we extract (1) the top 100 entities based on frequency, and (2) the entities extracted from 100 randomly selected texts, and check whether they have disambiguation pages in Wikipedia or not. We discover that 71.0% of the top 100 entities and 53.6% of the entities picked at random have disambiguation pages, which shows that most entities are prone to ambiguity problems.</p><p>Second, the linked entities may also be too common to be considered an entity. This may introduce errors and irrelevance to the summary. In the example, "Wednesday" is erroneous because it is wrongly linked to the entity "Wednesday Night Baseball". Also, "swap" is irrelevant because although it is linked correctly to the entity "Trade (Sports)", it is too common and irrelevant when generating the summaries. In our experimental data, we randomly select 100 data instances and tag the correctness and relevance of extracted entities into one of four labels: A: correct and relevant, B: correct and somewhat relevant, C: correct but irrelevant, and D: incorrect. Results show that 29.4%, 13.7%, 30.0%, and 26.9% are tagged with A, B, C, and D, respectively, which shows that there is a large amount of incorrect and irrelevant entities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Our model</head><p>To solve the issues described above, we present Entity2Topic (E2T), a module that can be easily attached to any sequence-to-sequence based abstractive summarization model. E2T encodes the linked entities extracted from the text and transforms them into a single topic vector. This vector is ultimately concatenated to the decoder hidden state vectors. The module contains two submodules specifically for the issues presented by the en-tity linking systems: the entity encoding submodule with selective disambiguation and the pooling submodule with firm attention.</p><p>Overall, our full architecture can be illustrated as in <ref type="figure" target="#fig_1">Figure 2</ref>, which consists of an entity linking system (ELS), a sequence-to-sequence with attention mechanism model, and the E2T module. We note that our proposed module can be easily attached to more sophisticated abstractive summarization models <ref type="bibr" target="#b27">(Zhou et al., 2017;</ref><ref type="bibr" target="#b22">Tan et al., 2017)</ref> that are based on the traditional encoderdecoder framework and consequently can produce better results. The code of the base model and the E2T are available online 3 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Base model</head><p>As our base model, we employ a basic encoderdecoder RNN used in most neural machine translation <ref type="bibr" target="#b0">(Bahdanau et al., 2014)</ref> and text summarization <ref type="bibr" target="#b14">(Nallapati et al., 2016)</ref> tasks. We employ a two-layer bidirectional GRU (BiGRU) as the recurrent unit of the encoder. The BiGRU consists of a forward and backward GRU, which results to sequences of forward and backward hidden states</p><formula xml:id="formula_0">( ? ? h 1 , ? ? h 2 , ..., ? ? h n ) and ( ? ? h 1 , ? ? h 2 , ..., ? ? h n ), respec- tively: ? ? h i = GRU (x i , ? ? h i?1 ) ? ? h i = GRU (x i , ? ? h i+1 )</formula><p>The forward and backward hidden states are concatenated to get the hidden state vectors of the</p><formula xml:id="formula_1">tokens (i.e. h i = [ ? ? h i ; ? ? h i ]).</formula><p>The final states of the forward and backward GRU are also concatenated to create the final text representation vector of the encoder s = [ ? ? h n ; ? ? h 1 ]. These values are calculated per layer, where x t of the second layer is h t of the first layer. The final text representation vectors are projected by a fully connected layer and are passed to the decoder as the initial hidden states s 0 = s.</p><p>For the decoder, we use a two-layer unidirectional GRU with attention. At each time step t, the previous token y t?1 , the previous hidden state s t?1 , and the previous context vector c t?1 are passed to a GRU to calculate the new hidden state s t , as shown in the equation below.  The context vector c t is computed using the additive attention mechanism <ref type="bibr" target="#b0">(Bahdanau et al., 2014)</ref>, which matches the current decoder state s t and each encoder state h i to get an importance score. The scores are then passed to a softmax and are used to pool the encoder states using weighted sum. The final pooled vector is the context vector, as shown in the equations below.</p><formula xml:id="formula_2">s t = GRU (w t?1 , s t?1 , c t?1 ) 3 https://github.com/rktamplayo/ Entity2Topic</formula><formula xml:id="formula_3">g t,i = v a tanh(W a s t?1 + U a h i ) a t,i = exp(g t,i ) i exp(g t,i ) c t = i a t,i h i</formula><p>Finally, the previous token y t?1 , the current context vector c t , and the current decoder state s t are used to generate the current word y t with a softmax layer over the decoder vocabulary, as shown below.</p><formula xml:id="formula_4">o t = W w w t?1 + W c c t + W s s t p(y t |y &lt;t ) = sof tmax(W o o t )</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Entity encoding submodule</head><p>After performing entity linking to the input text using the ELS, we receive a sequential list of linked entities, arranged based on their location in the text. We embed these entities to d-dimensional vectors E = {e 1 , e 2 , ..., e m } where e i ? R d . Since these entities may still contain ambiguity, it is necessary to resolve them before applying them to the base model. Based on the idea that an ambiguous entity can be disambiguated using its neighboring entities, we introduce two kinds of disambiguating encoders below.</p><p>Globally disambiguating encoder One way to disambiguate an entity is by using all the other entities, putting more importance to entities that are nearer. For this purpose, we employ an RNNbased model to globally disambiguate the entities. Specifically, we use BiGRU and concatenate the forward and backward hidden state vectors as the new entity vector:</p><formula xml:id="formula_5">? ? h i = GRU (e i , ? ? h i?1 ) ? ? h i = GRU (e i , ? ? h i+1 ) e i = [ ? ? h i ; ? ? h i ]</formula><p>Locally disambiguating encoder Another way to disambiguate an entity is by using only the direct neighbors of the entity, putting no importance value to entities that are far. To do this, we employ a CNN-based model to locally disambiguate the entities. Specifically, we do the convolution operation using filter matrices W f ? R h?d with filter size h to a window of h words. We do this for different sizes of h. This produces new feature vectors c i,h as shown below, where f (.) is a non-linear function:</p><formula xml:id="formula_6">c i,h = f ([e i?(h?1)/2 ; ...; e i+h(+1)/2 ] W f + b f )</formula><p>The convolution operation reduces the number of entities differently depending on the filter size h. To prevent loss of information and to produce the same amount of feature vectors c i,h , we pad the entity list dynamically such that when the filter size is h, the number of paddings on each side is (h ? 1)/2. The filter size h therefore refers to the number of entities used to disambiguate a middle entity. Finally, we concatenate all feature vectors </p><formula xml:id="formula_7">e i = [c i,h 1 ; c i,h 2 ; ...]</formula><p>The question on which disambiguating encoder is better has been a debate; some argued that using only the local context is appropriate <ref type="bibr" target="#b10">(Lau et al., 2013)</ref> while some claimed that additionally using global context also helps <ref type="bibr" target="#b23">(Wang et al., 2015)</ref>. The RNN-based encoder is good as it smartly makes use of all entities, however it may perform bad when there are many entities as it introduces noise when using a far entity during disambiguation. The CNN-based encoder is good as it minimizes the noise by totally ignoring far entities when disambiguating, however determining the appropriate filter sizes h needs engineering. Overall, we argue that when the input text is short (e.g. a sentence), both encoders perform comparably, otherwise when the input text is long (e.g. a document), the CNN-based encoder performs better.</p><p>Selective disambiguation It is obvious that not all entities need to be disambiguated. When a correctly linked and already adequately disambiguated entity is disambiguated again, it would make the entity very context-specific and might not be suitable for the summarization task. Our entity encoding submodule therefore uses a selective mechanism that decides whether to use the disambiguating encoder or not. This is done by introducing a selective disambiguation gate d. The final entity vector? i is calculated as the linear transfor-mation of e i and e i :</p><formula xml:id="formula_8">e i = encoder(e i ) d = ?(W d e i + b d ) e i = d ? f (W x e i + b x )+ (1 ? d) ? f (W y e i + b y )</formula><p>The full entity encoding submodule is illustrated in <ref type="figure" target="#fig_2">Figure 3</ref>. Ultimately, the submodule outputs the disambiguated entity vectors? = {? 1 ,? 2 , ...,? m }.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Pooling submodule</head><p>The entity vectors? are pooled to create a single topic vector t that represents the topic of the summary. One possible pooling technique is to use soft attention <ref type="bibr" target="#b24">(Xu et al., 2015)</ref> on the vectors to determine the importance value of each vector, which can be done by matching each entity vector with the text vector s from the text encoder as the context vector. The entity vectors are then pooled using weighted sum. One problem with soft attention is that it considers all entity vectors when constructing the topic vector. However, not all entities are important and necessary when generating summaries. Moreover, a number of these entities may be erroneous and irrelevant, as reported in Section 2.2. Soft attention gives non-negligible important scores to these entities, thus adds unnecessary noise to the construction of the topic vector.</p><p>Our pooling submodule instead uses firm attention mechanism to consider only top k entities when constructing the topic vector. This is done in a differentiable way as follows:</p><formula xml:id="formula_9">G = v a tanh(W a? + U a s) K = top k(G) P = sparse vector(K, 0, ??) g i = g i + p i a i = exp(g i ) i exp(g i ) t = i a i?i</formula><p>where the functions K = top k(G) gets the indices of the top k vectors in G and P = sparse vector(K, 0, ??) creates a sparse vector where the values of K is 0 and ?? otherwise 4 . The sparse vector P is added to the original importance score vector G to create a new importance score vector. In this new vector, important scores of non-top k entities are ??. When softmax is applied, this gives very small, negligible, and closeto-zero values to non-top k entities. The value k depends on the lengths of the input text and summary. Moreover, when k increases towards infinity, firm attention becomes soft attention. We decide k empirically (see Section 5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Extending from the base model</head><p>Entity2Topic module extends the base model as follows. The final text representation vector s is used as a context vector when constructing the topic vector t in the pooling submodule. The topic vector t is then concatenated to the decoder hidden state vectors s i , i.e. s i = [s i ; t]. The concatenated vector is finally used to create the output vector:</p><formula xml:id="formula_10">o i = W w w i?1 + W c c i + W s s i 4 Related work</formula><p>Due to its recent success, neural network models have been used with competitive results on abstractive summarization. A neural attention model was first applied to the task, easily achieving stateof-the-art performance on multiple datasets <ref type="bibr">(Rush et al., 2015)</ref>. The model has been extended to instead use recurrent neural network as decoder <ref type="bibr" target="#b4">(Chopra et al., 2016)</ref>. The model was further extended to use a full RNN encoder-decoder framework and further enhancements through lexical and statistical features <ref type="bibr" target="#b14">(Nallapati et al., 2016)</ref>. The current state-of-the-art performance is achieved by selectively encoding words as a process of distilling salient information <ref type="bibr" target="#b27">(Zhou et al., 2017)</ref>.</p><p>Neural abstractive summarization models have also been explored to summarize longer documents. Word extraction models have been previously explored, performing worse than sentence extraction models <ref type="bibr" target="#b3">(Cheng and Lapata, 2016)</ref>. Hierarchical attention-based recurrent neural networks have also been applied to the task, owing to the idea that there are multiple sentences in a document <ref type="bibr" target="#b14">(Nallapati et al., 2016)</ref>. Finally, distractionbased models were proposed to enable models to traverse the text content and grasp the overall meaning <ref type="bibr" target="#b2">(Chen et al., 2016)</ref>. The current state-ofthe-art performance is achieved by a graph-based attentional neural model, considering the key factors of document summarization such as saliency, fluency and novelty <ref type="bibr" target="#b22">(Tan et al., 2017)</ref>. Previous studies on the summarization tasks have only used entities in the preprocessing stage to anonymize the dataset <ref type="bibr" target="#b14">(Nallapati et al., 2016)</ref> and to mitigate out-of-vocabulary problems <ref type="bibr" target="#b22">(Tan et al., 2017)</ref>. Linked entities for summarization are still not properly explored and we are the first to use linked entities to improve the performance of the summarizer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experimental settings</head><p>Datasets We use two widely used summarization datasets with different text lengths. First, we use the Annotated English Gigaword dataset as used in <ref type="bibr">(Rush et al., 2015)</ref>. This dataset receives the first sentence of a news article as input and use the headline title as the gold standard summary. Since the development dataset is large, we randomly selected 2000 pairs as our development dataset. We use the same held-out test dataset used in <ref type="bibr">(Rush et al., 2015)</ref> for comparison. Second, we use the CNN dataset released in <ref type="bibr" target="#b7">(Hermann et al., 2015)</ref>. This dataset receives the full news article as input and use the human-generated multiple sentence highlight as the gold standard summary. The original dataset has been modified and preprocessed specifically for the document summarization task <ref type="bibr" target="#b14">(Nallapati et al., 2016)</ref>. In addition to the previously provided datasets, we extract linked entities using Dexter 5 <ref type="bibr" target="#b1">(Ceccarelli et al., 2013)</ref>, an open source ELS that links text snippets found in a given text to entities contained in Wikipedia. We use the default recommended parameters stated in the website. We summarize the statistics of both datasets in <ref type="table">Table 1</ref>.</p><p>Implementation For both datasets, we further reduce the size of the input, output, and entity vocabularies to at most 50K as suggested in <ref type="bibr" target="#b19">(See et al., 2017)</ref> and replace less frequent words to "&lt;unk&gt;". We use 300D Glove 6 <ref type="bibr" target="#b17">(Pennington et al., 2014)</ref> and 1000D wiki2vec 7 pre-trained vectors to initialize our word and entity vectors. For GRUs, we set the state size to 500. For CNN, we set h = 3, 4, 5 with 400, 300, 300 feature maps, respectively. For firm attention, k is tuned by calculating the perplexity of the model starting with smaller values (i. <ref type="figure" target="#fig_0">e. k = 1, 2, 5, 10, 20, ...)</ref> and stopping when the perplexity of the model becomes worse than the previous model. Our preliminary tuning showed that k = 5 for Gigaword dataset and k = 10 for CNN dataset are the best choices. We use dropout <ref type="bibr" target="#b20">(Srivastava et al., 2014)</ref> on all non-linear connections with a dropout rate of 0.5. We set the batch sizes of Gigaword and CNN datasets to 80 and 10, respectively. Training is done via stochastic gradient descent over shuffled mini-batches with the Adadelta update rule, with l 2 constraint (Hinton et al., 2012) of 3. We perform early stopping using a subset of the given development dataset. We use beam search of size 10 to generate the summary.</p><p>Baselines For the Gigaword dataset, we compare our models with the following abstractive baselines: ABS+ <ref type="figure" target="#fig_0">(Rush et al., 2015)</ref> is a fine tuned version of ABS which uses an attentive CNN encoder and an NNLM decoder, Feat2s (Nallapati et al., 2016) is an RNN sequence-to-sequence model with lexical and statistical features in the encoder, Luong-NMT <ref type="bibr" target="#b13">(Luong et al., 2015)</ref> is a two-layer LSTM encoder-decoder model, RAS-Elman <ref type="bibr" target="#b4">(Chopra et al., 2016)</ref> uses an attentive CNN encoder and an Elman RNN decoder, and SEASS <ref type="bibr" target="#b27">(Zhou et al., 2017)</ref> uses BiGRU encoders and GRU decoders with selective encoding. For the CNN dataset, we compare our models with the following extractive and abstractive baselines: Lead-3 is a strong baseline that extracts the first three sentences of the document as summary, LexRank extracts texts using LexRank <ref type="bibr" target="#b5">(Erkan and Radev, 2004)</ref>, Bi-GRU is a non-hierarchical one-layer sequence-to-sequence abstractive baseline, Distraction-M3 <ref type="bibr" target="#b2">(Chen et al., 2016)</ref> uses a sequence-to-sequence abstractive model with distraction-based networks, and GBA <ref type="bibr" target="#b22">(Tan et al., 2017)</ref> is a graph-based attentional neural abstractive model. All baseline results used beam search and are gathered from previous papers. Also, 6 https://nlp.stanford.edu/projects/ glove/ 7 https://github.com/idio/wiki2vec   we compare our final model BASE+E2T with the base model BASE and some variants of our model (without selective disambiguation, using soft attention).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Results</head><p>We report the ROUGE F1 scores for both datasets of all the competing models using ROUGE F1 scores <ref type="bibr" target="#b11">(Lin, 2004)</ref>. We report the results on the Gigaword and the CNN dataset in <ref type="table" target="#tab_3">Table 2 and Table 3</ref>, respectively. In Gigaword dataset where the texts are short, our best model achieves a comparable performance with the current state-of-the-art.</p><p>In CNN dataset where the texts are longer, our best model outperforms all the previous models. We emphasize that E2T module is easily attachable to better models, and we expect E2T to improve  their performance as well. Overall, E2T achieves a significant improvement over the baseline model BASE, with at least 2 ROUGE-1 points increase in the Gigaword dataset and 6 ROUGE-1 points increase in the CNN dataset. In fact, all variants of E2T gain improvements over the baseline, implying that leveraging on linked entities improves the performance of the summarizer. Among the model variants, the CNN-based encoder with selective disambiguation and firm attention performs the best. Automatic evaluation on the Gigaword dataset shows that the CNN and RNN variants of BASE+E2T have similar performance. To break the tie between both models, we also conduct human evaluation on the Gigaword dataset. We instruct two annotators to read the input sentence and rank the competing summaries from first to last according to their relevance and fluency: (a) the original summary GOLD, and from models (b) BASE, (c) BASE+E2T cnn , and (d) BASE+E2T rnn . We then compute (i) the proportion of every ranking of each model and (ii) the mean rank of each model. The results are reported in <ref type="table" target="#tab_6">Table 4</ref>. The model with the best mean rank is BASE+E2T cnn , followed by GOLD, then by BASE+E2T rnn and BASE, respectively. We also perform ANOVA and post-hoc Tukey tests to show that the CNN variant is significantly (p &lt; 0.01) better than the RNN variant and the base model. The RNN variant does not perform as well as the CNN variant, contrary to the automatic ROUGE evaluation above. Interestingly, the CNN variant produces better (but with no significant difference) summaries than the gold summaries. We posit that this is due to the fact that the article title does not correspond to the summary of the first sentence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Selective disambiguation of entities</head><p>We show the effectiveness of the selective disambiguation gate d in selecting which entities to disambiguate or not. <ref type="table">Table 6</ref> shows a total of four different examples of two entities with the highest/lowest d values. In the first example, sentence E1.1 contains the entity "United States" and is linked with the country entity of the same name, however the correct linked entity should be "United States Davis Cup team", and therefore is given a high d value. On the other hand, sentence E1.2 is linked correctly to the country "United States", and thus is given a low d value.. The second example provides a similar scenario, where sentence E2.1 is linked to the entity "Gold" but should be linked to the entity "Gold medal". Sentence E2.2 is linked correctly to the chemical element. Hence, the former case received a high value d while the latter case received a low d value.</p><p>Entities as summary topic Finally, we provide one sample for each dataset in <ref type="table">Table 5</ref> for case study, comparing our final model that uses firm attention (BASE cnn+sd ), a variant that uses soft attention (BASE cnn+soft ), and the baseline model (BASE). We also show the attention weights of the firm and soft models.</p><p>In the Gigaword example, we find three observations. First, the base model generated a less informative summary, not mentioning "mexico state" and "first edition". Second, the soft model produced a factually wrong summary, saying that "guadalajara" is a mexican state, while actually it is a city. Third, the firm model is able to solve the problem by focusing only on the five most important entities, eliminating possible noise such as "Unk" and less crucial entities such as "Country club". We can also see the effectiveness of the selective disambiguation in this example, where the entity "U.S. state" is corrected to mean the entity "Mexican state" which becomes relevant and is therefore selected.</p><p>In the CNN example, we also find that the baseline model generated a very erroneous summary. We argue that this is because the length of the input text is long and the decoder is not guided as to which topics it should focus on. The soft model generated a much better summary, however it focuses on the wrong topics, specifically on "Iran's nuclear program", making the summary less general. A quick read of the original article tells us that the main topic of the article is all about the two political parties arguing over the deal with Iran. However, the entity "nuclear" appeared a lot in the article, which makes the soft model wrongly focus on the "nuclear" entity. The firm model produced the more relevant summary, focusing on the po-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Gigaword Dataset Example</head><p>Original western mexico @state @jalisco will host the first edition of the @UNK dollar @lorena ochoa invitation @golf tournament on nov. ##-## #### , in @guadalajara @country club , the @lorena ochoa foundation said in a statement on wednesday . Baseline netanyahu says he is a country of " UNK cheating " and that it is a country of " UNK cheating " netanyahu says he is a country of " UNK cheating " and that " is a very bad deal " he says he says he says the plan is a country of " UNK cheating " and that it is a country of " UNK cheating " he says the u.s. is a country of " UNK cheating " and that is a country of " UNK cheating " Soft benjamin netanyahu : " i think there 's a third alternative , and that is standing firm , " netanyahu tells cnn . he says he does not roll back iran 's nuclear ambitions . " it does not roll back iran 's nuclear program . "</p><p>Firm new : netanyahu : " i think there 's a third alternative , and that is standing firm , " netanyahu says . obama 's comments come as democrats and republicans spar over the framework announced last week to lift western sanctions on iran . <ref type="table">Table 5</ref>: Examples from Gigaword and CNN datasets and corresponding summaries generated by competing models. The tagged part of text is marked bold and preceded with at sign (@). The red color fill represents the attention scores given to each entity. We only report the attention scores of entities in the Gigaword example for conciseness since there are 80 linked entities in the CNN example.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Text d</head><p>Linked entity: https://en.wikipedia.org/wiki/United_States E1.1: andy roddick got the better of dmitry tursunov in straight sets on friday , assuring the @united states a #-# lead over defending champions russia in the #### davis cup final .</p><p>0.719 E1.2: sir alex ferguson revealed friday that david beckham 's move to the @united states had not surprised him because he knew the midfielder would not return to england if he could not come back to manchester united .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>0.086</head><p>Linked entity: https://en.wikipedia.org/wiki/Gold E2.1: following is the medal standing at the ##th olympic winter games -lrb-tabulated under team , @gold , silver and bronze -rrb-: UNK 0.862 E2.2: @gold opened lower here on monday at ###.##-### .## us dollars an ounce , against friday 's closing rate of ###.##-### .## . 0.130 <ref type="table">Table 6</ref>: Examples with highest/lowest disambiguation gate d values of two example entities (United States and gold). The tagged part of text is marked bold and preceded with at sign (@). litical entities (e.g. "republicans", "democrats"). This is due to the fact that only the k = 10 most important elements are attended to create the summary topic vector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>We proposed to leverage on linked entities to improve the performance of sequence-to-sequence models on neural abstractive summarization task. Linked entities are used to guide the decoding process based on the summary topic and commonsense learned from a knowledge base. We introduced Entity2Topic (E2T), a module that is easily attachable to any model using an encoder-decoder framework. E2T applies linked entities into the summarizer by encoding the entities with selective disambiguation and pooling them into one summary topic vector with firm attention mechanism. We showed that by applying E2T to a basic sequence-to-sequence model, we achieve significant improvements over the base model and consequently achieve a comparable performance with more complex summarization models.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>TheFigure 1 :</head><label>1</label><figDesc>Los Angeles Dodgers acquired South Korean right-hander Jae Seo from the New York Mets on Wednesday in a four-player swap. Observations on linked entities in summaries. O1: Summaries are mainly composed of entities. O2: Entities can be used to represent the topic of the summary. O3: Entity commonsense learned from a large corpus can be used.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Full architecture of our proposed sequence-to-sequence model with Entity2Topic (E2T) module.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Entity encoding submodule with selective disambiguation applied to the entity 3 . The left figure represents the full submodule while the right figure represents the two choices of disambiguating encoders. of different h's for each i as the new entity vector:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>The Los Angeles Dodgers acquired South Korean right-hander Jae Seo from the New York Mets on Wednesday in a four-player swap. Input Text Entity List ? /wiki/Los_Angeles_Dodgers ? /wiki/South_Korean ? /wiki/Seo_Jae-woong ? /wiki/New_York_Mets ? /wiki/Wednesday_Night_Baseball ? /wiki/Trade_(sports) Entity Linking System The Los Angeles Dodgers acquired South ? Sequence-to-Sequence with Attention</head><label></label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Korea's</cell><cell>Seo</cell><cell>headed</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Attention Mechanism</cell></row><row><cell>Bi-GRU Text</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>?</cell><cell>GRU Text</cell></row><row><cell>Encoder</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Decoder</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>&lt;START&gt;</cell><cell>Korea's</cell><cell>Seo</cell></row><row><cell cols="2">Entity2Topic Module</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="4">Entity Encoder with Selective Disambiguation</cell><cell cols="2">Pooling with Firm Attention</cell></row><row><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Results on the Gigaword dataset using the fulllength F1 variants of ROUGE.</figDesc><table><row><cell>Model</cell><cell cols="3">RG-1 RG-2 RG-L</cell></row><row><cell>BASE: s2s+att</cell><cell>25.5</cell><cell>5.8</cell><cell>20.0</cell></row><row><cell>BASE+E2T cnn+sd</cell><cell>31.9</cell><cell>10.1</cell><cell>23.9</cell></row><row><cell>BASE+E2T rnn+sd</cell><cell>27.6</cell><cell>7.9</cell><cell>21.5</cell></row><row><cell>BASE+E2T cnn</cell><cell>26.6</cell><cell>7.3</cell><cell>20.7</cell></row><row><cell>BASE+E2T rnn</cell><cell>26.1</cell><cell>6.9</cell><cell>20.1</cell></row><row><cell>BASE+E2T cnn+soft</cell><cell>26.6</cell><cell>7.0</cell><cell>20.6</cell></row><row><cell>BASE+E2T rnn+soft</cell><cell>25.0</cell><cell>6.7</cell><cell>19.8</cell></row><row><cell>Lead-3</cell><cell>26.1</cell><cell>9.6</cell><cell>17.8</cell></row><row><cell>LexRank</cell><cell>26.1</cell><cell>9.6</cell><cell>17.7</cell></row><row><cell>Bi-GRU</cell><cell>19.5</cell><cell>5.2</cell><cell>15.0</cell></row><row><cell>Distraction-M3</cell><cell>27.1</cell><cell>8.2</cell><cell>18.7</cell></row><row><cell>GBA</cell><cell>30.3</cell><cell>9.8</cell><cell>20.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Results on the CNN dataset using the fulllength F1 ROUGE metric.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Human evaluations on the Gigaword dataset. Bold-faced values are the best while red-colored values are the worst among the values in the evaluation metric.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/idio/wiki2vec</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://en.wikipedia.org/wiki/South_ Korean</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">We use ?10 9 to represent ??.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">http://dexter.isti.cnr.it/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>We would like to thank the three anonymous reviewers for their valuable feedback. This work was supported by Microsoft Research, and Institute for Information communications Technology Promotion (IITP) grant funded by the Korea government (MSIT) (No.2017-0-01778 , Development of Explainable Humanlevel Deep Machine Learning Inference Framework). S. Hwang is a corresponding author.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Dexter: an open source framework for entity linking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diego</forename><surname>Ceccarelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claudio</forename><surname>Lucchese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salvatore</forename><surname>Orlando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raffaele</forename><surname>Perego</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salvatore</forename><surname>Trani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the sixth international workshop on Exploiting semantic annotations in information retrieval</title>
		<meeting>the sixth international workshop on Exploiting semantic annotations in information retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="17" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenhua</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Si</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Jiang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.08462</idno>
		<title level="m">Distraction-based neural networks for document summarization</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianpeng</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.07252</idno>
		<title level="m">Neural summarization by extracting sentences and words</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Abstractive sentence summarization with attentive recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander M</forename><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="93" to="98" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Lexrank: Graph-based lexical centrality as salience in text summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G?nes</forename><surname>Erkan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dragomir R Radev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="457" to="479" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">On the reproducibility of the tagme entity linking system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faegheh</forename><surname>Hasibi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krisztian</forename><surname>Balog</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svein</forename><forename type="middle">Erik</forename><surname>Bratsberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Information Retrieval</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="436" to="449" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Teaching machines to read and comprehend</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Moritz Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Kocisky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lasse</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mustafa</forename><surname>Suleyman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1693" to="1701" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Improving neural networks by preventing coadaptation of feature detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Geoffrey E Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan R</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1207.0580</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Unsupervised graph-based topic labelling using dbpedia</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioana</forename><surname>Hulpus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Conor</forename><surname>Hayes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcel</forename><surname>Karnstedt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Greene</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the sixth ACM international conference on Web search and data mining</title>
		<meeting>the sixth ACM international conference on Web search and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="465" to="474" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">unimelb: Topic modelling-based word sense induction for web snippet clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Jey Han Lau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Baldwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SemEval@ NAACL-HLT</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="217" to="221" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Rouge: A package for automatic evaluation of summaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Text summarization branches out: Proceedings of the ACL-04 workshop</title>
		<meeting><address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Design challenges for entity linking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="315" to="328" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Effective approaches to attentionbased neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.04025</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Abstractive text summarization using sequence-to-sequence rnns and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramesh</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xiang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.06023</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Analyzing entities and topics in news articles using statistical topic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Newman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaitanya</forename><surname>Chemudugunta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Padhraic</forename><surname>Smyth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Steyvers</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="93" to="104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Semantic documents relatedness using concept graph representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Qiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yosi</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dafna</forename><surname>Mass</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><forename type="middle">Jia</forename><surname>Sheinwald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shao Sheng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Ninth ACM International Conference on Web Search and Data Mining</title>
		<meeting>the Ninth ACM International Conference on Web Search and Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="635" to="644" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)</title>
		<meeting>the 2014 conference on empirical methods in natural language processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Alexander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rush</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1509.00685</idno>
		<title level="m">Sumit Chopra, and Jason Weston. 2015. A neural attention model for abstractive sentence summarization</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abigail</forename><surname>See</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04368</idno>
		<title level="m">Get to the point: Summarization with pointer-generator networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Abstractive document summarization with a graphbased attentional neural model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojun</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianguo</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1171" to="1181" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A sense-topic model for word sense induction with unsupervised data enrichment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Brian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T Yu</forename><surname>Ziebart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Clement</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="59" to="71" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelvin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhudinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rich</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2048" to="2057" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ikuya</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroyuki</forename><surname>Shindo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hideaki</forename><surname>Takeda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshiyasu</forename><surname>Takefuji</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.02494</idno>
		<title level="m">Learning distributed representations of texts and entities from knowledge base</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Recent advances in document summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin-Ge</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojun</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianguo</forename><surname>Xiao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Selective encoding for abstractive sentence summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingyu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.07073</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
