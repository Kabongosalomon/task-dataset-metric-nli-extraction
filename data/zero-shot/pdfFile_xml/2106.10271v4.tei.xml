<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">End-to-end Temporal Action Detection with Transformer</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="20211">AUGUST 2021 1</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Journal Of L A T E X Class</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Files</surname></persName>
						</author>
						<title level="a" type="main">End-to-end Temporal Action Detection with Transformer</title>
					</analytic>
					<monogr>
						<imprint>
							<biblScope unit="volume">14</biblScope>
							<biblScope unit="issue">8</biblScope>
							<date type="published" when="20211">AUGUST 2021 1</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T15:08+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Transformer</term>
					<term>Temporal Action Detection</term>
					<term>Tem- poral Action Localization</term>
					<term>Action Recognition</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Temporal action detection (TAD) aims to determine the semantic label and the temporal interval of every action instance in an untrimmed video. It is a fundamental and challenging task in video understanding. Previous methods tackle this task with complicated pipelines. They often need to train multiple networks and involve hand-designed operations, such as non-maximal suppression and anchor generation, which limit the flexibility and prevent end-to-end learning. In this paper, we propose an end-to-end Transformer-based method for TAD, termed TadTR. Given a small set of learnable embeddings called action queries, TadTR adaptively extracts temporal context information from the video for each query and directly predicts action instances with the context. To adapt Transformer to TAD, we propose three improvements to enhance its locality awareness. The core is a temporal deformable attention module that selectively attends to a sparse set of key snippets in a video. A segment refinement mechanism and an actionness regression head are designed to refine the boundaries and confidence of the predicted instances, respectively. With such a simple pipeline, TadTR requires lower computation cost than previous detectors, while preserving remarkable performance. As a self-contained detector, it achieves state-of-the-art performance on THUMOS14 (56.7% mAP) and HACS Segments (32.09% mAP). Combined with an extra action classifier, it obtains 36.75% mAP on ActivityNet-1.3. Code is available at https://github.com/xlliu7/TadTR. Fig. 1: Comparison of different pipelines of temporal action detection. (a) Multi-stage pipeline in [1], [2], etc.; (b) Twostage pipeline in [6], [7]; (c) Top-down one-stage pipeline in [8], (d) Bottom-up pipeline in [9] (e) The set prediction pipeline in this work.</p><p>cation and regression on a large amount of candidate segments. Bottom-up methods [9], [12] perform per-frame classification and group these predictions into segment-level predictions. While these methods achieve state-of-the-art performance on standard benchmarks, they have complex pipelines.</p><p>As shown in <ref type="figure">Fig. 1</ref>, these pipelines involve post-processing operations, such as non-maximum suppression (NMS) and grouping. These operations, together with anchor setting in many top-down methods, are hand-designed with prior knowledge about this task and not learnable, which restricts the flexibility. Besides, most proposal-based methods [2], [11], [13], requires a standalone classifier to classify action proposals. These issues block the gradient flow and prevent end-to-end learning. Thus, it is necessary to develop a simple end-toend method that directly predicts action instances in a single differentiable network 1 without hand-crafted components.</p><p>In this paper, we introduce an end-to-end temporal action detection framework to address the above issues. Inspired by the object detection Transformer (DETR) [15], we directly map a set of learnable embeddings, called action queries, to action instances in parallel. As the queries do not directly indicate the initial locations of actions like anchors or proposals, we are unable to extract features for each query <ref type="bibr" target="#b0">1</ref> The single network means the detection network upon the video encoder. Training the video encoders along with the detection head for long videos often requires excessive computing resources. Therefore, most methods use offline features (e.g. I3D <ref type="bibr" target="#b13">[14]</ref>) trained separately with large mini-batch size on large amounts of short (around 2 seconds) videos.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>V IDEO understanding has become more important than ever as the rapid growth of media prompts the generation, sharing, and consumption of videos. As a fundamental task in video understanding, temporal action detection (TAD) aims to predict the semantic label, the start time, and the end time of every action instance in an untrimmed and possibly long video. For its wide range of applications, including security surveillance, home care, video editing, video recommendation, and so on, temporal action detection has gained increasing attention from the community in recent years <ref type="bibr" target="#b0">[1]</ref>- <ref type="bibr" target="#b4">[5]</ref>.</p><p>Previous methods for TAD can be roughly categorized into two groups. Top-down methods <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref> perform classifi- This paper has supplementary downloadable material available at http://ieeexplore.ieee.org., provided by the author. from specific locations like previous methods. The detector is required to extract sufficient long-term context information before knowing which interval an action falls in. Besides, the context should be adaptive and relevant with each query, in order to differentiate between these queries. Traditional 1D convolutional neural networks cannot easily achieve them, due to a fixed receptive field and fixed weights. Recently, Transformers <ref type="bibr" target="#b15">[16]</ref> have shown great power in sequence modeling. It is able to reason the relations between sequence elements and adaptively capture long-term context with the self-attention module. A video is naturally a sequence of frames and there is abundant context in it <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b16">[17]</ref>. Therefore Transformer is a desirable choice for the above goals.</p><p>Based on the above motivations, we propose a Temporal Action Detection TRansformer (TadTR) that predicts actions by extracting relevant context for each action query. It has an encoder-decoder structure. The encoder models inter-snippet relations to capture snippet-level context. The decoder models action-snippet relations to enhance each action query with snippet-level context, and inter-action relations to capture instance-level context from the other action queries. In this way, we can exploit richer context than previous methods that only exploit snippet-level context <ref type="bibr" target="#b1">[2]</ref> or instance-level context <ref type="bibr" target="#b12">[13]</ref>. Upon the decoder, two feed-forward networks (FFNs) predict the class and the segment for each action query. During training, an action matching module dynamically determines a one-to-one ground truth assignment according to the predictions. Owing to this, our detector avoids duplicate detections and NMS is unnecessary. It produces a very sparse set of action detections (10 ? 10 2 ), orders of magnitude fewer than previous methods (10 3 ? 10 4 ).</p><p>However, due to the intrinsic difference between space and time, a direct application of Transformer is not appropriate. We observe that different frames in a video of actions are highly similar, because of the temporal redundancy and the slow changes in backgrounds or actors. Besides, the boundaries of actions are less clear than those of objects <ref type="bibr" target="#b17">[18]</ref>. Therefore, to precisely detect actions, a detector needs to be localityaware, which means being aware of the subtle local changes in the temporal domain. The dense attention module in primitive Transformer that attends to all elements in a sequence, is less sensitive to such local changes by design. To mitigate this issue, we draw inspiration from <ref type="bibr" target="#b18">[19]</ref> and propose a temporal deformable attention (TDA) module as the basic building block of Transformer. It selectively attends to a sparse set of key elements around a reference location in the input sequence, where the sampling locations and attention weights are learned and dynamically adjusted in accordance with the inputs. In this way, it can adaptively extract context information while preserving locality awareness.</p><p>Besides TDA, we make two additional improvements to enhance locality awareness. First, a segment refinement mechanism is employed to refine the boundaries of predicted actions. To be concrete, we iteratively re-attend to the video according to the previous predictions and refine the boundaries with the newly extracted context. Second, we attach an actionness regression head to Transformer to predict a reliable confidence score called actionness for detection ranking. It extracts the local features with RoIAlign <ref type="bibr" target="#b19">[20]</ref> for each predicted action and estimates its IoU with the best-matched ground truth action. This is more reliable than simply using classification scores, as the classification branch may find a shortcut from context but ignore the complete local details. Despite being seemingly small changes, they significantly improve performance.</p><p>We conduct comprehensive experiments on three datasets to evaluate TadTR. With a surprisingly simple pipeline, TadTR achieves remarkable performance with a low computation cost. Without any extra classifier, it achieves state-of-the-art performance on HACS Segments <ref type="bibr" target="#b20">[21]</ref> and THUMOS14 <ref type="bibr" target="#b21">[22]</ref>. When combined an extra classifier, it reaches 36.75% mAP on ActivityNet-1.3 <ref type="bibr" target="#b22">[23]</ref>, outperforming strong competitors such as G-TAD <ref type="bibr" target="#b1">[2]</ref> and BMN <ref type="bibr" target="#b10">[11]</ref>. In terms of run time, it takes only 155 ms per video on THUMOS14, which is much faster than recent state-of-the-art methods, as shown in <ref type="figure">Fig. 2</ref>. We believe that the simplicity, the flexibility, and the strong performance of the new method will benefit and ease future research on temporal action detection.</p><p>The contributions of this work are as follows:</p><p>? We introduce an end-to-end set prediction (SP) framework that simplifies the pipeline for temporal action detection (TAD). It can detect actions in a single differentiable network without hand-crafted components. ? We propose a Transformer architecture that is enhanced with locality awareness to better adapt to the TAD task. The core is a temporal deformable attention (TDA) module that selectively attends to a sparse set of key snippets in a video. We show that TDA is crucial for the success of the SP framework for TAD. ? Different from previous works that ignore context or only exploit snippet-level or instance-level context, we model inter-snippet, inter-action, and action-snippet relations to capture both levels of context for more accurate temporal action detection. ? Our method achieves state-of-the-art performance of self-contained detectors on HACS Segments and THU-MOS14, and competitive results on ActivityNet-1.3. Besides, it requires a lower computation cost than its competitors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>Temporal Action Detection. Previous TAD methods can be roughly categorized into top-down methods and bottom-up methods according to the pipeline. Top-down methods can be further categorized into multi-stage, two-stage, and onestage methods. (a) Multi-stage methods <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b23">[24]</ref>- <ref type="bibr" target="#b26">[27]</ref> first generate candidate segments and train a binary classifier that associates each segment with a confidence score, resulting in proposals. Those proposals with high scores are fed to a multi-class classifier to classify the actions. The candidate segments are generated by dense uniform sampling <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b27">[28]</ref> or grouping local frames that may contain actions <ref type="bibr" target="#b28">[29]</ref>. Some methods <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b30">[31]</ref> combine multiple schemes for complementarity. (b) Two-stage methods <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b32">[33]</ref>  pre-defined multi-scale anchors associated with each temporal location. These methods need to manually set multiple anchor scales, which restricts the flexibility. Note that multi-stage methods can also be seen as generalized two-stage methods.</p><p>(c) Top-down one-stage methods <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b33">[34]</ref> can be seen as the class-aware variant of the one-stage proposal generator. (d) Bottom-up methods perform frame-level action classification and merge the frame-level results to segment-level predictions. For example, <ref type="bibr" target="#b8">[9]</ref> first predicts the action and boundary probabilities and then groups frames with maximal structured sum as actions. Recent anchor-free methods (e.g., AFSD <ref type="bibr" target="#b34">[35]</ref> and A2Net <ref type="bibr" target="#b35">[36]</ref>) also belong to this group. Besides these methods, a few works (e.g., CTAP <ref type="bibr" target="#b29">[30]</ref> and PCG-TAL <ref type="bibr" target="#b36">[37]</ref>) combine different pipelines to enhance the performance. All the above methods require post-processing steps such as NMS or grouping, which prevent end-to-end learning. An early work by Yeung et al. <ref type="bibr" target="#b37">[38]</ref> also proposes a TAD method without hand-crafted components. Based on recurrent neural networks (RNN) and reinforcement learning (RL), it learns action detection by training an agent that iteratively picks an observation location and deciding whether to emit or refine a candidate action after observation. However, its reward function is not differetiable. Therefore it does not meet the criteria of end-to-end in this paper. All the above methods are fully-supervised. There are also some weakly-supervised methods that only utilize single-frame supervision <ref type="bibr" target="#b38">[39]</ref> or video-level supervision <ref type="bibr" target="#b39">[40]</ref>- <ref type="bibr" target="#b50">[51]</ref> during training.</p><p>Transformers and Context in Video Understanding. Transformers have achieved great success in natural language processing <ref type="bibr" target="#b15">[16]</ref> and image understanding <ref type="bibr" target="#b51">[52]</ref>- <ref type="bibr" target="#b53">[54]</ref>. The core of Transformer is the self-attention mechanism that aggregates non-local cues through a weighted sum of features at attended locations. Compared with convolutions, self-attention can capture long-range context and dynamically adjust weights according to the input. Recently, many works have revealed the great potential of Transformers in video understanding tasks <ref type="bibr" target="#b54">[55]</ref>- <ref type="bibr" target="#b56">[57]</ref>. For example, VideoBERT <ref type="bibr" target="#b54">[55]</ref> and Act-BERT <ref type="bibr" target="#b57">[58]</ref> utilize Transformers to learn an joint representation for video and text. TimeSformer <ref type="bibr" target="#b55">[56]</ref> decouples spatial and temporal self-attention for video classification. Zhou et al. <ref type="bibr" target="#b58">[59]</ref> capture the temporal dependency with Transformer for video captioning. Girdhar et al. <ref type="bibr" target="#b59">[60]</ref> apply Transformer to model the relationship between spatial proposals for spatio-temporal action detection. In this paper, Transformer is used to capture temporal context information for temporal action detection. Specifically, we employ attention modules to model the relations between video snippets, the relations actions and snippets, and the relations between actions. Several concurrent works also employ Transformer for context modeling in temporal action detection (AGT <ref type="bibr" target="#b60">[61]</ref>) and temporal action proposal generation (RTD-Net <ref type="bibr" target="#b61">[62]</ref> and TAPG <ref type="bibr" target="#b62">[63]</ref>). However, these works either adopt a traditional TAD pipeline or have difficulty in training. TAPG still relies on hand-crafted anchors and post-processing steps. RTD-Net requires a three-step training scheme to optimize different parts of the network separately and relies on extra action classifiers to classify the proposals. AGT suffers from slow training convergence (1000? more iterations than TadTR). In addition, different from these works that exploit the vanilla attention module, TadTR introduces a more efficient temporal deformable attention module that adaptively attends to a sparse set of key snippets in a video. As a result, it enjoys lower computation costs and easier training. Therefore, TadTR is more practical.</p><p>In the field of temporal action detection, some previous works also exploit context in other ways. For example, increasing the receptive field by a fixed ratio <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b16">[17]</ref>. However, this is not flexible enough and may introduce irrelevant information from unrelated frames. Another line of works exploit context by modeling the relations between different snippets <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b63">[64]</ref> or the relations between different proposals <ref type="bibr" target="#b12">[13]</ref> with graph. The attention modules in this work are alternatives to them. Moreover, we model different kinds of relations and can capture richer context of different levels.</p><p>DETR and Deformable DETR. Temporal action detection methods <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b35">[36]</ref> often draw inspiration from object detection methods. This work is inspired by DETR <ref type="bibr" target="#b14">[15]</ref> and Deformable DETR <ref type="bibr" target="#b18">[19]</ref>. DETR proposes a Transformer-based Set Prediction (SP) framework to achieve end-to-end object detection without hand-crafted components. Deformable DETR proposes multi-scale deformable attention to address the issues of slow convergence and limited feature resolution of DETR. While extending them for direct TAD is intuitive, the effectiveness remains unclear. Our main contribution over DETR and Deformable DETR is that we adapt the SP framework and deformable attention for direct TAD and validate their effectiveness. Although the high-level design of TadTR is similar to Deformable DETR, the implementation is different as TadTR aims to temporally localize actions in videos while Deformable DETR is designed for object detection in images.</p><p>Besides, we reveal that deformable attention is crucial for the success of the SP framework for TAD and segment refinement is also important. Furthermore, directly extending Deformable DETR to TAD does not achieve satisfactory performance as the confidence scores predicted by the decoder are not reliable.  The decoder extracts relevant context from the encoder for each action query and models the relations between action queries. Upon the decoder, we use feed-forward networks to predict the segments and classes of output actions. A segment refinement mechanism (the blue box) and an actionness regression head (the orange box) are utilized to refine the boundaries and the confidence scores of the predicted actions, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CNN</head><p>To relieve this issue, we add a simple yet effective actionness regression head to refine the confidence scores. To sum up, the adaptation and improvement over DETR and Deformable DETR make the SP framework practical for TAD and TadTR can serve as a strong baseline for SP-based TAD.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. TADTR</head><p>TadTR is constructed on video features encoded with a pretrained video classification network (e.g., I3D <ref type="bibr" target="#b13">[14]</ref>). <ref type="figure" target="#fig_2">Fig. 3</ref> shows the overall architecture of TadTR. TadTR takes as input the video features and a set of learnable action queries. Then it outputs a set of action predictions. Each action prediction is represented as a tuple of the temporal segment, the confidence score, and the semantic label. It consists of a Transformer encoder to model the interactions between video snippets, a Transformer decoder to predict action segments, and an extra actionness regression head to estimate the confidence score of the predicted segments. During training, an action matching module is used to determine a one-to-one ground truth assignment to the action predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Architecture</head><p>Encoder. Let X V ? R T S ?C denotes the video feature sequence, where T S and C are the length and dimension, respectively. Each frame in the feature sequence is a feature vector extracted from a certain snippet in the video. Here, a snippet means a sequence of a few (e.g., 8) consecutive frames. We use linear projection to make C = 256. The encoder models the relations between different snippets and outputs a feature sequence X E ? R T S ?C enhanced with temporal context. As depicted in <ref type="figure" target="#fig_2">Fig. 3</ref>, it consists of L E Transformer encoder layers of the homogeneous architecture. Each encoder layer has two sub-layers, i.e., a temporal deformable attention (TDA) module, and a feed-forward network (FFN). Layer normalization <ref type="bibr" target="#b64">[65]</ref> is used after each sub-layer and a residual connection is added between the input of each sub-layer and the output of the follow-up normalization layer. Except for TDA, all the other components are identical to the primitive Transformer <ref type="bibr" target="#b15">[16]</ref>.</p><p>TDA is an alternative to the dense attention module in <ref type="bibr" target="#b15">[16]</ref>. The high similarities between different frames and the vagueness of action boundaries require a detector to possess locality awareness. In other words, the detector should be more sensitive to local changes in the temporal domain. The dense attention module that attends to all locations in an input feature sequence, is less sensitive to such local changes. Besides, it suffers from high computation cost and slow convergence <ref type="bibr" target="#b18">[19]</ref>. To better fit the TAD task, we draw inspiration from <ref type="bibr" target="#b18">[19]</ref> and propose a temporal deformable attention (TDA) module that adaptively attends to a sparse set of temporal locations around a reference location in the input feature sequence.</p><p>Let z q ? R C be the feature of query q and t q ? [0, 1] be the normalized coordinate of the corresponding reference point. Given an input feature sequence X ? R T S ?C , the output h m ? R T S ?(C/M ) of the m-th (m ? {1, 2, ..., M }) head of a TDA module is computed by an weighted sum of a set of key elements sampled from X:</p><formula xml:id="formula_0">h m = K k=1 a mqk W V m X((t q + ?t mqk )T S ),<label>(1)</label></formula><p>where K is the number of sampling points, a mqk ? [0, 1] is the normalized attention weight, and ?t mqk ? [0, 1] is the sampling offset relative to t q . X((t q + ?t mqk )T S ) is the linear interpolated feature at (t q + ?t mqk )T S as it is fractional. Following <ref type="bibr" target="#b18">[19]</ref>, the attention weight a mqk and the sampling offset ?t mqk are predicted from the query feature z q by linear projection. We normalize the attention weight with softmax to make</p><formula xml:id="formula_1">K k=1 a mqk = 1. W V m ? R C?(C/M ) is a learnable weight.</formula><p>The output of TDA is computed by a linear combination of the outputs of different heads:</p><formula xml:id="formula_2">TDA(z q , t q , X) = W O Concat(h 1 , h 2 , ..., h m ),<label>(2)</label></formula><p>where</p><formula xml:id="formula_3">W O ? R C?C is a learnable weight.</formula><p>When computing the ? -th frame in the output sequence, the query and the reference point are both the ? -th frame in the input sequence. Therefore, we refer to TDA in the encoder as temporal deformable self-attention (TDSA). The query feature is the summation of the input feature of that frame and the position embedding at that location. The position embedding is used to differentiate between different locations in the input sequence. In this paper, we use the sinusoidal position embedding following <ref type="bibr" target="#b15">[16]</ref>.</p><formula xml:id="formula_4">X P (?, ?) = sin ? 10000 ?/C ? is even cos ? 10000 (??1)/C ? is odd .<label>(3)</label></formula><p>The feed-forward network consists of two fully connected (FC) layers and a ReLU activation in between. It is the same across different positions and can be viewed as a stack of two 1D convolution layers with kernel size 1. The dimensions of the two FC layers are C F = 2048 and C = 256, respectively.</p><p>Decoder. The decoder takes as input the encoder features X E and N q action queries with learnable embeddings? (0) = {?</p><formula xml:id="formula_5">(0) i } Nq i=1</formula><p>. It transforms these embeddings to N q action predic-tions? = {? i }. As illustrated in <ref type="figure" target="#fig_2">Fig. 3</ref>, the decoder consists of L D sequential decoder layers. Each decoder layer has three major sub-layers: a self-attention module, a temporal deformable cross-attention (TDCA) module, and a feed-forward network. Similar to each encoder layer, we add a residual connection between each sub-layer and the following layer normalization function. The output of the l-th decoder layer is denoted by z (l) . The self-attention module models the relation between action queries and updates their embeddings. The motivation here is that multiple actions in one video are often related. For example, a cricket shot action often appears after a cricket bowling action. To make an action prediction, each query extracts relevant context information from the video via the TDCA module. Given the encoder features X E and the input embedding? i ? R C , the output query embedding of TDCA is formulated as T DA(? i ,t i , X E ). Here,t i is the coordinate of the reference point in X E . By default, it is predicted by a projection function f IR from? which is more flexible than hand-crafted anchor setting or proposal sampling. An analysis is given in Sec. IV-D.</p><p>Prediction Heads. Upon the output (the updated query embeddings) of each decoder layer, we apply FFNs to predict the classification probabilitiesp i and the temporal segment s i = (t i ,d i ) of the action instance? i corresponding to each query. Botht i andd i are normalized. To make the boundaries of the instances more accurate, a segment refinement mechanism is proposed. Besides, an additional actionness regression head is employed to refine the confidence score. They are detailed below. Segment Refinement. Transformer is able to capture longrange context information. However, the predicted action boundaries might be unsatisfactory for lack of locality. Inspired by <ref type="bibr" target="#b18">[19]</ref>, we introduce a refinement mechanism to enhance locality awareness and improve localization performance. It involves two strategies. The first is the incremental refinement of segments. Instead of predicting the segments independently at each decoder layer, we adjust the segments according to previously predicted segments layer by layer. Formally, given each action segment?</p><formula xml:id="formula_6">(l?1) i = (t (l?1) i ,d (l?1) i )</formula><p>predicted at the (l ? 1)-th decoder layer, the l-th decoder layer predicts the location offsets (?t</p><formula xml:id="formula_7">(l) i , ?d (l) i ) relative to? (l?1) i .</formula><p>The corresponding refined segment?</p><formula xml:id="formula_8">(l) i = (t (l) i ,d (l) i ) is then computed by: t (l) i = ?(?t (l) i + ? ?1 (t (l?1) i )), l ? {1, 2, ..., L D } (4) d (l) i = ?(?d (l) i + ? ?1 (d (l?1) i )), l ? {2, 3, ..., L D },<label>(5)</label></formula><p>where ?(?) and ? ?1 (?) are the sigmoid and the inverse sigmoid function, respectively. Specially,t</p><p>i , is the initial reference pointt i predicted by f IR . The initial value ofd</p><formula xml:id="formula_10">(l) i isd (1) i</formula><p>predicted at the first decoder layer. The second is iterative reference point adjustment. We update the reference points of TDCA in each decoder layer instead of always usingt</p><formula xml:id="formula_11">(0) i . Specifically,t (l?1) i</formula><p>, the refined segment center at the (l ? 1)th decoder layer, is used as the reference point of TDCA at the l-th decoder layer. In this way, TDCA can be adaptive to the input video and better aligned with the local features of the action instances. We validate the effectiveness of the two strategies in the experiments.</p><p>Actionness Regression. One challenge of temporal action detection is to generate reliable confidence scores for ranking. Typically, classification scores are used. However, the classification task focuses more on discriminative features and is less sensitive to the localization quality of an action. As a result, the classification score of the detections may be unreliable for ranking. An example is shown in <ref type="figure">Fig. 4</ref>.</p><p>To mitigate this issue, we employ an actionness regression head that extracts context aligned with the interval of a predicted segment and predicts an actionness score upon it. Given the encoder feature sequence X E and a predicted segment s i by the decoder, we first apply temporal RoIAlign <ref type="bibr" target="#b19">[20]</ref> upon X E to obtain the aligned features X si ? R T R ?C within the interval defined by s i from X E . Here, T R is the number of bins for RoIAlign. To include a certain amount of context information around the boundaries, we slightly expand Here, a prediction with a lower overlap with the curling action has a higher score than a more accurate prediction (0.50 vs. 0.39).</p><p>the segment by a factor of when applying RoIAlign. The expanded segment can be expressed as (t i , d i ). Then, a feedforward network is used to predict the actionness score? i from the aligned feature.? i is supervised by the maximal IoU g i (intersection over union) between s i and all ground truth actions. In this way, the detector is enforced to be more sensitive to local features in order to differentiate between different segments.</p><p>Discussion. The actionness regression head is somewhat similar to the second stage of the traditional two-stage method, as they can both refine the confidence scores. However, the set prediction pipeline adopted by TadTR is significantly different from traditional pipelines. It is hard to categorize TadTR and its variants into one-stage or two-stage methods, as it does not need anchor setting and post-processing steps like traditional one-stage or two-stage methods. Besides, the actionness regression head is very lightweight. It only needs to predict class-agnostic confidence scores. The computation cost (in FLOPs) of this head is 6.59% of that of the full model. Differently, the second stage of a two-stage method often contributes to a major amount of computation cost (e.g., 99.98% in BMN <ref type="bibr" target="#b10">[11]</ref>). We also note that TadTR can already make sparse and complete detections without actionness regression.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Training and Inference</head><p>Action Matching. The action matching module determines the targets assigned to each detection during training. Inspired by DETR <ref type="bibr" target="#b14">[15]</ref> in object detection, we frame it as a set-toset bipartite matching problem to ensure a one-to-one ground truth assignment. Let Y = {y j } Nq j=1 be a set of ground truth actions padded with ? (no action) and ? be the permutation that assigns each target y j to the corresponding detection? ?(j) . Bipartie matching aims to find a permutation that minimizes the overall matching cost:? = arg min Nq j=1 C(y j ,? ?(j) ).</p><p>The matching cost considers the classification probabilities and the distance between ground truth and predicated segments. Specifically, C(y j ,? ?(j) ) is defined as</p><formula xml:id="formula_13">1 cj =? [L cls (p ?(j) , c j ) + L seg (s j ,? ?(j) )],<label>(7)</label></formula><p>where c j and s j are the class label and the temporal segment of y j . L cls (p ?(j) , c j ) is the classification term. We use crossentropy loss by default. L seg (s j ,? ?(j) ) is the distance between the predicted location and the ground truth location, defined as ? iou L iou (s j ,? ?(j) ) + ? coord L L1 (s j ,? ?(j) ),</p><p>where L L1 is the L 1 distance and L iou is the IoU loss. IoU loss is defined as the the opposite number of the IoU. ? iou and ? coord are hyper-parameters. The matching problem is solved with the Hungarian algorithm. Through the set-based action matching, each ground truth will be assigned to only one prediction, thus avoiding duplicate predictions. This brings two merits. First, TadTR does not rely on the non-differentiable non-maximal suppression (NMS) for post-processing and enjoys end-to-end training. Second, we can make sparse predictions with limited queries (e.g. 10) instead of dense predictions in many previous works (e.g. tens of thousands for BMN <ref type="bibr" target="#b10">[11]</ref> and G-TAD <ref type="bibr" target="#b1">[2]</ref>), which saves the computation cost.</p><p>In a way, the action matching module performs a learnable NMS. The matching cost takes the classification scores of the detections into account. In this way, those detections with lower scores are more likely to be assigned with a non-action target. As a result, their classification scores will be suppressed in the training process.</p><p>Loss Functions. Once the ground truth assignment is determined, we optimize the network by minimizing the following multi-part loss functions:</p><formula xml:id="formula_15">L = Nq j=1 [L cls (p? (j) , c j ) + 1 cj =? L seg (s j ,?? (j) ) +? act L L1 (?? (j) , g? (j) )],<label>(9)</label></formula><p>where the first two items optimize the detections from the decoder and the last one optimizes the outputs of actionness regression. L cls uses focal loss <ref type="bibr" target="#b65">[66]</ref>.? is the solution of Equation <ref type="bibr" target="#b5">6</ref>. ? act is a hyper-parameter.</p><p>Inference. During inference, we ignore the action predictions from all but the last decoder layer. The confidence score for a detection? i is computed by p i (? i ) ?? i , where? i is the predicted action label.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Experimental Setup</head><p>Datasets and Evaluation Metrics. We conduct experiments on THUMOS14 <ref type="bibr" target="#b21">[22]</ref>, HACS Segments <ref type="bibr" target="#b20">[21]</ref>, and ActivityNet-1.3 <ref type="bibr" target="#b22">[23]</ref>. THUMOS14 is built on videos from 20 sports action classes. It contains 200 and 213 untrimmed videos for training and testing. There are 3007 and 3358 action instances on the two sets. The average length of actions is 5 seconds. ActivityNet-1.3 and HACS Segments share the same 200 classes of daily activities. Both datasets are split into three sets: training, validation, and testing. The numbers of videos in these sets are 10024, 4926, and 5044 respectively on ActivityNet-1.3, and 37613, 5981, and 5987 on HACS Segments. The average length of actions is 48 seconds on ActivityNet-1.3 and 33 seconds on HACS Segments. On both datasets, the annotations on the testing set are reserved by the organizers. Therefore, we evaluate on the validation set.</p><p>Following conventions, the mean average precision (mAP) at different IoU thresholds is used for performance evaluation. On THUMOS14, the IoU thresholds for computing mAPs are [0.3 : 0.7 : 0.1]. On the other two datasets, we report mAPs at the thresholds {0.5, 0.75, 0.95} and the average mAP at the thresholds [0.5 : 0.95 : 0.05]. For simplicity, we denote mAP at the IoU threshold ? as mAP ? and the average mAP is referred to as mAP unless specially noted.</p><p>Video Feature Extraction. Most TAD methods are based on offline extracted video features. For easier comparison with them, we also use video features as the input of our method. For experiments on HACS Segments, we directly use the official I3D features 2 , which are extracted with I3D trained on Kinetics at 2FPS. On the other datasets, we use the commonly used features in previous works. On THUMOS14, the twostream I3D <ref type="bibr" target="#b13">[14]</ref> networks pre-trained on Kinetics <ref type="bibr" target="#b13">[14]</ref> are taken as the video encoder, and the features are extracted every 8 frames. On ActivityNet-1.3, we use the two-stream TSN <ref type="bibr" target="#b66">[67]</ref> features extracted at 5FPS. Following previous works <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b10">[11]</ref>, we resize the video features to a fixed length of 100 via linear interpolation on ActivityNet-1.3 and HACS Segments. Since the videos are long on THUMOS14, we follow <ref type="bibr" target="#b10">[11]</ref> to crop each video feature sequence with windows of length 128 and stride 64 for training. In each window, we reserve the instances contained in it and clip the instances that partially overlap with it. We ignore the instances that have less than 1-second overlap with the window during training. This strategy is called length-based instance filtering. During inference, the stride is increased to 96 and the duplicate detections in the overlapped region are merged with NMS. This strategy is called crosswindow fusion (CWF). We also report the performance of TadTR tested on non-overlapping windows (with a stride of 128). In this case, we simply take the union of detections from all windows of a video. Implementation Details. L E and L D are set to 2 and 4, respectively. The loss weights ? iou , ? coord and ? act are set to 2, 5 and 5 respectively. The numbers of attention heads M and sampling points K are set to 8 and 4, respectively. The parameters of the linear layers that predict attention weights are initialized to zero. We initialize the linear layers that predict sampling offsets to make {?p mqk } 8 m=1 = (k, 0, ?k, 0, k, 0, ?k, 0) at initialization. The expanding factor and the number of bins T R for RoIAlign in the actionness regression head are 1.5 and 16 respectively.</p><p>TadTR is trained using AdamW <ref type="bibr" target="#b67">[68]</ref> optimizer. The initial learning rate is 2 ? 10 ?4 and scaled by a factor of 0.1 after training for a certain number of epochs. The learning rates of the linear projection layers for predicting attention weights and sampling offsets are multiplied by 0.1. We train the models for 30, 15, and 30 epochs and decrease the learning rates after 25, 12, and 25 epochs on THUMOS14, ActivityNet-1.3, and HACS Segments respectively. The batch size is set to <ref type="bibr">16.</ref> In the experiments, we also explore an improved training setting. Following RetinaNet <ref type="bibr" target="#b65">[66]</ref> and AFSD <ref type="bibr" target="#b34">[35]</ref>, we use focal loss <ref type="bibr" target="#b65">[66]</ref> for the classification term in the matching cost 2 http://hacs.csail.mit.edu/hacs_segments_features.zip (Eq. 7). We find that this modification speeds up convergence. Therefore the total numbers of training epochs are reduced to <ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b11">12</ref>, and 20 on the three datasets, respectively. The learning rates are decreased after 14, 9, and 18 epochs, respectively. Besides, we use the integrity-based instance filtering (IBIF) strategy in G-TAD <ref type="bibr" target="#b1">[2]</ref> and AFSD <ref type="bibr" target="#b34">[35]</ref> to replace the default length-based instance filtering strategy on THUMOS14. To be specific, in each window, we only keep those ground truth instances whose integrity exceeds 0.75. Here, the integrity of an instance s g in a window s w is defined as |s g ? s w |/|s g |, where | ? | means the length. It is similar to IoU but has a different denominator. Those windows without such instances are ignored during training.</p><p>The experiments are conducted on a workstation with a single Tesla P100 GPU card, and Intel(R) Xeon(R) CPU E5-2682 v4 @ 2.50GHz. It takes around 10 minutes, 36 minutes, and 150 minutes to finish training on THUMOS14, ActivityNet-1.3, and HACS Segments, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Main Results</head><p>THUMOS14. <ref type="table" target="#tab_4">Table I</ref> demonstrates the temporal action detection performance and run time comparison on the testing set of THUMOS14. We measure the run time of these methods with publicly available implementations under the same environment (a single P100 GPU). We run methods on the full testing set with batch size set to 1 and report the average time and FLOPs per video. The average length of videos on THUMOS14 is 217 seconds. BMN <ref type="bibr" target="#b10">[11]</ref> and G-TAD <ref type="bibr" target="#b1">[2]</ref> use two-stream TSN <ref type="bibr" target="#b66">[67]</ref> features originally. For a fair comparison, we also report their performance with I3D features. For AFSD <ref type="bibr" target="#b34">[35]</ref>, we have excluded the computation cost of the feature extractor. For TadTR, we report the performance with different training and inference settings. The entry with ? is with integrity-based instance filtering (IBIF). The entries with * are with IBIF and focal loss. TadTR-lite is the variant that does not use cross-window fusion (CWF) during inference. We observe that: 1) TadTR* achieves the best performance among all the compared methods in terms of mAP at all IoU thresholds. Even the variant without CWF can achieve state-of-the-art performance. TadTR* is slightly better than TadTR ? owing to focal loss.  and 8? faster than the competitive single-network detector A2Net. It also requires much fewer FLOPs. The efficiency of our method is owing to the simple framework and the sparsity of predictions.</p><p>The above results indicate that our method is both accurate and efficient. We also note that many other methods are composed of multiple independently trained networks. Those proposal generation methods (BSN, MGG, BMN, G-TAD, MR, BC-GNN, and RTD-Net) are not self-contained, as they rely on an extra classifier (such as P-GCN) to accomplish the TAD task. Differently, TadTR can achieve action detection with only a single unified network.</p><p>We note that the computation cost of TadTR is not comparable with those methods that directly take video frames as input, such as R-C3D. Most previous works use video features as input and focus on the design of detection networks.</p><p>HACS Segments. We report the performance of TadTR, SSN <ref type="bibr" target="#b9">[10]</ref>, and the state-of-the-art method G-TAD <ref type="bibr" target="#b1">[2]</ref> in <ref type="table" target="#tab_4">Table II</ref>. Our method achieves an average mAP of 30.83%, which outperforms SSN (+11.86% mAP) and G-TAD (+3.35% mAP). Besides, our method requires 455? fewer GFLOPs than G-TAD. As for run time, the network inference and postprocessing step of G-TAD take 33 ms and 908 ms per video, respectively. The total run time is 941 ms, 49.5? that of TadTR <ref type="bibr">(19 ms)</ref>. We also try the improved training setting, which results in 32.09% mAP. The results again illustrate the superiority of TadTR.</p><p>ActivityNet-1.3. <ref type="table" target="#tab_4">Table III</ref> compares the performance of different methods on the validation set of ActivityNet-1.3. Some methods (e.g., G-TAD <ref type="bibr" target="#b1">[2]</ref>) only implement action proposal generation and cannot produce action detections without external action classifiers. We divide the methods into two groups according to whether external action classifiers are used. Being simple and end-to-end trainable, TadTR achieves an average mAP of 28.21%, which is stronger than all the other methods. With the improved training setting, TadTR achieves 1.69% higher mAP. This variant is 2.56% better than the second-best method PCG-TAL <ref type="bibr" target="#b36">[37]</ref> in terms of mAP. Compared with the second-best single-network detector TAL-Net, we improve the performance by 9.68%. For comparison with previous methods <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b35">[36]</ref> that are combined with an ensemble of classifiers <ref type="bibr" target="#b68">[69]</ref>, we also try such a combination. To be concrete, we pass the detections by TadTR to the classifiers and fuse the classification scores of TadTR and the classifiers by multiplication. When fused with <ref type="bibr" target="#b9">[10]</ref>, TadTR enjoys a significant performance boost, achieving an average mAP of 34.64%. It is better than the other compared methods in terms of average mAP, although some methods use the stronger I3D features. When using the  I3D features, the average mAP is further boosted to 36.11%, outperforming the second-best method AFSD by 1.72%. Besides, the performance is achieved at a low computation cost, as indicated by the smaller FLOPs. TadTR can also be combined with BMN. This is implemented by connecting the encoder of TadTR to the detection head of BMN. Owing to the adaptive context captured by the Transformer encoder, TadTR+BMN achieves an improvement of 0.7% over BMN, reaching 34.55% mAP. It also outperforms the recent method G-TAD. This indicates the advantage of Transformer in temporal action detection. Note that 99.98% of the computation cost is on the proposal classification branch of BMN. Therefore the total computation cost of TadTR+BMN is close to that of BMN.</p><p>With the stronger TSP <ref type="bibr" target="#b70">[71]</ref> features, the performance of TadTR reaches 36.75% mAP. It outperforms G-TAD by 0.94% and BMN by 1.08%. This again demonstrates the superiority of TadTR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Ablation Study</head><p>In this subsection, we validate the effectiveness of different components of TadTR and evaluate the effects of various hyper-parameters. Unless specially noted, all reported results are with the default training and setting. TadTR-lite is used for THUMOS14.</p><p>The importance of context information. The key of Transformer is the self-attention mechanism that incorporates the context in a video sequence. In TadTR, we leverage two kinds of context, snippet-level context from related snippets and instance-level context from related action queries, which are captured by Transformer encoder and the self-attention module in Transformer decoder respectively. By removing Transformer encoder, we get a variant "TadTR w/o encoder". By removing the self-attention module in the decoder, we get a variant "TadTR w/o instance-level context". We report the performance of the two variants in <ref type="table" target="#tab_4">Table IV</ref>. It is observed that removing the encoder leads to a 3.89% drop on HACS Segments, 6.93% drop on THUMOS14, and 0.87% drop on ActivityNet in terms of average mAP. It indicates that the Transformer encoder is crucial for our model, as the decoder requires long-range and adaptive context to reason the relations between the actions and the video. Removing instancelevel context, the average mAP drops by 1.13% on HACS Segments, 2.66% on THUMOS14, and 1.98% on ActivityNet. We conclude that the context information from other action instances is also helpful for action detection. Transformer encoder v.s. CNN encoder. We try replacing the Transformer encoder with a 1D CNN encoder, which is common for temporal modeling in previous TAD methods. The 1D CNN encoder is composed of two 1D convolutional layers with 256 filters of kernel size 3 and ReLU activation. As can be observed in <ref type="table" target="#tab_8">Table V</ref>, using 1D CNN encoder leads to 2.88% average mAP drop when NMS is not applied (the default option), and 1.41% average mAP drop when NMS is applied. Interestingly, the performance of this variant with NMS is improved by 1.17% over that without NMS. It indicates that there are many duplicate detections. One possible reason is that CNN features are locally correlated. Therefore, it is hard to differentiate between close predictions as they have similar features.</p><p>To further dissect the performance gap between TadTR with 1D CNN encoder (equipped with NMS) and TadTR, we divide the ground truth instances into 3 groups according to the normalized duration: short (0 ? 0.1), medium (0.1 ? 0.2) and long (0.2 ? 1) and report the average mAP for each group in <ref type="figure">Fig. 5</ref>. Here we use the normalized duration because we resize the video features into a fixed length. For reference, the average duration per video is 148 seconds. As can be observed, TadTR with Transformer encoder achieves better performance for medium-length and long actions. 1D CNN encoder is slightly better for short actions. The result is reasonable, as 1D CNN is good at modeling short-term dependency but poor at modeling long-term dependency.</p><p>We also explore deeper CNNs and larger convolution kernels, but no improvement is observed. In terms of computation cost, this variant has much higher FLOPs than TadTR with Transformer encoder. The results show that 1D CNN is inferior to Transformer encoder.  <ref type="figure">Fig. 6</ref>: Actionness regression improves the ranking of detections. In each of the above cases, the initial scores are unreliable. The more accurate detection obtains a lower score than the less accurate one before rescoring. With the new scores (on the right of the arrows) applied, the ranking order turns satisfactory. Best viewed in color. with vanilla dense attention modules. This variant is called "TadTR with dense attention". As depicted in <ref type="table" target="#tab_4">Table IV</ref>, the performance of this variant is far behind TadTR, especially on THUMOS14 (-23.77% mAP), even if the model is trained for 180 epochs. It means temporal deformable attention is crucial for the success of TadTR. The main reason is that the dense attention lacks locality awareness. As different frames are usually similar in background, a dense attention module tends to over-smooth input sequence at initialization (see an example in the supplementary material). As a result, it is hard to localize temporal segments with different semantics. Besides, the variant with dense attention has 5.7? higher computation cost than that of TadTR. Therefore, temporal deformable attention is a better choice.</p><p>The effects of actionness regression and segment refinement. We study the effects of the two components by removing them individually, resulting in 2 different model variants. Their results are presented in <ref type="table" target="#tab_4">Table IV</ref>. Comparing TadTR w/o actionness regression and TadTR, we observe that actionness regression leads to improvements of all metrics. Specifically, the improvements are 3.06%, 2.26%, 1.55%, and 2.32% in terms of mAP at IoU 0.5, 0.75, 0.95 and the average mAP on HACS Segments. On THUMOS14 and ActivityNet, the average mAP improves by 2.83% and 2.08%.</p><p>Several qualitative examples are presented in <ref type="figure">Fig. 6</ref> to show how actionness regression helps. It can produce more reliable confidence scores for the action predictions.</p><p>Comparing TadTR w/o segment refinement and TadTR, we find this component helpful for improving the localization accuracy. On HACS Segments, it improves the mAP at the strict threshold of 0.95 by a large margin of 2.24%. The mAPs at other thresholds are also consistently improved. On THUMOS14 and ActivityNet, the average mAP improves by 3.85% and 0.81%.  To investigate how the segment refinement mechanism improves the performance, we evaluate the effect of incremental refinement and reference point adjustment. As can be observed in <ref type="table" target="#tab_4">Table VI</ref>, disabling incremental refinement leads to a 1.31% decrease in mAP, which suggests that the incremental refinement strategy is better than independent prediction at each decoder layer. Disabling reference point adjustment in standard segment refinement mechanism also leads to a 1.85% decrease in mAP. This is reasonable, as the initial referent points associated with the queries are invariant for different input samples. The adjustment strategy makes them adaptive to the input and aligned with the local features of the target actions.</p><p>Besides the detection performance, another important aspect is the computation cost. In terms of FLOPs, adding or removing the two components has little impact, which is shown in Table IV. In terms of run time, the average time cost per video on THUMOS14 is 130 ms for TadTR-lite without the two components. Adding the actionness regression head will increase the average time cost to 141 ms. With segment refinement enabled, the average time cost per video becomes 155 ms, which is still very efficient compared with state-ofthe-art methods.</p><p>The effects of the number of action queries. <ref type="table" target="#tab_4">Table VIII</ref> compares the performance of TadTR using different number of action queries (N q ). The best performance is achieved at N q = 40 on THUMOS14, N q = 30 on HACS Segments and N q = 10 on ActivityNet-1.3. The results are reasonable, as the average number of action instances per video on THUMOS14 <ref type="bibr">(15.4)</ref> is larger than that on ActivityNet-1.3 (1.5) and HACS Segments <ref type="bibr">(2.8)</ref>.</p><p>The effects of the numbers of encoder layers and decoder layers. We evaluate TadTR with different numbers of encoder layers (L E ) and decoder layers (L D ) and report results in <ref type="table" target="#tab_4">Table VII</ref>. With L D fixed, the best performance is achieved when L E is 2. Larger L E gives inferior results probably due to the difficulty of training. Therefore, we set L E to 2. With L E fixed, the average mAP increases by 2.03% when L D increases from 2 to 4. Larger L D gives a slightly lower performance.  Therefore we suggest setting L D to 4.</p><p>The effect of the number of sampling points. <ref type="table" target="#tab_4">Table IX</ref> compares the performance and computation cost using different numbers of sampling points K in TDA modules on HACS Segments. We observe that moderately increasing K improves the performance, as more temporal details can be captured. The best performance is achieved at K = 4. The number of sampling points has little impact on the computation cost.</p><p>The effect of the number of attention heads. The effects of the hyper-parameters in actionness regression. In <ref type="table" target="#tab_4">Table XI</ref>, we compare different choices of the expanding factor and the number of bins T R for actionness regression. We see that the best performance is achieved when = 1.5. This result is 0.56% higher than that of the variant without RoI expansion ( = 1), showing the effectiveness of RoI expansion. Among different choices of T R , T R = 16 results in the best performance.  <ref type="figure">Fig. 8</ref>: Visualization of self-attention between action queries in the last decoder layer. We average the attention over all heads. The queries are represented by the predicted instances.</p><p>The arrows indicate the attention that the topmost query casts on the other four queries with the largest weights. The attention weight is encoded by the color. The darker the arrow, the larger the attention weight. In the first example, the four attended instances are semantically related to the topmost instance.</p><p>In the second example, the topmost instance casts the most attention to a nearby instance (#37) with the same class. The other three non-action instances are also attended, probability for context from the background. Best viewed in color.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Analysis</head><p>Visualization of attention. <ref type="figure">Fig. 12</ref>   <ref type="figure">Fig. 9</ref>: Visualization of all action predictions on all videos from HACS Segments validation set from 30 action query slots. In each 1-by-1 square, we use scattered points to represent all predictions from this query. The horizontal and the vertical coordinates are the coordinate of the center and the normalized length of these predictions, respectively. We observe that each query is responsible for action predictions in certain locations and lengths.</p><p>sampling points in the decoder almost cover the full extent of an action prediction, providing a large receptive field. Differently, the sampling points in the encoder have a relatively short temporal extent, capturing a moderate amount of context. <ref type="figure">Fig. 8</ref> visualizes self-attention between action queries in the last decoder layer. We find that the topmost query in each example cast the most attention on the queries whose predicted instances are semantically related to that of it. It suggests that the self-attention layer in the decoder can model the relations between action queries (or instances).</p><p>Visualization of action queries. <ref type="figure">Fig. 9</ref> illustrates the distribution of locations and scales (lengths) of output actions associated with each action query. We observe that each query produces action predictions in certain locations and scales. Different locations and scales are covered by a small number of queries. It means that the detector learns the distribution of actions in the training dataset. This is more flexible than the hand-crafted anchor design in previous methods.  formance, it may fail on some short actions, as depicted in <ref type="figure" target="#fig_4">Fig. 10</ref>. The quantitative results in <ref type="figure">Fig. 5</ref> also illustrate the lower performance of TadTR on short actions. One possible reason is that Transformer is inferior to 1D CNN in modeling short-term dependency. Combing Transformer and 1D CNN might improve the performance on short actions. Another limitation is that TadTR will miss actions when the number of true actions in a video is larger than the number of queries N q , although such cases might be rare. This is a common issue of DETR-alike detectors. How to maintain the performance while increasing N q is worth studying in future works.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>We propose TadTR, a simple end-to-end method for temporal action detection (TAD) based on Transformer. It views the TAD task as a direct set prediction problem and maps a series of learnable embeddings to action instances in parallel by adaptively extracting temporal context in the video. It simplifies the pipeline of TAD and removes hand-crafted components such as anchor setting and post-processing. We make three improvements to enhance the Transformer with locality awareness to better adapt to the TAD task. Extensive experiments validate the remarkable performance and efficiency of TadTR and the effectiveness of different components. TadTR achieves state-of-the-art or competitive performance on HACS Segments, THUMOS14, and ActivityNet-1.3 with lower computation costs. We hope that this work could trigger the development of Transformers and efficient models for temporal action detection. The current implementation of TadTR is based on offline extracted CNN features for a fair comparison with previous methods. In the future, we plan to explore joint learning of the video encoder and TadTR <ref type="bibr" target="#b72">[73]</ref>, and temporal action detectors purely based on Transformers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX</head><p>In this supplement, we present several visualization results. <ref type="figure" target="#fig_5">Fig. 11</ref> illustrates the smoothing effect of dense attention. <ref type="figure">Fig. 12</ref> supplements <ref type="figure">Fig. 7</ref> in the main document and gives more examples to demonstrate temporal deformable attention.   <ref type="figure">Fig. 12</ref>: Visualization of temporal deformable attention. The first row is uniformly sampled video frames. The second row visualizes the attention at two randomly picked reference points in the last encoder layer. The third row visualizes the attention for the predicted action in the last decoder layer. We use different markers to represent sampling points in different heads and separate points from different heads vertically. The color of a point indicates the attention weight. Best viewed in color.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>This work was supported by National Key R&amp;D Program of China (No. 2018YFB1004600). (Corresponding author: Xiang Bai.) X. Liu (email: brucelio@outlook.com) and Q. Wang are with the School of Electronic Information and Communications, Huazhong University of Science and Technology. X. Bai (email: xbai@hust.edu.cn) is with the School of Artificial Intelligence and Automation, Huazhong University of Science and Technology. Y. Hu, X. Tang, and S. Zhang are with Alibaba Group. S. Bai is with ByteDance Inc. Part of this work was done when X. Liu was an intern at Alibaba Group.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 :</head><label>3</label><figDesc>The architecture of TadTR. It takes the video features extracted with a CNN and a set of learnable action queries as input and decodes a set of action predictions in parallel via a Transformer. The encoder captures the long-term context in the input feature sequence.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>i</head><label></label><figDesc>. f IR is implemented with a linear layer and a follow-up sigmoid function for normalization. The reference point can be seen as the initial estimation of the center of the corresponding action segment. FFNs in the decoder layers have the same architecture as those in the encoder layers.Different from TDSA in the encoder, the query embeddin? z(0) i and the reference point are learnable and shared by all input videos. This allows the network to learn the global distribution of the action locations in the training dataset,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 10 :</head><label>10</label><figDesc>Failure cases. TadTR misses the two short actions that are hard to detect while the 1D CNN-based method MUSES partially detects them.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 11 :</head><label>11</label><figDesc>Different frames in a video is usually highly similar. The dense attention tends to cast uniform attention to different locations in the input sequence at initialization. Left: The similarity matrix of each pair of snippets in CNN features of a randomly selected video. Middle: The attention weight. Right: The similarity matrix of the output feature of the dense-attention. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Time ?</head><label></label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell cols="3">Video Features ? ? ?</cell><cell></cell><cell cols="2">Encoder</cell><cell>?</cell><cell cols="2">Actionness Regression</cell><cell>Time Output</cell><cell>Add &amp; Norm ( )</cell><cell>FFN</cell></row><row><cell></cell><cell></cell><cell cols="3">Pos. Embed. ? ? ?</cell><cell>TDA</cell><cell>Add &amp; Norm</cell><cell>FFN</cell><cell>Add &amp; Norm</cell><cell>RoIAlign</cell><cell>FFN</cell><cell>Actionness {? }</cell><cell>49.2s Baseball pitch Score: 0.83</cell><cell>? ( ?1)</cell><cell>FFN TDA Add &amp; Norm Add &amp; Norm</cell></row><row><cell></cell><cell></cell><cell cols="2">( ?1)</cell><cell></cell><cell></cell><cell>?</cell><cell>( )</cell><cell cols="2">Detection ? :</cell><cell></cell><cell>52.5s</cell><cell>Self Attention</cell></row><row><cell>(0)</cell><cell>? (0)</cell><cell>? ( ?1)</cell><cell>? ( ?1)</cell><cell cols="3">Dec. Layer-l FFN FFN ? ( ) ? ( ) ? ( )</cell><cell></cell><cell cols="2">Segment ? = ( ? , ? ) Class ? = argmax ? Confidence score ? ( ? )  *  ? Class Scores {? } Non-action</cell><cell></cell><cell>54.7s Baseball pitch Score: 0.85 57.7s</cell><cell>( ?1) Dec. Layer details TDA: Temporal Deformable Attention FFN: Feed-Forward Network ? ?Segment Refinement Function Pos. Embed.: Position embedding : Initial Reference Points Projection</cell></row><row><cell cols="2">Action Queries</cell><cell cols="2">Decoder &amp;</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="3">Segment Refinement</cell><cell></cell><cell></cell><cell cols="2">Segments {? }</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>The Transformer may generate unreliable confidence scores.</figDesc><table><row><cell></cell><cell></cell><cell>Ground Truth</cell><cell>Detections</cell><cell>Time</cell></row><row><cell>0.3s</cell><cell></cell><cell></cell><cell>Curling</cell><cell>52.0s</cell></row><row><cell>1.0s</cell><cell>Curling: 0.50 5.5s</cell><cell>8.0s</cell><cell>Curling: 0.39</cell><cell>54.8s</cell></row><row><cell cols="2">Fig. 4:</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>It surpasses AGT by 9.7% in terms of mAP 0.5 . It also outperforms RTD-Net by 7.7% (56.7% vs. 49.0%) in terms</figDesc><table><row><cell>2) TadTR* outperforms the second-best method MUSES [27]</cell></row><row><cell>by 3.3% in terms of average mAP, which demonstrates the</cell></row><row><cell>advantage of our method. Compared with the competitive</cell></row><row><cell>single-network method A2Net [36], TadTR achieves 15.1%</cell></row><row><cell>higher average mAP.</cell></row><row><cell>3) Compared with the concurrent Transformer-based methods</cell></row><row><cell>AGT [61] and RTD-Net [62], TadTR* achieves better perfor-</cell></row><row><cell>mance.</cell></row></table><note>of average mAP. Besides the advantage in accuracy, TadTR is easier to train. Differently, AGT requires 1000? more training iterations (3000k vs. 3k for TadTR) due to slow convergence of dense attention. RTD-Net requires a three-step training scheme to optimize different parts of the network. 4) Our results are achieved at a low computation cost. TadTR is around 11? faster than the second-best method MUSES,</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE I :</head><label>I</label><figDesc>Comparison with state-of-the-art methods on THUMOS14. Run time is the average inference time per video, including post-processing operations, such as NMS. SN: single-network. E2E: end-to-end. TS: two-stream. For proposal generation methods, the computation cost of the extra classifiers is not included (marked with &gt;). ? Results copied from<ref type="bibr" target="#b35">[36]</ref>. ? Our implementation. * With focal loss and IBIF. ? With IBIF. mAP 0.4 mAP 0.5 mAP 0.6 mAP 0.7 mAP</figDesc><table><row><cell>Method</cell><cell>Feature</cell><cell>SN</cell><cell>E2E</cell><cell cols="7">mAP 0.3 Time/ms</cell><cell>GFLOPs</cell></row><row><cell>Yeung et al. [38]</cell><cell>VGG16</cell><cell></cell><cell>-</cell><cell>36.0</cell><cell>26.4</cell><cell>17.1</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Yuan et al. [9]</cell><cell>TS</cell><cell>-</cell><cell>-</cell><cell>36.5</cell><cell>27.8</cell><cell>17.8</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>SSAD [8]</cell><cell>TS</cell><cell></cell><cell>-</cell><cell>43.0</cell><cell>35.0</cell><cell>24.6</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>R-C3D [6]</cell><cell>C3D</cell><cell></cell><cell>-</cell><cell>44.8</cell><cell>35.6</cell><cell>28.9</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>SSN [69]</cell><cell>TS</cell><cell>-</cell><cell>-</cell><cell>51.9</cell><cell>41.0</cell><cell>29.8</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>TAL-Net [7]</cell><cell>I3D</cell><cell></cell><cell>-</cell><cell>53.2</cell><cell>48.5</cell><cell>42.8</cell><cell>33.8</cell><cell>20.8</cell><cell>39.8</cell><cell>-</cell><cell>-</cell></row><row><cell>BSN [29]</cell><cell>TS</cell><cell>-</cell><cell>-</cell><cell>53.5</cell><cell>45.0</cell><cell>36.9</cell><cell>28.4</cell><cell>20.0</cell><cell>36.8</cell><cell>&gt;2065</cell><cell>&gt;3.4</cell></row><row><cell>MGG [31]</cell><cell>TS</cell><cell>-</cell><cell>-</cell><cell>53.9</cell><cell>46.8</cell><cell>37.4</cell><cell>29.5</cell><cell>21.3</cell><cell>37.8</cell><cell>-</cell><cell>-</cell></row><row><cell>BMN [11]</cell><cell>TS</cell><cell>-</cell><cell>-</cell><cell>56.0</cell><cell>47.4</cell><cell>38.8</cell><cell>29.7</cell><cell>20.5</cell><cell>38.5</cell><cell>&gt;483</cell><cell>&gt;171.0</cell></row><row><cell>BC-GNN [64]</cell><cell>TS</cell><cell>-</cell><cell>-</cell><cell>57.1</cell><cell>49.1</cell><cell>40.4</cell><cell>31.2</cell><cell>23.1</cell><cell>40.2</cell><cell>-</cell><cell>-</cell></row><row><cell>G-TAD [2]</cell><cell>TS</cell><cell>-</cell><cell>-</cell><cell>54.5</cell><cell>47.6</cell><cell>40.2</cell><cell>30.8</cell><cell>23.4</cell><cell>39.3</cell><cell>&gt;4440</cell><cell>&gt;639.8</cell></row><row><cell>BMN  ? [11]</cell><cell>I3D</cell><cell>-</cell><cell>-</cell><cell>56.4</cell><cell>47.9</cell><cell>39.2</cell><cell>30.2</cell><cell>21.2</cell><cell>39.0</cell><cell>-</cell><cell>-</cell></row><row><cell>G-TAD  ? [2]</cell><cell>I3D</cell><cell>-</cell><cell>-</cell><cell>58.7</cell><cell>52.7</cell><cell>44.9</cell><cell>33.6</cell><cell>23.8</cell><cell>42.7</cell><cell>&gt;3552</cell><cell>&gt;368.9</cell></row><row><cell>MR [70]</cell><cell>I3D</cell><cell>-</cell><cell>-</cell><cell>53.9</cell><cell>50.7</cell><cell>45.4</cell><cell>38.0</cell><cell>28.5</cell><cell>43.3</cell><cell>&gt;644</cell><cell>&gt;36.8</cell></row><row><cell>A2Net [36]</cell><cell>I3D</cell><cell></cell><cell>-</cell><cell>58.6</cell><cell>54.1</cell><cell>45.5</cell><cell>32.5</cell><cell>17.2</cell><cell>41.6</cell><cell>1554</cell><cell>30.4</cell></row><row><cell>P-GCN [13]</cell><cell>I3D</cell><cell>-</cell><cell>-</cell><cell>63.6</cell><cell>57.8</cell><cell>49.1</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>7298</cell><cell>4.4</cell></row><row><cell>P-GCN  ? [13]</cell><cell>I3D</cell><cell>-</cell><cell>-</cell><cell>64.9</cell><cell>59.0</cell><cell>49.4</cell><cell>36.7</cell><cell>22.6</cell><cell>46.5</cell><cell>7298</cell><cell>4.4</cell></row><row><cell>G-TAD [2]+P-GCN [13]</cell><cell>I3D</cell><cell>-</cell><cell>-</cell><cell>66.4</cell><cell>60.4</cell><cell>51.6</cell><cell>37.6</cell><cell>22.9</cell><cell>47.8</cell><cell>-</cell><cell>-</cell></row><row><cell>AGT [61]</cell><cell>I3D</cell><cell></cell><cell></cell><cell>65.0</cell><cell>58.1</cell><cell>50.2</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>PCG-TAL [37]</cell><cell>I3D</cell><cell>-</cell><cell>-</cell><cell>64.2</cell><cell>57.3</cell><cell>48.3</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>RTD-Net [62]</cell><cell>I3D</cell><cell>-</cell><cell>-</cell><cell>68.3</cell><cell>62.3</cell><cell>51.9</cell><cell>38.8</cell><cell>23.7</cell><cell>49.0</cell><cell>&gt;211</cell><cell>&gt;32.1</cell></row><row><cell>AFSD [35]</cell><cell>I3D</cell><cell>-</cell><cell>-</cell><cell>67.3</cell><cell>62.4</cell><cell>55.5</cell><cell>43.7</cell><cell>31.1</cell><cell>52.0</cell><cell>3245</cell><cell>84.1</cell></row><row><cell>MUSES [27]</cell><cell>I3D</cell><cell>-</cell><cell>-</cell><cell>68.9</cell><cell>64.0</cell><cell>56.9</cell><cell>46.3</cell><cell>31.0</cell><cell>53.4</cell><cell>2101</cell><cell>34.1</cell></row><row><cell>TadTR* (Ours)</cell><cell>I3D</cell><cell></cell><cell></cell><cell>74.8</cell><cell>69.1</cell><cell>60.1</cell><cell>46.6</cell><cell>32.8</cell><cell>56.7</cell><cell>195</cell><cell>1.07</cell></row><row><cell>TadTR-lite* (Ours)</cell><cell>I3D</cell><cell></cell><cell></cell><cell>71.3</cell><cell>65.9</cell><cell>57.0</cell><cell>44.6</cell><cell>30.4</cell><cell>53.8</cell><cell>155</cell><cell>0.85</cell></row><row><cell>TadTR  ? (Ours)</cell><cell>I3D</cell><cell></cell><cell></cell><cell>70.3</cell><cell>64.3</cell><cell>55.7</cell><cell>44.0</cell><cell>30.0</cell><cell>52.9</cell><cell>195</cell><cell>1.07</cell></row><row><cell>TadTR (Ours)</cell><cell>I3D</cell><cell></cell><cell></cell><cell>67.1</cell><cell>61.1</cell><cell>52.0</cell><cell>39.9</cell><cell>26.2</cell><cell>49.3</cell><cell>195</cell><cell>1.07</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE II :</head><label>II</label><figDesc>Comparison of different methods on the validation set of HACS Segments. The results of SSN are from<ref type="bibr" target="#b20">[21]</ref>. * With focal loss.</figDesc><table><row><cell>Method</cell><cell cols="4">mAP0.5 mAP0.75 mAP0.95 mAP</cell><cell>Time/ms</cell><cell>GFLOPs</cell></row><row><cell>SSN [69]</cell><cell>28.82</cell><cell>18.80</cell><cell>5.32</cell><cell>18.97</cell><cell>-</cell><cell>-</cell></row><row><cell>G-TAD [2]</cell><cell>41.08</cell><cell>27.59</cell><cell>8.34</cell><cell>27.48</cell><cell>941</cell><cell>45.7</cell></row><row><cell>TadTR</cell><cell>45.16</cell><cell>30.70</cell><cell>11.78</cell><cell>30.83</cell><cell>19</cell><cell>0.1</cell></row><row><cell>TadTR*</cell><cell>47.14</cell><cell>32.11</cell><cell>10.94</cell><cell>32.09</cell><cell>19</cell><cell>0.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE III :</head><label>III</label><figDesc>Comparison of different methods on ActivityNet-1.3. Methods in the second group are combined with an ensemble of action classifiers<ref type="bibr" target="#b68">[69]</ref>. The computation costs (in FLOPs) of the action classifiers are not included. The results of BMN and G-TAD with TSP<ref type="bibr" target="#b70">[71]</ref> features are from<ref type="bibr" target="#b70">[71]</ref>. TS: two-stream. SN: single-network. * With focal loss.</figDesc><table><row><cell>Method</cell><cell>Feature</cell><cell>SN</cell><cell>E2E</cell><cell>mAP 0.5</cell><cell>mAP 0.75</cell><cell>mAP 0.95</cell><cell>mAP</cell><cell>GFLOPs</cell></row><row><cell>Self-contained methods</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>R-C3D [6]</cell><cell>C3D</cell><cell></cell><cell>-</cell><cell>26.80</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>SSN [10]</cell><cell>TS</cell><cell>-</cell><cell>-</cell><cell>39.12</cell><cell>23.48</cell><cell>5.49</cell><cell>23.98</cell><cell>-</cell></row><row><cell>TAL-Net [7]</cell><cell>I3D</cell><cell></cell><cell>-</cell><cell>38.23</cell><cell>18.30</cell><cell>1.30</cell><cell>20.22</cell><cell>-</cell></row><row><cell>P-GCN [13]</cell><cell>I3D</cell><cell>-</cell><cell>-</cell><cell>42.90</cell><cell>28.14</cell><cell>2.47</cell><cell>26.99</cell><cell>5.0</cell></row><row><cell>PCG-TAL [37]</cell><cell>I3D</cell><cell>-</cell><cell>-</cell><cell>42.14</cell><cell>28.34</cell><cell>6.12</cell><cell>27.34</cell><cell>-</cell></row><row><cell>TadTR (Ours)</cell><cell>TS</cell><cell></cell><cell></cell><cell>41.40</cell><cell>28.85</cell><cell>7.86</cell><cell>28.21</cell><cell>0.038</cell></row><row><cell>TadTR* (Ours)</cell><cell>TS</cell><cell></cell><cell></cell><cell>43.67</cell><cell>30.58</cell><cell>8.32</cell><cell>29.90</cell><cell>0.038</cell></row><row><cell cols="4">Combined with an ensemble of action classifiers [69]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>CDC [72]</cell><cell>C3D</cell><cell>-</cell><cell>-</cell><cell>43.83</cell><cell>25.88</cell><cell>0.21</cell><cell>22.77</cell><cell>-</cell></row><row><cell>BMN [11]</cell><cell>TS</cell><cell>-</cell><cell>-</cell><cell>50.07</cell><cell>34.78</cell><cell>8.29</cell><cell>33.85</cell><cell>45.6</cell></row><row><cell>G-TAD [2]</cell><cell>TS</cell><cell>-</cell><cell>-</cell><cell>50.36</cell><cell>34.60</cell><cell>9.02</cell><cell>34.09</cell><cell>45.7</cell></row><row><cell>P-GCN [13]</cell><cell>I3D</cell><cell>-</cell><cell>-</cell><cell>48.26</cell><cell>33.16</cell><cell>3.27</cell><cell>31.11</cell><cell>5.0</cell></row><row><cell>MR [70]</cell><cell>I3D</cell><cell>-</cell><cell>-</cell><cell>43.47</cell><cell>33.91</cell><cell>9.21</cell><cell>30.12</cell><cell>-</cell></row><row><cell>A2Net [36]</cell><cell>I3D</cell><cell>-</cell><cell>-</cell><cell>43.55</cell><cell>28.69</cell><cell>3.70</cell><cell>27.75</cell><cell>1.2</cell></row><row><cell>PCG-TAL [37]</cell><cell>I3D</cell><cell>-</cell><cell>-</cell><cell>50.24</cell><cell>35.21</cell><cell>7.84</cell><cell>34.01</cell><cell>-</cell></row><row><cell>RTD-Net [62]</cell><cell>I3D</cell><cell>-</cell><cell>-</cell><cell>47.21</cell><cell>30.68</cell><cell>8.61</cell><cell>30.83</cell><cell>3.1</cell></row><row><cell>AFSD [35]</cell><cell>I3D</cell><cell>-</cell><cell>-</cell><cell>52.38</cell><cell>35.27</cell><cell>6.47</cell><cell>34.39</cell><cell>3.3</cell></row><row><cell>TadTR* (Ours)</cell><cell>TS</cell><cell>-</cell><cell>-</cell><cell>51.29</cell><cell>34.99</cell><cell>9.49</cell><cell>34.64</cell><cell>0.038</cell></row><row><cell>TadTR+BMN (Ours)</cell><cell>TS</cell><cell>-</cell><cell>-</cell><cell>50.51</cell><cell>35.35</cell><cell>8.18</cell><cell>34.55</cell><cell>45.6</cell></row><row><cell>TadTR* (Ours)</cell><cell>I3D</cell><cell>-</cell><cell>-</cell><cell>52.83</cell><cell>37.05</cell><cell>10.83</cell><cell>36.11</cell><cell>0.038</cell></row><row><cell>BMN [11]</cell><cell>TSP</cell><cell>-</cell><cell>-</cell><cell>51.23</cell><cell>36.78</cell><cell>9.50</cell><cell>35.67</cell><cell>45.6</cell></row><row><cell>G-TAD [2]</cell><cell>TSP</cell><cell>-</cell><cell>-</cell><cell>51.26</cell><cell>37.12</cell><cell>9.29</cell><cell>35.81</cell><cell>45.7</cell></row><row><cell>TadTR* (Ours)</cell><cell>TSP</cell><cell>-</cell><cell>-</cell><cell>53.62</cell><cell>37.52</cell><cell>10.56</cell><cell>36.75</cell><cell>0.038</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE IV :</head><label>IV</label><figDesc>Comparison of different variants of TadTR.</figDesc><table><row><cell>Method</cell><cell cols="4">HACS Segments mAP 0.5 mAP 0.75 mAP 0.95 mAP</cell><cell>MFLOPs</cell><cell>THUMOS14 mAP</cell><cell>ActivityNet mAP</cell></row><row><cell>TadTR</cell><cell>45.16</cell><cell>30.70</cell><cell>11.78</cell><cell>30.83</cell><cell>100.5</cell><cell>47.92</cell><cell>28.21</cell></row><row><cell>TadTR w/o encoder</cell><cell>39.65</cell><cell>26.99</cell><cell>9.08</cell><cell>26.94</cell><cell>95.3</cell><cell>40.99</cell><cell>27.34</cell></row><row><cell>TadTR w/o instance-level context</cell><cell>43.11</cell><cell>29.97</cell><cell>10.43</cell><cell>29.70</cell><cell>66.8</cell><cell>45.26</cell><cell>26.23</cell></row><row><cell>TadTR with dense attention</cell><cell>22.76</cell><cell>12.52</cell><cell>4.19</cell><cell>13.58</cell><cell>564.5</cell><cell>24.15</cell><cell>23.79</cell></row><row><cell>TadTR w/o actionness regression</cell><cell>42.10</cell><cell>28.44</cell><cell>10.23</cell><cell>28.51</cell><cell>99.4</cell><cell>45.09</cell><cell>26.13</cell></row><row><cell>TadTR w/o segment refinement</cell><cell>39.89</cell><cell>28.03</cell><cell>9.54</cell><cell>27.65</cell><cell>99.8</cell><cell>44.07</cell><cell>27.40</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE V :</head><label>V</label><figDesc>Comparison of the variants of TadTR with Transformer encoder and 1D CNN encoder on HACS Segments.</figDesc><table><row><cell>Encoder</cell><cell cols="3">Average mAP w/o NMS w/ NMS</cell><cell>MFLOPs</cell></row><row><cell>Transformer</cell><cell>30.83</cell><cell>30.53</cell><cell></cell><cell>100.5</cell></row><row><cell>1D CNN</cell><cell>27.95</cell><cell>29.12</cell><cell></cell><cell>134.8</cell></row><row><cell></cell><cell cols="2">7DG75Z'&amp;11HQFRGHU</cell><cell></cell></row><row><cell></cell><cell>7DG75</cell><cell></cell><cell></cell></row><row><cell>$YHUDJHP$3</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>6KRUW</cell><cell>0HGLXP</cell><cell>/RQJ</cell></row><row><cell cols="5">Fig. 5: Comparison of the performance of TadTR with 1D</cell></row><row><cell cols="5">CNN encoder and TadTR (with Transformer encoder) for</cell></row><row><cell cols="5">actions with different durations on HACS Segments, measured</cell></row><row><cell>by average mAP.</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>0.22?0.46 Cleaning Windows Curling</head><label></label><figDesc>Dense attention v.s. temporal deformable attention. We try replacing all temporal deformable attention modules in TadTR</figDesc><table><row><cell></cell><cell></cell><cell>Ground Truth</cell><cell>Detections</cell><cell>Time</cell></row><row><cell>0.3s</cell><cell></cell><cell></cell><cell>52.0s</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Curling: 0.39?0.60</cell></row><row><cell></cell><cell>5.5s</cell><cell></cell><cell>54.8s</cell></row><row><cell>1.0s</cell><cell>Curling: 0.50?0.47</cell><cell>8.0s</cell></row><row><cell></cell><cell></cell><cell></cell><cell>76.6s</cell></row><row><cell></cell><cell></cell><cell>11.0s</cell></row><row><cell></cell><cell></cell><cell>13.7s</cell><cell>Cleaning Windows:</cell><cell>71.5s</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Cleaning Windows: 0.24?0.42</cell></row><row><cell></cell><cell></cell><cell>19.6s</cell><cell>49.0s</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>TABLE VI :</head><label>VI</label><figDesc>Ablation study of segment refinement (SR) on ActivityNet. The results are with focal loss.</figDesc><table><row><cell>Variants</cell><cell>0.5</cell><cell>0.75</cell><cell>0.95</cell><cell>mAP</cell></row><row><cell>Standard SR</cell><cell cols="4">43.67 30.58 8.32 29.90</cell></row><row><cell>SR w/o incremental refinement</cell><cell cols="4">42.72 29.00 6.92 28.59</cell></row><row><cell>SR w/o reference point adjustment</cell><cell cols="4">42.61 28.62 6.28 28.05</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>TABLE VII :</head><label>VII</label><figDesc>Effects of the numbers of encoder layers and decoder layers on HACS Segments.</figDesc><table><row><cell>L E</cell><cell>L D</cell><cell cols="4">mAP 0.5 mAP 0.75 mAP 0.95 mAP</cell><cell>MFLOPs</cell></row><row><cell>2</cell><cell>4</cell><cell>45.16</cell><cell>30.70</cell><cell>11.78</cell><cell>30.83</cell><cell>100.5</cell></row><row><cell>4</cell><cell>4</cell><cell>44.63</cell><cell>30.39</cell><cell>10.76</cell><cell>30.39</cell><cell>105.6</cell></row><row><cell>6</cell><cell>4</cell><cell>40.55</cell><cell>27.55</cell><cell>9.88</cell><cell>27.63</cell><cell>110.8</cell></row><row><cell>2</cell><cell>2</cell><cell>42.10</cell><cell>29.05</cell><cell>9.57</cell><cell>28.84</cell><cell>79.6</cell></row><row><cell>2</cell><cell>4</cell><cell>45.16</cell><cell>30.70</cell><cell>11.78</cell><cell>30.83</cell><cell>100.5</cell></row><row><cell>2</cell><cell>6</cell><cell>45.20</cell><cell>30.82</cell><cell>10.67</cell><cell>30.74</cell><cell>121.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>TABLE VIII :</head><label>VIII</label><figDesc>Effect of the number of action queries on different datasets. The average mAPs are reported.</figDesc><table><row><cell>#queries Nq</cell><cell>10</cell><cell>20</cell><cell>30</cell><cell>40</cell><cell>50</cell></row><row><cell>THUMOS14</cell><cell>44.06</cell><cell>46.48</cell><cell>46.94</cell><cell>47.92</cell><cell>46.78</cell></row><row><cell>HACS Segments</cell><cell>29.63</cell><cell>30.73</cell><cell>30.83</cell><cell>29.99</cell><cell>29.47</cell></row><row><cell>ActivityNet-1.3</cell><cell>28.21</cell><cell>26.47</cell><cell>26.27</cell><cell>26.29</cell><cell>22.96</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>TABLE IX :</head><label>IX</label><figDesc>Effect of the number of sampling points K on the performance and computation cost on HACS Segments. The models are trained for half of the full training cycle (15 epochs). #points K mAP 0.5 mAP 0.75 mAP 0.95</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>mAP</cell><cell>MFLOPs</cell></row><row><cell>1</cell><cell>39.22</cell><cell>27.07</cell><cell>9.80</cell><cell>27.03</cell><cell>99.1</cell></row><row><cell>2</cell><cell>40.22</cell><cell>27.62</cell><cell>10.15</cell><cell>27.61</cell><cell>99.6</cell></row><row><cell>4</cell><cell>41.20</cell><cell>28.52</cell><cell>10.63</cell><cell>28.49</cell><cell>100.5</cell></row><row><cell>8</cell><cell>39.77</cell><cell>27.15</cell><cell>9.86</cell><cell>27.20</cell><cell>102.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>TABLE X :</head><label>X</label><figDesc>Effect of the number of attention heads M on the performance and computation cost on HACS Segments. The models are trained for half of the full training cycle (15 epochs).</figDesc><table><row><cell>#heads M</cell><cell cols="3">mAP 0.5 mAP 0.75 mAP 0.95</cell><cell>mAP</cell><cell>MFLOPs</cell></row><row><cell>1</cell><cell>38.62</cell><cell>25.25</cell><cell>7.94</cell><cell>25.57</cell><cell>100.2</cell></row><row><cell>2</cell><cell>40.30</cell><cell>26.37</cell><cell>8.09</cell><cell>26.52</cell><cell>100.2</cell></row><row><cell>4</cell><cell>40.74</cell><cell>28.41</cell><cell>10.51</cell><cell>28.30</cell><cell>100.3</cell></row><row><cell>8</cell><cell>41.20</cell><cell>28.52</cell><cell>10.63</cell><cell>28.49</cell><cell>100.5</cell></row><row><cell>16</cell><cell>41.07</cell><cell>28.35</cell><cell>10.23</cell><cell>28.31</cell><cell>100.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head></head><label></label><figDesc>Table Xcompares the performance and computation cost using different number of attention heads M on HACS Segments. It is observed that moderately increasing M boosts the performance, as more diverse features can be learned. The performance is saturated at M = 8. Similar to the number of sampling points, the number of attention heads has little impact on the computation cost.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>TABLE XI :</head><label>XI</label><figDesc>Effect of the expanding factor and the number of bins T R for actionness regression on HACS Segments. Visualization of temporal deformable attention. The first row is uniformly sampled video frames. The second row visualizes the attention at two randomly picked reference points in the last encoder layer. The third row visualizes the attention for the predicted action in the last decoder layer. We use different markers to represent sampling points in different heads and separate the points from different heads vertically. The color of a point indicates the attention weight. Best viewed in color. More examples are given in the supplementary material.</figDesc><table><row><cell cols="3">Encoder reference point</cell><cell cols="4">Encoder sampling points</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">GT: HighJump</cell><cell cols="2">Pred: HighJump</cell><cell></cell><cell cols="4">Decoder reference point</cell><cell></cell><cell cols="4">Decoder sampling points</cell><cell>high</cell></row><row><cell></cell><cell></cell><cell>504</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>506</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>t/sec</cell><cell>508</cell><cell>510</cell><cell>512</cell><cell>low</cell></row><row><cell>CricketBowling Fig. 7: 260 0 #24 JavelinThrow</cell><cell>5</cell><cell>265 CricketShot 10 #20 #4 Non-action</cell><cell>270</cell><cell>15</cell><cell>t/sec t/sec</cell><cell>275 #4</cell><cell>20</cell><cell>280 #36 #12</cell><cell>25</cell><cell>285 #37</cell><cell>30 #28</cell><cell>#7 #38</cell><cell>290</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>1</cell><cell>1.25</cell><cell>1.5</cell><cell>2</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>mAP</cell><cell>30.27 30.40 30.83</cell><cell>30.01</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>T R</cell><cell>8</cell><cell>16</cell><cell>32</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>mAP</cell><cell>30.39</cell><cell>30.83 30.26</cell><cell>-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head></head><label></label><figDesc>visualizes temporal deformable attention of the last encoder layer and the last decoder layer. We observe that: (1) Different attention heads focus on different temporal regions and scales. For example, the sampling points marked with left-triangle and up-triangle are distributed on the left side of the reference point. In some heads, the sampling points that are farthest away from the reference point have relatively higher attention weights, to capture useful cues for action boundaries. (2) The encoder and the decoder have different preferences for context. The</figDesc><table><row><cell>?</cell></row><row><cell>Length</cell></row><row><cell>Center ?</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Temporal action localization in untrimmed videos via multi-stage cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1049" to="1058" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">G-TAD: Sub-graph localization for temporal action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Rojas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Thabet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10" to="156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning activity progression in lstms for activity detection and early detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sigal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sclaroff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016-06" />
			<biblScope unit="page" from="1942" to="1950" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Temporal action detection using a statistical language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3131" to="3140" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Fast temporal activity proposals for efficient detection of human actions in untrimmed videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Caba Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Carlos</forename><surname>Niebles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1914" to="1923" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">R-c3d: region convolutional 3d network for temporal activity detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5794" to="5803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Rethinking the faster r-cnn architecture for temporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-W</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Seybold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in CVPR</title>
		<imprint>
			<biblScope unit="page" from="1130" to="1139" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Single shot temporal action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="988" to="996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Temporal action localization by structured maximal sums</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-H</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Stroud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3684" to="3692" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Temporal action detection with structured segment networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<biblScope unit="page" from="2914" to="2923" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Bmn: Boundary-matching network for temporal action proposal generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3889" to="3898" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Temporal convolutional networks for action segmentation and detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Flynn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vidal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Reiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">D</forename><surname>Hager</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="156" to="165" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Graph convolutional networks for temporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7094" to="7103" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4724" to="4733" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">End-to-end object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="213" to="229" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Temporal context network for activity localization in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">Q</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5727" to="5736" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Diagnosing error in temporal action detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Alwassel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ECCV</title>
		<imprint>
			<biblScope unit="page" from="256" to="272" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Deformable detr: Deformable transformers for end-to-end object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2961" to="2969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">HACS: human action clips and segments dataset for recognition and temporal localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8667" to="8677" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">The THUMOS challenge on action recognition for videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Idrees</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-G</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gorban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>in the wild&quot;,&quot; pp. 1-23</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">ActivityNet: A large-scale video benchmark for human activity understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Caba Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Carlos</forename><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="961" to="970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Temporal action localization with pyramid of score distribution features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Kassim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3093" to="3102" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Scc: Semantic context cascade for efficient action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">C</forename><surname>Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Barrios</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3175" to="3184" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Self-similarity action proposal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="2064" to="2068" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Multi-shot temporal event localization: A benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021-06" />
			<biblScope unit="page" from="12" to="596" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Turn tap: Temporal unit regression network for temporal action proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3648" to="3656" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Bsn: Boundary sensitive network for temporal action proposal generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018-09" />
			<biblScope unit="page" from="3" to="21" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Ctap: Complementary temporal action proposal generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018-09" />
			<biblScope unit="page" from="70" to="85" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Multi-granularity generator for temporal action proposal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3604" to="3613" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Daps: Deep action proposals for action understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">C</forename><surname>Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Niebles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="768" to="784" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Sst: Single-stream temporal action proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Buch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6373" to="6382" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Gaussian temporal awareness networks for action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="344" to="353" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Learning salient boundary feature for anchor-free temporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CVPR</publisher>
			<biblScope unit="page" from="3320" to="3329" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Revisiting anchor mechanisms for temporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="8535" to="8548" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Pcg-tal: Progressive crossgranularity cooperation for temporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="2103" to="2113" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">End-to-end learning of action detection from frame glimpses in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2678" to="2687" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Sf-net: Single-frame supervision for temporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Feiszli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="420" to="437" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Weakly supervised action localization by sparse temporal pooling network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Prasad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6752" to="6761" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">W-talc: Weakly-supervised temporal activity localization and classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Roy-Chowdhury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018-09" />
			<biblScope unit="page" from="588" to="607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Completeness modeling and context separation for weakly supervised temporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1298" to="1307" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Autoloc: Weakly-supervised temporal action localization in untrimmed videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Miyazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ECCV</title>
		<imprint>
			<biblScope unit="page" from="154" to="171" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Temporal structure mining for weakly supervised action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5522" to="5531" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Modeling sub-actions for weakly supervised temporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="5154" to="5167" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Multiscale structure-aware network for weakly supervised temporal action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="5848" to="5861" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Breaking winner-takes-all: Iterative-winners-out networks for weakly supervised temporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="5797" to="5808" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Relational prototypical network for weakly supervised temporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page">60</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">A hybrid attention mechanism for weakly-supervised temporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Islam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Radke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="1637" to="1645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Two-stream consensus network for weakly-supervised temporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="37" to="54" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Foreground-action consistency network for weakly supervised temporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="8002" to="8011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Planetr: Structure-guided transformers for 3d plane recovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-S</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="4186" to="4195" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Transmix: Attend to mix for vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-N</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="page" from="12" to="135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Transcrowd: weakly-supervised crowd counting with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science China Information Sciences</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Videobert: A joint model for video and language representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Myers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7464" to="7473" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Is space-time attention all you need for video understanding?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bertasius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">in ICML</title>
		<imprint>
			<date type="published" when="2021-07" />
			<biblScope unit="page" from="813" to="824" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Seqformer: a frustratingly simple model for video instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.08275</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Actbert: Learning global-local video-text representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="8746" to="8755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">End-to-end dense video captioning with masked transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Corso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in CVPR</title>
		<imprint>
			<biblScope unit="page" from="8739" to="8748" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Video action transformer network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girdhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="244" to="253" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Activity graph transformer for temporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nawhal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.08540</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Relaxed transformer decoders for direct action proposal generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021-10" />
			<biblScope unit="page" from="13" to="526" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Temporal action proposal generation with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.12043</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Boundary content graph neural network for temporal action proposal generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="121" to="137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">Layer normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Temporal segment networks: Towards good practices for deep action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="20" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.08011</idno>
		<title level="m">CUHK &amp; ETHZ &amp; SIAT submission to ActivityNet challenge 2017</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="20" to="24" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Bottom-up temporal action localization with mutual regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Tsp: Temporally-sensitive pretraining of video encoders for localization tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Alwassel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Giancola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV Workshops, 2021</title>
		<imprint>
			<biblScope unit="page" from="3166" to="3176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<title level="m" type="main">Cdc: Convolutional-de-convolutional networks for precise temporal action localization in untrimmed videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zareian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Miyazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1417" to="1426" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">An empirical study of end-to-end temporal action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2022-06" />
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="10" to="20" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
