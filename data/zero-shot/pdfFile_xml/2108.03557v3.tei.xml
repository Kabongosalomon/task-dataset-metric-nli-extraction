<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Context-Aware Mixup for Domain Adaptive Semantic Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qianyu</forename><surname>Zhou</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengyang</forename><surname>Feng</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiqi</forename><surname>Gu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangmiao</forename><surname>Pang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangliang</forename><surname>Cheng</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuequan</forename><surname>Lu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lizhuang</forename><surname>Ma</surname></persName>
						</author>
						<title level="a" type="main">Context-Aware Mixup for Domain Adaptive Semantic Segmentation</title>
					</analytic>
					<monogr>
						<title level="j" type="main">IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS FOR VIDEO TECHNOLOGY</title>
						<meeting> <address><addrLine>X, NO. X, X 1</addrLine></address>
						</meeting>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T19:07+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Domain Adaptation</term>
					<term>Semantic Segmentation</term>
					<term>Domain Mixup</term>
					<term>Autonomous Driving</term>
					<term>Scene Understanding</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Unsupervised domain adaptation (UDA) aims to adapt a model of the labeled source domain to an unlabeled target domain. Existing UDA-based semantic segmentation approaches always reduce the domain shifts in pixel level, feature level, and output level. However, almost all of them largely neglect the contextual dependency, which is generally shared across different domains, leading to less-desired performance. In this paper, we propose a novel Context-Aware Mixup (CAMix) framework for domain adaptive semantic segmentation, which exploits this important clue of context-dependency as explicit prior knowledge in a fully end-to-end trainable manner for enhancing the adaptability toward the target domain. Firstly, we present a contextual mask generation strategy by leveraging the accumulated spatial distributions and prior contextual relationships. The generated contextual mask is critical in this work and will guide the context-aware domain mixup on three different levels. Besides, provided the context knowledge, we introduce a significancereweighted consistency loss to penalize the inconsistency between the mixed student prediction and the mixed teacher prediction, which alleviates the negative transfer of the adaptation, e.g., early performance degradation. Extensive experiments and analysis demonstrate the effectiveness of our method against the stateof-the-art approaches on widely-used UDA benchmarks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(a) source domain (b) target domain</head> <ref type="figure">Fig. 1</ref><p>: Previous domain adaptive semantic segmentation methods largely neglect the shared context-dependency across different domains when adapting from the source domain to the target domain, and could result in less-desired performance and severe negative transfer. We observe that exploiting contexts as explicit prior knowledge is essential for enhancing the adaptability toward the target domain during the adaptation.</p><p>time-consuming due to the process of annotating pixel-wise labels <ref type="bibr" target="#b10">[11]</ref>. A natural idea to overcome this bottleneck is using synthetic data <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref> to supervise the segmentation model instead of real data. However, the existing domain gap between the synthetic images <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref> and real images <ref type="bibr" target="#b10">[11]</ref> often leads to a significant performance drop when the learned source models are directly applied to the unlabelled target data. To address this issue, various unsupervised domain adaptation (UDA) techniques for semantic segmentation have been proposed to reduce the domain gap in pixel level <ref type="bibr" target="#b14">[15]</ref>- <ref type="bibr" target="#b21">[22]</ref>, feature level <ref type="bibr" target="#b22">[23]</ref>- <ref type="bibr" target="#b28">[29]</ref> and output level <ref type="bibr" target="#b29">[30]</ref>- <ref type="bibr" target="#b34">[35]</ref>. Among them, the most common practices are based on adversarial learning <ref type="bibr" target="#b29">[30]</ref>- <ref type="bibr" target="#b33">[34]</ref>, self-training <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b35">[36]</ref>- <ref type="bibr" target="#b37">[38]</ref>, consistency regularization <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b38">[39]</ref>- <ref type="bibr" target="#b40">[41]</ref>, and entropy minimization <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b41">[42]</ref>, <ref type="bibr" target="#b42">[43]</ref>. These works mainly focused on utilizing common knowledge, e.g., appearances, scales, textures, weather, etc., to narrow down the domain gap.</p><p>Nevertheless, context-dependency across different domains has been very sparsely exploited so far in UDA, and how to transfer such cross-domain context still remains underexplored. As shown in <ref type="figure">Fig. 1</ref>, we observe that the source and target images usually share similar semantic contexts, e.g., a rider is over the bicycle or motorcycle, the sidewalk is beside the road, and such context knowledge is crucial, particularly when adapting from the source domain to the target domain. The lack of context will lead to less-desired performance during the adaptation and even severe negative transfer, e.g., early performance degradation during the adaptation process. Previous works <ref type="bibr" target="#b40">[41]</ref>, <ref type="bibr" target="#b43">[44]</ref>, <ref type="bibr" target="#b44">[45]</ref> neglect the context dependency in the domain mixup, and we observe that images synthesized by these methods often violate the contextual relationships between objects. For instance, only un-occluded parts of midrange objects are copied onto the irrelevant classes of other images. Imagine how strange it is to see a pedestrian standing on top of a car or to see the sky through a hole in a building. Thus, the lack of such context information results in category confusion and label contamination in the mixed results (e.g., <ref type="figure">Fig. 5</ref>). Besides, the state-of-the-art approaches of domain adaptive semantic segmentation heavily depend on adversarial learning, image-to-image translation, or self-training, and most of them need to fine-tune or re-train the models in many offline stages, which are quite complex and hard to converge and cannot be trained in an end-to-end manner.</p><p>Motivated by the above facts, we propose a novel perspective of domain adaptive semantic segmentation that identifies context-dependency across domains as explicit prior domain knowledge when adapting from the source domain to the target domain. As such, we present a context-aware domain mixup (CAMix) framework to explicitly explore and transfer crossdomain contexts for domain adaptation. Our whole framework is fully end-to-end trainable and easy to implement.</p><p>The proposed CAMix framework consists of two key components: contextual mask generation (CMG) and significancereweighted consistency loss (SRC). Specifically, CMG firstly generates a contextual mask by selectively leveraging the accumulated spatial distribution of the source domain and the contextual relationship of the target domain. This mask is critical in our work and will act as prior knowledge to guide the context-aware domain mixup on three different levels, i.e., input level, output level, and significance mask level. Notice that the significance mask is a mask that we define to indicate where the pixels are credible. This contextual mask respectively mixes the input images, the labels, and the corresponding significance-masks to narrow down the domain gap. In addition, we introduce an SRC loss on the significance mask level to alleviate the negative transfer, e.g., early performance degradation, during the adaptation process. In particular, we calculate a significance mask with the help of the target predictive entropy and its dynamic threshold. Then, we mix the target and the source significance masks using the context knowledge and utilize the mixed significance mask to reweigh the consistency loss. Extensive experiments with analysis demonstrate that CAMix achieves superior performance against the state-of-the-art methods, as shown in <ref type="figure">Fig. 2</ref>.</p><p>Our contributions are summarized as follows.</p><p>? From a new perspective, we propose a novel contextaware mixup (CAMix) framework for domain adaptive semantic sefmentation, which exploits context dependency across domains as explicit prior domain knowledge for further improving the adaptability towards the target domain.</p><p>? We present a contextual mask generation strategy, which leverages the spatial distribution of the source domain and the contextual relationship of the target domain for guiding the context-aware domain mixup on three different levels. Besides, <ref type="figure">Fig. 2</ref>: Progress of UDA for semantic segmentation on GTAV <ref type="bibr" target="#b12">[13]</ref> ? Cityscapes <ref type="bibr" target="#b10">[11]</ref>. Our CAMix shows superior performance compared to the state-of-the-art methods.</p><p>we introduce a significance-reweighted consistency loss, which alleviates the adverse impacts of the adaptation procedure, e.g., early performance degradation, under the guidance of context.</p><p>? Extensive experiments with analysis demonstrate the effectiveness of our method on two challenging UDA benchmarks. Our CAMix can be easily plugged into existing UDA frameworks, e.g., DACS <ref type="bibr" target="#b40">[41]</ref> and DAFormer <ref type="bibr" target="#b45">[46]</ref>, and achieve consistent improvements over the state-of-the-art methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>Unsupervised domain adaptive semantic segmentation. Unsupervised domain adaptation (UDA) aims to bridge the domain shifts between the labeled source domain and the unlabeled target domain. This problem has been well-studied in image recognition <ref type="bibr" target="#b46">[47]</ref>- <ref type="bibr" target="#b54">[55]</ref>. However, these methods only work on simple and small classification datasets, and may have very limited performance in more challenging and higher-structured tasks, e.g., semantic segmentation. Thus, researching unsupervised domain adaptive semantic segmentation is necessary and significant. Many recent methods can be mainly divided into three categories: namely, the input-level adaptation <ref type="bibr" target="#b14">[15]</ref>- <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b55">[56]</ref>, feature-level adaptation <ref type="bibr" target="#b22">[23]</ref>- <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b56">[57]</ref>, <ref type="bibr" target="#b57">[58]</ref>, and output-level adaptation <ref type="bibr" target="#b29">[30]</ref>- <ref type="bibr" target="#b34">[35]</ref>. However, almost all of them largely overlook the shared context-dependency across domains, leading to less-desired performance. Instead, our method explicitly exploits context dependency across domains as prior domain knowledge for enhancing the adaptability toward the target domain. Besides, most recent methods <ref type="bibr" target="#b15">[16]</ref>- <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b31">[32]</ref> involve many sophisticated sub-components, e.g., computationally-expensive adversarial learning <ref type="bibr" target="#b29">[30]</ref>- <ref type="bibr" target="#b33">[34]</ref>, offline self-training <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b35">[36]</ref>- <ref type="bibr" target="#b37">[38]</ref> and image translation models <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b21">[22]</ref>, which are complex and hard to converge, and cannot be trained in an end-to-end manner. In contrast, our method is fully end-to-end trainable and can be easily plugged into existing UDA frameworks, e.g., DACS <ref type="bibr" target="#b40">[41]</ref> and DAFormer <ref type="bibr" target="#b45">[46]</ref>. Domain mixup: Mixup has been well-studied in other communities to improve the robustness of models., e.g., semi-  <ref type="figure">Fig. 3</ref>: Overview of the proposed context-aware mixup (CAMix) architecture. Firstly, we generate a contextual mask (CMG) by leveraging the spatial distribution of the source domain and the contextual relationship of the target domain. Guided by this mask M , we perform context-aware mixup (CAMix) in three levels, i.e., input level, output level and significance mask level. Provided the context knowledge, we design a significance re-weighted consistency (SRC) loss to ease the over-alignment between the mixed student and teacher prediction and alleviate the negative transfer during the adaptation.</p><p>supervised learning <ref type="bibr" target="#b43">[44]</ref>, <ref type="bibr" target="#b44">[45]</ref>, and point cloud classification <ref type="bibr" target="#b58">[59]</ref>, <ref type="bibr" target="#b59">[60]</ref>. A few works <ref type="bibr" target="#b60">[61]</ref>- <ref type="bibr" target="#b62">[63]</ref> studied cross-domain mixup in UDA. Nevertheless, these methods work well on simple and small classification datasets (e.g. MNIST <ref type="bibr" target="#b63">[64]</ref> and SVHN <ref type="bibr" target="#b64">[65]</ref>), but can hardly be applied to more challenging tasks, e.g., domain adaptive semantic segmentation. DACS <ref type="bibr" target="#b40">[41]</ref> was designed for segmentation and proposed to mix the source samples with the target ones via ClassMix <ref type="bibr" target="#b65">[66]</ref>. Besides, BAPA-Net <ref type="bibr" target="#b28">[29]</ref> considered the object boundaries to weigh each pixel for promoting CutMix <ref type="bibr" target="#b43">[44]</ref>, while little attention has been paid to exploiting explicit contextual dependency as prior knowledge to mitigate the domain gaps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Consistency regularization:</head><p>The key idea of consistency regularization is that the target prediction of the student model and that of the teacher model should be invariant under different perturbations. The teacher model is an exponential moving average (EMA) of the student model, and then the teacher model could transfer the learned knowledge to the student. Consistency regularization typically appears in Semisupervised Learning (SSL) <ref type="bibr" target="#b66">[67]</ref> and is recently applied to UDA recently <ref type="bibr" target="#b38">[39]</ref>- <ref type="bibr" target="#b40">[41]</ref>, <ref type="bibr" target="#b65">[66]</ref>, <ref type="bibr" target="#b67">[68]</ref>- <ref type="bibr" target="#b69">[70]</ref>. For simplicity, we choose <ref type="bibr" target="#b66">[67]</ref> as a base framework to realize end-to-end learning. Recent methods <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b40">[41]</ref>, <ref type="bibr" target="#b65">[66]</ref>, <ref type="bibr" target="#b69">[70]</ref> reveal that Cross-Entropy (CE) loss is more suitable than Mean Square Error (MSE) loss and Kullback-Leibler (KL) loss for the segmentation task. Thus, we design our SRC loss as a variant of CE loss. Uncertainty estimation: The idea of exploiting prediction uncertainty has been utilized in domain adaptation for classification, e.g., Bayesian classifier <ref type="bibr" target="#b71">[71]</ref> and Bayesian discriminator <ref type="bibr" target="#b72">[72]</ref>. These methods always require an extra discriminator in adversarial training, and can work well on simple and small classification datasets. Our method differs from these methods in several aspects. At first, we tackle the more challenging ... <ref type="figure">Fig. 4</ref>: Hierarchical prior contexts given in the Cityscapes <ref type="bibr" target="#b10">[11]</ref> dataset and are shared in all cross-domain scenarios. The top row shows the hierarchy of semantic categories where multiple fine categories may belong to one coarse category. The bottom row is a street-scene image (left), together with its coarse annotation (middle) and fine annotation (right).</p><p>task of semantic segmentation rather than image classification, where the uncertainty of dense pixel-wise predictions instead of image-wise prediction needs to be decreased. Secondly, we avoid using adversarial adaptation in uncertainty estimation which tends to be unstable and inaccurate. Thirdly, in comparison with the aforementioned approaches, we design significance mask level domain mixup between the target significance mask and the source mask, which enables a more informative entropy-guided mask during the domain mixup.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. METHODOLOGY A. Overview and Notations</head><p>Following the UDA protocols <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b41">[42]</ref>, we have access to the source images X S ? S with their corresponding labels Y S . For the target domain T , only unlabeled images  <ref type="figure">Fig. 3</ref> shows the overview of our proposed architecture. Firstly, we present a contextual mask generation (CMG) strategy for mining the prior spatial distribution of the source domain and the contextual relationships of the target domain, thus generating a contextual mask M . Guided by this mask M , we perform an efficient CAMix on three levels, i.e., input level, output level, and significance mask level (a mask that we define to indicate where the pixels are credible). In particular, the weights of the teacher model F ? are an exponential moving average (EMA) of the ones of the student model F ? . In other words, the proposed CAMix uses the labeled source samples (X S , Y S ) and unlabeled target samples X T to synthesize the mixed images X M , the mixed pseudo labels Y M (Section III-C), and the corresponding mixed significance masks U M (Section III-D). We introduce a significancereweighted consistency loss (SRC) on the significance mask (SigMask) level to alleviate the negative transfer and overalignment during the online adaptation procedure.</p><p>As for other notations, ? t is the model's weight at the t-th iteration, Q denotes the spatial prior tensor defined in Section III-B and (h, w) means the pixel at h in height and w in width. H and W are the height and width of the image. L denotes the number of stochastic forward passes, ? is the predictive entropy, and R is the dynamic threshold defined in Section III-D. P l is the predicted class scores of the perturbed sample X l T at the l-th stochastic forward pass, andP is the mean of the predictive probability P l of different forward passes. P S indicates the predicted scores of the source sample.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1: Contextual Mask Generation Algorithm</head><p>Input: teacher model F ? , target image X T , spatial matrix Q, a meta-class list m.</p><formula xml:id="formula_0">Output: contextual mask M for CAMix. 1F ? ? Q F ? (X T ); 2? T ? arg max c f ? (h, w, c ); 3 C ? Set of the classes present in? T ; 4 c ? Randomly select |C|/2 classes in C; 5 for each k ? c do 6</formula><p>if k ? c and k ? m then 7k ? the semantic-related classes of k; <ref type="bibr" target="#b7">8</ref> ifk ? C then 9 c.append(k); </p><formula xml:id="formula_1">M (h, w) = 1, if? T (h, w) ? c 0, otherwise 15 end 16 return M ;</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Contextual Mask Generation</head><p>Intuitively, the source and the target domain share similar context dependency between domains. With this in mind, we identify two kinds of semantic contexts as explicit prior domain knowledge for guiding the domain adaptation procedure. The former is prior spatial contexts of the source domain, shown in <ref type="figure">Fig. 3</ref>, and the latter is contextual relationships of the categories in the target domain, shown in <ref type="figure">Fig. 4</ref>.</p><p>Regarding that the scenes often have their intrinsic spatial structures, e.g., the sky tends to appear on the top of the image while roads are more likely to appear on the bottom, it is intuitive to explore the spatial relationships of the source domain. Thus, we generate a spatial prior tensor Q with the shape of C * H * W by counting the class frequencies in the source domain. Each spatial location of Q is a class distribution, and we treat it as prior knowledge to regularize the target prediction:</p><formula xml:id="formula_2">F ? ? Q F ? (X T ), where f ? (T )</formula><p>is the target prediction of the teacher model.</p><p>As shown in <ref type="figure">Fig. 4</ref>, the object categories of an outdoor scene can be organized by a semantic hierarchy. Cityscapes <ref type="bibr" target="#b10">[11]</ref> dataset gives both the coarse annotation and fine annotation, where multiple fine categories may belong to one coarse category. Strictly following the common UDA protocols that the target training set is unlabeled and only the validation set of the target domain has labels, we use the hierarchical contextual relationship, i.e., the name file of the categories, in the target domain to define the meta class groups of CAMix, as shown in <ref type="table" target="#tab_1">Table I</ref>, for enhancing the adaptability towards the target domain. To better exploit such contextual relationship for adaptation, e.g., the traffic sign should be beside the pole, our core idea is to find the semantic-related categories of the current class presented in the image during the domain mixup. In other words, these classes that have contextual relationships to each other can be treated as a meta-class, and then we copy them together from the target images and paste them onto the source images. Our strategy prevents certain semantic categories hanging on an inappropriate context.</p><p>Specifically, we first get the spatially-modulated pseudo label:? T ? arg max c F ? (h, w, c ). Next, we randomly select half of the classes present in the prediction? T , namely c. After that, we judge whether each category k ? c presented in? T is in the meta-class list m or not. As shown in <ref type="table" target="#tab_1">Table I</ref>, the meta-class list m involves several groups of meta-classes, e.g., pole, traffic sign, traffic light are in one group, namely "object", and bicycle, motorcycle, and rider, are in another group, namely "human-vehicle", etc. This list is chosen from the prior knowledge of the hierarchical contexts given in the target domain, i.e., Cityscapes <ref type="bibr" target="#b10">[11]</ref>, and is shared in all experiments. We empirically set m as the combination of group I and group II in all experiments since relatively little context knowledge is not enough to provide sufficient supervisions for the adaptation, while too much prior knowledge will easily push the learning falling into local optima. More experimental analysis on meta class lists m could be referred to Section IV-E. If k ? c, we append the semantic-related classesk of current class k to the current list c.</p><p>A binary contextual mask M is then generated by setting the pixels from the final class list c to value 1 in M , and all others to value 0, which can be formulated as follows:</p><formula xml:id="formula_3">M (h, w) = 1, if? T (h, w) ? c 0, otherwise<label>(1)</label></formula><p>where h ? H, w ? W , and H and W are the height and width of the image. We iterate each spatial location (h, w) to generate the mask M . This mask M is then utilized as prior knowledge to mix the images in the input level, the labels in output level (Section III-C), and the significance mask on the significance mask level (Section III-D) between the source domain and the target domain. The whole algorithm of contextual mask generation is described in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Input-level and Output-level Domain Mixup</head><p>In the input level, the image X S and X T sampled from the source domain and target domain are synthesized into X M :</p><formula xml:id="formula_4">X M = M X T + (1 ? M ) X S ,<label>(2)</label></formula><p>where denotes element-wise multiplication. The weights ? t of the teacher model at training step t are updated by the student's weights ? t with a smoothing coefficient ? ? [0, 1], which can be formulated as follows:</p><formula xml:id="formula_5">? t = ? ? ? t?1 + (1 ? ?) ? ? t ,<label>(3)</label></formula><p>where ? is the EMA decay that controls the updating rate.</p><p>Regarding the output level, the source label Y S and the target pseudo label? T = F ? (X T ) are mixed into Y M :</p><formula xml:id="formula_6">Y M = M ? T + (1 ? M ) Y S .<label>(4)</label></formula><p>Different from <ref type="bibr" target="#b40">[41]</ref>, <ref type="bibr" target="#b45">[46]</ref>, <ref type="bibr" target="#b65">[66]</ref>, we mix the images and the corresponding labels in a target-to-source direction rather than the source-to-target direction. In other words, we copy some categories from the target domain and paste them onto the source domain, where we can add our consideration of both spatial relationships and contextual relationships in such a direction. Considering that the target predictions are uncertain without sufficient supervision, these two kinds of context dependencies are more suitable to refine the domain mixup.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Significance-mask Level Domain Mixup</head><p>In the significance-mask (SigMask) level domain mixup, we aim to decrease the high uncertainties of the pixel-wise mixed teacher prediction with the guidance of contextual mask M as additional supervisory signals. As a result, we can alleviate the adverse impact, e.g., training instability and early performance degradation, and transfer more reasonable knowledge from the teacher model to the student model. Stochastic forward passes. In particular, following prior works <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b73">[73]</ref>, we repeat each target image X T for L copies and inject a random Gaussian noise for each copy. Then, for each stochastic forward pass l of the perturbed target sample X l T , we get a set of predicted class scores {P (h,w,c) l } at the pixel (h, w) of the c-th class. Next, we calculate the mean of the predictive probability in L forward passes:</p><formula xml:id="formula_7">P (h,w,c) = 1 L L l=1 P (h,w,c) l (X l T ).<label>(5)</label></formula><p>Note that we do not use any dropout layers during stochastic forward passes. The predictive entropy ? is calculated as:</p><formula xml:id="formula_8">? (h,w) = ? C c=1P (h,w,c) ? log(P (h,w,c) ),<label>(6)</label></formula><p>where all volumes of pixel-wise entropy ? (h,w) form a set: K = {?} N j=1 , and N is the number of pixels in one sample. Dynamic threshold. Inspired by the ramp-up function of consistency weight <ref type="bibr" target="#b38">[39]</ref>, a dynamic threshold R is then determined by the predictive entropy rather than the softmax probabilities, which is for filtering out the unreliable pixel-wise mixed teacher predictions. It increases with a lower speed in the early training and a higher speed in the later training:</p><formula xml:id="formula_9">R = ? + (1 ? ?) ? e ?(1?t/tmax) 2 ? K sup ,<label>(7)</label></formula><p>where t denotes the current training step and t max is the maximum training step. K sup means the upper-bound of the volumes' self-information, which is denoted as: K sup = sup{?} N j=1 . ? is the initial state of the dynamic threshold R, and ? controls the exponential speed of the dynamic threshold. Significance mask. To filter out the unreliable pixel-wise prediction of the mixed teacher predictions, we denote the SigMask U T = I(? &lt; R) with the help of target predictive entropy ? and its dynamic threshold R, where I is an indicator function. Only those high-confident pixels where the predictive entropy is lower than the dynamic threshold will remain.</p><p>Given the contextual mask M as additional supervisory signals to promote the domain mixup, we perform SigMask level domain mixup. The significance mask of the source domain U S and the target domain U T are mixed into U M : Compute ? ? L total by back-propagation; <ref type="bibr" target="#b12">13</ref> Perform stochastic gradient descent on ?; 14 end</p><formula xml:id="formula_10">U M = M U T + (1 ? M ) U S ,<label>(8)</label></formula><formula xml:id="formula_11">Algorithm 2: Context Aware Mixup Algorithm Input: student model F ? , teacher model F ? , source domain D S , target domain D T , total iterations N . Output: teacher model F ? . 1 Initialize network parameters ? randomly. ; 2 for i=1 to N do 3 X S , Y S ? D S ; 4 X T ? D T ; 5? T ? f ? (X T ); 6 X M ? Input-level mixup by Eq.(2); 7? S ? F ? (X S ) ,? M ? F ? (X M ); 8 Y M ? Output-level</formula><p>where U S is a tensor full of 1, because the source labels are provided without uncertainties. And these certain areas do not need to reweigh the consistency loss. Only the uncertain areas in the target U T which is below the dynamic threshold R, are set to 0 to reweigh the consistency loss. Significance-reweighted consistency (SRC) loss. To encourage the teacher model to transfer more credible knowledge to the student model, we define an SRC loss to penalize the inconsistency between the mixed teacher prediction and the mixed student prediction with the guidance of U M :</p><formula xml:id="formula_12">L con (f ? , f ? ) = j (U M ? CE(F ? (X M ), Y M )) j U M ,<label>(9)</label></formula><p>where F ? and F ? are the teacher model and the student model, respectively. CE is the abbreviation of the crossentropy loss. As recent methods <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b40">[41]</ref>, <ref type="bibr" target="#b65">[66]</ref> reveal that Cross-Entropy loss is more suitable than M SE and KL loss for the semantic segmentation task, we thus design this SRC loss on top of CE loss. The pixel-wise SigMask U M is used to reweigh the consistency loss in a weighted averaging manner.</p><p>In particular, we normalize the loss L con by the summation of all pixels in the SigMask U M . As a result, we could further alleviate the adverse impacts and the negative transfer, e.g., early performance degradation, during the online adaptation of consistency regularization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. End-to-End Training and Inference</head><p>Segmentation loss. The segmentation loss L seg is a crossentropy loss for optimizing the source images:</p><formula xml:id="formula_13">L seg = ? H h=1 W w=1 C c=1 Y (h,w,c) S log(P (h,w,c) S ),<label>(10)</label></formula><p>where Y S is the ground truth for source images and P S = f ? (X S ) (h,w,c) ) is the segmentation output of source images.</p><p>Total loss. During training, all models on three different levels are jointly trained in an end-to-end manner. The whole framework is optimized by integrating all the loss functions:</p><formula xml:id="formula_14">L total = L seg + ? con L con ,<label>(11)</label></formula><p>where ? con is the weight of consistency loss. , and we use the same adaptive schedule for the weight ? con as <ref type="bibr" target="#b40">[41]</ref> in all experiments. Algorithm 2 illustrates the CAMix algorithm of the whole end-to-end training process.</p><p>Inference phase: Since the teacher model is the exponential moving average (EMA) of the student model in the Mean Teacher <ref type="bibr" target="#b66">[67]</ref> architecture, as shown in Eq. 3, the teacher model always performs slightly better than the student model. Thus, following <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b66">[67]</ref>, <ref type="bibr" target="#b69">[70]</ref>, <ref type="bibr" target="#b73">[73]</ref>, we only use the teacher model to make predictions in the inference phase.</p><p>F. Discussions on differences from related work UACR <ref type="bibr" target="#b73">[73]</ref> In this subsection, we discuss the differences from the related work UACR <ref type="bibr" target="#b73">[73]</ref> from the following three aspects: Different motivations: UACR <ref type="bibr" target="#b73">[73]</ref> focuses on addressing the unreliable guidance of the teacher model in Mean Teacher <ref type="bibr" target="#b66">[67]</ref> architecture and utilizes uncertainty to re-calibrate the teacher predictions. However, UCAR <ref type="bibr" target="#b73">[73]</ref> does not consider exploiting contexts as explicit prior knowledge for enhancing the adaptability towards the target domain. In contrast, in this work, we observe that almost all existing UDA frameworks largely neglect such context-dependency, which is generally shared across different domains, leading to less-desired performance. From a new perspective, our goal is to exploit this important clue of context-dependency as explicit prior knowledge to promote the domain mixup, which is different from <ref type="bibr" target="#b73">[73]</ref>. Different frameworks: Although our CAMix also performs stochastic forward passes to estimate uncertainty, our main contribution is not it but a CAMix framework that performs context-aware mixup in three different levels, which selectively leverages the spatial distribution of the source domain and the contextual relationship of the target domain. Besides, UCAR <ref type="bibr" target="#b73">[73]</ref> needs to utilize the image translation model, e.g., CycleGAN <ref type="bibr" target="#b74">[74]</ref>, to stylize the source domain to the intermediate domain with target styles, which requires two-stage training for adaptation. In contrast, our CAMix framework can be trained in a fully end-to-end manner, which largely simplifies the training procedure and is more practical in realworld applications. In the experimental part, we demonstrate that our CAMix outperforms UCAR <ref type="bibr" target="#b73">[73]</ref> by a large margin in two benchmarks, shown in <ref type="table" target="#tab_1">Table II and Table III</ref>. Different constraints: As for the consistency loss, UCAR <ref type="bibr" target="#b73">[73]</ref> utilized both the uncertainty mask and classdrop mask of the target images for reweighing the original teacher prediction without any mixing operations. In particular, UCAR <ref type="bibr" target="#b73">[73]</ref> utilized a ClassOut strategy to ensure the model will produce consistent predictions under the ClassDrop perturbations. In contrast, in this work, we mix the source significance mask and the target mask via the proposed CAMix, and then utilize the mixed significance mask to reweigh the mixed teacher predictions, which is different from <ref type="bibr" target="#b73">[73]</ref>. IV. EXPERIMENTS In this section, we first describe the experimental setup in Section IV-A and implementation details in Section IV-B. Then, we demonstrate the effectiveness of our framework on two widely-used UDA benchmarks, i.e., GTAV [13] ? Cityscapes <ref type="bibr" target="#b10">[11]</ref>, and SYNTHIA <ref type="bibr" target="#b13">[14]</ref> ? Cityscapes <ref type="bibr" target="#b10">[11]</ref>. Finally, we provide extensive ablation studies with analysis (Section IV-E) and visualizations (Section IV-F) to reveal the contribution of each component of our proposed method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Datasets</head><p>Following common UDA protocols <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b85">[85]</ref>, we use the labeled synthetic dataset, i.e., GTAV <ref type="bibr" target="#b12">[13]</ref> and SYNTHA <ref type="bibr" target="#b13">[14]</ref>, as the source domain, and the unlabeled real dataset i.e., Cityscapes <ref type="bibr" target="#b10">[11]</ref> as the target domain. Cityscapes <ref type="bibr" target="#b10">[11]</ref> is a dataset focused on autonomous driving, which consists of 2,975 images in the training set and 500 images in the validation set. The images have a fixed spatial resolution of 2048 ? 1024 pixels. Following common practice, we trained the model on the unlabeled training set and report our results on the validation set. GTAV <ref type="bibr" target="#b12">[13]</ref> is a synthetic dataset including 24,966 photorealistic images rendered by the gaming engine Grand Theft Auto V (GTAV). The semantic categories are compatible between the two datasets. We used all the 19 official training classes in our experiments. SYNTHIA <ref type="bibr" target="#b13">[14]</ref> is a synthetic dataset composed of 9,400 annotated images with the resolution of 1280 ? 960. It also has semantically compatible annotations with Cityscapes. Following prior works <ref type="bibr" target="#b86">[86]</ref>- <ref type="bibr" target="#b88">[88]</ref>, we use the SYNTHIA-RAND-CITYSCAPES subset <ref type="bibr" target="#b13">[14]</ref> as our training set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Implementation Details</head><p>Following common UDA protocols <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b30">[31]</ref>, when the source domain is GTAV <ref type="bibr" target="#b12">[13]</ref>, we resize all images to 1280 ? 720; when the source domain is SYNTHIA <ref type="bibr" target="#b13">[14]</ref>, we resize all images to 1280 ? 760. Then, both the source and target images are randomly cropped to 512?512. To demonstrate the effectiveness, we implement our method in two popular network architectures, i.e., DeepLabV2 <ref type="bibr" target="#b0">[1]</ref> and SegFormer <ref type="bibr" target="#b89">[89]</ref>. Implementation Details with DeepLabV2 <ref type="bibr" target="#b0">[1]</ref>: Following the widely used implementation protocol in previous works <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b40">[41]</ref>, we employ DeepLabV2 <ref type="bibr" target="#b0">[1]</ref> with ResNet 101 backbone <ref type="bibr" target="#b90">[90]</ref>. Following <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b40">[41]</ref>, <ref type="bibr" target="#b65">[66]</ref>, the backbone is pre-trained on ImageNet <ref type="bibr" target="#b91">[91]</ref> and MSCOCO <ref type="bibr" target="#b9">[10]</ref>. For the DeepLabV2 network, we use Adam <ref type="bibr" target="#b92">[92]</ref> as the optimizer. The initial learning rate is 2.5 ? 10 ?4 which is then decreased using polynomial decay with an exponent of 0.9. The weight decay is 5 ? 10 ?5 and the momentum is 0.9. We use the same data augmentation as DACS <ref type="bibr" target="#b40">[41]</ref>, i.e., color jittering and Gaussian blurring. Our method is implemented in Pytorch on a single NVIDIA Tesla V100, and we train the model for 250K iterations with an early stop setting. Implementation Details with SegFormer <ref type="bibr" target="#b89">[89]</ref>: Following DAFormer <ref type="bibr" target="#b45">[46]</ref>, our basic network is based on Seg-Former <ref type="bibr" target="#b89">[89]</ref>, which consists of an MiT-B5 encoder <ref type="bibr" target="#b89">[89]</ref> and a context-aware feature fusion decoder. All encoders are pretrained on ImageNet-1k dataset <ref type="bibr" target="#b91">[91]</ref>. We use AdamW <ref type="bibr" target="#b93">[93]</ref> as the optimizer with a learning rate of ? base =6?10 ?5 for the encoder and 6?10 ?4 for the decoder, a weight decay of 0.01, linear learning rate warmup with t warm =1.5k, and linear decay afterwards. We use the same DACS <ref type="bibr" target="#b40">[41]</ref> data augmentation and set ?=0.99. We train the model for 90K iterations on a single NVIDIA Tesla V100. More Details of CAMix: Following prior works <ref type="bibr" target="#b73">[73]</ref>, <ref type="bibr" target="#b94">[94]</ref>, we perform N = 8 times of stochastic forward passes in our SigMask-level CAMix. Besides, we use the same adaptive schedule as previous consistency regularization works, e.g, CutMix <ref type="bibr" target="#b43">[44]</ref> and DACS <ref type="bibr" target="#b40">[41]</ref> for the consistency weight ? con . As suggested by <ref type="bibr" target="#b73">[73]</ref>, <ref type="bibr" target="#b94">[94]</ref>, we use the same hyper-parameters  <ref type="table" target="#tab_1">Table II and Table III</ref> present the comparison results with the state-of-the-art methods on two challenging UDA tasks: "GTAV ? Cityscapes" and "SYNTHIA ? Cityscapes". As we can see, our proposed method outperforms these competitors by a large margin with two different baselines methods, e.g., DACS <ref type="bibr" target="#b40">[41]</ref> and DAFormer <ref type="bibr" target="#b45">[46]</ref>. In particular, our method (w DACS <ref type="bibr" target="#b40">[41]</ref>) is superior to the DACS <ref type="bibr" target="#b40">[41]</ref> baseline by 3.1% and 4.9% of mIoU in these two benchmarks, and our method (w DAFormer <ref type="bibr" target="#b45">[46]</ref>) achieves improvements of 1.7% and 1.8% of mIoU compared to DAFormer <ref type="bibr" target="#b45">[46]</ref> in these two datasets.</p><p>Specifically, most recent UDA approaches perform adversarial learning, e.g., APODA <ref type="bibr" target="#b34">[35]</ref>, IntraDA <ref type="bibr" target="#b33">[34]</ref>, WLabel <ref type="bibr" target="#b75">[75]</ref>, FADA <ref type="bibr" target="#b26">[27]</ref> and DADA <ref type="bibr" target="#b27">[28]</ref>, and they need to carefully tune the optimization procedure for min-max problems through a domain discriminator. However, such domain discriminators tend to be unstable and inaccurate. Instead, our method does not require maintaining an extra discriminator during the domain adaptation process, and we outperform these approaches by more than 6% with DACS <ref type="bibr" target="#b40">[41]</ref> and 20% with DAFormer <ref type="bibr" target="#b45">[46]</ref>. To alleviate the adversarial feature misalignment and stabilize the training of the discriminator during the adversarial adaptation, SIBAN <ref type="bibr" target="#b22">[23]</ref> presents a significanceaware module to detect the channel-wise significance for each pixel-level feature, and for weighting the information bottleneck loss. In contrast, we do not rely on additional models, e.g., the significance-aware module in <ref type="bibr" target="#b22">[23]</ref>, for significance reweighting, and simply use Eq. 5 ? Eq. 8 to compute the significance mask. Besides, given the contextual mask M as additional supervisory signals to promote the domain mixup, our SigMask is used to reweigh the consistency loss, which has different focus from <ref type="bibr" target="#b22">[23]</ref>. <ref type="table" target="#tab_1">Table II and Table III</ref> demonstrate that our method is superior to SIBAN [23] by a large margin.</p><p>In contrast to the offline self-training methods that need to fine-tune the models in many rounds, e.g., CRST <ref type="bibr" target="#b24">[25]</ref>, LSE <ref type="bibr" target="#b37">[38]</ref>, CCM <ref type="bibr" target="#b77">[77]</ref>, our whole framework can be trained in a fully end-to-end manner. Benefiting from the online consistency regularization with our specially-designed components, our approach significantly outperforms them by at least 5% with DACS <ref type="bibr" target="#b40">[41]</ref> and 16% with DAFormer <ref type="bibr" target="#b45">[46]</ref>.</p><p>Compared to the methods which require an image-to-image (I2I) translation or style transfer algorithm to filter out the domain-specific texture or style information, e.g., BDL <ref type="bibr" target="#b16">[17]</ref>, LDR <ref type="bibr" target="#b21">[22]</ref>, LTIR <ref type="bibr" target="#b17">[18]</ref>, FDA <ref type="bibr" target="#b15">[16]</ref> and PCEDA <ref type="bibr" target="#b20">[21]</ref>, our contextaware domain mixup does not require any style/spectral transfer algorithms or deep neural networks for I2I translation. Our CAMix (w DACS <ref type="bibr" target="#b40">[41]</ref>) is simple and works very well, and it surpasses the translation-based methods by around 5% ? 8%.</p><p>CrCDA <ref type="bibr" target="#b76">[76]</ref> learned and enforced the prototypical local contextual relations in the feature space, and similarly, CD-SAM <ref type="bibr" target="#b78">[78]</ref> exploits contexts implicitly in the feature space, while the visual cues of context knowledge tend to be lost. Moreover, both of the learning <ref type="bibr" target="#b76">[76]</ref>, <ref type="bibr" target="#b78">[78]</ref> do not explicitly exploit the cross-domain contexts in the image space and cannot be trained end-to-end. In contrast, our CAMix explicitly explores the contexts in the image space rather than the feature space, and our architecture can be trained end-to-end. Our approach (w DACS <ref type="bibr" target="#b40">[41]</ref>) outperforms the CrCDA <ref type="bibr" target="#b76">[76]</ref> by  6.6% and 9.7% in two benchmarks, respectively. Compared to resampling-based methods <ref type="bibr" target="#b45">[46]</ref>, <ref type="bibr" target="#b82">[82]</ref> that aim to remedy the class imbalance issue in UDA, our method (w DACS <ref type="bibr" target="#b40">[41]</ref>) outperforms the UncerDA <ref type="bibr" target="#b82">[82]</ref> that uses the softbalance sampling by 2.6% in GTAV <ref type="bibr" target="#b12">[13]</ref> ? Cityscapes <ref type="bibr" target="#b10">[11]</ref> and 5.1% in SYNTHIA <ref type="bibr" target="#b13">[14]</ref> ? Cityscapes <ref type="bibr" target="#b10">[11]</ref>. Besides, our approach (w DAFormer <ref type="bibr" target="#b45">[46]</ref>) is superior to DAFormer <ref type="bibr" target="#b45">[46]</ref> that utilizes the rare class sampling (RCS) by 1.7% in GTAV <ref type="bibr" target="#b12">[13]</ref> ? Cityscapes <ref type="bibr" target="#b10">[11]</ref> and 1.8% in SYNTHIA <ref type="bibr" target="#b13">[14]</ref> ? Cityscapes <ref type="bibr" target="#b10">[11]</ref>. Taking a closer look at per-category performance in <ref type="table" target="#tab_1">Table II and Table III</ref>, our approach achieves the highest IoU on most categories, e.g., motorcycle, bicycle, traffic sign, etc, and is superior to those resampling-based methods in most categories. This phenomenon reveals the effectiveness of CAMix among different classes during the domain adaptation process. <ref type="table" target="#tab_1">Table IV</ref>, we present the comparison results with the existing domain mixup algorithms on GTAV <ref type="bibr" target="#b12">[13]</ref> ? Cityscapes <ref type="bibr" target="#b10">[11]</ref>. All existing domain mixup algorithms are implemented under the same settings and we choose the Mean Teacher <ref type="bibr" target="#b66">[67]</ref> as our baseline in this experiment. CowMix <ref type="bibr" target="#b44">[45]</ref>, CutMix <ref type="bibr" target="#b43">[44]</ref> are proposed for semi-supervised learning (SSL), and we adapt them to the UDA task, which mixes the source domain image and the target domain image. Besides, we implement the existing cross-domain mixup method, e.g., DACS <ref type="bibr" target="#b40">[41]</ref> and inverse DACS. The former DACS means using ClassMix to copy the source categories and paste them onto the target, and the latter Inverse DACS (iDACS) <ref type="bibr" target="#b40">[41]</ref> uses a target-to-source direction. Similarly, DAFormer <ref type="bibr" target="#b45">[46]</ref> and inverse DAFormer (iDAFormer) <ref type="bibr" target="#b45">[46]</ref> use the source-totarget and target-to-source directions, respectively, during the domain mixup. Note that all experiments of our method are based on the iDACS <ref type="bibr" target="#b40">[41]</ref> and iDAFormer baselines <ref type="bibr" target="#b45">[46]</ref>. <ref type="table" target="#tab_1">Table IV</ref> (a) and Table IV (b), with different basic models, i.e., DeepLabV2 <ref type="bibr" target="#b0">[1]</ref> and SegFormer <ref type="bibr" target="#b89">[89]</ref>, the results demonstrate the superiority of our CAMix to different domain mixup methods. The main reasons lie in the following aspects: firstly, we analyze that using CowMix <ref type="bibr" target="#b44">[45]</ref> results in the occurrence of partial objects in the mixed images, which are hard to learn in the training process. Secondly, CutMix <ref type="bibr" target="#b43">[44]</ref>, DACS <ref type="bibr" target="#b40">[41]</ref> and DAFormer <ref type="bibr" target="#b45">[46]</ref> tend to result in severe label contamination and category confusion when generating the mixed results, thus leading to negative transfer. Besides, iDACS <ref type="bibr" target="#b40">[41]</ref> and iDAFormer <ref type="bibr" target="#b45">[46]</ref> lack sufficient supervision and produce error-prone target pseudo labels, leading to less-desired performances. In contrast, we exploit the context-dependency across domains as important prior knowledge for facilitating the adaptability toward the target domain, which is largely overlooked by prior works.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Comparison with related Domain Mixup methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>As shown in</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>As shown in</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Ablation Studies and Analysis</head><p>In this section, we study the effectiveness of each component in our approach and investigate how they contribute to the final performance from GTAV <ref type="bibr" target="#b12">[13]</ref> to Cityscapes <ref type="bibr" target="#b10">[11]</ref>. Effectiveness of CMG: CMG is a fundamental component of our framework, which is designed to capture the shared context-dependency across domains for CAMix. Spatial prior (SP) and contextual relationship (CR) are two key components of CMG. The ablation studies of each component in CAMix are reported in Table V (a). Compared to iDACS <ref type="bibr" target="#b40">[41]</ref> baseline that performs domain mixup in the target-to-source direction, SP and CR could successfully bring 1.6% and 1.4% of improvements, achieving 53.1% and 54.5% on the former two levels, respectively. By adding the SRC loss on the SigMask level, we can achieve an even higher performance of 55.2%. Effectiveness of different levels: Table V (b) lists the impacts of different levels on the two settings, i.e., taking GTAV <ref type="bibr" target="#b12">[13]</ref> and SYNTHIA <ref type="bibr" target="#b13">[14]</ref> as source domains, respectively. Mean Teacher (MT) <ref type="bibr" target="#b66">[67]</ref> baseline achieves 43.1% and 45.9% on two benchmarks, respectively. In-Out means using both the input and output level mixup. By performing CAMix in the input and output level, our method respectively brings +11.4% and +13.1% improvements, reaching 54.5% and 59.0%. By integrating CAMix on three levels together, we finally achieve 55.2% and 59.7% mIoU, respectively. It also reveals that domain mixup in different levels are complementary and together they promote the performance. Effectiveness of SRC: <ref type="table" target="#tab_1">Table VI</ref> shows the contribution of the SRC loss on the GTAV ? Cityscapes benchmark. The full CAMix with all three levels and SRC loss achieves 55.2%. If we directly replace the SRC loss with a normal mean square error (MSE), the result is even worse and only reaches 44.5%.</p><p>Using the cross-entropy (CE) as the consistency loss boosts the mIoU to 54.2%, which is still 1.0% worse than our SRC loss in Eq. <ref type="bibr" target="#b8">(9)</ref>. The main benefits of the SRC loss are reflected as follows. The SigMask-level domain mixup with the SRC loss could further decrease the uncertainty of the teacher model and promote the teacher model to transfer reasonable knowledge to the student, thus improving the performance. As such, our approach tends to be more stable and effectively ease these negative impacts, i.e., training instability and early performance degradation, during the adaptation process. Ablation of different meta class lists: As mentioned in Section III-B, the meta-class list m involves several groups of meta-classes <ref type="table" target="#tab_1">(Table I)</ref> chosen from the prior knowledge of the hierarchical contexts given in Cityscapes <ref type="bibr" target="#b10">[11]</ref>. <ref type="table" target="#tab_1">Table VII</ref> reveals the effect of different combinations of meta class groups in GTAV <ref type="bibr" target="#b12">[13]</ref> ? Cityscapes <ref type="bibr" target="#b10">[11]</ref> with DAFormer <ref type="bibr" target="#b45">[46]</ref>. Class frequency of selected infrequent categories. From the table, we find that when the number of meta class groups is too small or too large, the performance is less desired, and we observe that the best performance occurs when choosing the first two groups. The main reasons behind this phenomenon can be explained as follows. Too little context knowledge is not enough to provide sufficient supervision signals to facilitate the adaptation, while too much prior knowledge of context-dependency limits the performance of neural networks due to the fact that too many constraints can easily make the learning fall into local optima. Thus, we set the first two groups as our meta class list m in all experiments to show the robustness of our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Visualization</head><p>Visual comparisons of different domain mixup algorithms.</p><p>As shown in <ref type="figure">Fig. 5</ref>, we visualize the mixed samples of different domain mixup algorithms. We use the same source image and target image for each row. We can find that mixed results of previous domain mixup method, DACS <ref type="bibr" target="#b40">[41]</ref>, involves some label contamination and category confusion.</p><p>The main reason is that they overlook the shared contextdependency across domains, and a direct mixup will place the semantic categories in an inappropriate context. Instead, our method (CAMix) explicitly respects the contextual structure of the scenes and generates fewer artifacts in the mixed results.</p><p>Comparisons of class frequency with iDACS <ref type="bibr" target="#b40">[41]</ref>  <ref type="figure" target="#fig_3">Fig. 6</ref> plots the visualizations of class frequency of selected infrequent categories in GTAV <ref type="bibr" target="#b12">[13]</ref> ? Cityscapes <ref type="bibr" target="#b10">[11]</ref>. From this figure, we can observe that: 1) the frequency of minority classes has significant improvement compared to the iDACS <ref type="bibr" target="#b40">[41]</ref> baseline model, leading to improvements in per-   class IoU, shown in <ref type="table" target="#tab_1">Table II and Table III.</ref> 2) By considering the contextual relationships, the proposed CAMix strategy can provide sufficient training data and alleviate the over-fitting problem. The main reason behind this phenomenon lies in the following aspects: the predictive likelihood of these selected infrequent categories of the iDACS <ref type="bibr" target="#b40">[41]</ref> baseline model is low, and these categories are challenging for domain mixup and usually lead to predictions with high uncertainties. With our CAMix, the predictions of these imbalanced categories become confident due to sufficient samples for training. Qualitative segmentation results. <ref type="figure" target="#fig_5">Fig. 7</ref> visualizes some segmentation results in the SYNTHIA ? Cityscapes (16 classes) set-up. The four columns plot (a) RGB input images, (b) ground truth, (c) DACS baseline outputs <ref type="bibr" target="#b40">[41]</ref> and (d) the predictions of CAMix. As we can see from the figure, due to the lack of context-dependency, DACS <ref type="bibr" target="#b40">[41]</ref> tends to produce noisy segmentation predictions on some large categories, e.g., 'road', 'sidewalk', 'truck', etc, and incorrectly classifies some large categories, e.g., the road as sidewalk or terrain, and produces some false predictions on some sophisticated classes, e.g., traffic sign. With the help of our proposed CAMix and SRC loss, our model manages to produce correct predictions at a high level of confidence. <ref type="figure" target="#fig_5">Fig. 7</ref> shows that CAMix achieves good performance on 'road', 'sidewalk', 'bus', 'car', 'truck', 'motorcycle', 'bicycle', 'building', and 'terrain' classes. Our proposed method is capable of outputting high confidence predictions compared to the previous work. Analysis of hyper-parameter ? and ? in Eq. 7. <ref type="figure" target="#fig_6">Fig. 8</ref> plots the performance of models trained with different hyperparameter (? and ?) values on the setting of GTAV <ref type="bibr" target="#b12">[13]</ref> ? Cityscapes <ref type="bibr" target="#b10">[11]</ref> and SYNTHIA <ref type="bibr" target="#b13">[14]</ref> ? Cityscapes <ref type="bibr" target="#b10">[11]</ref>. As mentioned above, ? is the initial state of the dynamic LWKLWHUDWLRQV P,R8RQYDOVHW 0HDQ7HDFKHU 0HDQ7HDFKHU65&amp;/RVV <ref type="figure">Fig. 9</ref>: Performance curve on GTA5 <ref type="bibr" target="#b12">[13]</ref> to Cityscapes <ref type="bibr" target="#b10">[11]</ref>. The blue line corresponds to the conventional consistency regularization <ref type="bibr" target="#b38">[39]</ref>. The orange line indicates the consistencybased adaptation with our SRC loss. Our method eases the issue of training instability and early performance drop. threshold H, and ? controls the exponential speed of the dynamic threshold. The highest mIoU on the target domain is achieved when the value of ? is around 0.75 and ? is around ?5, which means that this initial state and exponential speed benefit domain adaptation the most. Thus, we simply set the same ? = 0.75 and ? = ?5 in all experiments to show the robustness of our method in different settings. Performance curve of adaptation. <ref type="figure">Fig. 9</ref> plots the performance curves to show the effectiveness of SRC loss when adapting from GTAV <ref type="bibr" target="#b12">[13]</ref> to Cityscapes <ref type="bibr" target="#b10">[11]</ref> with VGG16 <ref type="bibr" target="#b95">[95]</ref> backbone. We observe that the curve of Mean Teacher <ref type="bibr" target="#b38">[39]</ref>, which is representative of previous consistency regularization methods, fluctuates wildly and causes the training instability and early performance degradation. The main reason is that they largely neglect the context knowledge shared by different domains and perform a rough distribution matching, resulting in less-desired performances. Instead, we effectively ease these negative impacts and decrease the uncertainty of the segmentation model, by introducing the SRC loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>In this paper, we proposed a novel context-aware domain mixup (CAMix) framework via explicitly exploiting contextdependency across domains to enhance the adaptability for domain adaptive semantic segmentation. We present a contextual mask generation (CMG) strategy, which is critical for guiding the whole pipeline on three different levels, i.e., input level, output level and, significance mask level. Our approach can explicitly explore and transfer the shared contextdependency across domains, thus narrowing down the domain gap. We also introduce a significance-reweighted consistency loss (SRC) to penalize the inconsistency between the mixed student prediction and the mixed teacher prediction, which effectively eases the adverse impacts of the adaptation, e.g., training instability and early performance degradation. Extensive experiments with analysis demonstrate that our approach soundly outperforms the state-of-the-art methods in domain adaptive semantic segmentation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>9 U 10 U 11 L</head><label>91011</label><figDesc>mixup by Eq.(4); T ? Target SigMask by Eq.(5)?Eq.(7) ; M ? SigMask-level mixup by Eq.(8); total ? Total loss by Eq.(11);12</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 6 :</head><label>6</label><figDesc>Visualizations on class frequency of selected infrequent categories during the mixup in GTAV<ref type="bibr" target="#b12">[13]</ref> ? Cityscapes<ref type="bibr" target="#b10">[11]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 7 :</head><label>7</label><figDesc>Qualitative segmentation results in the SYNTHIA ? Cityscapes setup. The four columns plot (a) RGB input image, (b) ground-truth, (c) the predictions of DACS [41] and (d) the predictions of our CAMix. (Best viewed in color.) (a) Effect of the hyper-paramter ? in Eq. 7. (b) Effect of the hyper-paramter ? in Eq. 7.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 8 :</head><label>8</label><figDesc>Hyper-parameter analysis of ? and ? in Eq. 7.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Mixed student prediction EMA Significance re-weighted consistency loss Mixed teacher prediction Source significance mask Target significance mask Predictive entropy Average teacher prediction Source label Target pseudo label Target image Mixed image CE loss Source prediction Source image Input level Output level Significance mask level Mixed significance mask W Target flow Source flow Mixed flow CAMix Student Teacher CAMix CAMix</head><label></label><figDesc></figDesc><table><row><cell>?</cell><cell></cell><cell></cell></row><row><cell>Source spatial relationship</cell><cell>Class list</cell><cell>Contextual mask M (Target)</cell></row><row><cell></cell><cell></cell><cell>1-M (Source)</cell></row><row><cell>Target prediction</cell><cell cols="2">Contextual relationship</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I :</head><label>I</label><figDesc>Prior knowledge of hierarchical contexts given in the Cityscapes<ref type="bibr" target="#b10">[11]</ref>. We use it to define our meta class group.</figDesc><table><row><cell>Group</cell><cell>Coarse</cell><cell>Fine</cell></row><row><cell></cell><cell></cell><cell>pole</cell></row><row><cell>I</cell><cell>object</cell><cell>traffic sign</cell></row><row><cell></cell><cell></cell><cell>traffic light</cell></row><row><cell></cell><cell></cell><cell>rider</cell></row><row><cell>II</cell><cell>human-vehicle</cell><cell>motorcycle</cell></row><row><cell></cell><cell></cell><cell>bicycle</cell></row><row><cell>III</cell><cell>flat</cell><cell>road sidewalk</cell></row><row><cell></cell><cell></cell><cell>building</cell></row><row><cell>IV</cell><cell>construction</cell><cell>wall</cell></row><row><cell></cell><cell></cell><cell>fence</cell></row><row><cell>V</cell><cell>nature</cell><cell>vegetation terrain</cell></row><row><cell cols="3">X T ? T are available. Unlike most of the existing UDA</cell></row><row><cell cols="3">methods that overlook the shared context knowledge across</cell></row><row><cell cols="3">domains, we propose a novel context-aware domain mixup</cell></row><row><cell cols="3">(CAMix) to exploit and transfer such cross-domain contexts.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE II :</head><label>II</label><figDesc>Comparison results (mIoU) with state-of-the-art methods from GTAV to Cityscapes.</figDesc><table><row><cell>Method</cell><cell>Venue</cell><cell>road</cell><cell>sidewalk</cell><cell>building</cell><cell>wall</cell><cell>fence</cell><cell>pole</cell><cell>light</cell><cell>sign</cell><cell>vegetation</cell><cell>terrain</cell><cell>sky</cell><cell>person</cell><cell>rider</cell><cell>car</cell><cell>truck</cell><cell>bus</cell><cell>train</cell><cell>motocycle</cell><cell>bike</cell><cell>mIoU</cell></row><row><cell>SIBAN [23]</cell><cell>ICCV'19</cell><cell>88.5</cell><cell cols="3">35.4 79.5 26.3</cell><cell cols="12">24.3 28.5 32.5 18.3 81.2 40.0 76.5 58.1 25.8 82.6 30.3 34.4</cell><cell>3.4</cell><cell>21.6</cell><cell>21.5</cell><cell>42.6</cell></row><row><cell>BDL [17]</cell><cell>CVPR'19</cell><cell>91.0</cell><cell cols="3">44.7 84.2 34.6</cell><cell cols="12">27.6 30.2 36.0 36.0 85.0 43.6 83.0 58.6 31.6 83.3 35.3 49.7</cell><cell>3.3</cell><cell>28.8</cell><cell>35.6</cell><cell>48.5</cell></row><row><cell>APODA [35]</cell><cell>AAAI'20</cell><cell>85.6</cell><cell cols="3">32.8 79.0 29.5</cell><cell cols="12">25.5 26.8 34.6 19.9 83.7 40.6 77.9 59.2 28.3 84.6 34.6 49.2</cell><cell>8.0</cell><cell>32.6</cell><cell>39.6</cell><cell>45.9</cell></row><row><cell>IntraDA [34]</cell><cell>CVPR'20</cell><cell>90.6</cell><cell cols="3">37.1 82.6 30.1</cell><cell cols="12">19.1 29.5 32.4 20.6 85.7 40.5 79.7 58.7 31.1 86.3 31.5 48.3</cell><cell>0.0</cell><cell>30.2</cell><cell>35.8</cell><cell>46.3</cell></row><row><cell>SIM [32]</cell><cell>CVPR'20</cell><cell>90.6</cell><cell cols="3">44.7 84.8 34.3</cell><cell cols="12">28.7 31.6 35.0 37.6 84.7 43.3 85.3 57.0 31.5 83.8 42.6 48.5</cell><cell>1.9</cell><cell>30.4</cell><cell>39.0</cell><cell>49.2</cell></row><row><cell>LTIR [18]</cell><cell>CVPR'20</cell><cell>92.9</cell><cell cols="3">55.0 85.3 34.2</cell><cell cols="12">31.1 34.9 40.7 34.0 85.2 40.1 87.1 61.0 31.1 82.5 32.3 42.9</cell><cell>0.3</cell><cell>36.4</cell><cell>46.1</cell><cell>50.2</cell></row><row><cell>FDA [16]</cell><cell>CVPR'20</cell><cell>92.5</cell><cell cols="3">53.3 82.4 26.5</cell><cell cols="14">27.6 36.4 40.6 38.9 82.3 39.8 78.0 62.6 34.4 84.9 34.1 53.1 16.9 27.7</cell><cell>46.4</cell><cell>50.5</cell></row><row><cell>PCEDA [21]</cell><cell>CVPR'20</cell><cell>91.0</cell><cell cols="3">49.2 85.6 37.2</cell><cell cols="12">29.7 33.7 38.1 39.2 85.4 35.4 85.1 61.1 32.8 84.1 45.6 46.9</cell><cell>0.0</cell><cell>34.2</cell><cell>44.5</cell><cell>50.5</cell></row><row><cell>LSE [38]</cell><cell>ECCV'20</cell><cell>90.2</cell><cell cols="3">40.0 83.5 31.9</cell><cell cols="12">26.4 32.6 38.7 37.5 81.0 34.2 84.6 61.6 33.4 82.5 32.8 45.9</cell><cell>6.7</cell><cell>29.1</cell><cell>30.6</cell><cell>47.5</cell></row><row><cell>WLabel [75]</cell><cell>ECCV'20</cell><cell>91.6</cell><cell cols="3">47.4 84.0 30.4</cell><cell cols="12">28.3 31.4 37.4 35.4 83.9 38.3 83.9 61.2 28.2 83.7 28.8 41.3</cell><cell>8.8</cell><cell>24.7</cell><cell>46.4</cell><cell>48.2</cell></row><row><cell>CrCDA [76]</cell><cell>ECCV'20</cell><cell>92.4</cell><cell cols="3">55.3 82.3 31.2</cell><cell cols="12">29.1 32.5 33.2 35.6 83.5 34.8 84.2 58.9 32.2 84.7 40.6 46.1</cell><cell>2.1</cell><cell>31.1</cell><cell>32.7</cell><cell>48.6</cell></row><row><cell>FADA [27]</cell><cell>ECCV'20</cell><cell>92.5</cell><cell cols="3">47.5 85.1 37.6</cell><cell cols="12">32.8 33.4 33.8 18.4 85.3 37.7 83.5 63.2 39.7 87.5 32.9 47.8</cell><cell>1.6</cell><cell>34.9</cell><cell>39.5</cell><cell>49.2</cell></row><row><cell>LDR [22]</cell><cell>ECCV'20</cell><cell>90.8</cell><cell cols="3">41.4 84.7 35.1</cell><cell cols="12">27.5 31.2 38.0 32.8 85.6 42.1 84.9 59.6 34.4 85.0 42.8 52.7</cell><cell>3.4</cell><cell>30.9</cell><cell>38.1</cell><cell>49.5</cell></row><row><cell>CCM [77]</cell><cell>ECCV'20</cell><cell>93.5</cell><cell cols="3">57.6 84.6 39.3</cell><cell cols="12">24.1 25.2 35.0 17.3 85.0 40.6 86.5 58.7 28.7 85.8 49.0 56.4</cell><cell>5.4</cell><cell>31.9</cell><cell>43.2</cell><cell>49.9</cell></row><row><cell>CD-SAM [78]</cell><cell>WACV'21</cell><cell>91.3</cell><cell cols="3">46.0 84.5 34.4</cell><cell cols="12">29.7 32.6 35.8 36.4 84.5 43.2 83.0 60.0 32.2 83.2 35.0 46.7</cell><cell>0.0</cell><cell>33.7</cell><cell>42.2</cell><cell>49.2</cell></row><row><cell>ASA [79]</cell><cell>TIP'21</cell><cell>89.2</cell><cell cols="3">27.8 81.3 25.3</cell><cell cols="14">22.7 28.7 36.5 19.6 83.8 31.4 77.1 59.2 29.8 84.3 33.2 45.6 16.9 34.5</cell><cell>30.8</cell><cell>45.1</cell></row><row><cell>CLAN [33]</cell><cell>TPAMI'21</cell><cell>88.7</cell><cell cols="3">35.5 80.3 27.5</cell><cell cols="12">25.0 29.3 36.4 28.1 84.5 37.0 76.6 58.4 29.7 81.2 38.8 40.9</cell><cell>5.6</cell><cell>32.9</cell><cell>28.8</cell><cell>45.5</cell></row><row><cell>DAST [80]</cell><cell>AAAI'21</cell><cell>92.2</cell><cell cols="3">49.0 84.3 36.5</cell><cell cols="12">28.9 33.9 38.8 28.4 84.9 41.6 83.2 60.0 28.7 87.2 45.0 45.3</cell><cell>7.4</cell><cell>33.8</cell><cell>32.8</cell><cell>49.6</cell></row><row><cell>BiMaL [81]</cell><cell>ICCV'21</cell><cell>91.2</cell><cell cols="3">39.6 82.7 29.4</cell><cell cols="12">25.2 29.6 34.3 25.5 85.4 44.0 80.8 59.7 30.4 86.6 38.5 47.6</cell><cell>1.2</cell><cell>34.0</cell><cell>36.8</cell><cell>47.3</cell></row><row><cell>UncerDA [82]</cell><cell>ICCV'21</cell><cell>90.5</cell><cell cols="3">38.7 86.5 41.1</cell><cell cols="12">32.9 40.5 48.2 42.1 86.5 36.8 84.2 64.5 38.1 87.2 34.8 50.4</cell><cell>0.2</cell><cell>41.8</cell><cell>54.6</cell><cell>52.6</cell></row><row><cell>DPL-Dual [83]</cell><cell>ICCV'21</cell><cell>92.8</cell><cell cols="3">54.4 86.2 41.6</cell><cell cols="14">32.7 36.4 49.0 34.0 85.8 41.3 86.0 63.2 34.2 87.2 39.3 44.5 18.7 42.6</cell><cell>43.1</cell><cell>53.3</cell></row><row><cell>RPLR [84]</cell><cell>TPAMI'22</cell><cell>92.3</cell><cell cols="3">52.3 84.8 34.7</cell><cell cols="12">29.7 32.6 36.7 32.7 83.2 42.5 81.5 60.6 33.3 85.0 44.2 48.0</cell><cell>3.8</cell><cell>35.7</cell><cell>37.3</cell><cell>50.1</cell></row><row><cell>UACR [73]</cell><cell>CVIU'22</cell><cell>91.3</cell><cell cols="3">48.6 85.5 35.8</cell><cell cols="14">31.4 36.7 37.5 36.8 86.3 40.3 85.7 64.3 31.1 87.7 36.7 44.9 15.9 38.9</cell><cell>55.4</cell><cell>51.9</cell></row><row><cell>DACS [41]</cell><cell>WACV'21</cell><cell>89.9</cell><cell cols="3">39.7 87.9 30.7</cell><cell cols="12">39.5 38.5 46.4 52.8 88.0 44.0 88.8 67.2 35.8 84.5 45.7 50.2</cell><cell>0.0</cell><cell>27.3</cell><cell>34.0</cell><cell>52.1</cell></row><row><cell>Ours (w DACS [41])</cell><cell>-</cell><cell>93.3</cell><cell cols="3">58.2 86.5 36.8</cell><cell cols="14">31.5 36.4 35.0 43.5 87.2 44.6 88.1 65.0 24.7 89.7 46.9 56.8 27.5 41.1</cell><cell>56.0</cell><cell>55.2</cell></row><row><cell>DAFormer [46]</cell><cell>CVPR'22</cell><cell>95.7</cell><cell cols="3">70.2 89.4 53.5</cell><cell cols="6">48.1 49.6 55.8 59.4 89.9 47.9</cell><cell cols="3">92.5 72.2 44.7</cell><cell cols="5">92.3 74.5 78.2 65.1 55.9</cell><cell>61.8</cell><cell>68.3</cell></row><row><cell>Ours (w DAFormer [46])</cell><cell>-</cell><cell>96.0</cell><cell cols="3">73.1 89.5 53.9</cell><cell cols="6">50.8 51.7 58.7 64.9 90.0 51.2</cell><cell cols="3">92.2 71.8 44.0</cell><cell cols="5">92.8 78.7 82.3 70.9 54.1</cell><cell>64.3</cell><cell>70.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE III :</head><label>III</label><figDesc>Comparison results (mIoU) with state-of-the-art methods from SYNTHIA to Cityscapes.</figDesc><table><row><cell>Method</cell><cell>Venue</cell><cell>road</cell><cell>sidewalk</cell><cell>building</cell><cell>light</cell><cell>sign</cell><cell>vegetation</cell><cell>sky</cell><cell>person</cell><cell>rider</cell><cell>car</cell><cell>bus</cell><cell>motocycle</cell><cell>bike</cell><cell>mIoU 13</cell></row><row><cell>SIBAN [23]</cell><cell>ICCV'19</cell><cell cols="7">82.5 24.0 79.4 16.5 12.7 79.2 82.8</cell><cell cols="6">58.3 18.0 79.3 25.3 17.6 25.9</cell><cell>46.3</cell></row><row><cell>DADA [28]</cell><cell>ICCV'19</cell><cell cols="3">89.2 44.8 81.4</cell><cell>8.6</cell><cell cols="3">11.1 81.8 84.0</cell><cell cols="6">54.7 19.3 79.7 40.7 14.0 38.8</cell><cell>49.8</cell></row><row><cell>BDL [17]</cell><cell>CVPR'19</cell><cell cols="7">86.0 46.7 80.3 14.1 11.6 79.2 81.3</cell><cell cols="6">54.1 27.9 73.7 42.2 25.7 45.3</cell><cell>51.4</cell></row><row><cell>APODA [35]</cell><cell>AAAI'20</cell><cell cols="7">86.4 41.3 79.3 22.6 17.3 80.3 81.6</cell><cell cols="6">56.9 21.0 84.1 49.1 24.6 45.7</cell><cell>53.1</cell></row><row><cell>IntraDA [34]</cell><cell>CVPR'20</cell><cell cols="3">84.3 37.7 79.5</cell><cell>9.2</cell><cell>8.4</cell><cell cols="2">80.0 84.1</cell><cell cols="6">57.2 23.0 78.0 38.1 20.3 36.5</cell><cell>48.9</cell></row><row><cell>LTIR [18]</cell><cell>CVPR'20</cell><cell cols="3">92.6 53.2 79.2</cell><cell>1.6</cell><cell>7.5</cell><cell cols="2">78.6 84.4</cell><cell cols="6">52.6 20.0 82.1 34.8 14.6 39.4</cell><cell>49.3</cell></row><row><cell>SIM [32]</cell><cell>CVPR'20</cell><cell cols="7">83.0 44.0 80.3 17.1 15.8 80.5 81.8</cell><cell cols="6">59.9 33.1 70.2 37.3 28.5 45.8</cell><cell>52.1</cell></row><row><cell>FDA [16]</cell><cell>CVPR'20</cell><cell cols="7">79.3 35.0 73.2 19.9 24.0 61.7 82.6</cell><cell cols="6">61.4 31.1 83.9 40.8 38.4 51.1</cell><cell>52.5</cell></row><row><cell>LSE [38]</cell><cell>ECCV'20</cell><cell cols="3">82.9 43.1 78.1</cell><cell>9.1</cell><cell cols="3">14.4 77.0 83.5</cell><cell cols="6">58.1 25.9 71.9 38.0 29.4 31.2</cell><cell>49.4</cell></row><row><cell>CrCDA [76]</cell><cell>ECCV'20</cell><cell cols="3">86.2 44.9 79.5</cell><cell>9.4</cell><cell cols="3">11.8 78.6 86.5</cell><cell cols="6">57.2 26.1 76.8 39.9 21.5 32.1</cell><cell>50.0</cell></row><row><cell>WLabel [75]</cell><cell>ECCV'20</cell><cell cols="3">92.0 53.5 80.9</cell><cell>3.8</cell><cell>6.0</cell><cell cols="2">81.6 84.4</cell><cell cols="6">60.8 24.4 80.5 39.0 26.0 41.7</cell><cell>51.9</cell></row><row><cell>CCM [77]</cell><cell>ECCV'20</cell><cell cols="7">79.6 36.4 80.6 22.4 14.9 81.8 77.4</cell><cell cols="6">56.8 25.9 80.7 45.3 29.9 52.0</cell><cell>52.9</cell></row><row><cell>LDR [22]</cell><cell>ECCV'20</cell><cell cols="7">85.1 44.5 81.0 16.4 15.2 80.1 84.8</cell><cell cols="6">59.4 31.9 73.2 41.0 32.6 44.7</cell><cell>53.1</cell></row><row><cell>CD-SAM [78]</cell><cell>WACV'21</cell><cell cols="7">82.5 42.2 81.3 18.3 15.9 80.6 83.5</cell><cell cols="6">61.4 33.2 72.9 39.3 26.6 43.9</cell><cell>52.4</cell></row><row><cell>CLAN [33]</cell><cell>TPAMI'21</cell><cell cols="7">82.7 37.2 81.5 17.1 13.1 81.2 83.3</cell><cell cols="6">55.5 22.1 76.6 30.1 23.5 30.7</cell><cell>48.8</cell></row><row><cell>ASA [79]</cell><cell>TIP'21</cell><cell cols="3">91.2 48.5 80.4</cell><cell>5.5</cell><cell>5.2</cell><cell cols="2">79.5 83.6</cell><cell cols="6">56.4 21.9 80.3 36.2 20.0 32.9</cell><cell>49.3</cell></row><row><cell>DAST [80]</cell><cell>AAAI'21</cell><cell cols="7">87.1 44.5 82.3 13.9 13.1 81.6 86.0</cell><cell cols="6">60.3 25.1 83.1 40.1 24.4 40.5</cell><cell>52.5</cell></row><row><cell>BiMaL [81]</cell><cell>ICCV'21</cell><cell cols="7">92.8 51.5 81.5 17.6 15.9 82.4 84.6</cell><cell cols="6">55.9 22.3 85.7 44.5 24.6 38.8</cell><cell>53.7</cell></row><row><cell>UncerDA [82]</cell><cell>ICCV'21</cell><cell cols="7">79.4 34.6 83.5 32.1 26.9 78.8 79.6</cell><cell cols="6">66.6 30.3 86.1 36.6 19.5 56.9</cell><cell>54.6</cell></row><row><cell>DPL-Dual [83]</cell><cell>ICCV'21</cell><cell cols="7">87.5 45.7 82.8 22.0 20.1 83.1 86.0</cell><cell cols="6">56.6 21.9 83.1 40.3 29.8 45.7</cell><cell>54.2</cell></row><row><cell>RPLR [84]</cell><cell>TPAMI'22</cell><cell cols="7">81.5 36.7 78.6 20.7 23.6 79.1 83.4</cell><cell cols="6">57.6 30.4 78.5 38.3 24.7 48.4</cell><cell>52.4</cell></row><row><cell>UACR [73]</cell><cell>CVIU'22</cell><cell cols="7">85.5 42.5 83.0 20.9 25.5 82.5 88.0</cell><cell cols="6">63.2 31.8 86.5 41.2 25.9 50.7</cell><cell>55.9</cell></row><row><cell>DACS [41]</cell><cell>WACV'21</cell><cell cols="7">80.6 25.1 81.9 22.7 24.0 83.7 90.8</cell><cell cols="6">67.6 38.3 82.9 38.9 28.5 47.6</cell><cell>54.8</cell></row><row><cell>Ours (w DACS [41])</cell><cell>-</cell><cell cols="7">91.8 54.9 83.6 23.0 29.0 83.8 87.1</cell><cell cols="6">65.0 26.4 85.5 55.1 36.8 54.1</cell><cell>59.7</cell></row><row><cell>DAFormer [46]</cell><cell>CVPR'22</cell><cell cols="7">84.5 40.7 88.4 55.0 54.6 86.0 89.8</cell><cell>73.2</cell><cell>48.2</cell><cell>87.2</cell><cell>53.2</cell><cell cols="2">53.9 61.7</cell><cell>67.4</cell></row><row><cell>Ours (w DAFormer [46])</cell><cell>-</cell><cell cols="7">87.4 47.5 88.8 55.2 55.4 87.0 91.7</cell><cell>72.0</cell><cell>49.3</cell><cell>86.9</cell><cell>57.0</cell><cell cols="2">57.5 63.6</cell><cell>69.2</cell></row></table><note>of ? = 0.75, ? = ?5 by default in Eq. 7 in all experiments.C. Comparison with the State-of-the-Art Methods</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE IV :</head><label>IV</label><figDesc>Comparisons with related domain mixup methods using different basic architectures from GTAV to Cityscapes.</figDesc><table><row><cell cols="3">(a) Comparison results using DeeplabV2 [1] as basic architecture.</cell><cell cols="3">(b) Comparison results using SegFormer [89] as basic architecture.</cell></row><row><cell>method (w Deeplab-v2 [1])</cell><cell>mIoU (%)</cell><cell>Gain (%)</cell><cell>method (w SegFormer [89])</cell><cell>mIoU (%)</cell><cell>Gain (%)</cell></row><row><cell>Mean Teacher [67]</cell><cell>43.1</cell><cell>-</cell><cell>Mean Teacher [67]</cell><cell>51.6</cell><cell>-</cell></row><row><cell>+ CowMix [45]</cell><cell>48.3</cell><cell>+5.2</cell><cell>+ CowMix [45]</cell><cell>58.9</cell><cell>+7.3</cell></row><row><cell>+ CutMix [44]</cell><cell>48.7</cell><cell>+5.6</cell><cell>+ CutMix [44]</cell><cell>58.7</cell><cell>+7.1</cell></row><row><cell>+ DACS [41]</cell><cell>52.1</cell><cell>+9.0</cell><cell>+ DAFormer [46]</cell><cell>68.3</cell><cell>+16.7</cell></row><row><cell>+ iDACS [41]</cell><cell>51.5</cell><cell>+8.4</cell><cell>+ iDAFormer [46]</cell><cell>62.4</cell><cell>+10.8</cell></row><row><cell>+ Ours (CAMix)</cell><cell>55.2</cell><cell>+12.1</cell><cell>+ Ours (CAMix)</cell><cell>70.0</cell><cell>+18.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE V :</head><label>V</label><figDesc>Ablation study of each component and each level in CAMix.</figDesc><table><row><cell cols="3">(a) Ablation study of each component in CAMix.</cell><cell></cell><cell cols="3">(b) Ablation study of each level in CAMix.</cell></row><row><cell>Baseline [41] ? ? ? ?</cell><cell>SP CR SRC ? ? ? ? ? ?</cell><cell>mIoU 51.5 53.1 54.5 55.2</cell><cell>Mean Teacher ? ? ?</cell><cell>In-Out SigMask ? ? ?</cell><cell>mIoU (GTAV) 43.1 54.5 55.2</cell><cell>mIoU 13 (SYN) 45.9 59.0 59.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE VI :</head><label>VI</label><figDesc>Ablation study of the SRC loss. We use the same source image and target image in each row. Left: source ground truth. Middle: mixed results of DACS<ref type="bibr" target="#b40">[41]</ref>. It cannot place the semantic categories in an appropriate context, which results in label contamination and category confusion. Right: Mixed results of our proposed method (CAMix), which can effectively mitigate these issues.</figDesc><table><row><cell>L con</cell><cell></cell><cell>mIoU</cell></row><row><cell cols="2">Ours (w SRC loss)</cell><cell>55.2</cell><cell>-</cell></row><row><cell cols="2">Ours (w/o SRC loss + MSE Loss)</cell><cell>44.5</cell><cell>? 9.7</cell></row><row><cell cols="2">Ours (w/o SRC loss + CE Loss)</cell><cell>54.2</cell><cell>? 1.0</cell></row><row><cell>Source GT</cell><cell>DACS</cell><cell cols="2">Ours (CAMix)</cell></row><row><cell cols="4">Fig. 5: Visual comparisons of different domain mixup algo-</cell></row><row><cell>rithms.</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE VII :</head><label>VII</label><figDesc>Ablation study of different meta class lists.</figDesc><table><row><cell cols="5">Group I Group II Group III Group IV Group V</cell><cell>mIoU</cell></row><row><cell>? ? ? ? ?</cell><cell>? ? ? ?</cell><cell>? ? ?</cell><cell>? ?</cell><cell>?</cell><cell>68.8 70.0 69.1 67.8 67.3</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="834" to="848" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Encoderdecoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="801" to="818" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2881" to="2890" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Encoder-decoder with cascaded crfs for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Miao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="1926" to="1938" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Computation and memory efficient image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-T</forename><surname>Do</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N.-M</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="46" to="61" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Novel class discovery in semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="page" from="4340" to="4349" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Mirror detection with the visual chirality cue</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">W H</forename><surname>Lau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="303" to="338" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3213" to="3223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Night-time scene parsing with a large real dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">W H</forename><surname>Lau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="9085" to="9098" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Playing for data: Ground truth from computer games</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Richter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vineet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="102" to="118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The synthia dataset: A large collection of synthetic images for semantic segmentation of urban scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sellart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Materzynska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Vazquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Lopez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3234" to="3243" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">CyCADA: Cycle-consistent adversarial domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1989" to="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Fda: Fourier domain adaptation for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4085" to="4095" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Bidirectional learning for domain adaptation of semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6936" to="6945" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning texture invariant representation for domain adaptation of semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Byun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="12" to="975" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Crdoco: Pixellevel domain transfer with cross-domain consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1791" to="1800" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Labelfree regional consistency for image-to-image translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE International Conference on Multimedia and Expo (ICME)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Phase consistent ecological domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sundaramoorthi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9011" to="9020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Labeldriven reconstruction for domain adaptation in semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">12372</biblScope>
			<biblScope unit="page" from="480" to="498" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Significance-aware information bottleneck for domain adaptive semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6778" to="6787" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation for semantic segmentation via class-balanced self-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="289" to="305" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Confidence regularized self-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5982" to="5991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Stochastic classifiers for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-Z</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9111" to="9120" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Classes matter: A fine-grained adversarial approach to cross-domain semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="642" to="659" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">DADA: depthaware domain adaptation in semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bucher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>P?rez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7363" to="7372" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Bapa-net: Boundary adaptation and prototype alignment for cross-domain semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Duan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="8801" to="8811" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning to adapt structured output space for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-H</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-C</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schulter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chandraker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7472" to="7481" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Taking a closer look at domain shift: Category-level adversaries for semantics consistent domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2507" to="2516" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Differential treatment for stuff and things: A simple unsupervised domain adaptation method for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Feris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hwu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="12" to="635" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Categorylevel adversarial adaptation for semantic segmentation using purified features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1" to="1" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Unsupervised intradomain adaptation for semantic segmentation through self-supervision,&quot; in Unsupervised Intra-domain Adaptation for Semantic Segmentation through Self-Supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Rameau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">S</forename><surname>Kweon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3764" to="3773" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">An adversarial perturbation oriented domain adaptation approach for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page">620</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Two-phase pseudo label densification for self-training based domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Woo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">S</forename><surname>Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="532" to="548" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Instance adaptive self-training for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="415" to="430" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Learning from scale-invariant examples for domain adaptation in semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naseer</forename><surname>Subhani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ali</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="290" to="306" />
		</imprint>
	</monogr>
	<note>in European conference on computer vision</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Self-ensembling with gan-based data augmentation for domain adaptation in semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6830" to="6840" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Selfensembling attention networks: Addressing domain shift for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="5581" to="5588" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Dacs: Domain adaptation via cross-domain mixed sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Tranheden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Olsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pinto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Svensson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1379" to="1389" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Advent: Adversarial entropy minimization for domain adaptation in semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-H</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bucher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>P?rez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2517" to="2526" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Domain adaptation for semantic segmentation with maximum squares loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2090" to="2099" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Semi-supervised semantic segmentation needs strong, varied perturbations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>French</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mackiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Finlayson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Milking cowmask for semisupervised image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>French</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.12022</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Daformer: Improving network architectures and training strategies for domain-adaptive semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="page" from="9924" to="9935" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation by backpropagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">S</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="1180" to="1189" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Rethinking triplet loss for domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="29" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation using robust classwise matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Hengel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1339" to="1349" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Slimmable domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="page" from="7141" to="7150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Joint adaptive dual graph and feature selection for domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1453" to="1466" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Margin-based adversarial joint alignment domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="2057" to="2067" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Confidence regularized label propagation based domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="3319" to="3333" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Partial domain adaptation on semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Generative domain adaptation for face anti-spoofing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Pit: Position-invariant transform for cross-fov domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="8761" to="8770" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Weaklysupervised cross-domain road scene segmentation via multi-level curriculum adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Duan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Source-free open compound domain adaptation in semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Pointcutmix: Regularization strategy for point cloud classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.01461</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Pointmixup: Augmentation for point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">T</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mensink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mettes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">G</forename><surname>Snoek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2020: 16th European Conference</title>
		<meeting><address><addrLine>Glasgow, UK</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="330" to="345" />
		</imprint>
	</monogr>
	<note>Proceedings, Part III 16</note>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Adversarial domain adaptation with domain mixup</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="6502" to="6509" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Dual mixup regularized learning for adversarial domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Inkpen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>El-Roby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="540" to="555" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Virtual mixup training for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.04215</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Proc</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">Reading digits in natural images with unsupervised feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Netzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bissacco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
	<note>in NeurIPS workshop</note>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Classmix: Segmentation-based data augmentation for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Olsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Tranheden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pinto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Svensson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1369" to="1378" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tarvainen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Valpola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="1195" to="1204" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Self-ensembling for visual domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>French</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mackiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fisher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
		<meeting>the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation for medical imaging segmentation with selfensembling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">S</forename><surname>Perone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ballester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Barros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cohen-Adad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeuroImage</title>
		<imprint>
			<biblScope unit="volume">194</biblScope>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Domain adaptive semantic segmentation with regional contrastive consistency regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2022 IEEE International Conference on Multimedia and Expo (ICME)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<title level="m" type="main">Bayesian uncertainty matching for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.09693</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Attending to discriminative certainty for domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">K</forename><surname>Kurmi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">P</forename><surname>Namboodiri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="491" to="500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Uncertainty-aware consistency regularization for cross-domain semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="page">103448</biblScope>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Unpaired image-to-image translation using cycle-consistent adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2223" to="2232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Domain adaptive semantic segmentation using weak labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schulter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Roy-Chowdhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chandraker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">12354</biblScope>
			<biblScope unit="page" from="571" to="587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Contextual-relation consistent domain adaptation for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">12360</biblScope>
			<biblScope unit="page" from="705" to="722" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Content-consistent matching for domain adaptive semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">12359</biblScope>
			<biblScope unit="page" from="440" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Context-aware domain adaptation in semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="514" to="524" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Affinity space adaptation for semantic segmentation across domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="2549" to="2561" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Dast: Unsupervised domain adaptation in semantic segmentation based on discriminator attention and self-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="10" to="754" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Bimal: Bijective maximum likelihood approach to domain adaptation in semantic scene segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-D</forename><surname>Truong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">N</forename><surname>Duong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">L</forename><surname>Phung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rainwater</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Luu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Uncertainty-aware pseudo label refinery for domain adaptive semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="9092" to="9101" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Dual path learning for domain adaptation of semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="9082" to="9091" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Feature re-representation and reliable pseudolabel retraining for crossdomainsemantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Fcns in the wild: Pixel-level adversarial and constraint-based adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno>abs/1612.02649</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">No more discrimination: Cross city adaptation of road scene segmenters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B.-C</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-C. Frank</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1992" to="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Curriculum domain adaptation for semantic segmentation of urban scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2020" to="2030" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Road: Reality oriented adaptation for semantic segmentation of urban scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7892" to="7901" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">Segformer: Simple and efficient design for semantic segmentation with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Anandkumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page">90</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">Self-training and adversarial background regularization for unsupervised domain adaptive one-stage object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6092" to="6101" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<monogr>
		<title level="m" type="main">Before that, he received a B.Sc. degree in Jilin University in 2019. His current research interests focus on computer vision, scene understanding, domain adaptation. He serves as the reviewer of IEEE TPAMI</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ieee Tip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cvpr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaai</forename><surname>Eccv</surname></persName>
		</author>
		<imprint/>
		<respStmt>
			<orgName>Qianyu Zhou is currently pursuing his Ph.D. degree in the Department of Computer Science and Engineering, Shanghai Jiao Tong University</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">Before that, he received a B.E. degree in information se</title>
	</analytic>
	<monogr>
		<title level="m">2020. His current research interests focus on pattern recognition with limited human supervision</title>
		<meeting><address><addrLine>Weihai, China</addrLine></address></meeting>
		<imprint/>
		<respStmt>
			<orgName>Department of Computer Science and Engineering, Shanghai Jiao Tong University ; Harbin Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note>Zhengyang Feng is currently pursuing his M.Sc. degree in the</note>
</biblStruct>

<biblStruct xml:id="b98">
	<monogr>
		<title level="m" type="main">Her current research interests focus on domain adaptation of object detection and semantic segmentation</title>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
		<respStmt>
			<orgName>Qiqi Gu received a MA.Eng. degree in Department of Computer Science and Engineering, Shanghai Jiao Tong University</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<monogr>
		<title level="m" type="main">Jiangmiao Pang is currently a research scientist at Shanghai AI Laboratory. He obtained his Ph.D. degree from Zhejiang University in 2021, and did his postdoc at MMLab, The Chinese University of Hongkong, afterwards. His research interests include computer vision and robotics</title>
		<imprint/>
	</monogr>
	<note>especially their applications in autonomous driving</note>
</biblStruct>

<biblStruct xml:id="b100">
	<analytic>
		<title level="a" type="main">Chinese Academy of Sciences, China, and he received his Ph.D. degree with national laboratory of pattern recognition (NLPR) from the Institute of Automation</title>
	</analytic>
	<monogr>
		<title level="j">Chinese Academy of Sciences</title>
		<imprint/>
	</monogr>
	<note>His research interests include autonomous driving, scene understanding. domain adaptation and remote sensing image processing</note>
</biblStruct>

<biblStruct xml:id="b101">
	<monogr>
		<title level="m" type="main">His research interests mainly fall into the category of visual computing, for example, geometry modeling, processing and analysis, animation/simulation, 2D data processing and analysis</title>
		<ptr target="http://www.xuequanlu.com" />
		<imprint>
			<date type="published" when="2016-06" />
		</imprint>
		<respStmt>
			<orgName>Xuequan Lu is an Assistant Professor at the School of Information Technology, Deakin University</orgName>
		</respStmt>
	</monogr>
	<note>Australia. He spent more than two years as a Research Fellow in Singapore. Prior to that, he earned his Ph.D at Zhejiang University (China</note>
</biblStruct>

<biblStruct xml:id="b102">
	<analytic>
		<title level="a" type="main">Currently her team works on developing algorithms for autonomous driving, scene understanding, remote sensing, etc. She got her Ph.D. degree in Computer Science and Engineering Department in the Chinese University of Hong Kong in 2015 under the supervision of Prof. Jiaya Jia. Before that, she received the B. Eng degree from Zhejiang University in 2011</title>
	</analytic>
	<monogr>
		<title level="m">She has served regularly on the organization committees of numerous conferences</title>
		<imprint/>
	</monogr>
	<note>Jianping Shi is an Executive Research Director at SenseTime. such as Area Chair of CVPR, ICCV, etc</note>
</biblStruct>

<biblStruct xml:id="b103">
	<analytic>
		<title level="a" type="main">He received his B.S. and Ph.D. degrees from the Zhejiang University, China in 1985 and 1991, respectively. He was also a Visiting Professor at the Frounhofer IGD, Darmstadt, Germany in 1998, and was a Visiting Professor at the Center for Advanced Media Technology</title>
	</analytic>
	<monogr>
		<title level="m">Lizhuang Ma is now a Distinguished Professor, Ph.D. Tutor, and the Head of the Digital Media and</title>
		<meeting><address><addrLine>China; Singapore</addrLine></address></meeting>
		<imprint>
			<publisher>CAD/CAM</publisher>
			<date type="published" when="1999" />
		</imprint>
		<respStmt>
			<orgName>Computer Vision Laboratory at the Department of Computer Science and Engineering, Shanghai Jiao Tong University ; Nanyang Technological University</orgName>
		</respStmt>
	</monogr>
	<note>He has published more than 200 academic research papers in both domestic and international journals. His research interests include computer aided geometric design, computer graphics, computer vision, scientific data visualization, computer animation</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
