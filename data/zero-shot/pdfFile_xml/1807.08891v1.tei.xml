<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Skin Lesion Segmentation Using Atrous Convolution via DeepLab v3</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2018-07">July 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujie</forename><surname>Wang</surname></persName>
							<email>wangyuji431@gmail.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>School, MI</roleName><forename type="first">? -Troy</forename><surname>High</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu -Troy</forename><surname>Jahow</surname></persName>
							<email>jahowyu@gmail.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>School, MI</roleName><surname>High</surname></persName>
						</author>
						<title level="a" type="main">Skin Lesion Segmentation Using Atrous Convolution via DeepLab v3</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2018-07">July 2018</date>
						</imprint>
					</monogr>
					<note>Simon Sun Dr. Limin Yu -Beaumont Health, limin.yu@beaumont.edu ? Authors contributed equally</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T18:22+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>As melanoma diagnoses increase across the US, automated efforts to identify malignant lesions become increasingly of interest to the research community. Segmentation of dermoscopic images is the first step in this process, thus accuracy is crucial. Although techniques utilizing convolutional neural networks have been used in the past for lesion segmentation, we present a solution employing the recently published DeepLab 3, an atrous convolution method for image segmentation. Although the results produced by this run are not ideal, with a mean Jaccard index of 0.498, we believe that with further adjustments and modifications to the compatibility with the DeepLab code and with training on more powerful processing units, this method may achieve better results in future trials.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>More than 5 million cases of skin cancer are diagnosed in the United States each year, surpassing diagnoses of all other cancers combined. As new diagnoses have increased by 53% in the last decade, it is imperative that medical professionals are able to properly recognize and identify malignant lesions. Dermatologists frequently rely on techniques such as the ABCDE rule (checking if the lesion is Asymmetric, has an irregular Border, has a uniform Color, whether the Diameter is greater than 6 mm, and whether the lesion Evolves and changes over time) to accomplish this task. However, with the advances of image processing and machine learning technologies, one growing interest among the research community is automatic lesion segmentation, feature classification, and disease classification.</p><p>The International Skin Imaging Collaboration (ISIC), sponsored by the International Society for Digital Imaging of the Skin, has hosted its annual challenge since 2016, divided among those three sub-tasks. As a part of the 2018 challenge, we present a lesion segmentation technique using DeepLab 3, a semantic image segmentation model utilizing atrous convolution in convolutional neural networks (CNNs), implemented in Google's Tensorflow framework. As segmentation is the first step in diagnosis, it is especially important that accuracy is maintained-many structural factors of diagnosis, such as asymmetry and border shape, are extracted from this phase.</p><p>Neural networks offer themselves as a natural solution to the image segmentation problem. A key fixture among machine learning applications, neural networks are composed of a number of hidden layers, an input layer, and an output layer, with each layer composed of an adjustable amount of nodes. In supervised learning, which our method uses, the network is fed training data, the results of which are compared to a given ground truth. Based on the error between the expected values of the output nodes and the returned values, the relations between each node are adjusted.This repeated process continues as the network approaches its maximum accuracy. In a sense, we can treat neural networks as a black box, only worrying about certain hyperparameters such as the number and type of layers, number of times the test data is run, batch size, and learning rate. Convolutional neural networks (CNNs) are a special type of neural network, most often used in image processing. These networks have two specialized kinds of layers, namely convolution layers and pooling layers, as well as a fully-connected layer. Convolution layers apply a filter to the input matrix to output a feature map. These layers are then followed by any number of pooling layers, which retains the key features while reducing the dimensionality of the input. Finally, the fully-connected layer relate the extracted features to the various traits we are hoping to classify. The number, order, and combinations of all these layers are adjustable and thus affect the overall effectiveness of the CNN.</p><p>The top submission from the 2017 edition of the ISIC challenge used CNN variations to produce segmentation predictions with an average Jaccard index of 0.765. Our approach aims to accomplish the goal using the 2017 release of DeepLab 3. This uses atrous convolution, in which the filter is not applied to all adjacent pixels of an image but rather a spaced-out lattice of pixels. This allows the resulting feature map to retain spacial resolution without significant cost to computation time. DeepLab uses several of these convolutions in cascade, followed by a "Pyramid Pooling" method in which four parallel atrous convolutions are applied to the input, each with differing atrous rates. Using these strategies, DeepLab 3 was able to perform significantly better than previous DeepLab versions and had comparable performances to other methods on the PASCAL VOC 2012 dataset.</p><p>Previous lesion segmentation methods have employed CNNs, but have not utilized the architecture of DeepLab 3. For instance, the 2016 effort by Mohammad H. Jafari et al. utilized two parallel paths of calculation, one for local analysis and one for general analysis, each with two pairs of convolution and a max pooling layer. These two paths are then followed by a fully-connected layer before the output layer. This method was able to achieve 98% accuracy on the Dermquest dataset, better than other algorithms such as Otsu.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Approach</head><p>Our method was trained and tested on the ISIC Archive dataset, which contains quality-controlled dermoscopic images of lesions. This means that there is little if any preprocessing that needs to be done on the input images before entering into the CNN. Additionally, each training data item is provided with a ground truth image, so computing error and performing stochastic descent is made easier.</p><p>In order to make the dataset compatible with the DeepLab code, the images were resized to 513x513 and then converted into TFRecord. Various options regarding network model were available, including ResNet and Xception. Additionally, various hyperparameters were adjustable, including weighting of foreground and background, learning rate, and learning rate decay. Most of the challenge was involved in adapting the dataset to successfully train the model. As we wished to use the existing DeepLab code, our efforts went into adjusting and debugging the code to allow the model to accept the ISIC images. We received multiple errors on this front, but for our most successful run, we initialized using the weights in the checkpoint used to segment the Pascal dataset with 0.001 learning rate and foreground loss weight 100 for 100 steps. We believe that training for more steps would help us achieve better results.</p><p>In order to evaluate the model, we initially worked to run the built in evaluate function contained in DeepLab used to evaluate the example models on our trained dataset. However, after various errors, we decided to write our own evaluation script to calculate a Jaccard value for each image by counting the tuples of pixels of each color in the prediction and ground truth images. We ran this on 963 ISIC images to give us generalized data about the model.</p><p>In order to finalize the prediction images to fit the ISIC challenge criteria, we wrote a script to recolor the prediction images and upscaled them back to the original size using OpenCV. This does result in more pixelated images, but we believe that using a more powerful computer may allow running the neural network on larger image sizes, which may improve accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Results</head><p>In our most successful run, we achieved a mean Jaccard value of 0.498, median Jaccard value of 0.500, and standard deviation of 0.273. Unfortunately, ISIC considers Jaccard values under 0.650 to be unsegmented, meaning that overall, most of the images are unsuccessful. Of 963 images we evaluated on, 335 pass this threshold, giving us a 34.787% success rate using this method.</p><p>From a subjective standpoint, we noticed that our neural network worked best on images that were clear, had even lighting, and were free of other objects such as hair or follicles. Further training may improve these results. We believe that with further compatibility with the DeepLab framework, training for more steps (say, 100,000), and more time to allow tweaking of hyperparameters, perhaps using stochastic gradient descent, that the results may improve to an acceptable successful result. Much of the error in our result we believe to be a consequence of our implementation rather than the method itself.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>To segment skin lesions for the 2018 ISIC challenge, we attempted an approach that used atrous convolution in a convolutional neural network. Specifically, we adapted the DeepLab v3 code to formulate and train a model. Despite many compatibility issues, we ultimately succeeded in running the dataset through it. While the results leave much room for improvement, with mean Jaccard value of 0.498, we are optimistic that further testing can improve the segmentation success rate to a favorable level.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>A sampling of segmentation prediction results (left of image) compared to ground truths (right). Jaccard indexes from top to bottom: 0.943, 0.875, 0.271, 0.527</figDesc></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Rethinking Atrous Convolution for Semantic Image Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.05587</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Hosted by the International Skin Imaging Collaboration (ISIC)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">F</forename><surname>Noel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Codella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Emre</forename><surname>Gutman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Celebi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">A</forename><surname>Helba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><forename type="middle">W</forename><surname>Marchetti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aadi</forename><surname>Dusza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantinos</forename><surname>Kalloo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nabin</forename><surname>Liopyris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harald</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Allan</forename><surname>Kittler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Halpern</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.05006</idno>
	</analytic>
	<monogr>
		<title level="m">Skin Lesion Analysis Toward Melanoma Detection: A Challenge at the 2017 International Symposium on Biomedical Imaging (ISBI)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Extraction of Skin Lesions from Non-Dermoscopic Images Using Deep Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Mohammad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ebrahim</forename><surname>Jafari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nader</forename><surname>Nasr-Esfahani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Karimi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shadrokh</forename><surname>Soroushmehr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kayvan</forename><surname>Samavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Najarian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02374</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The HAM10000 dataset, a large collection of multi-source dermatoscopic images of common pigmented skin lesions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tschandl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rosendahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kittler</surname></persName>
		</author>
		<idno type="DOI">10.1038/sdata.2018.161</idno>
	</analytic>
	<monogr>
		<title level="j">Sci. Data</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">180161</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title/>
		<ptr target="https://www.skincancer.org/skin-cancer-information/skin-cancer-facts" />
	</analytic>
	<monogr>
		<title level="j">The Skin Cancer Foundation</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
