<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Searching Central Difference Convolutional Networks for Face Anti-Spoofing</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zitong</forename><surname>Yu</surname></persName>
							<email>zitong.yu@oulu.fi</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">CMVS</orgName>
								<orgName type="institution" key="instit2">University of Oulu</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxu</forename><surname>Zhao</surname></persName>
							<email>zhaochenxu@mininglamp.com</email>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">Mininglamp Academy of Sciences</orgName>
								<orgName type="institution" key="instit2">Mininglamp Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zezheng</forename><surname>Wang</surname></persName>
							<affiliation key="aff2">
								<address>
									<settlement>Aibee</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunxiao</forename><surname>Qin</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">Northwestern Polytechnical University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuo</forename><surname>Su</surname></persName>
							<email>zhuo.su@oulu.fi</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">CMVS</orgName>
								<orgName type="institution" key="instit2">University of Oulu</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaobai</forename><surname>Li</surname></persName>
							<email>xiaobai.li@oulu.fi</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">CMVS</orgName>
								<orgName type="institution" key="instit2">University of Oulu</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Zhou</surname></persName>
							<email>fzhoug@aibee.com</email>
							<affiliation key="aff2">
								<address>
									<settlement>Aibee</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoying</forename><surname>Zhao</surname></persName>
							<email>guoying.zhao@oulu.fi</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">CMVS</orgName>
								<orgName type="institution" key="instit2">University of Oulu</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Searching Central Difference Convolutional Networks for Face Anti-Spoofing</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T02:14+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Face anti-spoofing (FAS) plays a vital role in face recognition systems. Most state-of-the-art FAS methods 1) rely on stacked convolutions and expert-designed network, which is weak in describing detailed fine-grained information and easily being ineffective when the environment varies (e.g., different illumination), and 2) prefer to use long sequence as input to extract dynamic features, making them difficult to deploy into scenarios which need quick response. Here we propose a novel frame level FAS method based on Central Difference Convolution (CDC), which is able to capture intrinsic detailed patterns via aggregating both intensity and gradient information. A network built with CDC, called the Central Difference Convolutional Network (CDCN), is able to provide more robust modeling capacity than its counterpart built with vanilla convolution. Furthermore, over a specifically designed CDC search space, Neural Architecture Search (NAS) is utilized to discover a more powerful network structure (CDCN++), which can be assembled with Multiscale Attention Fusion Module (MAFM) for further boosting performance. Comprehensive experiments are performed on six benchmark datasets to show that 1) the proposed method not only achieves superior performance on intra-dataset testing (especially 0.2% ACER in Protocol-1 of OULU-NPU dataset), 2) it also generalizes well on cross-dataset testing (particularly 6.5% HTER from CASIA-MFSD to Replay-Attack datasets).</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Face recognition has been widely used in many interactive artificial intelligence systems for its convenience. However, vulnerability to presentation attacks (PA) curtail its reliable deployment. Merely presenting printed images or videos to the biometric sensor could fool face recognition systems. Typical examples of presentation attacks are print, video replay, and 3D masks. For the reliable use of face recognition systems, face anti-spoofing (FAS) methods are important to detect such presentation attacks.</p><p>In recent years, several hand-crafted features based <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b43">44]</ref> and deep learning based <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b63">64,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b61">62,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20]</ref> methods have been proposed for presentation attack detection (PAD). On one hand, the classical handcrafted descriptors (e.g., local binary pattern (LBP) <ref type="bibr" target="#b6">[7]</ref>) leverage local relationship among the neighbours as the discriminative features, which is robust for describing the detailed invariant information (e.g., color texture, moir? pattern and noise artifacts) between the living and spoofing faces. On the other hand, due to the stacked convolution operations with nonlinear activation, the convolutional neural networks (CNN) hold strong representation abilities to distinguish the bona fide and PA. However, CNN based methods focus on the deeper semantic features, which are weak in describing detailed fine-grained information between liv-ing and spoofing faces and easily being ineffective when the environment varies (e.g., different light illumination). How to integrate local descriptors with convolution operation for robust feature representation is worth exploring.</p><p>Most recent deep learning based FAS methods are usually built upon image classification task based backbones <ref type="bibr" target="#b60">[61,</ref><ref type="bibr" target="#b61">62,</ref><ref type="bibr" target="#b19">20]</ref>, such as VGG <ref type="bibr" target="#b53">[54]</ref>, ResNet <ref type="bibr" target="#b21">[22]</ref> and DenseNet <ref type="bibr" target="#b22">[23]</ref>. The networks are usually supervised by binary cross-entropy loss, which easily learns the arbitrary patterns such as screen bezel instead of the nature of spoofing patterns. In order to solve this issue, several depth supervised FAS methods <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b35">36]</ref>, which utilize pseudo depth map label as auxiliary supervised signal, have been developed. However, all these network architectures are carefully designed by human experts, which might not be optimal for FAS task. Hence, to automatically discover best-suited networks for FAS task with auxiliary depth supervision should be considered.</p><p>Most existing state-of-the-art FAS methods <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b61">62,</ref><ref type="bibr" target="#b31">32]</ref> need multiple frames as input to extract dynamic spatiotemporal features (e.g., motion <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b55">56]</ref> and rPPG <ref type="bibr" target="#b61">[62,</ref><ref type="bibr" target="#b31">32]</ref>) for PAD. However, long video sequence may not be suitable for specific deployment conditions where the decision needs to be made quickly. Hence, frame level PAD approaches are advantageous from the usability point of view despite inferior performance compared with video level methods. To design high-performing frame level methods is crucial for real-world FAS applications.</p><p>Motivated by the discussions above, we propose a novel convolution operator called Central Difference Convolution (CDC), which is good at describing fine-grained invariant information. As shown in <ref type="figure" target="#fig_0">Fig. 1</ref>, CDC is more likely to extract intrinsic spoofing patterns (e.g., lattice artifacts) than vanilla convolution in diverse environments. Furthermore, over a specifically designed CDC search space, Neural Architecture Search (NAS) is utilized to discover the excellent frame level networks for depth supervised face antispoofing task. Our contributions include:</p><p>? We design a novel convolution operator called Central Difference Convolution (CDC), which is suitable for FAS task due to its remarkable representation ability for invariant fine-grained features in diverse environments. Without introducing any extra parameters, CDC can replace the vanilla convolution and plug and play in existing neural networks to form Central Difference Convolutional Networks (CDCN) with more robust modeling capacity. ? We propose CDCN++, an extended version of CDCN, consisting of the searched backbone network and Multiscale Attention Fusion Module (MAFM) for aggregating the multi-level CDC features effectively. ? To our best knowledge, this is the first approach that searches neural architectures for FAS task. Different from the previous classification task based NAS supervised by softmax loss, we search the well-suited frame level networks for depth supervised FAS task over a specifically designed CDC search space.</p><p>? Our proposed method achieves state-of-the-art performance on all six benchmark datasets with both intraas well as cross-dataset testing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Face Anti-Spoofing. Traditional face anti-spoofing methods usually extract hand-crafted features from the facial images to capture the spoofing patterns. Several classical local descriptors such as LBP <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b14">15]</ref>, SIFT <ref type="bibr" target="#b43">[44]</ref>, SURF <ref type="bibr" target="#b8">[9]</ref>, HOG <ref type="bibr" target="#b28">[29]</ref> and DoG <ref type="bibr" target="#b44">[45]</ref> are utilized to extract frame level features while video level methods usually capture dynamic clues like dynamic texture <ref type="bibr" target="#b27">[28]</ref>, micro-motion <ref type="bibr" target="#b52">[53]</ref> and eye blinking <ref type="bibr" target="#b40">[41]</ref>. More recently, a few deep learning based methods are proposed for both frame level and video level face anti-spoofing. For frame level methods <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b25">26]</ref>, pre-trained deep CNN models are fine-tuned to extract features in a binary-classification setting. In contrast, auxiliary depth supervised FAS methods <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b35">36]</ref> are introduced to learn more detailed information effectively. On the other hand, several video level CNN methods are presented to exploit the dynamic spatio-temporal <ref type="bibr" target="#b55">[56,</ref><ref type="bibr" target="#b61">62,</ref><ref type="bibr" target="#b32">33]</ref> or rPPG <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b31">32]</ref> features for PAD. Despite achieving state-of-the-art performance, video level deep learning based methods need long sequence as input. In addition, compared with traditional descriptors, CNN overfits easily and is hard to generalize well on unseen scenes. Convolution Operators. The convolution operator is commonly used in extracting basic visual features in deep learning framework. Recently extensions to the vanilla convolution operator have been proposed. In one direction, classical local descriptors (e.g., LBP <ref type="bibr" target="#b1">[2]</ref> and Gabor filters <ref type="bibr" target="#b24">[25]</ref>) are considered into convolution design. Representative works include Local Binary Convolution <ref type="bibr" target="#b26">[27]</ref> and Gabor Convolution <ref type="bibr" target="#b37">[38]</ref>, which is proposed for saving computational cost and enhancing the resistance to the spatial changes, respectively. Another direction is to modify the spatial scope for aggregation. Two related works are dialated convolution <ref type="bibr" target="#b62">[63]</ref> and deformable convolution <ref type="bibr" target="#b13">[14]</ref>. However, these convolution operators may not be suitable for FAS task because of the limited representation capacity for invariant fine-grained features. Neural Architecture Search. Our work is motivated by recent researches on NAS <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr">35,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b68">68,</ref><ref type="bibr" target="#b69">69,</ref><ref type="bibr" target="#b59">60]</ref>, while we focus on searching for a depth supervised model with high performance instead of a binary classification model for face anti-spoofing task. There are three main categories of existing NAS methods: 1) Reinforcement learning based <ref type="bibr" target="#b68">[68,</ref><ref type="bibr" target="#b69">69]</ref>, 2) Evolution algorithm based <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b51">52]</ref>, and 3) Gradient based [35, <ref type="bibr" target="#b59">60,</ref><ref type="bibr" target="#b11">12]</ref>. Most of NAS approaches search networks on a small proxy task and transfer the found architecture to another large target task. For the perspective of computer vision applications, NAS has been developed for face recognition <ref type="bibr" target="#b67">[67]</ref>, action recognition <ref type="bibr" target="#b45">[46]</ref>, person ReID <ref type="bibr" target="#b49">[50]</ref>, object detection <ref type="bibr" target="#b20">[21]</ref> and segmentation <ref type="bibr" target="#b65">[65]</ref> tasks. To the best of our knowledge, no NAS based method has ever been proposed for face anti-spoofing task. In order to overcome the above-mentioned drawbacks and fill in the blank, we search the frame level CNN over a specially designed search space with the new proposed convolution operator for depth-supervised FAS task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head><p>In this section, we will first introduce our Central Difference Convolution in Section 3.1, then introduce the Central Difference Convolutional Networks (CDCN) for face antispoofing in Section 3.2, and at last present the searched networks with attention mechanism (CDCN++) in Section 3.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Central Difference Convolution</head><p>In modern deep learning frameworks, the feature maps and convolution are represented in 3D shape (2D spatial domain and extra channel dimension). As the convolution operation remains the same across the channel dimension, for simplicity, in this subsection the convolutions are described in 2D while extension to 3D is straightforward.</p><p>Vanilla Convolution. As 2D spatial convolution is the basic operation in CNN for vision tasks, here we denote it as vanilla convolution and review it shortly first. There are two main steps in the 2D convolution: 1) sampling local receptive field region R over the input feature map x; 2) aggregation of sampled values via weighted summation. Hence, the output feature map y can be formulated as</p><formula xml:id="formula_0">y(p 0 ) = pn?R w(p n ) ? x(p 0 + p n ),<label>(1)</label></formula><p>where p 0 denotes current location on both input and output feature maps while p n enumerates the locations in R. For instance, local receptive field region for convolution operation with 3?3 kernel and dilation 1 is R = {(?1, ?1), (?1, 0), ? ? ? , (0, 1), (1, 1)}. Vanilla Convolution Meets Central Difference. Inspired by the famous local binary pattern (LBP) <ref type="bibr" target="#b6">[7]</ref> which describes local relations in a binary central difference way, we also introduce central difference into vanilla convolution to enhance its representation and generalization capacity. Similarly, central difference convolution also consists of two steps, i.e., sampling and aggregation. The sampling step is similar to that in vanilla convolution while the aggregation step is different: as illustrated in <ref type="figure">Fig</ref> </p><formula xml:id="formula_1">y(p 0 ) = pn?R w(p n ) ? (x(p 0 + p n ) ? x(p 0 )).<label>(2)</label></formula><p>When p n = (0, 0), the gradient value always equals to zero with respect to the central location p 0 itself. For face anti-spoofing task, both the intensity-level semantic information and gradient-level detailed message are crucial for distinguishing the living and spoofing faces, which indicates that combining vanilla convolution with central difference convolution might be a feasible manner to provide more robust modeling capacity. Therefore we generalize central difference convolution as</p><formula xml:id="formula_2">y(p 0 ) = ? ? pn?R w(p n ) ? (x(p 0 + p n ) ? x(p 0 )) central difference convolution +(1 ? ?) ? pn?R w(p n ) ? x(p 0 + p n ) vanilla convolution ,<label>(3)</label></formula><p>where hyperparameter ? ? [0, 1] tradeoffs the contribution between intensity-level and gradient-level information. The higher value of ? means the more importance of central difference gradient information. We will henceforth refer to this generalized Central Difference Convolution as CDC, which should be easy to identify according to its context. Implementation for CDC. In order to efficiently implement CDC in modern deep learning framework, we decompose and merge Eq. (3) into the vanilla convolution with additional central difference term</p><formula xml:id="formula_3">y(p 0 ) = pn?R w(p n ) ? x(p 0 + p n ) vanilla convolution +? ? (?x(p 0 ) ? pn?R w(p n )) central difference term .<label>(4)</label></formula><p>According to the Eq. (4), CDC can be easily implemented by a few lines of code in PyTorch <ref type="bibr" target="#b41">[42]</ref> and TensorFlow <ref type="bibr" target="#b0">[1]</ref>. The derivation of Eq. (4) and codes based on Pytorch are shown in Appendix A.</p><p>Relation to Prior Work. Here we discuss the relations between CDC and vanilla convolution, local binary convolution <ref type="bibr" target="#b26">[27]</ref> and gabor convolution <ref type="bibr" target="#b37">[38]</ref>, which share similar design philosophy but with different focuses. The ablation study is in Section 4.3 to show superior performance of CDC for face anti-spoofing task.</p><p>Relation to Vanilla Convolution. CDC is more generalized. It can be seen from Eq. (3) that vanilla convolution is a special case of CDC when ? = 0, i.e., aggregating local intensity information without gradient message.</p><p>Relation to Local Binary Convolution <ref type="bibr" target="#b26">[27]</ref>. Local binary convolution (LBConv) focuses on computational reduction so its modeling capacity is limited. CDC focuses on enhancing rich detailed feature representation capacity without any additional parameters. On the other side, LBConv uses pre-defined filters to describe the local feature relation while CDC can learn these filters automatically.</p><p>Relation to Gabor Convolution <ref type="bibr" target="#b37">[38]</ref>. Gabor convolution (GaborConv) devotes to enhancing the representation capacity of spatial transformations (i.e., orientation and scale changes) while CDC focuses more on representing finegrained robust features in diverse environments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">CDCN</head><p>Depth-supervised face anti-spoofing methods <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b3">4]</ref> take advantage of the discrimination between spoofing and living faces based on 3D shape, and provide pixel-wise detailed information for the FAS model to capture spoofing cues. Motivated by this, a similar depth-supervised network <ref type="bibr" target="#b35">[36]</ref> called "DepthNet" is built up as baseline in this paper. In order to extract more fine-grained and robust features for estimating the facial depth map, CDC is introduced to form Central Difference Convolutional Networks (CDCN). Note that DepthNet is the special case of the proposed CDCN when ? = 0 for all CDC operators.</p><p>The details of CDCN are shown in <ref type="table" target="#tab_0">Table 1</ref>. Given a single RGB facial image with size 3 ? 256 ? 256, multilevel (low-level, mid-level and high-level) fused features are extracted for predicting the grayscale facial depth with size 32 ? 32. We use ? = 0.7 as the default setting and ablation study about ? will be shown in Section 4.3.</p><p>For the loss function, mean square error loss L M SE is utilized for pixel-wise supervision. Moreover, for the sake of fine-grained supervision needs in FAS task, contrastive depth loss L CDL <ref type="bibr" target="#b55">[56]</ref> is considered to help the networks learn more detailed features. So the overall loss L overall can be formulated as L overall = L M SE + L CDL .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">CDCN++</head><p>It can be seen from <ref type="table" target="#tab_0">Table 1</ref> that the architecture of CDCN is designed coarsely (e.g., simply repeating the same block structure for different levels), which might be suboptimized for face anti-spoofing task. Inspired by the classical visual object understanding models <ref type="bibr" target="#b39">[40]</ref>, we propose an extended version CDCN++ (see <ref type="figure" target="#fig_7">Fig. 5</ref>), which consists of a NAS based backbone and Multiscale Attention Fusion</p><formula xml:id="formula_4">Level Output DepthNet [36] CDCN (? = 0.7) 256 ? 256 3 ? 3 conv, 64 3 ? 3 CDC, 64</formula><p>Low  <ref type="bibr" target="#b59">60]</ref>, and more technical details can be referred to the original papers. Here we mainly state the new contributions about searching backbone for FAS task.</p><formula xml:id="formula_5">128 ? 128 ? ? ? ? 3 ? 3 conv, 128 3 ? 3 conv, 196 3 ? 3 conv, 128 3 ? 3 max pool ? ? ? ? ? ? ? ? 3 ? 3 CDC, 128 3 ? 3 CDC, 196 3 ? 3 CDC, 128 3 ? 3 max pool ? ? ? ? Mid 64 ? 64 ? ? ? ? 3 ? 3 conv, 128 3 ? 3 conv, 196 3 ? 3 conv, 128 3 ? 3 max pool ? ? ? ? ? ? ? ? 3 ? 3 CDC, 128 3 ? 3 CDC, 196 3 ? 3 CDC, 128 3 ? 3 max pool ? ? ? ? High 32 ? 32 ? ? ? ? 3 ? 3 conv, 128 3 ? 3 conv, 196 3 ? 3 conv, 128 3 ? 3 max pool ? ? ? ? ? ? ? ? 3 ? 3 CDC, 128 3 ? 3 CDC, 196 3 ? 3 CDC, 128 3 ? 3 max pool ? ? ? ? 32 ? 32 [concat (Low, Mid, High), 384] 32 ? 32 ? ? 3 ? 3 conv, 128 3 ? 3 conv, 64 3 ? 3 conv, 1 ? ? ? ? 3 ? 3 CDC, 128 3 ? 3 CDC, 64 3 ? 3 CDC, 1 ? ? # params 2.25 ? 10 6 2.25 ? 10 6</formula><p>As illustrated in <ref type="figure" target="#fig_2">Fig. 3(a)</ref>, the goal is to search for cells in three levels (low-level, mid-level and high-level) to form a network backbone for FAS task. Inspired by the dedicated neurons for hierarchical organization in human visual system <ref type="bibr" target="#b39">[40]</ref>, we prefer to search these multi-level cells freely (i.e., cells with varied structures), which is more flexible and generalized. We name this configuration as "Varied Cells" and will study its impacts in Sec. 4.3 (see Tab. 2). Different from previous works <ref type="bibr">[35,</ref><ref type="bibr" target="#b59">60]</ref>, we adopt only one output of the latest incoming cell as the input of the current cell.</p><p>As for the cell-level structure, <ref type="figure" target="#fig_2">Fig. 3(b)</ref> shows that each cell is represented as a directed acyclic graph (DAG) of N nodes {x} N ?1 i=0 , where each node represents a network layer. We denote the operation space as O, and <ref type="figure" target="#fig_2">Fig. 3(c)</ref> shows eight designed candidate operations (none, skip-connect and CDCs). Each edge (i, j) of DAG represents the information flow from node x i to node x j , which consists of the candidate operations weighted by the architecture parameter ? (i,j) . Specially, each edge (i, j) can be formulated by a function? <ref type="bibr">(i,j)</ref> </p><formula xml:id="formula_6">where? (i,j) (x i ) = o?O ? (i,j) o ? o(x i ). Softmax function is utilized to relax architecture parame- ter ? (i,j) into operation weight o ? O, that is ? (i,j) o = exp(? (i,j) o ) o ?O exp(? (i,j) o )</formula><p>. The intermediate node can be denoted as x j = i&lt;j? (i,j) (x i ) and the output node x N ?1 is represented by weighted summation of all intermediate nodes</p><formula xml:id="formula_7">x N ?1 = 0&lt;i&lt;N ?1 ? i (x i ).</formula><p>Here we propose a node at- In the searching stage, L train and L val are denoted as the training and validation loss respectively, which are all based on the depth-supervised loss L overall described in Sec. 3.2. Network parameters w and architecture parameters ? are learned with the following bi-level optimization problem:</p><formula xml:id="formula_8">min ? L val (w * (?), ?), s.t. w * (?) = arg min w L train (w, ?)<label>(5)</label></formula><p>After convergence, the final discrete architecture is de- , and 3) for each output node, choosing the one incoming intermediate node with largest value of max 0&lt;i&lt;N ?1 ? i (denoted as "Node Attention") as input. In contrast, choosing last intermediate node as output node is more straightforward. We will compare these two settings in Sec. 4.3 (see Tab. 2).</p><formula xml:id="formula_9">rived by: 1) setting o (i,j) = arg max o?O,o =none p (i,j) o</formula><p>MAFM. Although simply fusing low-mid-high levels features can boost performance for the searched CDC architecture, it is still hard to find the important regions to focus, which goes against learning more discriminative features. Inspired by the selective attention in human visual system  [ <ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b54">55]</ref>, neurons at different levels are likely to have stimuli in their receptive fields with various attention. Here we propose a Multiscale Attention Fusion Module (MAFM), which is able to refine and fuse low-mid-high levels CDC features via spatial attention.</p><p>As illustrated in <ref type="figure" target="#fig_5">Fig. 4</ref>, features F from different levels are refined via spatial attention <ref type="bibr" target="#b57">[58]</ref> with receptive fields related kernel size (i.e., the high/semantic level should be with small attention kernel size while low level with large attention kernel size in our case) and then concatenate together. The refined features F can be formulated as</p><formula xml:id="formula_10">F i = F i (?(C i ([A(F i ), M(F i )]))), i ? {low, mid, high} ,<label>(6)</label></formula><p>where represents Hadamard product. A and M denotes avg and max pool layer respectively. ? means the sigmoid function while C is the convolution layer. Vanilla convolutions with 7?7, 5?5 and 3?3 kernels are utilized for C low , C mid and C high , respectively. CDC is not chosen here because of its limited capacity of global semantic cognition, which is vital in spatial attention. The corresponding ablation study is conducted in Sec. 4.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, extensive experiments are performed to demonstrate the effectiveness of our method. In the following, we sequentially describe the employed datasets &amp; metrics (Sec </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets and Metrics</head><p>Databases. Six databases OULU-NPU <ref type="bibr" target="#b9">[10]</ref>, SiW <ref type="bibr" target="#b35">[36]</ref>, CASIA-MFSD <ref type="bibr" target="#b66">[66]</ref>, Replay-Attack <ref type="bibr" target="#b12">[13]</ref>, MSU-MFSD <ref type="bibr" target="#b56">[57]</ref> and SiW-M <ref type="bibr" target="#b36">[37]</ref> are used in our experiments. OULU-NPU and SiW are high-resolution databases, containing four and three protocols to validate the generalization (e.g., unseen illumination and attack medium) of models respectively, which are utilized for intra testing. CASIA-MFSD, Replay-Attack and MSU-MFSD are databases which contain lowresolution videos, which are used for cross testing. SiW-M is designed for cross-type testing for unseen attacks as there are rich <ref type="formula" target="#formula_0">(13)</ref>   Performance Metrics. In OULU-NPU and SiW dataset, we follow the original protocols and metrics, i.e., Attack Presentation Classification Error Rate (APCER), Bona Fide Presentation Classification Error Rate (BPCER), and ACER <ref type="bibr" target="#b23">[24]</ref> for a fair comparison. Half Total Error Rate (HTER) is adopted in the cross testing between CASIA-MFSD and Replay-Attack. Area Under Curve (AUC) is utilized for intra-database cross-type test on CASIA-MFSD, Replay-Attack and MSU-MFSD. For the cross-type test on SiW-M, APCER, BPCER, ACER and Equal Error Rate (EER) are employed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation Details</head><p>Depth Generation. Dense face alignment PRNet <ref type="bibr" target="#b17">[18]</ref> is adopted to estimate the 3D shape of the living face and generate the facial depth map with size 32 ? 32. More details and samples can be found in <ref type="bibr" target="#b55">[56]</ref>. To distinguish living faces from spoofing faces, at the training stage, we normalize living depth map in a range of [0, 1], while setting spoofing depth map to 0, which is similar to <ref type="bibr" target="#b35">[36]</ref>.</p><p>Training and Testing Setting. Our proposed method is implemented with Pytorch. In the training stage, models are trained with Adam optimizer and the initial learning rate (lr) and weight decay (wd) are 1e-4 and 5e-5, respectively. We train models with maximum 1300 epochs while lr halves every 500 epochs. The batch size is 56 on eight 1080Ti GPUs. In the testing stage, we calculate the mean value of the predicted depth map as the final score.</p><p>Searching Setting. Similar to <ref type="bibr" target="#b59">[60]</ref>, partial channel connection and edge normalization are adopted.</p><p>The initial number of channel is sequentially {32, 64, 128, 128, 128, 64, 1} in the network (see <ref type="figure" target="#fig_2">Fig. 3(a)</ref>), which doubles after searching. Adam optimizer with lr=1e-4 and wd=5e-5 is utilized when training the model weights. The architecture parameters are trained with Adam optimizer with lr=6e-4 and wd=1e-3. We search 60 epochs on Protocol-1 of OULU-NPU with batchsize 12 while architecture parameters are not updated in the first 10 epochs. The whole process costs one day on three 1080Ti.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation Study</head><p>In this subsection, all ablation studies are conducted on Protocol-1 (different illumination condition and location between train and test sets) of OULU-NPU <ref type="bibr" target="#b9">[10]</ref> to explore  the details of our proposed CDC, CDCN and CDCN++. Impact of ? in CDCN. According to Eq. (3), ? controls the contribution of the gradient-based details, i.e., the higher ?, the more local detailed information included. As illustrated in <ref type="figure" target="#fig_8">Fig. 6(a)</ref>, when ? 0.3, CDC always achieves better performance than vanilla convolution (? = 0, ACER=3.8%), indicating the central difference based fine-grained information is helpful for FAS task. As the best performance (ACER=1.0%) is obtained when ? = 0.7, we use this setting for the following experiments. Besides keeping the constant ? for all layers, we also explore an adaptive CDC method to learn ? for every layer, which is shown in Appendix B.</p><p>CDC vs. Other Convolutions. As discussed in Sec. 3.1 about the relation between CDC and prior convolutions, we argue that the proposed CDC is more suitable for FAS task as the detailed spoofing artifacts in diverse environments should be represented by the gradient-based invariant features. <ref type="figure" target="#fig_8">Fig. 6(b)</ref> shows that CDC outperforms other convolutions by a large margin (more than 2% ACER). It is interesting to find that LBConv performs better than vanilla convolution, indicating that the local gradient information is important for FAS task. GaborConv performs the worst because it is designed for capturing spatial invariant features, which is not helpful in face anti-spoofing task.</p><p>Impact of NAS Configuration. <ref type="table" target="#tab_2">Table 2</ref> shows the ablation study about the two NAS configurations described in Sec. 3.3, i.e., varied cells and node attention. Compared to the baseline setting with the shared cells and last inter- mediate node as output node, both these two configurations can boost the searching performance. The reason behind is twofold: 1) with more flexible searching constraints, NAS is able to find dedicated cells for different levels, which is more similar to human visual system <ref type="bibr" target="#b39">[40]</ref>, and 2) taking the last intermediate node as output might not be optimal while choosing the most important one is more reasonable. Effectiveness of NAS Based Backbone and MAFM. The proposed CDCN++, consisting of NAS based backbone and MAFM, is shown in <ref type="figure" target="#fig_7">Fig. 5</ref>. It is obvious that cells from multiple levels are quite different and the mid-level cell has deeper (four CDC) layers. <ref type="table" target="#tab_3">Table 3</ref> shows the ablation studies about NAS based backbone and MAFM. It can be seen from the first two rows that NAS based backbone with direct multi-level fusion outperforms (0.3% ACER) the backbone without NAS, indicating the effectiveness of our searched architecture. Meanwhile, backbone with MAFM achieves 0.5% ACER lower than that with direct multi-level fusion, which shows the effectiveness of MAFM. We also analyse the convolution type and kernel size in MAFM and find that vanilla convolution is more suitable for capturing the semantic spatial attention. Besides, the attention kernel size should be large (7x7) and small (3x3) enough for low-level and high-level features, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Intra Testing</head><p>The intra testing is carried out on both the OULU-NPU and the SiW datasets. We strictly follow the four protocols on OULU-NPU and three protocols on SiW for the evaluation. All compared methods including STASN <ref type="bibr" target="#b61">[62]</ref> are trained without extra datasets for a fair comparison.</p><p>Results on OULU-NPU. As shown in <ref type="table">Table 4</ref>, our proposed CDCN++ ranks first on all 4 protocols (0.2%, 1.3%, 1.8% and 5.0% ACER, respectively), which indicates the proposed method performs well at the generalization of the external environment, attack mediums and input camera variation. Unlike other state-of-the-art methods (Auxiliary <ref type="bibr" target="#b35">[36]</ref>, STASN <ref type="bibr" target="#b61">[62]</ref>, GRADIANT <ref type="bibr" target="#b5">[6]</ref> and FAS-TD <ref type="bibr" target="#b55">[56]</ref>) extracting multi-frame dynamic features, our method needs only frame-level inputs, which is suitable for real-world deployment. It's worth noting that the NAS based backbone for CDCN++ is transferable and generalizes well on all protocols although it is searched on Protocol-1.</p><p>Results on SiW. <ref type="table" target="#tab_4">Table 5</ref> compares the performance of our method with three state-of-the-art methods Auxiliary <ref type="bibr" target="#b35">[36]</ref>, STASN <ref type="bibr" target="#b61">[62]</ref> and FAS-TD <ref type="bibr" target="#b55">[56]</ref> on SiW dataset. It can be seen from <ref type="table" target="#tab_4">Table 5</ref> that our method performs the best for all three protocols, revealing the excellent generalization capacity of CDC for (1) variations of face pose and expression, (2) variations of different spoof mediums, (3) cross/unknown presentation attack.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Inter Testing</head><p>To further testify the generalization ability of our model, we conduct cross-type and cross-dataset testing to verify the generalization capacity to unknown presentation attacks and unseen environment, respectively.  Cross-type Testing. Following the protocol proposed in <ref type="bibr" target="#b2">[3]</ref>, we use CASIA-MFSD <ref type="bibr" target="#b66">[66]</ref>, Replay-Attack <ref type="bibr" target="#b12">[13]</ref> and MSU-MFSD <ref type="bibr" target="#b56">[57]</ref> to perform intra-dataset cross-type testing between replay and print attacks. As shown in <ref type="table" target="#tab_5">Table 6</ref>, our proposed CDC based methods achieve the best overall performance (even outperforming the zero-shot learning based method DTN <ref type="bibr" target="#b36">[37]</ref>), indicating our consistently good generalization ability among unknown attacks. Moreover, we also conduct the cross-type testing on the latest SiW-M <ref type="bibr" target="#b36">[37]</ref> dataset and achieve the best average ACER (12.7%) and EER (11.9%) among 13 attacks. The detailed results are shown in Appendix C.</p><p>Cross-dataset Testing.</p><p>In this experiment, there are two cross-dataset testing protocols. One is that training on the CASIA-MFSD and testing on Replay-Attack, which is named as protocol CR; the second one is exchanging the training dataset and the testing dataset, named protocol RC. As shown in <ref type="table" target="#tab_6">Table 7</ref>, our proposed CDCN++ has 6.5% HTER on protocol CR, outperforming the prior state-of-theart by a convincing margin of 11%. For protocol RC, we also outperform state-of-the-art frame-level methods (see bottom half part of <ref type="table" target="#tab_6">Table 7</ref> ). The performance might be further boosted via introducing the similar temporal dynamic features in Auxiliary <ref type="bibr" target="#b35">[36]</ref> and FAS-TD <ref type="bibr" target="#b55">[56]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Analysis and Visualization.</head><p>In this subsection, two perspectives are provided to demonstrate the analysis why CDC performs well.</p><p>Robustness to Domain Shift. Protocol-1 of OULU-NPU is used to verify the robustness of CDC when encoun-  tering the domain shifting, i.e., huge illumination difference between train/development and test set. <ref type="figure" target="#fig_9">Fig. 7</ref> shows that the network using vanilla convolution has low ACER on development set (blue curve) while high ACER on test set (gray curve), which indicates vanilla convolution is easily overfitted in seen domain but generalizes poorly when illumination changes. In contrast, the model with CDC is able to achieve more consistent performance on both development (red curve) and test set (yellow curve), indicating the robustness of CDC to domain shifting. Features Visualization. Distribution of multi-level fused features for the testing videos on Protocol-1 OULU-NPU is shown in <ref type="figure" target="#fig_10">Fig. 8</ref> via t-SNE <ref type="bibr" target="#b38">[39]</ref>. It is clear that the features with CDC ( <ref type="figure" target="#fig_10">Fig. 8(a)</ref>) presents more well-clustered behavior than that with vanilla convolution <ref type="figure" target="#fig_10">(Fig. 8(b)</ref>), which demonstrates the discrimination ability of CDC for distinguishing the living faces from spoofing faces. The visualization of the feature maps (w/o or w/ CDC) and attention maps of MAFM can be found in Appendix D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions and Future Work</head><p>In this paper, we propose a novel operator called Central Difference Convolution (CDC) for face anti-spoofing task. Based on CDC, a Central Difference Convolutional Network (CDCN) is designed. We also propose CDCN++, con-sisting of a searched CDC backbone and Multiscale Attention Fusion Module (MAFM). Extensive experiments are performed to verify the effectiveness of the proposed methods. We note that the study of CDC is still at an early stage. Future directions include: 1) designing context-aware adaptive CDC for each layer/channel; 2) exploring other properties (e.g., domain generalization) and applicability on other vision tasks (e.g., image quality assessment <ref type="bibr" target="#b33">[34]</ref> and Face-Forensics).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Acknowledgment</head><p>This work was supported by the Academy of Finland for project MiGA (grant 316765), ICT 2023 project (grant 328115), and Infotech Oulu. As well, the authors wish to acknowledge CSC IT Center for Science, Finland, for computational resources. central difference gradient information might be more important for mid level features. In terms of the performance comparison, it can be seen from <ref type="figure" target="#fig_0">Fig. 10(b)</ref> that adaptive CDC achieves comparable results (1.8% vs. 1.0% ACER) with CDC using constant ? = 0.7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Cross-type Testing on SiW-M</head><p>Following the same cross-type testing protocol (13 attacks leave-one-out) on SiW-M dataset <ref type="bibr" target="#b36">[37]</ref>, we compare our proposed methods with three recent face anti-spoofing methods <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b36">37]</ref> to valid the generalization capacity of unseen attacks. As shown in <ref type="table">Table 8</ref>, our CDCN++ achieves an overall better ACER and EER, with the improvement of previous state-of-the-art <ref type="bibr" target="#b36">[37]</ref> by 24% and 26% respectively. Specifically, we detect almost all "Impersonation" and "Partial Paper" attacks (EER=0%) while the previous methods perform poorly on "Impersonation" attack. It is obvious that we reduce the both the EER and ACER of Mask attacks ("HalfMask", "SiliconeMask", "TransparentMask" and "MannequinHead") sharply, which shows our CDC based methods generalize well on 3D nonplanar attacks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Feature Visualization</head><p>The low-level features and corresponding spatial attention maps of MAFM are visualized in <ref type="figure" target="#fig_0">Fig. 11</ref>. It is clear that both the features and attention maps between living and spoofing faces are quite different. 1) For the low-level features (see 2nd and 3rd row in <ref type="figure" target="#fig_0">Fig. 11</ref>), neural activation from the spoofing faces seems to be more homogeneous between the facial and background regions than that from living faces. It's worth noting that features with CDC are more likely to capture the detailed spoofing patterns (e.g., lattice artifacts in "Print1" and reflection artifacts in "Replay2"). 2) For the spatial attention maps of MAFM (see 4th row in <ref type="figure" target="#fig_0">Fig. 11</ref>), all the regions of hair, face and background have the relatively strong activation for the living faces while the facial regions contribute weakly for the spoofing faces. <ref type="table">Table 8</ref>. The evaluation and comparison of the cross-type testing on SiW-M <ref type="bibr" target="#b36">[37]</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Feature response of vanilla convolution (VanillaConv) and central difference convolution (CDC) for spoofing faces in shifted domains (illumination &amp; input camera). VanillaConv fails to capture the consistent spoofing pattern while CDC is able to extract the invariant detailed spoofing features, e.g., lattice artifacts.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>. 2, central difference convolution prefers to aggregate the center-oriented gradient of sampled values. Eq. (1) becomes Central difference convolution.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Architecture search space with CDC. (a) A network consists of three stacked cells with max pool layer while stem and head layers adopt CDC with 3?3 kernel and ? = 0.7. (b) A cell contains 6 nodes, including an input node, four intermediate nodes B1, B2, B3, B4 and an output node. (c) The edge between two nodes (except output node) denotes a possible operation. The operation space consists of eight candidates, where CDC 2 r means using two stacked CDC to increase channel number with ratio r first and then decrease back to the original channel size. The size of total search space is 3 ? 8 (1+2+3+4) = 3 ? 8 10 . tention strategy to learn the importance weights ? among intermediate nodes, that is ? i = exp(? i ) 0&lt;j&lt;N ?1 exp(? j ) , where ? i is the softmax of the original learnable weight ? i for intermediate node x i .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>; 2 )</head><label>2</label><figDesc>for each intermediate node, choosing one incoming edge with the largest value of max o?O,o =none p (i,j) o</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 .</head><label>4</label><figDesc>Multiscale Attention Fusion Module.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>. 4.1), implementation details (Sec. 4.2), results (Sec. 4.3 -4.5) and analysis (Sec. 4.6).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 .</head><label>5</label><figDesc>The architecture of CDCN++. It consists of the searched CDC backbone and MAFM. Each cell is followed by a max pool layer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 6 .</head><label>6</label><figDesc>(a) Impact of ? in CDCN. (b) Comparison among various convolutions (only showing the hyperparameters with best performance). The lower ACER, the better performance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 7 .</head><label>7</label><figDesc>The performance of CDCN on development and test set when training on Protocol-1 OULU-NPU.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 8 .</head><label>8</label><figDesc>3D visualization of feature distribution. (a) Features w/o CDC. (b) Features w/ CDC. Color code used: red=live, green=printer1, blue=printer2, cyan=replay1, black=replay2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 10 .</head><label>10</label><figDesc>Adaptive CDC with learnable ? for each layer. (a) The learned ? weights for the first ten layers. (b) Performance comparison on Protocol-1 OULU-NPU.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Architecture of DepthNet and CDCN. Inside the brackets are the filter sizes and feature dimensionalities. "conv" and "CDC" suggest vanilla and central difference convolution, respectively. All convolutional layers are with stride=1 and followed by a BN-ReLU layer while max pool layers are with stride=2.</figDesc><table /><note>Module (MAFM) with selective attention capacity. Search Backbone for FAS task. Our searching al- gorithm is based on two gradient-based NAS methods [35,</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>attacks types inside.</figDesc><table><row><cell>RGB Input</cell><cell>3x256x256</cell><cell>CDC</cell><cell>64x256x256</cell><cell>CDC</cell><cell>128x256x256</cell><cell>CDC_2_1.6</cell><cell>128x128x128</cell><cell>CDC_2_1.2</cell><cell>CDC_2_1.4</cell><cell>128x64x64</cell><cell>CDC</cell><cell>CDC_2_1.2</cell><cell>128x32x32</cell><cell>MAFM</cell><cell>384x32x32</cell><cell>CDC</cell><cell>128x32x32</cell><cell>CDC</cell><cell>1x32x32</cell><cell>Depth Map</cell></row><row><cell></cell><cell></cell><cell>stem0</cell><cell></cell><cell>stem1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>head0</cell><cell></cell><cell>head1</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Low-level Cell</cell><cell></cell><cell cols="2">Mid-level Cell</cell><cell></cell><cell cols="2">High-level Cell</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>The ablation study of NAS configuration.</figDesc><table><row><cell>Model</cell><cell cols="3">Varied cells Node attention ACER(%)</cell></row><row><cell>NAS Model 1 NAS Model 2 NAS Model 3 NAS Model 4</cell><cell>? ?</cell><cell>? ?</cell><cell>1.7 1.5 1.4 1.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>The ablation study of NAS based backbone and MAFM.</figDesc><table><row><cell cols="2">Backbone</cell><cell cols="3">Multi-level Fusion</cell><cell>ACER(%)</cell></row><row><cell cols="2">w/o NAS</cell><cell cols="3">w/ multi-level concat</cell><cell>1.0</cell></row><row><cell cols="2">w/ NAS</cell><cell cols="3">w/ multi-level concat</cell><cell>0.7</cell></row><row><cell cols="2">w/ NAS</cell><cell cols="3">w/ MAFM (3x3,3x3,3x3 CDC)</cell><cell>1.2</cell></row><row><cell cols="2">w/ NAS</cell><cell cols="3">w/ MAFM (3x3,3x3,3x3 VanillaConv)</cell><cell>0.6</cell></row><row><cell cols="2">w/ NAS</cell><cell cols="3">w/ MAFM (5x5,5x5,5x5 VanillaConv)</cell><cell>1.1</cell></row><row><cell cols="2">w/ NAS</cell><cell cols="3">w/ MAFM (7x7,5x5,3x3 VanillaConv)</cell><cell>0.2</cell></row><row><cell cols="6">Table 4. The results of intra testing on four protocols of OULU-</cell></row><row><cell cols="6">NPU. We only report the results "STASN [62]" trained without</cell></row><row><cell cols="4">extra datasets for a fair comparison.</cell><cell></cell></row><row><cell>Prot.</cell><cell></cell><cell>Method</cell><cell cols="3">APCER(%) BPCER(%) ACER(%)</cell></row><row><cell></cell><cell cols="2">GRADIANT [6]</cell><cell>1.3</cell><cell>12.5</cell><cell>6.9</cell></row><row><cell></cell><cell cols="2">STASN [62]</cell><cell>1.2</cell><cell>2.5</cell><cell>1.9</cell></row><row><cell>1</cell><cell cols="2">Auxiliary [36] FaceDs [26]</cell><cell>1.6 1.2</cell><cell>1.6 1.7</cell><cell>1.6 1.5</cell></row><row><cell></cell><cell cols="2">FAS-TD [56]</cell><cell>2.5</cell><cell>0.0</cell><cell>1.3</cell></row><row><cell></cell><cell cols="2">DeepPixBiS [20]</cell><cell>0.8</cell><cell>0.0</cell><cell>0.4</cell></row><row><cell></cell><cell cols="2">CDCN (Ours)</cell><cell>0.4</cell><cell>1.7</cell><cell>1.0</cell></row><row><cell></cell><cell cols="2">CDCN++ (Ours)</cell><cell>0.4</cell><cell>0.0</cell><cell>0.2</cell></row><row><cell></cell><cell cols="2">DeepPixBiS [20]</cell><cell>11.4</cell><cell>0.6</cell><cell>6.0</cell></row><row><cell></cell><cell cols="2">FaceDs [26]</cell><cell>4.2</cell><cell>4.4</cell><cell>4.3</cell></row><row><cell>2</cell><cell cols="2">Auxiliary [36] GRADIANT [6]</cell><cell>2.7 3.1</cell><cell>2.7 1.9</cell><cell>2.7 2.5</cell></row><row><cell></cell><cell cols="2">STASN [62]</cell><cell>4.2</cell><cell>0.3</cell><cell>2.2</cell></row><row><cell></cell><cell cols="2">FAS-TD [56]</cell><cell>1.7</cell><cell>2.0</cell><cell>1.9</cell></row><row><cell></cell><cell cols="2">CDCN (Ours)</cell><cell>1.5</cell><cell>1.4</cell><cell>1.5</cell></row><row><cell></cell><cell cols="2">CDCN++ (Ours)</cell><cell>1.8</cell><cell>0.8</cell><cell>1.3</cell></row><row><cell></cell><cell cols="3">DeepPixBiS [20] 11.7?19.6</cell><cell>10.6?14.1</cell><cell>11.1?9.4</cell></row><row><cell>3</cell><cell cols="2">FAS-TD [56] GRADIANT [6]</cell><cell>5.9?1.9 2.6?3.9</cell><cell>5.9?3.0 5.0?5.3</cell><cell>5.9?1.0 3.8?2.4</cell></row><row><cell></cell><cell cols="2">FaceDs [26]</cell><cell>4.0?1.8</cell><cell>3.8?1.2</cell><cell>3.6?1.6</cell></row><row><cell></cell><cell cols="2">Auxiliary [36]</cell><cell>2.7?1.3</cell><cell>3.1?1.7</cell><cell>2.9?1.5</cell></row><row><cell></cell><cell cols="2">STASN [62]</cell><cell>4.7?3.9</cell><cell>0.9?1.2</cell><cell>2.8?1.6</cell></row><row><cell></cell><cell cols="2">CDCN (Ours)</cell><cell>2.4?1.3</cell><cell>2.2?2.0</cell><cell>2.3?1.4</cell></row><row><cell></cell><cell cols="2">CDCN++ (Ours)</cell><cell>1.7?1.5</cell><cell>2.0?1.2</cell><cell>1.8?0.7</cell></row><row><cell></cell><cell cols="3">DeepPixBiS [20] 36.7?29.7</cell><cell cols="2">13.3?14.1 25.0?12.7</cell></row><row><cell>4</cell><cell cols="2">GRADIANT [6] Auxiliary [36]</cell><cell>5.0?4.5 9.3?5.6</cell><cell>15.0?7.1 10.4?6.0</cell><cell>10.0?5.0 9.5?6.0</cell></row><row><cell></cell><cell cols="2">FAS-TD [56]</cell><cell>14.2?8.7</cell><cell>4.2?3.8</cell><cell>9.2?3.4</cell></row><row><cell></cell><cell cols="2">STASN [62]</cell><cell>6.7?10.6</cell><cell>8.3?8.4</cell><cell>7.5?4.7</cell></row><row><cell></cell><cell cols="2">FaceDs [26]</cell><cell>1.2?6.3</cell><cell>6.1?5.1</cell><cell>5.6?5.7</cell></row><row><cell></cell><cell cols="2">CDCN (Ours)</cell><cell>4.6?4.6</cell><cell>9.2?8.0</cell><cell>6.9?2.9</cell></row><row><cell></cell><cell cols="2">CDCN++ (Ours)</cell><cell>4.2?3.4</cell><cell>5.8?4.9</cell><cell>5.0?2.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 .</head><label>5</label><figDesc>The results of intra testing on three protocols of SiW<ref type="bibr" target="#b35">[36]</ref>.</figDesc><table><row><cell>Prot.</cell><cell>Method</cell><cell cols="2">APCER(%) BPCER(%)</cell><cell>ACER(%)</cell></row><row><cell></cell><cell>Auxiliary [36]</cell><cell>3.58</cell><cell>3.58</cell><cell>3.58</cell></row><row><cell>1</cell><cell>STASN [62]</cell><cell>-</cell><cell>-</cell><cell>1.00</cell></row><row><cell></cell><cell>FAS-TD [56]</cell><cell>0.96</cell><cell>0.50</cell><cell>0.73</cell></row><row><cell></cell><cell>CDCN (Ours)</cell><cell>0.07</cell><cell>0.17</cell><cell>0.12</cell></row><row><cell></cell><cell>CDCN++ (Ours)</cell><cell>0.07</cell><cell>0.17</cell><cell>0.12</cell></row><row><cell></cell><cell>Auxiliary [36]</cell><cell>0.57?0.69</cell><cell>0.57?0.69</cell><cell>0.57?0.69</cell></row><row><cell>2</cell><cell>STASN [62]</cell><cell>-</cell><cell>-</cell><cell>0.28?0.05</cell></row><row><cell></cell><cell>FAS-TD [56]</cell><cell>0.08?0.14</cell><cell>0.21?0.14</cell><cell>0.15?0.14</cell></row><row><cell></cell><cell>CDCN (Ours)</cell><cell>0.00?0.00</cell><cell>0.13?0.09</cell><cell>0.06?0.04</cell></row><row><cell></cell><cell cols="2">CDCN++ (Ours) 0.00?0.00</cell><cell>0.09?0.10</cell><cell>0.04?0.05</cell></row><row><cell></cell><cell>STASN [62]</cell><cell>-</cell><cell>-</cell><cell>12.10?1.50</cell></row><row><cell>3</cell><cell>Auxiliary [36]</cell><cell>8.31?3.81</cell><cell>8.31?3.80</cell><cell>8.31?3.81</cell></row><row><cell></cell><cell>FAS-TD [56]</cell><cell>3.10?0.81</cell><cell>3.09?0.81</cell><cell>3.10?0.81</cell></row><row><cell></cell><cell>CDCN (Ours)</cell><cell>1.67?0.11</cell><cell>1.76?0.12</cell><cell>1.71?0.11</cell></row><row><cell></cell><cell cols="2">CDCN++ (Ours) 1.97?0.33</cell><cell>1.77?0.10</cell><cell>1.90?0.15</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 .</head><label>6</label><figDesc>AUC (%) of the model cross-type testing on CASIA-MFSD, Replay-Attack, and MSU-MFSD.</figDesc><table><row><cell>Method</cell><cell cols="9">CASIA-MFSD [66] Video Cut Photo Wrapped Photo Video Digital Photo Printed Photo Printed Photo HR Video Mobile Video Replay-Attack [13] MSU-MFSD [57]</cell><cell>Overall</cell></row><row><cell cols="2">OC-SVM RBF +BSIF [3] 70.74</cell><cell>60.73</cell><cell>95.90</cell><cell>84.03</cell><cell>88.14</cell><cell>73.66</cell><cell>64.81</cell><cell>87.44</cell><cell>74.69</cell><cell>78.68?11.74</cell></row><row><cell>SVM RBF +LBP [10]</cell><cell>91.94</cell><cell>91.70</cell><cell>84.47</cell><cell>99.08</cell><cell>98.17</cell><cell>87.28</cell><cell>47.68</cell><cell>99.50</cell><cell>97.61</cell><cell>88.55?16.25</cell></row><row><cell>NN+LBP [59]</cell><cell>94.16</cell><cell>88.39</cell><cell>79.85</cell><cell>99.75</cell><cell>95.17</cell><cell>78.86</cell><cell>50.57</cell><cell>99.93</cell><cell>93.54</cell><cell>86.69?16.25</cell></row><row><cell>DTN [37]</cell><cell>90.0</cell><cell>97.3</cell><cell>97.5</cell><cell>99.9</cell><cell>99.9</cell><cell>99.6</cell><cell>81.6</cell><cell>99.9</cell><cell>97.5</cell><cell>95.9?6.2</cell></row><row><cell>CDCN (Ours)</cell><cell>98.48</cell><cell>99.90</cell><cell>99.80</cell><cell>100.00</cell><cell>99.43</cell><cell>99.92</cell><cell>70.82</cell><cell>100.00</cell><cell>99.99</cell><cell>96.48?9.64</cell></row><row><cell>CDCN++ (Ours)</cell><cell>98.07</cell><cell>99.90</cell><cell>99.60</cell><cell>99.98</cell><cell>99.89</cell><cell>99.98</cell><cell>72.29</cell><cell>100.00</cell><cell>99.98</cell><cell>96.63?9.15</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 .</head><label>7</label><figDesc>The results of cross-dataset testing between CASIA-MFSD and Replay-Attack. The evaluation metric is HTER(%). The multiple-frame based methods are shown in the upper half part while single-frame based methods in bottom half part.</figDesc><table><row><cell>Method</cell><cell>Train CASIA-</cell><cell>Test Replay-</cell><cell>Train Replay-</cell><cell>Test CASIA-</cell></row><row><cell></cell><cell>MFSD</cell><cell>Attack</cell><cell>Attack</cell><cell>MFSD</cell></row><row><cell>Motion-Mag [5]</cell><cell cols="2">50.1</cell><cell cols="2">47.0</cell></row><row><cell>LBP-TOP [16]</cell><cell cols="2">49.7</cell><cell cols="2">60.6</cell></row><row><cell>STASN [62]</cell><cell cols="2">31.5</cell><cell cols="2">30.9</cell></row><row><cell>Auxiliary [36]</cell><cell cols="2">27.6</cell><cell cols="2">28.4</cell></row><row><cell>FAS-TD [56]</cell><cell cols="2">17.5</cell><cell cols="2">24.0</cell></row><row><cell>LBP [7]</cell><cell cols="2">47.0</cell><cell cols="2">39.6</cell></row><row><cell>Spectral cubes [48]</cell><cell cols="2">34.4</cell><cell cols="2">50.0</cell></row><row><cell>Color Texture [8]</cell><cell cols="2">30.3</cell><cell cols="2">37.7</cell></row><row><cell>FaceDs [26]</cell><cell cols="2">28.5</cell><cell cols="2">41.1</cell></row><row><cell>CDCN (Ours)</cell><cell cols="2">15.5</cell><cell cols="2">32.6</cell></row><row><cell>CDCN++ (Ours)</cell><cell>6.5</cell><cell></cell><cell cols="2">29.8</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A. Derivation and Code of CDC</head><p>Here we show the detailed derivation (Eq.(4) in draft) of CDC in Eq. <ref type="bibr" target="#b6">(7)</ref> and Pytorch code of CDC in <ref type="figure">Fig. 9</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Adaptive ? for CDC</head><p>Although the best hyperparameter ? = 0.7 can be manually measured for face anti-spoofing task, it is still troublesome to find the best-suited ? when applying Central Difference Convolution (CDC) to other datasets/tasks. Here we treat ? as the data-driven learnable weights for each layer. A simple implementation is to utilize Sigmoid(?) to guarantee the output range within [0, 1].</p><p>As illustrated in <ref type="figure">Fig. 10(a)</ref>, it is interesting to find that the values of learned weights in low (2nd to 4th layer) and high (8th to 10th layer) levels are relatively small while that in mid (5th to 7th layer) level are large. It indicates that the </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Tensorflow: a system for large-scale machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mart?n</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjay</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Isard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">OSDI</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="265" to="283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Face description with local binary patterns: Application to face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Ahonen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdenour</forename><surname>Hadid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matti</forename><surname>Pietikainen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis &amp; Machine Intelligence</title>
		<imprint>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2037" to="2041" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">An anomaly detection approach to face spoofing detection: A new formulation and evaluation protocol</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Shervin Rahimzadeh Arashloo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Kittler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Christmas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="13868" to="13882" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Face anti-spoofing using patch and depth-based cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yousef</forename><surname>Atoum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaojie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amin</forename><surname>Jourabloo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Joint Conference on Biometrics (IJCB)</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="319" to="328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Computationally efficient face spoofing detection with motion magnification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samarth</forename><surname>Bharadwaj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Tejas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mayank</forename><surname>Dhamecha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richa</forename><surname>Vatsa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition workshops</title>
		<meeting>the IEEE conference on computer vision and pattern recognition workshops</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="105" to="110" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A competition on generalized software-based face presentation attack detection in mobile scenarios</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zinelabdine</forename><surname>Boulkenafet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jukka</forename><surname>Komulainen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zahid</forename><surname>Akhtar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Azeddine</forename><surname>Benlamoudi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Djamel</forename><surname>Samai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdelkrim</forename><surname>Salah Eddine Bekhouche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fadi</forename><surname>Ouafi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdelmalik</forename><surname>Dornaika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Taleb-Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Qin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Joint Conference on Biometrics (IJCB)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="688" to="696" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Face anti-spoofing based on color texture analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zinelabidine</forename><surname>Boulkenafet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jukka</forename><surname>Komulainen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdenour</forename><surname>Hadid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE international conference on image processing (ICIP)</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="2636" to="2640" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Face spoofing detection using colour texture analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zinelabidine</forename><surname>Boulkenafet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jukka</forename><surname>Komulainen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdenour</forename><surname>Hadid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Forensics and Security</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1818" to="1830" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Face antispoofing using speeded-up robust features and fisher vector encoding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zinelabidine</forename><surname>Boulkenafet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jukka</forename><surname>Komulainen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdenour</forename><surname>Hadid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="141" to="145" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Oulu-npu: A mobile face presentation attack database with real-world variations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zinelabinde</forename><surname>Boulkenafet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jukka</forename><surname>Komulainen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdenour</forename><surname>Hadid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FGR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Smash: one-shot model architecture search through hypernetworks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theodore</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Ritchie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weston</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.05344</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Proxylessnas: Direct neural architecture search on target task and hardware</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ligeng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ternational Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">On the effectiveness of local binary patterns in face antispoofing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivana</forename><surname>Chingovska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andr?</forename><surname>Anjos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S?bastien</forename><surname>Marcel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrics Special Interest Group</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="1" to="7" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deformable convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haozhi</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuwen</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="764" to="773" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Lbp-top based countermeasure against face spoofing attacks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Freitas</forename><surname>Tiago De</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andr?</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Anjos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="121" to="132" />
		</imprint>
	</monogr>
	<note>Jos? Mario De Martino, and S?bastien Marcel</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Can face anti-spoofing countermeasures work in a real world scenario</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Freitas</forename><surname>Tiago De</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andr?</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Anjos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 international conference on biometrics (ICB)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
	<note>Jos? Mario De Martino, and S?bastien Marcel</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Searching for a robust neural architecture in four gpu hours</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanyi</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1761" to="1770" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Joint 3d face reconstruction and dense alignment with position map regression network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohu</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanfeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">3d convolutional neural network based on face anti-spoofing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junying</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanlu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yikui</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengyun</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICMIP</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep pixel-wise binary supervision for face presentation attack detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anjith</forename><surname>George</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S?bastien</forename><surname>Marcel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Biometrics, number CONF</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Nas-fpn: Learning scalable feature pyramid architecture for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Golnaz</forename><surname>Ghiasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7036" to="7045" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4700" to="4708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Information technology biometric presentation attack detection part 1: Framework</title>
		<ptr target="https://www.iso.org/obp/ui/iso,2016.6" />
	</analytic>
	<monogr>
		<title level="j">biometrics</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Unsupervised texture segmentation using gabor filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Anil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Farshid</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Farrokhnia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern recognition</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1167" to="1186" />
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Face despoofing: Anti-spoofing via noise modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amin</forename><surname>Jourabloo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaojie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="290" to="306" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Juefei-Xu</surname></persName>
		</author>
		<title level="m">Vishnu Naresh Boddeti, and Marios Savvides. Local binary convolutional neural networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Face spoofing detection using dynamic texture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jukka</forename><surname>Komulainen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdenour</forename><surname>Hadid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matti</forename><surname>Pietik?inen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="146" to="157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Context based face anti-spoofing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jukka</forename><surname>Komulainen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdenour</forename><surname>Hadid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matti</forename><surname>Pietikainen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 IEEE Sixth International Conference on Biometrics: Theory, Applications and Systems (BTAS)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">An original face anti-spoofing approach using partial convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zinelabidine</forename><surname>Boulkenafet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoqiang</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdenour</forename><surname>Hadid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In IPTA</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="6" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Generalized face anti-spoofing by detecting pulse from face videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaobai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jukka</forename><surname>Komulainen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoying</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pong-Chi</forename><surname>Yuen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matti</forename><surname>Pietik?inen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 23rd International Conference on Pattern Recognition (ICPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4244" to="4249" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Face liveness detection by rppg features and contextual patch-based cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bofan</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaobai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zitong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoying</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 3rd International Conference on Biometric Engineering and Applications</title>
		<meeting>the 2019 3rd International Conference on Biometric Engineering and Applications</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="61" to="68" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Live face verification with multiple instantialized local homographic parameterization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhouyingcheng</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianguo</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingbing</forename><surname>Ni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="814" to="820" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">The role of structure and textural information in image utility and quality assessment tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suiyi</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><forename type="middle">Le</forename><surname>Callet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zitong</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Electronic Imaging</title>
		<imprint>
			<biblScope unit="issue">14</biblScope>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<title level="m">Darts: Differentiable architecture search. International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Learning deep models for face anti-spoofing: Binary or auxiliary supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaojie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amin</forename><surname>Jourabloo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Deep tree learning for zero-shot face anti-spoofing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaojie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Stehouwer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amin</forename><surname>Jourabloo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Gabor convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shangzhen</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baochang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianzhuang</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
			<date type="published" when="2008-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Visual object understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isabel</forename><surname>Palmeri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gauthier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Reviews Neuroscience</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Eyeblink-based anti-spoofing in face recognition from a generic webcamera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaohui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shihong</forename><surname>Lao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Cross-database face antispoofing with robust feature representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keyurkumar</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Chinese Conference on Biometric Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="611" to="619" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Secure face unlock: Spoof detection on smartphones</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keyurkumar</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on information forensics and security</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2268" to="2283" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Face liveness detection under bad illumination conditions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bruno</forename><surname>Peixoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carolina</forename><surname>Michelassi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anderson</forename><surname>Rocha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICIP</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="3557" to="3560" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Video action recognition via neural architecture searching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaopeng</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoying</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE International Conference on Image Processing (ICIP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="11" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Efficient neural architecture search via parameter sharing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Melody</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Face spoofing detection through visual codebooks of spectral temporal cubes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Allan</forename><surname>Pinto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helio</forename><surname>Pedrini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">Robson</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anderson</forename><surname>Rocha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="4726" to="4740" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Learning meta model for zero-and few-shot face antispoofing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunxiao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zezheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zitong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyu</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Lei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Auto-reid: Searching for a part-aware convnet for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruijie</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanyi</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linchao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Regularized evolution for image classifier architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Esteban</forename><surname>Real</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alok</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanping</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="4780" to="4789" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Large-scale evolution of image classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Esteban</forename><surname>Real</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherry</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Selle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutaka</forename><forename type="middle">Leon</forename><surname>Suematsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kurakin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="2902" to="2911" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Face anti-spoofing with multifeature videolet aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmad</forename><surname>Talha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samarth</forename><surname>Siddiqui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bharadwaj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Tejas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akshay</forename><surname>Dhamecha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mayank</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richa</forename><surname>Vatsa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nalini</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ratha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 23rd International Conference on Pattern Recognition (ICPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1035" to="1040" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Mechanisms of visual attention in the human cortex</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sabine</forename><surname>Kastner Ungerleider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leslie</forename><forename type="middle">G</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="315" to="341" />
		</imprint>
	</monogr>
	<note>Annual review of neuroscience</note>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Exploiting temporal and depth information for multi-frame face anti-spoofing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zezheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunxiao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiusheng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Lei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.05118</idno>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Face spoof detection with image distortion analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Forensics and Security</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="746" to="761" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Joon-Young Lee, and In So Kweon. Cbam: Convolutional block attention module</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyun</forename><surname>Woo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongchan</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Unknown presentation attack detection with face rgb images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wael</forename><surname>Abdalmageed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE 9th International Conference on Biometrics Theory, Applications and Systems (BTAS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Pc-darts: Partial channel connections for memory-efficient differentiable architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhui</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingxi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaopeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guo-Jun</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongkai</forename><surname>Xiong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.05737</idno>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.5601</idno>
		<title level="m">Learn convolutional neural network for face anti-spoofing</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Face antispoofing: Model matters, so does data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhan</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linchao</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dihong</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shibao</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Multi-scale context aggregation by dilated convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.07122</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Autofas: Searching lightweight networks for face anti-spoofing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zitong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunxiao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaqing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zezheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoying</forename><surname>Zhao</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title/>
		<idno>2020. 1</idno>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Customizable architecture search for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaofan</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="11641" to="11650" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">A face antispoofing database with diverse attacks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sifei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICB</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="26" to="31" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">Neural architecture search for deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Bai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.09523</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<title level="m">Neural architecture search with reinforcement learning. International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Learning transferable architectures for scalable image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8697" to="8710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<title level="m" type="main">Method Metrics(%) Replay Print Mask Attacks Makeup Attacks Partial Attacks Average Half Silicone Trans. Paper Manne. Obfusc. Imperson. Cosmetic Funny Eye Paper Glasses Partial Paper</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<title level="m" type="main">The four rows represent the RGB images, low-level features w/o CDC, w/ CDC and low-level spatial attention maps respectively</title>
		<imprint/>
	</monogr>
	<note>Features visualization on living face (the first column) and spoofing faces (four columns to the right). Best view when zoom in</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
