<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">TagRec: Automated Tagging of Questions with Hierarchical Learning Taxonomy</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Venktesh</forename><forename type="middle">V</forename></persName>
							<email>venkteshv@iiitd.ac.in</email>
							<affiliation key="aff0">
								<orgName type="institution">Indraprastha Institute of Information Technology</orgName>
								<address>
									<settlement>Delhi</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mukesh</forename><surname>Mohania</surname></persName>
							<email>mukesh@iiitd.ac.in</email>
							<affiliation key="aff0">
								<orgName type="institution">Indraprastha Institute of Information Technology</orgName>
								<address>
									<settlement>Delhi</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vikram</forename><surname>Goyal</surname></persName>
							<email>vikram@iiitd.ac.in</email>
							<affiliation key="aff0">
								<orgName type="institution">Indraprastha Institute of Information Technology</orgName>
								<address>
									<settlement>Delhi</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">TagRec: Automated Tagging of Questions with Hierarchical Learning Taxonomy</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T17:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Hinge rank loss ? multi-class classification ? Information retrieval</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Online educational platforms organize academic questions based on a hierarchical learning taxonomy (subject-chapter-topic). Automatically tagging new questions with existing taxonomy will help organize these questions into different classes of hierarchical taxonomy so that they can be searched based on the facets like chapter, topic. This task can be formulated as a flat multi-class classification problem. Usually, flat classification based methods ignore the semantic relatedness between the terms in the hierarchical taxonomy and the questions. Some traditional methods also suffer from the class imbalance issues as they consider only the leaf nodes ignoring the hierarchy. Hence, we formulate the problem as a similarity-based retrieval task where we optimize the semantic relatedness between the taxonomy and the questions. We demonstrate that our method helps to handle the unseen labels and hence can be used for taxonomy tagging in the wild, like the question-answer forums. In this method, we augment the question with its corresponding answer to capture more semantic information and then align the question-answer pair's contextualized embedding with the corresponding label (taxonomy) vector representations. The representations are aligned by fine-tuning a transformer based model with a loss function that is a combination of the cosine similarity and hinge rank loss. The loss function maximizes the similarity between the question-answer pair and the correct label representations and minimizes the similarity to unrelated labels. Finally, we perform extensive experiments on two real-world datasets. We empirically show that the proposed learning method outperforms representations learned using the multi-class classification method and other state of the art methods by 6% as measured by Recall@k. We also demonstrate the performance of the proposed method on unseen but related learning content like the learning objectives without re-training the network.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>(a) Training phase -aligns input and label embeddings. Online learning platforms organize academic questions according to a hierarchical learning taxonomy (subject-chapter-topic). For instance a question about "electromotive force" is tagged with "science -physics -electricity". This method of organization helps individuals navigate over large question banks. The taxonomy can also aid in faceted search. The facets could be topics, concepts, or chapters. However, manually tagging each question with the appropriate learning taxonomy is cumbersome. Hence there is a need for automated methods for tagging a question with the appropriate learning taxonomy. Automated tagging helps to organize acquired questions from third party vendors, which may be rarely linked to a learning taxonomy or are linked only at a "chapter" level. Also, the learning taxonomy is subject to change as the topic names or concept names could be replaced by synonyms or related concepts. Hence, the taxonomy tagging method should adapt to minor changes in the label (taxonomy) space without changes in the model architecture or re-training. Automated categorization of content in online platforms is usually formulated as a multi-class classification problem <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b4">5]</ref>. However, there are some unique challenges when dealing with a hierarchical taxonomy and tagging short questions in the e-learning domain. Firstly, some of the traditional multi-class classification methods ignore the hierarchy and consider only leaf nodes of the hierarchical labels as labels. However, this formulation of the problem would suffer from class imbalance issues since a large number of contents may be tagged with a small number of leaf nodes leaving a smaller number of samples for other leaf nodes. The second challenge is that the labels are dynamic in nature as new topics could be added to the syllabus, and the old topics may no longer be valid or could be retired. This results in a change in the label space and thus gives rise to new labels. The new labels would have some similarity to some of the existing labels as the subject name and the chapter names could be semantically related to the existing chapter names. The traditional multi-class classification methods cannot exploit this semantic relatedness as they do not consider label representations. They require a change in architecture to incorporate the new labels and must be retrained. However, the hierarchical labels are an abstraction of their word descriptions and hence some of the terms in the hierarchical labels are semantically related to the words in the given questions. Hence, by learning a representation that captures the similarity between the labels and the related questions, the model can adapt to changes in label space.</p><p>To capture more semantic information from the given inputs, we augment the question with its answer as an auxiliary information. Hence, we refer to the augmented content as a "question-answer" pair and the hierarchical learning taxonomy is referred to as "label" or "taxonomy". Our method, however would work even in cases where the answer is not given along with the question.</p><p>We propose a new method, named TagRec, for question-answer categorization in online learning platforms. In our method, the goal is to recommend relevant hierarchical learning taxonomy (label) for every question-answer pair to assist in organizing the learning content. Hence we adopt a similarity based retrieval method where hierarchical labels which are semantically related to the given question-answer pair. <ref type="figure" target="#fig_0">Figure 1</ref> shows the basic architecture of the proposed method. Here, in the <ref type="figure" target="#fig_0">Figure 1(a)</ref>, the method projects the question-answer text and the corresponding label as inputs to a continuous vector space and aligns the input representations T emb with the label representations O emb . In the <ref type="figure" target="#fig_0">Figure  1</ref>(b), during the recommendation (test time), when a new question arrives, the method projects the new question-answer pair to the vector space and computes the cosine similarity between the input representations and vector representations of all known labels. The labels are then ranked according to the similarity score, and the top-k labels are recommended for the given new question.</p><p>The proposed method can be used for tag recommendation in open source platforms like StackExchange. For example, a question about "Batch normalization" with tags "deep-learning" and "normalization" can be tagged with a hierarchical label AI? ?deep learning? ?normalization? ?Batch normalization. The preprocessed data can then be fed to TagRec, which would be able to recommend hierarchical labels to new questions after the training.</p><p>The following are the key technical contributions of the paper:</p><p>? We propose a novel and efficient similarity based retrieval method to recommend a hierarchical taxonomy label to a given question-answer pair. The method decouples the computation of vector representations for the question input and the taxonomy labels, thus allowing label representations to be pre-computed and indexed for lookup. ? We propose a learning method to align the input and hierarchical label representations that involves a loss function combining the cosine similarity and the hinge rank loss <ref type="bibr" target="#b3">[4]</ref>. ? We employ a transformer based sentence representation method to represent the hierarchical labels. We conduct extensive experiments by varying the label representations in the architecture shown in <ref type="figure" target="#fig_0">Figure 1</ref>(a) to empirically determine the effect of the label representations on the performance of the method. The proposed TagRec method outperforms the state of the art methods by upto 6% with Recall@k as the metric. ? We demonstrate the ability of our method to adapt the changes in label space without any changes in architecture or retraining. ? We further demonstrate the ability of our method to categorize the unseen but related learning content like learning objectives. We extract 417 learning objectives from science textbooks and apply the proposed method to this data without any re-training. We observe that the proposed method is able to achieve high Recall@k at top-2 predictions and outperforms the existing state of the art methods by 7%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>In this section, we first provide an overview of multi-class classification methods that consider the hierarchical label structure and then briefly discuss the current state of the art sentence representation methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Multi-class classification with hierarchical taxonomy</head><p>Many websites in the e-commerce and e-learning domains organize their content based on a hierarchical taxonomy <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b4">5]</ref>. The most common approaches for automatic categorization of the content to the hierarchical labels are flat multiclass single-step classification and hierarchical multi-step classifiers <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b18">19]</ref>. In multi-class single-step methods, the hierarchy is ignored and the leaf nodes are considered as labels. This leads to class imbalance issue, as discussed in Section 1. In the hierarchical multi-step approach, a classifier is trained to predict the top-level category and the process is repeated for predicting the sub-categories. However, the main problems associated with this approach are that the error from the classifiers at one level propagates to the next level and the number of classifiers increases at every step. Several single-step classifiers have been proposed for the task of hierarchical classification. In <ref type="bibr" target="#b18">[19]</ref>, the word level features like n-grams were used with SVM as classifier to predict level 1 categories, whereas in <ref type="bibr" target="#b4">[5]</ref> the authors have leveraged ngram features and distributed representations from Word2Vec to obtain features and fed them to a linear classifier for multi-class classification. Several deep learning methods like CNN <ref type="bibr" target="#b5">[6]</ref> and LSTM <ref type="bibr" target="#b16">[17]</ref> have been proposed for the task of question classification. Since the pre-trained language models, like BERT <ref type="bibr" target="#b2">[3]</ref>, improve the performance, the authors in <ref type="bibr" target="#b17">[18]</ref> propose a model BERT-QC, which fine tunes BERT on a sample of questions from science domain to classify them to a hierarchical taxonomy. The hierarchical multi-class classification problem has also been cast as a machine translation problem in <ref type="bibr" target="#b13">[14]</ref> where the authors provide the product titles as input and use a seq2seq architecture to translate them to product categories that exhibit a hierarchy. However, all these above approaches do not consider the label representations. The hierarchical neural attention model <ref type="bibr" target="#b11">[12]</ref> has been proposed, which leverages attention to obtain useful input sentence representation and uses an encoder-decoder architecture to predict each category in the hierarchical taxonomy. However, this approach may not scale to deep hierarchies. In this paper, we take a similarity-based retrieval approach with the aim to recommend the relevant label (i.e., the hierarchical learning taxonomy) by aligning the input embeddings and the label embeddings. We do not explore the multi-level classifier approach owing to the shortcomings explained earlier in this section. The proposed method can also adapt to changes in the label space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Sentence representation methods</head><p>Distributed representations that capture the semantic relationships <ref type="bibr" target="#b7">[8]</ref> have helped to advance many NLP tasks like classification, retrieval. Methods like GloVe <ref type="bibr" target="#b9">[10]</ref> learn vector representation of word by performing dimensionality reduction on a co-occurrence count matrix. Rather than averaging word representations to obtain sentence embeddings, an unsupervised method named Sent2Vec <ref type="bibr" target="#b8">[9]</ref> for composing n-gram embeddings to learn sentence representations was proposed.</p><p>The Bidirectional Encoder Representation from Transformers (BERT) <ref type="bibr" target="#b2">[3]</ref> is one of the current state of the art methods. However, one of the disadvantages of the BERT network structure is that no independent sentence embeddings are computed. The Sentence-BERT <ref type="bibr" target="#b10">[11]</ref> model was proposed to generate useful sentence embeddings by fine-tuning BERT. Another transformer based sentence encoding model is the Universal Sentence Encoder (USE) <ref type="bibr" target="#b1">[2]</ref> that has been specifically trained on semantic textual similarity task and generates useful sentence representations.</p><p>In this paper, we treat each label as a sentence and embed it using the sentence representation methods. For example, the label Science -Physicselectricity is treated as a sentence. In our experiments, we observe that USE embeddings and Sentence-BERT embeddings perform better than averaging word embeddings. These results are discussed in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head><p>In this section, we describe our method for classifying questions to hierarchical labels. The method consists of a training phase and testing phase, as shown in <ref type="figure" target="#fig_0">Figure 1</ref>. The input to the method is a corpus of documents, </p><formula xml:id="formula_0">C = {D 1 , D 2 ...D n } Algorithm 1 Tag Recommender Input: Training set T ? docs {D1, ..Dn}, labels O of form (Subject-Chapter-Topic) Output: Set of tags for test set , RO Training (batch mode) 1: Get input text embeddings , T emb ? BERT (D) 2: Obtain label embeddings, O emb ? SEN T BERT (O) 3: Index(labels) ? O emb 4: loss ? j =label max(0, margin ? cos(T emb , O emb (label)) + cos(T emb , O emb (j))</formula><formula xml:id="formula_1">= {(S 1 , Ch 1 , T 1 ), (S 2 , Ch 2 , T 2 )...} where S i , Ch i and T i denote subject,</formula><p>chapter, and topic respectively. The goal here is to learn an input representation that is close to the correct label in the vector space. We consider the label (S i , Ch i , T i ) as a sequence, (S i + Ch i + T i ) and obtain a sentence representation for it using pre-trained models. We obtain contextualized representations for the inputs using BERT <ref type="bibr" target="#b2">[3]</ref> followed by two projection layers. The linear projection layers are transformations that map the 768-D representation from BERT to the 1024-D or 512-D vector representation.</p><p>The steps of the proposed method are given in Algorithm 1. The details of the two phases in Algorithm 1 are as follows:</p><p>? In the training phase, the input question-answer pair is passed through a transformer based language model BERT followed by projection layers. The vector representations for the labels are obtained using a sentence representation method like USE <ref type="bibr" target="#b1">[2]</ref> or Sentence-BERT <ref type="bibr" target="#b10">[11]</ref>. The vector representations for all unique set of labels can be pre-computed and indexed for lookup. This saves computation cost and time during training and testing phases. The model is fine-tuned using a loss function that is a combination of cosine similarity and hinge rank loss <ref type="bibr" target="#b3">[4]</ref>. This helps to align the contextualized input representations with the label representations.</p><p>? In the testing phase, as shown in <ref type="figure" target="#fig_0">Figure 1b</ref>, the results are obtained in three steps. Firstly, the vector representations (embedding) for the input are computed using the fine-tuned BERT model. Secondly, the labels are ranked by computing cosine similarity between the input embeddings and the pre-computed label embeddings. Finally, top-k labels are chosen and metrics like Recall@k are computed for evaluating the performance of the model.</p><p>Our method is efficient as the label representations are pre-computed and indexed. Hence the time complexity at inference or testing time is O(T M N qa ), where T M is the time cost of the model (BERT + projection layers) and N qa is the number of question-answer pairs at test time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Contextualized Input representations</head><p>The academic questions are mostly comprised of technical terms or concepts that are related with the "topic" component of the label. For example, a question that contains terms like "ethyl alcohol" is closely related with the topic "alcohols and ethers" and hence the question can be tagged with the label "science -chemistry -alcohols and ethers". Academic questions also have terms that refer to different meanings depending on the context of their occurrence in the input sentence. For instance, the word "imaginary" in the sentence "Consider an imaginary situation" and its occurrence in the sentence "Given two imaginary numbers" has different meanings. This is an example of polysemy where the same word has different meanings in different contexts. Hence we need a method that can focus on important terms in the sequence and also tackle the problem of polysemy. To tackle the mentioned problems, we use a transformer based language model BERT for projecting the input text to the vector space. The BERT is a language model where the representations are learnt in two stages. In the first stage, the model is trained in an unsupervised manner. In the second stage, the model is finetuned on task specific labelled data to produce representations for downstream tasks. The "self-attention" mechanism in BERT helps in obtaining better vector representations and helps tackle the problem of polysemy.</p><p>Self-attention <ref type="bibr" target="#b14">[15]</ref> is the core of transformer based language models, and BERT leverages it to obtain better representation for a word by attending other relevant words in the context; Thus, a word has different representations depending on the context it has been used in. Self-attention encodes each word in the sentence using Query (Q), Key(K) and Value(V) vectors to obtain attention scores which determines how much attention to pay to each word when generating an embedding for the current word. Mathematically,</p><formula xml:id="formula_2">Attention(Q, K, V ) = Sof tmax(Q * K T ) ? d k * V (1) Sof tmax(x i ) = exp(x i ) N j exp(x j )<label>(2)</label></formula><p>where d k is the dimension of query, key, and value vectors and is used to scale the attention scores.</p><p>The self-attention mechanism helps to obtain contextualized representations that tackle the mentioned problems. We obtain contextualized representations of the input from BERT and pass them through the two projection layers, as shown in <ref type="figure" target="#fig_0">Figure 1a</ref>. We fine-tune BERT and the projection layers to align the generated contextualized representations with label representations as given in Algorithm 1. We further explore the training phase in Section 3.3</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Hierarchical label representations</head><p>Here, we describe how sentence representations are obtained for the labels. We consider the labels that have a hierarchical structure as a sequence of words and leverage sentence embedding methods to project them to vector space. We embed the labels this way to preserve the semantic relatedness between the labels. For instance, the label like science -physics -electricity must be closer to science -physics -magnetism than science -biology -biomolecules in the vector space. With simple vector arithmetic (cosine similarity), we observe that embedding the labels with sentence based representation methods like Sentence-BERT or Sent2Vec help to preserve the semantic relatedness when compared to averaging word embeddings from GLoVe <ref type="bibr" target="#b9">[10]</ref>. The sentence representation methods also do not suffer from constituent words being out of vocabulary unlike traditional word embedding methods and are able to handle such words. Since the Sentence-BERT and the USE models have been explicitly trained on semantic textual similarity tasks they provide rich textual representations that can be used for similarity based retrieval tasks. Hence, in this paper, we extensively experiment with various sentence embeddings methods like Sent2Vec, Universal Sentence Encoder (USE), and Sentence-BERT. We also propose a method where the labels are represented using the mean of the GloVe vectors. We observe that sentence embedding methods significantly outperform the averaging of word vectors. The results are discussed in detail in the Experiments and Results section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Loss function</head><p>In the training phase in Algorithm 1, hinge rank loss is employed to maximize the similarity between contextualized input text embeddings and the vector representation of the correct label.</p><p>The hinge ranking loss is defined as :</p><p>loss(text, label) ? j =label max(0, margin ? cos(T emb , v(label)) + cos(T emb , v(j)))</p><p>where T emb denotes the input text embeddings from BERT, v(label) denotes the vector representation of the correct label, v(j) denotes the vector representation of an incorrect label, and cos denotes the cosine similarity function. The derivative of the loss function is propagated, and the linear projection layers are trained and the BERT layers are fine-tuned to minimize the loss as given in Algorithm 1. The margin was set to a value of 0.1, which is a fraction of the norm of the embedding vectors (1.0), and it yields the best performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section, we discuss the experimental setup and the datasets on which the experiments were performed. All experiments are carried out on Google colab.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>To evaluate the effectiveness of the proposed method, we perform experiments on the following datasets: In our experiments we concatenate the question and the answer and it is considered as the input to the model (BERT), and the hierarchical taxonomy is considered as the label. Though BERT model has a context limit of 512 tokens, the length of each question-answer pair is within this range.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Analysis of representation methods for encoding the hierarchical labels</head><p>In this section, we briefly provide an analysis of different vector representation methods for projecting the hierarchical labels (learning taxonomy) to a continuous vector space. We embed the hierarchical labels using sentence representations methods like Sent2Vec <ref type="bibr" target="#b8">[9]</ref> and Sentence-BERT <ref type="bibr" target="#b10">[11]</ref>. Additionally, we also average the word embeddings of individual terms in the hierarchical label using Glove to represent the label. We then compute the cosine similarity between the vectors of two different labels, and the results are as shown in <ref type="table" target="#tab_1">Table 2</ref>. From <ref type="table" target="#tab_1">Table  2</ref>, we observe that though "science ? ? physics ? ? electricity" and "science ? ? chemistry ? ? acids" are different, the representations obtained by averaging Glove embeddings output a high similarity score. This may be due to the loss of information by averaging word vectors. Additionally here, the context of words like electricity is not taken into account when encoding the word physics. Additionally, "physics" and "chemistry" are co-hyponyms which may result in their vectors being close in the continuous vector space. We also observe that Sent2Vec is also unable to capture the semantics of the labels as it gives a similar high cosine similarity score. However, we observe that the vectors obtained using Sentence-BERT are not very similar, as indicated by the cosine similarity score. This indicates that Sentence-BERT is able to produce semantically meaningful sentence representations for the hierarchical labels. We also observe that Sentence-BERT outputs high similarity scores for semantically related hierarchical labels. Since this analysis is not exhaustive, we also provide a detailed comparison of methods using different vector representation methods in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Methods and Experimental setup</head><p>We compare TagRec with flat multi-class classification methods and other state of the art methods. In TagRec, the labels are represented using transformer based sentence representation methods like Sentence-BERT (Sent BERT) <ref type="bibr" target="#b10">[11]</ref> or Universal Sentence Encoder <ref type="bibr" target="#b1">[2]</ref>. The methods we compare against are:</p><p>? BERT+Sent2Vec : In this method the training and testing phases are similar to TagRec. The labels representations are obtained using Sent2vec <ref type="bibr" target="#b8">[9]</ref> instead of USE or Sent BERT.</p><p>? BERT+GloVE : In this method, the labels are represented as the average of the word embeddings of their constituent words. The word embeddings are obtained from GloVe.</p><p>V (label) = mean((Gl(subject), Gl(chapter), Gl(topic)))</p><p>where, V (label) denotes vector representation of the label, Gl denotes GloVe pre-trained model. The training and testing phases are same as TagRec.</p><p>? Twin BERT: This method is adapted from Twin BERT <ref type="bibr" target="#b6">[7]</ref>. In this method, instead of using pre-trained sentence representation methods , we fine-tune a pre-trained BERT model to compute the label representations. The label representations correspond to the last layer hidden state of the first token. The first token is denoted as [CLS] in BERT, which is considered as the aggregate sequence representation. The BERT model that computes representations for the input and the BERT model for computing the label representations are fine-tuned simultaneously.</p><p>? BERT multi-class (label relation) <ref type="bibr" target="#b17">[18]</ref>: In this method, we fine-tune a pre-trained BERT model to classify the input question-answer pairs to one of the labels. Here the labels are encoded using label encoder, and hence this is a flat multi-class classification method. At inference time, we compute the representations for the question-answer pairs and labels using the fine-tuned model. Then the labels are ranked according to the cosine similarity scores computed between the input text embeddings and the label embeddings.</p><p>? BERT multi-class (prototypical embeddings) <ref type="bibr" target="#b12">[13]</ref>: To provide a fair comparison with TagRec, we propose another baseline that considers the similarity between samples rather than the samples and the label. A BERT model is fine-tuned in a flat multi-class classification setting similar to the previous baseline. Then for each class, we compute a prototype, which is the mean of the embeddings of randomly chosen samples for each class from the training set. The embedding for each chosen sample is computed as the concatenation of the [CLS] token of the last 4 layers of the fine-tuned BERT model. We observe that this combination provides the best result for this baseline.</p><p>After the prototypes are formed for each class, at inference time, we obtain the embeddings for each test sample in the same way and compute cosine similarity with the prototype embeddings for each class. Then the classes are ranked using the cosine similarity and top-k classes are returned.</p><p>? Pretrained Sent BERT: We implement a simple baseline where the vector representations of the input texts and the labels are obtained using a pretrained Sentence-BERT model. There is no training involved in this baseline. For each input top closest matching labels are retrieved according to cosine similarity.</p><p>All the BERT models were fine-tuned for 30 epochs (with early stopping) with the ADAM optimizer, with learning rate of 2e-5 <ref type="bibr" target="#b2">[3]</ref> and epsilon which is a hyperparameter to avoid division by zero errors is set to 1e-8. The random seed was set to a value of 42. The margin parameter in the hinge rank loss was set to a value of 0.1. All the implementations were done in Pytorch. The huggingface library <ref type="bibr" target="#b15">[16]</ref> was used to fine-tune pre-trained BERT models.</p><p>Our code and datasets are publicly available at https://bit.ly/3jQpzEv</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results and Discussion</head><p>The performance comparison of the methods described in the previous section is shown in <ref type="table" target="#tab_2">Table 3</ref>. We use the Recall@k metric, which is a common metric for ranked retrieval tasks. From the results, we observe that the proposed method TagRec (BERT+USE and BERT+Sent BERT) outperforms flat multi-class classification based baselines and other state of the art methods. We observe that representing the labels with transformer based sentence embedding methods perform the best. This is evident from the table as TagRec(BERT+USE) and TagRec(BERT+Sent BERT) outperform BERT+Sent2Vec and BERT+GloVe methods. This is because Universal Sentence Encoder (USE) and Sentence-BERT use self-attention to produce better representations. This reinforces the hypothesis that averaging the word vectors to represent the labels does not preserve the required semantic relatedness between labels. The Twin BERT architecture does not perform well when compared with TagRec. This is because the label representations obtained through fine-tuned BERT may not preserve the semantic relatedness than the label representations obtained from pre-trained sentence embedding models. Also both the Sentence-BERT and the Universal Sentence Encoder models are trained on semantic text similarity (STS) tasks thereby rendering them the ideal candidates for retrieval based tasks. Finally we observe that the TagRec method outperforms the flat classification based baselines confirming the hypothesis that the representations learnt by aligning the input text and label representations provide better performance. This is pivotal to the task of question-answer pair categorization as the technical terms in the short input text are strongly correlated with the words in the label. The first baseline (BERT label relation) performs poorly as it has not been explicitly trained to minimize the distance between the input and label representations. This implies that the representations learnt through flat classification has no notion of label similarity. But the prototypical embeddings based baseline performs better as the classification is done based on similarity between train and test sample representations. However this baseline also has no notion of label similarity. Hence does not perform well when compared to our proposed method, TagRec. We also observe that the simple baseline of performing semantic search using pretrained Sentence-BERT does not work well as the model is not fine-tuned to align the input and labels.</p><p>To further show the efficacy of our method, we perform statistical significance tests and observe that the predicted results are statistically significant. For instance, for Recall@20 we observe that the predicted outputs from TagRec are statistically significant (t-test) with p-values 0.000218 and 0.000816 for QC-Science and ARC respectively. The proposed method TagRec was also able to adapt to new labels. For instance, two samples in the test set of the ARC dataset were tagged with "matter? ?properties of material? ?reflect" unseen during the training phase as shown in <ref type="table" target="#tab_3">Table 4</ref>. At test time, the label "matter? ?properties of material? ?reflect" appeared in top 2 predictions output by the proposed method (TagRec (BERT + USE)) for the two samples. We also observe that for the method (TagRec (BERT + Sent BERT)) the label "matter? ?properties of material? ?reflect" appears in its top 5 predictions. We observe that for other methods shown in <ref type="table" target="#tab_3">Table 4</ref> the correct label does not appear even in top 10 predictions. The top 2 predictions from other methods for the samples are shown in <ref type="table" target="#tab_3">Table 4</ref>. We also make similar observations for the BERT classification (label relation) and BERT classification (prototypical embeddings) baselines. We do not show them in <ref type="table" target="#tab_3">Table 4</ref> owing to space constraints. The top 2 predictions from BERT classification (prototypical embeddings) baseline for example 1 in <ref type="table" target="#tab_3">Table 4</ref> are matter? ?properties of objects? ?temperature and matter? ?properties of objects? ?shape.</p><p>For example 2, in <ref type="table" target="#tab_3">Table 4</ref>, the top 2 predictions from BERT classification (prototypical embeddings) are energy? ?light? ?reflect and matter? ?properties of material? ?color.</p><p>The top 2 predictions from BERT classification (label relation) baseline for example 1 in <ref type="table" target="#tab_3">Table 4</ref> are matter? ?properties of objects? ? density and matter? ? properties of material? ?density. For example 2, in <ref type="table" target="#tab_3">Table 4</ref>, the top 2 predictions from BERT classification (label relation) are energy? ?light? ?refract and matter? ?properties of material? ?luster. This confirms our hypothesis that the proposed method can adapt to new labels without re-training or change in the model architecture unlike existing methods.</p><p>We also demonstrate the performance of TagRec on unseen but related learning content like the learning objectives. Learning objectives convey the learning goals and can be linked to learning content through the learning taxonomy.</p><p>We obtain the predictions for the given learning objectives using the models trained on the QC ? Science dataset. We do not fine-tune them on the given learning objectives dataset and directly use them as test set to obtain predictions.</p><p>The results of the learning objective categorization task are shown in <ref type="table" target="#tab_4">Table 5</ref>. We show the recall at top 1 and top 2 predictions as the best results were obtained in top 2 predictions. We observe that the proposed method TagRec outperforms other methods. Particularly TagRec (BERT + Sent BERT) which uses Sentence-BERT to represent the hierarchical labels gives the best performance. This demonstrates that the proposed method is able to generalize to unseen but related learning content without any re-training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we proposed a new method for learning to suggest hierarchical taxonomy (label) for short questions. We demonstrated that the representations learnt using the proposed similarity based learning method is better than flat classification methods and other state of the art methods <ref type="bibr" target="#b6">[7]</ref>. Our method can easily adapt to unseen labels without a change in the architecture unlike flat classification based methods. We also demonstrated that the trained model can be used to categorize any related learning content like learning objectives without any retraining. The proposed method can also be used for taxonomy tagging in the forums like Quora and other discussion forums. The questions in Quora have a character limit of 50 words, but the answers could be longer than the context limit of the BERT model. To handle such long sequence lengths, we plan to explore new methods like Longformer <ref type="bibr" target="#b0">[1]</ref>. Also in the future, we aim to explore the hyperbolic space to represent the hierarchical labels.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>(b) Testing (inference) phaserecommends labels Training and testing phases for tagging questions with hierarchical labels 1 Introduction</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>) 5 :</head><label>5</label><figDesc>Fine-tune BERT to minimize loss and align T emb and O emb Testing Phase 6: Compute embeddings for test set S using fine-tuned BERT S emb ? BERT (S) 7: Rank set of unique labels RO ? sorted(Sim(S emb , O emb )) 8: return Top-k labels from RO where each document corresponds to a question-answer pair and the hierarchical labels O</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Some samples from the QC-Science dataset</figDesc><table><row><cell>Question</cell><cell>Answer</cell><cell>Taxonomy</cell></row><row><cell>The value of electron</cell><cell>Fluorine atom is small so</cell><cell>Science? ?chemistry? ?classification</cell></row><row><cell>gain enthalpy of chlo-</cell><cell>electron charge density on F</cell><cell>of elements and periodicity in</cell></row><row><cell>rine is more than that</cell><cell>atom is very high</cell><cell>properties</cell></row><row><cell>of fluorine. Give rea-</cell><cell></cell><cell></cell></row><row><cell>sons</cell><cell></cell><cell></cell></row><row><cell>What are artificial</cell><cell>The chemical substances</cell><cell>Science? ?chemistry? ?chemistry in</cell></row><row><cell>sweetening agents?</cell><cell>which are sweet in taste but</cell><cell>everyday life</cell></row><row><cell></cell><cell>do not add any calorie</cell><cell></cell></row><row><cell cols="3">? QC-Science: This dataset contains 47832 question-answer pairs belonging</cell></row><row><cell cols="3">to the science domain tagged with labels of the form subject -chapter -topic.</cell></row></table><note>The dataset was collected with the help of a leading e-learning platform. The dataset consists of 40895 samples for training, 2153 samples for validation and 4784 samples for testing. Some samples are shown in Table 1. The average number of words per question is 37.14, and per answer, it is 32.01.? ARC [18]: This dataset consists of 7775 science multiple choice exam ques- tions with answer options and 406 hierarchical labels. The average number of words per question in the dataset is 20.5. The number of train, validation and test samples are 5597, 778 and 1400 respectively.? Learning Objectives: This dataset consists of 417 learning objectives collected from the "What you learnt" section in class 8,9 and 10 science textbooks (K-12 system). The corresponding learning taxonomy was extracted from the "Table of contents" of the textbooks.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Comparison of different representation methods for hierarchical labels</figDesc><table><row><cell>Method</cell><cell>Label1 (L1)</cell><cell>Label2 (L2)</cell><cell>cos(L1, L2)</cell></row><row><cell>Sentence-BERT</cell><cell>science ? ? physics ? ?</cell><cell>science ? ? chemistry</cell><cell>0.3072</cell></row><row><cell></cell><cell>electricity</cell><cell>? ? acids</cell><cell></cell></row><row><cell>Sent2vec</cell><cell>science ? ? physics ? ?</cell><cell>science ? ? chemistry</cell><cell>0.6242</cell></row><row><cell></cell><cell>electricity</cell><cell>? ? acids</cell><cell></cell></row><row><cell>GloVe</cell><cell>science ? ? physics ? ?</cell><cell>science ? ? chemistry</cell><cell>0.6632</cell></row><row><cell></cell><cell>electricity</cell><cell>? ? acids</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Performance comparison of TagRec with variants and baselines, ? indicates TagRec's significant improvement at 0.001 level using t-test</figDesc><table><row><cell cols="2">Dataset Method</cell><cell cols="4">R@5 R@10 R@15 R@20</cell></row><row><cell>QC-</cell><cell>TagRec(BERT+USE) (proposed method)</cell><cell cols="2">0.86 0.92</cell><cell cols="2">0.95 0.96</cell></row><row><cell>Science</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="5">TagRec(BERT+Sent BERT) (proposed method) 0.85 ? 0.93 ? 0.95 ? 0.97 ?</cell></row><row><cell></cell><cell>BERT+sent2vec</cell><cell>0.79</cell><cell>0.89</cell><cell>0.93</cell><cell>0.95</cell></row><row><cell></cell><cell>Twin BERT [7]</cell><cell>0.72</cell><cell>0.86</cell><cell>0.91</cell><cell>0.94</cell></row><row><cell></cell><cell>BERT+GloVe</cell><cell>0.76</cell><cell>0.87</cell><cell>0.92</cell><cell>0.94</cell></row><row><cell></cell><cell>BERT classification (label relation) [18]</cell><cell>0.39</cell><cell>0.50</cell><cell>0.57</cell><cell>0.63</cell></row><row><cell></cell><cell cols="2">BERT classification (prototypical embeddings) [13] 0.83</cell><cell>0.91</cell><cell>0.93</cell><cell>0.95</cell></row><row><cell></cell><cell>Pretrained Sent BERT</cell><cell>0.30</cell><cell>0.40</cell><cell>0.47</cell><cell>0.52</cell></row><row><cell>ARC</cell><cell>TagRec(BERT+USE) (proposed method)</cell><cell cols="4">0.67 ? 0.81 ? 0.86 ? 0.89 ?</cell></row><row><cell></cell><cell cols="2">TagRec(BERT+Sent BERT) (proposed method) 0.65</cell><cell>0.77</cell><cell>0.84</cell><cell>0.88</cell></row><row><cell></cell><cell>BERT+sent2vec</cell><cell>0.55</cell><cell>0.72</cell><cell>0.81</cell><cell>0.87</cell></row><row><cell></cell><cell>Twin BERT [7]</cell><cell>0.46</cell><cell>0.63</cell><cell>0.72</cell><cell>0.78</cell></row><row><cell></cell><cell>BERT+GloVe</cell><cell>0.56</cell><cell>0.73</cell><cell>0.82</cell><cell>0.86</cell></row><row><cell></cell><cell>BERT classification (label relation) [18]</cell><cell>0.27</cell><cell>0.37</cell><cell>0.42</cell><cell>0.49</cell></row><row><cell></cell><cell cols="2">BERT classification (prototypical embeddings) [13] 0.64</cell><cell>0.75</cell><cell>0.80</cell><cell>0.83</cell></row><row><cell></cell><cell>Pretrained Sent BERT</cell><cell>0.31</cell><cell>0.46</cell><cell>0.54</cell><cell>0.59</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Examples demonstrating the performance for unseen labels at test time.</figDesc><table><row><cell>Question text</cell><cell>Ground truth</cell><cell>Top 2 predictions</cell><cell>Method</cell></row><row><cell>A boy can see his face</cell><cell></cell><cell></cell><cell></cell></row><row><cell>when he looks into a calm pond. Which physi-cal property of the pond</cell><cell>matter? ?properties of material? ?reflect</cell><cell>matter? ?properties of material? ?flex and</cell><cell>TagRec (BERT+USE)</cell></row><row><cell>makes this happen? (A)</cell><cell></cell><cell>matter? ?properties</cell><cell></cell></row><row><cell>flexibility (B) reflective-</cell><cell></cell><cell>of</cell><cell></cell></row><row><cell>ness (C) temperature (D)</cell><cell></cell><cell>material? ?reflect</cell><cell></cell></row><row><cell>volume</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>matter? ?properties of</cell><cell></cell></row><row><cell></cell><cell></cell><cell>objects? ?mass</cell><cell></cell></row><row><cell></cell><cell></cell><cell>and</cell><cell></cell></row><row><cell></cell><cell></cell><cell>matter? ?properties of</cell><cell>Twin BERT [7]</cell></row><row><cell></cell><cell></cell><cell>objects? ?density</cell><cell></cell></row><row><cell></cell><cell></cell><cell>matter? ?states? ?solid</cell><cell></cell></row><row><cell></cell><cell></cell><cell>and</cell><cell></cell></row><row><cell></cell><cell></cell><cell>matter? ?properties of</cell><cell>BERT + GloVe</cell></row><row><cell></cell><cell></cell><cell>material? ?density</cell><cell></cell></row><row><cell></cell><cell></cell><cell>matter? ?properties</cell><cell></cell></row><row><cell></cell><cell></cell><cell>of material? ?specific</cell><cell></cell></row><row><cell></cell><cell></cell><cell>heat and</cell><cell></cell></row><row><cell></cell><cell></cell><cell>matter? ?properties of</cell><cell>BERT+sent2vec</cell></row><row><cell></cell><cell></cell><cell>material</cell><cell></cell></row><row><cell>Which object best re-</cell><cell>matter? ? properties</cell><cell>energy? ?light? ?reflect</cell><cell></cell></row><row><cell>flects light? (A) gray door (B) white floor (C) black sweater (D) brown carpet</cell><cell>of material? ?reflect</cell><cell>and matter? ?properties of material? ?reflect</cell><cell>TagRec (BERT+USE)</cell></row><row><cell></cell><cell></cell><cell>energy? ?thermal? ?</cell><cell></cell></row><row><cell></cell><cell></cell><cell>radiation and</cell><cell></cell></row><row><cell></cell><cell></cell><cell>energy? ?light? ?generic</cell><cell>Twin BERT [7]</cell></row><row><cell></cell><cell></cell><cell>properties</cell><cell></cell></row><row><cell></cell><cell></cell><cell>energy? ?light and</cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">energy? ?light? ?refract BERT + GloVe</cell></row><row><cell></cell><cell></cell><cell>energy? ?light? ?reflect</cell><cell></cell></row><row><cell></cell><cell></cell><cell>and</cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">energy? ?light? ?refract BERT+sent2vec</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Performance comparison for learning objective categorization</figDesc><table><row><cell>Method</cell><cell cols="2">R@1 R@2</cell></row><row><cell>TagRec(BERT+USE) (proposed method)</cell><cell>0.69</cell><cell>0.85</cell></row><row><cell cols="3">TagRec(BERT+Sent BERT) (proposed method) 0.77 0.91</cell></row><row><cell>BERT+sent2vec</cell><cell>0.49</cell><cell>0.64</cell></row><row><cell>Twin BERT [7]</cell><cell>0.54</cell><cell>0.79</cell></row><row><cell>BERT+GloVe</cell><cell>0.62</cell><cell>0.84</cell></row><row><cell>BERT classification (label relation) [18]</cell><cell>0.46</cell><cell>0.59</cell></row><row><cell cols="2">BERT classification (prototypical embeddings) [13] 0.60</cell><cell>0.76</cell></row><row><cell>Pretrained Sent BERT</cell><cell>0.39</cell><cell>0.54</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Longformer: The long-document transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cohan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.05150</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Universal sentence encoder for English</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Y</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Limtiaco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>St</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Constant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Guajardo-Cespedes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Strope</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kurzweil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations<address><addrLine>Brussels; Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018-11" />
			<biblScope unit="page" from="169" to="174" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">BERT: pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<idno>abs/1810.04805</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Devise: A deep visual-semantic embedding model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Frome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2121" to="2129" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Everyone likes shopping! multi-class product categorization for ecommerce</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Kozareva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1329" to="1333" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A novel cnn-based method for question classification in intelligent question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 International Conference on Algorithms, Computing and Artificial Intelligence. ACAI 2018</title>
		<meeting>the 2018 International Conference on Algorithms, Computing and Artificial Intelligence. ACAI 2018<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Twinbert: Distilling knowledge to twin-structured compressed bert models for large-scale retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM &apos;20</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2645" to="2652" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th International Conference on Neural Information Processing Systems</title>
		<meeting>the 26th International Conference on Neural Information Processing Systems<address><addrLine>Red Hook, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Curran Associates Inc</publisher>
			<date type="published" when="2013" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
	<note>NIPS&apos;13</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Unsupervised learning of sentence embeddings using compositional n-gram features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pagliardini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaggi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.02507</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014-10" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Sentence-BERT: Sentence embeddings using Siamese BERT-networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Reimers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Gurevych</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A hierarchical neural attentionbased text classifier</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C K</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ruths</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="817" to="823" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Prototypical networks for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4077" to="4087" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">E-commerce product categorization via machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kok</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Manage. Inf. Syst</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Transformers: State-of-the-art natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Louf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Funtowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shleifer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Von Platen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Plu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Scao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gugger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Drame</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Lhoest</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</meeting>
		<imprint>
			<date type="published" when="2020-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Novel architecture for long short-term memory used in question classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">299</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Multi-class hierarchical question classification for multiple choice science exams</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Yadav</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tayyar Madabushi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Tafjord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th Language Resources and Evaluation Conference</title>
		<meeting>the 12th Language Resources and Evaluation Conference<address><addrLine>Marseille, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-05" />
			<biblScope unit="page" from="5370" to="5382" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Product title classification versus text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arunachalam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Somaiya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Csie. Ntu. Edu. Tw pp</title>
		<imprint>
			<biblScope unit="page" from="1" to="25" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
