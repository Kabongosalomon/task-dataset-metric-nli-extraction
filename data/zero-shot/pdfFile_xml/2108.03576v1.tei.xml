<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">BEATNET: CRNN AND PARTICLE FILTERING FOR ONLINE JOINT BEAT DOWNBEAT AND METER TRACKING</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mojtaba</forename><surname>Heydari</surname></persName>
							<email>mheydari@ur.rochester.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">University of Rochester</orgName>
								<address>
									<addrLine>500 Wilson Blvd</addrLine>
									<postCode>14627</postCode>
									<settlement>Rochester</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Cwitkowitz</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">University of Rochester</orgName>
								<address>
									<addrLine>500 Wilson Blvd</addrLine>
									<postCode>14627</postCode>
									<settlement>Rochester</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyao</forename><surname>Duan</surname></persName>
							<email>zhiyao.duan@rochester.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">University of Rochester</orgName>
								<address>
									<addrLine>500 Wilson Blvd</addrLine>
									<postCode>14627</postCode>
									<settlement>Rochester</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">BEATNET: CRNN AND PARTICLE FILTERING FOR ONLINE JOINT BEAT DOWNBEAT AND METER TRACKING</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T18:55+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The online estimation of rhythmic information, such as beat positions, downbeat positions, and meter, is critical for many real-time music applications. Musical rhythm comprises complex hierarchical relationships across time, rendering its analysis intrinsically challenging and at times subjective. Furthermore, systems which attempt to estimate rhythmic information in real-time must be causal and must produce estimates quickly and efficiently. In this work, we introduce an online system for joint beat, downbeat, and meter tracking, which utilizes causal convolutional and recurrent layers, followed by a pair of sequential Monte Carlo particle filters applied during inference. The proposed system does not need to be primed with a time signature in order to perform downbeat tracking, and is instead able to estimate meter and adjust the predictions over time. Additionally, we propose an information gate strategy to significantly decrease the computational cost of particle filtering during the inference step, making the system much faster than previous sampling-based methods. Experiments on the GTZAN dataset, which is unseen during training, show that the system outperforms various online beat and downbeat tracking systems and achieves comparable performance to a baseline offline joint method.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Rhythm plays an essential role in nearly all musical endeavors, including listening to, playing, learning, or composing music. This is why the estimation of rhythmic information, such as beat positions, downbeat positions and meter has always been an important subject of study in the field of Music Information Retrieval (MIR). Depending on the requirements and constraints imposed by the application at hand, these estimation tasks can either be performed in an offline or online fashion. Offline approaches are typically non-causal, meaning that they make predictions for a given time using data or features associated with a future time. These approaches are suitable for applications such as music transcription, music search and indexing, and musicological analysis. Online approaches are causal, meaning that they operate using only past and present features. These are typically desirable for human-computer interaction (HCI) systems, which must make immediate predictions, like real-time music accompaniment systems.</p><p>Many offline methods have been proposed for beat tracking <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref>. Most of them are unsupervised and attempt to utilize low-level features like onset strengths with some inference model to estimate beat positions within a music piece. However, with the growing success of deep learning, supervised beat tracking methods have become more prominent. B?ck et al. <ref type="bibr" target="#b3">[4]</ref> employed Recurrent Neural Networks (RNNs) to estimate beat positions; Various other neural network structures have also been proposed for onset detection and beat tracking <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6]</ref>.</p><p>Some methods have also been proposed for online beat tracking. However, many of them, e.g., <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref>, feed a sliding window of data into an offline model to estimate beat positions within upcoming frames. The sliding window strategy has several major drawbacks, including the discontinuity of beat predictions and the need for priming for predictions in the first window, which causes a delay <ref type="bibr" target="#b10">[11]</ref>. Some other approaches involve inferring beat positions in real time using multi agent models <ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref>, which initialize a set of agents with various hypotheses that try to validate their respective hypotheses based on observations across time.</p><p>The task of downbeat tracking is often considered to be more difficult than beat tracking. This is because a deeper understanding of rhythmic structure in music is required to be able to differentiate between beats and downbeats. Making matters worse, at the signal level, these two events have very similar characteristics. For instance, downbeats are not necessarily associated with stronger signal energy, nor do they necessarily feature a distinct percussive profile. Moreover, both beats and downbeats are likely to be the intersection of melodic and harmonic changes. These factors can make it challenging, and in some cases subjective, to distinguish between the two rhythmic events. For instance, for a 4/4 music piece with kick drum events on the first and third beats, it is hard to distinguish downbeats and determine whether the time signature is 4/4 or 2/4.</p><p>There has been some previous work on offline downbeat tracking, both as an isolated task and within a joint beat and downbeat tracking framework. Durand et al. . Overview of the joint beat, downbeat, and meter tracking procedure using the proposed BeatNet model. 17] used some combinations of features and CNN structures to obtain downbeats. Giorgi et al. <ref type="bibr" target="#b17">[18]</ref> proposed tempo-invariant convolutional filters for downbeat tracking. Peeters and Papadopoulos <ref type="bibr" target="#b18">[19]</ref> performed joint beat and downbeat tracking by decoding hidden states using the Viterbi algorithm. B?ck et al. <ref type="bibr" target="#b19">[20]</ref> and Krebs et al. <ref type="bibr" target="#b20">[21]</ref> employed an RNN structure for joint beat and downbeat tracking and only downbeat tracking using beat synchronous features, respectively. Furthermore, some recent works investigate Convolutional Recurrent Neural Network (CRNN) structures for beat and downbeat tracking. Fuentes et al. <ref type="bibr" target="#b21">[22]</ref> showed that CRNN structures outperform RNNs in downbeat tracking when taking the input observations over a tatum grid. Cheng et al. <ref type="bibr" target="#b22">[23]</ref> found that CRNN structures with larger receptive fields outperform other downbeat tracking models. B?ck and Davies. <ref type="bibr" target="#b23">[24]</ref> used a CNN and Temporal Convolutional Network (TCN) structure to improve the performance of their offline beat and downbeat tracking model, and also performed data augmentation to expose the neural network to more tempi.</p><p>The task of online downbeat tracking has received considerably less attention. Goto and Muranoka <ref type="bibr" target="#b12">[13]</ref> introduced an unsupervised model which leverages a measure inference stage for detecting chord changes. In <ref type="bibr" target="#b24">[25]</ref>, the same beat tracking neural network with forward algorithm from <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b19">20]</ref> is paired with <ref type="bibr" target="#b20">[21]</ref> to estimate downbeats and other rhythmic patterns by extracting percussive and harmonic beat-synchronous features. It is important to note that this method must be primed with a known time signature and all possible rhythmic pattern choices. Liang <ref type="bibr" target="#b25">[26]</ref> proposed an online downbeat tracking method which feeds a sliding window of data to an offline model <ref type="bibr" target="#b16">[17]</ref>. This method is vulnerable to the sliding window strategy drawbacks described above.</p><p>Particle filtering is advantageous for two main reasons when it comes to online processing. The first reason is that it does not require future data. Popular maximum a posteriori (MAP) algorithms like the Viterbi algorithm and maximizer of the posterior marginals (MPM) smoothing algorithms, e.g. forward-backward, are not applicable to online processing. The second reason is that, among the filtering methods which are causal, particle filtering is a general (non-parametric) approach which can be utilized to decode any unknown distribution. However, most music rhythmic analysis approaches that utilize particle filtering, e.g., <ref type="bibr" target="#b26">[27]</ref><ref type="bibr" target="#b27">[28]</ref><ref type="bibr" target="#b28">[29]</ref><ref type="bibr" target="#b29">[30]</ref>, are classical and do not incorporate neural networks. Alternatively, in our previous work <ref type="bibr" target="#b30">[31]</ref>, we utilized a particle filtering inference model to infer beat positions using the activations produced by an RNN in an online fashion, but that approach does not attempt to estimate downbeats nor meter.</p><p>In this paper, we propose BeatNet, a novel online system for joint beat, downbeat, and meter tracking. The system produces beat and downbeat activations using a CNN and RNN combination, and performs inference using two particle filtering stages. The beat tracking stage outperforms state-of-the-art online beat tracking methods. The other stage simultaneously infers downbeats and time signature and achieves comparable results to state-of-the-art offline downbeat tracking models that require the time signature as input. In contrast, BeatNet actively monitors tempo and time signature changes over time. Finally, we introduce an information gate mechanism in the inference module to speed up the inference significantly, making our method suitable for many real-time applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">METHOD</head><p>In this section, we describe BeatNet, our online system for joint beat, downbeat, and meter tracking, illustrated in Figure 1. BeatNet consists of a causal neural network stage for producing activations and a particle filtering stage for inference. The neural network comprises convolutional, recurrent and fully connected layers as described in section 2.2 which compute beat and downbeat activations for each frame of audio. The activations are fed to a two-stage particle filtering module to infer beat and downbeat positions and to estimate meter. The code for the BeatNet model is open-source 1 , along with video demos and further docu-mentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Feature Representation</head><p>The input of the network module is a sequence of filterbank magnitude responses, each of which corresponds to one audio frame. Specifically, short-time Fourier transform (STFT) with a Hann window of the length of 93 ms and hop size of 46 ms is applied to the audio signal to compute the log-amplitude magnitude spectrogram. Then a logarithmically spaced filterbank ranging from 30 Hz to 17 kHz with 24 bands per octave is applied to yield a 136-d filterbank response. The first-order temporal difference of this response is also calculated and concatenated, resulting in a 272-d filterbank response vector for each frame.</p><p>We also experimented with alternative feature representations, including the 329-d hand-crafted feature set from <ref type="bibr" target="#b14">[15]</ref>, which comprises chroma features, onset strengths, low-frequency spectral features, and melodic constant-Q spectral features. The motivation for this feature set is to aggregate the harmonic, percussive, bass, and melodic content of the music. However the 272-d filterbank response feature set described above achieved notably better performance than these hand-crafted features, and was thus chosen for subsequent experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Network Architecture</head><p>Following the common design of other similar works, we employ a convolutional-recurrent neural network (CRNN) architecture, illustrated in <ref type="figure" target="#fig_0">Figure 2</ref>, to process the input features in order to obtain beat and downbeat activations. Ideally, the convolution models relationships along the frequency axis, and the unidirectional recurrence models long-term relationships across time in a causal fashion.</p><p>The input features are fed into a 1D convolutional layer with 2 filters of kernel size 10, followed by ReLU activation. The two filter responses are max pooled with kernel size 2 along frequency and then concatenated into a single feature embedding for each frame. Then, a fullyconnected layer with 150 neurons reduces the dimensionality of the embedding, and feeds it through two subsequent unidirectional Long Short-Term Memory (LSTM) layers, each with a hidden size of 150. The embedding is then fed through a final fully-connected layer and a softmax operation to obtain three activations which represent beat, downbeat, and non-beat, respectively. Note that due to the softmax function, the final activations for each class always sum to one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Particle Filtering Inference</head><p>In this section, we discuss the two-stage online Monte Carlo particle filtering inference module, which generates the beat and downbeat predictions. Sequential Monte Carlo particle filtering is a sampling-based model which iteratively estimates any unknown distribution p(x) by gathering a large number of independent samples from an arbitrary proposal distribution. The unknown distribution of interest in our case, up to the K-th frame, is the following posterior p(x 1:K |y 1:K ) of underlying beat or downbeat positions x 1:K conditioned on beat observations y 1:K . It can be inferred according to the key equations below. For more detailed information, please refer to our previous work <ref type="bibr" target="#b30">[31]</ref>.</p><formula xml:id="formula_0">p(x) = lim N ?? N i=1 ? (i) N i=1 ? (i) ? x ? x (i) ,<label>(1)</label></formula><formula xml:id="formula_1">p x (i) 1:K |y 1:K ? K k=1 p y k |x (i) k p x (i) k |x (i) k?1 , (2) ? (i) k = p y k |x (i) k ? (i) k?1 ,<label>(3)</label></formula><p>where ? (i) is the importance weight of particle i, and ?(?) is the Dirac function. Eq. (1) describes the estimation of p(x) using a large number of particles (N ? ?) and their importance weights. Eq. (2) is a dynamic model which updates the posterior of each frame k using the transition (motion) and observation (correction) probabilities. Eq.</p><p>(3) describes a recursive process to update the importance weights using the current observation and the importance weights of the previous step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.1">State spaces, transition and observation models</head><p>We use a cascade of two sequential Monte Carlo particle filters, one for beat tracking, and the other for downbeat and meter tracking. The state space and transition model of the beat estimator are similar to <ref type="bibr" target="#b31">[32]</ref>. The beat state space is a type of 2D bar pointer model and its transition for the phase (horizontal) and the tempo (vertical) of the frame are described in Eqs. (4) and (5), respectively. The phase of frame k within a beat interval and the tempo at frame k are respectively denoted by ? b,k and? b,k . A constant ? b influences the intensity of potential jumps across the tempo axis.</p><p>We propose a new beat observation model in Eq. <ref type="formula" target="#formula_4">(6)</ref>, where x b,k and y b,k are the beat state and beat observations at frame k. For non-beat states we allocate a small likelihood as ? = 0.03 instead of using the non-beat activation output from the neural network. For beat frames, since downbeats can also be considered beats, we assess the maximum of the beat and downbeat activations. If the maximum exceeds a certain threshold, i.e., T = 0.4, then it is set as the likelihood; Otherwise, ? is used. When ? is used, we also bypass the costly re-sampling step in the beat particle filtering module. Therefore, the threshold serves as an information gate, through which the computational cost is significantly reduced.</p><formula xml:id="formula_2">? b,k = (? b,k?1 +? b,k?1 ) mod (? max b + 1),<label>(4)</label></formula><formula xml:id="formula_3">p(? b,k |? b,k?1 ) = exp ?? b ? b,k ? b,k?1 if ? b,k = 0 1(? b,k =? b,k?1 ) if ? b,k &gt; 0 ,<label>(5)</label></formula><formula xml:id="formula_4">p(y b,k |x b,k ) = ? ? ? max(b k , d k ) if ? b,k = 0 and max(b k , d k ) ? T ? otherwise ,<label>(6)</label></formula><p>The second particle filter detects downbeats and the time signature jointly. The state space is similar to that of beat tracking. However, here we introduce? d,k corresponding to the meter, i.e.,? d,k ? 2, 3, ...,? max d , and ? d,k to describe the phase of the beat within the bar interval, i.e. ? d,k ? 0, 1, 2, ..., ? max d . Eqs. <ref type="formula" target="#formula_5">(7)</ref> and <ref type="formula">(8)</ref> describe the phase and meter transition models. We only let meter change at the states belonging to the downbeat area i.e. ? d,k = 0, and ? d is a constant parameter that decides what percent of the particles jump to other meters at the downbeat states. Also, in Eq. (9) we define the observation model used in the downbeat particle filter. The first states within the bar (downbeat area) take the downbeat activation and the rest of them (beat states) take the beat activation. Note that as the second particle filter operates less often, i.e., only when a beat is detected, no information gate is needed here.</p><formula xml:id="formula_5">? d,k = (? d,k?1 +? d,k?1 ) mod (? max d + 1),<label>(7)</label></formula><formula xml:id="formula_6">p(? d,k |? d,k?1 ) = ? ? ? ? ? ? ? ? ? ? ? ? d if ? d,k = 0 an? ? d,k =? d,k?1 1 ? ? d if ? d,k = 0 and , ? d,k =? d,k?1 1(? d,k =? d,k?1 ) if ? d,k &gt; 0 (8) p(y d,k |x d,k ) = d k if ? d,k = 0 b k if ? d,k &gt; 0 ,<label>(9)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.2">Inference process</head><p>Algorithm 1 describes the inference process in detail. Particles are initialized randomly for both inference modules by sampling from a uniform distribution within their state space. By proceeding to a new frame, particles within the beat state space are transferred to the new positions by sampling from the transition model, and new importance weights are then calculated and normalized. If the activations of the frame satisfy the information gate condition, the re-sampling process is invoked for all particles; Otherwise, the re-sampling step is skipped as it is likely a non-beat frame. Afterwards, if the median of the particles is within the tolerance window T w of a beat area and the time of the current frame is longer enough than the last detected beat considering the estimated tempo, the frame is classified as a beat frame. A similar process follows for the downbeat and meter inference module. </p><formula xml:id="formula_7">Sample (x (i) b,0 ) ? U(S b ), (x (j) d,0 ) ? U(S d ) Set w (i) b,0 = 1 N b , w (j) d,0 = 1 N d for k = 1 to K do Sample (x (i) b,k ) ? p(? (i) b,k |? (i) b,k?1 ), p(? (i) b,k |? (i) b,k?1 ) ? (i) b,k = ? (i) b,k?1 ? p(y b,k |x (i) b,k ) ?i ? N b ? (i) b,k =? (i) b,k ? (i) b,k ?i ? N b if max(b k , d k ) ? T then Resample x (i) b,k according to ? (i) b,k end if if median(? (i) b,k ) &lt; T w and (k? ? beats[?1]) &gt; 0.4 median(? (i) b,k ) then Append (beats, k?) Sample (x (j) d,k ) ? p(? (j) d,k |? (j) d,k?1 ), p(? (j) d,k |? (j) d,k?1 ) ? (j) d,k = ? (j) d,k?1 ? p(y d,k |x (j) d,k ) ?j ? N d ? (j) d,k =? (j) d,k ? (j) d,k ?j ? N d Resample x (j) d,k according to ? (j) d,k if mode(? (j) d,k ) == 0 then append (downbeats, k?) append (meters, mode(? (j) d,k )) end if end if end for</formula><p>A visualization of the inference process is presented in <ref type="figure" target="#fig_1">Figure 3</ref>. Each pair of plots demonstrates one step of the inference procedure, where the top and the bottom plots show the beat and downbeat tracking process, respectively. In the first pair of plots, the beat particles are initialized randomly. In the second pair, the first beat is detected and the downbeat state particles are simultaneously initialized randomly. In the third pair, beat tracking particles have converged, but the downbeat particles have not yet converged. Here the downbeat clutter is located in the lowest row of the downbeat state space, which represents a six-beat time signature. The next few plot pairs illustrate convergence of both the beat and downbeat particles, producing an estimate of the tempo and beat phase (top plots), and the meter and bar phase (bottom plots).  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Methodology</head><p>In order to analyze the performance of BeatNet, we compare it to several publicly available online beat tracking methods, We additionally provide the online downbeat tracking performance of BeatNet for each of the experiments. Following standard evaluation practices, in this work, F-measure with a tolerance window of T w = ?70 ms is used as the evaluation metric for all experiments.</p><p>We utilize all five datasets <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b32">[33]</ref><ref type="bibr" target="#b33">[34]</ref><ref type="bibr" target="#b34">[35]</ref><ref type="bibr" target="#b35">[36]</ref><ref type="bibr" target="#b36">[37]</ref><ref type="bibr" target="#b37">[38]</ref> described in <ref type="table" target="#tab_1">Table 1</ref> for training, validation, and testing, with different splits and arrangements for various experiments. In the first comparison, we evaluate BeatNet on the GTZAN dataset, which covers 10 different music genres and was unseen from training of all comparison methods. In order to demonstrate the generalization ability of our approach, we also experiment with two other comparison schema where we respectively set aside the Ballroom and Rock datasets during training and use them entirely for evaluation. Note that all of the supervised comparison methods included the Ballroom and Rock datasets in their training set, so we only compare BeatNet with unsupervised methods in these cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Training Details</head><p>For training the beat and downbeat activation neural network described in Section 2.2, all weights and biases are initialized randomly, and the network is trained using Adam optimizer with a learning rate of 5 ? 10 ?4 and a batch size of 200. Since the number of non-beat frames within a music piece is typically much larger than the number of beat and downbeat frames, our objective function is chosen to be weighted cross entropy loss of the beat, downbeat, and non-beat, where the weights are inverse proportional to the frequency of occurrence of each type of frame. Batches comprise 8-second long excerpts randomly sampled from each audio file available in the training set. Given that some datasets (e.g., Beatles) contain full songs and others (e.g., Ballroom) contain short excerpts of songs, we sample from longer audio files more often during the training Batch creation. Training proceeds until the performance on the validation set has not increased over a span of 20 epochs for a given experiment.  <ref type="bibr" target="#b3">[4]</ref> 64.63 -B?ck FF <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b19">20]</ref> 74.18 -DLB <ref type="bibr" target="#b30">[31]</ref> 73.77 -IBT <ref type="bibr" target="#b10">[11]</ref> 68.99 -</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ballroom Dataset</head><p>Aubio <ref type="bibr" target="#b8">[9]</ref> 56.73 -BeatNet 77.41 47.45 IBT <ref type="bibr" target="#b10">[11]</ref> 70.79 -</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Rock Corpus Dataset</head><p>Aubio <ref type="bibr" target="#b8">[9]</ref> 59.83 -BeatNet 73.13 44.98 IBT <ref type="bibr" target="#b10">[11]</ref> 68.55 -</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparison of Offline Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GTZAN Dataset</head><p>BeatNet + DBN 80.64 54.07 B?ck <ref type="bibr" target="#b19">[20]</ref> 79.09 51.36 <ref type="table">Table 2</ref>. Comparison of BeatNet with other beat and downbeat tracking methods on various datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Results and Discussion</head><p>The evaluation results of the proposed BeatNet model and comparison methods are presented in <ref type="table">Table 2</ref>. All online comparison methods only perform beat tracking, and all except IBT <ref type="bibr" target="#b10">[11]</ref> and Aubio <ref type="bibr" target="#b8">[9]</ref> are supervised methods using deep neural networks. We can see that the online beat tracking portion of BeatNet outperforms all comparison methods. The B?ck FF <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b19">20]</ref> and Don't Look Back (DLB) models <ref type="bibr" target="#b30">[31]</ref> achieve the next best performance. B?ck FF uses the forward algorithm to estimate beats in a similar manner to the other online joint model described earlier <ref type="bibr" target="#b24">[25]</ref>. Aside from the different neural network structures, the beat tracking inference processes of the DLB model <ref type="bibr" target="#b30">[31]</ref> and BeatNet are largely the same. The main difference is that the latter benefits from the information gate, which decreases the computational time drastically. Additionally, we report the performance comparison with an offline joint beat and downbeat tracking model <ref type="bibr" target="#b19">[20]</ref> on the GTZAN dataset. In this case, we replaced the particle filtering modules of BeatNet with the DBN used in <ref type="bibr" target="#b19">[20]</ref> to directly compare neural network architectures in BeatNet and <ref type="bibr" target="#b19">[20]</ref>. Same to <ref type="bibr" target="#b19">[20]</ref>, we also provided the time signatures to the DBN. For <ref type="bibr" target="#b19">[20]</ref>, we utilized the Madmom <ref type="bibr" target="#b38">[39]</ref> library, which is the official implementation of the paper. Note that due to the existence of different GTZAN beat annotations, the reported offline results obtained by us differ from those of the original paper <ref type="bibr" target="#b19">[20]</ref>. However, since we used the same annotations for all of the experiments, the offline comparison is valid. As the table suggests, with the same DBN estimator, both neural networks yield similar results for beat tracking. However, for downbeat tracking, the BeatNet architecture yields marginally better performance. These results are interesting, since we are comparing a causal network to a non-causal network which leverages bidirectional recurrence. However, our network is larger and contains more parameters.</p><p>The comparison between BeatNet (second row) and <ref type="bibr" target="#b19">[20]</ref> (last row) is also interesting. BeatNet underperforms <ref type="bibr" target="#b19">[20]</ref> by 3.65% on beat tracking and by 4.9% on downbeat tracking. However, it is noted that BeatNet is an online method and it does not require the time signature input, while <ref type="bibr" target="#b19">[20]</ref> is offline method and it requires the time signature input.</p><p>One limitation of our model is that the performance of the downbeat tracker depends on the beat tracker. This means that if the beat tracker makes incorrect predictions, errors will carry through to the downbeat tracker. This is a common characteristic of cascade systems such as <ref type="bibr" target="#b24">[25]</ref>. Another limitation is the high computation cost of sequential Monte Carlo particle filtering methods. This limitation has been partially addressed in our previous work <ref type="bibr" target="#b30">[31]</ref> by using efficient models, e.g., <ref type="bibr" target="#b31">[32]</ref> in the inference stage. The information gate proposed in this paper further reduces the computational cost.</p><p>On a typical windows machine with AMD Ryzen 9 3900X CPU and 3.80 GHz clock, the processing time for the pre-processing stage and passing a frame through the neural network is 0.12 ms and 0.01 ms, respectively. These times are relatively insignificant, as the inference process takes more time. The inference process takes 5.23 and 8.87 seconds using 1000 and 1750 particles, respectively, to process a 30-sec long music excerpt. This is much faster than the previous sampling-based model <ref type="bibr" target="#b30">[31]</ref> which took 21.30 seconds using a 1000 particle setup. Larger numbers of particles lead to longer processing times with a roughly linear relationship. Hence, we reported these results using 1500 particles for the beat inference block and 250 for the downbeat inference block (1750 particles in total) to keep the process minimal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">CONCLUSION</head><p>We proposed BeatNet, a new online system for joint beat, downbeat, and meter tracking. The system incorporates a convolutional-recurrent neural network for generating beat and downbeat activations in each audio frame, and a two-stage particle filtering algorithm to estimate tempo, beats, downbeats, and musical meter. An information gate is added to the beat tracking particle filter to skip many re-sampling steps hence reduces the computational cost significantly. The system is compared to multiple online and offline methods under various experimental conditions, and it achieves superior performance for both online beat and downbeat tracking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">ACKNOWLEDGEMENT</head><p>This work has been partially supported by the National Science Foundation grants 1846184 and DGE-1922591.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Proposed CRNN architecture for processing input features and computing beat and downbeat activations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Inference example, detailed in Section 2.3.2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Datasets used for training and testing.</figDesc><table><row><cell>Dataset</cell><cell># Files</cell><cell>Total Length</cell></row><row><cell>Ballroom [33, 34]</cell><cell>685</cell><cell>5 h 57 m</cell></row><row><cell>Beatles [2]</cell><cell>180</cell><cell>8 h 9 m</cell></row><row><cell>Carnatic [35]</cell><cell>176</cell><cell>16 h 38 m</cell></row><row><cell>GTZAN [36, 37]</cell><cell>999</cell><cell>8 h 20 m</cell></row><row><cell>Rock Corpus [38]</cell><cell>200</cell><cell>12 h 53 m</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/mjhydri/BeatNet</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Beat tracking with dynamic programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ellis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of New Music Research</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="51" to="60" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Evaluation methods for musical audio beat tracking algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E P</forename><surname>Davies</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Degara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Plumbley</surname></persName>
		</author>
		<idno>C4DM-TR-09-06</idno>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
		<respStmt>
			<orgName>Centre for Digital Music, Queen Mary University of London</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">A computational approach to rhythm description Audio features for the computation of rhythm periodicity functions and their use in tempo induction and music content processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Gouyon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
		<respStmt>
			<orgName>Universitat Pompeu Fabra</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Enhanced beat tracking with context-aware neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>B?ck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schedl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ofthe 14th International Conference on Digital Audio Effects</title>
		<meeting>ofthe 14th International Conference on Digital Audio Effects</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="135" to="140" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Temporal convolutional networks for musical audio beat tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E P</forename><surname>Davies</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>B?ck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 27th European Signal Processing Conference (EUSIPCO)</title>
		<meeting>of the 27th European Signal essing Conference (EUSIPCO)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A multi-model approach to beat tracking considering heterogeneous music styles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>B?ck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Krebs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Widmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 15th Intl. Conf. on Music Information Retrieval</title>
		<meeting>of the 15th Intl. Conf. on Music Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="603" to="608" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Beat tracking towards automatic musical accompaniment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E P</forename><surname>Davies</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">M</forename><surname>Brossier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Plumbley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Audio Eng. Soc. Conv. Spring Prepr</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="751" to="757" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for real-time beat tracking: A dancing robot application</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gkiokas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Katsouros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 18th Intl. Conf. on Music Information Retrieval</title>
		<meeting>of the 18th Intl. Conf. on Music Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="286" to="293" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Automatic annotation of musical audio for interactive applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">M</forename><surname>Brossier</surname></persName>
		</author>
		<editor>P. dissertation</editor>
		<imprint>
			<date type="published" when="2006-08" />
			<biblScope unit="page" from="58" to="102" />
			<pubPlace>London, UK</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Queen Marry University</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">OBTAIN: Real-time beat tracking in audio signals index terms-onset strength signal, tempo estimation, beat onset, cumulative beat strength signal, peak detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mottaghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Behdin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Esmaeili</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Heydari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Marvasti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Signal Processing Systems</title>
		<imprint>
			<biblScope unit="page" from="123" to="129" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">IBT: A real-time tempo and beat tracking system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Oliveira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Gouyon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">G</forename><surname>Martins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">P</forename><surname>Reis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 11th Intl. Conf. on Music Information Retrieval</title>
		<meeting>of the 11th Intl. Conf. on Music Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="291" to="296" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">AIST annotation for the RWC music database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Goto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 7th Intl. Conf. on Music Information Retrieval</title>
		<meeting>of the 7th Intl. Conf. on Music Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="359" to="360" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Real-time rhythm tracking for drumless audio signals: Chord change detection for musical decisions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Goto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Muraoka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Speech Communication</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="311" to="335" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Music understanding at the beat level real-time beat tracking for audio signals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Goto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Muraok</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IJCAI-95 Workshop on Computational Auditory Scene Analysis</title>
		<meeting>IJCAI-95 Workshop on Computational Auditory Scene Analysis</meeting>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="page" from="67" to="75" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Robust downbeat tracking using an ensemble of convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Durand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Bello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Richard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th International Society for Music Information Retrieval Conference</title>
		<meeting>the 17th International Society for Music Information Retrieval Conference</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="255" to="261" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Downbeat tracking with multiple features and deep neural networks,&quot; in In</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Durand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">D</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Richard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Acoust. Speech Signal Process. (ICASSP)</title>
		<meeting>IEEE Int. Conf. Acoust. Speech Signal ess. (ICASSP)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Feature adapted convolutional neural networks for downbeat tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Durand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Bello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">D</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Richard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Acoust. Speech Signal Process. (ICASSP)</title>
		<meeting>IEEE Int. Conf. Acoust. Speech Signal ess. (ICASSP)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Downbeat tracking with tempo-invariant convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">D</forename><surname>Giorgi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mauch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Levy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 17th Intl. Conf. on Music Information Retrieval</title>
		<meeting>of the 17th Intl. Conf. on Music Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="216" to="222" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Simultaneous beat and downbeat-tracking using a probabilistic framework: Theory and large-scale evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Peeters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Papadopoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on audio, speech, and language processing</title>
		<imprint>
			<date type="published" when="2011-08" />
			<biblScope unit="volume">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Joint beat and downbeat tracking with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>B?ck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Krebs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Widmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 7th Intl. Conf. on Music Information Retrieval (ISMIR)</title>
		<meeting>of the 7th Intl. Conf. on Music Information Retrieval (ISMIR)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Downbeat tracking using beat-synchronous features and recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Krebs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>B?ck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dorfer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Widmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 17th Intl. Conf. on Music Information Retrieval (ISMIR)</title>
		<meeting>of the 17th Intl. Conf. on Music Information Retrieval (ISMIR)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Analysis of common design choices in deep learning systems for downbeat tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fuentes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mcfee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">C</forename><surname>Crayencour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Essid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Bello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 19th Int. Society for Music Information Retrieval Conf</title>
		<meeting>of the 19th Int. Society for Music Information Retrieval Conf</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Joint beat and downbeat tracking based on CRNN models and a comparison of using different context ranges in convolutional layers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fukayama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Goto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the International Computer Music Conference (ICMC)</title>
		<meeting>of the International Computer Music Conference (ICMC)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deconstruct, analyse, reconstruct: How to improve tempo, beat, and downbeat estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>B?ck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E P</forename><surname>Davies</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 21th Int. Society for Music Information Retrieval Conf., 2020</title>
		<meeting>of the 21th Int. Society for Music Information Retrieval Conf., 2020</meeting>
		<imprint>
			<biblScope unit="page" from="574" to="582" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">ROBOD: a real-time online beat and offbeat drummer sebastian</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>B?ck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Krebs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Durand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>P?ll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Balsyte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE signal processing cup</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Implementing and adapting a downbeat tracking system for real-time applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Liang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
		<respStmt>
			<orgName>Carnegie Mellon University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Master Thesis</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Particle filtering applied to musical tempo tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hainsworth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Macleod</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EURASIP Journal on Applied Signal Processing</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="2385" to="2395" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Beat tracking with particle filtering algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hainsworth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Macleod</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. in the IEEE Workshop on Applications of Sienal Processing to Audio and Acoustics</title>
		<meeting>in the IEEE Workshop on Applications of Sienal essing to Audio and Acoustics</meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Monte carlo methods for tempo tracking and rhythm quantization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">T</forename><surname>Cemgil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kappen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="111" to="222" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Inferring metrical structure in music using particle filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Krebs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Holzapfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">T</forename><surname>Cemgil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Widmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on audio</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="111" to="222" />
			<date type="published" when="2015-05" />
		</imprint>
	</monogr>
	<note>and language processing</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Don&apos;t look back: An online beat tracking method using RNN and enhanced particle filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Heydari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Duan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Acoust. Speech Signal Process. (ICASSP)</title>
		<meeting>IEEE Int. Conf. Acoust. Speech Signal ess. (ICASSP)</meeting>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">An efficient statespace model for joint tempo and meter tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Krebs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>B?ck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Widmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 16th Intl. Conf. on Music Information Retrieval (ISMIR)</title>
		<meeting>of the 16th Intl. Conf. on Music Information Retrieval (ISMIR)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">An experimental comparison of audio tempo induction algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Gouyon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Klapuri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dixon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Alonso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tzanetakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Uhle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Cano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Rhythmic pattern modeling for beat and downbeat tracking in musical audio</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Krebs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>B?ck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Widmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 14th Int. Society for Music Information Retrieval Conf</title>
		<meeting>of the 14th Int. Society for Music Information Retrieval Conf</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A supervised approach to hierarchical metrical cycle tracking from audio music recordings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Srinivasamurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Serra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE Int. Conference on Acoustics, Speech, and Signal Processing (ICASSP)</title>
		<meeting>of the IEEE Int. Conference on Acoustics, Speech, and Signal essing (ICASSP)</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Swing ratio estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Marchand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Peeters</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 18th Int. Conference on Digital Audio Effects (DAFx)</title>
		<meeting>of the 18th Int. Conference on Digital Audio Effects (DAFx)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Musical genre classification of audio signals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tzanetakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Cook</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Speech and Audio Processing</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A corpus analysis of rock harmony</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>De Clercq</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Temperley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Popular Music</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="47" to="70" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Madmom: A new python audio and music signal processing library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>B?ck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Korzeniowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schl?ter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Krebs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Widmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM Multimed. Conf, MM 2016</title>
		<meeting>ACM Multimed. Conf, MM 2016</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1174" to="1178" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
