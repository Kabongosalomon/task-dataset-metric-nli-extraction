<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Focal Frequency Loss for Image Reconstruction and Synthesis</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liming</forename><surname>Jiang</surname></persName>
							<email>liming002@ntu.edu.sg</email>
							<affiliation key="aff0">
								<orgName type="laboratory">S-Lab</orgName>
								<orgName type="institution">Nanyang Technological University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Dai</surname></persName>
							<email>bo.dai@ntu.edu.sg</email>
							<affiliation key="aff0">
								<orgName type="laboratory">S-Lab</orgName>
								<orgName type="institution">Nanyang Technological University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wayne</forename><surname>Wu</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">SenseTime Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
							<email>ccloy@ntu.edu.sg</email>
							<affiliation key="aff0">
								<orgName type="laboratory">S-Lab</orgName>
								<orgName type="institution">Nanyang Technological University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Focal Frequency Loss for Image Reconstruction and Synthesis</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T02:56+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Image reconstruction and synthesis have witnessed remarkable progress thanks to the development of generative models. Nonetheless, gaps could still exist between the real and generated images, especially in the frequency domain. In this study, we show that narrowing gaps in the frequency domain can ameliorate image reconstruction and synthesis quality further. We propose a novel focal frequency loss, which allows a model to adaptively focus on frequency components that are hard to synthesize by down-weighting the easy ones. This objective function is complementary to existing spatial losses, offering great impedance against the loss of important frequency information due to the inherent bias of neural networks. We demonstrate the versatility and effectiveness of focal frequency loss to improve popular models, such as VAE, pix2pix, and SPADE, in both perceptual quality and quantitative performance. We further show its potential on StyleGAN2. 1, 2</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>We have seen remarkable progress in image reconstruction and synthesis along with the development of generative models <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b64">65]</ref>, and the progress continues with the emergence of various powerful deep learning-based approaches <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b63">64</ref>]. Despite their immense success, one could still observe the gaps between the real and generated images in certain cases.</p><p>These gaps are sometimes manifested in the form of artifacts that are discernible. For instance, upsampling layers using transposed convolutions tend to produce checkerboard artifacts <ref type="bibr" target="#b48">[49]</ref>. The gaps, in some other cases, may only be revealed through the frequency spectrum analysis. Recent studies <ref type="bibr" target="#b67">[68,</ref><ref type="bibr" target="#b82">83,</ref><ref type="bibr" target="#b23">24]</ref> in media forensics have shown some notable periodic patterns in the frequency spectra of manipulated images, which may be consistent with artifacts in the spatial domain. In <ref type="figure">Figure 1</ref>, we show some paired examples of real images and the fake ones generated by typical generative models for image reconstruction and synthesis. It is observed that the frequency domain gap between 1 GitHub: https://github.com/EndlessSora/focal-frequency-loss. <ref type="bibr" target="#b1">2</ref> Project page: https://www.mmlab-ntu.com/project/ffl/index.html. real fake real fake</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Vanilla AE</head><p>real fake VAE real fake GAN (pix2pix) <ref type="figure">Figure 1</ref>. Frequency domain gaps between the real and the generated images by typical generative models in image reconstruction and synthesis. Vanilla AE <ref type="bibr" target="#b20">[21]</ref> loses important frequencies, leading to blurry images (Row 1 and 2). VAE <ref type="bibr" target="#b37">[38]</ref> biases to a limited spectrum region (Row 3), losing high-frequency information (outer regions and corners). Unnatural periodic patterns can be spotted on the spectra of images generated by GAN (pix2pix) <ref type="bibr" target="#b25">[26]</ref> (Row 4), consistent with the observable checkerboard artifacts (zoom in for view). In some cases, a frequency spectrum region shift occurs to GAN-generated images (Row 5).</p><p>the real and fake images may be a common issue for these methods, albeit in slightly different forms. The observed gaps in the frequency domain may be imputed to some inherent bias of neural networks when applied to reconstruction and synthesis tasks. Fourier analysis highlights a phenomenon called spectral bias <ref type="bibr" target="#b54">[55,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b61">62]</ref>, a learning bias of neural networks towards lowfrequency functions. Thus, generative models tend to eschew frequency components that are hard to synthesize, i.e., hard frequencies, and converge to an inferior point. F-Principle <ref type="bibr" target="#b76">[77]</ref> shows that the priority of fitting certain fre-quencies in a network is also different throughout the training, usually from low to high. Consequently, it is difficult for a model to maintain important frequency information as it tends to generate frequencies with a higher priority.</p><p>In this paper, we carefully study the frequency domain gap between real and fake images and explore ways to ameliorate reconstruction and synthesis quality by narrowing this gap. Existing methods <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b49">50]</ref> usually adopt pixel losses in the spatial domain, while spatial domain losses hardly help a network find hard frequencies and synthesize them, in that every pixel shares the same significance for a certain frequency. In comparison, we transform both the real and generated samples to their frequency representations using the standard discrete Fourier transform (DFT). The images are decomposed into sines and cosines, exhibiting periodic properties. Each coordinate value on the frequency spectrum depends on all the image pixels in the spatial domain, representing a specific spatial frequency. Explicitly minimizing the distance of coordinate values on the real and fake spectra can help networks easily locate difficult regions on the spectrum, i.e., hard frequencies.</p><p>To tackle these hard frequencies, inspired by hard example mining <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b58">59]</ref> and focal loss <ref type="bibr" target="#b40">[41]</ref>, we propose a simple yet effective frequency-level objective function, named focal frequency loss. We map each spectrum coordinate value to a Euclidean vector in a two-dimensional space, with both the amplitude and phase information of the spatial frequency put under consideration. The proposed loss function is defined by the scaled Euclidean distance of these vectors by down-weighting easy frequencies using a dynamic spectrum weight matrix. Intuitively, the matrix is updated on the fly according to a non-uniform distribution on the current loss of each frequency during training. The model will then rapidly focus on hard frequencies and progressively refine the generated frequencies to improve image quality.</p><p>The main contribution of this work is a novel focal frequency loss that directly optimizes generative models in the frequency domain. We carefully motivate how a loss can be built on a space where frequencies of an image can be well represented and distinguished, facilitating optimization in the frequency dimension. We further explain the way that enables a model to focus on hard frequencies, which may be pivotal for quality improvement. Extensive experiments demonstrate the effectiveness of the proposed loss on representative baselines <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b49">50]</ref>, and the loss is complementary to existing spatial domain losses such as perceptual loss <ref type="bibr" target="#b29">[30]</ref>. We further show the potential of focal frequency loss to improve state-of-the-art StyleGAN2 <ref type="bibr" target="#b33">[34]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Image reconstruction and synthesis. Autoencoders (AE) <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b37">38]</ref> and generative adversarial networks (GAN) <ref type="bibr" target="#b15">[16]</ref> are two popular models for image reconstruction and synthesis. The vanilla AE <ref type="bibr" target="#b20">[21]</ref> aims at learning latent codes while reconstructing images. It is typically used for dimensionality reduction and feature learning. Autoencoders have been widely used to generate images since the development of variational autoencoders (VAE) <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b36">37]</ref>. Their applications have been extended to various tasks, e.g., face manipulation <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b26">27]</ref>. GAN <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b52">53]</ref>, on the other hand, is extensively applied in face generation <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b33">34]</ref>, imageto-image translation <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b84">85,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b28">29]</ref>, style transfer <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b22">23]</ref>, and semantic image synthesis <ref type="bibr" target="#b68">[69,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b42">43]</ref>. Existing approaches usually apply spatial domain loss functions, e.g., perceptual loss <ref type="bibr" target="#b29">[30]</ref>, to improve quality while seldom consider optimization in the frequency domain. Spectral regularization <ref type="bibr" target="#b10">[11]</ref> presents a preliminary attempt. Different from <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b10">11]</ref>, the proposed focal frequency loss dynamically focuses the model on hard frequencies by downweighting the easy ones and ameliorates image quality through the frequency domain directly. Some concurrent works on image reconstruction and synthesis via the frequency domain include <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b30">31]</ref>. Frequency domain analysis of neural networks. In addition to the studies <ref type="bibr" target="#b54">[55,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b61">62,</ref><ref type="bibr" target="#b76">77]</ref> we discussed in the introduction, we highlight some recent works that analyze neural networks through the frequency domain. Using coordinatebased MLPs, Fourier features <ref type="bibr" target="#b61">[62,</ref><ref type="bibr" target="#b55">56]</ref> and positional encoding <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b65">66]</ref> are adopted to recover missing high frequencies in single image regression problems. Besides, several studies have incorporated frequency analysis with network compression <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b75">76,</ref><ref type="bibr" target="#b77">78,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b17">18]</ref> and feature reduction <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b69">70]</ref> to accelerate the training and inference of networks. The application areas of the frequency domain analysis have been further extended, including media forensics <ref type="bibr" target="#b67">[68,</ref><ref type="bibr" target="#b82">83,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b12">13]</ref>, super-resolution <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b71">72]</ref>, generalization analysis <ref type="bibr" target="#b66">[67,</ref><ref type="bibr" target="#b21">22]</ref>, magnetic resonance imaging <ref type="bibr" target="#b60">[61]</ref>, image rescaling <ref type="bibr" target="#b72">[73]</ref>, etc. Despite the wide exploration of various problems, improving reconstruction and synthesis quality via the frequency domain remains much less explored. Hard example processing. Hard example processing is widely explored in object detection and image classification to address the class imbalance problem. A common solution is to use a bootstrapping technique called hard example mining <ref type="bibr" target="#b58">[59,</ref><ref type="bibr" target="#b11">12]</ref>, where a representative method is online hard example mining (OHEM) <ref type="bibr" target="#b58">[59]</ref>. The training examples are sampled following the current loss of each example to modify the stochastic gradient descent. The model is encouraged to learn hard examples more to boost performance. An alternative solution is focal loss <ref type="bibr" target="#b40">[41]</ref>, which is a scaled cross-entropy loss. The scaling factor down-weights the contribution of easy examples during training so that a model can focus on learning hard examples. The proposed focal frequency loss is inspired by these techniques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Focal Frequency Loss</head><p>To formulate our method, we explicitly exploit the frequency representation of images (Section 3.1), facilitating the network to locate the hard frequencies. We then define a frequency distance (Section 3.2) to quantify the differences between images in the frequency domain. Finally, we adopt a dynamic spectrum weighting scheme (Section 3.3) that allows the model to focus on the on-the-fly hard frequencies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Frequency Representation of Images</head><p>In this section, we revisit and highlight several key concepts of the discrete Fourier transform. We demonstrate the effect of missing frequencies in the image and the advantage of frequency representation for locating hard frequencies.</p><p>Discrete Fourier transform (DFT) is a complex-valued function that converts a discrete finite signal into its constituent frequencies, i.e., complex exponential waves. An image 3 can be treated as a two-dimensional discrete finite signal with only real numbers. Thus, to convert an image into its frequency representation, we perform the 2D discrete Fourier transform:</p><formula xml:id="formula_0">F (u, v) = M ?1 x=0 N ?1 y=0 f (x, y) ? e ?i2?( ux M + vy N ) ,<label>(1)</label></formula><p>where the image size is M ? N ; (x, y) denotes the coordinate of an image pixel in the spatial domain; f (x, y) is the pixel value; (u, v) represents the coordinate of a spatial frequency on the frequency spectrum; F (u, v) is the complex frequency value; e and i are Euler's number and the imaginary unit, respectively. Following Euler's formula:</p><formula xml:id="formula_1">e i? = cos ? + i sin ?,<label>(2)</label></formula><p>the natural exponential function in Eq. (1) can be written as:</p><formula xml:id="formula_2">e ?i2?( ux M + vy N ) = cos 2? ux M + vy N ? i sin 2? ux M + vy N .<label>(3)</label></formula><p>According to Eq. (1) and Eq. (3), the image is decomposed into orthogonal sine and cosine functions, constituting the imaginary and the real part of the frequency value, respectively, after applied 2D DFT. Each sine or cosine can be regarded as a binary function of (x, y), where its angular frequency is decided by the spectrum position (u, v). The mixture of these sines and cosines provides both the horizontal and vertical frequencies of an image. Therefore, spatial frequency manifests as the 2D sinusoidal components in the image. The spectrum coordinate (u, v) also represents the angled direction of a spatial frequency (visualizations can be found in the Appendix), and F (u, v) shows the "response" of the image to this frequency. Due to the periodicity of trigonometric functions, the frequency representation of an image also acquires periodic properties.</p><p>Note that in Eq. (1), F (u, v) is the sum of a function that traverses every image pixel in the spatial domain, hence a spectrum image original low-pass high-pass band-stop point-stop <ref type="figure">Figure 2</ref>. Standard bandlimiting operations on the frequency spectrum with the origin (low frequencies) center shifted and respective images in the spatial domain. These manual operations can be regarded as a simulation to show the effect of missing frequencies.</p><p>specific spatial frequency on the spectrum depends on all the image pixels. For an intuitive visualization, we suppress the single center point (the lowest frequency) of the spectrum (Column 2 of <ref type="figure">Figure 2</ref>), leading to all the image pixels being affected. To further ascertain the spatial frequency at the different regions on the spectrum, we perform some other standard bandlimiting operations and visualize their physical meanings in the spatial domain ( <ref type="figure">Figure 2)</ref>. A low-pass filter (Column 3), i.e., missing high frequencies, causes blur and typical ringing artifacts. A high-pass filter (Column 4), i.e., missing low frequencies, tends to retain edges and boundaries. Interestingly, a simple band-stop filter (Column 5), i.e., missing certain frequencies, produces visible common checkerboard artifacts (zoom in for view).</p><p>Observably, the losses of different regions on the frequency spectrum correspond to different artifacts on the image. One may deduce that compensating for these losses may reduce artifacts and improve image reconstruction and synthesis quality. The analysis here shows the value of using the frequency representation of images for profiling and locating different frequencies, especially the hard ones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Frequency Distance</head><p>To devise a loss function for the missing frequencies, we need a distance metric that quantifies the differences between real and fake images in the frequency domain. The distance has to be differentiable to support stochastic gradient descent. In the frequency domain, the data objects are different spatial frequencies on the frequency spectrum, appearing as different 2D sinusoidal components in an image. To design our frequency distance, we further study the real and imaginary part of the complex value F (u, v) in Eq. (1).</p><p>Let R (u, v) = a and I (u, v) = b be the real and the imaginary part of F (u, v), respectively. F (u, v) can be rewritten as:</p><formula xml:id="formula_3">F (u, v) = R (u, v) + I (u, v) i = a + bi.<label>(4)</label></formula><p>According to the definition of 2D discrete Fourier transform, there are two key elements in F (u, v).The first el-real amplitude only phase only amplitude + phase <ref type="figure">Figure 3</ref>. The necessity of both amplitude and phase information for a frequency distance verified by single-image reconstruction. "Amplitude/phase only" means solely applying Eq. (5)/(6) to calculate the distance between the real and reconstructed images. ement is amplitude, which is defined as:</p><formula xml:id="formula_4">|F (u, v)| = R (u, v) 2 + I (u, v) 2 = a 2 + b 2 . (5)</formula><p>Amplitude manifests the energy, i.e., how strongly an image responds to the 2D sinusoidal wave with a specific frequency. We typically show the amplitude as an informative visualization of the frequency spectrum (e.g., <ref type="figure">Figure 1</ref> and 2). The second element is phase, which is written as:</p><formula xml:id="formula_5">?F (u, v) = arctan I (u, v) R (u, v) = arctan b a .<label>(6)</label></formula><p>Phase represents the shift of a 2D sinusoidal wave from the wave with the origin value (the beginning of a cycle). A frequency distance should consider both the amplitude and the phase as they capture different information of an image. We show a single-image reconstruction experiment in <ref type="figure">Figure 3</ref>. Merely minimizing the amplitude difference returns a reconstructed image with irregular color patterns. Conversely, using only the phase information, the synthesized image resembles a noise. A faithful reconstruction can only be achieved by considering both amplitude and phase.</p><p>Our solution is to map each frequency value to a Euclidean vector in a two-dimensional space (i.e., a plane). Following the standard definition of a complex number, the real and imaginary parts correspond to the x-axis and yaxis, respectively. Let F r (u, v) = a r + b r i be the spatial frequency value at the spectrum coordinate (u, v) of the real image, and the corresponding F f (u, v) = a f + b f i with the similar meaning w.r.t. the fake image. We denote r r and r f as two respective vectors mapped from F r (u, v) and F f (u, v) (see <ref type="figure" target="#fig_0">Figure 4</ref>). Based on the definition of amplitude and phase, we note that the vector magnitude | r r | and | r f | correspond to the amplitude, and the angle ? r and ? f correspond to the phase. Thus, the frequency distance corresponds to the distance between r r and r f , which considers both the vector magnitude and angle. We use the (squared) Euclidean distance for a single frequency:</p><formula xml:id="formula_6">d ( r r , r f ) = r r ? r f 2 2 = |F r (u, v) ? F f (u, v) | 2 . (7)</formula><p>The frequency distance between the real and fake images can be written as the average value: </p><formula xml:id="formula_7">d (Fr, F f ) = 1 M N M ?1 u=0 N ?1 v=0 |Fr (u, v) ? F f (u, v) | 2 . (8) ! ! " " ! " ! " | ! | | " | &amp; ( , ) ' ( , ) ( &amp; , ' )</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Dynamic Spectrum Weighting</head><p>The frequency distance we defined in Eq. (8) quantitatively compares the real and fake images in the frequency domain. However, a direct use of Eq. (8) as a loss function is not helpful in coping with hard frequencies since the weight of each frequency is identical. A model would still bias to easy frequencies due to the inherent bias.</p><p>Inspired by hard example mining <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b58">59]</ref> and focal loss <ref type="bibr" target="#b40">[41]</ref>, we formulate our method to focus the training on the hard frequencies. To implement this, we introduce a spectrum weight matrix to down-weight the easy frequencies. The spectrum weight matrix is dynamically determined by a non-uniform distribution on the current loss of each frequency during training. Each image has its own spectrum weight matrix. The shape of the matrix is the same as that of the spectrum. The matrix element w (u, v), i.e., the weight for the spatial frequency at (u, v), is defined as:</p><formula xml:id="formula_8">w (u, v) = |F r (u, v) ? F f (u, v) | ? ,<label>(9)</label></formula><p>where ? is the scaling factor for flexibility (? = 1 in our experiments). We further normalize the matrix values into the range [0, 1], where the weight 1 corresponds to the currently most lost frequency, and the easy frequencies are downweighted. The gradient through the spectrum weight matrix is locked, so it only serves as the weight for each frequency. By performing the Hadamard product for the spectrum weight matrix and the frequency distance matrix, we have the full form of the focal frequency loss (FFL):</p><formula xml:id="formula_9">FFL = 1 M N M ?1 u=0 N ?1 v=0 w (u, v) |Fr (u, v) ? F f (u, v) | 2 . (10)</formula><p>The focal frequency loss can be seen as a weighted average of the frequency distance between the real and fake images. It focuses the model on synthesizing hard frequencies by down-weighting easy frequencies. Besides, the focused region is updated on the fly to complement the immediate hard frequencies, thus progressively refining the generated images and being adaptable to different methods. In practice, to apply the proposed focal frequency loss to a model, we first transform both the real and fake images into their frequency presentations using the 2D DFT. We then perform the orthonormalization for each frequency value F (u, v), i.e., dividing it by ? M N , so that the 2D DFT is unitary to ensure a smooth gradient. Finally, we employ Eq. (10) to calculate the focal frequency loss. We note that the exact form of focal frequency loss is not crucial. Some studies on the loss variants are provided in the Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Settings</head><p>Baselines. We start from image reconstruction by vanilla AE <ref type="bibr" target="#b20">[21]</ref> (i.e., a simple 2-layer MLP) and VAE <ref type="bibr" target="#b37">[38]</ref> (i.e., CNN-based). We then study unconditional image synthesis using VAE, i.e., generating images from the Gaussian noise. Besides, we also investigate conditional image synthesis using GAN-based methods. Specifically, we select two typical image-to-image translation approaches, i.e., pix2pix <ref type="bibr" target="#b25">[26]</ref> and SPADE <ref type="bibr" target="#b49">[50]</ref>. We further explore the potential of focal frequency loss (FFL) on state-of-the-art StyleGAN2 <ref type="bibr" target="#b33">[34]</ref>. In addition, we compare FFL with relevant losses <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b10">11]</ref>. The implementation details are provided in the Appendix. Datasets. We use a total of seven datasets. The datasets vary in types, sizes, and resolutions. For vanilla AE, we exploit the Describable Textures Dataset (DTD) <ref type="bibr" target="#b8">[9]</ref> and CelebA <ref type="bibr" target="#b43">[44]</ref>. For VAE, we use CelebA and CelebA-HQ <ref type="bibr" target="#b31">[32]</ref> with different resolutions. For pix2pix, we utilize the officially prepared CMP Facades <ref type="bibr" target="#b53">[54]</ref> and edges ? shoes <ref type="bibr" target="#b78">[79]</ref> datasets. For SPADE, we select two challenging datasets, i.e., Cityscapes <ref type="bibr" target="#b9">[10]</ref> and ADE20K <ref type="bibr" target="#b83">[84]</ref>. For Style-GAN2, we reuse CelebA-HQ. Please refer to the Appendix for the dataset details. Evaluation metrics. To evaluate frequency domain difference, we introduce a frequency-level metric, named Log Frequency Distance (LFD), which is defined by a modified version of Eq. <ref type="formula">(8)</ref>:</p><formula xml:id="formula_10">LFD = log 1 M N M ?1 u=0 N ?1 v=0 |Fr (u, v) ? F f (u, v) | 2 + 1 ,<label>(11)</label></formula><p>where the logarithm is only used to scale the value into a reasonable range. A lower LFD is better. Note that LFD is a full reference metric (i.e., requiring the ground truth image), so we use it in the reconstruction tasks. Besides, we integrate the evaluation protocols from prior works <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b28">29]</ref>. Specifically, we employ FID (lower is better) <ref type="bibr" target="#b19">[20]</ref> for all tasks. For the reconstruction tasks of vanilla AE and VAE, we use PSNR (higher is better), SSIM (higher is better) <ref type="bibr" target="#b70">[71]</ref>, and LPIPS (lower is better) <ref type="bibr" target="#b81">[82]</ref> in addition to LFD and FID. For the synthesis tasks of VAE,  pix2pix, and StyleGAN2, we apply IS (higher is better) <ref type="bibr" target="#b57">[58]</ref> in addition to FID. For SPADE (task-specific method for semantic image synthesis), besides FID, we follow their paper <ref type="bibr" target="#b49">[50]</ref> to use mIoU (higher is better) and pixel accuracy (accu, higher is better) for the segmentation performance of synthesized images. We use DRN-D-105 <ref type="bibr" target="#b79">[80]</ref> for Cityscapes and UperNet101 <ref type="bibr" target="#b73">[74]</ref> for ADE20K.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Results and Analysis</head><p>Vanilla AE. The results of vanilla AE <ref type="bibr" target="#b20">[21]</ref> image reconstruction are shown in <ref type="figure" target="#fig_1">Figure 5</ref>. On DTD, without the focal frequency loss (FFL), the vanilla AE baseline synthesizes blurry images, which lack sufficient texture details and only contain some low-frequency information. With FFL, the reconstructed images become clearer and show more texture details. The results on CelebA show that FFL improves a series of quality problems, e.g., face blur (Column 4), identity shift (Column 5), and expression loss (Column 6).</p><p>The quantitative evaluation results are presented in Table 1. Adding the proposed FFL to the vanilla AE baseline leads to a performance boost in most cases on the DTD and CelebA datasets w.r.t. five evaluation metrics. We note that the performance boost on CelebA is larger, indicating the effectiveness of FFL for the natural images. VAE. The results of VAE <ref type="bibr" target="#b37">[38]</ref> image reconstruction and unconditional image synthesis on CelebA are shown in Figure 6. For reconstruction, FFL helps the VAE model better retain the image clarity (Column 1), expression (Column 2), and skin color (Column 3). The unconditional synthesis results (Column 4, <ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6)</ref> show that the quality of generated images is improved after applying FFL. The generated faces become clearer and gain more texture details. For a higher resolution, we present the VAE reconstruction and synthe-     achieves better performance w.r.t. all the metrics. Besides, both FID and IS are better in the unconditional image synthesis task ( <ref type="table" target="#tab_2">Table 3)</ref>, indicating that the generated images are clearer and more photorealistic. The results suggests the effectiveness of the focal frequency loss in helping VAE to improve image reconstruction and synthesis quality. pix2pix. For conditional image synthesis, the results of pix2pix <ref type="bibr" target="#b25">[26]</ref> image-to-image translation (I2I) are shown in <ref type="figure" target="#fig_3">Figure 8</ref>. On CMP Facades, FFL improves the image synthesis quality of pix2pix by reducing unnatural colors (Column 1) or the black artifacts on the building (Column 2). Meanwhile, the semantic information alignment with the mask becomes better after applying FFL. For the edges ? shoes translation, pix2pix baseline sometimes introduces colored checkerboard artifacts to the white background (Column 3, zoom in for view). Besides, atypical colors appear in certain cases (Column 4). In comparison, the model trained with FFL yields fewer artifacts. The quantitative evaluation results of pix2pix image-toimage translation are shown in <ref type="table" target="#tab_3">Table 4</ref>. FFL contributes to a performance boost on both of the two datasets. The results of the pix2pix baseline show the adaptability of the focal frequency loss for the image-to-image translation problem. SPADE. We further explore semantic image synthesis (i.e., synthesizing a photorealistic image from a semantic seg- mentation mask) on more challenging datasets. The results of SPADE <ref type="bibr" target="#b49">[50]</ref> are shown in <ref type="figure">Figure 10</ref>. In the street scene of Cityscapes (Column 1), SPADE baseline distorts the car and road, missing some important details (e.g., road line). The model trained with FFL demonstrates better perceptual quality for these details. In the outdoor scene of ADE20K (Column 2), applying FFL to SPADE boosts its ability to generate details on the buildings. Besides, for the ADE20K indoor images (Column 3), SPADE baseline produces some abnormal artifacts in certain cases. The model trained with the proposed FFL synthesizes more photorealistic images.</p><p>The quantitative test results are presented in <ref type="table">Table 5</ref> (the values used for comparison are taken from <ref type="bibr" target="#b49">[50]</ref>). We compare SPADE trained with/without FFL against a series of open-source task-specific semantic image synthesis meth- <ref type="table">Table 5</ref>. The mIoU (higher is better), pixel accuracy (accu, higher is better) and FID (lower is better) scores for the SPADE semantic image synthesis trained with/without the focal frequency loss (FFL) compared to a series of task-specific methods.  ods <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b68">69]</ref>. SIMS <ref type="bibr" target="#b51">[52]</ref> obtains the best FID but poor segmentation scores on Cityscapes in that it directly stitches the training image patches from a memory bank while not keeping the exactly consistent positions. Without modifying the SPADE network structure, training with FFL contributes a further performance boost, greatly outperforming the benchmark methods, which suggests the effectiveness of FFL for semantic image synthesis. StyleGAN2. We apply FFL to the mini-batch average spectra of the real images and the generated images by the stateof-the-art unconditional image synthesis method, i.e., Style-GAN2 <ref type="bibr" target="#b33">[34]</ref>, intending to narrow the frequency distribution gap and improve quality further. The results on CelebA-HQ (256 ? 256) without truncation <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b33">34]</ref> are shown in <ref type="figure">Figure 9</ref>. Although StyleGAN2 (w/o FFL) generates photorealistic images in most cases, some tiny artifacts can still be spotted on the background (Column 2 and 4) and face (Column 5). Applying FFL, such artifacts are reduced, ameliorating synthesis quality further. Observably, the frequency domain gaps between mini-batch average spectra are clearly mitigated by FFL (Column 8). Some higher-resolution re- sults can be found in the Appendix.</p><p>The quantitative results are reported in <ref type="table" target="#tab_5">Table 6</ref>. FFL improves both FID and IS, in line with the visual quality enhancement. The results on StyleGAN2 show the potential of FFL to boost state-of-the-art baseline performance. Comparison with relevant losses. For completeness and fairness, we compare the proposed focal frequency loss (FFL) with relevant loss functions that aim at improving image reconstruction and synthesis quality. Specifically, we select the widely used spatial-based method, i.e., perceptual loss (PL) <ref type="bibr" target="#b29">[30]</ref>, which depends on high-level features from a pre-trained VGG <ref type="bibr" target="#b59">[60]</ref> network. We also study the frequency-based approach, i.e., spectral regularization (SpReg) <ref type="bibr" target="#b10">[11]</ref>, which is derived based on the azimuthal integration of the Fourier power spectrum. Besides, we further compare with another transformation form for FFL, i.e., discrete cosine transform (DCT).</p><p>The comparison results are reported in <ref type="table" target="#tab_6">Table 7</ref>. FFL outperforms the relevant approaches (i.e., PL and SpReg) when applied to our baselines in different image reconstruction and synthesis tasks. It is noteworthy that FFL and PL are complementary, as shown by our previous experiments on SPADE, which also uses PL. Even if we replace DFT with DCT as the transformation form of FFL, the results are still better than previous methods. The performance is only slightly inferior to that obtained by FFL with DFT (i.e., Eq. <ref type="formula" target="#formula_0">(10)</ref>). We deduce that the transformation form for FFL may be flexible. At this stage, DFT may be a better choice. Ablation studies. We present ablation studies of each key component for FFL in <ref type="figure">Figure 11</ref> and corresponding quantitative results in <ref type="table" target="#tab_7">Table 8</ref>. For intuitiveness and simplicity, we use vanilla AE image reconstruction on CelebA for the evaluation.</p><p>The full FFL achieves the best performance. If we do not use the frequency representation of images (Section 3.1) and focus the model on hard pixels in the spatial domain, the synthesized images become more blurry. The quantitative results degrade. Discarding either the phase or amplitude information (Section 3.2) harms the metric performance vastly. Visually, using no phase information (amplitude only), the contour of reconstructed faces is retained, but the color is completely shifted. Without amplitude (phase only), the model cannot reconstruct the faces at all, and the full identity information is lost. This further verifies the necessity of considering both amplitude and phase information. Without focusing the model on the hard frequen-  <ref type="figure">Figure 11</ref>. Ablation studies of each key component for the focal frequency loss (FFL), i.e., frequency representation (freq), phase and amplitude (ampli) information, and dynamic spectrum weighting (focal) in the vanilla AE image reconstruction task on CelebA. cies by the dynamic spectrum weighting (i.e., directly using Eq. <ref type="formula">(8)</ref>), the results are visually similar to baseline, in line with our discussion in Section 3.3. The metrics decrease, being close to but slightly better than baseline, which may benefit from the frequency representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>The proposed focal frequency loss directly optimizes image reconstruction and synthesis methods in the frequency domain. The loss adaptively focuses the model on the frequency components that are hard to deal with to ameliorate quality. The loss is complementary to existing spatial losses of diverse baselines varying in categories, network structures, and tasks, outperforming relevant approaches. We further show the potential of focal frequency loss to improve synthesis results of StyleGAN2. Exploring other applications and devising better frequency domain optimization strategies can be interesting future works.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head><p>This appendix provides supplementary information that is not elaborated in our main paper: Section A shows some additional illustrations to explain our method further. Section B describes the implementation details in our experiments. Section C details our used datasets under diverse settings. Section D provides some studies on the variants of focal frequency loss. Section E presents additional results and analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Additional Illustrations of Methodology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1. Spatial Frequency Visualization</head><p>After applying 2D discrete Fourier transform, an image is converted into its frequency representation and decomposed into orthogonal sine and cosine functions. The angular frequency of each sine and cosine function is decided by the frequency spectrum coordinate (u, v). The spatial frequency manifests as the 2D sinusoidal components in the image. The spectrum coordinate also represents the angled direction of a specific spatial frequency. As an intuitive view, we show some examples of the 2D sinusoidal components with specific spatial frequencies in <ref type="figure">Figure 12</ref>. It is observed that the angled direction and density (angular frequency) of the waves depend on the spectrum coordinate (u, v). Besides, the complex frequency value F (u, v) can be regarded as the weight for each wave, and the weighted sum corresponds to the whole image in the spatial domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. More Intuitive Illustration</head><p>To further explain the proposed focal frequency loss (FFL), we will provide a more intuitive illustration in this section. According to <ref type="figure">Figure 12</ref>, an image (gray-scale for simplicity) is the weighted sum of different spatial frequencies. We expand the accumulated frequencies into a new dimension, thus the image can be seen as a cube in a space. The length (L) and width (W) dimensions of the cube correspond to the pixel domain, and the height (H) dimension corresponds to the frequency domain, as shown in <ref type="figure">Figure 13</ref>. Therefore, a single pixel can be seen as the orange prism, and a specific frequency can be regarded as the green plane. It is observed that each frequency (i.e., each coordinate value on the frequency spectrum) depends on all the image pixels. Due to the inherent bias of neural networks <ref type="bibr" target="#b54">[55,</ref><ref type="bibr" target="#b76">77]</ref>, a model tends to eschew some frequency components that are hard to synthesize, i.e., hard frequencies, in the H dimension. Optimizing in the spatial domain (i.e., in the L and W dimensions) hardly help the model locate these hard frequencies in the H dimension. Similarly, focusing on certain pixels (e.g., orange prism) hardly help the model tackle the hard frequencies (e.g., green plane). Intuitively, when directly optimizing in the H dimension (i.e., explicitly using the frequency representation of the image in our method), the model can easily locate hard frequencies and in turn focus on them.</p><p>In <ref type="figure">Figure 13</ref>, it is noteworthy that each frequency also affects all the image pixels in the spatial domain. When FFL directly optimizes and adaptively focuses a model in the frequency domain, the frequency components in the H dimension will be reconstructed and synthesized better. Meanwhile, the general alignment and quality of all the image pixels in the L and W dimensions will be indirectly improved by FFL, thus boosting some pixel-based metrics (e.g., PSNR and SSIM <ref type="bibr" target="#b70">[71]</ref>) and ameliorating the image reconstruction and synthesis quality.</p><p>We wish to highlight that both the spatial-based loss and frequency-based loss are important since they consider different aspects and dimensions of an image, as illustrated in <ref type="figure">Figure 13</ref>. Hence, they are complementary and not replaceable. The proposed FFL is intending to complement existing spatial losses of different methods to improve reconstruction and synthesis quality further.</p><p>In fact, the actual situation of the frequency components in an image is much more complicated, which may be a higher-dimensional representation. The visualization in this section just provides a simple and intuitive illustration to help understand the proposed method in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Implementation Details</head><p>The code used for our experiments will be made publicly available. All the experiments are conducted on the NVIDIA Tesla V100 GPUs with 32 GB memory capacity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1. Baseline Details</head><p>In this section, we will provide the implementation details of all the baselines in different image reconstruction and synthesis tasks. We select five representative methods from the two popular categories: autoencoder-based and GAN-based. Besides, we evaluate different network structures. Specifically, we explore the multilayer perceptron (MLP) network and the convolutional neural network (CNN). For CNN, the network details also vary, e.g., with or without the skip connections. In addition, we consider various basic spatial domain losses, e.g., MSE loss, L1 loss, GAN loss <ref type="bibr" target="#b15">[16]</ref>, perceptual loss <ref type="bibr" target="#b29">[30]</ref>, etc., to test the ability of focal frequency loss to complement these losses. Vanilla AE. Vanilla autoencoder <ref type="bibr" target="#b20">[21]</ref> learns the image latent representation in an unsupervised manner, traditionally used for dimension reduction and feature learning. We employ vanilla AE in the image reconstruction task. The network is a simple 2-layer MLP with a hidden size of 256. We adopt ReLU activations (except the last layer using Tanh) and no norm layers. We use Adam <ref type="bibr" target="#b34">[35]</ref> optimizer and set ? 1 = 0.9, ? 2 = 0.999. The learning rate is 0.001. Normal initialization (with mean 0.0 and standard deviation 0.02) is applied to all the networks of vanilla AE. The spatial loss is MSE loss. The models are trained on 1 GPU with a batch size of 128. We perform 200 epochs of training on DTD <ref type="bibr" target="#b8">[9]</ref> and 20 epochs of training on CelebA <ref type="bibr" target="#b43">[44]</ref>. VAE. Exploiting a reparameterization trick, the variational autoencoder <ref type="bibr" target="#b37">[38]</ref> generates images by learning the latent representation in a probability distribution manner. We use VAE for image reconstruction and unconditional image synthesis. We employ CNN for VAE, with typical convolution and transposed convolution layers. Batch normalization <ref type="bibr" target="#b24">[25]</ref> and Leaky ReLU (with a negative slope of 0.2, except the last layer using Tanh) are applied. Each convolution layer has a kernel size 4 ? 4, stride 2, and zeropadding amount 1. In the encoder, the feature map resolution is halved after each convolution block. Images are down-sampled to 4 ? 4, so the number of blocks depends on the input size (e.g., if the input size is 64 ? 64, there will be 4 blocks). After an input layer, the number of feature channels is 64. Then, the number of feature channels will double after each convolution block, while we set a maximum channel number to 512 to avoid using redundant parameters. We apply two linear layers to the encoded feature to learn ? and ? for the reparameterization. The latent size is 256. After reparameterization, an additional linear layer is used to adjust the feature to the original shape. In the decoder, the network structure is completely inverse to the encoder by replacing convolution layers with the transposed convolution layers. We use Adam <ref type="bibr" target="#b34">[35]</ref> optimizer and set ? 1 = 0.9, ? 2 = 0.999. The learning rate is 0.001. Normal initialization (with mean 0.0 and standard deviation 0.02) is applied to all the networks of VAE. The spatial losses are MSE loss and KL divergence loss <ref type="bibr" target="#b37">[38]</ref>. The models are trained on 1 GPU with a batch size of 128. We train our models for 20 epochs on CelebA <ref type="bibr" target="#b43">[44]</ref> and 400 epochs on CelebA-HQ <ref type="bibr" target="#b31">[32]</ref>. pix2pix. pix2pix <ref type="bibr" target="#b25">[26]</ref> adopts conditional GAN <ref type="bibr" target="#b46">[47]</ref> as a general-purpose solution to image-to-image translation with training pairs. We employ pix2pix for conditional image synthesis. The U-Net <ref type="bibr" target="#b56">[57]</ref> generator is applied, which is an encoder-decoder with skip connections between mirrored layers in the encoder and decoder stacks. There are 8 skip connection blocks in the generator. The patch-based discriminator is used. Adam <ref type="bibr" target="#b34">[35]</ref> optimizer is used with ? 1 = 0.5, ? 2 = 0.999. The learning rate is 0.0002. Normal initialization (with mean 0.0 and standard deviation 0.02) is applied to all the networks of pix2pix. The spatial losses are vanilla GAN loss <ref type="bibr" target="#b15">[16]</ref> and L1 loss. The models are trained on 1 GPU. We conduct 200 epochs of training on CMP Facades <ref type="bibr" target="#b53">[54]</ref> with a batch size of 1. We train the models for 15 epochs on edges ? shoes <ref type="bibr" target="#b78">[79]</ref> with a batch size of 4. For other detailed network structures and parameters, we follow the original paper <ref type="bibr" target="#b25">[26]</ref> and their released code. SPADE. As a task-specific GAN-based method for semantic image synthesis (i.e., synthesizing a photorealistic image from a semantic segmentation mask), SPADE <ref type="bibr" target="#b49">[50]</ref> resizes the segmentation mask for modulating the activations in normalization layers by a learned affine transformation. The generator is built on a series of residual blocks <ref type="bibr" target="#b18">[19]</ref> with the synchronized version of batch normalization. The multiscale patch-based discriminator <ref type="bibr" target="#b68">[69]</ref> with the instance normalization <ref type="bibr" target="#b62">[63]</ref> is exploited. Besides, spectral normalization <ref type="bibr" target="#b47">[48]</ref> is applied to all the convolutional layers in the generator and discriminator. Adam <ref type="bibr" target="#b34">[35]</ref> optimizer is exploited with ? 1 = 0, ? 2 = 0.9. Two time-scale update rule <ref type="bibr" target="#b19">[20]</ref> is applied, where the learning rates for the generator and the discriminator are 0.0001 and 0.0004, respectively. The spatial losses are hinge-based GAN loss <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b80">81]</ref>, perceptual loss <ref type="bibr" target="#b29">[30]</ref> calculated by VGG-19 <ref type="bibr" target="#b59">[60]</ref> model, and feature matching loss <ref type="bibr" target="#b68">[69]</ref>. The models are trained for 200 epochs on Cityscapes <ref type="bibr" target="#b9">[10]</ref> and ADE20K <ref type="bibr" target="#b83">[84]</ref> using 4 GPUs. The batch size is 32. For other detailed network structures and parameters, we follow the original paper <ref type="bibr" target="#b49">[50]</ref> and their released code. StyleGAN2. We further explore the potential of focal frequency loss on the state-of-the-art unconditional image synthesis method, StyleGAN2 <ref type="bibr" target="#b33">[34]</ref>. We construct the Style-GAN2 baseline on top of its open-source official implementation. The mapping network consists of 8 fully connected layers. The dimensionality of both the input latent space and intermediate latent space is 512. The activation func-tion is Leaky ReLU with a negative slope of 0.2. Several standard techniques in <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b32">33]</ref> are applied, such as the exponential moving average of generator weights, mini-batch standard deviation layer at the end of the discriminator, equalized learning rate for all the trainable parameters, etc. Adam <ref type="bibr" target="#b34">[35]</ref> optimizer is used with ? 1 = 0, ? 2 = 0.99. The spatial loss is non-saturating logistic loss <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b33">34]</ref> with R 1 regularization <ref type="bibr" target="#b44">[45]</ref>. All the models are trained with 8 V100 GPUs. The batch size is 64 for CelebA-HQ <ref type="bibr" target="#b31">[32]</ref> (256?256) and 32 for the resolution of 1024 ? 1024. For other detailed network structures and parameters, we follow the original paper <ref type="bibr" target="#b33">[34]</ref> and their released code.</p><p>As for the relevant losses used for comparison, i.e., perceptual loss <ref type="bibr" target="#b29">[30]</ref> and spectral regularization <ref type="bibr" target="#b10">[11]</ref>, we follow all the details in their papers and released code.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2. Computational Cost</head><p>The computational cost of the proposed focal frequency loss (FFL) is negligible. Take pix2pix image-to-image translation on the CMP Facades dataset as an example. The average computational training time only increases from 0.064 to 0.067 seconds per iteration after applying FFL. The memory consumption increases from 3513 to 3515 MB. This cost test is conducted on 1 NVIDIA Tesla V100 GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Dataset Details</head><p>In this section, we will provide detailed information about the seven datasets we explored. The datasets vary in types, sizes, and resolutions.</p><p>? Describable Textures Dataset (DTD). We use DTD provided by <ref type="bibr" target="#b8">[9]</ref>, which is an evolving collection of textural images in the wild, annotated with human-centric attributes. DTD contains texture images with special frequency patterns. We perform vanilla AE image reconstruction using this dataset, with 4, 512 images for training and 1, 128 images for testing. The original images are scaled and center cropped to 64 ? 64.</p><p>? CelebA. CelebA <ref type="bibr" target="#b43">[44]</ref> is a large-scale face attributes dataset covering large pose variations and background clutter. We conduct image reconstruction with vanilla AE and VAE on CelebA. Besides, we perform VAE unconditional image synthesis on CelebA. We use the cropped and aligned faces, which are more natural images. The training set contains 199, 599 images, and the test set has 3, 000 images. The images are resized and center cropped to 64 ? 64.</p><p>? CelebA-HQ. CelebA-HQ is a higher-quality version of the CelebA dataset provided by <ref type="bibr" target="#b31">[32]</ref>. The original resolution is 1024 ? 1024. We perform VAE image reconstruction on this dataset. Besides, we study the unconditional image synthesis by VAE and Style-GAN2 using CelebA-HQ. The dataset is randomly split, yielding 27, 000 images for training and 3, 000 images for evaluation. All the cropped and aligned face images are uniformly resized to 256 ? 256. For StyleGAN2, we also tried to synthesize images with a resolution of 1024 ? 1024 besides 256 ? 256.</p><p>? CMP Facades. For pix2pix image-to-image translation, we utilize the officially prepared CMP Facades <ref type="bibr" target="#b53">[54]</ref> dataset. The facades are collected from different cities around the world with diverse architectural styles. CMP Facades contains architectural labels and photos, which is suitable for mask ? image translation. The sizes of training and test sets are 400 and 106, respectively. The resolution is 256 ? 256.</p><p>? Edges ? shoes. We also exploit the officially prepared edges ? shoes dataset for pix2pix image-toimage translation. The shoe images are from UT Zap-pos50K <ref type="bibr" target="#b78">[79]</ref>. The shoes are centered on a white background. The edge maps are detected by HED <ref type="bibr" target="#b74">[75]</ref>. The numbers of images for training and testing are 49, 825 and 200, respectively. The image size is 256 ? 256.</p><p>? Cityscapes. We use the Cityscapes <ref type="bibr" target="#b9">[10]</ref> dataset for SPADE semantic image synthesis. Cityscapes dataset consists of street scene images that are mostly collected in Germany. The dataset provides instancewise, dense pixel annotations of 30 classes. The training set has 2, 975 images, and the test set contains 500 images. The images are scaled to 512 ? 256.</p><p>? ADE20K. ADE20K <ref type="bibr" target="#b83">[84]</ref> dataset contains challenging in-the-wild images with fine annotations of 150 semantic classes. We also use ADE20K for SPADE semantic image synthesis, with 20, 210 images for training and 2, 000 images for evaluation. All the images are resized to 256 ? 256.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Variant Studies</head><p>In our main paper, we mentioned that the exact form of the proposed focal frequency loss (FFL) is not crucial. In this section, we will provide some variants to extend and modify FFL. We will show some studies on these variants. For simplicity and intuitiveness, we revisit the vanilla AE image reconstruction task on CelebA. We report quantitative evaluation results for the variant studies. The visual results of variants are similar.</p><p>Several simple variants can be derived by adjusting the spectrum weight matrix parameter ?. The parameter ? controls how close the weight matrix values are, i.e., how focused the model is. The larger ? is, the model will be more focused on the hard frequencies, i.e., the weight difference for easy and hard frequencies will be larger. For the experiments we present in our main paper, we set ? = 1 (we call the main version). The results are shown in <ref type="table">Table 9</ref>. <ref type="table">Table 9</ref>. The PSNR (higher is better), SSIM (higher is better), LPIPS (lower is better), FID (lower is better) and LFD (lower is better) scores for the variant studies on the spectrum weight matrix parameter ? for the focal frequency loss. <ref type="bibr">PSNR?</ref>  Applying the main version of FFL (? = 1) shows better performance than the baseline without FFL in all the five metrics. If we set ? = 2, the quantitative results degrade from the main version, especially FID. This suggests that the model may be too focused on the hard frequencies while ignoring some important easy frequency information, albeit the results are still better than the baseline in most cases. When setting ? = 0.5, all the metric results are better than the baseline. The LPIPS and FID scores become better than the main version. The results of this variant are close to the main version of FFL. If we set ? = 0.1, the quantitative results degrade from the main version despite still better than the baseline. This indicates that the model may be too unfocused. For a trade-off, we select ? = 1 as the main version of FFL, while one may consider choosing other variants regarding the parameter ? in certain tasks for the flexibility. Besides, we study another category of variants, the patch-based focal frequency loss, where we crop an image into small patches so that the focused frequencies are at the patch level. We define the patch factor p as the number of patches on each edge. For instance, if p = 2, the image will be cropped into 2 ? 2 = 4 patches. Obviously, using the original image without cropping it into patches, i.e., the main version of FFL we defined before, corresponds to p = 1. The results are shown in <ref type="table" target="#tab_0">Table 10</ref>. We note that p = 1, 2, 4 achieve close performance regarding the five evaluation metrics, all of which are much better than the baseline. However, if we set p = 8, the quantitative performance will degrade from the previous versions, especially FID. Although the results are still better than the baseline in most cases, this indicates that the patch size should not be too small. We simply choose p = 1 as the main version of FFL for our experiments in the main paper. However, the variant studies show that the patch-based focal frequency loss may contribute to an additional performance boost in <ref type="figure" target="#fig_0">Figure 14</ref>. The spatial losses (MSE) with the same weight and random seed of the two training processes with/without focal frequency loss (FFL) for vanilla AE image reconstruction on CelebA. The spatial loss converges to a lower point with the help of FFL. certain cases. Thus, this may be another direction to extend and modify FFL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Additional Results and Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.1. Training Loss</head><p>In the main paper, we have mentioned that the proposed focal frequency loss (FFL) is complementary to existing spatial losses, e.g., MSE loss, to improve image reconstruction and synthesis quality. We further analyze the training loss in this section. We choose the vanilla AE image reconstruction task on CelebA <ref type="bibr" target="#b43">[44]</ref> for simplicity. We plot the spatial losses with the same weight and random seed of the two training processes with/without FFL in <ref type="figure" target="#fig_0">Figure 14</ref>.</p><p>It is readily observed that the spatial loss (MSE) converges to a lower point after applying FFL. This indicates that the model may converge to a better point with the help of FFL, in line with the better perceptual quality and quantitative performance we presented in our main paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.2. Frequency Domain Gap</head><p>As mentioned in the main paper, we wish to improve the image reconstruction and synthesis quality by narrowing the frequency domain gap between the real and generated images using the proposed focal frequency loss (FFL). We have shown that the gaps between mini-batch average spectra of state-of-the-art StyleGAN2 are clearly mitigated by FFL. We will show some more examples of VAE image reconstruction on the CelebA <ref type="bibr" target="#b43">[44]</ref> dataset and provide more analysis about the frequency domain gap in this section.</p><p>The results are shown in <ref type="figure" target="#fig_1">Figure 15</ref>. In the spatial domain, without applying FFL, the reconstructed faces are blurry. This may be attributed to the reparameterization operation in the latent space between the encoder and decoder, which increases the difficulty for reconstruction. Trained with FFL, the VAE model can synthesize much clearer results, being closer to the ground truth real images. The perceptual quality is better after applying FFL. In the fre-   <ref type="figure">Figure 16</ref>. Additional ablation studies of each key component for the focal frequency loss (FFL), i.e., frequency representation (freq), phase and amplitude (ampli) information, and dynamic spectrum weighting (focal) in the pix2pix image-to-image translation task on edges ? shoes (256 ? 256). The corresponding FID (lower is better) and IS (higher is better) scores are reported below the images. quency domain, in line with our visualizations in the main paper, the VAE baseline without FFL bias to a limited spectrum region, losing high-frequency information (outer regions and corners). The frequency domain gaps are clearly narrowed after adopting FFL. The spectrum distribution becomes closer to the ground truth. Besides, some essential special spectrum patterns can be generated by applying FFL. This suggests the effectiveness of focal frequency loss to narrow the frequency domain gaps and ameliorate image quality further.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.3. Additional Ablation Studies</head><p>In the main paper, we provided the ablation studies of vanilla AE image reconstruction on CelebA for intuitiveness and simplicity, intending to study the importance of each key component for the proposed focal frequency loss (FFL) while reducing the influence of other factors, such as the adversarial loss. In this section, we provide the additional ablation studies on higher-resolution images with GAN. We show the studies of pix2pix <ref type="bibr" target="#b25">[26]</ref> (i.e., GANbased method) image-to-image translation on edges ? shoes (256 ? 256) in <ref type="figure">Figure 16</ref>. The results are in line with the ablation studies in our main paper, further suggesting the importance of each key component for FFL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.4. Results on Non-Photorealistic Images</head><p>We further study the benefit of the proposed focal frequency loss (FFL) on non-photorealistic images. As an ex- <ref type="table" target="#tab_0">Table 11</ref>. The PSNR (higher is better), SSIM (higher is better), LPIPS (lower is better), FID (lower is better) and LFD (lower is better) scores for the vanilla AE image reconstruction on Dan-booru2019 Portraits (Anime) trained with/without the focal frequency loss (FFL). ample, we provide the vanilla AE image reconstruction results on Danbooru2019 Portraits <ref type="bibr" target="#b2">[3]</ref> (Anime) in <ref type="table" target="#tab_0">Table 11</ref>. Empirically, we observe that all the metrics can still be boosted by FFL. Our intuition is that FFL can also help generate non-photorealistic images since they still possess special frequency patterns that may be hard for a network to learn. FFL is adaptive for dealing with these frequencies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.5. Higher-Resolution Results on StyleGAN2</head><p>In <ref type="figure" target="#fig_8">Figure 17</ref>, we show some higher-resolution images synthesized by StyleGAN2 <ref type="bibr" target="#b33">[34]</ref> trained with or without the proposed focal frequency loss (FFL) on CelebA-HQ (1024 ? 1024). The truncation trick <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b33">34]</ref> is not applied. Although the original StyleGAN2 (w/o FFL) generates plausible images in most cases, it sometimes produces tiny artifacts on the face (Row 2) and eyes (Row 3). The details on the teeth are missing in certain cases (Row 1). The synthesized images by StyleGAN2 with FFL (w/ FFL) are very photorealistic. Besides, StyleGAN2 achieves a better FID score after applying FFL, indicating that the quality of generated images becomes better with the help of FFL. More random sampled synthesized images without truncation are shown in <ref type="figure" target="#fig_3">Figure 18</ref>, and the examples with truncation using ? = 0.5 <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b33">34]</ref> are presented in <ref type="figure">Figure 19</ref>. It is observed that all the images generated by StyleGAN2 with FFL are with very high fidelity.   <ref type="figure">Figure 19</ref>. More random sampled images (with truncation applied using ? = 0.5 <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b33">34]</ref>) synthesized by StyleGAN2 trained with the proposed FFL on CelebA-HQ (1024 ? 1024).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 4 .</head><label>4</label><figDesc>Frequency distance between rr and r f mapped from two corresponding real and fake frequency values Fr (u, v) and F f (u, v) at the spectrum position (u, v). The Euclidean distance (purple line) is used, considering both the amplitude (magnitude | rr| and | r f |) and phase (angle ?r and ? f ) information.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 5 .</head><label>5</label><figDesc>Vanilla AE image reconstruction results on the DTD (64 ? 64) and CelebA (64 ? 64) datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 6 .Figure 7 .</head><label>67</label><figDesc>VAE image reconstruction and unconditional image synthesis results on the CelebA (64 ? 64) dataset. VAE image reconstruction and unconditional image synthesis results on the CelebA-HQ (256 ? 256) dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 8 .</head><label>8</label><figDesc>pix2pix image-to-image translation results on CMP Facades (256 ? 256) and edges ? shoes (256 ? 256) datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 9 .Figure 10 .</head><label>910</label><figDesc>StyleGAN2 unconditional image synthesis results (without truncation) and the mini-batch average spectra (adjusted to better contrast) on the CelebA-HQ (256 ? 256) dataset. SPADE semantic image synthesis results on the Cityscapes (512 ? 256) and ADE20K (256 ? 256) datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>freq w/o phase w/o ampli w/o focal</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>"Figure 12 .Figure 13 .</head><label>1213</label><figDesc>Two-dimensional sinusoidal components with specific spatial frequencies in an image. The angled direction and density (angular frequency) of the waves depend on the spectrum coordinate (u, v), and F (u, v) can be seen as the weight for each wave. According toFigure 12, an image (gray-scale for simplicity) can be seen as a cube in a space, where its length (L) and width (W) dimensions correspond to the pixel domain, and the height (H) dimension corresponds to the frequency domain.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 15 .</head><label>15</label><figDesc>Frequency domain gaps are narrowed by the focal frequency loss (FFL) for VAE image reconstruction on CelebA.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 17 .</head><label>17</label><figDesc>Synthesis results (without truncation) of StyleGAN2 trained with/without the proposed FFL on CelebA-HQ (1024 ? 1024). The model with FFL achieves the FID score of 3.374, outperforming the original StyleGAN2 without FFL of 3.733.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 18 .</head><label>18</label><figDesc>More random sampled images (without truncation) synthesized by StyleGAN2 trained with the proposed FFL on CelebA-HQ (1024 ? 1024).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>The PSNR (higher is better), SSIM (higher is better), LPIPS (lower is better), FID (lower is better) and LFD (lower is better) scores for the vanilla AE image reconstruction trained with/without the focal frequency loss (FFL).</figDesc><table><row><cell>Dataset</cell><cell>FFL</cell><cell cols="4">PSNR? SSIM? LPIPS? FID?</cell><cell>LFD?</cell></row><row><cell>DTD</cell><cell>w/o</cell><cell>20.133</cell><cell>0.407</cell><cell>0.414</cell><cell cols="2">246.870 14.764</cell></row><row><cell></cell><cell>w/</cell><cell>20.151</cell><cell>0.400</cell><cell>0.404</cell><cell cols="2">240.373 14.760</cell></row><row><cell>CelebA</cell><cell>w/o</cell><cell>20.044</cell><cell>0.568</cell><cell>0.237</cell><cell>97.035</cell><cell>14.785</cell></row><row><cell></cell><cell>w/</cell><cell>21.703</cell><cell>0.642</cell><cell>0.199</cell><cell>83.801</cell><cell>14.403</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>The PSNR (higher is better), SSIM (higher is better), LPIPS (lower is better), FID (lower is better) and LFD (lower is better) scores for the VAE image reconstruction trained with/without the focal frequency loss (FFL).</figDesc><table><row><cell>Dataset</cell><cell>FFL</cell><cell cols="4">PSNR? SSIM? LPIPS? FID?</cell><cell>LFD?</cell></row><row><cell>CelebA</cell><cell>w/o</cell><cell>19.961</cell><cell>0.606</cell><cell>0.217</cell><cell cols="2">69.900 14.804</cell></row><row><cell></cell><cell>w/</cell><cell>22.954</cell><cell>0.723</cell><cell>0.143</cell><cell cols="2">49.689 14.115</cell></row><row><cell>CelebA-</cell><cell>w/o</cell><cell>21.310</cell><cell>0.616</cell><cell>0.367</cell><cell cols="2">71.081 17.266</cell></row><row><cell>HQ</cell><cell>w/</cell><cell>22.253</cell><cell>0.637</cell><cell>0.344</cell><cell cols="2">59.470 17.049</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>The FID (lower is better) and IS (higher is better) scores for the VAE unconditional image synthesis trained with/without the focal frequency loss (FFL). HQ inFigure 7. By adding FFL to the VAE baseline, the reconstructed images keep more original image information, e.g., mouth color (Column 2) and opening angle (Column 1). Besides, high-frequency details on the hair are clearly enhanced (Column 1). For unconditional image synthesis, FFL helps reduce artifacts and ameliorates the perceptual quality of synthesized images.The quantitative test results of VAE image reconstruction are shown inTable 2. Adding FFL to the VAE baseline</figDesc><table><row><cell>Dataset</cell><cell>FFL</cell><cell>FID?</cell><cell>IS?</cell></row><row><cell>CelebA</cell><cell>w/o</cell><cell>80.116</cell><cell>1.873</cell></row><row><cell></cell><cell>w/</cell><cell>71.050</cell><cell>2.010</cell></row><row><cell>CelebA-</cell><cell>w/o</cell><cell>93.778</cell><cell>2.057</cell></row><row><cell>HQ</cell><cell>w/</cell><cell>84.472</cell><cell>2.060</cell></row><row><cell cols="2">sis results on CelebA-</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>The FID (lower is better) and IS (higher is better) scores for the pix2pix image-to-image translation trained with/without the focal frequency loss (FFL).</figDesc><table><row><cell>Dataset</cell><cell>FFL</cell><cell>FID?</cell><cell>IS?</cell></row><row><cell>CMP Facades</cell><cell>w/o</cell><cell>128.492</cell><cell>1.571</cell></row><row><cell></cell><cell>w/</cell><cell>123.773</cell><cell>1.738</cell></row><row><cell>edges ? shoes</cell><cell>w/o</cell><cell>80.279</cell><cell>2.674</cell></row><row><cell></cell><cell>w/</cell><cell>74.359</cell><cell>2.804</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 .</head><label>6</label><figDesc></figDesc><table><row><cell>Dataset</cell><cell>FFL</cell><cell>FID?</cell><cell>IS?</cell></row><row><cell>CelebA-HQ</cell><cell>w/o</cell><cell>5.696</cell><cell>3.383</cell></row><row><cell>(256 ? 256)</cell><cell>w/</cell><cell>4.972</cell><cell>3.432</cell></row></table><note>The FID (lower is better) and IS (higher is better) scores for the StyleGAN2 unconditional image synthesis trained with/without the focal frequency loss (FFL).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 .</head><label>7</label><figDesc>Comparison of our focal frequency loss (FFL) with relevant losses, i.e., perceptual loss (PL), spectral regularization (SpReg), and another transformation form for FFL, i.e., discrete cosine transform (DCT), in different image reconstruction and synthesis tasks.</figDesc><table><row><cell></cell><cell></cell><cell cols="3">VAE reconstruction (CelebA)</cell><cell></cell><cell cols="2">VAE synthesis (CelebA)</cell><cell cols="2">pix2pix I2I (edges ? shoes)</cell></row><row><cell>Method</cell><cell>PSNR?</cell><cell>SSIM?</cell><cell>LPIPS?</cell><cell>FID?</cell><cell>LFD?</cell><cell>FID?</cell><cell>IS?</cell><cell>FID?</cell><cell>IS?</cell></row><row><cell>baseline</cell><cell>19.961</cell><cell>0.606</cell><cell>0.217</cell><cell>69.900</cell><cell>14.804</cell><cell>80.116</cell><cell>1.873</cell><cell>80.279</cell><cell>2.674</cell></row><row><cell>+ PL [30]</cell><cell>20.964</cell><cell>0.658</cell><cell>0.143</cell><cell>62.795</cell><cell>14.573</cell><cell>78.825</cell><cell>1.788</cell><cell>78.916</cell><cell>2.722</cell></row><row><cell>+ SpReg [11]</cell><cell>19.974</cell><cell>0.607</cell><cell>0.218</cell><cell>69.118</cell><cell>14.796</cell><cell>78.079</cell><cell>1.898</cell><cell>79.300</cell><cell>2.700</cell></row><row><cell>+ FFL (DCT)</cell><cell>22.677</cell><cell>0.711</cell><cell>0.150</cell><cell>51.536</cell><cell>14.179</cell><cell>71.827</cell><cell>1.932</cell><cell>79.045</cell><cell>2.754</cell></row><row><cell>+ FFL (Ours)</cell><cell>22.954</cell><cell>0.723</cell><cell>0.143</cell><cell>49.689</cell><cell>14.115</cell><cell>71.050</cell><cell>2.010</cell><cell>74.359</cell><cell>2.804</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 8 .</head><label>8</label><figDesc></figDesc><table><row><cell></cell><cell>PSNR?</cell><cell>SSIM?</cell><cell>LPIPS?</cell><cell>FID?</cell><cell>LFD?</cell></row><row><cell>baseline</cell><cell>20.044</cell><cell>0.568</cell><cell>0.237</cell><cell>97.035</cell><cell>14.785</cell></row><row><cell>full FFL</cell><cell>21.703</cell><cell>0.642</cell><cell>0.199</cell><cell>83.801</cell><cell>14.403</cell></row><row><cell>w/o freq</cell><cell>18.200</cell><cell>0.470</cell><cell>0.265</cell><cell>123.833</cell><cell>15.210</cell></row><row><cell>w/o phase</cell><cell>13.273</cell><cell>0.380</cell><cell>0.407</cell><cell>233.170</cell><cell>16.344</cell></row><row><cell>w/o ampli</cell><cell>15.640</cell><cell>0.359</cell><cell>0.539</cell><cell>323.528</cell><cell>15.799</cell></row><row><cell>w/o focal</cell><cell>20.163</cell><cell>0.574</cell><cell>0.234</cell><cell>94.497</cell><cell>14.758</cell></row></table><note>The PSNR (higher is better), SSIM (higher is better), LPIPS (lower is better), FID (lower is better) and LFD (lower is better) scores for the ablation studies of each key component for the focal frequency loss (FFL).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 10 .</head><label>10</label><figDesc>The PSNR (higher is better), SSIM (higher is better), LPIPS (lower is better), FID (lower is better) and LFD (lower is better) scores for the variant studies on patch-based focal frequency loss. Patch factor p is the number of patches on each edge.</figDesc><table><row><cell></cell><cell></cell><cell>SSIM?</cell><cell>LPIPS?</cell><cell>FID?</cell><cell>LFD?</cell></row><row><cell>baseline</cell><cell>20.044</cell><cell>0.568</cell><cell>0.237</cell><cell>97.035</cell><cell>14.785</cell></row><row><cell>? = 1 (main)</cell><cell>21.703</cell><cell>0.642</cell><cell>0.199</cell><cell>83.801</cell><cell>14.403</cell></row><row><cell>? = 2</cell><cell>21.376</cell><cell>0.621</cell><cell>0.203</cell><cell cols="2">102.329 14.478</cell></row><row><cell>? = 0.5</cell><cell>21.521</cell><cell>0.635</cell><cell>0.197</cell><cell>82.561</cell><cell>14.445</cell></row><row><cell>? = 0.1</cell><cell>20.497</cell><cell>0.591</cell><cell>0.225</cell><cell>89.792</cell><cell>14.681</cell></row><row><cell></cell><cell cols="2">PSNR? SSIM?</cell><cell>LPIPS?</cell><cell>FID?</cell><cell>LFD?</cell></row><row><cell>baseline</cell><cell>20.044</cell><cell>0.568</cell><cell>0.237</cell><cell>97.035</cell><cell>14.785</cell></row><row><cell>p = 1 (main)</cell><cell>21.703</cell><cell>0.642</cell><cell>0.199</cell><cell>83.801</cell><cell>14.403</cell></row><row><cell>p = 2</cell><cell>21.836</cell><cell>0.648</cell><cell>0.185</cell><cell>88.475</cell><cell>14.372</cell></row><row><cell>p = 4</cell><cell>21.752</cell><cell>0.643</cell><cell>0.170</cell><cell>90.612</cell><cell>14.392</cell></row><row><cell>p = 8</cell><cell>21.414</cell><cell>0.627</cell><cell>0.176</cell><cell cols="2">102.334 14.470</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">For simplicity, the formulas in this section are applied to gray-scale images, while the extension to color images is straightforward by processing each channel separately in the same way.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments. This study is supported under the RIE2020 Industry Alignment Fund -Industry Collaboration Projects (IAF-ICP) Funding Initiative, as well as cash and in-kind contribution from the industry partner(s).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Deepfacelab</surname></persName>
		</author>
		<ptr target="https://github.com/iperov/DeepFaceLab/.Accessed" />
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Deepfakes</surname></persName>
		</author>
		<ptr target="https://github.com/deepfakes/faceswap/" />
		<imprint>
			<biblScope unit="page" from="2019" to="2027" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Danbooru2019 Portraits: A large-scale anime head illustration dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gwern</forename><surname>Branwen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anonymous</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danbooru</forename><surname>Community</surname></persName>
		</author>
		<idno>danbooru2019-portraits. Accessed: 2021-04-10. 16</idno>
		<ptr target="https://www.gwern.net/Crops#" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Large scale GAN training for high fidelity natural image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Frequency domain image translation: More photo-realistic, better identity-preserving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mu</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huijuan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qichuan</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.13611</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Photographic image synthesis with cascaded refinement networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Compressing convolutional neural networks in the frequency domain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenlin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Tyree</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Kilian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixin</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">KDD</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">StarGAN: Unified generative adversarial networks for multi-domain image-to-image translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunjey</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minje</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Munyoung</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jung-Woo</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sunghun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaegul</forename><surname>Choo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Describing textures in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cimpoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The Cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uwe</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Watch your up-convolution: CNN based generative deep neural networks are failing to reproduce spectral distributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ricard</forename><surname>Durall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margret</forename><surname>Keuper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janis</forename><surname>Keuper</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Object detection with discriminatively trained part-based models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pedro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Leveraging frequency analysis for deep fake image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thorsten</forename><surname>Eisenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lea</forename><surname>Sch?nherr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asja</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dorothea</forename><surname>Kolossa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thorsten</forename><surname>Holz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Frequency separation for real-world super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manuel</forename><surname>Fritsche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuhang</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCVW</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rinon</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dana</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amit</forename><surname>Bermano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cohen-Or</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.06108</idno>
		<title level="m">SWAGAN: A style-based wavelet-driven generative model</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Faster neural networks straight from JPEG</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lionel</forename><surname>Gueguen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Sergeev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Kadlec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rosanne</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Yosinski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungwook</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akash</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cole</forename><surname>Hurwitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prasanna</forename><surname>Sattigeri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David D</forename><surname>Cox</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.04433</idno>
		<title level="m">not-so-biggan: Generating highfidelity images on a small compute budget</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">GANs trained by a two time-scale update rule converge to a local nash equilibrium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hubert</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Nessler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Reducing the dimensionality of data with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Geoffrey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan R</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">313</biblScope>
			<biblScope unit="page">13</biblScope>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">RDA: Robust domain adaptation via fourier adversarial attacking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dayan</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aoran</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shijian</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV, 2021</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Multimodal unsupervised image-to-image translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">FakeRetouch: Evading deepfakes detection via the guidance of deliberate noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Juefei-Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofei</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weikai</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geguang</forename><surname>Pu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.09213</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Image-to-image translation with conditional adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinghui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">DeeperForensics Challenge 2020 on real-world face forgery detection: Methods and results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liming</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengkui</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wayne</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoyang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.09471</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">DeeperForensics-1.0: A large-scale dataset for real-world face forgery detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liming</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ren</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wayne</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen Change</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">TSIT: A simple and versatile framework for image-to-image translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liming</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changxu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingyang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen Change</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV, 2020</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Perceptual losses for real-time style transfer and super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Spectral distribution aware image generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steffen</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margret</forename><surname>Keuper</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.03110</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Progressive growing of GANs for improved quality, stability, and variation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10196</idno>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A style-based generator architecture for generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Analyzing and improving the image quality of StyleGAN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miika</forename><surname>Aittala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janne</forename><surname>Hellsten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Glow: Generative flow with invertible 1x1 convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Durk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dhariwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Semi-supervised learning with deep generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shakir</forename><surname>Durk P Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Jimenez Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Auto-encoding variational bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6114</idno>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Convolutional neural network feature reduction using wavelet transform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Levinskis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Elektronika ir Elektrotechnika</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="61" to="64" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jae</forename><forename type="middle">Hyun</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jong</forename><surname>Chul Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Geometric</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.02894</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr Doll?r. Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Unsupervised image-to-image translation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Breuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Learning to predict layout-to-image conditional convolutions for semantic image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xihui</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guojun</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Deep learning face attributes in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Which training methods for gans do actually converge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><surname>Mescheder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Nowozin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">NeRF: Representing scenes as neural radiance fields for view synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Mildenhall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pratul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">T</forename><surname>Tancik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ravi</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ren</forename><surname>Ramamoorthi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV, 2020. 1</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Osindero</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.1784</idno>
		<title level="m">Conditional generative adversarial nets</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeru</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toshiki</forename><surname>Kataoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masanori</forename><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuichi</forename><surname>Yoshida</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.05957</idno>
		<title level="m">Spectral normalization for generative adversarial networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Deconvolution and checkerboard artifacts. Distill, 1:e3</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Augustus</forename><surname>Odena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Olah</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Semantic image synthesis with spatially-adaptive normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting-Chun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Adversarial latent autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanislav</forename><surname>Pidhorskyi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Donald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gianfranco</forename><surname>Adjeroh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Doretto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Semi-parametric image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Unsupervised representation learning with deep convolutional generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06434</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Spatial pattern templates for recognition of objects with regular structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tyle?ek</forename><surname>Radim??ra Radim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">GCPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">On the spectral bias of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aristide</forename><surname>Nasim Rahaman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devansh</forename><surname>Baratin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Arpit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Draxler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fred</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Hamprecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Random features for largescale kernel machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Rahimi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Recht</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">U-Net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Improved techniques for training GANs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vicki</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Training region-based object detectors with online hard example mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Joint frequency-and image-space learning for fourier imaging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nalini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">Eugenio</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elfar</forename><surname>Iglesias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><forename type="middle">V</forename><surname>Adalsteinsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Polina</forename><surname>Dalca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Golland</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.01441</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Fourier features let networks learn high frequency functions in low dimensional domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Tancik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pratul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sara</forename><surname>Mildenhall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nithin</forename><surname>Fridovich-Keil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Utkarsh</forename><surname>Raghavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ravi</forename><surname>Singhal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">T</forename><surname>Ramamoorthi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ren</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Instance normalization: The missing ingredient for fast stylization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Ulyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.08022</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">NVAE: A deep hierarchical variational autoencoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arash</forename><surname>Vahdat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Conditional image generation with PixelCNN decoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lasse</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">High-frequency component helps explain the generalization of convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haohan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xindi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeyi</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">CNN-generated images are surprisingly easy to spot...for now</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng-Yu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Owens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">High-resolution image synthesis and semantic manipulation with conditional GANs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting-Chun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title level="m" type="main">CNNpack: Packing convolutional neural networks in the frequency domain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shan</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Xu</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Image quality assessment: from error visibility to structural similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hamid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eero P</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<title level="m" type="main">Unsupervised real-world image super resolution via domaindistance aware training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunxuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuhang</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yawei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longcun</forename><surname>Jin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.01178</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingqing</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuxin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guolin</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhouchen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
		<title level="m">Invertible image rescaling</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
	<note>ECCV</note>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Unified perceptual parsing for scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tete</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingcheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuning</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Holistically-nested edge detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Learning in the frequency domain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghai</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Kuang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fengbo</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<monogr>
		<title level="m" type="main">Frequency principle: Fourier analysis sheds light on deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi-Qin John</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaoyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanyang</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Ma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.06523</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Photorealistic style transfer via wavelet transforms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaejun</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjung</forename><surname>Uh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyuk</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byeongkyu</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jung-Woo</forename><surname>Ha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Fine-grained visual comparisons with local learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aron</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristen</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Dilated residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Funkhouser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.08318</idno>
		<title level="m">Dimitris Metaxas, and Augustus Odena. Self-attention generative adversarial networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">The unreasonable effectiveness of deep features as a perceptual metric</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Detecting and simulating artifacts in gan fake images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svebor</forename><surname>Karaman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Fu</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WIFS</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Scene parsing through ADE20K dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Puig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adela</forename><surname>Barriuso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Unpaired image-to-image translation using cycleconsistent adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
