<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Contrastive Boundary Learning for Point Cloud Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liyao</forename><surname>Tang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The University of Sydney</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yibing</forename><surname>Zhan</surname></persName>
							<email>zhanyibing@jd.com</email>
							<affiliation key="aff1">
								<orgName type="institution">JD Explore Academy</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The University of Sydney</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baosheng</forename><surname>Yu</surname></persName>
							<email>baosheng.yu@sydney.edu.audacheng.tao@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution">The University of Sydney</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The University of Sydney</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">JD Explore Academy</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Contrastive Boundary Learning for Point Cloud Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T05:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Point cloud segmentation is fundamental in understanding 3D environments. However, current 3D point cloud segmentation methods usually perform poorly on scene boundaries, which degenerates the overall segmentation performance. In this paper, we focus on the segmentation of scene boundaries. Accordingly, we first explore metrics to evaluate the segmentation performance on scene boundaries. To address the unsatisfactory performance on boundaries, we then propose a novel contrastive boundary learning (CBL) framework for point cloud segmentation. Specifically, the proposed CBL enhances feature discrimination between points across boundaries by contrasting their representations with the assistance of scene contexts at multiple scales. By applying CBL on three different baseline methods, we experimentally show that CBL consistently improves different baselines and assists them to achieve compelling performance on boundaries, as well as the overall performance, e.g. in mIoU. The experimental results demonstrate the effectiveness of our method and the importance of boundaries for 3D point cloud segmentation. Code and model will be made publicly available at</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>3D point cloud semantic segmentation aims to assign semantic categories to each 3D data point, while robust 3D segmentation is very important for various applications <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b66">64]</ref>, including autonomous driving, unmanned aerial vehicles, and augmented reality.</p><p>However, despite that various point cloud segmentation methods have been developed, little attention has been put on boundaries in 3D point clouds. Accurate segmentation on scene boundaries can be of great importance. Firstly, a clean boundary estimation can be beneficial for overall segmentation performance. For example, in 2D image segmentation, accurate segmentation on boundary is the key to generate high-fidelity masks <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b38">36,</ref><ref type="bibr" target="#b73">69]</ref>. Secondly, compared  <ref type="figure">Figure 1</ref>. Contrastive Boundary Learning (top) discovers boundary from ground truth in each sub-sampled point cloud, i.e., subscene, through the sub-sampling procedure. By imposing contrastive optimization on boundary areas at multiple scales, CBL enhances the feature discrimination across boundaries (middle). Without an explicit boundary prediction, CBL improves boundary segmentation and achieves better scene segmentation results (bottom). The visualization is conducted on S3DIS testset <ref type="bibr">Area 5.</ref> to object categories that usually have a large portion of 3D points, such as buildings and trees, erroneous boundary segmentation could affect the recognition of object categories with much fewer points (e.g., pedestrians and pillars) to a greater extent. This can be particularly hazardous for applications like autonomous driving, e.g., crashing into curbs if boundaries are recognized inaccurately by a self-driving car.</p><p>Unfortunately, most previous 3D segmentation methods generally overlook the segmentation on scene boundaries. Though a few methods have considered boundaries, they still lack an explicit and comprehensive investigation to analyze the segmentation performance on boundary areas. They also perform unsatisfactorily on the overall segmentation performance.</p><p>Therefore, to deliver a more thorough study of the segmentation on boundaries, we first explore metrics to quantify the segmentation performance on scene boundaries. After revealing the unsatisfactory performance, we propose a novel Contrastive Boundary Learning (CBL) framework to help optimize the segmentation performance on boundaries particularly, which also consistently improves the overall performance for different baseline methods.</p><p>In particular, current popular segmentation metrics lack specific measurements on boundaries, making it difficult to reveal the boundary segmentation quality in existing methods. To make a clearer view on the performance on boundaries, we calculate the popular mean intersection-overunion (mIoU) for boundary areas and inner (non-boundary) areas separately. By comparing the performance on types of areas as well as the overall performance, the unsatisfactory performance on boundary areas can be directly revealed. Moreover, to describe the performance on boundaries more comprehensively, we consider the alignment between the boundary in the ground truth and the boundary in model segmentation results. Therefore, we introduce the popular boundary IoU <ref type="bibr" target="#b7">[8]</ref> score (B-IoU) used in 2D instance segmentation for evaluation, which also gives a much lower score compared with the overall performance in mIoU.</p><p>After identifying the boundary segmentation difficulties, we further propose a novel contrastive boundary learning (CBL) framework to better align the boundaries of model predictions with ground-truth data's boundaries. As shown in <ref type="figure">Fig. 1</ref>, CBL optimizes a model on the feature representation of points in boundary areas, enhancing the feature discrimination across the scene boundaries. Furthermore, to make model better aware of the boundary areas at multiple semantic scales, we also develop a sub-scene boundary mining strategy, which leverages the sub-sampling procedure to discover boundary points in each sub-sampled point cloud, i.e., sub-scene. Specifically, CBL operates across different sub-sampling stages and facilitates 3D segmentation methods to learn better feature representation around boundary areas.</p><p>Empirically, we experiment with three baselines across four datasets. We first present the unsatisfactory performance on boundary areas when using current point cloud segmentation methods and then show that CBL can assist baseline in achieving promising boundary and overall performance. For example, the proposed CBL helps RandLA-Net surpass current state-of-the-art methods on the Seman-tic3D dataset and enables a basic ConvNet to achieve leading performance on the S3DIS dataset.</p><p>Our contributions are as follows:</p><p>? We explore the boundary problem in current 3D point cloud segmentation and quantify it with metrics that consider boundary area, e.g., boundary IoU. The results reveal that current methods deliver much worse accuracy in boundary areas than their overall performance. ? We propose a novel Contrastive Boundary Learning (CBL) framework, which improves the feature representation by contrasting the point features across the scene boundaries. It thus improves the segmentation performance around boundary areas and subsequently the overall performance. ? We conduct extensive experiments and show that CBL can bring significant and consistent improvements on boundary area as well as overall performance across all baselines. These empirical results further demonstrate that CBL is effective for improving boundary segmentation performance, and accurate boundary segmentation is important for robust 3D segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>Point cloud segmentation. Point cloud semantic segmentation aims to assign semantic labels to each 3D point. Recent deep learning methods have taken over traditional methods <ref type="bibr" target="#b34">[33,</ref><ref type="bibr" target="#b52">50]</ref> that use hand-crafted features, which can be roughly divided into projection-based and point-based methods.</p><p>Projection-based methods project 3D points to gridlike structure, either 2D image <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b33">32,</ref><ref type="bibr" target="#b45">43,</ref><ref type="bibr" target="#b67">65]</ref> or 3D voxels <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b54">52,</ref><ref type="bibr" target="#b59">57]</ref>. For the 2D image plane, we can make use of existing studies for 2D image processing. However, a complete 3D segmentation generally requires taking multiple viewpoints and re-projection <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b37">35]</ref>, which may result in surface occlusions. For 3D voxels, sparse convolutions <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b63">61]</ref> are proposed to alleviate the resource consumption in voxel construction, considering the large emptiness in 3D space. In general, the voxel resolution incurs the trade-off between losing detail and being resource-demanding <ref type="bibr" target="#b48">[46]</ref>. Point-based network directly operates on 3D points, while a pioneering work in this direction is PointNet <ref type="bibr" target="#b47">[45]</ref>, which uses point-wise MLPs to process per-point feature. Following this success, recent works adopt an encoder-decoder paradigm <ref type="bibr" target="#b49">[47]</ref>. Various local aggregation modules are proposed to examine the local context in point clouds, including 3D convolution <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b39">37,</ref><ref type="bibr" target="#b55">53,</ref><ref type="bibr" target="#b64">62]</ref>, attentional operations <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b26">26,</ref><ref type="bibr" target="#b78">73]</ref>, and graph-based operation <ref type="bibr" target="#b35">[34,</ref><ref type="bibr" target="#b62">60]</ref>. To better process unstructured point cloud, sub-sampling <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b69">67,</ref><ref type="bibr" target="#b71">68]</ref>, up-sampling <ref type="bibr" target="#b50">[48,</ref><ref type="bibr" target="#b58">56]</ref>, and postprocessing modules <ref type="bibr" target="#b28">[28,</ref><ref type="bibr" target="#b43">41]</ref> are also considered to enhance </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sub-scene Boundary Mining</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input Prediction</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Skip connection</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Shared sub-sampling</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sub-sampled points</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Boundary determination</head><p>Features to be optimized</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Push apart (across boundary)</head><p>Pull together</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Contrastive Boundary Learning</head><p>Sub-scene annotation 0 1 1 point cloud representation. Despite these developments in different modules, the boundary in point cloud segmentation has rarely been explored. Boundary in segmentation. Boundary problem has a long history in 2D image processing <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b38">36,</ref><ref type="bibr" target="#b44">42,</ref><ref type="bibr" target="#b73">69]</ref>, whereas only few works <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b28">28]</ref> realize the significance of boundary in 3D point cloud segmentation. However, both works involve complex modules for explicit boundary prediction <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b28">28]</ref> or local aggregation <ref type="bibr" target="#b28">[28]</ref>. These operations largely increase the model complexity, yet yield limited performance gain for overall metric. Regarding segmentation performance on boundaries, they also only give qualitative results. In comparison, we present a contrastive learning framework that brings little overhead to the model and can improve upon various baselines with simple adaption. Additionally, we would like to note that, we for the first time, quantify the boundary quality with numeric metric, and demonstrate that boundary problem is indeed widely existing across current methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ground truth label</head><p>Contrastive learning. Contrastive learning <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b23">23,</ref><ref type="bibr">31,</ref><ref type="bibr" target="#b57">55,</ref><ref type="bibr" target="#b68">66]</ref> has shown promising performance in representation learning for computer vision tasks, ranging from unsupervised settings to supervised settings. In recent works, contrastive learning has also been introduced into 2D segmentation <ref type="bibr" target="#b60">[58,</ref><ref type="bibr" target="#b61">59]</ref> as well as unsupervised representation learning in point cloud processing <ref type="bibr" target="#b25">[25,</ref><ref type="bibr" target="#b41">39,</ref><ref type="bibr" target="#b65">63]</ref>. Especially, PointContrast <ref type="bibr" target="#b65">[63]</ref> conducts point-wise contrastive learning to overcome geometric transformation, such as rigid transformation. P4Contrast <ref type="bibr" target="#b41">[39]</ref> suggests a more flexible contrasting strategy to promote multi-modal fusion between geometric and RGB information. In contrast, in our work, we take a supervised setting and demonstrate with CBL that contrastive learning is well-suited for improving segmentation quality on boundary areas. Additionally, unlike the above works that only use points at input point cloud, we utilize the sub-sampled point cloud to examine scene context at multiple scales.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Segmentation on Boundaries</head><p>Since most of the current works focus on the improvement of general metrics, such as mean intersection over union (mIoU), overall accuracy (OA), and mean average precision (mAP), the boundary quality in point cloud segmentation is usually overlooked. Unlike recent boundaryrelated works <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b28">28]</ref> that give only qualitative results on boundaries, we are the first to quantify the quality of segmentation on boundaries. Particularly, we introduce a series of metrics for presentation, including mIoU@boundary, mIoU@inner and the boundary IoU (B-IoU) score from 2D instance segmentation tasks <ref type="bibr" target="#b7">[8]</ref>.</p><p>Based on ground-truths data, we consider a point as a boundary point if there exist points that have a different annotated label in its neighborhood. Similarly, for model predictions, we consider a point as a boundary point if there exist nearby points with a different predicted label. More formally, we note the point cloud as X and the i-th point as x i , whose local neighborhood is N i = N (x i ), corresponding ground truth label is l i , and the model predicted label is p i . We further note the set of boundary points in groundtruth as B l and those in predicted segmentation as B p , thus we have:</p><formula xml:id="formula_0">B l = {x i ? X | ? x j ? N i , l j = l i }, B p = {x i ? X | ? x j ? N i , p j = p i },<label>(1)</label></formula><p>where we set N i to be the radius neighborhood with a radius of 0.1 following the common practice <ref type="bibr" target="#b42">[40,</ref><ref type="bibr" target="#b55">53]</ref>.</p><p>To examine the boundary segmentation results, an intuitive way is to calculate the mIoU within the boundary area, i.e., mIoU@boundary. To further compare the model performance in boundary and non-boundary (inner) area, we further calculate the mIoU in the inner area, i.e. mIoU@inner. Given that mIoU is calculated on the whole point cloud X as:</p><formula xml:id="formula_1">mIoU(X ) = 1 K K k=1 xi?X 1[p i = k ? l i = k] xj ?X 1[p j = k ? l j = k] ,<label>(2)</label></formula><p>where K is the total number of classes and 1[?] represents a boolean function that outputs 1 if the condition within [?] is true and 0 otherwise. We have the mIoU@boundary and mIoU@inner defined as:</p><formula xml:id="formula_2">mIoU@boundary = mIoU(B l ), mIoU@inner = mIoU(X ? B l ),<label>(3)</label></formula><p>where X ? B l is the set of points in inner area. However, the mIoU@boundary and mIoU@inner do not consider the false boundary in model predicted segmentation. Inspired by boundary IoU <ref type="bibr" target="#b7">[8]</ref> for 2D instance segmentation, for better evaluation, we consider the alignment between boundary in segmentation predictions and boundary in ground truth data. It thus leads to the following B-IoU for evaluation:</p><formula xml:id="formula_3">B-IoU = |B l ? B p | |B l ? B p | .<label>(4)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Method</head><p>In this section, we present our contrastive boundary learning (CBL) framework, shown in <ref type="figure" target="#fig_2">Fig. 2</ref>. It imposes contrastive learning to enhance the feature discrimination across boundaries. Then, to deeply augment the model performance on boundaries, we enable the CBL in subsampled point clouds, i.e., sub-scene, through the sub-scene boundary mining. Contrastive Boundary Learning. We follow the widely used InfoNCE loss <ref type="bibr" target="#b57">[55]</ref> and its generalization <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b19">20]</ref> to define the contrastive optimization goal on boundary points. In particular, for a boundary point x i ? B l , we encourage learned representations more similar to its neighbor points from the same category and more distinguished from other neighbor points from different categories, i.e.,</p><formula xml:id="formula_4">L CBL = ?1 |B l | xi?B l log xj ?Ni?lj =li exp(?d(f i , f j )/? ) x k ?Ni exp(?d(f i , f k )/? ) , (5) where f i is the feature of x i , d(?, ?)</formula><p>is a distance measurement and ? is the temperature in contrastive learning. The contrastive learning described by Eq. (5) focuses on boundary points only (the dashed circles in red in <ref type="figure" target="#fig_2">Fig. 2</ref>). First, we consider all the boundary points B l from groundtruth data as defined in Eq. (1). Then, for each point x i ? B l , we restrict the sampling of its positive and negative points to be within its local neighborhood N i . With such strong spatial restriction, we obtain positive pairs for x i as {x j ? N i ? l j = l i }, and other neighboring points, i.e. {x j ? N i ? l j = l i }, are negative pairs. Therefore, the contrastive learning enhances the feature discrimination across scene boundaries, which is important for improving segmentation on boundary areas. Sub-scene Boundary Mining. To better explore scene boundaries, we examine the boundaries in sub-sampled point clouds at multiple scales, which enables the contrastive boundary learning on different sub-sampling stages of a backbone model. Collecting boundary points from the input point cloud is straightforward with the ground truth label. However, after sub-sampling, it is difficult to obtain a proper definition of boundary point set following Eq. (1), due to the undefined label for sub-sampled points <ref type="bibr" target="#b13">[14]</ref>. Therefore, to enable CBL in sub-sampled point cloud, we propose the sub-scene boundary mining that determines the set of ground-truth boundary points in each sub-sampling stage. Specifically, we use superscripts to denote stage. At the sub-sampling stage n, we represent its sub-sampled point cloud as X n . For input point cloud, we have X 0 = X . When collecting a set of boundary points B n l ? X n in stage n, it is required to determine the label l n i of a sub-sampled point x n i ? X n , i.e., the sub-scene annotation. As each sub-sampled point x n i ? X n is aggregated from a group of points in its previous point cloud X n?1 ; we thus utilize the sub-sampling procedure to determine the label iteratively. We take l 0 i to be the one-hot label of ground truth label l i for point x 0 i = x i , and have the following:</p><formula xml:id="formula_5">l n i = AVG({l n?1 j |x n?1 j ? N n?1 (x n i )}),<label>(6)</label></formula><p>where N n?1 (x n i ) denotes the local neighbors of x n i in previous stage (the dashed circles in grey in <ref type="figure" target="#fig_2">Fig. 2</ref>), i.e., the group of points aggregated from X n?1 to be represented by the single point x n i ? X n after sub-sampling procedure, and AVG is the average-pooling.</p><p>With Eq. (6) and ground-truth labels, we can iteratively obtain the sub-scene annotation l n i as a distribution, whose  <ref type="figure">Figure 3</ref>. The architecture of the 3D ConvNet model, which follows the widely adopted encoder-decoder paradigm, with an optional multi-scale prediction head. More details are provided in the appendix.</p><p>k-th location describes the proportion of k-th class in its corresponding group of points in the input point cloud. To determine the set of boundary points in sub-sampled point cloud X n , we simply take arg max l n i to allow the evaluation of boundary point in Eq. (1) 1 , and use the feature of sub-sampled point for the contrastive boundary optimization in Eq. <ref type="bibr" target="#b4">(5)</ref>. Finally, with sub-scene boundary mining, we have CBL applied at all stages and the final loss is</p><formula xml:id="formula_6">L = L cross entropy + ? n L n CBL ,<label>(7)</label></formula><p>where L n CBL is the CBL loss at stage n and ? is the loss weight.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Implementation Details and Baselines</head><p>As 3D ConvNet has been a popular backbone model for point cloud processing, to present a generalized implementation, we illustrate with a ConvNet baseline ( <ref type="figure">Fig. 3)</ref> as a case study for applying CBL in point cloud processing. Following <ref type="bibr" target="#b1">[2,</ref><ref type="bibr">24]</ref>, we build the ConvNet with convolution in 3D continuous space:</p><formula xml:id="formula_7">f i = (h ? g)(x i ) = xj ?Ni g(x i ? x j )h(x j ),<label>(8)</label></formula><p>where ? denotes convolution operator and the continuous kernel g(?) is approximated by one-layer MLP and set h(x j ) = f j to simply use the feature of point x j . We note that the 3D convolution in Eq. <ref type="formula" target="#formula_7">(8)</ref> is purely based on spatial location between the center point and its neighbors, compared to other advanced local aggregation modules that utilize the local context <ref type="bibr" target="#b26">[26,</ref><ref type="bibr" target="#b78">73]</ref>.</p><p>To better utilize the boundary features optimized by CBL at multiple scales, we use a multi-scale head for prediction, which simply concatenates the point feature from each mIoU methods overall @boundary @inner B-IoU sub-sampled point cloud into the last output layer. As we would show in the ablation study (Sec. 6.3), such concatenation across multiple scales fails without the CBL. Note that CBL can be married to any other multi-stage backbone. Specifically, we also apply the CBL to two other popular baselines: the RandLA-Net <ref type="bibr" target="#b26">[26]</ref> and CloserLook3D <ref type="bibr" target="#b42">[40]</ref>, to demonstrate the generalizability. RandLA-Net leverages random sampling and attentive local aggregation to handle the large-scale scene with fast processing; CloserLook3D proposes a parameter-free PosPool module that largely reduces model parameters and resources consumption, while achieving comparable performance against other methods with parametric aggregation module, such as KPConv <ref type="bibr" target="#b55">[53]</ref>. Together with the ConvNet baseline, our experiments cover the backbone with most of the typical local aggregation methods for point cloud, ranging from convolution, attentional operation, to parameter-free operation. For training, we follow the setup of baseline and set the loss weight ? = 0.1. More details will be provided in the appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Experiments</head><p>We first present the boundary problem with experiments. We then evaluate the benefits of the proposed CBL on multiple large-scale point cloud segmentation datasets, including in-door scenes (S3DIS <ref type="bibr" target="#b0">[1]</ref>, ScanNet <ref type="bibr" target="#b9">[10]</ref>) and out-door scenes (Semantic3D <ref type="bibr" target="#b20">[21]</ref>, NPM3D <ref type="bibr" target="#b51">[49]</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">The Boundary Problem in Experiment</head><p>We experimentally compare the score given by mIoU, mIoU@boundary, mIoU@inner as well as the B-IoU. As shown in Tab. 1, for recent 3D point cloud segmentation methods, the mIoU@boundary is much lower than the mIoU@inner. With the overall performance sitting between these two scores, it suggests that it is the boundary area that degenerates the overall segmentation performance.  <ref type="table">Table 2</ref>. Quantitative results on S3DIS Area 5 dataset <ref type="bibr" target="#b0">[1]</ref>, showing the mean IoU (mIoU) overall accuracy (OA) and the mean accuracy (mACC). The red denotes improvement over baseline and the bold or bold denotes the best performance. Method with * also consider boundaries in their design.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ground Truth</head><p>Baseline CBL Improvement Input <ref type="figure">Figure 4</ref>. We compare the results of ConvNet baseline with CBL on several different scenes and show that the improvements are from boundaries. In offices (top 2), CBL can effectively improve the results on boundary areas, especially in a cluttered one (2nd row). In the last two rows (hallway and others), CBL avoids unnecessary boundaries, and repairs the missing boundary between walls and doors/objects at the right place. The visualization is done on S3DIS testset Area 5.</p><p>Similarly, B-IoU also agrees with the mIoU@boundary by giving a score that is far lagged behind the general performance of mIoU score. Hence, such observation indicates the unsatisfied segmentation quality on boundary areas. While with the proposed CBL, the improvement on both mIoU@boundary and B-IoU is larger than the improvement on overall mIoU as well as the mIoU@inner, across all three baselines. Due to the limited space, we provide more thorough studies in presenting the boundary problem in the appendix.   <ref type="table">Table 4</ref>. Quantitative results on Semantic3D reduced-8 benchmark <ref type="bibr" target="#b20">[21]</ref>. The metrics shown the mean IoU (mIoU) and overall accuracy (OA) obtained from benchmark site with only the recent published works included. The red denotes improvement over baseline and the bold or bold denotes the best performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Performance Comparison</head><p>egories, e.g., ceiling, floor, clutter. As shown in Tab. 2, our methods consistently improve across all three baselines, showing to be effective with different local aggregation modules. Notably, the improvements are much more significant in classes, such as column (+13 compared to ConvNet baseline), than in other classes with large areas, such as wall and ceiling. Such observation shows our effectiveness on boundary areas; and with the consistent improvement across different classes, it also suggests that the CBL is NOT trading off between scenes of major and minor classes, but is indeed separating them more clearly. With the benefit of a cleaner boundary, the ConvNet finally achieves a leading performance of 69.4 in mIoU.</p><p>We further demonstrate qualitatively in <ref type="figure">Fig. 4</ref> that, the CBL effectively improves the overall performance by improving segmentation on boundary areas. Compared with JSENet <ref type="bibr" target="#b28">[28]</ref> that also considers boundaries, we demonstrate our superiority by obtaining a much larger relative improvement to our baselines than that made by JSENet on its baseline, i.e., KPConv <ref type="bibr" target="#b55">[53]</ref>, especially in classes that boundaries are important, e.g., column, window, sofa, bookcase and clutter, as well as the overall performance. To avoid overfitting on S3DIS Area 5, we further conduct the 6-fold cross-validation, with the result reported in Tab. 3. A large improvement is also shown in column (+9.5), and consistent improvement is made across all classes except one (-0.2). Therefore, the proposed CBL can be indeed regarded as a general and effective method, achieving 73.1 in mIoU with a common ConvNet baseline. Semantic3D Outdoor Scene Segmentation. In addition to improvement on S3DIS <ref type="bibr" target="#b0">[1]</ref>, we demonstrate the generalizability across different types of scenes by evaluating CBL on point cloud collected at the outdoor environment, the Se-mantic3D <ref type="bibr" target="#b20">[21]</ref> dataset. It is a large-scale dataset comprising over 4 billion points and provides 15 large point clouds for training, with each point annotated to one of the 8 classes, e.g., cars, buildings. We use the reduced-8 benchmark and present the quantitative results in Tab. 4. We evaluate with both ConvNet and RandLA-Net <ref type="bibr" target="#b26">[26]</ref> as baselines and observe consistent improvements. Especially, RandLA-Net has achieved state-of-the-art performance on multiple outdoor datasets and the improvement made on it can better demonstrate the effectiveness of our CBL. Notably, significant improvement is made in the high vegetation and low vegetation class, which are two classes that confuse most of the other methods. It is because the high/low vegetation usually co-exists at a near spatial distance and has a similar appearance, e.g., trees surrounded by bushes/grass, methods modality mIoU (%) DCM-Net <ref type="bibr" target="#b53">[51]</ref> 3D + Mesh 65.8 VMNet <ref type="bibr" target="#b27">[27]</ref> 74.6 SparseConvNet <ref type="bibr" target="#b15">[16]</ref> 3D (voxel) 72.5 MinkowskiNet <ref type="bibr" target="#b8">[9]</ref> 73.6 O-CNN <ref type="bibr" target="#b59">[57]</ref> 76.2 OccuSeg <ref type="bibr" target="#b21">[22]</ref> 76.4 Mix3D <ref type="bibr" target="#b46">[44]</ref> 78.1 BA-GEM <ref type="bibr">[</ref> methods mIoU (%) HDGCN <ref type="bibr" target="#b40">[38]</ref> 68.3 ConvPoint <ref type="bibr" target="#b1">[2]</ref> 75.9 RandLANet <ref type="bibr" target="#b26">[26]</ref> 78.5 KP-Conv <ref type="bibr" target="#b55">[53]</ref> 82.0 FKAConv <ref type="bibr" target="#b2">[3]</ref> 82.7 PyramidPoint <ref type="bibr" target="#b58">[56]</ref> 82.9 ConvNet 76.2 + CBL 78.6 <ref type="table">Table 6</ref>. Quantitative results on Paris-Lille-3D of NPM3D <ref type="bibr" target="#b51">[49]</ref> benchmark, results obtained from online benchmark site by the time of submission.</p><p>which makes the separation of these two scenes challenging. The large improvement in both of these two classes demonstrates the effective improvement on scene boundaries. Lastly, with CBL, RandLA-Net obtains a leading performance of 78.4 in mIoU.</p><p>Further experiments on NPM3D and ScanNet. To further demonstrate the generalization of the proposed CBL, we report on another two popular dataset, the ScanNet <ref type="bibr" target="#b9">[10]</ref> (indoor scene) and NPM3D <ref type="bibr" target="#b51">[49]</ref> (outdoor scene). As shown in Tab. 5 and Tab. 6, our method achieves reasonable results and consistent improvement over the baseline. It thus shows that CBL is robust to different baselines, datasets, and types of scenes. Detailed results are available in the appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">Ablation Studies</head><p>We conduct ablation studies on the ScanNet validation set to evaluate the effectiveness of different components in the proposed CBL scheme.  <ref type="table">Table 7</ref>. Results on validation set of ScanNet <ref type="bibr" target="#b9">[10]</ref>. The CBL @input refers to only conduct contrastive boundary learning on the input point cloud (with point feature extracted from last upsampled stage), and @sub-scene refers to the CBL with sub-scene boundary mining. The red indicates relative improvement.</p><p>The Effectiveness of CBL. As shown in Tab. 7, the direct application of CBL on the input point cloud (without sub-scene boundary mining) can improve the performance, which demonstrates that boundary areas are worth more attention. By introducing sub-scene boundary mining, a more significant improvement is gained, as boundaries at multiple scales are identified and optimized in the CBL.</p><p>The Effect of Multi-scale Head. Comparing the ConvNet baseline with and without the multi-scale head, we find that a direct application of multi-scale head can even hurt the performance (-0.09 in OA). It shows that a direct concatenation across multiple scales can not bring much benefit. In contrast, with multi-scale head, ConvNet with CBL is further boosted to gain a larger improvement in both mIoU and OA. It shows that the main improvement is originated from the more discriminative features learned by CBL at different sub-sampled point clouds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>In this paper, we comprehensively analyze the segmentation performance on scene boundaries for the current point cloud segmentation methods. We show that the current segmentation accuracy on boundaries is unsatisfactory and quantitatively present the boundary problem with metrics, including mIoU@boundary and B-IoU. We further propose Contrastive Boundary Learning (CBL) to explicitly optimize the feature on boundaries and improve the model performance on boundaries. The leading performance and consistent improvement across various baselines and datasets demonstrate the effectiveness of CBL and the importance of scene boundaries in 3D point cloud segmentation. Limitation and future work. One of our limitation is that we mainly concentrate on the scene boundaries while ignoring the broad inner areas. Therefore, in the future, we would like to further explore the role of boundary in point cloud segmentation and its relation with inner areas. Acknowledgement. Dr Baosheng Yu and Mr Liyao Tang are supported by ARC FL-170100117, and Dr Zhe Chen is supported by ARC IH-180100002.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Introduction.</head><p>In this supplementary material, we provide more details regarding baseline architecture (Appendix B), the boundary problem Appendix C, visualization results (Appendix D), the training setup (Appendix E), the effect of temperature (Appendix F), the effect of design regarding subscene annotation (Appendix G), and experiment results (Appendix H).</p><p>Especially, CBL achieves a new stat-of-the-art on S3DIS with the newly released transformer model <ref type="figure">(Tab. 14)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Architecture of ConvNet Baseline</head><p>We show the specific architecture of our ConvNet baseline in <ref type="figure" target="#fig_3">Fig. 5</ref>. With a consistent notation, X n is the point cloud in sub-sampling stage n, f i is the feature of point x i , and N n = |X n | with N = N 0 . We use the multi-scale head on all baselines when adapting the CBL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Further Analysis on Boundary Problem</head><p>We further account for the type of areas and classspecific analysis for better exploring the boundary problem. Specifically, we provide per-class IoU score that is separately calculated on boundary area B l and inner area X ?B l .</p><p>As shown in Tab. 9, we evaluate for all three baselines with and without the proposed CBL. We notice that, large improvements are made on small objects, e.g. column, which aligns with the observation in Tab. 2 in main paper. We would like to add that, despite that CBL focuses only on boundaries, improvements are also made on inner area. We hypothesize the reason might be that the false boundary in model predicted segmentation is restrained, as features in inner area implicitly becomes more similar when the features across boundaries are optimized to be more distinctive by the CBL.</p><p>Moreover, for all three baselines, the improvement on boundary area is much more than that made on inner area, which is summarized in Tab. 8. Therefore, with metrics separately calculated on boundary and inner area, we clearly see that the improvement brought by CBL is mainly from the boundary areas. Such observation further emphasizes the importance of clear scene boundaries in point cloud segmentation task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. More Visualizations</head><p>We provide more qualitative results as a support for the improvement made by CBL on boundaries. The visualization results include various scenes, including rooms <ref type="figure">(Fig. 7)</ref>, cluttered space ( <ref type="figure">Fig. 8)</ref>, hallways <ref type="figure">(Fig. 9)</ref>, and offices ( <ref type="figure">Fig. 10)</ref>. For each scene, we further attempt to visualize the features discrimination between center points and their corresponding neighbors and the results are presented in the every second row. Specifically, we calculate the normalized feature distance between the point feature f i and features of its neighboring points {f j | x j ? N i }. We then take the mean distance for visualization. According to the presented figures, it shows that the CBL significantly enhances the feature distances around the scene boundaries and improves the baseline to obtain a more detailed and cleaner boundary in prediction for different type of scenes. The visualization is done on S3DIS testset Area 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Training Setup in Details</head><p>For the RandLA-Net <ref type="bibr" target="#b26">[26]</ref> and CloserLook3D <ref type="bibr" target="#b42">[40]</ref> baselines, we follow their instructions of released code for training and evaluation, which are here (RandLA-Net) and here (CloserLook3D), respectively. Especially, in Closer-Look3D <ref type="bibr" target="#b42">[40]</ref>, there are two non-parametric module, we use the one with sin/cos spatial embedding.</p><p>For the ConvNet baseline, we use the SGD optimizer to train for 600 epoch, with a weight decay of 0.001. We set the initial learning rate to 0.01 and use a momentum of 0.98 with a decay rate of 0.1 <ref type="bibr">1/200</ref> . It roughly takes 24 hours to train on 4 Nividia v100 GPUs, and we does not observe obvious increase in training time after applying the CBL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Effect of Temperature in CBL</head><p>We conduct empirical study on ScanNet <ref type="bibr" target="#b9">[10]</ref> validation set to analyze the effect of temperature ? in the CBL (Eq. (5)). We use the ConvNet baseline and train for 600 epoch on training set. As shown in Tab. 10, we find that the proper temperature for CBL is within (0.5, 2), and we set the temperature to ? = 1 by default.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Effect of Design of Sub-scene annotation</head><p>While the sub-scene annotation is a distribution, we only use the simple arg max when evaluating the boundary points. Therefore, it raises two particular question: 1) is it necessary to maintain the distribution? 2) is there any better way in utilizing the sub-scene annotation than the arg max?</p><p>In this section, we explore other alternatives and answer to this two questions with a particular focus of how they affect the model performance on boundaries. Necessities of maintaining distribution. There are two main reasons to leverage the average pooling on labels and maintain the distribution. First, current methods may not</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Skip Connection</head><p>Input:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MLP</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Output:</head><p>Output:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Concat Convolution Block MLPs</head><p>Sub-sampling Block Up-sampling Block Input:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Max-pool</head><p>Output:   preserve the original input points after sub-sampling, e.g. grid sub-sampling in KPConv <ref type="bibr" target="#b55">[53]</ref>. Therefore, the original label of a sub-sampled point is not presented and the sub-scene annotation is thus demanded. Although we may use the label of the nearest point for approximation, Tab. 12 shows that CBL (nearest) is sub-optimal. Second, despite that we only use the "argmax" result of the sub-scene annotation, maintaining distribution still preserves more infor-mation than just maintaining "argmax" result. As "argmax" discards the minor classes during sampling, such elimination of minority may further accumulate through more subsampling stages and leads to imprecise boundary, as depicted in <ref type="figure" target="#fig_5">Fig. 6</ref>. Experimentally, in Tab. 12, though CBL (argmax) improves boundary (B-IoU), it compromises overall performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Interpolation</head><p>Better treatment than Argmax. While "argmax" is straight forward, it introduces the problem of "labelflipping" when the distribution of sub-scene annotation is close to a uniform distribution, i.e., when the number of points of different classes are roughly the same.</p><p>To avoid this, we leverage the KL divergence as a measure of the semantic distance among sub-scene annotations. We then threshold on the KL-distance to determine if two sub-scene annotations belong to the same semantic class <ref type="table" target="#tab_1">Table 11</ref>. Quantitative results on Paris-Lille-3D of NPM3D <ref type="bibr" target="#b51">[49]</ref> benchmark, results obtained from online benchmark site by the time of submission. The red denotes the improvement made on baseline.   or not, which further enables us to determine the boundary points in sub-sampled point cloud. Specifically, we set the threhold to 0.5 and CBL (kl) can be bring a small improvement on overall performance, and a slightly larger boost on boundary performance, as in Tab. 12. Yet, as "thresholding KL distance" introduces extra hyper-parameters and complexity, we opt for "argmax" for simplicity in the main paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input label Stage 1 label</head><p>Summary. Therefore, we summarize the reason for designing the sub-scene annotation as a distribution as it can preserve much more information and can be extended to a more robust boundary determination using KL-distance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H. Further Experiments</head><p>Results on ScanNet and NPM3D datasets. We provide the detail results on ScanNet in Tab. 13; and the detail results on NPM3D in Tab. 11. CBL with Transformer.</p><p>We use the open-source code base (here) to re-produce the performance of newly released point Transformer <ref type="bibr" target="#b78">[73]</ref> on S3DIS [1] Area 5 dataset. In Tab. 14, the same consistent improvement is made on classes such as column. CBL with better boundaries further boosts the overall performance to 71.0 in mIoU, achieving a new state-of-the-art performance. <ref type="table" target="#tab_1">Table 13</ref>. Quantitative results on ScanNet <ref type="bibr" target="#b9">[10]</ref> benchmark, results obtained from online benchmark site by the time of submission. We group method by the 3D representation type, which is respectively, from top to down, 3D + mesh, 3D voxel and 3D point, and we also use 3D point. The empty line denotes no record of detailed performance found. The method with * also considers boundary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ground Truth</head><p>Baseline CBL Improvement Input  <ref type="figure">Figure 7</ref>. Large rooms. We compare the results of ConvNet baseline with CBL. On the every second row, we visualize the boundary points calculated from the ground truth label, and the feature discrimination among neighboring points for each model. The improvement on the first row and the enhanced feature discrimination on the second row show that CBL improves the features across boundaries to obtain a better segmentation quality on boundary areas. The visualization is done on S3DIS testset Area 5.  <ref type="table" target="#tab_1">Table 14</ref>. Quantitative results on S3DIS Area 5 dataset <ref type="bibr" target="#b0">[1]</ref>, showing the mean IoU (mIoU), overall accuracy (OA), mean accuracy (mACC), and per-class IoU scores. We include both performance reported in original paper (with *, the first row) and the re-produced performance (without *, the second row). We use red to denote improvement over the re-produced point transformer, and * to denote the improvement over the performance reported in original paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ground Truth</head><p>Baseline CBL Improvement Input <ref type="figure">Figure 8</ref>. Cluttered space. Same as above ( <ref type="figure">Fig. 7)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ground Truth</head><p>Baseline CBL Improvement Input (a) (b) <ref type="figure">Figure 9</ref>. Hallways. Same as above ( <ref type="figure">Fig. 7)</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>The detailed illustration of the Contrastive Boundary Learning.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>The detail architecture of ConvNet baseline.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 .</head><label>6</label><figDesc>With every 3 points being sub-sampled into 1 in each stage, tracking distribution (soft label) describes original input faithfully, but hard label fails due to accumulated errors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>The results are obtained on the S3DIS datasets testset Area 5, following the instruction of the officially released code of each method. Method with * also consider boundaries.</figDesc><table><row><cell>pointnet [45]</cell><cell>41.1</cell><cell>30.2</cell><cell>53.4</cell><cell>35.6</cell></row><row><cell>KPConv [53]</cell><cell>67.3</cell><cell>50.5</cell><cell>71.1</cell><cell>58.9</cell></row><row><cell>JSE-Net [28]*</cell><cell>67.7</cell><cell>50.5</cell><cell>71.4</cell><cell>60.9</cell></row><row><cell>RandLA-Net [26]</cell><cell>62.6</cell><cell>44.1</cell><cell>65.8</cell><cell>45.4</cell></row><row><cell>CloserLook3D [40]</cell><cell>66.9</cell><cell>50.0</cell><cell>70.7</cell><cell>59.2</cell></row><row><cell>ConvNet</cell><cell>67.4</cell><cell>50.1</cell><cell>71.2</cell><cell>59.6</cell></row><row><cell>RandLA-Net + CBL</cell><cell>65.3 +2.7</cell><cell>47.4 +3.3</cell><cell>67.2 +1.4</cell><cell>49.9 +4.5</cell></row><row><cell>CloserLook3D + CBL</cell><cell>67.5 +0.6</cell><cell>50.6 +0.6</cell><cell>71.0 +0.3</cell><cell>60.4 +1.2</cell></row><row><cell>ConvNet + CBL</cell><cell>69.4 +2.0</cell><cell>52.6 +2.5</cell><cell>73.1 +1.9</cell><cell>61.5 +1.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>methods mIoU OA mACC ceiling floor wall beam column window door table chair sofa bookcase board clutter</figDesc><table><row><cell>PointNet [45]</cell><cell>41.1</cell><cell>-</cell><cell>49.0</cell><cell>88.8</cell><cell>97.3 69.8</cell><cell>0.1</cell><cell>3.9</cell><cell>46.3</cell><cell>10.8 59.0 52.6</cell><cell>5.9</cell><cell>40.3</cell><cell>26.4</cell><cell>33.2</cell></row><row><cell>SegCloud [52]</cell><cell>48.9</cell><cell>-</cell><cell>57.4</cell><cell>90.1</cell><cell>96.1 69.9</cell><cell>0.0</cell><cell>18.4</cell><cell>38.4</cell><cell cols="2">23.1 70.4 75.9 40.9</cell><cell>58.4</cell><cell>13.0</cell><cell>41.6</cell></row><row><cell>PointCNN [37]</cell><cell cols="2">57.3 85.9</cell><cell>63.9</cell><cell>92.3</cell><cell>98.2 79.4</cell><cell>0.0</cell><cell>17.6</cell><cell>22.8</cell><cell cols="2">62.1 74.4 80.6 31.7</cell><cell>66.7</cell><cell>62.1</cell><cell>56.7</cell></row><row><cell>SPGraph [34]</cell><cell cols="2">58.0 86.4</cell><cell>66.5</cell><cell>89.4</cell><cell>96.9 78.1</cell><cell>0.0</cell><cell>42.8</cell><cell>48.9</cell><cell cols="2">61.6 84.7 75.4 69.8</cell><cell>52.6</cell><cell>2.1</cell><cell>52.2</cell></row><row><cell>PCT [18]</cell><cell>61.3</cell><cell>-</cell><cell>67.7</cell><cell>92.5</cell><cell>98.4 80.6</cell><cell>0.0</cell><cell>19.4</cell><cell>61.6</cell><cell cols="2">48.0 76.6 85.2 46.2</cell><cell>67.7</cell><cell>67.9</cell><cell>52.3</cell></row><row><cell>HPEIN [30]</cell><cell cols="2">61.9 87.2</cell><cell>68.3</cell><cell>91.5</cell><cell>98.2 81.4</cell><cell>0.0</cell><cell>23.3</cell><cell>65.3</cell><cell cols="2">40.0 75.5 87.7 58.5</cell><cell>67.8</cell><cell>65.6</cell><cell>49.4</cell></row><row><cell>MinkowskiNet [9]</cell><cell>65.4</cell><cell>-</cell><cell>71.7</cell><cell>91.8</cell><cell>98.7 86.2</cell><cell>0.0</cell><cell>34.1</cell><cell>48.9</cell><cell cols="2">62.4 81.6 89.8 47.2</cell><cell>74.9</cell><cell>74.4</cell><cell>58.6</cell></row><row><cell>KPConv [53]</cell><cell>67.1</cell><cell>-</cell><cell>72.8</cell><cell>92.8</cell><cell>97.3 82.4</cell><cell>0.0</cell><cell>23.9</cell><cell>58.0</cell><cell cols="2">69.0 81.5 91.0 75.4</cell><cell>75.3</cell><cell>66.7</cell><cell>58.9</cell></row><row><cell>JSENet [28]*</cell><cell>67.7</cell><cell>-</cell><cell>-</cell><cell>93.8</cell><cell>97.0 83.0</cell><cell>0.0</cell><cell>23.2</cell><cell>61.3</cell><cell cols="2">71.6 89.9 79.8 75.6</cell><cell>72.3</cell><cell>72.7</cell><cell>60.4</cell></row><row><cell>CGA-Net [41]</cell><cell>68.6</cell><cell>-</cell><cell>-</cell><cell>94.5</cell><cell>98.3 83.0</cell><cell>0.0</cell><cell>25.3</cell><cell>59.6</cell><cell cols="2">71.0 92.2 82.6 76.4</cell><cell>77.7</cell><cell>69.5</cell><cell>61.5</cell></row><row><cell>RandLA-Net [26]</cell><cell cols="2">62.4 87.2</cell><cell>71.4</cell><cell>91.1</cell><cell>95.6 80.2</cell><cell>0.0</cell><cell>24.7</cell><cell>62.3</cell><cell cols="2">47.7 76.2 83.7 60.2</cell><cell>71.1</cell><cell>65.7</cell><cell>53.8</cell></row><row><cell>+ CBL</cell><cell cols="2">65.3 87.5</cell><cell>74.5</cell><cell>92.2</cell><cell>97.7 81.0</cell><cell>0.0</cell><cell>36.8</cell><cell>61.0</cell><cell cols="2">39.4 78.1 88.1 81.4</cell><cell>71.5</cell><cell>68.7</cell><cell>52.6</cell></row><row><cell>CloserLook3D [40]</cell><cell cols="2">66.9 90.0</cell><cell>72.1</cell><cell>94.8</cell><cell>98.4 82.5</cell><cell>0.0</cell><cell>25.5</cell><cell>51.3</cell><cell cols="2">70.9 92.1 81.9 76.7</cell><cell>70.1</cell><cell>64.5</cell><cell>61.2</cell></row><row><cell>+ CBL</cell><cell cols="2">67.5 90.2</cell><cell>72.7</cell><cell>94.9</cell><cell>98.4 83.1</cell><cell>0.0</cell><cell>27.3</cell><cell>55.0</cell><cell cols="2">71.2 91.9 82.9 75.9</cell><cell>71.3</cell><cell>63.5</cell><cell>60.4</cell></row><row><cell>ConvNet</cell><cell cols="2">67.4 90.1</cell><cell>72.9</cell><cell>94.1</cell><cell>98.1 83.1</cell><cell>0.0</cell><cell>24.9</cell><cell>53.5</cell><cell cols="2">73.0 91.7 82.3 76.5</cell><cell>72.3</cell><cell>66.9</cell><cell>60.8</cell></row><row><cell>+ CBL</cell><cell cols="2">69.4 90.6</cell><cell>75.2</cell><cell>93.9</cell><cell>98.4 84.2</cell><cell>0.0</cell><cell>37.0</cell><cell>57.7</cell><cell cols="2">71.9 91.7 81.8 77.8</cell><cell>75.6</cell><cell>69.1</cell><cell>62.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>S3DIS [1] is a challenging point cloud dataset of indoor scenes. It contains 3D RGB point clouds of 6 indoor areas covering 272 rooms. Each point is annotated with one of the 13 semantic cat-methods mIoU OA mACC ceiling floor wall beam column window door table chair sofa bookcase board clutter</figDesc><table><row><cell>PointNet [45]</cell><cell cols="2">47.6 78.6</cell><cell>66.2</cell><cell>88.0</cell><cell>88.7 69.3 42.4</cell><cell>23.1</cell><cell>47.5</cell><cell>51.6 54.1 42.0</cell><cell>9.6</cell><cell>38.2</cell><cell>29.4</cell><cell>35.2</cell></row><row><cell>RSNet [29]</cell><cell>56.5</cell><cell>-</cell><cell>66.5</cell><cell>92.5</cell><cell>92.8 78.6 32.8</cell><cell>34.4</cell><cell>51.6</cell><cell cols="2">68.1 59.7 60.1 16.4</cell><cell>50.2</cell><cell>44.9</cell><cell>52.0</cell></row><row><cell>SPG [34]</cell><cell cols="2">62.1 86.4</cell><cell>73.0</cell><cell>89.9</cell><cell>95.1 76.4 62.8</cell><cell>47.1</cell><cell>55.3</cell><cell cols="2">68.4 73.5 69.2 63.2</cell><cell>45.9</cell><cell>8.7</cell><cell>52.9</cell></row><row><cell>PointCNN [37]</cell><cell cols="2">65.4 88.1</cell><cell>75.6</cell><cell>94.8</cell><cell>97.3 75.8 63.3</cell><cell>51.7</cell><cell>58.4</cell><cell cols="2">57.2 71.6 69.1 39.1</cell><cell>61.2</cell><cell>52.2</cell><cell>58.6</cell></row><row><cell>PointWeb [72]</cell><cell cols="2">66.7 87.3</cell><cell>76.2</cell><cell>93.5</cell><cell>94.2 80.8 52.4</cell><cell>41.3</cell><cell>64.9</cell><cell cols="2">68.1 71.4 67.1 50.3</cell><cell>62.7</cell><cell>62.2</cell><cell>58.5</cell></row><row><cell>ShellNet [71]</cell><cell cols="2">66.8 87.1</cell><cell>-</cell><cell>90.2</cell><cell>93.6 79.9 60.4</cell><cell>44.1</cell><cell>64.9</cell><cell cols="2">52.9 71.6 84.7 53.8</cell><cell>64.6</cell><cell>48.6</cell><cell>59.4</cell></row><row><cell>RandLA-Net [26]</cell><cell cols="2">70.0 88.0</cell><cell>82.0</cell><cell>93.1</cell><cell>96.1 80.6 62.4</cell><cell>48.0</cell><cell>64.4</cell><cell cols="2">69.4 69.4 76.4 60.0</cell><cell>64.2</cell><cell>65.9</cell><cell>60.1</cell></row><row><cell>KPConv [53]</cell><cell>70.6</cell><cell>-</cell><cell>79.1</cell><cell>93.6</cell><cell>92.4 83.1 63.9</cell><cell>54.3</cell><cell>66.1</cell><cell cols="2">76.6 57.8 64.0 69.3</cell><cell>74.9</cell><cell>61.3</cell><cell>60.3</cell></row><row><cell>SCF-Net [12]</cell><cell cols="2">71.6 88.4</cell><cell>82.7</cell><cell>93.3</cell><cell>96.4 80.9 64.9</cell><cell>47.4</cell><cell>64.5</cell><cell cols="2">70.1 71.4 81.6 67.2</cell><cell>64.4</cell><cell>67.5</cell><cell>60.9</cell></row><row><cell>BAAF [48]</cell><cell cols="2">72.2 88.9</cell><cell>83.1</cell><cell>93.3</cell><cell>96.8 81.6 61.9</cell><cell>49.5</cell><cell>65.4</cell><cell cols="2">73.3 72.0 83.7 67.5</cell><cell>64.3</cell><cell>67.0</cell><cell>62.4</cell></row><row><cell>ConvNet</cell><cell cols="2">69.7 88.6</cell><cell>76.8</cell><cell>93.8</cell><cell>91.9 84.2 46.3</cell><cell>52.1</cell><cell>66.7</cell><cell cols="2">78.5 75.2 72.8 70.1</cell><cell>71.7</cell><cell>57.1</cell><cell>61.3</cell></row><row><cell>+ CBL</cell><cell cols="2">73.1 89.6</cell><cell>79.4</cell><cell>94.1</cell><cell>94.2 85.5 50.4</cell><cell>58.8</cell><cell>70.3</cell><cell cols="2">78.3 75.7 75.0 71.8</cell><cell>74.0</cell><cell>60.0</cell><cell>62.4</cell></row></table><note>S3DIS Indoor Scene Segmentation.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 .</head><label>3</label><figDesc>Quantitative results on S3DIS [1] with 6-fold cross validation. The red denotes improvement over baseline and the bold or bold denotes the best performance.</figDesc><table><row><cell cols="11">methods mIoU (%) OA (%) man-made. natural. high veg. low veg. buildings hard scape scanning art. cars</cell></row><row><cell>SnapNet [4]</cell><cell>59.1</cell><cell>88.6</cell><cell>82.0</cell><cell>77.3</cell><cell>79.7</cell><cell>22.9</cell><cell>91.1</cell><cell>18.4</cell><cell>37.3</cell><cell>64.4</cell></row><row><cell>SEGCloud [52]</cell><cell>61.3</cell><cell>88.1</cell><cell>83.9</cell><cell>66.0</cell><cell>86.0</cell><cell>40.5</cell><cell>91.1</cell><cell>30.9</cell><cell>27.5</cell><cell>64.3</cell></row><row><cell>SPG [34]</cell><cell>73.2</cell><cell>94.0</cell><cell>97.4</cell><cell>92.6</cell><cell>87.9</cell><cell>44.0</cell><cell>83.2</cell><cell>31.0</cell><cell>63.5</cell><cell>76.2</cell></row><row><cell>RGNet [54]</cell><cell>74.7</cell><cell>94.5</cell><cell>97.5</cell><cell>93.0</cell><cell>88.1</cell><cell>48.1</cell><cell>94.6</cell><cell>36.2</cell><cell>72.0</cell><cell>68.0</cell></row><row><cell>KPConv [53]</cell><cell>74.6</cell><cell>92.9</cell><cell>90.9</cell><cell>82.2</cell><cell>84.2</cell><cell>47.9</cell><cell>94.9</cell><cell>40.0</cell><cell>77.3</cell><cell>79.7</cell></row><row><cell>RFCR [14]</cell><cell>77.8</cell><cell>94.3</cell><cell>94.2</cell><cell>89.1</cell><cell>85.7</cell><cell>54.4</cell><cell>95.0</cell><cell>43.8</cell><cell>76.2</cell><cell>83.7</cell></row><row><cell>SCF-Net [12]</cell><cell>77.6</cell><cell>94.7</cell><cell>97.1</cell><cell>91.8</cell><cell>86.3</cell><cell>51.2</cell><cell>95.3</cell><cell>50.5</cell><cell>67.9</cell><cell>80.7</cell></row><row><cell>ConvNet</cell><cell>72.8</cell><cell>92.6</cell><cell>92.2</cell><cell>79.9</cell><cell>84.4</cell><cell>41.3</cell><cell>95.2</cell><cell>41.2</cell><cell>62.6</cell><cell>85.6</cell></row><row><cell>+ CBL</cell><cell>75.0</cell><cell>94.0</cell><cell>96.2</cell><cell>90.1</cell><cell>84.0</cell><cell>47.5</cell><cell>94.7</cell><cell>36.0</cell><cell>64.8</cell><cell>86.3</cell></row><row><cell>RandLA-Net [26]</cell><cell>77.4</cell><cell>94.8</cell><cell>95.6</cell><cell>91.4</cell><cell>86.6</cell><cell>51.5</cell><cell>95.7</cell><cell>51.5</cell><cell>69.8</cell><cell>76.8</cell></row><row><cell>+ CBL</cell><cell>78.4</cell><cell>95.0</cell><cell>95.3</cell><cell>91.3</cell><cell>87.9</cell><cell>55.6</cell><cell>96.3</cell><cell>56.2</cell><cell>65.9</cell><cell>78.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>+0.34 89.01 +0.04 ConvNet 70.98 +1.27 89.31 +0.34 69.83 +0.12 88.88 -0.09 ConvNet (multiscale head) 71.33 +1.62 89.40 +0.43</figDesc><table><row><cell>CBL @input @sub-scenes</cell><cell cols="2">mIoU(%)</cell><cell>OA(%)</cell></row><row><cell></cell><cell>69.71</cell><cell>-</cell><cell>88.97</cell><cell>-</cell></row><row><cell></cell><cell>70.05</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 8 .</head><label>8</label><figDesc>The improvement brought by CBL on different baselines and types of area (boundary / inner area).</figDesc><table><row><cell cols="7">mIoU baselines ( + CBL) boundary inner boundary inner boundary inner OA mACC</cell></row><row><cell>RandLA-Net [26]</cell><cell>+3.3</cell><cell>+1.4</cell><cell>+4.1</cell><cell>-0.3</cell><cell>+3.4</cell><cell>+2.4</cell></row><row><cell>CloserLook3D [40]</cell><cell>+0.6</cell><cell>+0.2</cell><cell>+0.1</cell><cell>+0.2</cell><cell>+0.7</cell><cell>+0.4</cell></row><row><cell>ConvNet</cell><cell>+2.5</cell><cell>+2.0</cell><cell>+1.0</cell><cell>+0.7</cell><cell>+3.2</cell><cell>+2.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>The full metrics calculated on boundary points from ground truth (i.e., B l ) only. methods mIoU OA mACC ceiling floor wall beam column window door table chair sofa bookcase board clutter The full metrics calculated on inner points from ground truth (i.e., X ? Bp) only.</figDesc><table><row><cell cols="12">methods mIoU OA mACC ceiling floor wall beam column window door table chair sofa bookcase board clutter</cell></row><row><cell>RandLA-Net [26]</cell><cell>44.1 67.1</cell><cell>59.1</cell><cell>65.5</cell><cell>69.4 52.2</cell><cell>0.0</cell><cell>21.4</cell><cell>28.6</cell><cell>55.0 55.0 56.0 41.1</cell><cell>41.2</cell><cell>45.8</cell><cell>42.1</cell></row><row><cell>+ CBL</cell><cell>47.4 71.2</cell><cell>62.5</cell><cell>78.2</cell><cell>85.9 56.0</cell><cell>0.0</cell><cell>30.3</cell><cell>25.7</cell><cell>42.6 58.4 60.9 50.0</cell><cell>42.5</cell><cell>52.2</cell><cell>44.2</cell></row><row><cell>CloserLook3D [40]</cell><cell>50.0 76.6</cell><cell>58.5</cell><cell>80.7</cell><cell>88.6 63.9</cell><cell>0.0</cell><cell>21.1</cell><cell>15.6</cell><cell>57.5 73.3 64.7 52.2</cell><cell>43.1</cell><cell>37.2</cell><cell>52.6</cell></row><row><cell>+ CBL</cell><cell>50.6 76.7</cell><cell>59.2</cell><cell>80.9</cell><cell>88.6 64.6</cell><cell>0.0</cell><cell>26.5</cell><cell>15.6</cell><cell>55.9 73.0 65.0 50.4</cell><cell>47.6</cell><cell>38.4</cell><cell>51.2</cell></row><row><cell>ConvNet</cell><cell>50.1 76.5</cell><cell>58.3</cell><cell>80.4</cell><cell>88.3 63.5</cell><cell>0.0</cell><cell>26.5</cell><cell>15.2</cell><cell>58.3 72.1 63.4 52.3</cell><cell>40.8</cell><cell>38.7</cell><cell>52.2</cell></row><row><cell>+ CBL</cell><cell>52.6 77.5</cell><cell>61.5</cell><cell>80.5</cell><cell>88.8 65.7</cell><cell>0.0</cell><cell>32.5</cell><cell>20.9</cell><cell>61.8 71.7 62.4 52.5</cell><cell>46.7</cell><cell>47.4</cell><cell>52.5</cell></row><row><cell cols="2">(a) RandLA-Net [26] 65.8 89.6</cell><cell>73.0</cell><cell>93.3</cell><cell>98.6 84.6</cell><cell>0.0</cell><cell>25.9</cell><cell>65.7</cell><cell>46.5 81.1 88.9 65.4</cell><cell>75.5</cell><cell>71.9</cell><cell>58.2</cell></row><row><cell>+ CBL</cell><cell>67.2 89.3</cell><cell>75.4</cell><cell>93.0</cell><cell>99.1 84.6</cell><cell>0.0</cell><cell>37.3</cell><cell>64.1</cell><cell>39.4 82.7 91.5 79.3</cell><cell>75.9</cell><cell>73.9</cell><cell>56.0</cell></row><row><cell>CloserLook3D [40]</cell><cell>70.7 92.2</cell><cell>75.2</cell><cell>96.4</cell><cell>99.9 86.5</cell><cell>0.0</cell><cell>25.9</cell><cell>55.1</cell><cell>76.5 95.9 87.1 81.9</cell><cell>75.1</cell><cell>72.5</cell><cell>66.2</cell></row><row><cell>+ CBL</cell><cell>70.9 92.4</cell><cell>75.6</cell><cell>96.5</cell><cell>99.9 86.9</cell><cell>0.0</cell><cell>27.0</cell><cell>59.3</cell><cell>78.1 95.7 87.7 80.8</cell><cell>75.4</cell><cell>69.4</cell><cell>65.6</cell></row><row><cell>ConvNet</cell><cell>71.2 92.1</cell><cell>75.5</cell><cell>95.0</cell><cell>99.8 85.9</cell><cell>0.0</cell><cell>34.6</cell><cell>56.0</cell><cell>82.7 95.4 87.4 81.3</cell><cell>73.8</cell><cell>68.4</cell><cell>65.7</cell></row><row><cell>+ CBL</cell><cell>73.2 92.8</cell><cell>78.3</cell><cell>95.3</cell><cell>99.9 88.0</cell><cell>0.0</cell><cell>38.4</cell><cell>62.2</cell><cell>76.4 95.9 87.5 82.7</cell><cell>81.2</cell><cell>75.2</cell><cell>68.6</cell></row><row><cell></cell><cell>(b)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 9 .Table 10 .</head><label>910</label><figDesc>The improvement CBL brought on baselines, separately calculated in boundary area (a) and inner area (b). The red denotes improvement is made on baseline. The effect of temperature on CBL.</figDesc><table><row><cell cols="2">temperature mIoU</cell><cell>OA</cell><cell>mACC</cell></row><row><cell>0.3</cell><cell cols="3">70.67 89.16 77.91</cell></row><row><cell>0.5</cell><cell cols="3">70.98 89.31 78.27</cell></row><row><cell>1</cell><cell cols="3">71.33 89.40 78.69</cell></row><row><cell>2</cell><cell cols="3">70.73 89.10 77.98</cell></row><row><cell>10</cell><cell cols="3">70.03 88.97 77.58</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 12 .</head><label>12</label><figDesc>Same setting as in Tab. 1 in main paper.</figDesc><table><row><cell>methods</cell><cell cols="3">mIoU overall @boundary @inner</cell><cell>B-IoU</cell></row><row><cell>ConvNet</cell><cell>67.4</cell><cell>50.1</cell><cell>71.2</cell><cell>59.6</cell></row><row><cell>ConvNet + CBL</cell><cell>69.4</cell><cell>52.6</cell><cell>73.1</cell><cell>61.5</cell></row><row><cell>ConvNet + CBL (nearest)</cell><cell>68.3</cell><cell>52.1</cell><cell>71.8</cell><cell>60.9</cell></row><row><cell>ConvNet + CBL (argmax)</cell><cell>66.8</cell><cell>50.6</cell><cell>70.4</cell><cell>60.6</cell></row><row><cell>ConvNet + CBL (kl)</cell><cell>69.5</cell><cell>52.5</cell><cell>73.2</cell><cell>62.0</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">We choose arg max for its simplicity and non-parametric nature. We provide more analysis on this choice in the appendix.</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ground Truth</head><p>Baseline CBL Improvement Input  <ref type="figure">Figure 10</ref>. Offices. Same as above ( <ref type="figure">Fig. 7)</ref>.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Joint 2d-3d-semantic data for indoor scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iro</forename><surname>Armeni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sasha</forename><surname>Sax</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Amir R Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Savarese</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.01105</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Convpoint: Continuous convolutions for point cloud processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Boulch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers &amp; Graphics</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">14</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Fkaconv: Feature-kernel alignment for point cloud convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Boulch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gilles</forename><surname>Puy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renaud</forename><surname>Marlet</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Unstructured point cloud semantic labeling using deep segmentation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Boulch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">Le</forename><surname>Saux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Audebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on 3D Object Retrieval, 3Dor &apos;17</title>
		<meeting>the Workshop on 3D Object Retrieval, 3Dor &apos;17<address><addrLine>Goslar, DEU</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Sasa: Semantics-augmented set abstraction for point-based 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<biblScope unit="volume">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.05709,2020.3</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Progressive lidar adaptation for road detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/CAA Journal of Automatica Sinica</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="693" to="702" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Boundary IoU: Improving object-centric image segmentation evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">4d spatio-temporal convnets: Minkowski convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Scannet: Richly-annotated 3d reconstructions of indoor scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angel</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manolis</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maciej</forename><surname>Halber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Nie?ner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning to sample</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Dovrat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Itai</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shai</forename><surname>Avidan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Scf-net: Learning spatial contextual features for large-scale point cloud segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiulei</forename><surname>Siqi Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fenghua</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yisheng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peijun</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei-Yue</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="14504" to="14513" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Analyzing and improving representations with the soft nearest neighbor loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Frosst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Omni-supervised point cloud segmentation via gradual receptive field component reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingyu</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiachen</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haichuan</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanyun</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lizhuang</forename><surname>Ma</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
	<note>CoRR, abs/2105.10203, 2021. 4, 7, 8</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Boundary-aware geometric encoding for semantic segmentation of point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingyu</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiachen</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanyun</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lizhuang</forename><surname>Ma</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">3d semantic segmentation with submanifold sparse convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Engelcke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Submanifold sparse convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<idno>abs/1706.01307</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junxiong</forename><surname>Meng-Hao Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng-Ning</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tai-Jiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralph</forename><forename type="middle">R</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shi-Min</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hu</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>PCT: point cloud transformer. CoRR, abs/2012.09688, 2020. 2, 6</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Deep learning for 3d point clouds: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanyun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingyong</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammed</forename><surname>Bennamoun</surname></persName>
		</author>
		<idno>2020. 1</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Noise-contrastive estimation: A new estimation principle for unnormalized statistical models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Gutmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aapo</forename><surname>Hyv?rinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research -Proceedings Track</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2010-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">NET: A new large-scale point cloud classification benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Hackel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Savinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ladicky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><forename type="middle">D</forename><surname>Wegner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Semantic3d</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISPRS Annals of the Photogrammetry, Remote Sensing and Spatial Information Sciences, volume IV-1-W1</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Occuseg: Occupancy-aware 3d instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Fang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.05722</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Monte carlo convolution for learning on nonuniformly sampled point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hermosilla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ritschel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P-P</forename><surname>Vazquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vinacua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ropinski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (Proceedings of SIGGRAPH Asia</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">37</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Exploring data-efficient 3d scene understanding with contrastive scene contexts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Nie?ner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<idno>abs/2012.09165, 2020. 3</idno>
		<imprint>
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Randla-net: Efficient semantic segmentation of large-scale point clouds. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingyong</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linhai</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Rosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhihua</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Trigoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Markham</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1911" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Vmnet: Voxel-mesh network for geodesic-aware 3d semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeyu</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuyang</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxiang</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Runze</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiayu</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangyuan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongbo</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chiew-Lan</forename><surname>Tai</surname></persName>
		</author>
		<idno>abs/2107.13824</idno>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CoRR</publisher>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Jsenet: Joint semantic segmentation and edge detection network for 3d point clouds. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeyu</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingmin</forename><surname>Zhen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuyang</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongbo</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chiew-Lan</forename><surname>Tai</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Recurrent slice networks for 3d segmentation of point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiangui</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiyue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulrich</forename><surname>Neumann</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2626" to="2635" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Hierarchical point-edge interaction network for point cloud semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10432" to="10440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prannay</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Teterwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Sarna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Maschinot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ce</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
	<note>Supervised contrastive learning</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Virtual multi-view fusion for 3d semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhijit</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoqi</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alireza</forename><surname>Fathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">A</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Brewington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">A</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caroline</forename><surname>Pantofaru</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A structured regularization framework for spatially smoothing semantic labelings of 3d point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lo?c</forename><surname>Landrieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Raguet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bruno</forename><surname>Vallet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cl?ment</forename><surname>Mallet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Weinmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPRS Journal of Photogrammetry and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">132</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="102" to="118" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Large-scale point cloud semantic segmentation with superpoint graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Landrieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Simonovsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Deep projective 3d semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Felix J?remo Lawin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrik</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Goutam</forename><surname>Tosteberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fahad</forename><surname>Bhat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Shahbaz Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Felsberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Lecture Notes in Computer Science</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="95" to="107" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Structure boundary preserving segmentation for medical image with ambiguous boundary</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong Joo</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jung</forename><forename type="middle">Uk</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangmin</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hak</forename><surname>Gu Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><forename type="middle">Man</forename><surname>Ro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020-06" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Pointcnn: Convolution on x-transformed points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangyan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Bu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingchao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinhan</forename><surname>Di</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoquan</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2006" />
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Hierarchical depthwise graph convolutional neural network for 3d semantic segmentation of point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhidong</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liuyuan</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunxiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">P4contrast: Contrastive learning with pairs of point-pixel pairs for RGB-D scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunze</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanghang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingnan</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">A</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Dong</surname></persName>
		</author>
		<idno>abs/2012.13089, 2020. 3</idno>
		<imprint>
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">A closer look at local aggregation operators in point cloud analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Tong</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<publisher>ECCV</publisher>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Cga-net: Category guided aggregation for point cloud semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gangshan</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2021-06" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Efficient segmentation: Learning downsampling near semantic boundaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitrii</forename><surname>Marin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zijian</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Vajda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priyam</forename><surname>Chatterjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuri</forename><surname>Boykov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Rangenet++: Fast and accurate lidar semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Milioto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Vizzo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Behley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Stachniss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4213" to="4220" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Mix3d: Out-of-context data augmentation for 3d scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Nekrasov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Schult</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Or</forename><surname>Litany</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Leibe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francis</forename><surname>Engelmann</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Pointnet: Deep learning on point sets for 3d classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Charles Ruizhongtai Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaichun</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
		<idno>abs/1612.00593</idno>
		<imprint>
			<date type="published" when="2005" />
			<publisher>CoRR</publisher>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Volumetric and multi-view cnns for object classification on 3d data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Niebner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengyuan</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Pointnet++: Deep hierarchical feature learning on point sets in a metric space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Charles Ruizhongtai Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
		<idno>abs/1706.02413</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Semantic segmentation for real point cloud scenes via bilateral augmentation and adaptive fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saeed</forename><surname>Shi Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Anwar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Barnes</surname></persName>
		</author>
		<idno>abs/2103.07074</idno>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CoRR</publisher>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Paris-lille-3d: A large and high-quality groundtruth urban point cloud dataset for automatic segmentation and classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Roynard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Emmanuel</forename><surname>Deschaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fran?ois</forename><surname>Goulette</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The International Journal of Robotics Research</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">14</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Fast point feature histograms (fpfh) for 3d registration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Blodow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Beetz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE International Conference on Robotics and Automation</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="3212" to="3217" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Dualconvmesh-net: Joint geodesic and euclidean convolutions on 3d meshes. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Schult</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francis</forename><surname>Engelmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theodora</forename><surname>Kontogianni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Leibe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1002" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Segcloud: Semantic segmentation of 3d point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lyne</forename><surname>Tchapmi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iro</forename><surname>Armeni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on 3D Vision (3DV)</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Kpconv: Flexible and deformable convolution for point clouds. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugues</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Emmanuel</forename><surname>Deschaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beatriz</forename><surname>Marcotegui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fran?ois</forename><surname>Goulette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1904" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Fast point cloud registration using semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Truong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Gilani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M S</forename><surname>Islam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Suter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Digital Image Computing: Techniques and Applications (DICTA)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Representation learning with contrastive predictive coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yazhe</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Pyramid point: A multi-level focusing network for revisiting feature layers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nina</forename><surname>Varney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Vijayan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quinn</forename><surname>Asari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Graehling</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng-Shuai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Xiao</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun-Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>O-Cnn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Octree-based Convolutional Neural Networks for 3D Shape Analysis</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Exploring cross-image pixel contrast for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenguan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianfei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ender</forename><surname>Konukoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<idno>abs/2101.11939, 2021. 3</idno>
		<imprint>
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Dense contrastive learning for self-supervised visual pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rufeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Dynamic graph CNN for learning on point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongbin</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjay</forename><forename type="middle">E</forename><surname>Sarma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><forename type="middle">M</forename><surname>Solomon</surname></persName>
		</author>
		<idno>abs/1801.07829</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Voxsegnet: Volumetric cnns for semantic part segmentation of 3d shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zongji</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on visualization and computer graphics</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenxuan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongang</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuxin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pointconv</surname></persName>
		</author>
		<idno>abs/1811.07246</idno>
		<title level="m">Deep convolutional networks on 3d point clouds. CoRR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">Pointcontrast: Unsupervised pre-training for 3d point cloud understanding. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiatao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Demi</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Or</forename><surname>Litany</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Linking points with labels in 3d: A review of point cloud semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">X</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geoscience and Remote Sensing Magazine</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="38" to="59" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Squeeze-segv3: Spatially-adaptive convolution for efficient pointcloud segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenfeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bichen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zining</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Vajda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Keutzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masayoshi</forename><surname>Tomizuka</surname></persName>
		</author>
		<idno>abs/2004.01803</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">Regioncl: Can simple region swapping contribute to contrastive learning?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yufei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title level="m" type="main">Pointasnl: Robust point clouds processing using nonlocal neural networks with adaptive sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaoda</forename><surname>Xu Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuguang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cui</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Corr</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<title level="m" type="main">Modeling point clouds with self-attention and gumbel subset sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiancheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingbing</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linguo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinxian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengdie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Corr</surname></persName>
		</author>
		<idno>abs/1904.03375</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
		<title level="m" type="main">Multi receptive field network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlong</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zelu</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenbo</forename><surname>Lui</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Corr</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Deep fusionnet for point cloud semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feihu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><forename type="middle">W</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Shellnet: Efficient point cloud convolutional neural networks using concentric shells statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Binh-Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sai-Kit</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yeung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Pointweb: Enhancing local neighborhood features for point cloud processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi-Wing</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
	<note>Point transformer, 2021. 2, 5</note>
</biblStruct>

<biblStruct xml:id="b79">
	<monogr>
		<title level="m" type="main">mIoU (%) Ground Building Pole Bollard Trash can Barrier Pedestrian Car Natural</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
