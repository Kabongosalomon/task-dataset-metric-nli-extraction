<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">TOWARDS PRACTICAL LIPREADING WITH DISTILLED AND EFFICIENT MODELS</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pingchuan</forename><surname>Ma</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computing</orgName>
								<orgName type="institution">Imperial College London</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brais</forename><surname>Martinez</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Samsung AI Research Center</orgName>
								<address>
									<settlement>Cambridge</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stavros</forename><surname>Petridis</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computing</orgName>
								<orgName type="institution">Imperial College London</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Samsung AI Research Center</orgName>
								<address>
									<settlement>Cambridge</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maja</forename><surname>Pantic</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computing</orgName>
								<orgName type="institution">Imperial College London</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">TOWARDS PRACTICAL LIPREADING WITH DISTILLED AND EFFICIENT MODELS</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T12:15+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Visual Speech Recognition</term>
					<term>Lip-reading</term>
					<term>Knowledge Distillation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Lipreading has witnessed a lot of progress due to the resurgence of neural networks. Recent works have placed emphasis on aspects such as improving performance by finding the optimal architecture or improving generalization. However, there is still a significant gap between the current methodologies and the requirements for an effective deployment of lipreading in practical scenarios. In this work, we propose a series of innovations that significantly bridge that gap: first, we raise the state-of-the-art performance by a wide margin on LRW and LRW-1000 to 88.5 % and 46.6 %, respectively using self-distillation. Secondly, we propose a series of architectural changes, including a novel Depthwise Separable Temporal Convolutional Network (DS-TCN) head, that slashes the computational cost to a fraction of the (already quite efficient) original model. Thirdly, we show that knowledge distillation is a very effective tool for recovering performance of the lightweight models. This results in a range of models with different accuracy-efficiency trade-offs. However, our most promising lightweight models are on par with the current state-of-the-art while showing a reduction of 8.2? and 3.9? in terms of computational cost and number of parameters, respectively, which we hope will enable the deployment of lipreading models in practical applications.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Lipreading has attracted a lot of research interest recently thanks to the superior performance of deep architectures. Such models consist of either fully connected <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref> or convolutional layers <ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref> which extract features from the mouth region of interest, followed by recurrent layers or attention <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10]</ref> / self-attention architectures <ref type="bibr" target="#b7">[8]</ref>. However, one of the major limitations of current models barring their use in practical applications is their computational cost. Many speech recognition applications rely on on-device computing, where the computational capacity is limited, and memory footprint ? The first two authors contributed equally. and battery consumption are also important factors. As a consequence, few works have also focused on the computational complexity of visual speech recognition <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12]</ref>, but such models still trail massively behind full-fledged ones in terms of accuracy.</p><p>In this work we focus on improving the performance of the state-of-the-art model and training lightweight models without considerable decrease in performance. Lipreading is a challenging task due to the nature of the signal, where a model is tasked with distinguishing between e.g. million and millions solely based on visual information. We resort to Knowledge Distillation (KD) <ref type="bibr" target="#b12">[13]</ref> since it provides an extra supervisory signal with inter-class similarity information. For example, if two classes are very similar as in the case above, the KD loss will penalize less when the algorithm confuses them. We leverage this insight to produce a sequence of teacher-student classifiers in the same manner as <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15]</ref>, by which student and teacher have the same architecture, and the student will become the teacher in the next generation until no improvement is observed.</p><p>Our second contribution is to propose a novel lightweight architecture. The ResNet-18 backbone can be readily exchanged for an efficient one, such as a version of the Mo-bileNet <ref type="bibr" target="#b15">[16]</ref> or ShuffleNet <ref type="bibr" target="#b16">[17]</ref> families. However, there is no such equivalent for the head classifier. The key to designing the efficient backbones is the use of depthwise separable convolutions (a depthwise convolution followed by a pointwise convolution) <ref type="bibr" target="#b17">[18]</ref> to replace standard convolutions. This operation dramatically reduces the amount of parameters and the number of FLOPs. Thus, we devise a novel variant of the Temporal Convolutional Networks that relies on depthwise separable convolutions instead.</p><p>Our third contribution is to use the KD framework to recover some of the performance drop of these efficient networks. Unlike the full-fledged case, it is now possible to use a higher-capacity network to drive the optimization. However, we find that just using the best-performing model as the teacher, which is the standard practice in the literature, yields sub-optimal performance. Instead, we use intermediate networks whose architecture is in-between the full-fledged and the efficient one. Thus, similar to <ref type="bibr" target="#b18">[19]</ref>, we generate a sequence of teacher-student pairs that progressively bridges the architectural gap. We provide experimental evidence showing that a) we achieve new state-of-the-art performance on LRW <ref type="bibr" target="#b8">[9]</ref> and LRW-1000 <ref type="bibr" target="#b19">[20]</ref> by a wide margin and without any increase of computational performance 1 and b) our lightweight models can achieve competitive performance. For example, we match the current state-of-the-art on LRW <ref type="bibr" target="#b20">[21]</ref> using 8.2? fewer FLOPs and 3.9? fewer parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">TOWARDS PRACTICAL LIPREADING</head><p>Base Architecture We use the visual speech recognition architecture proposed in <ref type="bibr" target="#b20">[21]</ref> as our base architecture which achieves the state-of-the-art performance on the LRW and LRW1000 datasets. The details are shown in <ref type="figure" target="#fig_2">Fig. 2a</ref>. It consists of a modified ResNet-18 backbone in which the first convolution has been substituted by a 3D convolution of kernel size 5 ? 7 ? 7. The rest of the network follows a standard design up to the global average pooling layer. A multi-scale temporal convolutional network (MS-TCN) follows to model the short-term and long-term temporal information simultaneously. Efficient Backbone The efficient spatial backbone is produced by replacing the ResNet-18 with an efficient network based on depthwise separable convolutions. For the purpose of this study, we use ShuffleNet v2 (??) as the backbone, where ? is the width multiplier <ref type="bibr" target="#b16">[17]</ref>. This architecture uses depthwise convolutions and channel shuffling which is designed to enable information communication between different groups of channels. ShuffleNet v2 (1.0?) has 5? fewer parameters and 12? fewer FLOPs than ResNet-18. The architecture is shown in <ref type="figure" target="#fig_2">Fig. 2b</ref>. Depthwise Separable TCN We note that the cost of the convolution operation with kernel size greater than 1 in MS-TCN is non-negligible. To build an efficient architecture (shown in <ref type="figure" target="#fig_2">Fig. 2c</ref>), we replace standard convolutions with depthwise separable convolutions in MS-TCN. We first apply in each channel a convolution with kernel size k, where channel interactions are directly ignored. This is followed by a point-wise convolution with kernel size 1 which transforms the C in input channels to C out output channels. Thus, the cost of convolution is reduced from k ? C in ? C out (standard convolution) to k ? C in + C in C out . The architecture is denoted as a Depthwise Separable Temporal Convolutional  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Network (DS-TCN).</head><p>Knowledge Distillation Knowledge Distillation (KD) <ref type="bibr" target="#b12">[13]</ref> was initially proposed to transfer knowledge from a teacher model to a student model for compression purposes, i.e., the student capacity is much smaller than the teacher one. Recent studies <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b21">22]</ref> have experimentally shown that the student can still benefit when the teacher and student network have identical architectures. This naturally gave rise to the idea of training in generations. In particular, the student of one generation is used as the teacher of the subsequent generation. This self-distillation process, called born-again distillation, is iterated until no further improvement is observed. Finally, an ensemble can be optionally used so as to combine the predictions from multiple generations <ref type="bibr" target="#b13">[14]</ref>. The training pipeline is shown in <ref type="figure" target="#fig_0">Fig. 1</ref>.</p><p>In this work, we use born-again distillation for improving the performance of the state-of-the-art model. We also use the standard knowledge distillation to train a series of efficient models where each student has smaller capacity than the teacher. In both cases, we aim to minimise the combination of cross-entropy loss (L CE ) for hard targets and Kullback-Leibler (KL) divergence loss (L KD ) for soft targets. Let us denote the labels as y, the parameters of student and teacher models as ? s and ? t , respectively, and the predictions from the student and teacher models as z s and z t , respectively. ?(?) denotes the softmax function and ? is a hyperparameter to balance the loss terms. The overall loss function is calculated as follows:</p><formula xml:id="formula_0">L = L CE (y, ?(z s ; ? s )) + ?L KD (?(z s ; ? s ), ?(z t ; ? t )) (1)</formula><p>Note that we have omitted the temperature term, which is commonly used to soften the logits of the L KD term, since we found it to be unnecessary in our case.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Top-1 Acc. (%) 3D-CNN <ref type="bibr" target="#b22">[23]</ref> 61.1 Seq-to-Seq <ref type="bibr" target="#b8">[9]</ref> 76.2 ResNet34 + BLSTM <ref type="bibr" target="#b5">[6]</ref> 83.0 ResNet34 + BGRU <ref type="bibr" target="#b23">[24]</ref> 83.4 2-stream 3D-CNN + BLSTM <ref type="bibr" target="#b24">[25]</ref> 84.1 ResNet-18 + BLSTM <ref type="bibr" target="#b25">[26]</ref> 84.3 ResNet-18 + BGRU + Cutout <ref type="bibr" target="#b26">[27]</ref> 85.0 ResNet-18 + MS-TCN <ref type="bibr" target="#b20">[21]</ref> 85.3  <ref type="table">Table 1</ref>: Comparison with state-of-the-art methods on the LRW dataset in terms of classification accuracy. Each student is trained using the model from the line above as a teacher. Student i stands for the model after the i-th self-distillation iteration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ResNet</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">EXPERIMENTAL SETUP</head><p>Datasets Lip Reading in the Wild (LRW) <ref type="bibr" target="#b8">[9]</ref> is based on a collection of over 1000 speakers from BBC programs. It contains over half a million utterances of 500 English words. Each utterance is composed of 29 frames (1.16 seconds), where the target word is surrounded by other context words. LRW-1000 <ref type="bibr" target="#b19">[20]</ref> contains more than 2000 speakers and has 1000 Mandarin syllable-based classes with a total of 718018 utterances. It contains utterances of varying length from 0.01 up to 2.25 seconds.</p><p>Pre-processing For the video sequences in the LRW dataset, 68 facial landmarks are detected and tracked using dlib <ref type="bibr" target="#b27">[28]</ref>. The faces are aligned to a neural reference frame and a bounding box of 96?96 is used to crop the mouth region of interest. Video sequences in the LRW1000 dataset are already cropped so there is no need for pre-processing.</p><p>Training We use the same training parameters as <ref type="bibr" target="#b20">[21]</ref>. The only exception is the use of Adam with decoupled Weight decay (AdamW) <ref type="bibr" target="#b28">[29]</ref> with ? 1 = 0.9, ? 2 = 0.999, = 10 ?8 and a weight decay of 0.01. The network is trained for 80 epochs using an initial learning rate of 0.0003, and a mini-batch of 32. We decay the learning rate using a cosine annealing schedule without warm-up steps. All models are randomly initialised and no external datasets are used.</p><p>Data Augmentation During training, each sequence is flipped horizontally with a probability of 0.5, randomly cropped to a size of 88 ? 88 and mixup <ref type="bibr" target="#b29">[30]</ref> is used with a weight of 0.4. During testing, we use the 88 ? 88 center patch of the image sequence. To improve robustness, we train all models with variable-length augmentation similarly to <ref type="bibr" target="#b20">[21]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Top-1 Acc. (%) ResNet34 + DenseNet52 + ConvLSTM <ref type="bibr" target="#b30">[31]</ref> 36.9 ResNet34 + BLSTM <ref type="bibr" target="#b5">[6]</ref> 38.2 ResNet-18 + BGRU <ref type="bibr" target="#b26">[27]</ref> 38.6 ResNet-18 + MS-TCN <ref type="bibr" target="#b20">[21]</ref> 41.4 ResNet-18 + BGRU + Cutout <ref type="bibr" target="#b26">[27]</ref> 45.  <ref type="table">Table 2</ref>: Comparison with state-of-the-art methods on the LRW-1000 dataset in terms of classification accuracy using the publicly available version of the database (which provides the cropped mouth regions). Each student is trained using the model from the line above as a teacher. ? <ref type="bibr" target="#b26">[27]</ref> uses the full face version of the database, which is not publicly available. Student i stands for the model after the i-th self-distillation iteration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Born-Again Distillation</head><p>In here we show that adding a distillation loss adds valuable inter-class similarity information and in turn helps the optimization procedure. Thus, we resort to the self-distillation strategy (born-again distillation), e.g. <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b21">22]</ref>, where both the teacher and the student networks have the same architecture, as explained in section 2.</p><p>Results on the LRW dataset are shown in <ref type="table">Table.</ref> 1. This strategy leads to a new state-of-the-art performance by 2.6 % margin over the previous one without any computational cost increase. Furthermore, an ensemble of models, a strategy suggested in <ref type="bibr" target="#b13">[14]</ref>, reaches an accuracy of 88.5 %, which further pushes the state-of-the-art performance on LRW.</p><p>Results on the LRW-1000 dataset are shown in <ref type="table">Table 2</ref>. In this case, our proposed best single-model and ensemble result in an absolute improvement of 3.9 % and 5.2 %, respectively, over the state-of-the-art accuracy. We should note that we only compare with works which use the publicly available version of the database. These results confirm that adding inter-class similarity information is useful for lipreading. Efficient Lipreading The frame encoder can be made more efficient by replacing the ResNet-18 with a lightweight Shuf-fleNet v2 (shown in <ref type="figure" target="#fig_2">Fig. 2b)</ref>, as explained in section 2. We should note that we maintain the first convolution of the network as a 3D convolution. Preliminary experiments showed ShuffleNet v2 <ref type="bibr" target="#b16">[17]</ref> yields superior performance over other lightweight architectures like MobileNetV2 <ref type="bibr" target="#b31">[32]</ref>. It can be seen in <ref type="table">Table 3</ref> that this change results in a drop of 0.9 % while reducing both the number of parameters and FLOPs.</p><p>The next step is the replacement of the MS-TCN head with its depthwise-separable variant, noted as DS-MS-TCN. As shown in <ref type="table">Table 3</ref> this variant leads to a model with almost one third of parameters and a 50 % reduction in FLOPs while achieving the same accuracy as the ShuffleNet v2 with  <ref type="table">Table 3</ref>: Performance of different efficient models, ordered in descending computational complexity, and their comparison to the state-of-the-art on the LRW dataset. We use a sequence of 29-frames with a size of 88 by 88 pixels to compute the multiply-add operations (FLOPs). The number of channels is scaled for different capacities, marked as 0.5?, 1?, and 2?. Channel widths are the standard ones for ShuffleNet V2, while base channel width for TCN is 256 channels.</p><p>a MS-TCN head. Models can become even lighter (shown in <ref type="figure" target="#fig_2">Fig. 2c</ref>) by reducing the number of heads to 1, denoted by TCN, and by reducing the width multiplied of the ShuffleNet v2 to 0.5. In the former case, performance drops by 3.4 %, and in the latter by a further 1.9 % resulting in accuracy of 78.1 %. However, it should be noted that the number of parameters and FLOPs is significantly reduced for both models.</p><p>Results for LRW-1000 can be seen in <ref type="table">Table 4</ref>. In this case the use of a ShuffleNet v2 with single TCN head leads to a small drop of 0.7 % compared to the full model while reducing significantly the number of parameters and FLOPs. In addition, the use of DS-TCN results in a further drop of 1.6 %. It is also interesting to note that the use of ShuffleNet v2 with a width multiplier of 0.5 achieves the same performance as the baseline ShuffleNet v2. Sequential Distillation In order to partially bridge this gap, we explore Knowledge Distillation once again. Since now there are higher capacity models that can act as teachers, we do not need to resort to self distillation. We first explored the standard distillation approach in which we take the bestperforming model as the teacher. However, it is known that a wider gap in terms of architecture might mean a less effective transfer <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b32">33]</ref>. Thus, we also explore a sequential distillation approach. More specifically, for lower-capacity networks, we use intermediate-capacity networks to more progressively bridging the architectural gap. For example, for the ShuffleNet v2 (1?)+DS-MS-TCN, we can first train a model using the full fledged ResNet-18+MS-TCN model as teacher, and use the ShuffleNet v2 (1?)+MS-TCN as the student. Then, on the second step, we use the latter model as the teacher, and train our target model, ShuffleNet v2 (1?)+DS-MS-TCN, as the student. This procedure resembles the self-  <ref type="table">Table 4</ref>: Performance of different efficient models on the LRW-1000 dataset. We use a sequence of 29-frame with a size of 112 by 112 to report multiply-add operations (FLOPs). The number of channels is scaled for different capacities, marked as 0.5? and 1?. Channel widths are the standard ones for ShuffleNet v2, while base channel width for TCN is 256 channels. distillation strategy described above in the sense that trains a sequence of teacher-student pairs, where the previous student becomes the teacher in the next iteration. However, unlike that strategy, it progressively changes the architecture from the full-fledged model to the target architecture. The results on the LRW dataset are shown in <ref type="table">Table 3</ref>. Replacing the state-of-the-art ResNet-18+MS-TCN with Shuf-fleNet v2 (1?)+ DS-MS-TCN leads to the same accuracy, after distillation, than the previous state-of-the-art MS-TCN of <ref type="bibr" target="#b20">[21]</ref>, while requiring 8.2? fewer FLOPs and 3.9? fewer parameters. This is a significant finding since the MS-TCN is already quite efficient, having slightly lower computational cost than the lightweight architecture of MobiVSR-1 <ref type="bibr" target="#b11">[12]</ref>. Another interesting combination is the ShuffleNet v2 (0.5?)+ TCN model, which achieves 79.9 % accuracy on LRW with as little as 0.58G FLOPs and 2.9M parameters, a reduction of 17.8? and 12.5? respectively when compared to the ResNet-18+MS-TCN model of <ref type="bibr" target="#b20">[21]</ref>.</p><p>The same pattern is also observed on the LRW1000 dataset, which is shown in <ref type="table">Table 4</ref>. ShuffleNet v2 (0.5?)+DS-TCN (1?) after distillation results in a drop of 1.2% in accuracy, while requiring 22.9? fewer parameters and 18.8? fewer FLOPs than the state-of-the-art-model. <ref type="bibr" target="#b20">[21]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">CONCLUSIONS</head><p>In this work, we present state-of-the-art results on isolated word recognition by knowledge distillation. We also investigate efficient models for visual speech recognition and we achieve results similar to the current state-of-the-art while reducing the computational cost by 8 times. It would be interesting to investigate in future work how cross-modal distillation affects the performance of audiovisual speech recognition models.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>The pipeline of knowledge distillation in generations</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 :</head><label>2</label><figDesc>(a): Base architecture with ResNet18 and multi-scale TCN, (b): Lipreading model with ShuffleNet v2 backbone and multi-scale TCN back-end. (c): Lipreading model with ShuffleNet v2 backbone and depthwise separable TCN backend.</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">The models and code are available at https://sites.google. com/view/audiovisual-speech-recognition</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">End-to-end visual speech recognition with LSTMs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Petridis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2592" to="2596" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Visual-only recognition of normal, whispered and silent speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Petridis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cetin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ICASSP</title>
		<imprint>
			<biblScope unit="page" from="6219" to="6223" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">End-to-end multiview lipreading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Petridis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Lipreading with long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Koutn?k</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<biblScope unit="page" from="6115" to="6119" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">End-to-end audiovisual fusion with LSTMs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Petridis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AVSP</title>
		<imprint>
			<biblScope unit="page" from="36" to="40" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Combining residual networks with LSTMs for lipreading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Stafylakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3652" to="3656" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Large-scale visual speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shillingford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">M</forename><surname>Assael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">W</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Paine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hughes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Prabhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sak</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4135" to="4139" />
		</imprint>
	</monogr>
	<note>Interspeech</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep audio-visual speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Afouras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno>abs/1809.02108</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Lip reading sentences in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3444" to="3453" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Audio-visual speech recognition with a hybrid CTC/attention architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Petridis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Stafylakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in SLT</title>
		<imprint>
			<biblScope unit="page" from="513" to="520" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">MobiLipNet: Resourceefficient deep learning based lipreading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Koumparoulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Potamianos</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2763" to="2767" />
		</imprint>
	</monogr>
	<note>Interspeech</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Mobivsr: A visual speech recognition solution for mobile devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kaur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mahata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Interspeech</title>
		<imprint>
			<biblScope unit="page" from="2753" to="2757" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Workshops</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Born-again neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Furlanello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">C</forename><surname>Lipton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tschannen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Itti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Anandkumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ICML</title>
		<imprint>
			<biblScope unit="page" from="1602" to="1611" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Training deep neural networks in generations: A more tolerant teacher educates better students</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5628" to="5635" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">MobileNets: Efficient convolutional neural networks for mobile vision applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<idno>abs/1704.04861</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">ShufflenetV2: practical guidelines for efficient CNN architecture design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ECCV</title>
		<imprint>
			<biblScope unit="page" from="122" to="138" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Xception: Deep learning with depthwise separable convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chollet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1800" to="1807" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Training binary neural networks with real-to-binary convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bulat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">LRW-1000: A naturally-distributed large-scale benchmark for lip reading in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>FG</publisher>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Lipreading using temporal convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Petridis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<biblScope unit="page" from="6319" to="6323" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Label refinery: Improving ImageNet classification through label progression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bagherinezhad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Horton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rastegari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.02641</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Lip reading in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">10112</biblScope>
			<biblScope unit="page" from="87" to="103" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">End-to-end audiovisual speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Petridis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Stafylakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ICASSP</title>
		<imprint>
			<biblScope unit="page" from="6548" to="6552" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning spatio-temporal features with two-stream deep 3d cnns for lipreading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kitani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMVC</title>
		<imprint>
			<biblScope unit="page">269</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Pushing the boundaries of audiovisual word recognition using residual networks and lstms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Stafylakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Vis. Image Underst</title>
		<imprint>
			<biblScope unit="page" from="22" to="32" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Can we read speech beyond the lips? rethinking RoI selection for deep visual speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>FG</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Dlib-ml: A machine learning toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">E</forename><surname>King</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="1755" to="1758" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Mixup: Beyond empirical risk minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ciss?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lopez-Paz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Multi-grained spatio-temporal modeling for lipreading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMVC</title>
		<imprint>
			<biblScope unit="page">276</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Mobilenetv2: Inverted residuals and linear bottlenecks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in CVPR</title>
		<imprint>
			<biblScope unit="page" from="4510" to="4520" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Search to distill: Pearls are everywhere but not the eyes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vemulapalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="7536" to="7545" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
