<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">MSANet: Multi-Similarity and Attention Guidance for Boosting Few-Shot Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehtesham</forename><surname>Iqbal</surname></persName>
							<email>iqbal.ehtesham@aiv.aisafarov.sirojbek@aiv.aibang.seongdeok@aiv.ai</email>
							<affiliation key="aff0">
								<orgName type="laboratory">AiV Research Group</orgName>
								<address>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sirojbek</forename><surname>Safarov</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">AiV Research Group</orgName>
								<address>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seongdeok</forename><surname>Bang</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">AiV Research Group</orgName>
								<address>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">MSANet: Multi-Similarity and Attention Guidance for Boosting Few-Shot Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T03:57+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Few-shot segmentation aims to segment unseen-class objects given only a handful of densely labeled samples. Prototype learning, where the feature extracted from support images yields a single or several prototypes by averaging global and local object information, has been widely used in FSS. However, utilizing only prototype vectors may be insufficient to represent the features for all support images. To extract abundant features and make more precise predictions, we propose a Multi-Similarity and Attention Network (MSANet) including two novel modules, a multi-similarity module and an attention module. The multi-similarity module exploits multiple feature-maps of support images and query images to estimate accurate semantic relationships. The attention module instructs the MSANet to concentrate on class-relevant information. The network is tested on standard FSS datasets, PASCAL-5 i 1-shot, PASCAL-5 i 5-shot, COCO-20 i 1-shot, and COCO-20 i 5-shot. The MSANet with the backbone of ResNet101 achieves the state-of-the-art performances for all 4-benchmark datasets with mean intersection over union (mIoU) of 69.13%, 73.99%, 51.09%, 56.80%, respectively.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Following the development of well-established largescale datasets <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b25">26]</ref>, a series of supervised convolutional neural networks (CNNs) have shown great potential for semantic segmentation tasks <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b48">49]</ref>. The performance of these supervised CNNs is highly dependent on the quality and quantity of training datasets such as the numbers of well-annotated data, the balance of class distri- <ref type="figure">Figure 1</ref>. Comparison a meta learner between the existing network and the MSANet. The main difference is that the former uses only class-representative prototype vectors, while the MSANet includes the multi-similarity module for visual correspondences and an attention module for target category focus. The rest of the network is the same as the architecture of BAM <ref type="bibr" target="#b19">[20]</ref> . bution, and sample representation. However, in real-world applications, it is difficult to secure a lot of annotated data, especially in dense prediction tasks <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b58">59]</ref>. Moreover, traditional supervised CNNs may struggle with generalization capability on the images with unseen classes.</p><p>Inspired by the human cognitive ability to distinguish objects with only a few input data, a few-shot learning (FSL) technique is developed <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b55">56]</ref>. This technique builds a network that can be generalized to unseen domains with few available annotated samples. Few-shot segmentation (FSS) <ref type="bibr" target="#b26">[27]</ref><ref type="bibr" target="#b27">[28]</ref><ref type="bibr" target="#b28">[29]</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b59">60,</ref><ref type="bibr" target="#b60">61,</ref><ref type="bibr" target="#b62">63,</ref><ref type="bibr" target="#b63">64]</ref> is one of the application of few-shot learning, especially focused on semantic segmentation. The goal of FSS is to segment the targeted region of the selected category in the query image with their corresponding annotated masks.</p><p>The most prevalent approach of FSS is metric-based prototype learning <ref type="bibr" target="#b50">[51]</ref>. Referring to the upper part of <ref type="figure">Fig. 1)</ref>, a single or multiple class representative prototype vector is generated by the masked average pooling (MAP) <ref type="bibr" target="#b66">[67]</ref>. A feature processing network segments the target object in the query image leveraging class representative prototype vectors. Many researchers have tried to get more guidance from prototype vectors adopting different mechanism, for example, PANet <ref type="bibr" target="#b54">[55]</ref>, PFENet <ref type="bibr" target="#b50">[51]</ref>, SG-One Net <ref type="bibr" target="#b66">[67]</ref>, CANet <ref type="bibr" target="#b64">[65]</ref>, ASGNet <ref type="bibr" target="#b21">[22]</ref>. However, such prototypical networks can lose detailed spatial information of an image due to masked average pooling operation. In this context, we propose a Multi-Similarity and Attention Network (MSANet) consisting of two guiding modules. Referring to the lower part of <ref type="figure">Fig. 1</ref>, the network includes a multi-layer similarity module and an attention module. It is expected that two modules will support prototype learning paradigms and guide the MSANet to fine segmentation.</p><p>Recent works have represented that FSS networks can be upgraded by utilizing visual correspondences <ref type="bibr" target="#b11">[12]</ref> of support images and query images. To establish a more meaningful correspondence, dense intermediate layers <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b37">38]</ref> and correlation tensor learning <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b51">52]</ref> techniques are adopted. Juhong Min et al. designed HSNet <ref type="bibr" target="#b35">[36]</ref> that suggested a hyper-correlation squeeze network with the multi-layer dense feature correlation-based on 4D tensors. In addition to this, we propose a multi-similarity module that extracts multi-layer feature correlation from a backbone network and applies a simple convolution block to the feature. We also propose a lightweight CNN attention block for paying more attention to the target class content of an image. Following the architecture of BAM <ref type="bibr" target="#b19">[20]</ref>, we employ a base learner and an ensemble module to refine the segmentation results. We summarize our primary contribution to the FSS challenge as follows:</p><p>? We propose a multi-layer similarity module to get an informative visual correspondence between a support image and a query image.</p><p>? We propose a simple but effective attention module leveraging support images and their corresponding masks to better understand the class-relevant information.</p><p>? The MSANet outperforms existing FSS networks and shows the state-of-the-art (SOTA) results on PASCAL-5 i <ref type="bibr" target="#b44">[45]</ref> and COCO-20 i <ref type="bibr" target="#b38">[39]</ref> FSS benchmarks under 1shot and 5-shot settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Semantic Segmentation: Semantic segmentation is one of the computer vision tasks to classify each pixel on a given image within specified categories <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b48">49]</ref>. Thanks to advances in fully convolutional networks (FCNs) <ref type="bibr" target="#b33">[34]</ref>, many model structures such as encoder-decoder-based UNet <ref type="bibr" target="#b43">[44]</ref>, Pyramid Pooling Module (PPM) based PSP-Net <ref type="bibr" target="#b67">[68]</ref> and an Atrous Spatial Pyramid Pooling (ASPP) based deeplab <ref type="bibr" target="#b4">[5]</ref> have been proposed for improving segmentation performance. Moreover, a series of vision techniques are suggested, including dilated convolution <ref type="bibr" target="#b61">[62]</ref>, multi-level feature aggregation <ref type="bibr" target="#b24">[25]</ref> and attention mechanism <ref type="bibr" target="#b17">[18]</ref>. However, conventional segmentation models require a sufficient amount of annotated data and are difficult to predict unseen categories without fine-tuning, thus hindering practical application to some extent.</p><p>Few-shot Learning: To tackle these issues, FSL is introduced with the aim of understanding unseen categories with only a few annotated samples. FSL approaches can be further subdivided into three branches: (i) optimization-based <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b41">42]</ref>, (ii) augmentation-based <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7]</ref>, and (iii) metricbased <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b49">50]</ref>. The optimization-based methods suggest gradient update strategies to overcome data bias and improve the generalization of the model. The augmentationbased methods address the lack of data by generating synthetic training images. Our work is closely related to the metric-based methods that aim to learn a general metric function to compute the distances between a query image and a support image. There have been outstanding advancements in these metric-based methods. As one of them, matching networks <ref type="bibr" target="#b52">[53]</ref> utilize a special kind of minibatches called episodes to match training and testing environments. Relation networks <ref type="bibr" target="#b49">[50]</ref> convert query and support images to 1x1 vectors and then perform classification based on the Cosine Similarity (CS). Furthermore, prototypical networks <ref type="bibr" target="#b47">[48]</ref>, which directly leverage the feature representations (i.e., prototypes) computed through global average pooling operation, are proposed.</p><p>Few-shot Segmentation: Shaban, et al. <ref type="bibr" target="#b44">[45]</ref> proposed OSLSM, one of the pioneering works of FSS, to generate classifier weights for query image segmentation. The first branch took support images as input and produced a vector of parameters, and the second branch took these parameters as well as query images and generated a segmentation mask as an output. Afterward, the prototype learning paradigm <ref type="bibr" target="#b47">[48]</ref> was introduced for better information extraction from a support image and a query image. SG-One <ref type="bibr" target="#b66">[67]</ref> introduced masked average pooling operation for computing class representative prototype vectors, yielding the spatial similarity map. CANet <ref type="bibr" target="#b64">[65]</ref> proposed two dense comparison networks with an iterative refine module. PFENet <ref type="bibr" target="#b50">[51]</ref> calculated the CS on high-level features without trainable parameters to create a prior mask and in-troduced a feature enrichment module. Instead of prototype expansions, ASGNet <ref type="bibr" target="#b21">[22]</ref> offered a superpixel-guided clustering approach to extract multiple prototypes from the support image, and used an allocation strategy to reconstruct the support feature-map. However, most of the prototype learning methods can lead to spatial structural loss. To fully exploit the features of foreground objects, there is room for improvement in using the class representative prototype vectors. On the other hand, finding visual correspondences and processing correlation tensors show prominent results in FSS <ref type="bibr" target="#b35">[36]</ref><ref type="bibr" target="#b36">[37]</ref><ref type="bibr" target="#b37">[38]</ref>. HSNet <ref type="bibr" target="#b35">[36]</ref> was trained to squeeze a dense feature correlation tensor and transform it into a segmentation mask via high-dimensional convolutions. However, high-dimensional convolutions (4D convolutions) have high spatial and time complexity. To extract a lightweight CNN feature, DENet <ref type="bibr" target="#b29">[30]</ref> introduced a guided attention module to estimate the weights of novel classifiers inspired by traditional attention mechanisms. Tao Hu et al. <ref type="bibr" target="#b16">[17]</ref> proposed an attention-based multi-context guiding network that fuses small-to-large scale context information to guide query branches globally. Instead of working on feature extraction or visual correspondences, BAM <ref type="bibr" target="#b19">[20]</ref> introduced a new way for FSS, which uses an extra block of the supervised model trained on base classes. The supervised model predicts the base classes from the query image and helps the meta learner to suppress false predictions. Motivated by recent advances in a visual correspondence and an attention mechanism, we propose a multi-layer similarity module and a lightweight attention module in the context of prototypical networks to take FSS networks to the next level.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Problem description</head><p>FSS aims to train a model with base classes and segment novel classes from query images with a few annotated support samples. Current approaches typically train FSS models called a meta learner within a meta-learning paradigm, known as episodic training <ref type="bibr" target="#b52">[53]</ref>. Given two image sets D train (base classes) and D test (novel classes), the models are expected to learn transferable knowledge on D train (base classes) with sufficient annotated samples. They have exhibited good generalization capability on D test (novel classes) with a very few annotated examples. In particular, both sets are composed of numerous episodes, each with a small support set S = {(x s(i) , m s(i) )} k i=1 and a query set Q = {(x q , m q )}, where x * and m * represent a raw image and its corresponding binary mask for a specific category, respectively. The models are optimized during each training episode to make predictions on the query image x q under the condition of the support set S. Once the training is complete, we will evaluate the performance on D test across all the test episodes, without further optimization. Like the BAM <ref type="bibr" target="#b19">[20]</ref>, we follow the same traditional supervised training method for a base leaner network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Proposed Method</head><p>We propose two guiding modules, the multi-similarity module and the attention module. The former module finds a visual correspondence between the support image and query image, while the latter instructs the FSS network to focus more on the targeted objects of the query image. Taking advantage of a visual correspondence and an attention mechanism, we assist the prototypical network to get more accurate segmentation results.</p><p>Model Architecture: <ref type="figure" target="#fig_0">Fig. 2</ref> shows the architecture of the MSANet. First, the features of the query image and the support image are extracted from a pre-trained backbone network. The support features extracted from block 2 and 3 and their corresponding masks are utilized to find a class representative prototype vector V s . These features and their mask are fed to the attention module for finding the attention feature-map. The attention module first masks the support feature and then uses a simple convolutional network to produce a foreground-focused attention feature-map. The query feature and support feature generated from block 4 are utilized to generate a prior mask M pr following <ref type="bibr" target="#b50">[51]</ref>. At the same time, all features of the query image and the support image extracted from block 2, 3, and 4 are exploited to generate visual correspondences by leveraging the multisimilarity module. In the module, the CS distances between multi-layers query features and support features are calculated, and simple 1 ? 1 Conv is applied to the features. Details for this module are mentioned in Sec. 4.1. The generated visual correspondence, attention map, prior mask, and prototype vector along with query features are fed to the feature enrichment ASPP module. To focus on the approximate information of features, the dilated version of the ASPP module is utilized. After obtaining rich features from the ASPP module, a simple convolution block is used for feature processing. The classifier head consisting of 3 ? 3 Conv and 1 ? 1 Conv is utilized to produce a binary meta prediction mask. The structure of the convolution block and the classifier head is illustrated in <ref type="figure" target="#fig_1">Fig. 3</ref>. Finally, the output of the meta learner is refined with a base learner 1 using an ensemble module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Multi-Similarity Module</head><p>In this module, a pair of query image (I q ) and support image (I s ), such as (I q , I s ) ? R 3?H?W , are input to the backbone network 2 . The backbone network pretrained with base classes is frozen during the training process for generalization on unseen categories. To compute the visual correspondence, the last three blocks of the backbone network  remain the same spatial size. We extract the last three block feature-maps of the query image asF Q using Eq. (1) and the support image asF S with dimension of R C b ?H ?W using Eq. (2), where C b represents channel size according to bottleneck b and represents an image size, respectively.</p><formula xml:id="formula_0">F Q = {(F bn,b q ) B N bn=0 } B b=2 (1) F S = {(F bn,b s ) B N bn=0 } B b=2 (2)</formula><p>Here, B represents the block number and B N represents the bottleneck of B block , respectively. For instance, F 1,2 q represents the query feature extracted from the first bottleneck of block 2. Each support feature-map F bn,b s is masked with the bi-linear interpolated corresponding mask M s ? {0, 1} H?W using Eq. (3) to suppress the activation of background region. By masking the support feature-map, the query feature only correlates with the foreground region of the support image.</p><formula xml:id="formula_1">F bn,b ms = F bn,b s ? (M s )<label>(3)</label></formula><p>Here, ? (?) represents the bi-linear interpolation function that interpolates the support mask M s ? {0, 1} H?W according to the spatial dimension followed by the expansion along channel wise such as ? : R H?W ? R C b ?H ?W , and represents the Hadamard product. To escape from the over-fitting and to reduce the computation cost, we squeeze the masked support feature-maps F bn,b ms (Eq. (4)) by filtering the mean pixel values such that their dimensions reduce from</p><formula xml:id="formula_2">R C b ?H W ? R C b ?N , where N H W .</formula><p>The squeezing equation is as follow.</p><formula xml:id="formula_3">F bn,b,c ms = F bn,b ms if, [F bn,b ms &gt; c]<label>(4)</label></formula><p>Here, c is the mean value of F bn,b ms . To generate a visual correspondence, we first compute pixel-wise cosine distance between squeeze feature-map of the support image F bn,b,c ms and the extracted feature-map of the query image F bn,b q , following Eq. <ref type="bibr" target="#b4">(5)</ref>.</p><formula xml:id="formula_4">CS(x q , x s ) = mean ?{ x T q ? x s x q x s } q ? (1, 2, ...H W ), s ? (1, 2, ...N )<label>(5)</label></formula><p>Here, x q ? F bn,b q , x s ? F bn,b,c ms , ? represents the ReLU function used for the normalization of CS distance tensor and N represents the number of element in F bn,b,c ms , respectively. In Eq. (5), for the first value of q, we estimate a cosine distance vector utilizing all values of N and find its mean value to get a single value CS. This computation process repeats for all the values of q to generate a CS map, CS(x q , x s ) ? R H ?W , as shown in <ref type="figure" target="#fig_2">Fig. 4</ref>. The CS map represents the accurate visual correspondence of a single query feature-map with a single support feature-map. The same procedure proceeds for all the extracted feature layers of the query image and the support image to obtain multilayer visual correspondences using Eq. <ref type="bibr" target="#b5">(6)</ref>.</p><formula xml:id="formula_5">CS ml (x q , x s ) = {CS(x q , x s )} L l=1<label>(6)</label></formula><p>Here, L is the order number of feature-maps extracted from the backbone network <ref type="bibr" target="#b2">3</ref> . After finding multi-layer CS, we concatenate them and pass through 1 ? 1 Conv such as R C L ?H ?W ? R C ? ?H ?W . We choose ? =64, the number of filters for 1 ? 1 Conv.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Attention Module</head><p>In view of the limited number of data provided by novel classes, the information on novel classes may be suppressed by the base classes. To address this issue, we propose a lightweight attention module, which extracts the classrelevant information from the few support samples and directs the network to focus on the targeted region, as shown in <ref type="figure" target="#fig_0">Fig. 2</ref>. We first extract an intermediate feature-map of the support image and the query image from a backbone network, concatenate them, and apply 1 ? 1 Conv for dimensionality reduction according to Eq. <ref type="formula" target="#formula_6">(7)</ref>.</p><formula xml:id="formula_6">F 23 s = C 1?1 {F 2 s c F 3 s }<label>(7)</label></formula><p>Here, F 2 s , F 3 s represent support feature-maps of block 2 and block 3, respectively. These features along with support mask M s are utilized to get the attention vector using Eq. <ref type="bibr" target="#b7">(8)</ref>.</p><formula xml:id="formula_7">V a = ?(C N (P (F 23 s ?(M s ))))<label>(8)</label></formula><p>Here, P represents pooling operation, C N is a convolutional network and ? is an activation function, respectively. Fi-3 L=7,13,30 for VGG16, ResNet50 and ResNet101, respectively nally, a class representative attention feature-map is generated by exploiting the attention vector (V ? ) (Eq. <ref type="formula" target="#formula_8">(9)</ref>).</p><formula xml:id="formula_8">A s = F 23 s V a ,<label>(9)</label></formula><p>ASPP and Classifier: After finding a visual corresponding through the multi-similarity module and an attention feature-map from the attention module, we concatenate them with a prior mask, a class representative prototype vector and intermediate query feature-map. These concatenated features are proceeded through the ASPP module, where a dilated convolution is used for feature enhancement, as shown in <ref type="figure" target="#fig_0">Fig. 2</ref>. Finally, we apply the convolution block followed by a classifier to the final prediction mask p m .</p><formula xml:id="formula_9">p m = Sof tmax(D m (CS ml , A s , M pr , V s , F 23 q )) (10)</formula><p>Here, CS ml , A s , M pr , V s represent multi-layer similarity, attention features, prior mask, and prototype vector, respectively. F 23 q shows the concatenated query features extracted from block 2 and 3 of backbone network. D m collectively refers to the ASPP, convolution block and classifier.</p><p>Training Loss: The model is trained using a binary cross entropy (BCE) loss. The BCE loss between prediction mask p m of the query image and its corresponding ground truth mask m q is calculated.</p><formula xml:id="formula_10">L m = 1 ep ep i=1 BCE(p m(i) , m q(i) ),<label>(11)</label></formula><p>Here, ep is the total number of training episodes in each batch. Following the BAM, we also utilize the base leaner loss and the ensemble module loss for end-to-end training. K-shot Segmentation: In the K-shot (K &gt; 1) setting, there are more than one annotated support image. Different approaches have been proposed for K-shot segmentation. Prototype-based networks <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b66">67]</ref> mostly took average of the K class representative prototype vectors and then utilized the averaged features to guide the subsequent segmentation process. Whereas, the visual correspondences-based models <ref type="bibr" target="#b35">[36]</ref> performed K time forward pass and got prediction mask using threshold-based method. In this work, for K-shot segmentation, we perform K forward pass and compute K time CS {CS(x q , x s )} L l=1 , and then the generated K time CS along layer-wise is averaged. Afterwards, the mean CS {CS(x q , x s )} L l=1 is propagated to the ASPP module. We take the average of K times generated A s ,V s and M pr , respectively. Finally, we utilize the adjustment factor with two fully-connected layers following <ref type="bibr" target="#b19">[20]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Implementation Setup</head><p>In this section, three backbone networks 4 are used for PASCAL-5 i <ref type="bibr" target="#b44">[45]</ref> dataset and two backbone networks 5 are used for COCO-20 i <ref type="bibr" target="#b38">[39]</ref>. We adopted two-way training <ref type="bibr" target="#b19">[20]</ref>, where the base learner is trained using the supervised protocol. The meta-learner is trained using the traditional episodic training paradigm <ref type="bibr" target="#b47">[48]</ref>. We use the same base-learner as in BAM and fix the parameters during meta learner training. Here, we employ the stochastic gradient descent optimizer with learning rate 5e-2 for 200 epochs on PASCAL-5 i and 50 epochs on COCO-20 i , respectively. In both datasets, the batch size is set to 8, and the data augmentation techniques described in <ref type="bibr" target="#b50">[51]</ref> are applied. To limit the impact of selected support-query image pairs on performance, we calculate the average results of 5 runs with varied random seeds. The training of the MSANet is implemented in the PyTorch environment, running on the NVIDIA A100 40GB server.</p><p>Benchmark Dataset: We evaluate the performance of the MSANet on standard benchmark datasets, PASCAL-5 i and COCO-20 i . PASCAL-5 i consists of 20 object classes generated from PASCAL VOC 2012 <ref type="bibr" target="#b9">[10]</ref> with additional annotations from SDS <ref type="bibr" target="#b12">[13]</ref>. COCO-20 i consists of 80 object classes compiled from MSCOCO <ref type="bibr" target="#b25">[26]</ref>. The object categories are equally distributed into 4-folds such as {5 i : i ? {0, 1, 2, 3}} for PASCAL-5 i , {20 i : i ? {0, 1, 2, 3}} for COCO-20 i , respectively. Models are trained on 3 folds and tested on the remaining one fold based on a cross-validation protocol. The validation fold consists of 1000 random pairs of support images and query images.</p><p>Evaluation Metric We employ mean intersection overunion (mIoU) and foreground-background IoU (FBIoU) as the assessment metrics, following prior FSS approaches <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b64">65]</ref>.</p><formula xml:id="formula_11">mIoU = 1 C C c=1</formula><p>IoU c (12) In Eq. (12), C and IoU c represent total classes in the targeted fold and the intersection over union of class c, respectively. In Eq. (13), IoU f and IoU b represent foreground and background intersection over union values in the targeted fold, respectively.</p><formula xml:id="formula_12">F B IoU = 1 2 (IoU f + IoU b )<label>(13)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Result Analysis</head><p>We compare the performance of the MSANet with the other FSS networks using PASCAL-5 i and COCO-20 i datasets. The experiments are conducted with different backbone networks in 1-shot and 5-shot scenarios. The performances of the MSANet are verified in both quantitative and qualitative paradigms.</p><p>Quantitative Results: Tab. 1 and Tab. 2 illustrate the performances of the MSANet along with other FSS approaches. In both FSS dataset benchmarks, PASCAL-5 i and COCO-20 i , the MSANet outperforms all prior FSS networks under 1-shot and 5-shot settings in term of mIoU and F B IoU . Compared to SOTA <ref type="bibr" target="#b19">[20]</ref>, for PASCAL-5 i benchmark, in 1-shot setting, the MSANet with VGG16, ResNet50, and ResNet101 backbones show performance improvements of 1.35%, 0.71%, and 1.63%, respectively, and in 5-shot setting, of 1.64%, 1.69%, and 2.39%, respectively. For COCO-20 i benchmark, the networks with ResNet50 and ResNet101 backbones outperform with high margin such as 1.8% and 2.5% (1-shot) and 9.89% and 7.3% (5-shot), respectively.</p><p>Qualitative Results: <ref type="figure" target="#fig_3">Fig. 5</ref> presents the examples of the prediction results of the MSANet under 1-shot setting for PASCAL-5 i and COCO-20 i . In the figure, first two columns, third column, and the forth column represent the examples of support images and the query images, the output of the meta part for the MSANet, and the output of the MSANet, respectively. As shown in <ref type="figure" target="#fig_3">Fig. 5</ref>, it is found that the predicted results of the MSANet are almost identical to the ground truth in pixel wise segmentation, which demonstrate the performance of the MSANet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Ablation Tests</head><p>We undertake a series of ablation tests using ResNet101 backbone on PASCAL-5 i under 1-shot setting. This test can evaluate the impact of each component on segmentation performance and verify its effectiveness.</p><p>Performance of Module: Tab. 3 shows the effectiveness of each module in the MSANet through the ablation tests. Compared to the performance of the MSANet, the network without the multi-similarity, the attention, prototype, and prior mask module descends it to 1.66%, 0.63%, 0.1%, and 0.42%, respectively. These results demonstrate that two proposed modules, multi-similarity and attention, have more impact on performance improvement than the previous FSS prototype approaches (prior mask, prototype vector). The fifth row of Tab. 3 shows that the network with    <ref type="table">Table 3</ref>. The result of the ablation study. The meta mIoU represents the prediction of the MSANet without the base learner and the ensemble module.</p><p>the impact of each feature layer in computing similarity correlation, we experiment with different blocks of backbone networks. In the MSANet, multi-similarity correlations are computed using the three blocks from the backbone. <ref type="figure" target="#fig_4">Fig. 6</ref> exhibits the visualization of multi-similarity correlation according to different blocks with an energy map representing the average value of all similarities in one block. The correlation with low-level features holds the detailed informa-tion but lacks the objectness. On the contrary, the images with high-level features can understand the approximate information but loses the details such as edges. Accordingly, low-level (block 2), mid-level (block 3), and high-level features (block 4) are used for the computation of semantic similarity to obtain diverse context information about target objects. We figure out that leveraging visual correspondence by combining multiple feature layers of a backbone network can provide more guidance in segmenting target objects. Failure Case Study: We visualize the failure cases of the MSANet in <ref type="figure" target="#fig_5">Fig. 7</ref>. The predicted results of the MSANet are sometimes unclear and discontinuous, possibly due to the model's failure to obtain accurate clues from the support images. These issues similarly appear in few-shot semantic segmentation tasks, and are still one of the challenges in the computer vision field. The results in <ref type="figure" target="#fig_5">Fig. 7</ref> imply that failure cases may be proportional to the complexity of a pair of support and query image. In addition, input pairs that are relatively lacking in visual representation can result in  inaccurate segmentation masks. These difficulties in FSS can suggest future work directions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this paper, we propose the MSANet for few-shot image segmentation. Two new modules, named multisimilarity and attention, are introduced to the FSS to overcome the shortcomings of existing prototype-based models. The first module exploits the multiple feature-maps of the support images and the query images to generate an informative visual correspondence between them. The second module helps the MSANet to concentrate more on classrelevant information. Extensive experiments and ablation studies prove the effectiveness of the proposed network. We success to achieve the SOTA performances for 4-benchmark datasets, PASCAL-5 i and COCO-20 i datasets under 1-shot and 5-shot settings, respectively.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Meta Learner Architecture: Detailed visualization of the meta network for MSANet consisting of the multi-similarity module, the attention module, and the feature processing in the ASPP.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>The structure for conv block and classification head.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>The process of computing a visual correspondence.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>The examples of the prediction results for the MSANet on PASCAL-5 i and COCO-20 i under 1-shot setting. The support images with ground-truth masks (blue), the query images with GT masks (green), the meta results (red), and the final results (red) are represented in each row, from left to right. The column of the meta output represents the prediction results of the MSANet without the base learner and the ensemble module.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 .</head><label>6</label><figDesc>Visualization of multi-layer similarity correlation from different blocks. CorrB2, CorrB3 and CorrB4 represent the multisimilarity from block 2, block 3, and block 4 of the backbone network, respectively</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 .</head><label>7</label><figDesc>Visualization of failure cases.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .Table 2 .</head><label>12</label><figDesc>Comparison of the MSANet with other FSS networks on PASCAL-5 i under 1-shot and 5-shot settings. The results with underlined denote the second best and with bold shows best performance. The row of the meta learner represents the prediction result for the MSANet without the base learner and the ensemble module. Comparison of the MSANet with other FSS networks on COCO-20 i under 1-shot and 5-shot settings. The results with underlined denote the second best and with bold shows best performance. The row of the meta learner represents the prediction result for the MSANet without the base learner and the ensemble module.only two modules achieves 68.87%, which is higher than all previous FSS performance shown in Tab. 1. Referring to the final row of Tab. 3, the combination of the two modules and the previous FSS prototype modules leads to the MSANet accomplishing the highest performance. The table also shows that the base learner and the ensemble modules play a significant role in the MSANet.</figDesc><table><row><cell>Backbone</cell><cell></cell><cell>Method</cell><cell cols="12">1-shot Fold-0 Fold-1 Fold-2 Fold-3 MIoU% FB-IoU% Fold-0 Fold-1 Fold-2 Fold-3 MIoU% FB-IoU% 5-shot</cell></row><row><cell></cell><cell cols="2">SG-One (TCYB-19) [67]</cell><cell>40.20</cell><cell cols="2">58.40</cell><cell cols="2">48.40</cell><cell>38.40</cell><cell>46.30</cell><cell>-</cell><cell>41.9</cell><cell>58.60</cell><cell>48.60</cell><cell>39.40</cell><cell>47.10</cell><cell>-</cell></row><row><cell></cell><cell cols="2">PANet (ICCV-19) [55]</cell><cell>42.30</cell><cell cols="2">58.00</cell><cell cols="2">51.10</cell><cell>41.20</cell><cell>48.10</cell><cell>-</cell><cell>51.80</cell><cell>64.60</cell><cell>59.80</cell><cell>46.50</cell><cell>55.70</cell><cell>-</cell></row><row><cell></cell><cell cols="2">FWB (ICCV-19) [39]</cell><cell>47.00</cell><cell cols="2">59.60</cell><cell cols="2">52.60</cell><cell>48.30</cell><cell>51.90</cell><cell>-</cell><cell>50.90</cell><cell>62.90</cell><cell>56.50</cell><cell>50.10</cell><cell>55.10</cell><cell>-</cell></row><row><cell></cell><cell cols="2">CRNet (CVPR-20) [31]</cell><cell>-</cell><cell>-</cell><cell></cell><cell>-</cell><cell></cell><cell>-</cell><cell>55.20</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>58.50</cell><cell>-</cell></row><row><cell>VGG16</cell><cell cols="2">PFENet (TPAMI-20) [51]</cell><cell>56.9</cell><cell cols="2">68.2</cell><cell cols="2">54.40</cell><cell>52.40</cell><cell>58.00</cell><cell>72.00</cell><cell>59.00</cell><cell>69.10</cell><cell>54.80</cell><cell>52.90</cell><cell>59.00</cell><cell>72.3</cell></row><row><cell></cell><cell cols="2">HSNet (ICCV-21) [36]</cell><cell>59.6</cell><cell cols="2">65.7</cell><cell cols="2">59.60</cell><cell>54.00</cell><cell>59.70</cell><cell>73.40</cell><cell>64.90</cell><cell>69.00</cell><cell>64.10</cell><cell>58.60</cell><cell>64.10</cell><cell>76.60</cell></row><row><cell></cell><cell cols="2">BAM(CVPR-22) [20]</cell><cell>63.18</cell><cell cols="2">70.77</cell><cell cols="2">66.14</cell><cell>57.53</cell><cell>64.41</cell><cell>77.26</cell><cell>67.36</cell><cell>73.05</cell><cell>70.61</cell><cell>64.00</cell><cell>68.76</cell><cell>81.10</cell></row><row><cell></cell><cell cols="2">Meta Learner</cell><cell>60.92</cell><cell cols="2">70.00</cell><cell cols="2">65.82</cell><cell>57.39</cell><cell>63.53</cell><cell>74.61</cell><cell>66.82</cell><cell>72.05</cell><cell>72.41</cell><cell>63.90</cell><cell>68.80</cell><cell>79.62</cell></row><row><cell></cell><cell></cell><cell>Final</cell><cell>64.87</cell><cell cols="2">71.47</cell><cell cols="2">67.40</cell><cell>59.33</cell><cell>65.76</cell><cell>78.01</cell><cell>69.33</cell><cell>73.51</cell><cell>73.59</cell><cell>65.18</cell><cell>70.40</cell><cell>80.50</cell></row><row><cell></cell><cell cols="2">PANet(ICCV-19) [55]</cell><cell>44.00</cell><cell cols="2">57.50</cell><cell cols="2">50.8</cell><cell>44.0</cell><cell>49.10</cell><cell>-</cell><cell>55.30</cell><cell>67.20</cell><cell>61.30</cell><cell>53.20</cell><cell>59.30</cell><cell>-</cell></row><row><cell></cell><cell cols="2">CANet (ICCV-19) [65]</cell><cell>52.50</cell><cell cols="2">65.90</cell><cell cols="2">51.30</cell><cell>51.90</cell><cell>55.40</cell><cell>-</cell><cell>55.50</cell><cell>67.80</cell><cell>51.90</cell><cell>53.20</cell><cell>57.10</cell><cell>-</cell></row><row><cell></cell><cell cols="2">PGNet (ICCV-19) [64]</cell><cell>56.00</cell><cell cols="2">66.90</cell><cell cols="2">50.60</cell><cell>50.40</cell><cell>56.00</cell><cell>69.90</cell><cell>57.70</cell><cell>68.70</cell><cell>52.90</cell><cell>54.60</cell><cell>58.50</cell><cell>70.50</cell></row><row><cell></cell><cell cols="2">CRNet (CVPR-20) [31]</cell><cell>-</cell><cell>-</cell><cell></cell><cell>-</cell><cell></cell><cell>-</cell><cell>55.70</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>58.80</cell><cell>-</cell></row><row><cell></cell><cell cols="2">PPNet (ECCV-20) [32]</cell><cell>48.58</cell><cell cols="2">60.58</cell><cell cols="2">55.71</cell><cell>46.47</cell><cell>52.84</cell><cell>69.19</cell><cell>58.85</cell><cell>68.28</cell><cell>66.77</cell><cell>57.98</cell><cell>62.97</cell><cell>75.76</cell></row><row><cell>ResNet50</cell><cell cols="2">PFENet (TPAMI-20) [51]</cell><cell>61.70</cell><cell cols="2">69.50</cell><cell cols="2">55.40</cell><cell>56.30</cell><cell>60.80</cell><cell>73.30</cell><cell>63.10</cell><cell>70.70</cell><cell>55.80</cell><cell>57.90</cell><cell>61.90</cell><cell>73.90</cell></row><row><cell></cell><cell cols="2">HSNet (ICCV-21) [36]</cell><cell>64.30</cell><cell cols="2">70.70</cell><cell cols="2">60.30</cell><cell>60.50</cell><cell>64.00</cell><cell>76.70</cell><cell>70.30</cell><cell>73.20</cell><cell>67.40</cell><cell>67.10</cell><cell>69.50</cell><cell>80.60</cell></row><row><cell></cell><cell cols="2">VAT (arXiv-21) [16]</cell><cell>67.60</cell><cell cols="2">71.20</cell><cell cols="2">62.30</cell><cell>60.10</cell><cell>65.30</cell><cell>77.40</cell><cell>72.40</cell><cell>73.60</cell><cell>68.60</cell><cell>65.70</cell><cell>70.00</cell><cell>80.90</cell></row><row><cell></cell><cell cols="2">BAM (CVPR-22) [20]</cell><cell>68.97</cell><cell cols="2">73.59</cell><cell cols="2">67.55</cell><cell>61.13</cell><cell>67.81</cell><cell>79.71</cell><cell>70.59</cell><cell>75.05</cell><cell>70.79</cell><cell>67.20</cell><cell>70.91</cell><cell>82.18</cell></row><row><cell></cell><cell cols="2">Meta Learner</cell><cell>63.35</cell><cell cols="2">70.77</cell><cell cols="2">65.25</cell><cell>59.53</cell><cell>64.73</cell><cell>75.97</cell><cell>70.14</cell><cell>74.99</cell><cell>71.39</cell><cell>66.64</cell><cell>70.79</cell><cell>81.09</cell></row><row><cell></cell><cell></cell><cell>Final</cell><cell>69.25</cell><cell cols="2">74.60</cell><cell cols="2">67.84</cell><cell>62.40</cell><cell>68.52</cell><cell>80.44</cell><cell>72.70</cell><cell>76.26</cell><cell>73.52</cell><cell>67.94</cell><cell>72.60</cell><cell>83.23</cell></row><row><cell></cell><cell cols="2">FWB (ICCV-19) [39]</cell><cell>51.30</cell><cell cols="2">64.50</cell><cell cols="2">56.70</cell><cell>52.20</cell><cell>56.20</cell><cell>-</cell><cell>54.80</cell><cell>67.40</cell><cell>62.20</cell><cell>55.30</cell><cell>59.90</cell><cell>-</cell></row><row><cell></cell><cell cols="2">PPNet (ECCV-20) [32]</cell><cell>52.70</cell><cell cols="2">62.80</cell><cell cols="2">57.40</cell><cell>47.70</cell><cell>55.20</cell><cell>70.90</cell><cell>60.30</cell><cell>70.00</cell><cell>69.40</cell><cell>60.70</cell><cell>65.1</cell><cell>77.5</cell></row><row><cell></cell><cell cols="2">DAN (ECCV-20) [54]</cell><cell>54.70</cell><cell cols="2">68.60</cell><cell cols="2">57.80</cell><cell>51.60</cell><cell>58.20</cell><cell>71.90</cell><cell>57.90</cell><cell>69.00</cell><cell>60.10</cell><cell>54.90</cell><cell>60.50</cell><cell>72.30</cell></row><row><cell></cell><cell cols="2">RePRI (CVPR-21) [4]</cell><cell>59.60</cell><cell cols="2">68.60</cell><cell cols="2">62.20</cell><cell>47.20</cell><cell>59.40</cell><cell>-</cell><cell>66.20</cell><cell>71.40</cell><cell>67.00</cell><cell>57.70</cell><cell>65.60</cell><cell>-</cell></row><row><cell>ResNet101</cell><cell cols="2">PFENet (TPAMI'20) [51] HSNet (ICCV'21) [36]</cell><cell>60.50 67.30</cell><cell cols="2">69.40 72.30</cell><cell cols="2">54.40 62.00</cell><cell>55.90 63.10</cell><cell>60.10 66.20</cell><cell>72.90 77.60</cell><cell>62.80 71.80</cell><cell>70.40 74.40</cell><cell>54.90 67.00</cell><cell>57.60 68.30</cell><cell>61.40 70.40</cell><cell>73.50 80.60</cell></row><row><cell></cell><cell cols="2">CyCTR (NIPs-21) [66]</cell><cell>69.30</cell><cell cols="2">72.70</cell><cell cols="2">56.50</cell><cell>58.60</cell><cell>64.30</cell><cell>72.90</cell><cell>73.50</cell><cell>74.00</cell><cell>58.60</cell><cell>60.20</cell><cell>66.60</cell><cell>75.00</cell></row><row><cell></cell><cell cols="2">VAT (arXiv-21) [16]</cell><cell>68.40</cell><cell cols="2">72.50</cell><cell cols="2">64.80</cell><cell>64.20</cell><cell>67.50</cell><cell>78.80</cell><cell>73.30</cell><cell>75.20</cell><cell>68.40</cell><cell>69.50</cell><cell>71.60</cell><cell>82.00</cell></row><row><cell></cell><cell cols="2">Meta Learner</cell><cell>67.56</cell><cell cols="2">72.90</cell><cell cols="2">64.94</cell><cell>61.91</cell><cell>66.82</cell><cell>77.31</cell><cell>72.14</cell><cell>76.66</cell><cell>70.77</cell><cell>69.27</cell><cell>72.21</cell><cell>81.94</cell></row><row><cell></cell><cell></cell><cell>Final</cell><cell>70.80</cell><cell cols="2">75.20</cell><cell cols="2">67.25</cell><cell>64.28</cell><cell>69.13</cell><cell>80.38</cell><cell>73.78</cell><cell>77.84</cell><cell>73.14</cell><cell>71.20</cell><cell>73.99</cell><cell>84.30</cell></row><row><cell></cell><cell>Backbone</cell><cell></cell><cell cols="2">Method</cell><cell cols="10">1-shot Fold-0 Fold-1 Fold-2 Fold-3 MIoU% Fold-0 Fold-1 Fold-2 Fold-3 MIoU% 5-shot</cell></row><row><cell></cell><cell></cell><cell cols="3">HFA (TIP-21) [28]</cell><cell cols="2">28.65</cell><cell>36.02</cell><cell>30.16</cell><cell>33.28</cell><cell>32.03</cell><cell>32.69</cell><cell>42.12</cell><cell>30.35</cell><cell>36.19</cell><cell>35.34</cell></row><row><cell></cell><cell></cell><cell cols="3">ASGNet (CVPR-21) [22]</cell><cell>-</cell><cell></cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>34.56</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>42.48</cell></row><row><cell></cell><cell></cell><cell cols="3">RePRI (CVPR-21) [4]</cell><cell cols="2">32.00</cell><cell>38.70</cell><cell>32.70</cell><cell>33.10</cell><cell>34.10</cell><cell>39.30</cell><cell>45.40</cell><cell>39.70</cell><cell>41.80</cell><cell>41.60</cell></row><row><cell></cell><cell></cell><cell cols="3">PPNet (ECCV-20) [32]</cell><cell cols="2">28.10</cell><cell>30.80</cell><cell>29.50</cell><cell>27.70</cell><cell>29.00</cell><cell>39.00</cell><cell>40.80</cell><cell>37.10</cell><cell>37.30</cell><cell>38.50</cell></row><row><cell></cell><cell></cell><cell cols="3">PFENet (TPAMI-20) [51]</cell><cell cols="2">36.50</cell><cell>38.60</cell><cell>34.50</cell><cell>33.80</cell><cell>35.80</cell><cell>36.50</cell><cell>43.30</cell><cell>37.80</cell><cell>38.40</cell><cell>39.00</cell></row><row><cell></cell><cell>ResNet50</cell><cell cols="3">HSNet (ICCV-21) [36]</cell><cell cols="2">36.30</cell><cell>43.10</cell><cell>38.70</cell><cell>38.7</cell><cell>39.20</cell><cell>43.30</cell><cell>51.30</cell><cell>48.20</cell><cell>45.00</cell><cell>46.90</cell></row><row><cell></cell><cell></cell><cell cols="3">VAT (arXiv-21) [16]</cell><cell cols="2">39.00</cell><cell>43.80</cell><cell>42.60</cell><cell>39.70</cell><cell>41.30</cell><cell>44.10</cell><cell>51.10</cell><cell>50.20</cell><cell>46.10</cell><cell>47.90</cell></row><row><cell></cell><cell></cell><cell cols="3">CyCTR (NIPs-21) [66]</cell><cell cols="2">38.90</cell><cell>43.00</cell><cell>39.60</cell><cell>39.80</cell><cell>40.30</cell><cell>41.10</cell><cell>48.90</cell><cell>45.20</cell><cell>47.00</cell><cell>45.60</cell></row><row><cell></cell><cell></cell><cell cols="3">BAM (CVPR-22) [20]</cell><cell cols="2">43.41</cell><cell>50.59</cell><cell>47.49</cell><cell>43.42</cell><cell>46.23</cell><cell>49.26</cell><cell>54.20</cell><cell>51.63</cell><cell>49.55</cell><cell>51.16</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Meta Learner</cell><cell cols="2">42.35</cell><cell>48.60</cell><cell>42.99</cell><cell>43.97</cell><cell>44.48</cell><cell>49.35</cell><cell>58.31</cell><cell>50.40</cell><cell>49.19</cell><cell>51.81</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Final</cell><cell cols="2">45.72</cell><cell>54.05</cell><cell>45.92</cell><cell>46.44</cell><cell>48.03</cell><cell>50.30</cell><cell>60.89</cell><cell>53.00</cell><cell>50.47</cell><cell>53.67</cell></row><row><cell></cell><cell></cell><cell cols="3">FWB (ICCV-19) [39]</cell><cell cols="2">17.00</cell><cell>18.00</cell><cell>21.00</cell><cell>28.90</cell><cell>21.20</cell><cell>19.10</cell><cell>21.50</cell><cell>23.90</cell><cell>30.10</cell><cell>23.70</cell></row><row><cell></cell><cell></cell><cell cols="3">DAN (ECCV-20) [54]</cell><cell>-</cell><cell></cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>24.40</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>29.60</cell></row><row><cell></cell><cell>ResNet101</cell><cell cols="3">PFENet (TPAMI-20) [51] HSNet (ICCV-21) [36]</cell><cell cols="2">36.80 37.20</cell><cell>41.80 44.10</cell><cell>38.70 42.40</cell><cell>36.70 41.30</cell><cell>38.50 41.20</cell><cell>40.40 45.90</cell><cell>46.80 53.00</cell><cell>43.20 51.80</cell><cell>40.50 47.10</cell><cell>42.70 49.50</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Meta Learner</cell><cell cols="2">43.89</cell><cell>51.98</cell><cell>45.51</cell><cell>47.55</cell><cell>47.23</cell><cell>50.49</cell><cell>59.41</cell><cell>54.31</cell><cell>53.70</cell><cell>54.48</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Final</cell><cell cols="2">47.83</cell><cell>57.43</cell><cell>48.65</cell><cell>50.45</cell><cell>51.09</cell><cell>53.23</cell><cell>62.25</cell><cell>55.43</cell><cell>56.30</cell><cell>56.80</cell></row></table><note>Layer Selection for Multi-Similarity: To understand</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">PSPNet trained on base classes 2 VGG16,ResNet50,ResNet101</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">A deep convolutional encoder-decoder architecture for image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><forename type="middle">Cipolla</forename><surname>Segnet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.00561</idno>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Yolact: Real-time instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Bolya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fanyi</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><forename type="middle">Jae</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF international conference on computer vision</title>
		<meeting>the IEEE/CVF international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9157" to="9166" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Yolact++: Better real-time instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Bolya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fanyi</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><forename type="middle">Jae</forename><surname>Lee</surname></persName>
		</author>
		<idno>2020. 1</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Few-shot segmentation without meta-learning: A good transductive inference is all you need?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Malik</forename><surname>Boudiaf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hoel</forename><surname>Kervadec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Ziko Imtiaz Masud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ismail</forename><surname>Piantanida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><surname>Ben Ayed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dolz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="13979" to="13988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="834" to="848" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Image block augmentation for one-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zitian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanwei</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiyu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Gang</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="3379" to="3386" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Image deformation meta-networks for one-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zitian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanwei</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Xiong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martial</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8680" to="8689" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Task-wise attention guided part complementary learning for few-shot image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gong</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruimin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunbo</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junwei</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science China Information Sciences</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE Computer Vision and Pattern Recognition</title>
		<meeting>of IEEE Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">I</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Modelagnostic meta-learning for fast adaptation of deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1126" to="1135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Proposal flow: Semantic correspondences from object proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bumsub</forename><surname>Ham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minsu</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="1711" to="1725" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Semantic contours from inverse detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Bharath Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubomir</forename><surname>Arbel?ez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhransu</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 international conference on computer vision</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="991" to="998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Piotr Doll?r, and Ross Girshick. Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2961" to="2969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Cost aggregation is all you need for few-shot segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sunghwan</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seokju</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jisu</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungryong</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.11685</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Attention-based multi-context guiding for few-shot semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengwan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chiliang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yadong</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">M</forename><surname>Cees</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Snoek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI conference on artificial intelligence</title>
		<meeting>the AAAI conference on artificial intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="8441" to="8448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Ccnet: Criss-cross attention for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zilong</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lichao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyu</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="603" to="612" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Task agnostic meta-learning for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammad</forename><forename type="middle">Abdullah</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamal</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guo-Jun</forename><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="11719" to="11727" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning what not to segment: A new perspective on fewshot segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunbo</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gong</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Binfei</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junwei</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="8057" to="8067" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Centermask: Realtime anchor-free instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngwan</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongyoul</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="13906" to="13915" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Adaptive prototype learning and allocation for few-shot segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Sevilla-Lara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonghyun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joongkyu</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Finding task-relevant features for few-shot learning by category traversal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Dodge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Correspondence networks with adaptive neighbourhood consensus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuda</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theo</forename><forename type="middle">W</forename><surname>Costain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henry</forename><surname>Howard-Jenkins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Prisacariu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10196" to="10205" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Refinenet: Multi-path refinement networks for highresolution semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guosheng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1925" to="1934" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Anti-aliasing semantic reconstruction for few-shot semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Binghao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbin</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyang</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixiang</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="9747" to="9756" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Harmonic feature activation for few-shot semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Binghao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbin</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixiang</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Prototype refinement network for few-shot segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinlu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongqiang</forename><surname>Qin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.03579</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Dynamic extension nets for few-shot semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lizhao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyi</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minqian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingkui</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM international conference on multimedia</title>
		<meeting>the 28th ACM international conference on multimedia</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1441" to="1449" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Crnet: Cross-reference networks for few-shot segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weide</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guosheng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fayao</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Songyang Zhang, and Xuming He. Part-aware prototype network for few-shot semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongfei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyi</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Semantic correspondence as an optimal transport problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanbin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linchao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Makoto</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4463" to="4472" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Simpler is better: Few-shot semantic segmentation with classifier weight transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhihe</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sen</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiatian</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Zhe</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="8741" to="8750" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Hypercorrelation squeeze for few-shot segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juhong</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahyun</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minsu</forename><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="6941" to="6952" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Hyperpixel flow: Semantic correspondence with multi-layer neural features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juhong</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongmin</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Ponce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minsu</forename><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Learning to compose hypercolumns for visual correspondence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juhong</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongmin</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Ponce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minsu</forename><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Feature weighting and boosting for few-shot segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khoi</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sinisa</forename><surname>Todorovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Bidirectional pyramid networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofeng</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Asian Conference on Computer Vision</title>
		<meeting>the Asian Conference on Computer Vision</meeting>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Large kernel matters-improve semantic segmentation by global convolutional network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guiming</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4353" to="4361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Optimization as a model for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sachin</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Neighbourhood consensus networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ignacio</forename><surname>Rocco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mircea</forename><surname>Cimpoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Relja</forename><surname>Arandjelovi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akihiko</forename><surname>Torii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Pajdla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Unet: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical image computing and computer-assisted intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Irfan Essa, and Byron Boots. One-shot learning for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amirreza</forename><surname>Shaban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shray</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.03410</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Adaptive masked proxies for few-shot segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mennatullah</forename><surname>Siam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boris</forename><surname>Oreshkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Jagersand</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.11123</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Prototypical networks for few-shot learning. Advances in neural information processing systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jake</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Scribblesupervised convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K Scribblesup</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition<address><addrLine>Las Vegas, NV, USA</addrLine></address></meeting>
		<imprint>
			<biblScope unit="volume">26</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Learning to compare: Relation network for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Flood</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongxin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">M</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1199" to="1208" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Prior guided feature enrichment network for few-shot segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuotao</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michelle</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruiyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Glunet: Global-local universal network for dense flow and correspondences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prune</forename><surname>Truong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6258" to="6268" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Matching networks for one shot learning. Advances in neural information processing systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Few-shot semantic segmentation with democratic attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haochen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xudong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutao</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yandan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note>Xianbin Cao, and Xiantong Zhen</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Panet: Few-shot image semantic segmentation with prototype alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaixin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Hao Liew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingtian</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daquan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Learning to learn: Model regression networks for easy small sample learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Xiong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martial</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="616" to="634" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Bidirectional attention for joint instance and semantic segmentation in point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangnan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyi</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changhe</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Asian Conference on Computer Vision</title>
		<meeting>the Asian Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Learning meta-class memory for few-shot semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhonghua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangxi</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guosheng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfei</forename><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="517" to="526" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Polarmask: Single shot instance segmentation with polar representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enze</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peize</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoge</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuebo</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ding</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="12193" to="12202" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Scaleaware graph neural network for few-shot semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Guo-Sen Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="5475" to="5484" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Prototype mixture models for few-shot semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boyu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bohao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbin</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixiang</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="763" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Multi-scale context aggregation by dilated convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.07122</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Self-guided and cross-guided learning for few-shot segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingfeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terry</forename><surname>Qin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="8312" to="8321" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Pyramid graph networks with connection attentions for region-based one-shot semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guosheng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fayao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiushuang</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingyao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Canet: Class-agnostic segmentation networks with iterative refinement and attentive few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guosheng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fayao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Few-shot segmentation via cycle-consistent transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gengwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoliang</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">34</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Sg-one: Similarity guidance network for one-shot semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Cybernetics</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2881" to="2890" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
