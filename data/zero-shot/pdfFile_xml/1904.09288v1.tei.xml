<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">STEP: Spatio-Temporal Progressive Learning for Video Action Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xitong</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Maryland</orgName>
								<address>
									<settlement>College Park</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Yang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">NVIDIA</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">NVIDIA</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fanyi</forename><surname>Xiao</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Davis</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry</forename><surname>Davis</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Maryland</orgName>
								<address>
									<settlement>College Park</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">NVIDIA</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">STEP: Spatio-Temporal Progressive Learning for Video Action Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T07:51+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we propose Spatio-TEmporal Progressive (STEP) action detector-a progressive learning framework for spatio-temporal action detection in videos. Starting from a handful of coarse-scale proposal cuboids, our approach progressively refines the proposals towards actions over a few steps. In this way, high-quality proposals (i.e., adhere to action movements) can be gradually obtained at later steps by leveraging the regression outputs from previous steps. At each step, we adaptively extend the proposals in time to incorporate more related temporal context. Compared to the prior work that performs action detection in one run, our progressive learning framework is able to naturally handle the spatial displacement within action tubes and therefore provides a more effective way for spatio-temporal modeling. We extensively evaluate our approach on UCF101 and AVA, and demonstrate superior detection results. Remarkably, we achieve mAP of 75.0% and 18.6% on the two datasets with 3 progressive steps and using respectively only 11 and 34 initial proposals.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Spatio-temporal action detection aims to recognize the actions of interest that present in a video and localize them in both space and time. Inspired by the advances in the field of object detection in images <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b20">21]</ref>, most recent work approaches this task based on the standard two-stage framework: in the first stage action proposals are produced by a region proposal algorithm or densely sampled anchors, and in the second stage the proposals are used for action classification and localization refinement.</p><p>Compared to object detection in images, spatio-temporal action detection in videos is however a more challenging problem. New challenges arise from both of the above two stages when the temporal characteristic of videos is taken into account. First, an action tube (i.e., a sequence of bounding boxes of action) usually involves spatial displace- * Work done during an internship at NVIDIA Research. <ref type="figure">Figure 1</ref>: A schematic overview of spatio-temporal progressive learning for action detection. Starting with a coarsescale proposal cuboid, it progressively refines the proposal towards the action, and adaptively extends the proposal to incorporate more related temporal context at each step. ment over time, which introduces extra complexity for proposal generation and refinement. Second, effective temporal modeling becomes imperative for accurate action classification, as a number of actions are only identifiable when temporal context information is available.</p><p>Previous work usually exploits temporal information by performing action detection at the clip (i.e., a short video snippet) level. For instance, <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b16">17]</ref> take as input a sequence of frames and output the action categories and regressed tubelets of each clip. In order to generate action proposals, they extend 2D region proposals to 3D by replicating them over time, assuming that the spatial extent is fixed within a clip. However, this assumption would be violated for the action tubes with large spatial displacement, in particular when the clip is long or involves rapid movement of actors or camera. Thus, using long cuboids directly as action proposals is not optimal, since they introduce extra noise for action classification and make action localization more challenging, if not hopeless. Recently, there are some attempts to use adaptive proposals for action detection <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b19">20]</ref>. However, these methods require an offline linking process to generate the proposals.</p><p>In this paper, we present a novel learning framework, Spatio-TEmporal Progressive (STEP) action detector, for video action detection. As illustrated in <ref type="figure">Figure 1</ref>, unlike existing methods that directly perform action detection in one run, our framework involves a multi-step optimization process that progressively refines the initial proposals towards the final solution. Specifically, STEP consists of two components: spatial refinement and temporal extension. Spatial refinement starts with a small number of coarse-scale proposals and updates them iteratively to better classify and localize action regions. We carry out the multiple steps in a sequential order, where the outputs of one step are used as the proposals for next step. This is motivated by the fact that the regression outputs can better follow actors and adapt to action tubes than the input proposals. Temporal extension focuses on improving classification accuracy by incorporating longer-range temporal information. However, simply taking a longer clip as input is inefficient and also ineffective since a longer sequence tends to have larger spatial displacement, as shown in <ref type="figure">Figure 1</ref>. Instead, we progressively process longer sequences at each step and adaptively extend proposals to follow action movement. In this manner, STEP can naturally handle the spatial displacement problem and therefore provide more efficient and effective spatio-temporal modeling. Moreover, STEP achieves superior performance by using only a handful (e.g., 11) of proposals, obviating the need to generate and process large numbers (e.g., &gt;1K) of proposals due to the tremendous spatial and temporal search space.</p><p>To our knowledge, this work provides the first end-toend progressive optimization framework for video action detection. We bring up the spatial displacement problem in action tubes and show that our method can naturally handle the problem in an efficient and effective way. Extensive evaluations find our approach to produce superior detection results while only using a small number of proposals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Action Recognition. A large family of the research in video action recognition is about action classification, which provides fundamental tools for action detection, such as two-stream networks on multiple modalities <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b34">35]</ref>, 3D-CNN for simultaneous spatial and temporal feature learning <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b12">13]</ref>, and RNNs to capture temporal context and handle variable-length video sequences <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b35">36]</ref>. Another active research line is the temporal action detection, which focuses on localizing the temporal extent of each action. Many methods have been proposed, from fast temporal action proposals <ref type="bibr" target="#b14">[15]</ref>, region convolutional 3D network <ref type="bibr" target="#b33">[34]</ref>, to budget-aware recurrent policy network <ref type="bibr" target="#b22">[23]</ref>.</p><p>Spatio-Temporal Action Detection. Inspired by the recent advances in image object detection, a number of efforts have been made to extend image object detectors (e.g., R-CNN, Fast R-CNN and SSD) to the task as frame-level action detectors <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b37">38]</ref>. The extensions mainly include: first, optical flow is used to capture motion cues, and second, linking algorithms are developed to connect frame-level detection results as action tubes. Although these methods have achieved promising results, the temporal property of videos is not explicitly or fully exploited as the detection is performed on each frame independently. To better leverage the temporal cues, several recent work has been proposed to perform action detection at clip level. For instance, ACT <ref type="bibr" target="#b16">[17]</ref> takes as input a short sequence of frames (e.g., 6 frames) and outputs the regressed tubelets, which are then linked by a tubelet linking algorithm to construct action tubes. Gu et al. <ref type="bibr" target="#b11">[12]</ref> further demonstrate the importance of temporal information by using longer clips (e.g., 40 frames) and taking advantage of I3D pre-trained on the large-scale video dataset <ref type="bibr" target="#b3">[4]</ref>. Rather than linking the frame or clip level detection results, there are also some methods that are developed to link the proposals before classification to generate action tube proposals <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b19">20]</ref>.</p><p>Progressive Optimization. This technique has been explored in a range of vision tasks from pose estimation <ref type="bibr" target="#b2">[3]</ref>, image generation <ref type="bibr" target="#b10">[11]</ref> to object detection <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b23">24]</ref>. Specifically, the multi-region detector <ref type="bibr" target="#b5">[6]</ref> introduces iterative bounding box regression with R-CNN to produce better regression results. AttractioNet in <ref type="bibr" target="#b6">[7]</ref> employs a multistage procedure to generate accurate object proposals that are then input to Fast R-CNN. G-CNN <ref type="bibr" target="#b23">[24]</ref> trains a regressor to iteratively move a grid of bounding boxes towards objects. Cascade R-CNN <ref type="bibr" target="#b1">[2]</ref> proposes a cascade framework for high-quality object detection, where a sequence of R-CNN detectors are trained with increasing IoU thresholds to iteratively suppress close false positives.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>In this section, we introduce the proposed progressive learning framework STEP for video action detection. We first formulate the problem and provide an overview of our approach. We then describe in details the two primary components of STEP including spatial refinement and temporal extension. Finally, the training algorithm and implementation details are presented.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Framework Overview</head><p>Proceeding with the recent work <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b16">17]</ref>, our approach performs action detection at clip level, i.e., detection results are first obtained from each clip and then linked to build action tubes across a whole video. We assume that each action tubelet of a clip has a constant action label, considering the short duration of a clip, e.g., within one second.</p><p>Our target is to tackle the action detection problem through a few progressive steps, rather than directly detecting actions all at one run. In order to detect the actions in a clip I t with K frames, according to the maximum progressive steps S max , we first extract the convolutional features for a set of clips I = {I t?Smax+1 , ..., I t , ..., I t+Smax?1 } using a backbone network such as VGG16 <ref type="bibr" target="#b28">[29]</ref> or I3D <ref type="bibr" target="#b3">[4]</ref>. The progressive learning starts with M pre-defined proposal</p><formula xml:id="formula_0">cuboids B 0 = b 0 i M i=1 and b 0 i ? R K?4</formula><p>, which are sparsely sampled from a coarse-scale grid of boxes and replicated across time to form the initial proposals. An example of the 11 initial proposals used in our experiments is illustrated in <ref type="figure" target="#fig_0">Figure 2</ref>. These initial proposals are then progressively updated to better classify and localize the actions. At each step s, we update the proposals by performing the following processes in order:</p><p>? Extend: the proposals are temporally extended to the adjacent clips to include longer-range temporal context, and the temporal extension is adaptive to the movement of actions, as described in Section 3.3. ? Refine: the extended proposals are forwarded to the spatial refinement, which outputs the classification and regression results, as presented in Section 3.2. ? Update: all proposals are updated using a simple greedy algorithm, i.e., each proposal is replaced by the regression output with the highest classification score:</p><formula xml:id="formula_1">b s i . = l s i (c * ), c * = arg max c p s i (c),<label>(1)</label></formula><p>where c is an action class, p s i ? R (C+1) is the probability distribution of the ith proposal over C action classes plus background, l s i ? R K?4?C denotes its parameterized coordinates (for computing the localization loss in Eq. 3) at each frame for each class, and . = indicates decoding the parameterized coordinates. We summarize the outline of our detection algorithm in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Spatial Refinement</head><p>At each step s, the spatial refinement solves a multitask learning problem that involves action classification and localization regression. Accordingly, we design a twobranch architecture, which learns separate features for the two tasks, as illustrated in <ref type="figure" target="#fig_3">Figure 3</ref>. Our motivation is that the two tasks have substantially different objectives and re-Algorithm 1: STEP Action Detection for Clip I t Input : video clips I, initial proposals B 0 , and maximum steps S max Output: detection results (p Smax</p><formula xml:id="formula_2">i , l Smax i ) M i=1 1 extract convolutional features for video clips I 2 for s ? 1 to S max do 3 if s == 1 then 4 // initial proposals 5B s?1 ? B 0 6 else 7 // temporal extension (Sec.3.3) 8B s?1 ? Extend(B s?1 ) 9 end 10 // spatial refinement (Sec.3.2) 11 (p s i , l s i ) M i=1 ? Refine(B s?1 ) 12 // update proposals (Eq.1) 13 B s ? Update (p s i , l s i ) M i=1 14 end</formula><p>quire different types of information. For accurate action classification, it demands context features in both space and time, while for robust localization regression, it needs more precise spatial cues at frame level. As a result, our twobranch network consists of a global branch that performs spatio-temporal modeling on the entire input sequence for action classification, as well as a local branch that performs bounding box regression at each frame. Given the frame-level convolutional features and the tubelet proposals for the current step, we first extract regional features through an ROI pooling <ref type="bibr" target="#b7">[8]</ref>. Then we take the regional features to the global branch for spatiotemporal modeling and produce the global feature. Each global feature encodes the context information of a whole tubelet and is further used to predict the classification output p s i . Moreover, the global feature is concatenated with the corresponding regional features at each frame to form the local feature, which is used to generate the class-specific regression output l s i . Our local feature not only captures the spatio-temporal context of a tubelet but also extracts the local details of each frame. By jointly training the two branches, the network learns the two separate features that are informative and adaptable for their own tasks.</p><p>Training Loss. We enforce a multi-task loss to jointly train for action classification and tubelet regression. Let P s denote the set of selected positive samples and N s the set of negative samples at step s (the sampling strategy is described in Section 3.4). We define the training loss L s as: <ref type="figure" target="#fig_3">Figure 3</ref>: Left: the architecture of our two-branch network. Right: the illustration of our progressive learning framework, where "S" indicates spatial refinement, "T" temporal extension, "P" classification, and "L" localization, the numbers correspond to the steps, and "L0" denotes the initial proposals.</p><formula xml:id="formula_3">L s = i?{P s , N s } L cls (p s i , u i ) + ? i?P s L loc (l s i (u i ), v i ),<label>(2)</label></formula><p>where u i and v i are the ground truth class label and localization target for the ith sample, and ? is the weight to control the importance of the two loss terms. We employ the multi-class cross-entropy loss as the classification loss</p><formula xml:id="formula_4">L cls (p s i , u i ) = ? log p s i (u i ) in Eq. 2.</formula><p>We define the localization loss using the averaged 1,smooth between predicted and ground truth bounding boxes over the frames of a clip:</p><formula xml:id="formula_5">L loc (l s i (u i ), v i ) = 1 K K k=1 1,smooth (l s i,k (u i ) ? v i,k ). (3)</formula><p>We apply the same parameterization for v i,k as in <ref type="bibr" target="#b8">[9]</ref> by using a scale-invariant center translation and a log-space height/width shift relative to the bounding box.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Temporal Extension</head><p>Video temporal information, especially the long-term temporal dependency, is critical for accurate action classification <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b35">36]</ref>. In order to leverage longer range of temporal context, we extend the proposals to include in more frames as input. However, the extension is not trivial since the spatial displacement problem becomes even more severe for longer sequences, as illustrated in <ref type="figure">Figure 1</ref>. Recently, some negative impacts caused by the spatial displacement problem for action detection have also been observed by <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b16">17]</ref>, which simply replicate 2D proposals across time to increase longer temporal length.</p><p>With the intention to alleviate the spatial displacement problem, we perform temporal extension progressively and adaptively. From the second step, we extend the tubelet proposals to the two adjacent clips at a time. In other words, at each step 1 ? s &lt; S max , the proposals B s with length K s are extended toB</p><formula xml:id="formula_6">s = B s ?1 ?B s ?B s +1 with length K s +2K, where ? denotes concatenation.</formula><p>Additionally, the temporal extension is adaptive to action movement by taking advantage of the regressed tubelets from the previous step. We introduce two methods to enable the temporal extension to be adaptive as described in the following.</p><p>Extrapolation. By assuming that the spatial movement of an action satisfies a linear function approximately within a short temporal range, such as a 6-frame clip, we can extend the tubelet proposals by using a simple linear extrapolation function:</p><formula xml:id="formula_7">B s +1,k = B s K s + k K ? 1 (B s K s ? B s K s ?K+1 ).<label>(4)</label></formula><p>A similar function can be applied to B s ?1 to adapt to the movement trend, but the assumption would be violated for long sequences and therefore results in drifted estimations.</p><p>Anticipation. We can also achieve the adaptive temporal extension by location anticipation, i.e., training an extra regression branch to conjecture the tubelet locations in adjacent clips based on the current clip. Intuitively, the anticipation requires the network to infer the movement trend in adjacent clips by action modeling in the current clip. A similar idea is explored in <ref type="bibr" target="#b36">[37]</ref>, where location anticipation is used at the region proposal stage.</p><p>We formulate our location anticipation as a residual learning problem <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b21">22]</ref> based on the assumption that the tubelets of two adjacent clips differ from each other by a small residual. Let x indicate the features forwarded to the output layer f of the location regressor L s = f (x) at step s. So the anticipated locations can be obtained as:</p><formula xml:id="formula_8">L s ?1 = L s + f ?1 (x), L s +1 = L s + f +1 (x),<label>(5)</label></formula><p>where f ?1 and f +1 are the anticipation regressors, which are lightweight and introduce negligible computational overhead. L s ?1 and L s +1 are then decoded to the proposals B s ?1 and B s +1 . The loss function of location anticipation is defined in a similar way as Eq. 3, and combined with L cls and L loc with a coefficient ? to form the overall loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Network Training</head><p>Although STEP involves multiple progressive steps, the whole framework can be trained end-to-end to optimize the models at different steps jointly. Compared against the stepwised training scheme used in <ref type="bibr" target="#b23">[24]</ref>, our joint training is simpler to implement, runs more efficiently, and achieves better performance in our experiments.</p><p>Given a mini-batch of training data, we first perform an (S max ? 1)-step inference pass, as illustrated in the right of <ref type="figure" target="#fig_3">Figure 3</ref>, to obtain the inputs needed for all progressive steps. In practice, the detection outputs</p><formula xml:id="formula_9">{(p s i , l s i )} M i=1</formula><p>at each step are collected and used to select the positive and negative samples P s and N s for training. We accumulate the losses of all steps and back-propagate to update the whole model at the same time. Distribution Change. Compared to the prior work that performs detection in one run, our training could be more challenging as the input/output distributions change over steps. As shown in <ref type="figure">Figure 4</ref>, the input distribution is rightskewed or centered in a low-IoU level at early steps, and reverses at later steps. This is because our approach starts from a coarse-scale grid (see <ref type="figure" target="#fig_0">Figure 2</ref>) and progressively refines them towards generating high-quality proposals. Accordingly, the range of output distribution (i.e., the scale of offset vectors) decreases over steps.</p><p>Inspired by <ref type="bibr" target="#b1">[2]</ref>, we tackle the distribution change in three ways. First, separate headers are used at different steps to adapt to the different input/output distributions. Second, we increase IoU thresholds over the multiple steps. Intuitively, a lower IoU threshold at early steps tolerates the initial proposals to include sufficient positive samples and a higher IoU threshold at late steps encourages high-quality detection. Third, a hard-aware sampling strategy is employed to select more informative samples during training.</p><p>Hard-Aware Sampling. We design the sampling strategy based on two principles: (i) the numbers of positive and negative samples should be roughly balanced, and (ii) the harder negatives should be selected more often. To measure the "hardness" of a negative sample, we use the classification scores from the previous step. The tubelet with a high confidence but a low overlap to any ground truth is viewed as a hard sample. We calculate the overlap of two tubelets by averaging the IoU of bounding boxes over K frames of the target clip. So the negative samples with higher classification scores will be sampled with a higher chance.</p><p>Formally, given a set of proposals and the overlap threshold ? s at step s, we first assign positive labels to the candidates with the highest overlap with ground truth. This is to ensure that each ground truth tube has at least one positive sample. After that, the proposals having an overlap higher than ? s with any ground truth tube are added to the positive pool and the rest to the negative pool. We then sample |P s | positives and |N s | negatives from the two pools,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IoU</head><p>IoU IoU</p><p>Step 1</p><p>Step 2</p><p>Step 3 <ref type="figure">Figure 4</ref>: Change of input distribution (IoU between input proposals and ground truth) over steps on UCF101.</p><p>respectively, with the sampling probability proportional to the classification score. For the first step, the highest overlap with ground truth tubes is used as the score for sampling. Each selected positive in P s is assigned to the ground truth tube with which it has the highest overlap. Note that a single proposal can be assigned to only one ground truth tube.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Full Model</head><p>We can also integrate our model with the common practices for video action detection <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b29">30]</ref>, such as twostream fusion and tubelet linking.</p><p>Scene Context. It has been proven to be beneficial to object and action detection <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b31">32]</ref>. Intuitively, some actionrelated semantic clues from scene context can be utilized to improve action classification, for example, the scene of a basketball court for recognizing "basketball dunk". We incorporate scene context by concatenating extended features to original regional features in the global branch. The extended features can be obtained by RoI pooling of the whole image. So the global features encode both spatial and temporal context useful for action classification.</p><p>Two-Stream Fusion. Most previous methods use late fusion to combine the results at test time, i.e., the detections are obtained independently from the two streams and then fused using either mean fusion <ref type="bibr" target="#b16">[17]</ref> or union fusion <ref type="bibr" target="#b29">[30]</ref>. In this work, we also investigate early fusion for two-stream fusion, which concatenates RGB frames and optical flow maps in channel and input to the network as a whole. Intuitively, early fusion can model the low-level interactions between the two modalities and also obviates the need for training two separate networks. In addition, a hybrid fusion can be further performed to combine detection results from the early fusion and the two streams. Our experiment shows that early fusion outperforms late fusion, and hybrid fusion achieves the best performance.</p><p>Tubelet Linking. Given the clip-level detection results, we link them in space and time to construct the final action tubes. We follow the same linking algorithm as described in <ref type="bibr" target="#b16">[17]</ref>, apart from that we do not apply global non-maximum suppression across classes but perform temporal trimming over the linked paths as commonly used in <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b26">27]</ref>. The temporal trimming enforces consecutive boxes to have smooth classification scores by solving an energy maximization problem via dynamic programming.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, we describe the experiments to evaluate STEP and compare against the recent competing algorithms. We start by performing a variety of ablation studies to better understand the contributions of each individual component in our approach. We then report comparisons to the state-of-the-art methods, provide in-depth analysis, and present the qualitative detection results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experimental Setup</head><p>Datasets. We evaluate our approach on the two benchmarks: UCF101 <ref type="bibr" target="#b30">[31]</ref> and AVA <ref type="bibr" target="#b11">[12]</ref>. In comparison with other action detection datasets, such as J-HMDB and UCF-Sports, the two benchmarks are much larger and more challenging, and more importantly, they are temporally untrimmed, which fits better to the spatio-temporal action detection task. UCF101 is originally an action classification dataset collected from online videos, and a subset of 24 classes with 3,207 videos are provided with the spatiotemporal annotations for action detection. Following the standard evaluation protocol <ref type="bibr" target="#b16">[17]</ref>, we report results on the first split of the dataset. AVA contains complex actions and scenes sourced from movies. We use the version 2.1 of AVA, which consists of the annotations at 1 fps over 80 action classes. Following the standard setup in <ref type="bibr" target="#b11">[12]</ref>, we report results on the most frequent 60 classes that have at least 25 validation examples per class.</p><p>Evaluation Metrics. We report the frame-level mean average precision (frame-mAP) with an IoU threshold of 0.5 for both datasets. This metric allows us to evaluate the quality of the detection results independently of the linking algorithm. We also use the video-mAP on UCF101 to compare with the state-of-the-art results.</p><p>Implementation Details. For the experiments on UCF101, we use VGG16 <ref type="bibr" target="#b28">[29]</ref> pre-trained on ImageNet <ref type="bibr" target="#b4">[5]</ref> as the backbone network. Although more advanced models are available, we choose the same backbone as <ref type="bibr" target="#b16">[17]</ref> for fair comparisons. For the temporal modeling in global branch, we use three 3D convolutional layers with adaptive max pooling along the temporal dimension. All frames are resized to 400 ? 400 and the clip length is set to K = 6. Similar to <ref type="bibr" target="#b16">[17]</ref>, 5 consecutive optical flow maps are stacked as a whole for the optical flow input. We train our models for 35 epochs using Adam <ref type="bibr" target="#b18">[19]</ref> with a batch size of 4. We set the initial learning rate to 5 ? 10 ?5 and perform step decay after 20 and 30 epochs with the decay rate 0.1.</p><p>For the experiments on AVA, we adopt I3D <ref type="bibr" target="#b3">[4]</ref> (up to Mixed 4f) pre-trained on Kinetics-400 <ref type="bibr" target="#b17">[18]</ref> as the backbone network. We take the two layers Mixed 5b and Mixed 5c of I3D for temporal modeling in our global branch. All frames are resized to 400 ? 400 and the clip length is set to K = 12. We use 34 initial proposals and perform temporal extension only at the third step. As the classification is more challenging on AVA, we first pre-train our model for an action classification task using the spatial ground truth of training set. We then train the model for action detection with a batch size of 4 for 10 epochs. We do not use optical flow on this dataset due to the heavy computation and instead combine results of two RGB models. Our initial learning rate is 5 ? 10 ?6 for the backbone network and 5 ? 10 ?5 for the two-branch networks, and step decay is performed after 6 epochs with the decay rate 0.1.</p><p>For all experiments, we extract optical flow (if used) with Brox <ref type="bibr" target="#b0">[1]</ref>, and perform data augmentation to the whole sequence of frames during training, including random flipping and cropping. More architecture and implementation details are available in the appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Ablation Study</head><p>We perform various ablation experiments on UCF101 to evaluate the impacts of different design choices in our framework. For all experiments in this section, we employ the 11 initial proposals as shown in <ref type="figure" target="#fig_0">Figure 2</ref> and RGB only, unless explicitly mentioned otherwise, and frame-mAP is used as the evaluation metric.</p><p>Effectiveness of Spatial Refinement. Our primary design of STEP is to progressively tackle the action detection problem through a few steps. We thus first verify the effectiveness of progressive learning by comparing the detection results at different steps with the spatial refinement. No temporal extension is applied in this comparison. <ref type="table">Table  1</ref>(a) demonstrates the step-wise performance under different maximum steps S max . Since our approach starts from the coarse-scale proposals, performing spatial refinement once is insufficient to achieve good results. We observe that the second step improves results consistently and substantially, indicating that the updated proposals have higher quality and provide more precise information for classification and localization. Further improvement can be obtained by additional steps, suggesting the effectiveness of our progressive spatial refinement. We use 3 steps for most of our experiments as the performance saturates after that. Note that using more steps also improves the results of early steps, due to the benefits of our multi-step joint training. Effectiveness of Temporal Extension. In addition to the spatial refinement, our progressive learning contains the temporal extension to progressively process a longer sequence at each step. We compare the detection results with and without temporal extension in <ref type="figure" target="#fig_1">Figure 5</ref>. We show the results of the models taking K = 6 and K = 30 frames as inputs directly without temporal extension, and the results of the extrapolation and anticipation methods. Note that the models with temporal extension also deal with 30 frames at the third step (extension process: 6 ? 18 ? 30).</p><p>Both of the temporal extension methods outperform the baseline (K = 6) by a large margin, which clearly shows the benefit of incorporating longer-range temporal context for action classification. More remarkably, simply taking K = 30 frames as input without temporal extension results in inferior performance, validating the importance of adaptively extending the temporal scale in the progressive manner. Furthermore, we observe that anticipation performs better than extrapolation for longer sequences, indicating that anticipation can better capture nonlinear movement trends and therefore generate better extensions.</p><p>Fusion Comparison. <ref type="table">Table 1</ref>(b) presents the detection results of different fusions: late, early and hybrid fusion. In all cases, using both modalities improves the performance compared to individual ones. We find that early fusion outperforms late fusion, and attribute the improvement to modeling between the two modalities at the early stage. Hybrid fusion achieves the best result by further utilizing the complementary information of different methods.</p><p>Miscellaneous. We describe several techniques to improve the training in Section 3, including incorporating scene context, hard-award sampling and increasing IoU threshold. To validate the contributions of the three techniques, we conduct ablation experiments by removing one at a time, which correspondingly results in a performance drop of 2.5%, 1.5% and 1%. In addition, we observe that incorporating scene context provides more gains for later steps, suggesting that scene context is more important for action classification when bounding boxes become tight. <ref type="figure">Figure 6</ref>: Analysis of runtime of our approach under various settings: (a) the inference speeds using different step numbers with and without temporal extension, and (b) the detection results (green dots) and speeds (blue bars) using different numbers of initial proposals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Runtime Analysis</head><p>Although STEP involves a multi-step optimization, our model is efficient since we only process a small number of proposals. STEP runs at 21 fps using early fusion with 11 initial proposals and 3 steps on a single GPU, which is comparable with the clip based approach (23 fps) <ref type="bibr" target="#b16">[17]</ref> and much faster than the frame based method (4 fps) <ref type="bibr" target="#b25">[26]</ref>. <ref type="figure">Figure 6</ref>(a) demonstrates the speeds of our approach with increasing number of steps under the settings with and without temporal extension. We also report the running time and detection performance of our approach (w/o temporal extension for 3 steps) with increasing number of initial proposals in <ref type="figure">Figure 6</ref>(b). We observe substantial gains in detection accuracy by increasing the number of initial proposals, but it also results in slowed inference speed. This trade-off between accuracy and speed can be controlled according to a specified time budget.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Comparison with State-of-the-Art Results</head><p>We compare our approach with the state-of-the-art methods on UCF101 and AVA in <ref type="table" target="#tab_2">Tables 2 and 4</ref>. Following the standard settings, we report the frame-mAP at IoU threshold 0.5 on both datasets and the video-mAP at various IoU thresholds on UCF101. STEP consistently performs better than the state-of-the-art methods on UCF101, and brings a clear gain in frame-mAP, producing 5.5% improvement over the second best result. Our approach also achieves superior result on AVA, outperforming the recently proposed ACRN by 1.2%. Notably, STEP performs detection simply from a handful of initial proposals, while other competing algorithms rely on a great amount of densely sampled anchors or an extra person detector trained with external large-scale image object detection datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Qualitative Results</head><p>We visualize the detection results of our approach at different steps in <ref type="figure" target="#fig_2">Figure 7</ref>. Each row indicates the detection outputs at a certain step. A bounding box is labeled in red if the detection result is correct, otherwise it is labeled in blue.      ing the actions and better localization results are obtained at later steps. Although starting from coarse-scale proposals, our approach is robust to various action scales thanks to the progressive spatial refinement, as illustrated in <ref type="figure">Figure 8</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we have proposed the spatio-temporal progressive learning framework STEP for video action detection. STEP involves spatial refinement and temporal extension, where the former starts from sparse initial proposals and iteratively updates bounding boxes, and the latter gradually and adaptively increases sequence length to incorporate more related temporal context. STEP is found to be able to more effectively make use of longer temporal information by handling the spatial displacement problem in action tubes. Extensive experiments on two benchmarks show that STEP consistently brings performance gains by using only a handful of proposals and a few updating steps.</p><p>A. Implementation Details UCF101 Dataset. <ref type="table" target="#tab_5">Table 4</ref> shows the details of our twobranch architecture. The network takes as inputs a sequence of 512 ? 25 ? 25 feature maps from the backbone network (i.e., VGG16) as well as a set of proposal tubelets. For each proposal, an RoI pooling layer extracts a sequence of fixedlength regional features from the feature maps. For temporal modeling in the global branch, we first spatially extend each proposal tubelet to incorporate more scene context, as described in Section 3.5 of the paper. We then forward the extended features to three 3D convolutional layers to obtain the global features. To perform action classification, the global features are flatten and fed into a sequence of fully connected (fc) layers, which finally output the softmax probability estimates over C classes plus background. To perform tubelet regression, the global features are concatenated along channel dimension with the regional features at each frame and then fed into another sequence of fc layers, which produce a class-specific regression output with the shape 4 ? (C + 1) for each frame.</p><p>AVA Dataset. The overall architecture is the same as the one in <ref type="table" target="#tab_5">Table 4</ref> except that we do not introduce extra 3D convolutional layers for temporal modeling. Instead, the Mixed 5b and Mixed 5c in I3D are used and followed by a 1 ? 1 ? 1 convolutional layer to downsample the channel dimension to 256.</p><p>We use 34 initial proposals in the experiments on AVA since this datasets involves more actions on average at each frame than UCF101. We define the 34 initial proposals following the practice in <ref type="bibr" target="#b23">[24]</ref>. In details, we generate the initial proposals using a two-level spatial pyramid with [4/3, 2] scales and [5/6, 3/4] overlap for each spatial scale. In other words, a sliding window with size 3W/4 ? 3H/4 pixels and overlap ratio 5/6 is used for the first level, and a sliding window with size W/2 ? H/2 pixels and overlap ratio 3/4 is used for the second level. Here, W and H denote the width and height of the frames, respectively. We extract video frames in 12 fps and resize them to 400 ? 400.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Spatial Displacement</head><p>The spatial displacement problem occurs in an action tube when the sequence is long and or involves rapid movement of people or camera. Here we analyze the spatial displacement problem on UCF101 by calculating the minimum IoU within tubes (MIUT). Given a ground truth ac-   tion tube, MIUT is defined by the minimum IoU overlap between the center bounding box (i.e., the box of the center frame) and the other bounding boxes within the tube. <ref type="figure" target="#fig_5">Figure  9</ref> demonstrates the statistics of different actions with different length using ground truth action tubes in the validation set. We observe that the spatial displacement problem is not very obvious for short clips (e.g., K = 6), as most action classes have high MIUT values. However, the spatial displacement problem becomes more severe for most actions when the sequence length increases. For example, "Skijet" (ID: 18) has a 0.12 MIUT and "CliffDiving" (ID: 4) has a 0.17 MIUT when K = 30, indicating both actions encounter large spatial displacements within the tubes. We also show some examples to illustrate the spatial displacement problem in <ref type="figure" target="#fig_6">Figure 10</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. More Analysis</head><p>In order to tackle the spatial displacement problem, we introduce two methods to adaptively perform the temporal extension, i.e., extrapolation and anticipation as defined in  Eqs.(4-5) of the paper. <ref type="figure" target="#fig_7">Figure 11</ref> illustrates the extrapolation: for each of the current proposals, following its first and last tubelets (one tubelet with 6 bounding boxes), the extrapolation linearly estimates the directions and scales of the extended tubelets. As for the impact of different action scales, we qualitatively show the examples in <ref type="figure">Figure 8</ref> of the paper, and we report the frame-APs and average sizes of different action classes of UCF101 in <ref type="figure" target="#fig_0">Figure 12</ref>. Thanks to the progressive learning, STEP is found to be robust to handle the actions with small scales, though it starts with coarsescale proposals. <ref type="figure" target="#fig_3">Figure 13</ref> demonstrates the per-class breakdown frame-AP on AVA.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Example of the 11 initial proposals: 2D boxes are replicated across time to obtain cuboids.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 5 :</head><label>5</label><figDesc>Comparison of frame-mAP (%) of our models trained with and without temporal extension.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 7 :</head><label>7</label><figDesc>Examples of the detection results on UCF101. Red boxes indicate correct detection and blue ones misclassification. (a) illustrates the effect of progressive learning to improve action classification over steps. (b) demonstrates the regression outputs by spatial refinement at each step.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>3 :</head><label>3</label><figDesc>Comparison with the state-of-the-art methods on AVA by frame-mAP (%) under IoU = 0.5. "*" means the results obtained by incorporating optical flow.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 (Figure 8 :</head><label>78</label><figDesc>a) demonstrates the effect of progressive learning for more accurate action classification. It can be observed by the fact that the blue boxes are eliminated at later steps. InFigure 7(b), the first row corresponds to the initial proposals and the next two rows show the effect of spatial refinement of the proposals over steps. It is clear that the proposals progressively move towards the persons perform-Examples of the small scale action detection by our approach. Red boxes indicate the initial proposals and orange ones the detection outputs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 9 :</head><label>9</label><figDesc>MIUT of ground truth action tubes on UCF101. K denotes different tube lengths, and red dash line cooresponds to MIUT = 0.5.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 10 :</head><label>10</label><figDesc>Examples of the spatial displacement problem. Red boxes indicate the ground truth bounding boxes and blue ones the spatial grids. From top to bottom are LongJump (ID: 12), FloorGymnastics (ID: 8) and CliffDiving (ID: 4).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 11 :</head><label>11</label><figDesc>Illustration of the extrapolation for adaptive temporal extension. Blue shaded boxes are the first and last bounding boxes of the corresponding tubelets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 12 :</head><label>12</label><figDesc>Analysis of the detection accuracy (blue) and the average bounding box size (green) of each action class.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 13 :</head><label>13</label><figDesc>Comparison of the per-class breakdown frame-AP at IoU threshold 0.5 on AVA.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table><row><cell cols="2">Comparison with the state-of-the-art methods on</cell></row><row><cell cols="2">UCF101 by frame-mAP (%) and video-mAP (%) under dif-</cell></row><row><cell>ferent IoU thresholds.</cell><cell></cell></row><row><cell>Method</cell><cell>frame-mAP</cell></row><row><cell>Single Frame  *  [12]</cell><cell>14.2</cell></row><row><cell>I3D [12]</cell><cell>14.7</cell></row><row><cell>I3D  *  [12]</cell><cell>15.6</cell></row><row><cell>ACRN  *  [32]</cell><cell>17.4</cell></row><row><cell>Ours</cell><cell>18.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table</head><label></label><figDesc></figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Architecture of the two-branch network, where T ? H?W, N represent the dimensions of convolutional kernels and output feature maps.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgement. Davis acknowledges the support from IARPA via Department of Interior/Interior Business Center (DOI/IBC) under contract number D17PC00345.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head><p>In this appendix, Section A summarizes the details of our two-branch architecture and how to generate the initial proposals. Section B presents more evidences of the spatial displacement problem in action detection. Section C provides more algorithm and result analysis.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">High accuracy optical flow estimation based on a theory for warping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andr?s</forename><surname>Bruhn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nils</forename><surname>Papenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joachim</forename><surname>Weickert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Cascade R-CNN: Delving into high quality object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaowei</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nuno</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Human pose estimation with iterative error feedback</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pulkit</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katerina</forename><surname>Fragkiadaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? A new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Object detection via a multi-region and semantic segmentation-aware CNN model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spyros</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
		<idno>ICCV. 2</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Attend refine repeat: Active box proposal generation via in-out localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spyros</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Fast R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Finding action tubes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">DRAW: A recurrent neural network for image generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karol</forename><surname>Gregor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivo</forename><surname>Danihelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><surname>Jimenez Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">AVA: A video dataset of spatio-temporally localized atomic visual actions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhui</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudheendra</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caroline</forename><surname>Pantofaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yeqing</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Susanna</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Ricco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Can spatiotemporal 3D CNNs retrace the history of 2D CNNs and ImageNet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kensho</forename><surname>Hara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hirokatsu</forename><surname>Kataoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutaka</forename><surname>Satoh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Fast temporal activity proposals for efficient detection of human actions in untrimmed videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><surname>Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><surname>Niebles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Tube convolutional neural network (T-CNN) for action detection in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Action tubelet detector for spatiotemporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vicky</forename><surname>Kalogeiton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philippe</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vittorio</forename><surname>Ferrari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chloe</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudheendra</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Back</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Natsev</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.06950</idno>
		<title level="m">The Kinetics human action video dataset</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Recurrent tubelet proposal and recognition networks for action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaofan</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">SSD: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Yang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation with residual transfer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Budget-aware activity detection with a recurrent policy network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Behrooz</forename><surname>Mahasseni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavlo</forename><surname>Molchanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">An iterative grid based object detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahyar</forename><surname>Najibi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Rastegari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry</forename><forename type="middle">Davis</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">. G-Cnn</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Beyond short snippets: Deep networks for video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue-Hei</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Hausknecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudheendra</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajat</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Toderici</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Multi-region twostream R-CNN for action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojiang</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deep learning for detecting multiple space-time action tubes in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suman</forename><surname>Saha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gurkirt</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Sapienza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Cuzzolin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Online real-time multiple spatiotemporal action localisation and prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gurkirt</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suman</forename><surname>Saha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Sapienza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Cuzzolin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">UCF101: A dataset of 101 human actions classes from videos in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khurram</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Amir Roshan Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shah</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.0402</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Actor-centric relation network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning to track for spatio-temporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philippe</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zaid</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">R-C3D: Region convolutional 3D network for temporal activity detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huijuan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abir</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Multilayer and multimodal fusion of deep neural networks for video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavlo</forename><surname>Molchanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Making convolutional networks recurrent for visual sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavlo</forename><surname>Molchanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Spatiotemporal action detection with cascade proposal and location anticipation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiyang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ram</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Discovering spatio-temporal action tubes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuancheng</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingli</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JVCI</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
