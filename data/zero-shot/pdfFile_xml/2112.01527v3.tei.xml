<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Masked-attention Mask Transformer for Universal Image Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Cheng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research (FAIR)</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Illinois at Urbana-Champaign (UIUC)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research (FAIR)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Illinois at Urbana-Champaign (UIUC)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research (FAIR)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohit</forename><surname>Girdhar</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research (FAIR)</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Masked-attention Mask Transformer for Universal Image Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T14:44+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Image segmentation groups pixels with different semantics, e.g., category or instance membership. Each choice of semantics defines a task. While only the semantics of each task differ, current research focuses on designing specialized architectures for each task. We present Maskedattention Mask Transformer (Mask2Former), a new architecture capable of addressing any image segmentation task (panoptic, instance or semantic). Its key components include masked attention, which extracts localized features by constraining cross-attention within predicted mask regions. In addition to reducing the research effort by at least three times, it outperforms the best specialized architectures by a significant margin on four popular datasets. Most notably, Mask2Former sets a new state-of-the-art for panoptic segmentation (57.8 PQ on COCO), instance segmentation (50.1 AP on COCO) and semantic segmentation (57.7 mIoU on ADE20K).</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Image segmentation studies the problem of grouping pixels. Different semantics for grouping pixels, e.g., category or instance membership, have led to different types of segmentation tasks, such as panoptic, instance or semantic segmentation. While these tasks differ only in semantics, current methods develop specialized architectures for each task. Per-pixel classification architectures based on Fully Convolutional Networks (FCNs) <ref type="bibr" target="#b36">[37]</ref> are used for semantic segmentation, while mask classification architectures <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b23">24]</ref> that predict a set of binary masks each associated with a single category, dominate instance-level segmentation. Although such specialized architectures <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b36">37</ref>] have advanced each individual task, they lack the flexibility to generalize to the other tasks. For example, FCN-based architectures struggle at instance segmentation, leading to the evolution of different architectures for instance segmentation compared to semantic segmentation. Thus, duplicate research and (hardware) optimization effort is spent on each * Work done during an internship at Facebook AI Research.  Although recent work has proposed universal architectures that attempt all tasks and are competitive on semantic and panoptic segmentation, they struggle with segmenting instances. We propose Mask2Former, which, for the first time, outperforms the best specialized architectures on three studied segmentation tasks on multiple datasets. specialized architecture for every task.</p><p>To address this fragmentation, recent work <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b61">62]</ref> has attempted to design universal architectures, that are capable of addressing all segmentation tasks with the same architecture (i.e., universal image segmentation). These architectures are typically based on an end-to-end set prediction objective (e.g., DETR <ref type="bibr" target="#b4">[5]</ref>), and successfully tackle multiple tasks without modifying the architecture, loss, or the training procedure. Note, universal architectures are still trained separately for different tasks and datasets, albeit having the same architecture. In addition to being flexible, universal architectures have recently shown state-of-the-art results on semantic and panoptic segmentation <ref type="bibr" target="#b13">[14]</ref>. However, recent work still focuses on advancing specialized architectures <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b44">45]</ref>, which raises the question: why haven't universal architectures replaced specialized ones?</p><p>Although existing universal architectures are flexible enough to tackle any segmentation task, as shown in <ref type="figure" target="#fig_1">Figure 1</ref>, in practice their performance lags behind the best specialized architectures. For instance, the best reported performance of universal architectures <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b61">62]</ref>, is currently lower (&gt; 9 AP) than the SOTA specialized architecture for instance segmentation <ref type="bibr" target="#b5">[6]</ref>. Beyond the inferior performance, universal architectures are also harder to train. They typically require more advanced hardware and a much longer training schedule. For example, training Mask-Former <ref type="bibr" target="#b13">[14]</ref> takes 300 epochs to reach 40.1 AP and it can only fit a single image in a GPU with 32G memory. In contrast, the specialized Swin-HTC++ <ref type="bibr" target="#b5">[6]</ref> obtains better performance in only 72 epochs. Both the performance and training efficiency issues hamper the deployment of universal architectures.</p><p>In this work, we propose a universal image segmentation architecture named Masked-attention Mask Transformer (Mask2Former) that outperforms specialized architectures across different segmentation tasks, while still being easy to train on every task. We build upon a simple meta architecture <ref type="bibr" target="#b13">[14]</ref> consisting of a backbone feature extractor <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b35">36]</ref>, a pixel decoder <ref type="bibr" target="#b32">[33]</ref> and a Transformer decoder <ref type="bibr" target="#b50">[51]</ref>. We propose key improvements that enable better results and efficient training. First, we use masked attention in the Transformer decoder which restricts the attention to localized features centered around predicted segments, which can be either objects or regions depending on the specific semantic for grouping. Compared to the cross-attention used in a standard Transformer decoder which attends to all locations in an image, our masked attention leads to faster convergence and improved performance. Second, we use multi-scale high-resolution features which help the model to segment small objects/regions. Third, we propose optimization improvements such as switching the order of self and cross-attention, making query features learnable, and removing dropout; all of which improve performance without additional compute. Finally, we save 3? training memory without affecting the performance by calculating mask loss on few randomly sampled points. These improvements not only boost the model performance, but also make training significantly easier, making universal architectures more accessible to users with limited compute.</p><p>We evaluate Mask2Former on three image segmentation tasks (panoptic, instance and semantic segmentation) using four popular datasets (COCO <ref type="bibr" target="#b34">[35]</ref>, Cityscapes <ref type="bibr" target="#b15">[16]</ref>, ADE20K <ref type="bibr" target="#b64">[65]</ref> and Mapillary Vistas <ref type="bibr" target="#b41">[42]</ref>). For the first time, on all these benchmarks, our single architecture performs on par or better than specialized architectures. Mask2Former sets the new state-of-the-art of 57.8 PQ on COCO panoptic segmentation <ref type="bibr" target="#b27">[28]</ref>, 50.1 AP on COCO instance segmentation <ref type="bibr" target="#b34">[35]</ref> and 57.7 mIoU on ADE20K semantic segmentation <ref type="bibr" target="#b64">[65]</ref> using the exact same architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Specialized semantic segmentation architectures typically treat the task as a per-pixel classification problem. <ref type="bibr" target="#b36">[37]</ref> independently predict a category label for every pixel. Follow-up methods find context to play an important role for precise per-pixel classification and focus on designing customized context modules <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b62">63]</ref> or self-attention variants <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b60">61,</ref><ref type="bibr" target="#b63">64]</ref>. Specialized instance segmentation architectures are typically based upon "mask classification." They predict a set of binary masks each associated with a single class label. The pioneering work, Mask R-CNN <ref type="bibr" target="#b23">[24]</ref>, generates masks from detected bounding boxes. Follow-up methods either focus on detecting more precise bounding boxes <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b5">6]</ref>, or finding new ways to generate a dynamic number of masks, e.g., using dynamic kernels <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b55">56]</ref> or clustering algorithms <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b28">29]</ref>. Although the performance has been advanced in each task, these specialized innovations lack the flexibility to generalize from one to the other, leading to duplicated research effort. For instance, although multiple approaches have been proposed for building feature pyramid representations <ref type="bibr" target="#b32">[33]</ref>, as we show in our experiments, BiFPN <ref type="bibr" target="#b46">[47]</ref> performs better for instance segmentation while FaPN <ref type="bibr" target="#b38">[39]</ref> performs better for semantic segmentation. Panoptic segmentation has been proposed to unify both semantic and instance segmentation tasks <ref type="bibr" target="#b27">[28]</ref>. Architectures for panoptic segmentation either combine the best of specialized semantic and instance segmentation architectures into a single framework <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b59">60]</ref> or design novel objectives that equally treat semantic regions and instance objects <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b51">52]</ref>. Despite those new architectures, researchers continue to develop specialized architectures for different image segmentation tasks <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b44">45]</ref>. We find panoptic architectures usually only report performance on a single panoptic segmentation task <ref type="bibr" target="#b51">[52]</ref>, which does not guarantee good performance on other tasks ( <ref type="figure" target="#fig_1">Figure 1</ref>). For example, panoptic segmentation does not measure architectures' abilities to rank predictions as instance segmentations. Thus, we refrain from referring to architectures that are only evaluated for panoptic segmentation as universal architectures. Instead, here, we evaluate our Mask2Former on all studied tasks to guarantee generalizability. Universal architectures have emerged with DETR <ref type="bibr" target="#b4">[5]</ref> and show that mask classification architectures with an end-toend set prediction objective are general enough for any image segmentation task. MaskFormer <ref type="bibr" target="#b13">[14]</ref> shows that mask classification based on DETR not only performs well on panoptic segmentation but also achieves state-of-the-art on semantic segmentation. K-Net <ref type="bibr" target="#b61">[62]</ref> further extends set prediction to instance segmentation. Unfortunately, these architectures fail to replace specialized models as their performance on particular tasks or datasets is still worse than the best specialized architecture (e.g., MaskFormer <ref type="bibr" target="#b13">[14]</ref> cannot segment instances well). To our knowledge, Mask2Former is the first architecture that outperforms state-of-the-art specialized architectures on all considered tasks and datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>FCN-based architectures</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Masked-attention Mask Transformer</head><p>We now present Mask2Former. We first review a meta architecture for mask classification that Mask2Former is built upon. Then, we introduce our new Transformer decoder with masked attention which is the key to better convergence and results. Lastly, we propose training improvements that make Mask2Former efficient and accessible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Mask classification preliminaries</head><p>Mask classification architectures group pixels into N segments by predicting N binary masks, along with N corresponding category labels. Mask classification is sufficiently general to address any segmentation task by assigning different semantics, e.g., categories or instances, to different segments. However, the challenge is to find good representations for each segment. For example, Mask R-CNN <ref type="bibr" target="#b23">[24]</ref> uses bounding boxes as the representation which limits its application to semantic segmentation. Inspired by DETR <ref type="bibr" target="#b4">[5]</ref>, each segment in an image can be represented as a C-dimensional feature vector ("object query") and can be processed by a Transformer decoder, trained with a set prediction objective. A simple meta architecture would consist of three components. A backbone that extracts lowresolution features from an image. A pixel decoder that gradually upsamples low-resolution features from the output of the backbone to generate high-resolution per-pixel embeddings. And finally a Transformer decoder that operates on image features to process object queries. The final binary mask predictions are decoded from per-pixel embeddings with object queries. One successful instantiation of such a meta architecture is MaskFormer <ref type="bibr" target="#b13">[14]</ref>, and we refer readers to <ref type="bibr" target="#b13">[14]</ref> for more details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Transformer decoder with masked attention</head><p>Mask2Former adopts the aforementioned meta architecture, with our proposed Transformer decoder <ref type="figure" target="#fig_7">(Figure 2</ref> right) replacing the standard one. The key components of our Transformer decoder include a masked attention operator, which extracts localized features by constraining crossattention to within the foreground region of the predicted mask for each query, instead of attending to the full feature map. To handle small objects, we propose an efficient multi-scale strategy to utilize high-resolution features. It feeds successive feature maps from the pixel decoder's feature pyramid into successive Transformer decoder layers in a round robin fashion. Finally, we incorporate optimization improvements that boost model performance without introducing additional computation. We now discuss these improvements in detail.  <ref type="figure" target="#fig_7">Figure 2</ref>. Mask2Former overview. Mask2Former adopts the same meta architecture as MaskFormer <ref type="bibr" target="#b13">[14]</ref> with a backbone, a pixel decoder and a Transformer decoder. We propose a new Transformer decoder with masked attention instead of the standard cross-attention (Section 3.2.1). To deal with small objects, we propose an efficient way of utilizing high-resolution features from a pixel decoder by feeding one scale of the multi-scale feature to one Transformer decoder layer at a time (Section 3.2.2). In addition, we switch the order of self and cross-attention (i.e., our masked attention), make query features learnable, and remove dropout to make computation more effective (Section 3.2.3). Note that positional embeddings and predictions from intermediate Transformer decoder layers are omitted in this figure for readability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Masked attention</head><p>Context features have been shown to be important for image segmentation <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b62">63]</ref>. However, recent studies <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b45">46]</ref> suggest that the slow convergence of Transformer-based models is due to global context in the cross-attention layer, as it takes many training epochs for cross-attention to learn to attend to localized object regions <ref type="bibr" target="#b45">[46]</ref>. We hypothesize that local features are enough to update query features and context information can be gathered through self-attention. For this we propose masked attention, a variant of crossattention that only attends within the foreground region of the predicted mask for each query.</p><p>Standard cross-attention (with residual path) computes</p><formula xml:id="formula_0">X l = softmax(Q l K T l )V l + X l?1 .<label>(1)</label></formula><p>Here, l is the layer index, X l ? R N ?C refers to N C-dimensional query features at the l th layer and Q l = f Q (X l?1 ) ? R N ?C . X 0 denotes input query features to the Transformer decoder. K l , V l ? R H l W l ?C are the image features under transformation f K (?) and f V (?) respectively, and H l and W l are the spatial resolution of image features that we will introduce next in Section 3.2.2. f Q , f K and f V are linear transformations.</p><p>Our masked attention modulates the attention matrix via</p><formula xml:id="formula_1">X l = softmax(M l?1 + Q l K T l )V l + X l?1 .<label>(2)</label></formula><p>Moreover, the attention mask M l?1 at feature location (x, y) is</p><formula xml:id="formula_2">M l?1 (x, y) = 0 if M l?1 (x, y) = 1 ?? otherwise .<label>(3)</label></formula><p>Here, M l?1 ? {0, 1} N ?H l W l is the binarized output (thresholded at 0.5) of the resized mask prediction of the previous (l ? 1)-th Transformer decoder layer. It is resized to the same resolution of K l . M 0 is the binary mask prediction obtained from X 0 , i.e., before feeding query features into the Transformer decoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">High-resolution features</head><p>High-resolution features improve model performance, especially for small objects <ref type="bibr" target="#b4">[5]</ref>. However, this is computationally demanding. Thus, we propose an efficient multi-scale strategy to introduce high-resolution features while controlling the increase in computation. Instead of always using the high-resolution feature map, we utilize a feature pyramid which consists of both low-and high-resolution features and feed one resolution of the multi-scale feature to one Transformer decoder layer at a time. Specifically, we use the feature pyramid produced by the pixel decoder with resolution 1/32, 1/16 and 1/8 of the original image. For each resolution, we add both a sinusoidal positional embedding e pos ? R H l W l ?C , following <ref type="bibr" target="#b4">[5]</ref>, and a learnable scale-level embedding e lvl ? R 1?C , following <ref type="bibr" target="#b65">[66]</ref>. We use those, from lowest-resolution to highest-resolution for the corresponding Transformer decoder layer as shown in <ref type="figure" target="#fig_7">Figure 2</ref> left. We repeat this 3-layer Transformer decoder L times. Our final Transformer decoder hence has 3L layers. More specifically, the first three layers receive a feature map of resolution H 1 = H/32, H 2 = H/16, H 3 = H/8 and W 1 = W/32, W 2 = W/16, W 3 = W/8, where H and W are the original image resolution. This pattern is repeated in a round robin fashion for all following layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Optimization improvements</head><p>A standard Transformer decoder layer <ref type="bibr" target="#b50">[51]</ref> consists of three modules to process query features in the following order: a self-attention module, a cross-attention and a feed-forward network (FFN). Moreover, query features (X 0 ) are zero initialized before being fed into the Transformer decoder and are associated with learnable positional embeddings. Furthermore, dropout is applied to both residual connections and attention maps.</p><p>To optimize the Transformer decoder design, we make the following three improvements. First, we switch the order of self-and cross-attention (our new "masked attention") to make computation more effective: query features to the first self-attention layer are image-independent and do not have signals from the image, thus applying selfattention is unlikely to enrich information. Second, we make query features (X 0 ) learnable as well (we still keep the learnable query positional embeddings), and learnable query features are directly supervised before being used in the Transformer decoder to predict masks (M 0 ). We find these learnable query features function like a region proposal network <ref type="bibr" target="#b42">[43]</ref> and have the ability to generate mask proposals. Finally, we find dropout is not necessary and usually decreases performance. We thus completely remove dropout in our decoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Improving training efficiency</head><p>One limitation of training universal architectures is the large memory consumption due to high-resolution mask prediction, making them less accessible than the more memory-friendly specialized architectures <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b23">24]</ref>. For example, MaskFormer <ref type="bibr" target="#b13">[14]</ref> can only fit a single image in a GPU with 32G memory. Motivated by PointRend <ref type="bibr" target="#b29">[30]</ref> and Implicit PointRend <ref type="bibr" target="#b12">[13]</ref>, which show a segmentation model can be trained with its mask loss calculated on K randomly sampled points instead of the whole mask, we calculate the mask loss with sampled points in both the matching and the final loss calculation. More specifically, in the matching loss that constructs the cost matrix for bipartite matching, we uniformly sample the same set of K points for all prediction and ground truth masks. In the final loss between predictions and their matched ground truths, we sample different sets of K points for different pairs of prediction and ground truth using importance sampling <ref type="bibr" target="#b29">[30]</ref>. We set K = 12544, i.e., 112 ? 112 points. This new training strategy effectively reduces training memory by 3?, from 18GB to 6GB per image, making Mask2Former more accessible to users with limited computational resources.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We demonstrate Mask2Former is an effective architecture for universal image segmentation through comparisons with specialized state-of-the-art architectures on standard benchmarks. We evaluate our proposed design decisions through ablations on all three tasks. Finally we show Mask2Former generalizes beyond the standard benchmarks, obtaining state-of-the-art results on four datasets. Datasets. We study Mask2Former using four widely used image segmentation datasets that support semantic, instance and panoptic segmentation: COCO <ref type="bibr" target="#b34">[35]</ref> (80 "things" and 53 "stuff" categories), ADE20K <ref type="bibr" target="#b64">[65]</ref> (100 "things" and 50 "stuff" categories), Cityscapes <ref type="bibr" target="#b15">[16]</ref> (8 "things" and 11 "stuff" categories) and Mapillary Vistas <ref type="bibr" target="#b41">[42]</ref> (37 "things" and 28 "stuff" categories). Panoptic and semantic seg- mentation tasks are evaluated on the union of "things" and "stuff" categories while instance segmentation is only evaluated on the "things" categories. Evaluation metrics. For panoptic segmentation, we use the standard PQ (panoptic quality) metric <ref type="bibr" target="#b27">[28]</ref>. We further report AP Th pan , which is the AP evaluated on the "thing" categories using instance segmentation annotations, and mIoU pan , which is the mIoU for semantic segmentation by merging instance masks from the same category, of the same model trained only with panoptic segmentation annotations. For instance segmentation, we use the standard AP (average precision) metric <ref type="bibr" target="#b34">[35]</ref>. For semantic segmentation, we use mIoU (mean Intersection-over-Union) <ref type="bibr" target="#b18">[19]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Implementation details</head><p>We adopt settings from <ref type="bibr" target="#b13">[14]</ref> with the following differences: Pixel decoder. Mask2Former is compatible with any existing pixel decoder module. In MaskFormer <ref type="bibr" target="#b13">[14]</ref>, FPN <ref type="bibr" target="#b32">[33]</ref> is chosen as the default for its simplicity. Since our goal is to demonstrate strong performance across different segmentation tasks, we use the more advanced multi-scale deformable attention Transformer (MSDeformAttn) <ref type="bibr" target="#b65">[66]</ref> as our default pixel decoder. Specifically, we use 6 MSDefor-mAttn layers applied to feature maps with resolution 1/8, 1/16 and 1/32, and use a simple upsampling layer with lateral connection on the final 1/8 feature map to generate the feature map of resolution 1/4 as the per-pixel embedding. In our ablation study, we show that this pixel decoder provides best results across different segmentation tasks. Transformer decoder. We use our Transformer decoder proposed in Section 3.2 with L = 3 (i.e., 9 layers total) and 100 queries by default. An auxiliary loss is added to every intermediate Transformer decoder layer and to the learnable query features before the Transformer decoder. Loss weights. We use the binary cross-entropy loss (instead of focal loss <ref type="bibr" target="#b33">[34]</ref> in <ref type="bibr" target="#b13">[14]</ref>) and the dice loss <ref type="bibr" target="#b40">[41]</ref> for our mask loss: L mask = ? ce L ce + ? dice L dice . We set ? ce = 5.0 and ? dice = 5.0. The final loss is a combination of mask loss and classification loss: L mask + ? cls L cls and we set ? cls = 2.0 for predictions matched with a ground truth and 0.1 for the "no object," i.e., predictions that have not been matched with any ground truth. Post-processing. We use the exact same post-processing as <ref type="bibr" target="#b13">[14]</ref> to acquire the expected output format for panoptic and semantic segmentation from pairs of binary masks and class predictions. Instance segmentation requires additional confidence scores for each prediction. We multiply class confidence and mask confidence (i.e., averaged foreground per-pixel binary mask probability) for a final confidence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Training settings</head><p>Panoptic and instance segmentation. We use Detec-tron2 <ref type="bibr" target="#b56">[57]</ref> and follow the updated Mask R-CNN <ref type="bibr" target="#b23">[24]</ref> baseline settings 1 for the COCO dataset. More specifically, we use AdamW <ref type="bibr" target="#b37">[38]</ref> optimizer and the step learning rate schedule. We use an initial learning rate of 0.0001 and a weight decay of 0.05 for all backbones. A learning rate multiplier of 0.1 is applied to the backbone and we decay the learning rate at 0.9 and 0.95 fractions of the total number of training steps by a factor of 10. If not stated otherwise, we train our models for 50 epochs with a batch size of 16. For data augmentation, we use the large-scale jittering (LSJ) augmentation <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b22">23]</ref> with a random scale sampled from range 0.1 to 2.0 followed by a fixed size crop to 1024?1024. We use the standard Mask R-CNN inference setting where we resize an image with shorter side to 800 and longer side up-to 1333. We also report FLOPs and fps. FLOPs are averaged over 100 validation images (COCO images have varying sizes). Frames-per-second (fps) is measured on a V100 GPU with a batch size of 1 by taking the average runtime on the entire validation set including post-processing time. Semantic segmentation. We follow the same settings as <ref type="bibr" target="#b13">[14]</ref> to train our models, except: 1) a learning rate multiplier of 0.1 is applied to both CNN and Transformer backbones instead of only applying it to CNN backbones in <ref type="bibr" target="#b13">[14]</ref>, 2) both ResNet and Swin backbones use an initial learning rate of 0.0001 and a weight decay of 0.05, instead of using Beyond the PQ metric, our Mask2Former also achieves higher performance on two other metrics compared to DETR <ref type="bibr" target="#b4">[5]</ref> and MaskFormer: AP Th pan , which is the AP evaluated on the 80 "thing" categories using instance segmentation annotation, and mIoU pan , which is the mIoU evaluated on the 133 categories for semantic segmentation converted from panoptic segmentation annotation. This shows Mask2Former's universality: trained only with panoptic segmentation annotations, it can be used for instance and semantic segmentation. Instance segmentation. We compare Mask2Former with state-of-the-art models on the COCO <ref type="bibr" target="#b34">[35]</ref>   all the highest gains come from large objects (+10.6 AP L ). The performance on AP S still lags behind other state-of-theart models. Hence there still remains room for improvement on small objects, e.g., by using dilated backbones like in DETR <ref type="bibr" target="#b4">[5]</ref>, which we leave for future work. Semantic segmentation. We compare Mask2Former with state-of-the-art models for semantic segmentation on the ADE20K [65] dataset in <ref type="table" target="#tab_3">Table 3</ref>. Mask2Former outperforms MaskFormer <ref type="bibr" target="#b13">[14]</ref> across different backbones, suggesting that the proposed improvements even boost semantic segmentation results where <ref type="bibr" target="#b13">[14]</ref> was already state-ofthe-art. With Swin-L as backbone and FaPN <ref type="bibr" target="#b38">[39]</ref> as pixel decoder, Mask2Former sets a new state-of-the-art of 57.7 mIoU. We also report the test set results in Appendix A.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Ablation studies</head><p>We now analyze Mask2Former through a series of ablation studies using a ResNet-50 backbone <ref type="bibr" target="#b24">[25]</ref>. To test the generality of the proposed components for universal image segmentation, all ablations are performed on three tasks.  Transformer decoder. We validate the importance of each component by removing them one at a time. As shown in <ref type="table" target="#tab_5">Table 4a</ref>, masked attention leads to the biggest improvement across all tasks. The improvement is larger for instance and panoptic segmentation than for semantic segmentation. Moreover, using high-resolution features from the efficient multi-scale strategy is also important. <ref type="table" target="#tab_5">Table 4b</ref> shows additional optimization improvements further improve the performance without extra computation. Masked attention. Concurrent work has proposed other variants of cross-attention <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b39">40]</ref> that aim to improve the convergence and performance of DETR <ref type="bibr" target="#b4">[5]</ref> for object detection. Most recently, K-Net <ref type="bibr" target="#b61">[62]</ref> replaced cross-attention with a mask pooling operation that averages features within mask regions. We validate the importance of our masked attention in <ref type="table" target="#tab_5">Table 4c</ref>. While existing cross-attention variants may improve on a specific task, our masked attention performs the best on all three tasks. Feature resolution. <ref type="table" target="#tab_5">Table 4d</ref> shows that Mask2Former benefits from using high-resolution features (e.g., a single scale of 1/8) in the Transformer decoder. However, this introduces additional computation. Our efficient multi-scale (efficient m.s.) strategy effectively reduces the FLOPs without affecting the performance. Note that, naively concatenating multi-scale features as input to every Transformer decoder layer (na?ve m.s.) does not yield additional gains. Pixel decoder. As shown in <ref type="table" target="#tab_5">Table 4e</ref>, Mask2Former is compatible with any existing pixel decoder. However, we observe different pixel decoders specialize in different tasks: while BiFPN <ref type="bibr" target="#b46">[47]</ref> performs better on instance-level segmentation, FaPN <ref type="bibr" target="#b38">[39]</ref> works better for semantic segmentation. Among all studied pixel decoders, the MSDefor-maAttn <ref type="bibr" target="#b65">[66]</ref> consistently performs the best across all tasks and thus is selected as our default. This set of ablations also suggests that designing a module like a pixel decoder for a specific task does not guarantee generalization across segmentation tasks. Mask2Former, as a universal model, could serve as a testbed for a generalizable module design.</p><p>Calculating loss with points vs. masks. In <ref type="table" target="#tab_6">Table 5</ref> we study the performance and memory implications when calculating the loss based on either mask or sampled points. Calculating the final training loss with sampled points reduces training memory by 3? without affecting the performance. Additionally, calculating the matching loss with sampled points improves performance across all three tasks. Learnable queries as region proposals. Region proposals <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b49">50]</ref>, either in the form of boxes or masks, are regions that are likely to be "objects." With learnable queries being supervised by the mask loss, predictions from learnable queries can serve as mask proposals. In <ref type="figure" target="#fig_4">Figure 3</ref> top, we visualize mask predictions of selected learnable queries before feeding them into the Transformer decoder (the proposal generation process is shown in <ref type="figure" target="#fig_4">Figure 3</ref> bottom right). In <ref type="figure" target="#fig_4">Figure 3</ref> bottom left, we further perform a quantitative analysis on the quality of these proposals by calculating the class-agnostic average recall with 100 predictions (AR@100) on COCO val2017. We find these learnable queries already achieve good AR@100 compared to the fi-   nal predictions of Mask2Former after the Transformer decoder layers, i.e., layer 9, and AR@100 consistently improves with more decoder layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Generalization to other datasets</head><p>To show our Mask2Former can generalize beyond the COCO dataset, we further perform experiments on other popular image segmentation datasets. In <ref type="table" target="#tab_7">Table 6</ref>, we show results on Cityscapes <ref type="bibr" target="#b15">[16]</ref>. Please see Appendix B for detailed training settings on each dataset as well as more results on ADE20K <ref type="bibr" target="#b64">[65]</ref> and Mapillary Vistas <ref type="bibr" target="#b41">[42]</ref>.  <ref type="table">Table 7</ref>. Limitations of Mask2Former. Although a single Mask2Former can address any segmentation task, we still need to train it on different tasks. Across three datasets we find Mask2Former trained with panoptic annotations performs slightly worse than the exact same model trained specifically for instance and semantic segmentation tasks with the corresponding data.</p><p>We observe that our Mask2Former is competitive to state-of-the-art methods on these datasets as well. It suggests Mask2Former can serve as a universal image segmentation model and results generalize across datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Limitations</head><p>Our ultimate goal is to train a single model for all image segmentation tasks. In <ref type="table">Table 7</ref>, we find Mask2Former trained on panoptic segmentation only performs slightly worse than the exact same model trained with the corresponding annotations for instance and semantic segmentation tasks across three datasets. This suggests that even though Mask2Former can generalize to different tasks, it still needs to be trained for those specific tasks. In the future, we hope to develop a model that can be trained only once for multiple tasks and even for multiple datasets.</p><p>Furthermore, as seen in <ref type="table" target="#tab_1">Tables 2 and 4d</ref>, even though it improves over baselines, Mask2Former struggles with segmenting small objects and is unable to fully leverage multiscale features. We believe better utilization of the feature pyramid and designing losses for small objects are critical.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We present Mask2Former for universal image segmentation. Built upon a simple meta framework <ref type="bibr" target="#b13">[14]</ref> with a new Transformer decoder using the proposed masked attention, Mask2Former obtains top results in all three major image segmentation tasks (panoptic, instance and semantic) on four popular datasets, outperforming even the best specialized models designed for each benchmark while remaining easy to train. Mask2Former saves 3? research effort compared to designing specialized models for each task, and it is accessible to users with limited computational resources. We hope to attract interest in universal model design.</p><p>Ethical considerations: While our technical innovations do not appear to have any inherent biases, the models trained with our approach on realworld datasets should undergo ethical review to ensure the predictions do not propagate problematic stereotypes, and the approach is not used for applications including but not limited to illegal surveillance. Acknowledgments: Thanks to Nicolas Carion and Xingyi Zhou for helpful feedback. BC and AS are supported in part by NSF #1718221, 2008387, 2045586, 2106825, MRI #1725729, NIFA 2020-67021-32799 and Cisco Systems Inc. (CG 1377144 -thanks for access to Arcetri).</p><p>We first provide more results for Mask2Former with different backbones as well as test-set performance on standard benchmarks (Appendix A): We use COCO panoptic <ref type="bibr" target="#b27">[28]</ref> for panoptic, COCO <ref type="bibr" target="#b34">[35]</ref> for instance, and ADE20K <ref type="bibr" target="#b64">[65]</ref> for semantic segmentation. Then, we provide more detailed results on additional datasets (Appendix B). Finally, we provide additional ablation studies (Appendix C) and visualization of Mask2Former predictions for all three segmentation tasks (Appendix D).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Additional results</head><p>Here, we provide more results of Mask2Former with different backbones on COCO panoptic <ref type="bibr" target="#b27">[28]</ref> for panoptic segmentation, COCO <ref type="bibr" target="#b34">[35]</ref> for instance segmentation and ADE20K <ref type="bibr" target="#b64">[65]</ref> for semantic segmentation. More specifically, for each benckmark, we evaluate Mask2Former with ResNet <ref type="bibr" target="#b24">[25]</ref> with 50 and 101 layers, as well as Swin <ref type="bibr" target="#b35">[36]</ref> Tiny, Small, Base and Large variants as backbones. We use ImageNet <ref type="bibr" target="#b43">[44]</ref> pre-trained checkpoints to initialize backbones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1. Panoptic segmentation.</head><p>In <ref type="table" target="#tab_11">Table I</ref>, we report Mask2Former with various backbones on COCO panoptic val2017. Mask2Former outperforms all existing panoptic segmentation models with various backbones. Our best model sets a new state-of-theart of 57.8 PQ.</p><p>In <ref type="table" target="#tab_11">Table II</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Instance segmentation.</head><p>In <ref type="table" target="#tab_11">Table III</ref>, we report Mask2Former results obtained with various backbones on COCO val2017. Mask2Former outperforms the best single-scale model, HTC++ <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b35">36]</ref>. Note that it is non-trivial to do multi-scale inference for instance-level segmentation tasks without introducing complex post-processing like non-maximum suppression. Thus, we only compare Mask2Former with other single-scale inference models. We believe multi-scale inference can further improve Mask2Former performance and it remains an interesting future work.</p><p>In <ref type="table" target="#tab_11">Table IV</ref>, we further report the best Mask2Former model on the test-dev set. Mask2Former achieves the absolute new state-of-the-art performance on both validation and test set. On the one hand, Mask2Former is extremely good at segmenting large objects: we can even outperform the challenge winner (which uses extra training data, model ensemble, etc.) on AP L by a large margin without any bells-and-whistles. On the other hand, the poor performance on small objects leaves room for further improvement in the future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3. Semantic segmentation.</head><p>In <ref type="table" target="#tab_16">Table V</ref>, we report Mask2Former results obtained with various backbones on ADE20K val. Mask2Former outperforms all existing semantic segmentation models with various backbones. Our best model sets a new stateof-the-art of 57.7 mIoU.</p><p>In <ref type="table" target="#tab_11">Table VI</ref>, we further report the best Mask2Former model on the test set.</p><p>Following <ref type="bibr" target="#b13">[14]</ref>, we train Mask2Former on the union of ADE20K train and val set with ImageNet-22K pre-trained checkpoint and use multi-scale inference. Mask2Former is able to outperform previous state-of-the-art methods on all metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Additional datasets</head><p>We study Mask2Former on three image segmentation tasks (panoptic, instance and semantic segmentation) using four datasets. Here we report additional results on Cityscapes <ref type="bibr" target="#b15">[16]</ref>, ADE20K <ref type="bibr" target="#b64">[65]</ref> and Mapillary Vistas <ref type="bibr" target="#b41">[42]</ref> as well as more detailed training settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1. Cityscapes</head><p>Cityscapes is an urban egocentric street-view dataset with high-resolution images (1024 ? 2048 pixels). It contains 2975 images for training, 500 images for validation and 1525 images for testing with a total of 19 classes. Training settings. For all three segmentation tasks: we use a crop size of 512 ? 1024, a batch size of 16 and train all models for 90k iterations. During inference, we operate on the whole image (1024 ? 2048). Other implementation details largely follow Section 4.1 (panoptic and instance segmentation follow semantic segmentation training settings), except that we use 200 queries for panoptic and instance segmentation models with Swin-L backbone. All other backbones or semantic segmentation models use 100 queries. Results. In <ref type="table" target="#tab_11">Table VII</ref>, we report Mask2Former results obtained with various backbones on Cityscapes for three segmentation tasks and compare it with other state-of-the-art methods without using extra data. For panoptic segmentation, Mask2Former with Swin-L backbone outperforms the state-of-the-art Panoptic-DeepLab <ref type="bibr" target="#b10">[11]</ref> with SWideRnet <ref type="bibr" target="#b8">[9]</ref> using single-scale inference. For semantic segmentation, Mask2Former with Swin-B backbone outperforms the state-of-the-art SegFormer <ref type="bibr" target="#b58">[59]</ref>.  Besides PQ for panoptic segmentation, we also report AP Th pan (the AP evaluated on the 80 "thing" categories using instance segmentation annotation) and mIoUpan (the mIoU evaluated on the 133 categories for semantic segmentation converted from panoptic segmentation annotation) of the same model trained for panoptic segmentation (note: we train all our models with panoptic segmentation annotation only). Backbones pre-trained on ImageNet-22K are marked with ? . </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2. ADE20K</head><p>Training settings. For panoptic and instance segmentation, we use the exact same training parameters as we used for semantic segmentation, except that we always use a crop size of 640 ? 640 for all backbones. Other implementation details largely follow Section 4.1 , except that we use 200 queries for panoptic and instance segmentation models with Swin-L backbone. All other backbones or semantic segmentation models use 100 queries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results.</head><p>In <ref type="table" target="#tab_11">Table VIII</ref>, we report the results of Mask2Former obtained with various backbones on ADE20K for three segmentation tasks and compare it with other state-of-the-art methods. Mask2Former with Swin-L backbone sets a new state-of-the-art performance on ADE20K for panoptic segmentation. As there are few papers reporting results on ADE20K, we hope this experiment could set up a useful benchmark for future research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3. Mapillary Vistas</head><p>Mapillary Vistas is a large-scale urban street-view dataset with 18k, 2k and 5k images for training, validation and testing. It contains images with a variety of resolutions, ranging from 1024 ? 768 to 4000 ? 6000. We only report panoptic and semantic segmentation results for this dataset. Training settings. For both panoptic and semantic segmentation, we follow the same data augmentation of <ref type="bibr" target="#b13">[14]</ref>: standard random scale jittering between 0.5 and 2.0, random horizontal flipping, random cropping with a crop size of 1024 ? 1024 as well as random color jittering. We train  our model for 300k iterations with a batch size of 16 using the "poly" learning rate schedule <ref type="bibr" target="#b6">[7]</ref>. During inference, we resize the longer side to 2048 pixels. Our panoptic segmentation model with a Swin-L backbone uses 200 queries. All other backbones or semantic segmentation models use 100 queries.</p><p>Results. In <ref type="table" target="#tab_11">Table IX</ref>, we report Mask2Former results obtained with various backbones on Mapillary Vistas for panoptic and semantic segmentation tasks and compare it with other state-of-the-art methods. Our Mask2Former is very competitive compared to state-of-the art specialized models even if it is not designed for Mapillary Vistas.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Additional ablation studies</head><p>We perform additional ablation studies of Mask2Former using the same settings that we used in the main paper: a single ResNet-50 backbone <ref type="bibr" target="#b24">[25]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1. Convergence analysis</head><p>We train Mask2Former with 12, 25, 50 and 100 epochs with either standard scale augmentation (Standard Aug.) <ref type="bibr" target="#b56">[57]</ref> or the more recent large-scale jittering augmentation (LSJ Aug.) <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b22">23]</ref>. As shown in <ref type="figure" target="#fig_7">Figure IV</ref>   scale jittering augmentation. This shows that Mask2Former with our proposed Transformer decoder converges faster than models using the standard Transformer decoder: e.g., DETR <ref type="bibr" target="#b4">[5]</ref> and MaskFormer <ref type="bibr" target="#b13">[14]</ref> require 500 epochs and 300 epochs respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2. Masked attention analysis</head><p>We quantitatively and qualitatively analyzed the COCO panoptic model with the R50 backbone. First, we visual-  . We train our model on the union of ADE20K train and val set with ImageNet-22K pre-trained checkpoint following <ref type="bibr" target="#b13">[14]</ref> and use multi-scale inference.</p><p>ize the last three attention maps of our model using crossattention ( <ref type="figure" target="#fig_7">Figure Ia top)</ref> and masked attention (Figure Ia bottom) of a single query that predicts the "cat." With cross-attention, the attention map spreads over the entire image and the region with highest response is outside the object of interest. We believe this is because the softmax used in cross-attention never attains zero, and small attention weights on large background regions start to dominate. Instead, masked attention limits the attention weights to focus on the object. We validate this hypothesis in <ref type="table" target="#tab_11">Table Ib:</ref> we compute the cumulative attention weights on foreground (defined by the matching ground truth to each prediction) and background for all queries on the entire COCO val set. On average, only 20% of the attention weights in crossattention focus on the foreground while masked attention increases this ratio to almost 60%. Second, we plot the panoptic segmentation performance using output from each Transformer decoder layer ( <ref type="figure" target="#fig_7">Figure II)</ref>. We find masked attention with a single Transformer decoder layer already outperforms cross-attention with 9 layers. We hope the effectiveness of masked attention, together with this analysis, leads to better attention design.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3. Object query analysis</head><p>Object queries play an important role in Mask2Former. We ablate different design choices of object queries including the number of queries and making queries learnable. Number of queries. We study the effect of different num-  <ref type="bibr" target="#b30">[31]</ref> Swin-L ? 65.9 -------Segmenter <ref type="bibr" target="#b44">[45]</ref> ViT</p><formula xml:id="formula_3">-L ? - - - - - - - 81.3 SETR [64] ViT-L ? - - - - - - - 82.2 SegFormer [59] MiT-B5 - - - - - - - 84.0</formula><p>Mask2Former (ours) ber of queries for three image segmentation tasks in Table Xa. For instance and semantic segmentation, using 100 queries achieves the best performance, while using 200 queries can further improve panoptic segmentation results. As panoptic segmentation is a combination of instance and semantic segmentation, it has more segments per image than the other two tasks. This ablation suggests that picking the number of queries for Mask2Former may depend on the number of segments per image for a particular task or dataset.</p><p>Learnable queries. An object query consists of two parts: object query features and object query positional embeddings. Object query features are only used as the initial input to the Transformer decoder and are updated through decoder layers; whereas query positional embeddings are added to query features in every Transformer decoder layer when computing the attention weights. In DETR <ref type="bibr" target="#b4">[5]</ref>, query features are zero-initialized and query positional embeddings are learnable. Furthermore, there is no direct supervision on these query features before feeding them into the Transformer (since they are zero vectors). In our Mask2Former, we still make query positional embeddings learnable. In addition, we make query features learnable as well and directly apply losses on these learnable query features before feeding them into the Transformer decoder.</p><p>In <ref type="table" target="#tab_21">Table Xb</ref>, we compare our learnable query features with zero-initialized query features in DETR. We find it is important to directly supervise object queries even before feeding them into the Transformer decoder. Learnable queries without supervision perform similarly well as zeroinitialized queries in DETR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.4. MaskFormer vs. Mask2Former</head><p>Mask2Former builds upon the same meta architecture as MaskFormer <ref type="bibr" target="#b13">[14]</ref> with two major differences: 1) We use more advanced training parameters summarized in Table XIa; and 2) we propose a new Transformer decoder with masked attention, instead of using the standard Transformer decoder, as well as some optimization improvements summarized in <ref type="table" target="#tab_11">Table XIb</ref>. To better understand Mask2Former's improvements over MaskFormer, we perform ablation studies on training parameter improvements and Transformer decoder improvements in isolation.</p><p>In <ref type="table" target="#tab_11">Table XIc</ref>  train the MaskFormer model with either its original training parameters in <ref type="bibr" target="#b13">[14]</ref> or our new training parameters. We observe significant improvements of using our new training parameters for MaskFormer as well. This shows the new training parameters are also generally applicable to other models.</p><p>In <ref type="table" target="#tab_11">Table XId</ref>, we study our new Transformer decoder. We train a MaskFormer model and a Mask2Former model with the exact same backbone, i.e., a ResNet-50; pixel decoder, i.e., a FPN; and training parameters. That is, the only difference is in the Transformer decoder, summarized in Table XIb. We observe improvements for all three tasks, suggesting that the new Transformer decoder itself is indeed better than the standard Transformer decoder.</p><p>While computational efficiency was not our primary goal, we find that Mask2Former actually has a better compute-performance trade-off compared to MaskFormer ( <ref type="figure" target="#fig_7">Figure III)</ref>. Even the lightest instantiation of Mask2Former outperforms the heaviest MaskFormer instantiation, using <ref type="bibr">1 4</ref> th the FLOPs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Visualization</head><p>We visualize sample predictions of the Mask2Former model with Swin-L [36] backbone on three tasks: COCO panoptic val2017 set for panoptic segmentation <ref type="bibr">(57.8 PQ)</ref> in <ref type="figure" target="#fig_7">Figure</ref>    <ref type="figure" target="#fig_7">Figure IV</ref>. Convergence analysis. We train Mask2Former with different epochs using either standard scale augmentation (Standard Aug.) <ref type="bibr" target="#b56">[57]</ref> or the more recent large-scale jittering augmentation (LSJ Aug.) <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b22">23]</ref>. Mask2Former converges in 25 epochs using standard augmentation and almost converges in 50 epochs using large-scale jittering augmentation. Using LSJ also improves performance with longer training epochs (i.e., with more than 25 epochs).   <ref type="bibr" target="#b13">[14]</ref> and our Mask2Former on the COCO dataset. * : in the original MaskFormer implementation, the model is trained with a batch size of 64 for 300 epochs. We find MaskFormer achieves similar performance when trained with a batch size of 16 for 75 epochs, i.e., the same number of iterations with a smaller batch size.     </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 .</head><label>1</label><figDesc>State-of-the-art segmentation architectures are typically specialized for each image segmentation task.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>el D ec od er</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 .</head><label>3</label><figDesc>Learnable queries as "region proposals". Top: We visualize mask predictions of four selected learnable queries before feeding them into the Transformer decoder (using R50 backbone). Bottom left: We calculate the class-agnostic average recall with 100 proposals (AR@100) and observe that these learnable queries provide good proposals compared to the final predictions of Mask2Former after the Transformer decoder layers (layer 9). Bottom right: Illustration of proposal generation process.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>, Mask2Former converges in 25 epochs using standard augmentation and almost converges in 50 epochs using large-(a) Visualization of cross-attention (top) and masked attention (bottom) for different resolutions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>77 0.23 0.77 0.15 0.85 0.20 0.80 masked attention 0.53 0.47 0.61 0.39 0.64 0.36 0.59 0.41 (b) Cumulative attention weights on foreground (fg) and background (bg) regions for different resolutions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure I .</head><label>I</label><figDesc>Masked attention analysis.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure</head><label></label><figDesc>II. Panoptic segmentation performance of each Transformer decoder layer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure</head><label></label><figDesc>III. MaskFormer<ref type="bibr" target="#b13">[14]</ref> vs. Mask2Former (ours) with different Swin Transformer backbones.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>V, COCO val2017 set for instance segmentation (50.1 AP) in Figure VI and ADE20K validation set for semantic segmentation (57.7 mIoU, multi-scale inference) in Figure VII.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure V .</head><label>V</label><figDesc>Visualization of panoptic segmentation predictions on the COCO panoptic dataset: Mask2Former with Swin-L backbone which achieves 57.8 PQ on the validation set. First and third columns: ground truth. Second and fourth columns: prediction. Last row shows failure cases.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure VI .</head><label>VI</label><figDesc>Visualization of instance segmentation predictions on the COCO dataset: Mask2Former with Swin-L backbone which achieves 50.1 AP on the validation set. First and third columns: ground truth. Second and fourth columns: prediction. Last row shows failure cases. We show predictions with confidence scores greater than 0.5.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure VII .</head><label>VII</label><figDesc>Visualization of semantic segmentation predictions on the ADE20K dataset: Mask2Former with Swin-L backbone which achieves 57.7 mIoU (multi-scale) on the validation set. First and third columns: ground truth. Second and fourth columns: prediction. Last row shows failure cases.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Instance segmentation on COCO val2017 with 80 categories. Mask2Former outperforms strong Mask R-CNN<ref type="bibr" target="#b23">[24]</ref> baselines for both AP and AP boundary<ref type="bibr" target="#b11">[12]</ref> metrics when training with 8? fewer epochs. Our best model is also competitive to the state-of-the-art specialized instance segmentation model on COCO and has higher boundary quality. For a fair comparison, we only consider single-scale inference and models trained using only COCO train2017 set data. Backbones pre-trained on ImageNet-22K are marked with ? . different learning rates in<ref type="bibr" target="#b13">[14]</ref>.</figDesc><table><row><cell>method</cell><cell>backbone</cell><cell>query type</cell><cell>epochs</cell><cell>AP</cell><cell>AP S</cell><cell>AP M</cell><cell>AP L</cell><cell cols="3">AP boundary #params. FLOPs</cell><cell>fps</cell></row><row><cell>MaskFormer [14]</cell><cell>R50</cell><cell>100 queries</cell><cell>300</cell><cell>34.0</cell><cell>16.4</cell><cell>37.8</cell><cell>54.2</cell><cell>23.0</cell><cell>45M</cell><cell>181G</cell><cell>19.2</cell></row><row><cell>Mask R-CNN [24]</cell><cell>R50</cell><cell>dense anchors</cell><cell>36</cell><cell>37.2</cell><cell>18.6</cell><cell>39.5</cell><cell>53.3</cell><cell>23.1</cell><cell>44M</cell><cell>201G</cell><cell>15.2</cell></row><row><cell cols="2">Mask R-CNN [18, 23, 24] R50</cell><cell>dense anchors</cell><cell>400</cell><cell>42.5</cell><cell>23.8</cell><cell>45.0</cell><cell>60.0</cell><cell>28.0</cell><cell>46M</cell><cell>358G</cell><cell>10.3</cell></row><row><cell>Mask2Former (ours)</cell><cell>R50</cell><cell>100 queries</cell><cell>50</cell><cell>43.7</cell><cell>23.4</cell><cell>47.2</cell><cell>64.8</cell><cell>30.6</cell><cell>44M</cell><cell>226G</cell><cell>9.7</cell></row><row><cell>Mask R-CNN [24]</cell><cell>R101</cell><cell>dense anchors</cell><cell>36</cell><cell>38.6</cell><cell>19.5</cell><cell>41.3</cell><cell>55.3</cell><cell>24.5</cell><cell>63M</cell><cell>266G</cell><cell>10.8</cell></row><row><cell cols="2">Mask R-CNN [18, 23, 24] R101</cell><cell>dense anchors</cell><cell>400</cell><cell>43.7</cell><cell>24.6</cell><cell>46.4</cell><cell>61.8</cell><cell>29.1</cell><cell>65M</cell><cell>423G</cell><cell>8.6</cell></row><row><cell>Mask2Former (ours)</cell><cell>R101</cell><cell>100 queries</cell><cell>50</cell><cell>44.2</cell><cell>23.8</cell><cell>47.7</cell><cell>66.7</cell><cell>31.1</cell><cell>63M</cell><cell>293G</cell><cell>7.8</cell></row><row><cell>QueryInst [20]</cell><cell>Swin-L  ?</cell><cell>300 queries</cell><cell>50</cell><cell>48.9</cell><cell>30.8</cell><cell>52.6</cell><cell>68.3</cell><cell>33.5</cell><cell>-</cell><cell>-</cell><cell>3.3</cell></row><row><cell>Swin-HTC++ [6, 36]</cell><cell>Swin-L  ?</cell><cell>dense anchors</cell><cell>72</cell><cell>49.5</cell><cell>31.0</cell><cell>52.4</cell><cell>67.2</cell><cell>34.1</cell><cell>284M</cell><cell>1470G</cell><cell>-</cell></row><row><cell>Mask2Former (ours)</cell><cell>Swin-L  ?</cell><cell>200 queries</cell><cell>100</cell><cell>50.1</cell><cell>29.9</cell><cell>53.9</cell><cell>72.1</cell><cell>36.2</cell><cell>216M</cell><cell>868G</cell><cell>4.0</cell></row></table><note>4.3. Main results Panoptic segmentation. We compare Mask2Former with state-of-the-art models for panoptic segmentation on the COCO panoptic [28] dataset in Table 1. Mask2Former consistently outperforms MaskFormer by more than 5 PQ across different backbones while converging 6? faster. With Swin-L backbone, our Mask2Former sets a new state- of-the-art of 57.8 PQ, outperforming existing state-of-the- art [14] by 5.1 PQ and concurrent work, K-Net [62], by 3.2 PQ. Mask2Former even outperforms the best ensemble models with extra training data in the COCO challenge (see Appendix A.1 for test set results).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>dataset in Table 2. With ResNet<ref type="bibr" target="#b24">[25]</ref> backbone, Mask2Former outperforms a strong Mask R-CNN<ref type="bibr" target="#b23">[24]</ref> baseline using largescale jittering (LSJ) augmentation<ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b22">23]</ref> while requiring 8? fewer training iterations. With Swin-L backbone, Mask2Former outperforms the state-of-the-art HTC++<ref type="bibr" target="#b5">[6]</ref>. Although we only observe +0.6 AP improvement over HTC++, the Boundary AP<ref type="bibr" target="#b11">[12]</ref> improves by 2.1, suggesting that our predictions have a better boundary quality thanks to the high-resolution mask predictions. Note that for a fair comparison, we only consider single-scale inference and models trained with only COCO train2017 set data.With a ResNet-50 backbone Mask2Former improves over MaskFormer on small objects by 7.0 AP S , while over-</figDesc><table><row><cell>method</cell><cell>backbone</cell><cell cols="3">crop size mIoU (s.s.) mIoU (m.s.)</cell></row><row><cell>MaskFormer [14]</cell><cell>R50</cell><cell>512</cell><cell>44.5</cell><cell>46.7</cell></row><row><cell>Mask2Former (ours)</cell><cell>R50</cell><cell>512</cell><cell>47.2</cell><cell>49.2</cell></row><row><cell>Swin-UperNet [36, 58]</cell><cell>Swin-T</cell><cell>512</cell><cell>-</cell><cell>46.1</cell></row><row><cell>MaskFormer [14]</cell><cell>Swin-T</cell><cell>512</cell><cell>46.7</cell><cell>48.8</cell></row><row><cell>Mask2Former (ours)</cell><cell>Swin-T</cell><cell>512</cell><cell>47.7</cell><cell>49.6</cell></row><row><cell>MaskFormer [14]</cell><cell>Swin-L  ?</cell><cell>640</cell><cell>54.1</cell><cell>55.6</cell></row><row><cell cols="2">FaPN-MaskFormer [14, 39] Swin-L-FaPN  ?</cell><cell>640</cell><cell>55.2</cell><cell>56.7</cell></row><row><cell>BEiT-UperNet [2, 58]</cell><cell>BEiT-L  ?</cell><cell>640</cell><cell>-</cell><cell>57.0</cell></row><row><cell>Mask2Former (ours)</cell><cell>Swin-L  ? Swin-L-FaPN  ?</cell><cell>640 640</cell><cell>56.1 56.4</cell><cell>57.3 57.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Semantic segmentation on ADE20K val with 150 categories. Mask2Former consistently outperforms Mask-Former<ref type="bibr" target="#b13">[14]</ref> by a large margin with different backbones (all Mask2Former models use MSDeformAttn<ref type="bibr" target="#b65">[66]</ref> as pixel decoder, except Swin-L-FaPN uses FaPN<ref type="bibr" target="#b38">[39]</ref>). Our best model outperforms the best specialized model, BEiT<ref type="bibr" target="#b1">[2]</ref>. We report both singlescale (s.s.) and multi-scale (m.s.) inference results. Backbones pre-trained on ImageNet-22K are marked with ? .</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>Masked attention and high-resolution features (from efficient multi-scale strategy) lead to the most gains. More detailed ablations are inTable 4candTable 4d. We remove one component at a time. Feature resolution. High-resolution features (single scale 1/8) are important. Our efficient multi-scale (efficient m.s.) strategy effectively reduces the FLOPs. Pixel decoder. MSDeformAttn [66] consistently performs the best across all tasks.</figDesc><table><row><cell cols="2">AP 43.7 37.8 (-5.9) 41.5 (-2.2) (a) AP PQ mIoU FLOPs Mask2Former (ours) 51.9 47.2 226G ? masked attention 47.1 (-4.8) 45.5 (-1.7) 213G ? high-resolution features 50.2 (-1.7) 46.1 (-1.1) 218G Mask2Former (ours) 43.7 ? learnable query features 42.9 (-0.8) ? cross-attention first 43.2 (-0.5) ? remove dropout 43.0 (-0.7) ? all 3 components above 42.3 (-1.4) (b) Optimization improvements increase the performance without introduc-PQ mIoU FLOPs 51.9 47.2 226G 51.2 (-0.7) 45.4 (-1.8) 226G 51.6 (-0.3) 46.3 (-0.9) 226G 51.3 (-0.6) 47.2 (-0.0) 226G 50.8 (-1.1) 46.3 (-0.9) 226G</cell></row><row><cell></cell><cell>ing extra compute. Following DETR [5], query features are zero-initialized</cell></row><row><cell></cell><cell>when not learnable. We remove one component at a time.</cell></row><row><cell>AP 37.8 47.1 45.5 213G PQ mIoU FLOPs 37.9 47.2 46.6 213G mask pooling [62] 43.1 51.5 46.0 217G cross-attention SMCA [22] masked attention 43.7 51.9 47.2 226G (c) Masked attention. Our masked attention</cell><cell>AP 41.5 50.2 46.1 218G PQ mIoU FLOPs 43.0 51.5 46.5 222G 44.0 51.8 47.4 239G 44.0 51.9 46.3 247G efficient m.s. (3 scales) 43.7 51.9 47.2 226G single scale (1/32) single scale (1/16) single scale (1/8) na?ve m.s. (3 scales) (d) AP FPN [33] 41.5 50.7 45.6 195G PQ mIoU FLOPs Semantic FPN [27] 42.1 51.2 46.2 258G FaPN [39] 42.4 51.8 46.8 -BiFPN [47] 43.5 51.8 45.6 204G MSDeformAttn [66] 43.7 51.9 47.2 226G (e)</cell></row><row><cell>performs better than other variants of cross-</cell><cell></cell></row><row><cell>attention across all tasks.</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 .</head><label>4</label><figDesc>Mask2Former ablations. We perform ablations on three tasks: instance (AP on COCO val2017), panoptic (PQ on COCO panoptic val2017) and semantic (mIoU on ADE20K val) segmentation. FLOPs are measured on COCO instance segmentation.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5</head><label>5</label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell>AP</cell><cell>PQ</cell><cell>mIoU</cell><cell>memory</cell></row><row><cell cols="2">matching loss training loss</cell><cell>(COCO)</cell><cell>(COCO)</cell><cell>(ADE20K)</cell><cell>(COCO)</cell></row><row><cell>mask</cell><cell>mask point</cell><cell>41.0 41.0</cell><cell>50.3 50.8</cell><cell>45.9 45.9</cell><cell>18G 6G</cell></row><row><cell>point (ours)</cell><cell>mask point (ours)</cell><cell>43.1 43.7</cell><cell>51.4 51.9</cell><cell>47.3 47.2</cell><cell>18G 6G</cell></row></table><note>. Calculating loss with points vs. masks. Training with point loss reduces training memory without influencing the perfor- mance. Matching with point loss further improves performance.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6</head><label>6</label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell cols="3">panoptic model</cell><cell cols="2">semantic model</cell></row><row><cell>method</cell><cell>backbone</cell><cell cols="5">PQ AP Th pan mIoUpan mIoU (s.s.) (m.s.)</cell></row><row><cell>Panoptic FCN [31]</cell><cell>Swin-L  ?</cell><cell>65.9</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="4">Panoptic-DeepLab [11] SWideRNet [9] 66.4 40.1</cell><cell>82.2</cell><cell>-</cell><cell>-</cell></row><row><cell cols="6">Panoptic-DeepLab [11] SWideRNet [9] 67.5  -</cell><cell>-</cell></row><row><cell>SETR [64]</cell><cell>ViT-L  ? [17]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>82.2</cell></row><row><cell>SegFormer [59]</cell><cell>MiT-B5 [59]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>84.0</cell></row><row><cell></cell><cell>R50</cell><cell cols="2">62.1 37.3</cell><cell>77.5</cell><cell>79.4</cell><cell>82.2</cell></row><row><cell>Mask2Former (ours)</cell><cell>Swin-B  ?</cell><cell cols="2">66.1 42.8</cell><cell>82.7</cell><cell>83.3</cell><cell>84.5</cell></row><row><cell></cell><cell>Swin-L  ?</cell><cell cols="2">66.6 43.6</cell><cell>82.9</cell><cell>83.3</cell><cell>84.3</cell></row></table><note>* 43.9* 82.9*. Cityscapes val. Mask2Former is competitive to spe- cialized models on Cityscapes. Panoptic segmentation models use single-scale inference by default, multi-scale numbers are marked with* . For semantic segmentation, we report both single-scale (s.s.) and multi-scale (m.s.) inference results. Backbones pre- trained on ImageNet-22K are marked with ? .</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>, we further report the best Mask2Former model on the test-dev set. Note that Mask2Former trained only with the standard train2017 data, achieves the absolute new state-of-the-art performance on both validation and test set. Mask2Former even outperforms the best COCO competition entry which uses extra training data and test-time augmentation.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table I .</head><label>I</label><figDesc></figDesc><table /><note>Panoptic segmentation on COCO panoptic val2017 with 133 categories. Mask2Former outperforms all existing panoptic segmentation models by a large margin with different backbones on all metrics. Our best model sets a new state-of-the-art of 57.8 PQ.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table III .</head><label>III</label><figDesc>Instance segmentation on COCO val2017 with 80 categories. Mask2Former outperforms strong Mask R-CNN<ref type="bibr" target="#b23">[24]</ref> baselines with 8? fewer training epochs for both AP and AP boundary<ref type="bibr" target="#b11">[12]</ref> metrics. Our best model is also competitive to the state-of-the-art specialized instance segmentation model on COCO and has higher boundary quality. For a fair comparison, we only consider single-scale inference and models trained using only COCO train2017 set data. Backbones pre-trained on ImageNet-22K are marked with ? .</figDesc><table><row><cell></cell><cell>method</cell><cell>backbone</cell><cell cols="2">search space</cell><cell>epochs</cell><cell>AP</cell><cell>AP S</cell><cell>AP M</cell><cell>AP L</cell><cell cols="2">AP boundary</cell><cell>#params.</cell><cell>FLOPs</cell></row><row><cell>CNN backbones</cell><cell>Mask R-CNN [24] Mask2Former (ours)</cell><cell>R50 R50 R101 R101 R50 R101</cell><cell cols="2">dense anchors dense anchors dense anchors dense anchors 100 queries 100 queries</cell><cell>36 400 36 400 50 50</cell><cell>37.2 42.5 38.6 43.7 43.7 44.2</cell><cell>18.6 23.8 19.5 24.6 23.4 23.8</cell><cell>39.5 45.0 41.3 46.4 47.2 47.7</cell><cell>53.3 60.0 55.3 61.8 64.8 66.7</cell><cell>23.1 28.0 24.5 29.1 30.6 31.1</cell><cell></cell><cell>44M 46M 63M 65M 44M 63M</cell><cell>201G 358G 266G 423G 226G 293G</cell></row><row><cell>Transformer backbones</cell><cell>QueryInst [20] Swin-HTC++ [6, 36] Mask2Former (ours)</cell><cell>Swin-L  ? Swin-B  ? Swin-L  ? Swin-T Swin-S Swin-B Swin-B  ? Swin-L  ?</cell><cell cols="2">300 queries dense anchors dense anchors 100 queries 100 queries 100 queries 100 queries 200 queries</cell><cell>50 36 72 50 50 50 50 100</cell><cell>48.9 49.1 49.5 45.0 46.3 46.7 48.1 50.1</cell><cell>30.8 -31.0 24.5 25.3 26.1 27.8 29.9</cell><cell>52.6 -52.4 48.3 50.3 50.5 52.0 53.9</cell><cell>68.3 -67.2 67.4 68.4 68.8 71.1 72.1</cell><cell>33.5 -34.1 31.8 32.9 33.2 34.4 36.2</cell><cell></cell><cell>-160M 284M 47M 69M 107M 107M 216M</cell><cell>-1043G 1470G 232G 313G 466G 466G 868G</cell></row><row><cell></cell><cell>method</cell><cell></cell><cell></cell><cell cols="2">backbone</cell><cell>AP</cell><cell>AP50</cell><cell>AP75</cell><cell>AP S</cell><cell>AP M</cell><cell>AP L</cell></row><row><cell></cell><cell cols="2">QueryInst [20]</cell><cell></cell><cell cols="2">Swin-L</cell><cell>49.1</cell><cell>74.2</cell><cell>53.8</cell><cell>31.5</cell><cell>51.8</cell><cell>63.2</cell></row><row><cell></cell><cell cols="2">Swin-HTC++ [6, 36]</cell><cell></cell><cell cols="2">Swin-L</cell><cell>50.2</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell cols="5">Swin-HTC++ [6, 36] (multi-scale) Swin-L</cell><cell>51.1</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell cols="3">Megvii (challenge winner)</cell><cell>-</cell><cell></cell><cell>53.1</cell><cell>76.8</cell><cell>58.6</cell><cell>36.6</cell><cell>56.5</cell><cell>67.7</cell></row><row><cell></cell><cell cols="2">Mask2Former (ours)</cell><cell></cell><cell cols="2">Swin-L</cell><cell>50.5</cell><cell>74.9</cell><cell>54.9</cell><cell>29.1</cell><cell>53.8</cell><cell>71.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table IV .</head><label>IV</label><figDesc>Instance segmentation on COCO test-dev with 80 categories. Mask2Former is extremely good at segmenting large objects: we can even outperform the challenge winner (which uses extra training data, model ensemble, etc.) on AP L by a large margin without any bells-and-whistles. We only train our model on the COCO train2017 set with ImageNet-22K pre-trained checkpoint.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table V .</head><label>V</label><figDesc>Semantic segmentation on ADE20K val with 150 categories.Table VI. Semantic segmentation on ADE20K test with 150 categories. Mask2Former outperforms previous state-of-the-art methods on all three metrics: pixel accuracy (P.A.), mIoU, as well as the final test score (average of P.A. and mIoU)</figDesc><table><row><cell></cell><cell></cell><cell cols="3">Mask2Former consistently outperforms MaskFormer [14] by</cell></row><row><cell cols="5">a large margin with different backbones (all Mask2Former models use MSDeformAttn [66] as pixel decoder, except Swin-L-FaPN uses</cell></row><row><cell cols="5">FaPN [39]). Our best model outperforms the best specialized model, BEiT [2], with less than half of the parameters. We report both</cell></row><row><cell cols="5">single-scale (s.s.) and multi-scale (m.s.) inference results. Backbones pre-trained on ImageNet-22K are marked with  ? .</cell></row><row><cell>method</cell><cell>backbone</cell><cell>P.A.</cell><cell>mIoU</cell><cell>score</cell></row><row><cell>SETR [64]</cell><cell>ViT-L</cell><cell>78.35</cell><cell>45.03</cell><cell>61.69</cell></row><row><cell cols="2">Swin-UperNet [36, 58] Swin-L</cell><cell>78.42</cell><cell>47.07</cell><cell>62.75</cell></row><row><cell>MaskFormer [14]</cell><cell>Swin-L</cell><cell>79.36</cell><cell>49.67</cell><cell>64.51</cell></row><row><cell>Mask2Former (ours)</cell><cell>Swin-L-FaPN</cell><cell>79.80</cell><cell>49.72</cell><cell>64.76</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>Table VII .</head><label>VII</label><figDesc>Image segmentation results on Cityscapes val. We report both single-scale (s.s.) and multi-scale (m.s.) inference results for PQ and mIoU. All other metrics are evaluated with single-scale inference. Since Mask2Former is an end-to-end model, we only use single-scale inference for instance-level segmentation tasks to avoid the need for further post-processing (e.g., NMS).Table VIII. Image segmentation results on ADE20K val. Mask2Former is competitive to specialized models on ADE20K. Panoptic segmentation models use single-scale inference by default, multi-scale numbers are marked with * . For semantic segmentation, we report both single-scale (s.s.) and multi-scale (m.s.) inference results.</figDesc><table><row><cell>R50</cell><cell></cell><cell>62.1</cell><cell>-</cell><cell>37.3</cell><cell>77.5</cell><cell>37.4</cell><cell></cell><cell>61.9</cell><cell>79.4</cell><cell>82.2</cell></row><row><cell cols="2">R101</cell><cell>62.4</cell><cell>-</cell><cell>37.7</cell><cell>78.6</cell><cell>38.5</cell><cell></cell><cell>63.9</cell><cell>80.1</cell><cell>81.9</cell></row><row><cell cols="2">Swin-T</cell><cell>63.9</cell><cell>-</cell><cell>39.1</cell><cell>80.5</cell><cell>39.7</cell><cell></cell><cell>66.9</cell><cell>82.1</cell><cell>83.0</cell></row><row><cell cols="2">Swin-S</cell><cell>64.8</cell><cell>-</cell><cell>40.7</cell><cell>81.8</cell><cell>41.8</cell><cell></cell><cell>70.4</cell><cell>82.6</cell><cell>83.6</cell></row><row><cell cols="2">Swin-B  ?</cell><cell>66.1</cell><cell>-</cell><cell>42.8</cell><cell>82.7</cell><cell>42.0</cell><cell></cell><cell>68.8</cell><cell>83.3</cell><cell>84.5</cell></row><row><cell cols="2">Swin-L  ?</cell><cell>66.6</cell><cell>-</cell><cell>43.6</cell><cell>82.9</cell><cell>43.7</cell><cell></cell><cell>71.4</cell><cell>83.3</cell><cell>84.3</cell></row><row><cell></cell><cell></cell><cell></cell><cell>panoptic model</cell><cell></cell><cell></cell><cell cols="2">instance model</cell><cell></cell><cell cols="2">semantic model</cell></row><row><cell>method</cell><cell>backbone</cell><cell>PQ</cell><cell>AP Th pan</cell><cell>mIoUpan</cell><cell>AP</cell><cell>AP S</cell><cell>AP M</cell><cell>AP L</cell><cell cols="2">mIoU (s.s.) mIoU (m.s.)</cell></row><row><cell>MaskFormer [14]</cell><cell>R50</cell><cell>34.7</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Panoptic-DeepLab [11]</cell><cell>SWideRNet [9]</cell><cell>37.9  *</cell><cell>-</cell><cell>50.0  *</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Swin-UperNet [36, 58]</cell><cell>Swin-L  ?</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>53.5</cell></row><row><cell>MaskFormer [14]</cell><cell>Swin-L  ?</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>54.1</cell><cell>55.6</cell></row><row><cell cols="2">FaPN-MaskFormer [14, 39] Swin-L  ?</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>55.2</cell><cell>56.7</cell></row><row><cell>BEiT-UperNet [2, 58]</cell><cell>BEiT-L  ?</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>57.0</cell></row><row><cell></cell><cell>R50</cell><cell>39.7</cell><cell>26.5</cell><cell>46.1</cell><cell>26.4</cell><cell>10.4</cell><cell>28.9</cell><cell>43.1</cell><cell>47.2</cell><cell>49.2</cell></row><row><cell>Mask2Former (ours)</cell><cell>Swin-L  ?</cell><cell>48.1</cell><cell>34.2</cell><cell>54.5</cell><cell>34.9</cell><cell>16.3</cell><cell>40.0</cell><cell>54.7</cell><cell>56.1</cell><cell>57.3</cell></row><row><cell></cell><cell>Swin-L-FaPN  ?</cell><cell>46.2</cell><cell>33.2</cell><cell>55.4</cell><cell>33.4</cell><cell>14.6</cell><cell>37.6</cell><cell>54.6</cell><cell>56.4</cell><cell>57.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19"><head></head><label></label><figDesc>Table IX. Image segmentation results on Mapillary Vistas val. Mask2Former is competitive to specialized models on Mapillary Vistas. Panoptic segmentation models use single-scale inference by default, multi-scale numbers are marked with * . For semantic segmentation, we report both single-scale (s.s.) and multi-scale (m.s.) inference results.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">panoptic model</cell><cell>semantic model</cell></row><row><cell></cell><cell></cell><cell>method</cell><cell></cell><cell>backbone</cell><cell>PQ</cell><cell>mIoUpan</cell><cell>mIoU (s.s.)</cell><cell>mIoU (m.s.)</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>ensemble</cell><cell>42.2  *</cell><cell>58.7  *</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell cols="2">Panoptic-DeepLab [11]</cell><cell>SWideRNet [9]</cell><cell>43.7</cell><cell>59.4</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>SWideRNet [9]</cell><cell>44.8  *</cell><cell>60.0  *</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell cols="2">Panoptic FCN [31]</cell><cell>Swin-L  ?</cell><cell>45.7</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell cols="2">MaskFormer [14]</cell><cell>R50</cell><cell>-</cell><cell>-</cell><cell>53.1</cell><cell>55.4</cell></row><row><cell></cell><cell></cell><cell>HMSANet [48]</cell><cell></cell><cell>HRNet [53]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>61.1</cell></row><row><cell></cell><cell></cell><cell cols="2">Mask2Former (ours)</cell><cell>R50 Swin-L  ?</cell><cell>36.3 45.5</cell><cell>50.7 60.8</cell><cell>57.4 63.2</cell><cell>59.0 64.7</cell></row><row><cell></cell><cell>58</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>56</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>54</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>PQ</cell><cell>52</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>50</cell><cell></cell><cell cols="2">MaskFormer</cell><cell></cell></row><row><cell></cell><cell>48</cell><cell></cell><cell cols="2">Mask2Former (ours)</cell><cell></cell></row><row><cell></cell><cell>200</cell><cell>400</cell><cell>600</cell><cell>800</cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">GFLOPs</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>, we study our new training parameters. We</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_20"><head></head><label></label><figDesc>Number of queries ablation. For instance and semantic segmentation, using 100 queries achieves the best performance while using 200 queries can further improve panoptic segmentation results. Learnable queries ablation. It is important to supervise object queries before feeding them into the Transformer decoder. Learnable queries without supervision perform similarly well as zero-initialized queries in DETR.</figDesc><table><row><cell>AP (COCO) 42.4 43.7 43.5 43.5 40.3 (a) AP PQ (COCO) mIoU (ADE20K) FLOPs (COCO) 50 50.5 46.2 217G 100 51.9 47.2 226G 200 52.2 47.0 246G 300 52.1 46.5 265G 1000 50.7 44.8 405G (COCO) zero-initialized (DETR [5]) 42.9 learnable w/o supervision 42.9 learnable w/ supervision 43.7 (b)</cell><cell>PQ (COCO) 51.2 51.2 51.9</cell><cell>mIoU (ADE20K) 45.5 47.0 47.2</cell><cell>FLOPs (COCO) 226G 226G 226G</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_21"><head>Table X .</head><label>X</label><figDesc>Analysis of object queries.Table Xa: ablation on number of queries. Table Xb: ablation on using learnable queries.</figDesc><table><row><cell>training parameters</cell><cell>MaskFormer</cell><cell>Mask2Former (ours)</cell></row><row><cell>learning rate</cell><cell>0.0001</cell><cell>0.0001</cell></row><row><cell>weight decay</cell><cell>0.0001</cell><cell>0.05</cell></row><row><cell>batch size</cell><cell>16  *</cell><cell>16</cell></row><row><cell>epochs</cell><cell>75  *</cell><cell>50</cell></row><row><cell>data augmentation</cell><cell>standard scale aug. w/ crop</cell><cell>LSJ aug.</cell></row><row><cell>?cls</cell><cell>1.0</cell><cell>2.0</cell></row><row><cell>?focal / ?ce</cell><cell>20.0 / -</cell><cell>-/ 5.0</cell></row><row><cell>?dice</cell><cell>1.0</cell><cell>5.0</cell></row><row><cell>mask loss</cell><cell>mask</cell><cell>12544 sampled points</cell></row></table><note>(a) Comparison of training parameters for MaskFormer</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_22"><head></head><label></label><figDesc>Comparison of Transformer decoder in MaskFormer<ref type="bibr" target="#b13">[14]</ref> and our Mask2Former. SA: self-attention, CA: cross-attention, FFN: feed-forward network, MA: masked attention, p.e.: positional embedding. Improvements from better training parameters.</figDesc><table><row><cell>Transformer decoder</cell><cell>MaskFormer</cell><cell>Mask2Former (ours)</cell></row><row><cell># of layers</cell><cell>6</cell><cell>9</cell></row><row><cell>single layer</cell><cell>SA-CA-FFN</cell><cell>MA-SA-FFN</cell></row><row><cell>dropout</cell><cell>0.1</cell><cell>0.0</cell></row><row><cell>feature resolution</cell><cell>{1/32} ? 6</cell><cell>{1/32, 1/16, 1/8} ? 3</cell></row><row><cell>input query features</cell><cell>zero init.</cell><cell>learnable</cell></row><row><cell>query p.e.</cell><cell>learnable</cell><cell>learnable</cell></row><row><cell cols="3">AP (COCO) 34.0 37.8 (+3.8) (c) Transformer decoder pixel decoder PQ mIoU training params. (COCO) (ADE20K) MaskFormer (b) model MaskFormer 46.5 44.5 MaskFormer Mask2Former 48.2 (+1.7) 45.3 (+0.8) MaskFormer FPN Mask2Former FPN (d) Improvements from better Transformer decoder. AP PQ (COCO) (COCO) (ADE20K) mIoU 37.8 48.2 45.3 41.5 (+3.7) 50.7 (+2.5) 45.6 (+0.3)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_23"><head>Table XI .</head><label>XI</label><figDesc>MaskFormer vs. Mask2Former. Table XIa and Table XIb provide an in-depth comparison between MaskFormer and our Mask2Former settings. Table XIc: MaskFormer benefits from our new training parameters as well. Table XId: Comparison between MaskFormer and our Mask2Former with the exact same backbone, pixel decoder and training parameters. The improvements solely come from a better Transformer decoder.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/facebookresearch/detectron2/blob/ main / MODEL _ ZOO . md # new -baselines -using -large -scalejitter-and-longer-training-schedule</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Multiscale combinatorial grouping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Arbel?ez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordi</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">T</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ferran</forename><surname>Marques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hangbo</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<title level="m">BEiT: BERT pretraining of image transformers. arXiv</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">YOLACT++: Better real-time instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Bolya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fanyi</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><forename type="middle">Jae</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Cascade R-CNN: Delving into high quality object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaowei</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nuno</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">End-toend object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Hybrid task cascade for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangmiao</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxiao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuyang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wansen</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">DeepLab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected CRFs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>PAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Rethinking atrous convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.05587</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Scaling wide residual networks for panoptic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huiyu</forename><surname>Liang-Chieh Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Qiao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.11675</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Encoder-decoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Liang-Chieh Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Panoptic-DeepLab: A simple, strong, and fast baseline for bottom-up panoptic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Maxwell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Boundary iou: Improving object-centric image segmentation evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Alexander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kirillov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Pointly-supervised instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omkar</forename><surname>Parkhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Per-pixel classification is not all you need for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Xception: Deep learning with depthwise separable convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fran?ois</forename><surname>Chollet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Uwe Franke, Stefan Roth, and Bernt Schiele. The Cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianzhi</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Chih</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.00057</idno>
		<title level="m">Simple training strategies and model scaling for object detection</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">The PASCAL visual object classes challenge: A retrospective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">I</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>IJCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shusheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Shan</surname></persName>
		</author>
		<title level="m">Bin Feng, and Wenyu Liu. Instances as queries. In ICCV</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Dual attention network for scene segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haijie</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongjun</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiwei</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanqing</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Fast convergence of detr with spatially modulated co-attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Simple copy-paste is a strong data augmentation method for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Golnaz</forename><surname>Ghiasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aravind</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zoph</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Piotr Doll?r, and Ross Girshick. Mask R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">CCNet: Criss-cross attention for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zilong</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lichao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyu</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr Doll?r. Panoptic feature pyramid networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Panoptic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">InstanceCut: from edges to instances with multicut</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeny</forename><surname>Levinkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bjoern</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bogdan</forename><surname>Savchynskyy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Rother</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Kaiming He, and Ross Girshick. PointRend: Image segmentation as rendering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Fully convolutional networks for panoptic segmentation with point-based supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.07682</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enze</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiding</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anima</forename><surname>Anandkumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Luo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.03814</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">Panoptic segformer. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Kaiming He, Bharath Hariharan, and Serge Belongie. Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr Doll?r. Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Microsoft COCO: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baining</forename><surname>Guo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.14030</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Fapn: Feature-aligned pyramid network for dense image prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shihua</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhichao</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ran</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>He</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Conditional detr for fast training convergence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Depu</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaokang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zejia</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houqiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhui</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">V-Net: Fully convolutional neural networks for volumetric medical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fausto</forename><surname>Milletari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nassir</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seyed-Ahmad</forename><surname>Ahmadi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">The mapillary vistas dataset for semantic understanding of street scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerhard</forename><surname>Neuhold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Ollmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><forename type="middle">Rota</forename><surname>Bul?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Kontschieder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Faster R-CNN: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>ImageNet Large Scale Visual Recognition Challenge. IJCV</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Segmenter: Transformer for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robin</forename><surname>Strudel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ricardo</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Rethinking transformer-based set prediction for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengcao</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kris</forename><forename type="middle">M</forename><surname>Kitani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Efficientdet: Scalable and efficient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruoming</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Hierarchical multi-scale attention for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karan</forename><surname>Sapra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.10821</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Conditional convolutions for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Selective search for object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jasper Rr Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">A</forename><surname>Koen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theo</forename><surname>Van De Sande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arnold Wm</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smeulders</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>IJCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">MaX-DeepLab: End-to-end panoptic segmentation with mask transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huiyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Deep high-resolution representation learning for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianheng</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Borui</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaorui</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yadong</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingkui</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>PAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enze</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng-Ping</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaitao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ding</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.13797</idno>
		<title level="m">Pvtv2: Improved baselines with pyramid vision transformer</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Abhinav Gupta, and Kaiming He. Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">SOLOv2: Dynamic and fast instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rufeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>NeurIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wan-Yen</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Detectron2</surname></persName>
		</author>
		<ptr target="https://github.com/facebookresearch/detectron2" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Unified perceptual parsing for scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tete</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingcheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuning</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Segformer: Simple and efficient design for semantic segmentation with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enze</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiding</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anima</forename><surname>Anandkumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Luo</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Upsnet: A unified panoptic segmentation network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuwen</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renjie</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ersin</forename><surname>Yumer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">OCNet: Object context for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhui</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianyuan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xilin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">K-net: Towards unified image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangmiao</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen Change</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Rethinking semantic segmentation from a sequence-to-sequence perspective with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sixiao</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiachen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiatian</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zekun</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yabiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanwei</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Scene parsing through ADE20K dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Puig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adela</forename><surname>Barriuso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Deformable detr: Deformable transformers for end-to-end object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xizhou</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijie</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lewei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
