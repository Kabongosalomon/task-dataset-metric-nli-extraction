<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Efficient Multi-Task RGB-D Scene Analysis for Indoor Environments</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Seichter</surname></persName>
							<email>daniel.seichter@tu-ilmenau.de@orcid:0000-0002-3828-2926</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Neuroinformatics and Cognitive Robotics Lab</orgName>
								<orgName type="institution">Ilmenau University of Technology</orgName>
								<address>
									<postCode>98684</postCode>
									<settlement>Ilmenau</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S?hnke</forename><forename type="middle">Benedikt</forename><surname>Fischedick</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Neuroinformatics and Cognitive Robotics Lab</orgName>
								<orgName type="institution">Ilmenau University of Technology</orgName>
								<address>
									<postCode>98684</postCode>
									<settlement>Ilmenau</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mona</forename><surname>K?hler</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Neuroinformatics and Cognitive Robotics Lab</orgName>
								<orgName type="institution">Ilmenau University of Technology</orgName>
								<address>
									<postCode>98684</postCode>
									<settlement>Ilmenau</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Horst-Michael</forename><surname>Gro?</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Neuroinformatics and Cognitive Robotics Lab</orgName>
								<orgName type="institution">Ilmenau University of Technology</orgName>
								<address>
									<postCode>98684</postCode>
									<settlement>Ilmenau</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Efficient Multi-Task RGB-D Scene Analysis for Indoor Environments</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T11:47+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Multi-task learning</term>
					<term>orientation estimation</term>
					<term>panoptic segmentation</term>
					<term>scene classification</term>
					<term>semantic segmenta- tion</term>
					<term>NYUv2</term>
					<term>SUNRGB-D</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Semantic scene understanding is essential for mobile agents acting in various environments. Although semantic segmentation already provides a lot of information, details about individual objects as well as the general scene are missing but required for many real-world applications. However, solving multiple tasks separately is expensive and cannot be accomplished in real time given limited computing and battery capabilities on a mobile platform. In this paper, we propose an efficient multi-task approach for RGB-D scene analysis (EMSANet) that simultaneously performs semantic and instance segmentation (panoptic segmentation), instance orientation estimation, and scene classification. We show that all tasks can be accomplished using a single neural network in real time on a mobile platform without diminishing performance -by contrast, the individual tasks are able to benefit from each other. In order to evaluate our multi-task approach, we extend the annotations of the common RGB-D indoor datasets NYUv2 and SUNRGB-D for instance segmentation and orientation estimation. To the best of our knowledge, we are the first to provide results in such a comprehensive multi-task setting for indoor scene analysis on NYUv2 and SUNRGB-D.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>In computer vision, semantic scene understanding is often equated with semantic segmentation as it enables gaining precise knowledge about the structure of a scene by assigning a semantic label to each pixel of an image. However, this kind of knowledge is not sufficient for the agents in our ongoing research projects MORPHIA and CO-HUMANICS that require operating autonomously in their environments. Imagine a mobile robot that is supposed to navigate to a semantic entity, e.g., a specific chair within a group of chairs in the living room, as shown in <ref type="figure" target="#fig_0">Fig. 1</ref>. Performing such a high-level task requires a much broader understanding of the scene. First, even with a semantic map of the environment <ref type="bibr" target="#b0">[1]</ref>, the robot still needs to know which part of its environment belongs to the living room. Subsequently, it needs to be able to distinguish individual instances of the same semantic class, and, finally, for approaching the chair from the right direction, its orientation is required.</p><p>In this paper, we present an approach called Efficient Multitask Scene Analysis Network (EMSANet) for tackling all This work has received funding from the German Federal Ministry of Education and Research (BMBF) to the project MORPHIA (grant agreement no. 16SV8426) and from Carl-Zeiss-Stiftung to the project CO-HUMANICS. the aforementioned challenges in order to accomplish such a high-level task. Our approach performs scene classification, semantic and instance segmentation (panoptic segmentation), as well as instance orientation estimation. However, given limited computing and battery resources on a mobile platform, solving all these tasks separately is expensive and cannot be accomplished in real time. Therefore, we design our approach to solve all aforementioned tasks using a single efficient multitask network. Our approach extends ESANet <ref type="bibr" target="#b1">[2]</ref>, an efficient approach for semantic segmentation, by adding additional heads for tackling panoptic segmentation, instance orientation estimation, and scene classification. ESANet processes both RGB and depth data as input. As shown in <ref type="bibr" target="#b1">[2]</ref>, especially for indoor environments, depth data provide complementary geometric information that help analyzing cluttered indoor scenes. In this paper, we show that this also holds true for panoptic segmentation, instance orientation estimation, and scene classification. Thus, our approach also relies on both RGB and depth data.</p><p>Training such a multi-task approach requires comprehensive datasets. However, to the best of our knowledge, there is no real-world RGB-D indoor dataset encompassing ground-truth annotations for all aforementioned tasks. Therefore, we enrich the existing datasets NYUv2 <ref type="bibr" target="#b2">[3]</ref> and SUNRGB-D <ref type="bibr" target="#b3">[4]</ref> with additional annotations for instance segmentation and instance orientation estimation. With this data at hand, we first train single-task baselines and subsequently combine multiple tasks in several multi-task settings. We experimentally show that all tasks can be solved using a single neural network in real time without diminishing performance -by contrast, the individual tasks are able to boost each other. Our full multi-task approach reaches 24.5 FPS on the mobile platform NVIDIA Jetson AGX Xavier, while achieving state-of-the-art performance. Thus, it is well suited for real-world applications on mobile platforms. In summary, our main contributions are:</p><p>? an efficient RGB-D multi-task approach for panoptic segmentation, scene classification, and instance orientation estimation (EMSANet) including a novel encoding for instance orientations ? enriched annotations for NYUv2 and SUNRGB-D ? detailed experiments regarding performance of each task in single-and multi-task settings as well as corresponding inference throughputs on an NVIDIA Jetson AGX Xavier. Our code, the additional annotations for NYUv2 and SUNRGB-D as well as the trained models are publicly available at: https://github.com/TUI-NICR/EMSANet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>In the following, we briefly summarize related work for each task. Moreover, we give some insights on combining tasks in multi-task settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Semantic Segmentation</head><p>Architectures for semantic segmentation typically follow an encoder-decoder design to accomplish dense pixel-wise predictions. Well-known approaches such as PSPNet <ref type="bibr" target="#b4">[5]</ref> or the DeepLab series <ref type="bibr" target="#b5">[6]</ref>- <ref type="bibr" target="#b7">[8]</ref> achieve good results but cannot be executed in real time on mobile platforms due to their low downsampling of intermediate feature representation. Thus, another line of research emerged, focusing on low inference time while still keeping high performance. For example, ERFNet <ref type="bibr" target="#b8">[9]</ref> introduces a more efficient block by spatially factorizing the expensive 3?3 convolution into a 3?1 and a 1?3 convolution and, thus, reduces computational effort. By contrast, SwiftNet <ref type="bibr" target="#b9">[10]</ref> simply uses a pretrained ResNet18 <ref type="bibr" target="#b10">[11]</ref> as encoder with early and high downsampling, resulting in low inference time but still good performance as well.</p><p>While the aforementioned approaches only process RGB data, especially for indoor applications, others <ref type="bibr" target="#b11">[12]</ref>- <ref type="bibr" target="#b15">[16]</ref> also incorporate depth data as they provide complementary geometric information that help analyzing cluttered scenes. Most approaches use two encoders for processing RGB and depth data (RGB-D) first separately and fuse the resulting features later in the network. However, almost all RGB-D approaches use deep and complex network architectures and do not focus on fast inference. By contrast, our recently published ESANet <ref type="bibr" target="#b1">[2]</ref> combines the merits of efficient and RGB-D semantic segmentation. It utilizes a carefully designed architecture featuring a dual-branch RGB-D ResNet-based encoder with high downsampling and spatially factorized convolutions enabling fast inference. Our experiments in <ref type="bibr" target="#b1">[2]</ref> reveal that processing both RGB and depth data with shallow backbones is superior to utilizing only RGB data and a deeper backbone.</p><p>Therefore, our approach follows ESANet and extends its architecture with additional heads tackling the remaining tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Panoptic Segmentation</head><p>Panoptic segmentation <ref type="bibr" target="#b16">[17]</ref> was introduced to unify semantic segmentation (assigning a class label to each pixel) and instance segmentation (assigning a unique id to pixels of the same instance) in a single task. In panoptic segmentation, semantic classes for countable objects are regarded as thing classes and represent foreground. Background classes, such as wall or floor -known as stuff classes -do not require instances. Thus, all associated pixels have the same instance id. Approaches for panoptic segmentation can be categorized in top-down, bottom-up, and end-to-end approaches. Top-down approaches typically extend two-stage instance segmentation approaches such as Mask R-CNN <ref type="bibr" target="#b17">[18]</ref> with an additional decoder for semantic segmentation <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>. Although topdown approaches typically achieve superior performance, they have several major drawbacks: As instance segmentation approaches can output overlapping instance masks, further logic is required to resolve these issues in order to merge instance and semantic segmentation without contradictions. Moreover, they require complex training and inference pipelines, making them less suitable for mobile applications. On the other hand, bottom-up approaches extend encoder-decoder-based architectures for semantic segmentation and separate thing classes into instances by grouping their pixels into clusters <ref type="bibr" target="#b20">[21]</ref>- <ref type="bibr" target="#b22">[23]</ref>. As bottom-up approaches neither require region proposals, estimating multiple masks independently, nor further refinement steps, their pipelines for training and inference are much simpler compared to top-down approaches. However, until Panoptic DeepLab <ref type="bibr" target="#b22">[23]</ref> bottom-up approaches could not compete with top-down approaches in terms of panoptic quality. Nevertheless, both top-down and bottom-up approaches require additional logic for merging instance and semantic segmentation. The recently proposed MaX-DeepLab <ref type="bibr" target="#b23">[24]</ref> follows another approach based on a novel dual-path transformer architecture <ref type="bibr" target="#b24">[25]</ref> and attempts to directly predict the panoptic segmentation using an end-to-end pipeline. However, research for this kind of approaches currently focuses on establishing new architectures and not on fast and efficient inference.</p><p>Unlike for semantic segmentation, there are only a few approaches targeting efficiency <ref type="bibr" target="#b25">[26]</ref>- <ref type="bibr" target="#b29">[30]</ref>. However, their target hardware is different as they only report inference times on high-end GPUs. Execution on mobile platforms, such as an NVIDIA Jetson AGX Xavier, is expected to be much slower.</p><p>Our approach follows the bottom-up idea as it is straightforward to be integrated into ESANet and expected to enable faster inference on mobile platforms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Orientation Estimation</head><p>Orientation estimation is often done along with 3D bounding box detection <ref type="bibr" target="#b30">[31]</ref>- <ref type="bibr" target="#b32">[33]</ref> and deeply integrated into such architectures. Adapting these detectors to also accomplish dense predictions would require fundamental changes and, thus, is not suitable for our application. Another field of research strongly related to orientation estimation is person perception <ref type="bibr" target="#b33">[34]</ref>- <ref type="bibr" target="#b37">[38]</ref>. Besides estimating a person's orientation inherently using its skeleton <ref type="bibr" target="#b33">[34]</ref>, there are also approaches directly estimating the orientation from patches <ref type="bibr" target="#b34">[35]</ref>- <ref type="bibr" target="#b37">[38]</ref>. This can be performed using either classification or regression. However, as shown in <ref type="bibr" target="#b34">[35]</ref>, classification adds further discretization inaccuracy and does not account well for periodicity. Therefore, approaches such as <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b35">[36]</ref> rely on regression and estimate the angle through its sine and cosine parts, which is often called Biternion encoding <ref type="bibr" target="#b34">[35]</ref>. The same authors also proposed to use the von Mises loss function <ref type="bibr" target="#b34">[35]</ref> instead of L1 or MSE loss as it further improves accounting periodicity and avoiding discontinuities.</p><p>Our approach follows the latter idea and formulates orientation estimation as regression. However, instead of using a patch-based approach, we propose a novel way to accomplish dense orientation estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Scene Classification</head><p>Scene classification, i.e., assigning a scene label such as kitchen or living room to an input image, is similar to other classification tasks such as the ImageNet-Challenge <ref type="bibr" target="#b38">[39]</ref>. Thus, well known architectures <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b39">[40]</ref>- <ref type="bibr" target="#b41">[42]</ref> can be used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Multi-task Learning</head><p>Multi-task learning refers to learning multiple tasks simultaneously in a single neural network. As these tasks commonly share at least some network parameters, inference is faster compared to using an independent network for each task. Moreover, in <ref type="bibr" target="#b42">[43]</ref> it is shown, that dense prediction tasks may benefit from another when trained together. Especially early network layers are known to learn common features and, thus, can be shared among multiple tasks -in literature this is referred to as hard-parameter sharing <ref type="bibr" target="#b43">[44]</ref>. Some approaches <ref type="bibr" target="#b44">[45]</ref>, <ref type="bibr" target="#b45">[46]</ref> also exchange information in the taskspecific heads, which is called soft-parameter sharing. However, when utilizing soft-parameter-shared task heads, these tasks cannot be decoupled anymore. This means that the whole network needs to be applied during inference, even though only a single task may be of interest. Therefore, our approach uses a hard-parameter shared RGB-D encoder and independent task-specific heads, not sharing any network parameters or information. We show that semantic and instance segmentation, instance orientation estimation as well as scene classification benefit from such a multi-task setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. EFFICIENT MULTI-TASK RGB-D SCENE ANALYSIS</head><p>Our Efficient Multi-task Scene Analysis Network (EM-SANet) extends the encoder-decoder-based ESANet <ref type="bibr" target="#b1">[2]</ref> for efficient RGB-D semantic segmentation. As shown in <ref type="figure">Fig. 2</ref> (top), ESANet features two identical encoders, one for processing RGB images and one for depth images. For efficiency reasons, both encoders are based on a ResNet34 <ref type="bibr" target="#b10">[11]</ref> backbone. For even faster inference and improved accuracy, the 3?3 convolutions are spatially factorized, resulting in the NonBottleneck1D block (NBt1D) <ref type="bibr" target="#b8">[9]</ref> (see <ref type="figure">Fig. 2</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>violet).</head><p>At each resolution stage of the encoders, an attention-based mechanism is used to fuse depth features into the RGB branch enhancing its representation with additional geometric information. After the last fusion, a context module similar to the Pyramid Pooling Module in PSPNet <ref type="bibr" target="#b4">[5]</ref> is attached. It incorporates context information at multiple scales using several branches with different pooling sizes (see <ref type="bibr" target="#b1">[2]</ref> for details). The decoder is comprised of three decoder modules (see <ref type="figure">Fig. 2</ref> light red), each is refining and upsampling the intermediate feature maps to gradually restore input resolution. This is done by a 3?3 convolution followed by three NonBottleneck1D blocks and a final learned upsampling by a factor of two. The learned upsampling (see <ref type="figure">Fig. 2</ref> dark green) is initialized to mimic bilinear upsampling first. However, as its weights are not fixed, the network is able to learn to combine adjacent features in a more useful manner during training. Additional encoder-decoder skip connections further help to restore spatial details that were lost during downsampling in the encoders. Following the last decoder module, a 3?3 convolution maps the features to semantic classes. Finally, two additional learned upsamplings restore the input resolution. The entire network is trained end-to-end with additional side outputs and multi-scale supervision as depicted in <ref type="figure">Fig. 2</ref>.</p><p>ESANet builds a strong and efficient baseline for semantic segmentation. However, the architecture is specifically tailored for semantic segmentation. In order to improve its generalization capability for the remaining tasks, we further add a slight dropout with rate of 0.1 to all NonBottle-neck1D blocks. Furthermore, we change the initialization in all RGB-D fusion modules to He-initialization <ref type="bibr" target="#b46">[47]</ref> and force zero-initialization <ref type="bibr" target="#b47">[48]</ref> in all NonBottleneck1D blocks. Finally, to incorporate the loss of other tasks more effectively, we do not reduce the accumulated loss by the sum of the applied semantic class weights but only by the number of all pixels across all outputs of the network.</p><p>Next, we present the extension to a multi-task network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Panoptic Segmentation</head><p>For panoptic segmentation, a second decoder for instance segmentation is required. As shown in <ref type="figure">Fig. 2</ref> (middle), our instance decoder follows the same architecture as the semantic decoder except the task-specific heads. The instance encoding follows the implementation of Panoptic DeepLab <ref type="bibr" target="#b22">[23]</ref>: Instances are represented by their center of mass encoded as small 2D Gaussian within a heatmap, similar to keypoint estimation in other domains <ref type="bibr" target="#b33">[34]</ref>. With an additional head, the instance decoder also predicts offset vectors for each pixel pointing towards a corresponding instance center in x and y direction. As instances are only required for pixels belonging to thing classes -i.e., all classes except wall, floor, and ceiling -a corresponding foreground mask is derived from the semantic segmentation. All thing pixels are then grouped into class-agnostic instances by combining both instance centers  <ref type="bibr" target="#b1">[2]</ref> for semantic segmentation (top) with an additional decoder for instance segmentation and instance orientation estimation as well as a head for scene classification. See <ref type="figure">Fig. 4</ref> for semantic label colors. and offset predictions. The semantic class for each instance is derived by a majority vote from the semantic segmentation.</p><p>Similar to Panoptic DeepLab, we use MSE loss for center prediction and L1 loss for offset prediction. However, unlike Panoptic DeepLab, we mask predicted centers using the ground-truth instance mask instead of the thing-class mask to account for missing instance annotations in the ground truth. We also adopt their postprocessing, including thresholding and keypoint non-maximum suppression using max-pooling for the centers and the final merging of instance and semantic segmentation putting more focus on instances. However, we faced some problems when applying their training regime. Panoptic DeepLab uses linear outputs for estimating both centers and absolute offsets. This results in losses being unbounded and quite imbalanced, and, thus, requires a carefully tuned initialization and loss weights such as 200 : 0.1 for center : offsets as used in their implementation. Moreover, absolute offset vectors do not generalize to varying input resolutions. To address these issues, we use sigmoid activation for instance centers and a tanh activation for encoding relative instance offsets. Thus, the outputs are bounded within [0, 1] and [?1, 1], respectively. We observed great improvements in terms of stability during optimization and performance for instance segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Instance Orientation Estimation</head><p>Our approach further predicts the orientation for instances of thing classes relevant for our indoor scenario, i.e., cabinet, bed, chair, sofa, bookshelf, shelves, dresser, refrigerator, tv, person, nightstand, and toilet. The orientation is crucial when approaching objects from the right direction (e.g., chairs or persons) or to restrict waiting positions (e.g., the robot should not wait in the sightline to a TV or in front of a freestanding chair or cabinet). To accomplish this, as shown in <ref type="figure">Fig. 2</ref> (middle), our instance decoder also predicts orientations as continuous angles around the axis perpendicular to the ground (see bottom-left legend for orientation encoding in <ref type="figure">Fig. 2</ref>). Instead of relying on a patch-based orientation estimation, we follow our dense prediction design and propose to predict the orientation for all pixels of an instance. This way, the instance awareness of our instance decoder can further be strengthened. Moreover, to determine an instance's orientation, we are able to average multiple predictions approximating an ensemble effect. We use the biternion encoding <ref type="bibr" target="#b34">[35]</ref> and the von Mises loss function <ref type="bibr" target="#b34">[35]</ref> to account for the periodicity of angles and to avoid discontinuities in the loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Scene Classification</head><p>For scene classification, as shown in <ref type="figure">Fig. 2 (bottom)</ref>, we simply apply a fully-connected layer on top of the context module. However, as scene classification requires global context, we connect the fully-connected layer directly to the global average pooled branch of the context module. Due to the noisy nature of scene classes, we further utilize label smoothing during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. DATASETS</head><p>Training our proposed multi-task approach is challenging as it requires comprehensive data providing annotations for all tasks. Moreover, our approach relies on both RGB and depth images as input. Based on these requirements and our application scenario, below, we examine common RGB-D datasets for their suitability. Furthermore, we describe how we enriched these datasets to enable training our multi-task approach. Additional annotations are publicly available.</p><p>NYUv2: The NYUv2 dataset <ref type="bibr" target="#b2">[3]</ref> provides dense annotations for both semantic and instance segmentation. For semantics, we use the common 40 classes setting. However, this may lead to very small instances getting assigned to misleading classes, e.g. door knobs are assigned to cabinet, dresser, or nightstand. To avoid such bad assignments, we restrict instances to have at least an area of 0.25% of the image area. For enabling panoptic segmentation, we declare wall, floor, and ceiling to be background (stuff classes) and consider the remaining classes as thing classes. In addition to these dense annotations, NYUv2 also provides ground-truth labels for scene classification. However, annotations for instance orientations are missing so far. Therefore, we manually annotated the orientation for instances of the semantic classes mentioned in Sec. III-B. Due to perspective distortions, exact annotation of the orientation as egocentric angle around the axis perpendicular to the ground is not possible in the pure RGB image, which is why we annotated them directly in a point cloud.</p><p>SUNRGB-D: The SUNRGB-D dataset <ref type="bibr" target="#b3">[4]</ref> combines multiple indoor RGB-D datasets, including NYUv2, and enriches them with additional annotations, making SUNRGB-D to be one of the most important datasets for real-world applications. The dataset comes with annotations for scene classification and semantic segmentation. Compared to NYUv2, the last three semantic filling classes, i.e., otherstructure, otherfurniture, and otherprop, are omitted and assigned to void. Moreover, some semantic annotations in the NYUv2 part have further been assigned to the void class, resulting in minor differences to the original NYUv2 dataset. Unfortunately, annotations for instance segmentation and instance orientation estimation are missing. However, fortunately, SUNRGB-D also provides 3D bounding boxes, each with a class label and orientation, that can be used for instance extraction. To obtain instances, we first created a mapping between semantic and box classes. Subsequently, we matched the box clusters with the semantic point cloud in 3D. During matching, a unique instance label was assigned to all pixels belonging to the same semantic class. This way, an instance mask as well as the orientation could be extracted for each bounding box. However, one limitation of this approach is that not all objects within a scene were annotated with a 3D bounding box, making the instance masks more sparse. To compensate this to some extent, we also merged the instance masks and orientations of NYUv2 back to SUNRGB-D. For panoptic segmentation, we consider the same semantic classes to belong to stuff as for NYUv2.  Hypersim: Unlike SUNRGB-D and NYUv2, Hypersim <ref type="bibr" target="#b48">[49]</ref> is a photo-realistic synthetic dataset. For its creation, virtual cameras were placed in 461 professionally rendered 3D scenes, resulting in 77,400 samples, of which we use 72,419. We blacklisted the remaining samples due to several scene or trajectory issues, i.e., void/single semantic label only, missing textures, or invalid depth. Each sample provides an RGB-D image, a mask for semantics and instances, instance orientations, and a scene label. However, the annotations for instance orientation are not consistent and, thus, cannot be used without further manual refinement. As Hypersim adopts the NYUv2 semantic classes, the same partitioning for stuff and thing can be applied for panoptic segmentation.</p><p>Final remarks and further adjustments: Due to the additional annotations, both NYUv2 and SUNRGB-D are suitable for training our full multi-task approach. HyperSim provides high-quality synthetic data and, thus, is well suited for pretraining. Tab. I summarizes important statistics for all datasets used for training and evaluating our multi-task approach. For scene classification, we further created an own spectrum of classes that unifies the classes in all datasets and accounts for similar classes. The resulting spectrum is tailored for indoor applications and is comprised of the classes listed in Tab. II. Note that the void class is used for images with unclear assignment that may disturb the learning process. Furthermore, images that show indoor scenes but cannot be assigned to one of the mentioned classes are considered as other indoor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. EXPERIMENTS</head><p>We evaluate our approach in several settings on the indoor datasets NYUv2 and SUNRGB-D. First, we use the smaller NYUv2 dataset to elaborate suitable hyperparameters and task weights. We establish single-task baselines for each task and, subsequently, compare them to several multi-task settings.</p><p>Finally, we extend our studies to SUNRGB-D and Hypersim to examine the applicability to larger datasets and the relevance of synthetic data for pretraining.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Implementation Details</head><p>Our architecture as well as the pipelines for training and evaluation are implemented using PyTorch <ref type="bibr" target="#b49">[50]</ref>. We used pretrained weights on ImageNet <ref type="bibr" target="#b38">[39]</ref> to initialize both encoders and trained each network for 500 epochs with a batch size of 8. For optimization, we used SGD with momentum of 0.9 and a small weight decay of 0.0001. For determining a suitable learning rate, we performed a grid search with values of {0.00125, 0.0025, 0.005, 0.01, 0.02, 0.03, 0.04, 0.08, 0.12}. The learning rate was further adapted during training using a one-cycle learning rate scheduler. To increase the number of samples, we augmented images using random scaling, cropping, and flipping. For RGB images, we further applied slight color jittering in HSV space.</p><p>For postprocessing instance centers, we first apply a threshold of 0.1 and max-pooling with pooling size of 17 to perform keypoint non-maximum suppression, and finally filter the top-64 instances. The pooling size results in the network not being able to predict instance centers closer than 8 pixels away from each other. However, for both NYUv2 and SUNRGB-D, this decision affects less than 1% of the instances. For further details and other hyperparameters, we refer to our implementation available on GitHub.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Metrics</head><p>As we focus on fast inference, we do not apply any testing tricks such as horizontal flipping or multiscale inputs. Before obtaining any performance metric for dense predictions, we resize the predictions to the full resolution of the ground truth.</p><p>Semantic Segmentation (Sem): As common for this task, we use the mean intersection over union (mIoU).</p><p>Panoptic Segmentation: The common metric for panoptic segmentation is the panoptic quality (PQ) <ref type="bibr" target="#b16">[17]</ref>. The panoptic quality PQ c for each class c is determined by the product of the recognition quality (RQ c ), i.e. the percentage of correctly detected instances similar to F 1 -score, and the segmentation quality (SQ c ), i.e., the segmentation accuracy similar to mIoU but only for matched segments. These metrics are typically averaged across all stuff (st) and thing (th) classes, resulting in RQ st , SQ st , PQ st and RQ th , SQ th , PQ th , respectively. It is also common to average the three metrics independently of stuff and things across all classes, resulting in RQ, SQ, and PQ. Note that determining the metrics this way results in PQ to be typically not equal to RQ ? SQ. For SUNRGB-D, we further ignore the classes floor mat and shower curtain as no instances of these classes occur in the test split.</p><p>Instance Segmentation (Ins): For instance segmentation, we also stick to panoptic quality instead of reporting the average precision (AP) as this would require assigning a confidence score to each instance <ref type="bibr" target="#b50">[51]</ref>. Moreover, AP and PQ track closely, which is why the latter also evaluates instance segmentation in a meaningful way <ref type="bibr" target="#b16">[17]</ref>.</p><p>Orientation Estimation (Or): For evaluating instance orientations, we use the mean absolute angular error (MAAE) in degrees similar to <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b37">[38]</ref>, i.e., in contrast to the other metrics, lower is better, and the maximum error is 180. We report the MAAE for two settings: 1) independently of other tasks, i.e., using the ground-truth instances, and 2) for matched instances after panoptic merging. Note that we do not penalize unmatched instances for the latter setting.</p><p>Scene Classification (Sce): As the scene class labels are imbalanced, we evaluate scene classification with the balanced accuracy (bAcc).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Single-task Setting</head><p>We aim at solving multiple tasks at once using a single neural network. To be able to elaborate whether this diminishes or boosts the performance of individual tasks, we first conducted experiments in a single-task setting. Furthermore, the goal was to examine how the new tasks behave for different modalities, and how the additional network parts affect inference time on a mobile platform. Note that performing instance segmentation solely requires semantics and a foreground mask. We use the ground-truth semantic segmentation in this case, leading to RQ st , SQ st , and PQ st to always be equal to 1. Moreover, as instance orientation estimation requires instance masks, we rely on the ground-truth instances as well. <ref type="figure">Fig. 3</ref> shows the results of this set of experiments. It becomes obvious that all tasks are able to benefit from incorporating complementary information processed in an additional depth branch instead of processing RGB only. For semantic and instance segmentation, the results coincide with our findings in <ref type="bibr" target="#b1">[2]</ref> that, whenever possible, processing both RGB and depth should be preferred over applying a more sophisticated single-modality RGB encoder. The results further show that depth is crucial for estimating the orientation more accurately, and that RGB is essential for scene classification. Finally, when comparing different backbones, it becomes obvious that, for all tasks except of scene classification, backbones featuring the NonBottleneck1D block (NBt1D) lead to better results in terms of both performance and inference throughput compared to their counterparts with BasicBlock. Even more, ResNet34 NBt1D often competes with the more complex ResNet50 while enabling faster inference. Therefore, we stick to ResNet34 NBt1D for the remaining experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Multi-task Setting</head><p>Learning multiple tasks using a single neural network is challenging, as the tasks may influence each other. Thus, tuning the task weights, which balance the losses to each other, is crucial. With uncertainty weighting <ref type="bibr" target="#b51">[52]</ref>, GradNorm <ref type="bibr" target="#b52">[53]</ref>, Dynamic Weight Average <ref type="bibr" target="#b53">[54]</ref>, and VarNorm <ref type="bibr" target="#b54">[55]</ref>, several approaches for determining the weights automatically have been proposed. Unfortunately, none of them led to good performance in our scenario. Therefore, we performed extensive experiments to determine suitable task weights. We first combined two tasks at a time to elaborate essential relations between tasks. With these findings at hand, we were able to (a) Semantic Segmentation (Sem) <ref type="bibr" target="#b19">20</ref>  restrict the search space for the full multi-task setting. Tab. III summarizes the best results and compares them to their singletask counterparts of <ref type="figure">Fig. 3</ref>. Furthermore, it lists the applied task weights, the learning rate, and the achieved frames per second on an NVIDIA Jetson AGX Xavier. Sem + Sce: As shown in Tab. III (MT I), combining both tasks requires a much larger weight for semantic segmentation to reach its single-task performance. However, even when putting more weight on semantic segmentation, scene classification benefits from such a setting, already closing the gap shown in <ref type="figure">Fig. 3 (d)</ref> for ResNet34 NBt1D and ResNet34. This shows that knowledge about the individual parts of a scene is shared and helps to classify the scene.</p><p>Sem + Ins: This setting allows obtaining panoptic results with predicted semantic segmentation for the first time. As the semantic segmentation provides the semantics and the foreground mask for instance segmentation, combing these two tasks is crucial for our multi-task system. As shown in Tab. III (MT II), the best PQ is achieved by putting more focus on instance segmentation. The mIoU obtained for the semantic decoder indicates that the network further benefits from performing both tasks in conjunction. When keeping in mind, that the PQ for the instance decoder is computed using the ground-truth semantic segmentation, it is reasonable that the PQ for the panoptic results is lower. With ground-truth semantic segmentation, the network reaches an RQ of 70.15 and a SQ of 85.78 (not listed in Tab. III). This shows that the drop in PQ is mainly due to a loss in RQ. We observed that this is mostly caused by small instances, which are not part of the predicted foreground mask.</p><p>Ins + Or: The results in Tab. III (MT III) reveal that both tasks can be performed using a single decoder. Compared to the single-task baseline, this setting slightly improves orientation estimation, almost surpassing the reachable level for annotating orientations in 3D. However, even when putting more weight on instance segmentation, we always observed a slight drop in PQ.</p><p>Sem + Sce + Ins + Or: With the findings of the aforementioned dual-task experiments at hand, we combined all tasks in a single neural network. The best result is presented in Tab. III (MT IV). It becomes obvious that both semantic segmentation and scene classification greatly benefit from the entire multi-task setting. Instance segmentation and instance orientation estimation almost reach the same level of accuracy as when performed in single-task settings. The panoptic results, i.e., after merging semantic and instance predictions, are similar to the multi-task setting MT II. The mIoU obtained after merging is slightly lower than before merging but still at a similar level. This indicates that the applied merging of both predictions with focus on instances does not diminish the semantic segmentation result. The detailed breakdown of the IoUs in <ref type="figure">Fig. 4</ref> shows that this holds true for almost all classes. Finally, when taking a look at the results for orientation estimation, it can be seen that the orientation error after merging is lower than the error for the instance decoder. However, this does not necessarily indicate better results, as the MAAE after panoptic merging only represents instances that could be matched. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Results on SUNRGB-D</head><p>After elaborating suitable multi-task parameters, we also applied our approach to the larger SUNRGB-D dataset. However, as instances in SUNRGB-D are more sparse, we put less weight on the instance decoder and used the mIoU for determining the best checkpoint. The results are listed in Tab. IV. Compared to the single-task baselines, our multitask approach reaches slightly better performance for semantic segmentation and scene classification. The result for orientation estimation is still suitable for real-world application. As shown in <ref type="figure" target="#fig_2">Fig. 5</ref>, the network generalizes well for instance segmentation even though annotations are much more sparse.   <ref type="figure">Fig. 4</ref>. Semantic IoUs on NYUv2 test split for the full multi-task network (MT IV) before and after merging semantic and instance segmentation.</p><p>Note that the panoptic quality does not account for these areas as they are labeled as void class in the ground truth. However, as shown in the last row, missing instance centers can still lead to assigning pixels far away to the same instance, lowering the PQ and mIoU after merging. We already observed a great benefit when masking centers based on the groundtruth instance mask during training, as proposed in Sec. III-A. For real-world application, we further tackle this issue by thresholding predicted offsets after shifting and assign an unknown instance label if they are too far away from a center.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Additional Pretraining on Hypersim</head><p>Finally, we examined how pretraining on the synthetic Hypersim dataset affects the performance of our derived multitask setting for both NYUv2 and SUNRGB-D. For further details on the pretraining, we refer to our implementation. The results are shown in Tab. III (MT V) and Tab. IV (MT V). It turns out that, for NYUv2, especially mIoU and PQ greatly benefit from additional pretraining, while, for SUNRGB-D, only the performance of instance-related tasks is improved. This can be deduced to the fact that SUNRGB-D alone is already much larger than NYUv2. <ref type="figure">Fig. 6</ref> shows qualitative  results from pretraining on Hypersim and subsequent training on NYUv2. The latter represents our best network for NYUv2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Comparison to State of the Art</head><p>Comparing our approach to the state of the art is challenging as tasks such as orientation estimation or scene classification have not been considered so far in related work due to missing annotations and a deviating class spectrum. Moreover, panoptic segmentation has not yet been attempted on NYUv2 or SUNRGB-D. Therefore, we first established comprehensive single-task baselines (see <ref type="figure">Fig. 3</ref>) covering common backbones ranging from sophisticated backbones to more efficient backbones that also enable mobile application. Beyond that, we further trained well-known approaches for panoptic segmentation and scene classification on NYUv2, as shown in Tab. V. For Panoptic DeepLab, we applied the same parameters as described in Sec. III-A for keypoint non-maximum suppression to postprocess instance centers. To summarize, our proposed lightweight EMSANet achieves comparable or even better results than other approaches. Moreover, larger backbones do not necessarily improve performance but significantly increase resource requirements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSION</head><p>In this paper, we have proposed an efficient RGB-D multitask approach for panoptic segmentation, instance orientation estimation, and scene classification, called EMSANet. For training and evaluation, we have enriched the annotations of the common RGB-D indoor datasets NYUv2 and SUNRGB-D, which we also make publicly available. To the best of our knowledge, we are the first to provide results in such a comprehensive multi-task setting for indoor scene analysis.</p><p>We have shown that all tasks can be solved using a single multi-task network. Moreover, the individual tasks can benefit from each other when trained together. Due to the efficient design, our approach enables fast inference, i.e. 24.5 FPS on an NVIDIA Jetson AGX Xavier and, thus, is well suited for mobile robotic applications.  <ref type="figure">Fig. 6</ref>. Qualitative results as RGB image overlayed with predicted panoptic segmentation, predicted scene class, and, for NYUv2, estimated orientations.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>to the closest chair in the living room and taking a good interaction pose...EMSANet"living room" Prediction of our proposed Efficient Multi-task Scene Analysis Network (EMSANet) that simultaneously performs panoptic segmentation, orientation estimation, and scene classification. With 24.5 FPS on an NVIDIA Jetson AGX Xavier it is well suited for mobile robotic applications. SeeFig. 4for semantic label colors. Color variations indicate individual instances.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 5 .</head><label>5</label><figDesc>Qualitative results on SUNRGB-D test split highlighting faced challenges (Sec V-E) as RGB image, ground-truth panoptic segmentation with orientations, and predicted panoptic segmentation with orientations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Batch Normalization, Up.: Upsampling, DW: Depthwise : attention-based fusion of depth features into RGB branch FC: Fully Connected Layer</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Legend:</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Convolution with kernel size</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>BN:</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">RGB Encoder</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">Depth Encoder</cell><cell>Context Module</cell><cell>Dec</cell><cell>Mod</cell><cell>Decoder</cell><cell>Module</cell><cell>Decoder</cell><cell>Module</cell><cell>Semantic Segmentation (Sem)</cell><cell>Foreground Mask</cell></row><row><cell cols="2">ESANet</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Semantic Decoder</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Panoptic Segmentation</cell></row><row><cell>Decoder Module Up. ?2</cell><cell>Multi-Scale Supervision</cell><cell>NBt1D NBt1D NBt1D</cell><cell>NonBottleneck1D (NBt1D)</cell><cell>ReLU BN, ReLU ReLU BN, Dropout ReLU</cell><cell>Dec</cell><cell>Mod</cell><cell>Decoder</cell><cell>Module</cell><cell>Decoder</cell><cell>Module</cell><cell>Center Offset</cell><cell>Instance Segmentation (Ins)</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">80?9</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Raw Orientation</cell><cell>Orientation (Or)</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Instance Decoder</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>0?270?O</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>0?1</cell><cell></cell><cell></cell><cell cols="2">"living room"</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>Offset Encoding</cell><cell cols="2">rientation Encoding</cell><cell cols="4">Scene Classification (Sce)</cell><cell></cell><cell></cell><cell>Scene Head</cell><cell></cell></row></table><note>Sofa EMSANet Fig. 2. Architecture of our Efficient Multi-task Scene Analysis Network (EMSANet) extending ESANet</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I</head><label>I</label><figDesc>Overview about the datasets used for our multi-task approach.</figDesc><table><row><cell></cell><cell>Split</cell><cell># Images</cell><cell># Instances</cell><cell># Orientations</cell></row><row><cell>NYUv2</cell><cell>train</cell><cell>795</cell><cell>12,092</cell><cell>2,696</cell></row><row><cell></cell><cell>test</cell><cell>654</cell><cell>9,874</cell><cell>2,069</cell></row><row><cell>SUNRGB-D</cell><cell>train</cell><cell>5,285</cell><cell>18,171</cell><cell>13,076</cell></row><row><cell></cell><cell>test</cell><cell>5,050</cell><cell>16,961</cell><cell>12,440</cell></row><row><cell>Hypersim</cell><cell>train</cell><cell>57,443</cell><cell>3,009,566</cell><cell>-</cell></row><row><cell></cell><cell>valid</cell><cell>7,286</cell><cell>261,677</cell><cell>-</cell></row><row><cell></cell><cell>test</cell><cell>7,690</cell><cell>374,052</cell><cell>-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE II</head><label>II</label><figDesc>Distribution of scene classes for all datasets and their splits.</figDesc><table><row><cell></cell><cell cols="2">NYUv2</cell><cell cols="2">SUNRGB-D</cell><cell></cell><cell>Hypersim</cell><cell></cell></row><row><cell></cell><cell>train</cell><cell>test</cell><cell>train</cell><cell>test</cell><cell>train</cell><cell>valid</cell><cell>test</cell></row><row><cell>void</cell><cell>29</cell><cell>0</cell><cell>745</cell><cell>507</cell><cell>1,370</cell><cell>0</cell><cell>0</cell></row><row><cell>bathroom</cell><cell>63</cell><cell>58</cell><cell>331</cell><cell>293</cell><cell>3,378</cell><cell>400</cell><cell>300</cell></row><row><cell>bedroom</cell><cell>192</cell><cell>191</cell><cell>558</cell><cell>526</cell><cell>4,386</cell><cell>499</cell><cell>400</cell></row><row><cell>dining room</cell><cell>66</cell><cell>55</cell><cell>311</cell><cell>296</cell><cell>1,894</cell><cell>200</cell><cell>100</cell></row><row><cell>discussion room</cell><cell>5</cell><cell>0</cell><cell>691</cell><cell>753</cell><cell>1,013</cell><cell>100</cell><cell>0</cell></row><row><cell>hallway</cell><cell>0</cell><cell>0</cell><cell>222</cell><cell>151</cell><cell>400</cell><cell>100</cell><cell>0</cell></row><row><cell>kitchen</cell><cell>125</cell><cell>110</cell><cell>288</cell><cell>297</cell><cell>6,221</cell><cell>400</cell><cell>600</cell></row><row><cell>living room</cell><cell>114</cell><cell>107</cell><cell>274</cell><cell>250</cell><cell>18,287</cell><cell>3,017</cell><cell>2,855</cell></row><row><cell>office</cell><cell>88</cell><cell>78</cell><cell>820</cell><cell>792</cell><cell>5,026</cell><cell>100</cell><cell>389</cell></row><row><cell>other indoor</cell><cell>113</cell><cell>55</cell><cell>1,041</cell><cell>1,180</cell><cell>15,068</cell><cell>2,370</cell><cell>2,846</cell></row><row><cell>stairs</cell><cell>0</cell><cell>0</cell><cell>4</cell><cell>5</cell><cell>400</cell><cell>100</cell><cell>200</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE III</head><label>III</label><figDesc>Results obtained on NYUv2 test split when training EMSANet with ResNet34 NBt1D backbone in various multi-task settings. See Sec. V-B for details on the reported metrics. Panoptic results are obtained after merging semantic and instance prediction. Legend: italic: metric used for determining the best checkpoint, *: best result within the same run, Lr: learning rate, pre.: additional pretraining on Hypersim, FPS: frames per second on an NVIDIA Jetson AGX Xavier (Jetpack 4.6, TensorRT 8, Float16). PQ ? RQ ? SQ ? MAEE ? FPS ?</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Semantic</cell><cell cols="2">Instance</cell><cell>Scene</cell><cell></cell><cell cols="3">Panoptic Results</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Decoder</cell><cell cols="2">Decoder</cell><cell>Head</cell><cell></cell><cell cols="3">(after merging)</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="8">Task Weights mIoU ? Semantic Segmentation (Sem) Lr mIoU ? PQ ? MAAE ? bAcc ? -0.04 49.66 ----</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>32.4</cell></row><row><cell cols="2">ST</cell><cell>Instance Segmentation (Ins) Orientation Estimation (Or)</cell><cell>--</cell><cell>0.08 0.02</cell><cell>--</cell><cell>61.39 -</cell><cell>-18.06</cell><cell>--</cell><cell>--</cell><cell>--</cell><cell>--</cell><cell>--</cell><cell>--</cell><cell>38.3 35.5</cell></row><row><cell></cell><cell></cell><cell>Scene Classification (Sce)</cell><cell>-</cell><cell>0.02</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>72.40</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>55.9</cell></row><row><cell>MT</cell><cell>I</cell><cell>Sem + Sce Sem + Sce *</cell><cell>3 : 1</cell><cell>0.02</cell><cell>49.57 49.57</cell><cell>--</cell><cell>--</cell><cell>74.29 77.30</cell><cell>--</cell><cell>--</cell><cell>--</cell><cell>--</cell><cell>--</cell><cell>32.2</cell></row><row><cell>MT</cell><cell>II</cell><cell>Sem + Ins Sem + Ins *</cell><cell>1 : 3</cell><cell>0.03</cell><cell>50.22 50.22</cell><cell>60.90 61.61</cell><cell>--</cell><cell>--</cell><cell cols="4">50.24 43.74 52.46 82.43 50.54 43.74 52.50 82.63</cell><cell>--</cell><cell>25.8</cell></row><row><cell>MT</cell><cell>III</cell><cell>Ins + Or Ins + Or *</cell><cell>3 : 1</cell><cell>0.04</cell><cell>--</cell><cell>59.72 59.72</cell><cell>17.66 17.53</cell><cell>--</cell><cell>--</cell><cell>--</cell><cell>--</cell><cell>--</cell><cell>--</cell><cell>35.5</cell></row><row><cell>MT</cell><cell>IV</cell><cell>Sem + Sce + Ins + Or Sem + Sce + Ins + Or *</cell><cell>1 : 0.25 : 3 : 1</cell><cell>0.04</cell><cell>50.97 51.15</cell><cell>61.35 61.53</cell><cell>19.01 18.93</cell><cell>76.46 78.18</cell><cell cols="5">50.54 43.56 52.20 82.48 16.38 51.31 43.56 52.27 82.70 15.76</cell><cell>24.5</cell></row><row><cell>MT</cell><cell>V</cell><cell>Sem + Sce + Ins + Or (pre.) Sem + Sce + Ins + Or (pre.) *</cell><cell>1 : 0.25 : 3 : 1</cell><cell>0.01</cell><cell>53.34 53.55</cell><cell>64.41 64.98</cell><cell>18.84 18.27</cell><cell>75.25 76.98</cell><cell cols="5">53.79 47.38 55.95 83.74 15.91 54.00 47.38 55.99 84.08 15.56</cell><cell>24.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>table door</head><label>door</label><figDesc></figDesc><table><row><cell></cell><cell>counter blinds</cell><cell cols="2">picture</cell><cell cols="2">book-shelf</cell><cell cols="2">window</cell></row><row><cell>desk</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>shelves</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>curtain</cell><cell cols="7">before merging (mIoU: 50.97) after merging (mIoU: 50.54)</cell></row><row><cell>dresser</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>pillow</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>mirror</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>floor mat</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>clothes</cell><cell></cell><cell>0</cell><cell cols="2">10</cell><cell></cell><cell></cell></row><row><cell>ceiling</cell><cell></cell><cell></cell><cell></cell><cell>20</cell><cell cols="2">30</cell><cell>40</cell><cell>other prop</cell></row><row><cell>books refrigerator</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>50</cell><cell>60</cell><cell>70</cell><cell>80</cell><cell>90</cell><cell>other furniture other structure</cell></row><row><cell>television</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>bag</cell></row><row><cell>paper</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>bathtub</cell></row><row><cell>towel</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>lamp</cell></row><row><cell cols="4">shower curtain box white-board person</cell><cell cols="2">night stand</cell><cell cols="2">toilet</cell><cell>sink</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE IV</head><label>IV</label><figDesc>Results obtained on SUNRGB-D test split when training EMSANet with ResNet34 NBt1D backbone in both single-task and multi-task settings. See Sec. V-B for details on the reported metrics. Panoptic results are obtained after merging semantic and instance prediction. Legend: italic: metric used for determining the best checkpoint, *: best result within the same run, Lr: learning rate, pre.: additional pretraining on Hypersim.PQ ? MAAE ? bAcc ? mIoU ? PQ ? RQ ? SQ ? MAEE ?</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Semantic</cell><cell cols="2">Instance</cell><cell>Scene</cell><cell></cell><cell cols="3">Panoptic Results</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Decoder</cell><cell cols="2">Decoder</cell><cell>Head</cell><cell></cell><cell cols="3">(after merging)</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Task Weights</cell><cell>Lr</cell><cell>mIoU ?</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>Semantic Segmentation (Sem)</cell><cell>-</cell><cell>0.005</cell><cell>48.23</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">ST</cell><cell>Instance Segmentation (Ins) Orientation Estimation (Or)</cell><cell>--</cell><cell>0.01 0.005</cell><cell>--</cell><cell>60.99 -</cell><cell>-13.68</cell><cell>--</cell><cell>--</cell><cell>--</cell><cell>--</cell><cell>--</cell><cell>--</cell></row><row><cell></cell><cell></cell><cell>Scene Classification (Sce)</cell><cell>-</cell><cell>0.001</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>58.66</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>MT</cell><cell>IV</cell><cell>Sem + Sce + Ins + Or Sem + Sce + Ins + Or *</cell><cell>1 : 0.25 : 2 : 0.5</cell><cell>0.005</cell><cell>48.39 48.39</cell><cell>60.60 61.48</cell><cell>16.81 16.70</cell><cell>61.83 62.66</cell><cell cols="5">45.56 50.15 58.14 84.85 14.24 45.66 50.53 58.66 85.20 14.15</cell></row><row><cell>MT</cell><cell>V</cell><cell>Sem + Sce + Ins + Or (pre.) Sem + Sce + Ins + Or (pre.) *</cell><cell>1 : 0.25 : 2 : 0.5</cell><cell>0.0025</cell><cell>48.47 48.47</cell><cell>64.24 64.82</cell><cell>18.40 17.94</cell><cell>57.22 59.39</cell><cell cols="5">44.18 52.84 60.67 86.01 14.10 45.04 53.35 61.31 86.25 14.04</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE V</head><label>V</label><figDesc>Comparison to other state-of-the-art approaches on NYUv2 test split without test-time augmentation. Legend: italic: metric used for determining the best checkpoint, *: (re)training on our enriched NYUv2 dataset, ?: ten-crop evaluation at 224?224. pre.: additional pretraining on Hypersim.</figDesc><table><row><cell></cell><cell>Backbone</cell><cell>Mod.</cell><cell>mIoU ?</cell><cell>PQ ?</cell><cell>bAcc ?</cell></row><row><cell>ESANet [2]</cell><cell>2?R34-Nbt1d</cell><cell>RGB-D</cell><cell>50.30</cell><cell>-</cell><cell>-</cell></row><row><cell>ShapeConv [16]</cell><cell cols="2">ResNext101 32x8d RGB-D</cell><cell>50.20</cell><cell></cell><cell></cell></row><row><cell cols="2">Panoptic DeepLab* [23] R50</cell><cell>RGB</cell><cell>39.42</cell><cell>30.99</cell><cell>-</cell></row><row><cell></cell><cell>R101</cell><cell>RGB</cell><cell>42.55</cell><cell>35.32</cell><cell>-</cell></row><row><cell>MobileNetV2* [41]</cell><cell>?=1</cell><cell>RGB</cell><cell>-</cell><cell>-</cell><cell>69.30  ?</cell></row><row><cell>EfficientNet* [42]</cell><cell>B0</cell><cell>RGB</cell><cell>-</cell><cell>-</cell><cell>70.83  ?</cell></row><row><cell>Mod. ResNet34 (ours)</cell><cell>R34-Nbt1d</cell><cell>RGB</cell><cell>-</cell><cell>-</cell><cell>70.25  ?</cell></row><row><cell>EMSANet (ours)</cell><cell>R34-Nbt1d</cell><cell>RGB</cell><cell>44.66</cell><cell>37.69</cell><cell>70.88</cell></row><row><cell></cell><cell>2?R34-Nbt1d</cell><cell>RGB-D</cell><cell>50.97</cell><cell>43.56</cell><cell>76.46</cell></row><row><cell></cell><cell>2?R101-Nbt1d</cell><cell>RGB-D</cell><cell>50.83</cell><cell>45.12</cell><cell>77.41</cell></row><row><cell cols="2">EMSANet pre. (ours) 2?R34-Nbt1d</cell><cell>RGB-D</cell><cell>53.34</cell><cell>47.38</cell><cell>75.25</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Efficient and robust semantic mapping for indoor environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Seichter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Langer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wengefeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lewandowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>H?chemer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-M</forename><surname>Gross</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICRA</title>
		<meeting>of ICRA</meeting>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Efficient rgb-d semantic segmentation for indoor scene analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Seichter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>K?hler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lewandowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wengefeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-M</forename><surname>Gross</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICRA</title>
		<meeting>of ICRA</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="13" to="525" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Indoor Segmentation and Support Inference from RGBD Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ECCV</title>
		<meeting>of ECCV</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">SUN RGB-D: A RGB-D Scene Understanding Benchmark Suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">P</forename><surname>Lichtenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of CVPR</title>
		<meeting>of CVPR</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="567" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of CVPR</title>
		<meeting>of CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2881" to="2890" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Semantic image segmentation with deep convolutional nets and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICLR</title>
		<meeting>of ICLR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Rethinking Atrous Convolution for Semantic Image Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.05587</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ECCV</title>
		<meeting>of ECCV</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="801" to="818" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">ERFNet: Efficient Residual Factorized ConvNet for Real-Time Semantic Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Romera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">M</forename><surname>Bergasa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Arroyo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ITS</title>
		<imprint>
			<biblScope unit="page" from="263" to="272" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">In Defense of Pretrained ImageNet Architectures for Real-time Semantic Segmentation of Road-driving Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Or?i?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kre?o</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bevandi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>?egvi?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of CVPR</title>
		<meeting>of CVPR</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">616</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of CVPR</title>
		<meeting>of CVPR</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">FuseNet: Incorporating Depth into Semantic Segmentation via Fusion-based CNN Architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hazirbas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Domokos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACCV</title>
		<meeting>of ACCV</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="213" to="228" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">ACNet: Attention Based Network to Exploit Complementary Features for RGBD Semantic Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICIP</title>
		<meeting>of ICIP</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Self-supervised model adaptation for multimodal semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Valada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Burgard</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">IJCV</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">RDFNet: RGB-D Multi-level Residual Feature Fusion for Indoor Semantic Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">S</forename><surname>Hong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICCV</title>
		<meeting>of ICCV</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4990" to="4999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Shapeconv: Shape-aware convolutional layer for indoor rgb-d semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Leng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lischinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cohen-Or</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of CVPR</title>
		<meeting>of CVPR</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="7088" to="7097" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Panoptic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of CVPR</title>
		<meeting>of CVPR</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9404" to="9413" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Mask R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICCV</title>
		<meeting>of ICCV</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2961" to="2969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Panoptic feature pyramid networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of CVPR</title>
		<meeting>of CVPR</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6399" to="6408" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Upsnet: A unified panoptic segmentation network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of CVPR</title>
		<meeting>of CVPR</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8818" to="8826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Deeperlab: Single-shot image parser</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-J</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.05093</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Ssap: Single-shot instance segmentation with affinity pyramid</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICCV</title>
		<meeting>of ICCV</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="642" to="651" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Panoptic-deeplab: A simple, strong, and fast baseline for bottom-up panoptic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of CVPR</title>
		<meeting>of CVPR</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">485</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Max-deeplab: End-to-end panoptic segmentation with mask transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of CVPR</title>
		<meeting>of CVPR</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="5463" to="5474" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NeurIPS</title>
		<meeting>of NeurIPS</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Efficientps: Efficient panoptic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Valada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">129</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1551" to="1579" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Lpsnet: A lightweight solution for fast panoptic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of CVPR</title>
		<meeting>of CVPR</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="16" to="746" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Epsnet: efficient panoptic segmentation network with cross-layer attention fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-E</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-Y</forename><surname>Hsiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACCV</title>
		<meeting>of ACCV</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Real-time panoptic segmentation from dense detections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of CVPR</title>
		<meeting>of CVPR</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="8523" to="8532" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Fast panoptic segmentation network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>De Geus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Meletis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Dubbelman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">RAL</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1742" to="1749" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">3d bounding box estimation using deep learning and geometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mousavian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Flynn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kosecka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of CVPR</title>
		<meeting>of CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7074" to="7082" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deep fitting degree scoring network for monocular 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of CVPR</title>
		<meeting>of CVPR</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1057" to="1066" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Objects are different: Flexible monocular 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of CVPR</title>
		<meeting>of CVPR</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="3289" to="3298" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Realtime multi-person 2d pose estimation using part affinity fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of CVPR</title>
		<meeting>of CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7291" to="7299" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Biternion nets: Continuous head pose regression from discrete training labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of GCPR</title>
		<meeting>of GCPR</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="157" to="168" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Deep orientation: Fast and robust upper body orientation estimation for mobile robotic applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lewandowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Seichter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wengefeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pfennig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Drumm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-M</forename><surname>Gross</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IROS</title>
		<meeting>of IROS</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="441" to="448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Real-time person orientation estimation using colored pointclouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wengefeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lewandowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Seichter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pfennig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-M</forename><surname>Gross</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ECMR</title>
		<meeting>of ECMR</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Multi-task deep learning for depth-based person perception in mobile robotics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Seichter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lewandowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>H?chemer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wengefeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-M</forename><surname>Gross</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IROS</title>
		<meeting>of IROS</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10" to="497" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">ImageNet Large Scale Visual Recognition Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="211" to="252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of CVPR</title>
		<meeting>of CVPR</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7132" to="7141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Mobilenetv2: Inverted residuals and linear bottlenecks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of CVPR</title>
		<meeting>of CVPR</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4510" to="4520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Efficientnet: Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICML</title>
		<meeting>of ICML</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6105" to="6114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Which tasks should be learned together in multi-task learning?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Standley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">&quot; in Proc. of ICML. PMLR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9120" to="9132" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Multi-task learning for dense prediction tasks: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vandenhende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Georgoulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Van Gansbeke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Proesmans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">TPAMI</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Mti-net: Multi-scale task interaction networks for multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vandenhende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Georgoulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ECCV</title>
		<meeting>of ECCV</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="527" to="543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Joint task-recursive learning for semantic segmentation and depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ECCV</title>
		<meeting>of ECCV</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="235" to="251" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICCV</title>
		<meeting>of ICCV</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1026" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Accurate, large minibatch sgd: Training imagenet in 1 hour</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02677</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Hypersim: A Photorealistic Synthetic Dataset for Holistic Indoor Scene Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Roberts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICCV</title>
		<meeting>of ICCV</meeting>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NeurIPS</title>
		<meeting>of NeurIPS</meeting>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8024" to="8035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y.</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ECCV</title>
		<meeting>of ECCV</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Multi-task learning using uncertainty to weigh losses for scene geometry and semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of CVPR</title>
		<meeting>of CVPR</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7482" to="7491" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Gradnorm: Gradient normalization for adaptive loss balancing in deep multitask networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y.</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICML. PMLR</title>
		<meeting>of ICML. PMLR</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="794" to="803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">End-to-end multi-task learning with attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Johns</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Davison</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of CVPR</title>
		<meeting>of CVPR</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1871" to="1880" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Omnidet: Surround view cameras based multi-task visual perception network for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">R</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">RAL</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="2830" to="2837" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
