<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">RWF-2000: An Open Large Scale Video Database for Violence Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Cheng</surname></persName>
							<email>ming.cheng@dukekunshan.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Duke Kunshan University Kunshan</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kunjing</forename><surname>Cai</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Sun Yat-sen Universit Guangzhou</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Li</surname></persName>
							<email>ming.li369@dukekunshan.edu.cn</email>
							<affiliation key="aff2">
								<orgName type="institution">Duke Kunshan University Kunshan</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">RWF-2000: An Open Large Scale Video Database for Violence Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T21:41+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In recent years, surveillance cameras are widely deployed in public places, and the general crime rate has been reduced significantly due to these ubiquitous devices. Usually, these cameras provide cues and evidence after crimes are conducted, while they are rarely used to prevent or stop criminal activities in time. It is both time and labor consuming to manually monitor a large amount of video data from surveillance cameras. Therefore, automatically recognizing violent behaviors from video signals becomes essential. This paper summarizes several existing video datasets for violence detection and proposes the RWF-2000 database with 2,000 videos captured by surveillance cameras in real-world scenes. Also, we present a new method that utilizes both the merits of 3D-CNNs and optical flow, namely Flow Gated Network. The proposed approach obtains an accuracy of 87.25% on the test set of our proposed database. The database and source codes are currently open to access 1 .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Recently, video-based violent behavior detection has attracted more and more attention. There is an increasing number of surveillance cameras in public places, collecting evidence and deter potential criminals. However, it is too expensive to monitor a large amount of video data in real-time manually. Thus, automatically recognizing criminal scenes from videos becomes essential and challenging.</p><p>Generally, the definition of video-based violence detection is detecting violent behaviors in video data. It is a subset of human action recognition that aims at recognizing common human actions. Compared to still images, video data has additional temporal sequences. A set of consecutive frames represent a continuous motion, and neighboring frames contain redundant information due to high inter-frame correlation. Thus, many researchers devote to fuse both spatial and temporal information properly.</p><p>Some earlier methods rely on detecting the presence of highly relevant objects (e.g., gun shooting, blaze, blood, blast) rather than directly recognizing the violent events <ref type="bibr" target="#b0">[1]</ref>- <ref type="bibr" target="#b2">[3]</ref>. In 2011, Nievas et al. firstly released two video datasets for violence detection, namely the Hockey Fight dataset <ref type="bibr" target="#b3">[4]</ref> and the Movies Fight dataset <ref type="bibr" target="#b4">[5]</ref>. Later, Hassner et al. <ref type="bibr" target="#b5">[6]</ref> proposed the Crowd Violence dataset in 2012. Since then, most works turn to develop methods that directly recognize violence in videos. There are mainly two categories of methods to recognize human action in videos: the traditional feature extraction with shallow classifiers and the end-to-end deep learning framework. In traditional methods, researchers mainly devote to build powerful video descriptors by feature extraction algorithms and feed them to a classifier (e.g., SVM). Based on this principle, many classical methods are presented: ViF <ref type="bibr" target="#b5">[6]</ref>, STIPs <ref type="bibr" target="#b6">[7]</ref>, SCOF <ref type="bibr" target="#b7">[8]</ref>, iDT <ref type="bibr" target="#b8">[9]</ref>, etc. While in recent years, more and more end-to-end deep learning based methods are proposed, e.g., two-stream method <ref type="bibr" target="#b9">[10]</ref>, ConvLSTM <ref type="bibr" target="#b10">[11]</ref>, C3D <ref type="bibr" target="#b11">[12]</ref>, TSN <ref type="bibr" target="#b12">[13]</ref>, ECO <ref type="bibr" target="#b13">[14]</ref>. Currently, most state-of-theart results are obtained through deep learning based methods.</p><p>Although some video datasets for violence detection already exist, they still have drawbacks of small scale, reduced diversity, and low image resolution. Moreover, some related datasets with high image quality come from movies, which are not close enough to real-world scenes. To solve insufficient high-quality data from real violent activities, we collect a new video dataset (RWF-2000) and freely release it to the research community. This dataset has a large scale with 2,000 clips extracted from surveillance videos. Besides, we present a novel </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Previous Datasets</head><p>According to the annotation method, there are mainly two kinds of video datasets for violence detection: trimmed and untrimmed. The videos in trimmed datasets are all short clips with a length of several seconds, and each one of them has a video-level annotation. While the videos in untrimmed datasets usually have a longer duration. Furthermore, the start time and end time of violent activities have frame-level annotations. <ref type="table" target="#tab_0">Table I</ref> shows the comparison between our proposed RWF-2000 dataset and previous others.</p><p>Blunsden et al. <ref type="bibr" target="#b14">[15]</ref> propose the BEHAVE dataset for multiperson behavior classification. This dataset consists of 2 to 5 people in one or two interacting groups. Invited actors perform ten group behaviors (InGroup, Approach, WalkTogether, Meet, Split, Ignore, Chase, Fight, RunTogether, Following). There are 171 interacting clips recorded by four surveillance videos from different shooting angles.</p><p>Nievas et al. present two video datasets for violence detection, namely the Movies Fight <ref type="bibr" target="#b4">[5]</ref> and the Hockey Fight <ref type="bibr" target="#b3">[4]</ref>. The Movies Fight dataset has 200 clips extracted from short movies. The number of videos is insufficient nowadays. In the Hockey Fight dataset, there are 1,000 clips captured in hockey games of the National Hockey League. One of its disadvantages is the lack of diversity because all the videos are captured in a single scene. Both of these two datasets have video-level annotations.</p><p>Hassner et al. <ref type="bibr" target="#b5">[6]</ref> collect 246 videos with or without violent behaviors, namely the Crow Violence dataset. This dataset aims at recognizing violence in crowded scenes. The length of videos is between 1.04 seconds and 6.53 seconds. The characteristic of this dataset is its overcrowded scenes but low image quality.</p><p>Compared to recognize violence by only RGB data, Yun et al. <ref type="bibr" target="#b18">[19]</ref> present the first violence dataset in the form of RGB-D data. There are eight interactions performed by actors (approaching, departing, pushing, kicking, punching, exchanging objects, hugging, and shaking hands). Using the Microsoft Kinect sensor, 21 pairs of two-actor interactions performed by seven participants are recorded. Also, this dataset provides video-level annotations and gives the joints data of the human skeleton.</p><p>Rota et al. <ref type="bibr" target="#b15">[16]</ref> present the RE-DID (Real-Life Events-Dyadic Interactions) dataset. This dataset is composed of 30 videos with high-quality resolution up to 1280?720. Videos captured by car-mounted cameras account for 25% of the total. Other types of devices take the rest (e.g., mobile phone). Moreover, the bounding boxes of participants with relative IDs, the position of the interpersonal spaces are both provided, which is useful for in-depth analysis.</p><p>Demarty et al. <ref type="bibr" target="#b16">[17]</ref> establish the VSD dataset. The dataset consists of 1,317 clips from 18 selected Hollywood movies (e.g., Kill Bill, Fight Club). The definition of violent content in the film involves blood, fights, fire, guns, cold weapons, car chases, gory scenes, gunshots, explosions, and screams. Each frame in this dataset is annotated as violence or non-violence by these criteria.</p><p>Those above datasets are mainly composed of videos captured in a single scene, performed by actors, or extracted from edited movies. Only a few parts of them are from real events. To make the trained model more practical, Sultani et al. <ref type="bibr" target="#b19">[20]</ref> present the UCF-Crime dataset, which contains 1,900 videos recorded by real-world surveillance cameras. This video dataset is preliminarily designed to detect 13 real anomalies, including abuse, arrest, arson, assault, accident, burglary, explosion, fighting, robbery, shooting, stealing, shoplifting, and vandalism. While there are still some limits, videos in this dataset usually have a long duration ranging from 1 to 10 minutes, but they have only video-level annotations. Hence, detecting violent activities from a long video is very tough.</p><p>CCTV-Fights <ref type="bibr" target="#b17">[18]</ref> is the latest large-scale video dataset for violence detection. It consists of 1,000 videos containing violent activities, and each violent activity is annotated with the start and end time. Only 280 videos are from real-world surveillance cameras, and 720 are taken by other types of devices, including mobile cameras, car cameras, drones, or helicopters. Only a small part of these videos comes from real surveillance cameras.</p><p>Summarizing these proposed datasets, each of them has at least one or more of the following limits:</p><p>? low image quality; ? lack of sufficient data amount; ? videos with long duration but crude annotations; ? hybrid sources of videos that are not close enough to realistic violence. To cope with the above issues, we collect a new RWF-2000 (Real-World Fighting) dataset from the YouTube website, consisting of 2,000 trimmed video clips captured by surveillance cameras from real-world scenes. <ref type="figure" target="#fig_0">Figure 1</ref> demonstrates some video examples in the RWF-2000 dataset. The detailed description of our proposed dataset will be introduced in Section III-A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Previous Methods</head><p>Traditional methods usually try to find a powfer feature extraction algorithm and implement a machine learning based classifier to complete the violence detection task. Some classical models involve space-time interest points (STIP) <ref type="bibr" target="#b6">[7]</ref>, Harris corner detector <ref type="bibr" target="#b20">[21]</ref>, improved dense trajectory (iDT) <ref type="bibr" target="#b8">[9]</ref>, motion scale-invariant feature transform (MoSIFT) <ref type="bibr" target="#b21">[22]</ref>, etc.</p><p>Chen et al. <ref type="bibr" target="#b1">[2]</ref> explore the way to detect violence from compressed video data. They utilize the encoded motion vector from the MPEG-1 video data as extracted features, representing the data component with high motion intensity. By this data format, they build a classifier to detect the presence of blood as violence.</p><p>Hassner et al. <ref type="bibr" target="#b5">[6]</ref> present the violent flows (ViF) descriptors for violence detection. This method can utilize the magnitude series of optical flow over time to detect violent activities. Later, Gao et al. <ref type="bibr" target="#b23">[23]</ref> improve this method by introducing the orientation of the violent flow features into the ViF descriptor, namely the oriented violent flows (OViF).</p><p>Not limited to the field of violence recognition, the improved dense trajectory (iDT) <ref type="bibr" target="#b8">[9]</ref> features has made a significant performance in generic human action recognition. Bilinski et al. <ref type="bibr" target="#b24">[24]</ref> introduce iDT features into the violence recognition. Also, they present an improved Fisher encoding method that is powerful to encode the spatial-temporal information in a video.</p><p>With the fast development of deep learning in recent years, methods based on convolutional neural networks have become the mainstream in video-based action recognition (e.g., TSN <ref type="bibr" target="#b12">[13]</ref>, ECO <ref type="bibr" target="#b13">[14]</ref>, C3D <ref type="bibr" target="#b11">[12]</ref>). Many researchers attempt to address the violence recognition task by introducing deep learning based methods <ref type="bibr" target="#b25">[25]</ref>- <ref type="bibr" target="#b29">[29]</ref>.</p><p>Dong et al. <ref type="bibr" target="#b28">[28]</ref> present a multi-stream deep neural network for inter-person violence detection from videos. The model adopts raw videos, optical flows, and acceleration flow maps as three input branches. Followed by the Long Short Term Memory (LSTM) network, a score-level fusion is employed to adopt the three streams to predict a final result for violence detection.</p><p>Sudhakaran et al. <ref type="bibr" target="#b29">[29]</ref> combine the advantages of both convolutional neural networks and recurrent neural networks to do violence detection. The ConvLSTM network <ref type="bibr" target="#b30">[30]</ref> is modified to take the difference of adjacent frames as input instead of to give a sequence of raw frames. The convolutional neural network is deployed to extract spatial features at the frame-level, and the following LSTM network is used to model the temporal relationships.</p><p>Zhou et al. <ref type="bibr" target="#b26">[26]</ref> build a FightNet to discover the violent activities in videos by multimodal data (e.g., RGB images, optical flow images, and acceleration images). They firstly pre-train the proposed model on a generic action recognition dataset named UCF101 <ref type="bibr" target="#b31">[31]</ref> and then fine-tune it on the Crowd Violence dataset <ref type="bibr" target="#b5">[6]</ref>. At the same time, it is found that the model is easy to fall into an over-fitting problem because of high model complexity under insufficient violence data.</p><p>In summary, deep learning based approaches usually outperform traditional feature extraction based models. Besides, most state-of-the-art results utilize multi-channel inputs (e.g., raw RGB images, optical flows, acceleration maps). At the same time, complex models are not very robust against Over-Fitting. In this paper, we only adopt RGB images and optical flows to build the neural network, which can process the spatial and temporal information. Furthermore, our proposed Flow-Gated architecture can reduce temporal channels of input videos by self-learning, instead of traditional pooling strategies. More details of the architecture will be described in Section III-B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. RWF-2000 DATABASE AND PROPOSED METHOD</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Data Collection</head><p>To make violence detection more practical in realistic applications, we collect a new Real-World Fighting (RWF) dataset from the YouTube platform, which consists of 2,000 video clips captured by surveillance cameras in real-world scenes. <ref type="figure" target="#fig_1">Figure 2</ref> illustrates the pipeline of data collection. Firstly we search on the YouTube website by specifying a set of keywords related to violence (e.g., real fights, violence under surveillance, violent events), and obtain a list of URLs. We then employ a program to automatically download videos from the obtained links and check each video to remove irrelevant ones. To exhaustively extend the diversity of our dataset, we repeat the above procedures by changing keywords to various languages, including English, Chinese, German, French, Japanese, Russian, etc. Our selection criteria do not limit the kind of violence, including any form of subjectively identified violent activities (e.g., fighting, robbery, explosion, shooting, blood, assault). After these, we obtain many raw videos and cut each video into a 5-second clip with 30 FPS. In the end, we elaborately delete the noisy clips which contain unrealistic and non-monitoring scenes and annotate each clip as Violent or Non-Violent.</p><p>Furthermore, as a total of 2,000 video clips are extracted from around 1,000 raw surveillance videos with extended footage, we take a set of means to avoid data leakage in splitting 'train/test' partitions. First, we employ a dictionary to record the ID of source surveillance video for each extracted clip. Second, a python script is used to allocate clips to the training set and test set randomly. Meanwhile, it guarantees that clips from the same source will not be assigned to the same collection by checking the dictionary records. Besides, we calculate the color histogram of each clip as a feature vector to compute the mutual cosine similarity between clips in the same collection. Based on the similarity ranking from high to low, we manually check the top 30% of the data to ensure the reliability of the dataset partition.</p><p>The proposed dataset has 2,000 video clips, split into two parts: the training set (80%) and the test set (20%). Half of the videos include violent behaviors, while others belong to nonviolent activities. <ref type="table" target="#tab_0">Table I</ref> thoroughly compares our RWF-2000 dataset with others. Our highlight is that surveillance cameras capture all videos in this dataset, and multimedia technologies modify none of them. The videos are close to real violent events. Besides, the number of videos also exceeds previous datasets. <ref type="figure" target="#fig_2">Figure 3</ref> demonstrates the distribution of video quality in the RWF-2000 dataset. As common camera settings, videos usually follow a series of resolution standards (e.g., 720P, 1080P, 2K, 4K). Many data points in the distribution map will overlap. Therefore, we add a few random jitters to the original resolution data to obtain a better visualization. It is shown that several significant clusters are around the standard formats of 240P, 320P, 720P, and 1080P, respectively. Furthermore, the proportion of violent videos is almost uniform at different scales.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Flow Gated Network</head><p>Most previous methods explore to extract appearance features from individual frames and then fuse them to model temporal information. Ng et al. <ref type="bibr" target="#b32">[32]</ref> summarize various architectures for temporal feature pooling, and most of them are human-designed and tested one by one. As motion information may be useless due to the coarse pooling mechanism, we aim to design a temporal pooling mechanism achieved by network self-learning. <ref type="figure" target="#fig_3">Figure 4</ref> shows the structure of our proposed model with four parts: the RGB channel, the Optical Flow channel, the Merging Block, and the Fully Connected Layer. RGB channel and Optical Flow channel consist of cascaded 3D CNNs, and they have consistent structures so that their output could be fused. Merging Block is also composed of basic 3D CNNs, which process information after self-learned temporal pooling. Finally, the fully-connected layers generate output. Furthermore, we adopt the concept of depth-wise separable convolutions from MobileNet <ref type="bibr" target="#b33">[33]</ref> and Pseudo-3D Residual Networks <ref type="bibr" target="#b34">[34]</ref> to modify the 3D convolutional layers in our model, which can significantly reduce the model parameters without performance loss.</p><p>The highlight of this model is to utilize a branch of the optical flow channel to help build a pooling mechanism. Relu activation is adopted at the end of the RGB channel, while the sigmoid function is placed at the end of the Optical Flow channel. Then, outputs from RGB and Optical Flow channels are multiplied together and processed by a temporal maxpooling. Since the output of the sigmoid function is between 0 and 1, it is a scaling factor to adjust the output of the RGB channel. Meanwhile, as max-pooling only can reserve local maximum, the outcome of the RGB channel multiplied by one will have a larger probability to be retained, and the value multiplied by zero is more natural to be dropped. This mechanism is a kind of self-learned pooling strategy, which utilizes a branch of optical flow as a gate to determine what information the model should preserve or drop. The detailed parameters of the model structure are described in <ref type="table" target="#tab_0">Table II</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Basic Settings</head><p>It is known that consecutive frames in a video are highly correlated, and the region of interest for recognizing human activity is usually a small area. We implement both cropping and sampling strategies to reduce the amount of input video data.</p><p>In the latency-intensive situation, there are methods to accelerate the computation of optical flow on different hardware devices (e.g., GPU <ref type="bibr" target="#b35">[35]</ref>, FPGA <ref type="bibr" target="#b36">[36]</ref>). While in this offline experimental case, we prefer a simple manner to complete this task by tools in OpenCV <ref type="bibr" target="#b37">[37]</ref>. Firstly, we employ Gunner Farneback's method <ref type="bibr" target="#b38">[38]</ref> to compute the dense optical flow between neighboring frames. The computed dense optical flow is a field of the 2-D displacement vector. Thus we calculate the norm of each vector to obtain a heat map for indicating the motion intensity. We use the sum of all the heat maps to be a final motion intensity map. The region of interest is extracted from the location with the most significant motion intensity (shown in <ref type="figure" target="#fig_4">Figure 5</ref>). Secondly, we sparsely sample frames from the video by a uniform interval for each input video and then generate a fixed-length video clip. By adopting both cropping and sampling, the amount of input data decreases significantly. In this project, the target length of video clips is 64, and the size of the cropped regions is 224 ? 224. After sampling and cropping processes, the input data has a shape of 64 ? 224 ? 224 ? 5. The last dimension has five channels containing three RGB channels and two optical flow channels (a horizontal component and a vertical component). We implement the SGD optimizer with momentum (0.9) and the learning rate decay (1e-6). Also, we introduce the brightness transformation and random rotation as tricks for data augmentation, which help mitigate the over-fitting problem by simulating various lighting conditions and camera angles in real-world scenes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Ablation Experiments</head><p>In this part, we present detailed ablation studies to evaluate our model performance on the proposed RWF-2000 dataset. <ref type="table" target="#tab_0">Table III</ref> shows the experimental results of four different cases.</p><p>The RGB Only represents removing the optical flow channel while only keeping the RGB channel as input. On the other hand, the OPT Only shows removing the RGB channel and only keeping the optical flow channel. Subsequently, the Fusion (P3D) proves that the fused version of our proposed method can achieve the best accuracy of 87.25%.</p><p>To compare the tradeoff of computation amount between depth-wise separable 3D convolutions and traditional 3D convolutions, we also test the fused version of our proposed method without adopting the concept from P3D-Net <ref type="bibr" target="#b34">[34]</ref>.</p><p>The Fusion (C3D) shows the case of utilizing traditional 3D convolutions, instead of depth-wise separable 3D convolutions. This model achieves an accuracy of 85.75%, with nearly double the amount of model parameters. This result shows that modifying the standard 3D convolutions to the depthwise separable 3D convolutions could greatly reduce the model parameters with increasing the classification performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Comparisons</head><p>Although we have listed a series of datasets for violence detection in Section II-A, it is still difficult to evaluate a method on all of them due to their heterogeneity in data format, annotation type, etc. Therefore, we only test our proposed method on the Hockey Fight, the Movies Fight, and the Crowd Violence datasets, all consistent with the RWF-2000 dataset in data format, annotation type, and close video footage. <ref type="table" target="#tab_0">Table IV</ref> shows comparisons between the Flow Gated Network and other methods. The authors report performances of the first eight methods (from ViF to 3D ConvNet) in their papers. The last six methods are implemented and trained by ourselves due to the lack of existing experiments on these datasets. Generally, deep learning based approaches significantly outperform methods which depend on hand-crafted features.</p><p>Table IV reveals two experimental results. First, the deep learning based methods are all over-fitted on the Movies Fight dataset. Hence, this dataset is no longer suitable to be used in the current situation. Second, The performance of most methods carrying optical flow information has a significant drop in the Crowd Violence dataset. The reason for this phenomenon is due to the computing error of optical flow. There are three premise assumptions <ref type="bibr" target="#b39">[39]</ref> of computing optical flow: the brightness between adjacent frames is constant; the time between the adjacent frames is continuous, or the motion change is small; pixels in the same sub-image have the same motion. While in terms of camera movements, stable lighting conditions, and low image quality, videos in the Crowd Violence dataset are far from the requirements.   <ref type="bibr" target="#b41">[41]</ref> 59.0% 88.60% -HOG+HIK <ref type="bibr" target="#b41">[41]</ref> 49.0% 91.70% -MoWLD+BoW <ref type="bibr" target="#b42">[42]</ref> -91.90% 82.56% MoSIFT+HIK <ref type="bibr" target="#b41">[41]</ref> 89.5% 90.90% -</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Deep-Learning Based</head><p>FightNet <ref type="bibr" target="#b26">[26]</ref> 100% 97.00% -3D ConvNet <ref type="bibr" target="#b43">[43]</ref> 99.97% 99.62% 94.30% ConvLSTM <ref type="bibr" target="#b29">[29]</ref> 100% 97.10% 94.57. C3D <ref type="bibr" target="#b11">[12]</ref> 100% 96.50% 84.44% I3D(RGB only) <ref type="bibr" target="#b44">[44]</ref> 100% 98.50% 86.67% I3D(Flow only) <ref type="bibr" target="#b44">[44]</ref> 100% 84.00% 88.89% I3D(Fusion) <ref type="bibr" target="#b44">[44]</ref> 100% 97.50% 88.89% Ours 100% 98.00% 88.87% </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Accuracy(%) Params (M) ConvLSTM <ref type="bibr" target="#b29">[29]</ref> 77.00 47.4 C3D <ref type="bibr" target="#b11">[12]</ref> 82.75 94.8 I3D (RGB only) <ref type="bibr" target="#b44">[44]</ref> 85.75 12.3 I3D (Flow only) <ref type="bibr" target="#b44">[44]</ref> 75.50 12.3 I3D (TwoStream) <ref type="bibr" target="#b44">[44]</ref> 81.50 24.6 Ours (best version) 87.25 0.27 the Crowd Violence dataset, which can visualize the reason why flow-based methods perform poorly in this situation. Compared to the generic human action recognition, the essence of video-based violence detection is a data-driven problem. Hence, we compare our method with others presented for violence detection and test some classical models for generic action recognition on different violence datasets. For a fair comparison, we train our model and others without any pre-training procedures. <ref type="table" target="#tab_4">Table V</ref> shows benchmarks of the RWF-2000 dataset. Our proposed method achieves an accurate result for violence recognition. Meanwhile, it has fewer trainable model parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>This paper presents both a novel dataset and a method for violence detection in surveillance videos. The proposed RWF-2000 dataset is the currently largest surveillance video dataset used for violence detection in realistic scenes. Moreover, a unique pooling mechanism is employed by optical flow, which could implement temporal feature pooling instead of humandesigned strategies. In the future, we will explore methods without utilizing optical flow explicitly. Also, we will proceed to expand the size of the RWF-2000 dataset. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Gallery of the RWF-2000 database.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Pipeline of data collection.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Resolution Distribution of the RWF-2000 Database.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>The structure of the Flow Gated Network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Cropping strategy using dense optical flow.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>Figure IV-C shows two bad cases of computing the optical flow in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>(a) Example 1 (b) Example 2 Figure 6 .</head><label>126</label><figDesc>Bad cases of computing optical flow in the Crowd Violence dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table I COMPARISONS</head><label>I</label><figDesc>BETWEEN THE RWF-2000 AND THE PREVIOUS DATASETS. THE 'NATURAL' REPRESENTS THAT VIDEOS ARE FROM REALISTIC SCENES, BUT RECORDED BY HYBRID TYPES OF DEVICES (E.G., MOBILE CAMERAS, CAR-MOUNTED CAMERAS).</figDesc><table><row><cell>Authors</cell><cell>Dataset</cell><cell>Data Scale</cell><cell>Length/Clip (sec)</cell><cell>Resolution</cell><cell>Annotation</cell><cell>Scenario</cell></row><row><cell>Blunsden et al. [15]</cell><cell>BEHAVE</cell><cell>4 Videos (171 Clips)</cell><cell>0.24-61.92</cell><cell>640?480</cell><cell>Frame-Level</cell><cell>Acted Fights</cell></row><row><cell>Rota et al. [16]</cell><cell>RE-DID</cell><cell>30 Videos</cell><cell>20-240</cell><cell>1280?720</cell><cell>Frame-Level</cell><cell>Natural</cell></row><row><cell>Demarty et al. [17]</cell><cell>VSD</cell><cell>18 Movies (1,317 Clips)</cell><cell>55.3-829.4</cell><cell>Variable</cell><cell>Frame-Level</cell><cell>Movie</cell></row><row><cell>Perez et al. [18]</cell><cell>CCTV-Fights</cell><cell>1,000 clips</cell><cell>5-720</cell><cell>Variable</cell><cell>Frame-Level</cell><cell>Natural</cell></row><row><cell>Nievas et al. [4]</cell><cell>Hockey Fight</cell><cell>1,000 Clips</cell><cell>1.6-1.96</cell><cell>360?288</cell><cell>Video-Level</cell><cell>Hockey Games</cell></row><row><cell>Nievas et al. [5]</cell><cell>Movies Fight</cell><cell>200 Clips</cell><cell>1.6-2</cell><cell>720?480</cell><cell>Video-Level</cell><cell>Movie</cell></row><row><cell>Hassner et al. [6]</cell><cell>Crowd Violence</cell><cell>246 Clips</cell><cell>1.04-6.52</cell><cell>Variable</cell><cell>Video-Level</cell><cell>Natural</cell></row><row><cell>Yun et al. [19]</cell><cell>SBU Kinect Interaction</cell><cell>264 Clips</cell><cell>0.67-3</cell><cell>640?480</cell><cell>Video-Level</cell><cell>Acted Fights</cell></row><row><cell>Sultani et al. [20]</cell><cell>UCF-Crime</cell><cell>1,900 Clips</cell><cell>60-600</cell><cell>Variable</cell><cell>Video-Level</cell><cell>Surveillance</cell></row><row><cell>Ours</cell><cell>RWF-2000</cell><cell>2,000 Clips</cell><cell>5</cell><cell>Variable</cell><cell>Video-Level</cell><cell>Surveillance</cell></row><row><cell cols="3">model with a self-learned pooling mechanism, which could</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">adopt both appearance features and temporal features well.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table II PARAMETERS</head><label>II</label><figDesc></figDesc><table><row><cell cols="4">OF THE MODEL ARCHITECTURE (THE 'T' REPRESENTS THE</cell></row><row><cell></cell><cell cols="2">NUMBER OF REPEATS)</cell><cell></cell></row><row><cell>Block Name</cell><cell>Type</cell><cell>Filter Shape</cell><cell>T</cell></row><row><cell></cell><cell>Conv3d</cell><cell>1?3?3@16</cell><cell></cell></row><row><cell></cell><cell>Conv3d</cell><cell>3?1?1@16</cell><cell>2</cell></row><row><cell>RGB/Flow</cell><cell>MaxPool3d</cell><cell>1?2?2</cell><cell></cell></row><row><cell>Channels</cell><cell>Conv3d</cell><cell>1?3?3@32</cell><cell></cell></row><row><cell></cell><cell>Conv3d</cell><cell>3?1?1@32</cell><cell>2</cell></row><row><cell></cell><cell>MaxPool3d</cell><cell>1?2?2</cell><cell></cell></row><row><cell>Fusion and</cell><cell>Multiply</cell><cell>None</cell><cell>1</cell></row><row><cell>Pooling</cell><cell>MaxPool3d</cell><cell>8?1?1</cell><cell>1</cell></row><row><cell></cell><cell>Conv3d</cell><cell>1?3?3@64</cell><cell></cell></row><row><cell></cell><cell>Conv3d</cell><cell>3?1?1@64</cell><cell>2</cell></row><row><cell>Merging</cell><cell>MaxPool3d</cell><cell>2?2?2</cell><cell></cell></row><row><cell>Block</cell><cell>Conv3d</cell><cell>1?3?3@128</cell><cell></cell></row><row><cell></cell><cell>Conv3d</cell><cell>3?1?1@128</cell><cell>1</cell></row><row><cell></cell><cell>MaxPool3d</cell><cell>2?2?2</cell><cell></cell></row><row><cell>Fully-connected</cell><cell>FC layer</cell><cell>128</cell><cell>2</cell></row><row><cell>Layers</cell><cell>Softmax</cell><cell>2</cell><cell>1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table III EVALUATION</head><label>III</label><figDesc>OF THE PROPOSED FLOW GATED NETWORK ON THE RWF-2000 DATASET</figDesc><table><row><cell>Method</cell><cell cols="3">Train Accuracy(%) Test Accuracy(%) Params</cell></row><row><cell>RGB Only</cell><cell>89.50</cell><cell>84.50</cell><cell>248,402</cell></row><row><cell>OPT Only</cell><cell>82.31</cell><cell>75.50</cell><cell>248,258</cell></row><row><cell>Fusion (P3D)</cell><cell>88.44</cell><cell>87.25</cell><cell>272,690</cell></row><row><cell>Fusion (C3D)</cell><cell>96.50</cell><cell>85.75</cell><cell>507,154</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table IV COMPARISONS</head><label>IV</label><figDesc>BETWEEN THE PROPOSED METHOD AND OTHERS ON THE PREVIOUS DATASETS</figDesc><table><row><cell>Type</cell><cell>Method</cell><cell cols="2">Movies Hockey</cell><cell>Crowd</cell></row><row><cell></cell><cell>ViF [6]</cell><cell>-</cell><cell>82.90%</cell><cell>81.30%</cell></row><row><cell></cell><cell>LHOG+LOF [40]</cell><cell>-</cell><cell>95.10%</cell><cell>94.31%</cell></row><row><cell>Hand-Crafted</cell><cell>HOF+HIK</cell><cell></cell><cell></cell></row><row><cell>Features</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table V COMPARISONS</head><label>V</label><figDesc>BETWEEN THE PROPOSED METHOD AND OTHERS ON THE RWF-2000 DATASET</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/mchengny/RWF2000-Video-Database-for-Violence-D etection</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Dove: Detection of movie violence using motion intensity analysis on skin and blood</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Clarin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dionisio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Echavez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Naval</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PCSC</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="150" to="156" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Violence detection in movies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-W</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-Y.</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-W</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 Eighth International Conference Computer Graphics, Imaging and Visualization</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="119" to="124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Semantic context detection based on hierarchical audio models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-H</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-T</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-L</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th ACM SIGMM international workshop on Multimedia information retrieval</title>
		<meeting>the 5th ACM SIGMM international workshop on Multimedia information retrieval</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="109" to="115" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Hockey fight detection dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">B</forename><surname>Nievas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">D</forename><surname>Suarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">B</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Analysis of Images and Patterns</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="332" to="339" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Movies fight detection dataset</title>
	</analytic>
	<monogr>
		<title level="m">Computer Analysis of Images and Patterns</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="332" to="339" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Violent flows: Real-time detection of violent crowd behavior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Itcher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Kliper-Gross</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Violence detection in video using spatio-temporal features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">D</forename><surname>Souza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">C</forename><surname>Chavez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">A</forename><surname>Valle</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">D A</forename><surname>Ara?jo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2010 23rd SIBGRAPI Conference on Graphics, Patterns and Images</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="224" to="230" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Detection of violent crowd behavior based on statistical characteristics of the optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-F</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-L</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2014 11th International Conference on Fuzzy Systems and Knowledge Discovery (FSKD)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="565" to="569" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Action recognition with improved trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3551" to="3558" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="568" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Long-term recurrent convolutional networks for visual recognition and description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2625" to="2634" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3d convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4489" to="4497" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Temporal segment networks: Towards good practices for deep action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="20" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Eco: Efficient convolutional network for online video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zolfaghari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="695" to="712" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The behave video dataset: ground truthed video for multi-person behavior classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Blunsden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fisher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of the BMVA</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1-12</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Real-life violent social interaction detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Rota</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Conci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on Image Processing (ICIP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3456" to="3460" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Vsd, a public dataset for the detection of violent scenes in movies: design, annotation, analysis and evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-H</forename><surname>Demarty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Penet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Soleymani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gravier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Multimedia Tools and Applications</title>
		<imprint>
			<biblScope unit="volume">74</biblScope>
			<biblScope unit="issue">17</biblScope>
			<biblScope unit="page" from="7379" to="7404" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Detection of real-world fights in surveillance videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Kot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rocha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2662" to="2666" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Two-person interaction detection using body-pose features and multiple instance learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Honorio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chattopadhyay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Samaras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="28" to="35" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Real-world anomaly detection in surveillance videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Sultani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6479" to="6488" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Recognition of aggressive human behavior using binary local motion descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wactlar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bharucha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hauptmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2008 30th Annual International Conference of the IEEE Engineering in Medicine and Biology Society</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="5238" to="5241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Violent video detection based on mosift feature and sparse coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2014 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint/>
	</monogr>
	<note>ICASSP</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<imprint>
			<biblScope unit="page" from="3538" to="3542" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Violence detection using oriented violent flows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and vision computing</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="37" to="41" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Human violence recognition and detection in surveillance videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bilinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bremond</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 13th IEEE International Conference on Advanced Video and Signal Based Surveillance (AVSS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="30" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Abnormal event detection in crowded scenes based on deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Multimedia Tools and Applications</title>
		<imprint>
			<biblScope unit="volume">75</biblScope>
			<biblScope unit="issue">22</biblScope>
			<biblScope unit="page" from="14" to="617" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Violent interaction detection in video based on deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Journal of physics: conference series</title>
		<imprint>
			<publisher>IOP Publishing</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">844</biblScope>
			<biblScope unit="page">12044</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Learning deep representations of appearance and motion for anomalous event detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ricci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1510.01553</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Multi-stream deep networks for person to person violence detection in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Chinese Conference on Pattern Recognition</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="517" to="531" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning to detect violent videos using convolutional long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sudhakaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Lanz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advanced Video and Signal Based Surveillance (AVSS)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
	<note>14th IEEE</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Convolutional lstm network: A machine learning approach for precipitation nowcasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xingjian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-Y</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-K</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-C</forename><surname>Woo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="802" to="810" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Ucf101: A dataset of 101 human actions classes from videos in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.0402</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Beyond short snippets: Deep networks for video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yue-Hei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hausknecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Toderici</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4694" to="4702" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Mobilenets: Efficient convolutional neural networks for mobile vision applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04861</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Learning spatio-temporal representation with pseudo-3d residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5533" to="5541" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Realtime phase-based optical flow on the gpu</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Pauwels</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Van Hulle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2008 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A comparison of fpga and gpu for real-time phase-based optical flow, stereo, and local image features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Pauwels</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tomasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Alonso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Van Hulle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Computers</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="999" to="1012" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Open source computer vision library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Itseez</surname></persName>
		</author>
		<ptr target="https://github.com/itseez/opencv" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Two-frame motion estimation based on polynomial expansion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Farneb?ck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SCIA</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2003" />
			<biblScope unit="page" from="363" to="370" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">The computation of optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Beauchemin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Barron</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM computing surveys (CSUR)</title>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="433" to="466" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Violence detection in surveillance video using low-level features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS one</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">10</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Violence detection in video using computer vision techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">B</forename><surname>Nievas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">D</forename><surname>Suarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">B</forename><surname>Garc?a</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on Computer analysis of images and patterns</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="332" to="339" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Mowld: a robust motion image descriptor for violence detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Multimedia Tools and Applications</title>
		<imprint>
			<biblScope unit="volume">76</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1419" to="1438" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">A novel violent video detection scheme based on modified 3d convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="39" to="172" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6299" to="6308" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
