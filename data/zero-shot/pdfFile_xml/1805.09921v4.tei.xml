<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">META-LEARNING PROBABILISTIC INFERENCE FOR PREDICTION</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Gordon</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Bronskill</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Bauer</surname></persName>
							<email>bauer@tue.mpg.de</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Nowozin</surname></persName>
							<email>nowozin@google.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">I</forename><surname>Google</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Berlin</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">E</forename><surname>Turner</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">University of Cambridge</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Max Planck Institute for Intelligent Systems</orgName>
								<orgName type="institution">University of Cambridge</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">University of Cambridge Microsoft Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">META-LEARNING PROBABILISTIC INFERENCE FOR PREDICTION</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>Published as a conference paper at ICLR 2019</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T04:31+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper introduces a new framework for data efficient and versatile learning. Specifically: 1) We develop ML-PIP, a general framework for Meta-Learning approximate Probabilistic Inference for Prediction. ML-PIP extends existing probabilistic interpretations of meta-learning to cover a broad class of methods. 2) We introduce VERSA, an instance of the framework employing a flexible and versatile amortization network that takes few-shot learning datasets as inputs, with arbitrary numbers of shots, and outputs a distribution over task-specific parameters in a single forward pass. VERSA substitutes optimization at test time with forward passes through inference networks, amortizing the cost of inference and relieving the need for second derivatives during training. 3) We evaluate VERSA on benchmark datasets where the method sets new state-of-the-art results, handles arbitrary numbers of shots, and for classification, arbitrary numbers of classes at train and test time. The power of the approach is then demonstrated through a challenging few-shot ShapeNet view reconstruction task. we share information between tasks about how to learn and perform inference using meta-learning <ref type="bibr" target="#b35">(Naik and Mammone, 1992;</ref><ref type="bibr" target="#b49">Thrun and Pratt, 2012;</ref><ref type="bibr" target="#b46">Schmidhuber, 1987)</ref>. Since uncertainty is rife in small datasets, we provide a procedure for metalearning probabilistic inference. Third, we enable fast learning that can flexibly handle a wide range of tasks and learning settings via amortization .</p><p>Building on the framework, we propose a new method -VERSA -which substitutes optimization procedures at test time with forward passes through inference networks. This amortizes the cost of inference, resulting in faster test-time performance, and relieves the need for second derivatives during training. VERSA employs a flexible amortization network that takes few-shot learning datasets, and outputs a distribution over task-specific parameters in a single forward pass. The network can handle arbitrary numbers of shots, and for classification, arbitrary numbers of classes at train and test time (see Section 3). In Section 5, we evaluate VERSA on (i) standard benchmarks where the method sets new state-of-the-art results, (ii) settings where test conditions (shot and way) differ from training, and (iii) a challenging one-shot view reconstruction task.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Many applications require predictions to be made on myriad small, but related datasets. In such cases, it is natural to desire learners that can rapidly adapt to new datasets at test time. These applications have given rise to vast interest in few-shot learning <ref type="bibr" target="#b9">(Fei-Fei et al., 2006;</ref><ref type="bibr" target="#b30">Lake et al., 2011)</ref>, which emphasizes data efficiency via information sharing across related tasks. Despite recent advances, notably in meta-learning based approaches <ref type="bibr" target="#b42">(Ravi and Larochelle, 2017;</ref><ref type="bibr" target="#b53">Vinyals et al., 2016;</ref><ref type="bibr" target="#b8">Edwards and Storkey, 2017;</ref><ref type="bibr" target="#b10">Finn et al., 2017;</ref>, there remains a lack of general purpose methods for flexible, data-efficient learning.</p><p>Due to the ubiquity of recent work, a unifying view is needed to understand and improve these methods. Existing frameworks <ref type="bibr" target="#b15">(Grant et al., 2018;</ref> are limited to specific families of approaches. In this paper we develop a framework for meta-learning approximate probabilistic inference for prediction (ML-PIP), providing this view in terms of amortizing posterior predictive distributions. In Section 4, we show that ML-PIP re-frames and extends existing point-estimate probabilistic interpretations of meta-learning <ref type="bibr" target="#b15">(Grant et al., 2018;</ref> to cover a broader class of methods, including gradient based meta-learning <ref type="bibr" target="#b10">(Finn et al., 2017;</ref><ref type="bibr" target="#b42">Ravi and Larochelle, 2017)</ref>, metric based meta-learning <ref type="bibr" target="#b47">(Snell et al., 2017)</ref>, amortized MAP inference <ref type="bibr" target="#b41">(Qiao et al., 2018)</ref> and conditional probability modelling <ref type="bibr" target="#b12">(Garnelo et al., 2018a;</ref>.</p><p>The framework incorporates three key elements. First, we leverage shared statistical structure between tasks via hierarchical probabilistic models developed for multi-task and transfer learning <ref type="bibr">(Heskes,</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">META-LEARNING PROBABILISTIC INFERENCE FOR PREDICTION</head><p>We now present the framework that consists of (i) a multi-task probabilistic model, and (ii) a method for meta-learning probabilistic inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">PROBABILISTIC MODEL</head><p>Two principles guide the choice of model. First, the use of discriminative models to maximize predictive performance on supervised learning tasks <ref type="bibr" target="#b37">(Ng and Jordan, 2002)</ref>. Second, the need to leverage shared statistical structure between tasks (i.e. multi-task learning). These criteria are met by the standard multi-task directed graphical model shown in <ref type="figure" target="#fig_1">Fig. 1</ref> that employs shared parameters ?, which are common to all tasks, and task specific parameters {? (t) } T t=1 . Inputs are denoted x and outputs y. Training data D (t) = {(x  Let X (t) and Y (t) denote all the inputs and outputs (both test and train) for task t. The joint probability of the outputs and task specific parameters for T tasks, given the inputs and global parameters is:</p><formula xml:id="formula_0">p {Y (t) , ? (t) } T t=1 |{X (t) } T t=1 , ? = T t=1 p ? (t) |? Nt n=1 p y (t) n |x (t) n , ? (t) , ? Mt m=1 p ? (t) m |x (t) m , ? (t) , ? .</formula><p>In the next section, the goal is to meta-learn fast and accurate approximations to the posterior predictive distribution p(? (t) |x (t) , ?) = p(? (t) |x (t) , ? (t) , ?)p(? (t) |D (t) , ?)d? (t) for unseen tasks t.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">PROBABILISTIC INFERENCE</head><p>This section provides a framework for meta-learning approximate inference that is a simple reframing and extension of existing approaches <ref type="bibr" target="#b10">(Finn et al., 2017;</ref><ref type="bibr" target="#b15">Grant et al., 2018)</ref>. We will employ point estimates for the shared parameters ? since data across all tasks will pin down their value. Distributional estimates will be used for the task-specific parameters since only a few shots constrain them.</p><p>Once the shared parameters are learned, the probabilistic solution to few-shot learning in the model above comprises two steps. First, form the posterior distribution over the task-specific parameters p(? (t) |x (t) , D (t) , ?). Second, compute the posterior predictive p(? (t) |x (t) , ?). These steps will require approximation and the emphasis here is on performing this quickly at test time. We will describe the form of the approximation, the optimization problem used to learn it, and how to implement this efficiently below. In what follows we initially suppress dependencies on the inputsx and shared parameters ? to reduce notational clutter, but will reintroduce these at the end of the section.</p><p>Specification of the approximate posterior predictive distribution. Our framework approximates the posterior predictive distribution by an amortized distribution q ? (?|D). That is, we learn a feed-forward inference network with parameters ? that takes any training dataset D (t) and test inputx as inputs and returns the predictive distribution over the test output? <ref type="bibr">(t)</ref> . We construct this by amortizing the approximate posterior q ? (?|D) and then form the approximate posterior predictive distribution using:</p><formula xml:id="formula_1">q ? (?|D) = p(?|?)q ? (?|D)d?.<label>(1)</label></formula><p>This step may require additional approximation e.g. Monte Carlo sampling. The amortization will enable fast predictions at test time. The form of these distributions is identical to those used in amortized variational inference <ref type="bibr" target="#b8">(Edwards and Storkey, 2017;</ref>. In this work, we use a factorized Gaussian distribution for q ? (?|D (t) ) with means and variances set by the amortization network. However, the training method described next is different.</p><p>Meta-learning the approximate posterior predictive distribution. The quality of the approximate posterior predictive for a single task will be measured by the KL-divergence between the true and approximate posterior predictive distribution KL [p(?|D) q ? (?|D)]. The goal of learning will be to minimize the expected value of this KL averaged over tasks,</p><formula xml:id="formula_2">? * = arg min ? E p(D) [KL [p(?|D) q ? (?|D)]] = arg max ? E p(?,D) log p(?|?)q ? (?|D)d? . (2)</formula><p>Training will therefore return parameters ? that best approximate the posterior predictive distribution in an average KL sense. So, if the approximate posterior q ? (?|D) is rich enough, global optimization will recover the true posterior p(?|D) (assuming p(?|D) obeys identifiability conditions <ref type="bibr" target="#b4">(Casella and Berger, 2002)</ref>). 1 Thus, the amortized procedure meta-learns approximate inference that supports accurate prediction. Appendix A provides a generalized derivation of the framework, grounded in Bayesian decision theory <ref type="bibr" target="#b20">(Jaynes, 2003)</ref>.</p><p>The right hand side of Eq.</p><p>(2) indicates how training could proceed: (i) select a task t at random, (ii) sample some training data D (t) , (iii) form the posterior predictive q ? (?|D (t) ) and, (iv) compute the log-density log q ? (? (t) |D (t) ) at test data? (t) not included in D <ref type="bibr">(t)</ref> . Repeating this process many times and averaging the results would provide an unbiased estimate of the objective which can then be optimized. This perspective also makes it clear that the procedure is scoring the approximate inference procedure by simulating approximate Bayesian held-out log-likelihood evaluation. Importantly, while an inference network is used to approximate posterior distributions, the training procedure differs significantly from standard variational inference. In particular, rather than minimizing KL(q ? (?|D) p(?|D)), our objective function directly focuses on the posterior predictive distribution and minimizes KL(p(?|D) q ? (?|D)).</p><p>End-to-end stochastic training. Armed by the insights above we now layout the full training procedure. We reintroduce inputs and shared parameters ? and the objective becomes:</p><formula xml:id="formula_3">L (?) = ? E p(D,?,x) [log q ? (?|x, ?)] = ? E p(D,?,x) log p(?|x, ?, ?)q ? (?|D, ?)d? .<label>(3)</label></formula><p>We optimize the objective over the shared parameters ? as this will maximize predictive performance (i.e., Bayesian held out likelihood). An end-to-end stochastic training objective for ? and ? is:</p><formula xml:id="formula_4">L (?, ?) = 1 M T M,T log 1 L L l=1 p ? (t) m |x (t) m , ? (t) l , ? , with ? (t) l ? q ? (?|D (t) , ?) (4) | | w (1) t ? ? ? w (C) t | | ? ? ? ? ? ? ? ? Linear Classifier h ? (x) x ? p(?|x, ?, ? t )</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Feature extraction</head><p>Softmax output</p><formula xml:id="formula_5">h ? x (1) 1 ? ? ? h ? x (1) k1 k 1 train examples from class 1 h ? x (C) 1 ? ? ? h ? x (C) kC k C train examples from class C ? ? Amortization Network Amortization Network q ? h ? x (1) 1 . . . h ? x (1) k ?pre ?pre h (1) 1 h (1) k h (1) ? post q(w (1) t )</formula><p>individual feature extraction instance pooling regression onto weights <ref type="figure" target="#fig_9">Figure 2</ref>: Computational flow of VERSA for few-shot classification with the context-independent approximation. Left: A test pointx is mapped to its softmax output through a feature extractor neural network and a linear classifier (fully connected layer). The global parameters ? of the feature extractor are shared between tasks whereas the weight vectors w </p><formula xml:id="formula_6">data {(x (t) m ,? (t) m )} Mt m=1</formula><p>). This type of training therefore uses episodic train / test splits at meta-train time. We have also approximated the integral over ? using L Monte Carlo samples. The local reparametrization  trick enables optimization. Interestingly, the learning objective does not require an explicit specification of the prior distribution over parameters, p(? (t) |?), learning it implicitly through q ? (?|D, ?) instead.</p><p>In summary, we have developed an approach for Meta-Learning Probabilistic Inference for Prediction (ML-PIP). A simple investigation of the inference method with synthetic data is provided in Section 5.1. In Section 4 we will show that this formulation unifies a number of existing approaches, but first we discuss a particular instance of the ML-PIP framework that supports versatile learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">VERSATILE AMORTIZED INFERENCE</head><p>A versatile system is one that makes inferences both rapidly and flexibly. By rapidly we mean that test-time inference involves only simple computation such as a feed-forward pass through a neural network. By flexibly we mean that the system supports a variety of tasks -including variable numbers of shots or numbers of classes in classification problems -without retraining. Rapid inference comes automatically with the use of a deep neural network to amortize the approximate posterior distribution q. However, it typically comes at the cost of flexibility: amortized inference is usually limited to a single specific task. Below, we discuss design choices that enable us to retain flexibility.</p><p>Inference with sets as inputs. The amortization network takes data sets of variable size as inputs whose ordering we should be invariant to. We use permutation-invariant instance-pooling operations to process these sets similarly to <ref type="bibr" target="#b40">Qi et al. (2017)</ref> and as formalized in <ref type="bibr" target="#b57">Zaheer et al. (2017)</ref>. The instance-pooling operation ensures that the network can process any number of training observations. VERSA for Few-Shot Image Classification. For few-shot image classification, our parameterization of the probabilistic model is inspired by early work from <ref type="bibr" target="#b17">Heskes (2000)</ref>; <ref type="bibr" target="#b0">Bakker and Heskes (2003)</ref> and recent extensions to deep learning <ref type="bibr" target="#b1">(Bauer et al., 2017;</ref><ref type="bibr" target="#b41">Qiao et al., 2018)</ref>. A feature extractor neural network h ? (x) ? R d ? , shared across all tasks, feeds into a set of task-specific linear classifiers with softmax outputs and weights and biases ? (t) = {W (t) , b (t) } (see <ref type="figure" target="#fig_9">Fig. 2</ref>).</p><p>A naive amortization requires the approximate posterior q ? (?|D, ?) to model the distribution over full weight matrices in R d ? ?C (and biases). This requires the specification of the number of few-shot classes C ahead of time and limits inference to this chosen number. Moreover, it is difficult to metalearn systems that directly output large matrices as the output dimensionality is high. We therefore propose specifying q ? (?|D, ?) in a context independent manner such that each weight vector ? c depends only on examples from class c, by amortizing individual weight vectors associated with a</p><formula xml:id="formula_7">x ? (t) Generator ? p(?|x, ?, ? (t) ) (y (t) 1 , x (t) 1 ) (y (t) k , x (t) k ) ? ? Amortization Network ? y (t) 1 . . . y (t) k ?pre ?pre h (t) 1 x (t) 1 h (t) k x (t) k ? mid ? midh (t) 1 h (t) k h (t) ?post ? (t)</formula><p>individual feature extraction instance pooling regression onto stochastic inputs </p><formula xml:id="formula_8">q ? (?|D, ?) = C c=1 q ? ? c |{h ? (x c n )} kc n=1 , ? .<label>(5)</label></formula><p>Note that in our implementation, end-to-end training is employed, i.e., we backpropagate to ? through the inference network. Here k c is the number of observed examples in class c and ? c = {w c , b c } denotes the weight vector and bias of the linear classifier associated with that class. Thus, we construct the classification matrix ? (t) by performing C feed-forward passes through the inference network q ? (?|D, ?) (see <ref type="figure" target="#fig_9">Fig. 2</ref>).</p><p>The assumption of context independent inference is an approximation. In Appendix B, we provide theoretical and empirical justification for its validity. Our theoretical arguments use insights from Density Ratio Estimation <ref type="bibr" target="#b34">(Mohamed, 2018;</ref><ref type="bibr" target="#b48">Sugiyama et al., 2012)</ref>, and we empirically demonstrate that full approximate posterior distributions are close to their context independent counterparts. Critically, the context independent approximation addresses all the limitations of a naive amortization mentioned above: (i) the inference network needs to amortize far fewer parameters whose number does not scale with number of classes C (a single weight vector instead of the entire matrix); (ii) the amortization network can be meta-trained with different numbers of classes per task, and (iii) the number of classes C can vary at test-time.</p><p>VERSA for Few-Shot Image Reconstruction (Regression). We consider a challenging few-shot learning task with a complex (high dimensional and continuous) output space. We define view reconstruction as the ability to infer how an object looks from any desired angle based on a small set of observed views. We frame this as a multi-output regression task from a set of training images with known orientations to output images with specified orientations.</p><p>Our generative model is similar to the generator of a GAN or the decoder of a VAE: A latent vector ? (t) ? R d ? , which acts as an object-instance level input to the generator, is concatenated with an angle representation and mapped through the generator to produce an image at the specified orientation. In this setting, we treat all parameters ? of the generator network as global parameters (see Appendix E.1 for full details of the architecture), whereas the latent inputs ? (t) are the task-specific parameters. We use a Gaussian likelihood in pixel space for the outputs of the generator. To ensure that the output means are between zero and one, we use a sigmoid activation after the final layer. ? parameterizes an amortization network that first processes the image representations of an object, concatenates them with their associated view orientations, and processes them further before instance-pooling. From the pooled representations, q ? (?|D, ?) produces a distribution over vectors ? <ref type="bibr">(t)</ref> . This process is illustrated in <ref type="figure" target="#fig_3">Fig. 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">ML-PIP UNIFIES DISPARATE RELATED WORK</head><p>In this section, we continue in the spirit of <ref type="bibr" target="#b15">Grant et al. (2018)</ref>, and recast a broader class of metalearning approaches as approximate inference in hierarchical models. We show that ML-PIP unifies a number of important approaches to meta-learning, including both gradient and metric based variants, as well as amortized MAP inference and conditional modelling approaches <ref type="bibr" target="#b12">(Garnelo et al., 2018a)</ref>. We lay out these connections, most of which rely on point estimates for the task-specific parame-</p><formula xml:id="formula_9">ters corresponding to q(? (t) |D (t) , ?) = ? ? (t) ? ? * (D (t) , ?)</formula><p>. In addition, we compare previous approaches to VERSA.</p><p>Gradient-Based Meta-Learning. Let the task-specific parameters ? (t) be all the parameters in a neural network. Consider a point estimate formed by taking a step of gradient ascent of the training loss, initialized at ? 0 and with learning rate ?.</p><formula xml:id="formula_10">? * (D (t) , ?) = ? 0 + ? ? ?? Nt n=1 log p(y (t) n |x (t) n , ?, ?) ?0 .<label>(6)</label></formula><p>This is an example of semi-amortized inference , as the only shared inference parameters are the initialization and learning rate, and optimization is required for each task (albeit only for one step). Importantly, Eq. (6) recovers Model-agnostic meta-learning <ref type="bibr" target="#b10">(Finn et al., 2017)</ref>, providing a perspective as semi-amortized ML-PIP. This perspective is complementary to that of <ref type="bibr" target="#b15">Grant et al. (2018)</ref> who justify the one-step gradient parameter update employed by MAML through MAP inference and the form of the prior p(?|?). Note that the episodic meta-train / meta-test splits do not fall out of this perspective. Instead we view the update choice as one of amortization which is trained using the predictive KL and naturally recovers the test-train splits. More generally, multiple gradient steps could be fed into an RNN to compute ? * which recovers <ref type="bibr" target="#b42">Ravi and Larochelle (2017)</ref>. In comparison to these methods, besides being distributional over ?, VERSA relieves the need to back-propagate through gradient based updates during training and compute gradients at test time, as well as enables the treatment of both local and global parameters which simplifies inference.</p><p>Metric-Based Few-Shot Learning. Let the task-specific parameters be the top layer softmax weights and biases of a neural network</p><formula xml:id="formula_11">? (t) = {w (t) c , b (t) c } C c=1 .</formula><p>The shared parameters are the lower layer weights. Consider amortized point estimates for these parameters constructed by averaging the top-layer activations for each class,</p><formula xml:id="formula_12">? * (D (t) , ?) = {w * c , b * c } C c=1 = ? (t) c , ? ? (t) c 2 /2 C c=1 where ? (t) c = 1 k c kc n=1 h ? (x (c) n ) (7)</formula><p>These choices lead to the following predictive distribution:</p><formula xml:id="formula_13">p(? (t) = c|x (t) , ?) ? exp ?d(h ? (x (t) ), ? (t) c ) = exp h ? (x (t) ) T ? (t) c ? 1 2 ? (t) c 2 ,<label>(8)</label></formula><p>which recovers prototypical networks <ref type="bibr" target="#b47">(Snell et al., 2017)</ref> using a Euclidean distance function d with the final hidden layer being the embedding space. In comparison, VERSA is distributional and it uses a more flexible amortization function that goes beyond averaging of activations.</p><p>Amortized MAP inference. <ref type="bibr" target="#b41">Qiao et al. (2018)</ref> proposed a method for predicting weights of classes from activations of a pre-trained network to support i) online learning on a single task to which new few-shot classes are incrementally added, ii) transfer from a high-shot classification task to a separate low-shot classification task. This is an example usage of hyper-networks <ref type="bibr" target="#b16">(Ha et al., 2017)</ref> to amortize learning about weights, and can be recovered by the ML-PIP framework by pre-training ? and performing MAP inference for ?. VERSA goes beyond point estimates and although its amortization network is similar in spirit, it is more general, employing end-to-end training and supporting full multi-task learning by sharing information between many tasks.</p><p>Conditional models trained via maximum likelihood. In cases where a point estimate of the task-specific parameters are used the predictive becomes</p><formula xml:id="formula_14">q ? (?|D, ?) = p(?|?, ?)q ? (?|D, ?)d? = p(?|? * (D, ?), ?).<label>(9)</label></formula><p>In such cases the amortization network that computes ? * (D, ?) can be equivalently viewed as part of the model specification rather than the inference scheme. From this perspective, the ML-PIP training procedure for ? and ? is equivalent to training a conditional model p(?|? * ? (D, ?), ?) via maximum likelihood estimation, establishing a strong connection to neural processes <ref type="bibr" target="#b12">(Garnelo et al., 2018a;</ref>.</p><p>Comparison to Variational Inference (VI). Standard application of amortized VI <ref type="bibr" target="#b3">Blundell et al., 2015)</ref> for ? in the multi-task discriminative model optimizes the Monte Carlo approximated free-energy w.r.t. ? and ?:</p><formula xml:id="formula_15">L(?, ?) = 1 T T t=1 ? ? (x,y)?D (t) 1 L L l=1 log p(y (t) |x (t) , ? (t) l , ?) ? KL q ? (?|D (t) , ?) p(?|?) ? ? , (10) where ? (t) l ? q ? (?|D (t) , ?)</formula><p>. In addition to the conceptual difference from ML-PIP (discussed in Section 2.1), this differs from the ML-PIP objective by i) not employing meta train / test splits, and ii) including the KL for regularization instead. In Section 5, we show that VERSA significantly improves over standard VI in the few-shot classification case and compare to recent VI/meta-learning hybrids.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTS AND RESULTS</head><p>We evaluate VERSA on several few-shot learning tasks. We begin with toy experiments to investigate the properties of the amortized posterior inference achieved by VERSA. We then report few-shot classification results using the Omniglot and miniImageNet datasets in Section 5.2, and demonstrate VERSA's ability to retain high accuracy as the shot and way are varied at test time. In Section 5.3, we examine VERSA's performance on a one-shot view reconstruction task with ShapeNet objects. 2</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">POSTERIOR INFERENCE WITH TOY DATA</head><p>To investigate the approximate inference performed by our training procedure, we run the following experiment. We first generate data from a Gaussian distribution with a mean that varies across tasks:</p><formula xml:id="formula_16">p(?) = ?(? ? 0); p ? (t) |? = N ? (t) ; ?, ? 2 ? ; p y (t) n |? (t) = N y (t) n ; ? (t) , ? 2 y .</formula><p>(11) We generate T = 250 tasks in two separate experiments, having N ? {5, 10} train observations and M = 15 test observations. We introduce the inference network q ? (?|D (t) ) = N (?; ? (t) q , ? (t)2 q ), amortizing inference as:</p><formula xml:id="formula_17">? (t) q = w ? N n=1 y (t) n + b ? , ? (t)2 q = exp w ? N n=1 y (t) n + b ? .<label>(12)</label></formula><p>The learnable parameters ? = {w ? , b ? , w ? , b ? } are trained with the objective function in Eq. (4). The model is trained to convergence with Adam <ref type="bibr" target="#b23">(Kingma and Ba, 2015)</ref> using mini-batches of tasks from the generated dataset. Then, a separate set of tasks is generated from the same generative process, and the posterior q ? (?|D) is inferred with the learned amortization parameters. The true posterior over ? is Gaussian with a mean that depends on the task, and may be computed analytically. <ref type="figure" target="#fig_4">Fig. 4</ref> shows the approximate posterior distributions inferred for unseen test sets by the trained amortization networks. The evaluation shows that the inference procedure is able to recover accurate posterior distributions over ?, despite minimizing a predictive KL divergence in data space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">FEW-SHOT CLASSIFICATION</head><p>We evaluate VERSA on standard few-shot classification tasks in comparison to previous work. Specifically, we consider the Omniglot <ref type="bibr" target="#b30">(Lake et al., 2011)</ref> and miniImageNet <ref type="bibr" target="#b42">(Ravi and Larochelle, 2017)</ref> datasets which are C-way classification tasks with k c examples per class. VERSA follows the implementation in Sections 2 and 3, and the approximate inference scheme in Eq. (5). We follow the experimental protocol established by <ref type="bibr" target="#b53">Vinyals et al. (2016)</ref> for Omniglot and Ravi and Larochelle   <ref type="bibr" target="#b1">(Bauer et al., 2017;</ref><ref type="bibr" target="#b41">Qiao et al., 2018;</ref><ref type="bibr" target="#b44">Rusu et al., 2019;</ref><ref type="bibr" target="#b14">Gidaris and Komodakis, 2018;</ref><ref type="bibr" target="#b45">Satorras and Estrach, 2018;</ref> have been excluded so that the quality of the learning algorithm can be assessed separately from the power of the underlying discriminative model.</p><formula xml:id="formula_18">?(t) ?(t) ?(t) ?(t)</formula><p>For Omniglot, the training, validation, and test splits have not been specified for previous methods, affecting the comparison. VERSA achieves a new state-of-the-art results (67.37% -up 1.38% over the previous best) on 5-way -5-shot classification on the miniImageNet benchmark and (97.66% -up 0.02%) on the 20-way -1 shot Omniglot benchmark for systems using a convolution-based network architecture and an end-to-end training procedure. VERSA is within error bars of state-of-the-art on three other benchmarks including 5-way -1-shot miniImageNet, 5-way -5-shot Omniglot, and 5-way -1-shot Omniglot. Results on the Omniglot 20 way -5-shot benchmark are very competitive with, but lower than other approaches. While most of the methods evaluated in <ref type="table" target="#tab_0">Table 3</ref> adapt all of the learned parameters for new tasks, VERSA is able to achieve state-of-the-art performance despite adapting only the weights of the top-level classifier.</p><p>Comparison to standard and amortized VI. To investigate the performance of our inference procedure, we compare it in terms of log-likelihood <ref type="table" target="#tab_1">(Table 1</ref>) and accuracy <ref type="table" target="#tab_0">(Table 3)</ref> to training the same model using both amortized and non-amortized VI (i.e., Eq. <ref type="formula" target="#formula_1">(10)</ref>). Derivations and further experimental details are provided in Appendix C. VERSA improves substantially over amortized VI even though the same amortization network is used for both. This is due to VI's tendency to under-fit, especially for small numbers of data points <ref type="bibr" target="#b51">(Trippe and Turner, 2018;</ref><ref type="bibr" target="#b52">Turner and Sahani, 2011)</ref> which is compounded when using inference networks <ref type="bibr" target="#b6">(Cremer et al., 2018)</ref>. Using non-amortized VI improves performance substantially, but does not reach the level of VERSA and forming the posterior is significantly slower as it requires many forward / backward passes through the network. This is similar in spirit to MAML <ref type="bibr" target="#b10">(Finn et al., 2017)</ref>, though MAML dramatically reduces the number of required iterations by finding good global initializations e.g., five gradient steps for miniImageNet. This is in contrast to the single forward pass required by VERSA.</p><p>Versatility. VERSA allows us to vary the number of classes C and shots k c between training and testing (Eq. <ref type="formula" target="#formula_8">(5)</ref>). <ref type="figure">Fig. 5a</ref> shows that a model trained for a particular C-way retains very high accuracy as C is varied. For example, when VERSA is trained for the 20-Way, 5-Shot condition, at test-time it can handle C = 100 way conditions and retain an accuracy of approximately 94%. <ref type="figure">Fig. 5b</ref> shows similar robustness as the number of shots k c is varied. VERSA therefore demonstrates considerable flexibility and robustness to the test-time conditions, but at the same time it is efficient as it only requires forward passes through the network. The time taken to evaluate 1000 test tasks with a 5-way, 5-shot miniImageNet trained model using MAML (https://github.com/cbfinn/maml) is 302.9 seconds whereas VERSA took 53.5 seconds on a NVIDIA Tesla P100-PCIE-16GB GPU. This is more than 5? speed advantage in favor of VERSA while bettering MAML in accuracy by 4.26%.  <ref type="figure">Figure 5</ref>: Test accuracy on Omniglot when varying (a) way (fixing shot to be that used for training) and (b) shot. In <ref type="figure">Fig. 5b</ref>, all models are evaluated on 5-way classification. Colors indicate models trained with different way-shot episodic combinations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">SHAPENET VIEW RECONSTRUCTION</head><p>ShapeNetCore v2 <ref type="bibr" target="#b5">(Chang et al., 2015)</ref> is a database of 3D objects covering 55 common object categories with ?51,300 unique objects. For our experiments, we use 12 of the largest object categories. We concatenate all instances from all 12 of the object categories together to obtain a dataset of 37,108 objects. This dataset is then randomly shuffled and we use 70% of the objects for training, 10% for validation, and 20% for testing. For each object, we generate 36 views of size 32 ? 32 pixels spaced evenly every 10 degrees in azimuth around the object.</p><p>We evaluate VERSA by comparing it to a conditional variational autoencoder (C-VAE) with view angles as labels <ref type="bibr" target="#b36">Narayanaswamy et al., 2017)</ref> and identical architectures. We train VERSA in an episodic manner and the C-VAE in batch-mode on all 12 object classes at once. We train on a single view selected at random and use the remaining views to evaluate the objective function. For full experimentation details see Appendix E. <ref type="figure" target="#fig_7">Fig. 6</ref> shows views of unseen objects from the test set generated from a single shot with VERSA as well as a C-VAE and compares both to ground truth views. Both VERSA and the C-VAE capture the correct orientation of the object in the generated images. However, VERSA produces images that contain much more detail and are visually sharper than the C-VAE images. Although important information is missing due to occlusion in the single shot, VERSA is often able to accurately impute this information presumably due to learning the statistics of these objects. <ref type="table">Table 2</ref> provides quantitative comparison results between VERSA with varying shot and the C-VAE. The quantitative metrics all show the superiority of VERSA over a C-VAE. As the number of shots increase to 5, the measurements show a corresponding improvement.</p><p>Model MSE SSIM C-VAE 1-shot 0.0269 0.5705 VERSA 1-shot 0.0108 0.7893 VERSA 5-shot 0.0069 0.8483 <ref type="table">Table 2</ref>: View reconstruction test results. Mean squared error (MSE -lower is better) and the structural similarity index (SSIM -higher is better) <ref type="bibr" target="#b55">(Wang et al., 2004)</ref> are measured between the generated and ground truth images. Error bars not shown as they are insignificant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSIONS</head><p>We have introduced ML-PIP, a probabilistic framework for meta-learning. ML-PIP unifies a broad class of recently proposed meta-learning methods, and suggests alternative approaches. Building on ML-PIP, we developed VERSA, a few-shot learning algorithm that avoids the use of gradient based optimization at test time by amortizing posterior inference of task-specific parameters. We evaluated VERSA on several few-shot learning tasks and demonstrated state-of-the-art performance and compelling visual results on a challenging 1-shot view reconstruction task. a We report the performance of Prototypical Networks when training and testing with the same "shot" and "way", which is consistent with the experimental protocol of the other methods listed. We note that Prototypical Networks perform better when trained on higher "way" than that of testing. In particular, when trained on 20-way classification and tested on 5-way, the model achieves 68.20 ? 0.66%.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A BAYESIAN DECISION THEORETIC GENERALIZATION OF ML-PIP</head><p>A generalization of the new inference framework presented in Section 2 is based upon Bayesian decision theory (BDT). BDT provides a recipe for making predictions? for an unknown test variabl? y by combining information from observed training data D (t) (here from a single task t) and a loss function L(?,?) that encodes the cost of predicting? when the true value is? <ref type="bibr" target="#b2">(Berger, 2013;</ref><ref type="bibr" target="#b20">Jaynes, 2003)</ref>. In BDT an optimal prediction minimizes the expected loss (suppressing dependencies on the inputs and ? to reduce notational clutter): 3 y * = argmin y p(?|D (t) )L(?,?)d?, where p(?|D (t) ) = p(?|? (t) )p(? (t) |D (t) )d? (t) (A.1) is the Bayesian predictive distribution and p(? (t) |D (t) ) the posterior distribution of ? (t) given the training data from task t.</p><p>BDT separates test and training data and so is a natural lens through which to view recent episodic approaches to training that utilize many internal training/test splits <ref type="bibr" target="#b53">(Vinyals et al., 2016)</ref>. Based on this insight, what follows is a fairly dense derivation of an ultimately simple stochastic variational objective for meta-learning probabilistic inference that is rigorously grounded in Bayesian inference and decision theory.</p><p>Distributional BDT. We generalize BDT to cases where the goal is to return a full predictive distribution q(?) over the unknown test variable? rather than a point prediction. The quality of q is quantified through a distributional loss function L(?, q(?)). Typically, if? (the true value of the underlying variable) falls in a low probability region of q(?) the loss will be high, and vice versa. The optimal predictive q * is found by optimizing the expected distributional loss with q constrained to a distributional family Q:</p><formula xml:id="formula_19">q * = argmin q?Q p(?|D (t) )L(?, q(?))d?. (A.2)</formula><p>Amortized variational training. Here, we amortize q to form quick predictions at test time and learn parameters by minimizing average expected loss over tasks. Let ? be a set of shared variational parameters such that q(?) = q ? (?|D) (or q ? for shorthand). Now the approximate predictive distribution can take any training dataset D (t) as an argument and directly perform prediction of? <ref type="bibr">(t)</ref> . The optimal variational parameters are found by minimizing the expected distributional loss across tasks</p><formula xml:id="formula_20">? * = argmin ? L [q ? ] , L [q ? ] = p(D)p(?|D)L(?, q ? (?|D))d? dD = E p(D,?) [L(?, q ? (?|D))] .</formula><p>(A.3) Here the variables D,x and? are placeholders for integration over all possible datasets, test inputs and outputs. Note that Eq. (A.3) can be stochastically approximated by sampling a task t and randomly partitioning into training data D and test data {x m ,? m } M m=1 , which naturally recovers episodic minibatch training over tasks and data <ref type="bibr" target="#b53">(Vinyals et al., 2016;</ref><ref type="bibr" target="#b42">Ravi and Larochelle, 2017)</ref>. Critically, this does not require computation of the true predictive distribution. It also emphasizes the meta-learning aspect of the procedure, as the model is learning how to infer predictive distributions from training tasks.</p><p>Loss functions. We employ the log-loss: the negative log density of q ? at?. In this case,</p><formula xml:id="formula_21">L [q ? ] = E p(D,?) [? log q ? (?|D)] = E p(D) [KL [p(?|D) q ? (?|D)] + H [p(?|D)]] , (A.4)</formula><p>where KL[p(y) q(y)] is the KL-divergence, and H [p(y)] is the entropy of p. Eq. (A.4) has the elegant property that the optimal q ? is the closest member of Q (in a KL sense) to the true predictive p(?|D), which is unsurprising as the log-loss is a proper scoring rule <ref type="bibr" target="#b19">(Huszar, 2013)</ref>. This is reminiscent of the sleep phase in the wake-sleep algorithm <ref type="bibr" target="#b18">(Hinton et al., 1995)</ref>. Exploration of alternative proper scoring rules <ref type="bibr" target="#b7">(Dawid, 2007)</ref> and more task-specific losses <ref type="bibr" target="#b29">(Lacoste-Julien et al., 2011)</ref> is left for future work.</p><p>Specification of the approximate predictive distribution. Next, we consider the form of q ? . Motivated by the optimal predictive distribution, we replace the true posterior by an approximation:</p><formula xml:id="formula_22">q ? (?|D) = p(?|?)q ? (?|D)d?. (A.5) B JUSTIFICATION FOR CONTEXT-INDEPENDENT APPROXIMATION</formula><p>In this section we lay out both theoretical and empirical justifications for the context-independent approximation detailed in Section 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 THEORETICAL ARGUMENT -DENSITY RATIO ESTIMATION</head><p>A principled justification for the approximation is best understood through the lens of density ratio estimation <ref type="bibr" target="#b34">(Mohamed, 2018;</ref><ref type="bibr" target="#b48">Sugiyama et al., 2012)</ref>. We denote the conditional density of each class as p(x|y = c) and assume equal a priori class probability p(y = c) = 1/C. Density ratio theory then uses Bayes' theorem to show that the optimal softmax classifier can be expressed in terms of the conditional densities <ref type="bibr" target="#b34">(Mohamed, 2018;</ref><ref type="bibr" target="#b48">Sugiyama et al., 2012)</ref>:</p><formula xml:id="formula_23">Softmax(y = c|x) = exp(h(x) w c ) c exp(h(x) w c ) = p(y = c|x) = p(x|y = c) c p(x|y = c ) , (B.1)</formula><p>This implies that the optimal classifier will construct estimators for the conditional density for each class, that is exp(h(x) w c ) ? p(x|y = c). Importantly for our approximation, notice that these estimates are constructed independently for each class, similarly to training a naive Bayes classifier. VERSA mirrors this optimal form using:</p><formula xml:id="formula_24">log p(x|y = c) ? h ? (x) w c , (B.2)</formula><p>where w c ? q ? (w|{x n |y n = c}) for each class in a given task. Under ideal conditions (i.e., if one could perfectly estimate p(x|y = c)), the context-independent assumption holds, further motivating our design.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 EMPIRICAL JUSTIFICATION</head><p>Here we detail a simple experiment to evaluate the validity of the context-independent inference assumption. The goal of the experiment is to examine if weights may be context-independent without imposing the assumption on the amortization network. To see this, we randomly generate fifty tasks from a dataset, where classes may appear a number of times in different tasks. We then perform free-form (non-amortized) variational inference on the weights for each of the tasks, with a Gaussian variational distribution:</p><formula xml:id="formula_25">q ? W (t) |D (t) , ? = N W (t) ; ? (t) ? , ? (t)2 ? . (B.3)</formula><p>If the assumption is reasonable, we may expect the distribution of the weights of a specific class to be similar regardless of the additional classes in the task.</p><p>We examine 5-way classification in the MNIST dataset. We randomly sample and fix fifty such tasks. We train the model twice using the same feature extraction network used in the few-shot classification experiments, and fix the d ? to be 16 and 2. We then train the model in an episodic manner by minibatching tasks at each iteration. The model is trained to convergence, and achieves 99% accuracy on held out test examples for the tasks. After training is complete we examine the optimized ? (t) ? for each class in each task. <ref type="figure" target="#fig_1">Fig. B.1a</ref> shows a t-SNE <ref type="bibr" target="#b32">(Maaten and Hinton, 2008)</ref> plot for the 16-dimensional weights. We see that when reduced to 2-dimensions, the weights cluster according to class. <ref type="figure">Fig. B</ref>.1b visualizes the weights in their original space. In this plot, weights from the same class are grouped together, and clear similarity patterns are evident across the image, showing that weights from the same class have similar means across tasks. <ref type="figure">Fig. B</ref>.2 details the task weights in 2-dimensional space. Here, each pentagon represents the weight means learned for one training task, where the nodes of the pentagon are colored according to the class the weights represent. <ref type="figure" target="#fig_9">In Fig. B.2a</ref> we see that overall,  the classes cluster in 2-dimensional space as well. However, there is some overlap (e.g., classes '1' and '2'), and that for some tasks a class-weight may appear away from the cluster. <ref type="figure" target="#fig_9">Fig. B.2b</ref> shows the same plot, but only for tasks that contain both class '1' and '2'. Here we can see that for these tasks, class '2' weights are all located away from their cluster.</p><p>This implies that each class-weights are typically well-approximated as being independent of the task. However, if the model lacks capacity to properly assign each set of class weights to different regions of space, for tasks where classes from similar regions of space appear, the inference procedure will 'move' one of the class weights to an 'empty' region of the space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C VARIATIONAL INFERENCE DERIVATIONS FOR THE MODEL</head><p>We derive a VI-based objective for our probabilistic model. By "amortized" VI we mean that q ? (?|D (t) , ?) is parameterized by a neural network with a fixed-sized ?. Conversely, "non-amortized" VI refers to local parameters ? (t) that are optimized independently (at test time) for each new task t, such that q(?|D (t) , ?) = N (?|? ? (t) , ? ? (t) ). However, the derivation of the objective function does not change between these options. For a single task t, an evidence lower bound (ELBO; <ref type="bibr" target="#b54">(Wainwright and Jordan, 2008)</ref>) may be expressed as:</p><formula xml:id="formula_26">L t = E q ? (?|D (t) ,?) ? ? (x,y)?D (t) log p(y|x, ?, ?) ? ? ? KL q ? (?|D (t) , ?) p(?|?) . (C.1)</formula><p>We can then derive a stochastic estimator to optimize Eq. (C.1) by sampling D (t) ? p(D) (approximated with a training set of tasks) and simple Monte Carlo integration over ? such that ? (l) ? q ? (?|D (t) , ?):</p><formula xml:id="formula_27">L = 1 T T t=1 ? ? (x,y)?D (t) 1 L L l=1 log p(y|x, ? (l) , ?) ? KL q ? (?|D (t) , ?) p(?|?) ? ? , (C.2)</formula><p>Eq. (C.2) differs from our objective function in Eq. (4) in two important ways: (i) Eq. (4) does not contain a KL term for q ? (?|D (t) , ?) (nor any other form of prior distribution over ?, and (ii) Eq. (C.1) does not distinguish between training and test data within a task, and therefore does not explicitly encourage the model to generalize in any way.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D EXPERIMENTATION DETAILS</head><p>In this section we provide comprehensive details on the few-shot classification experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1 OMNIGLOT FEW-SHOT CLASSIFICATION TRAINING PROCEDURE</head><p>Omniglot <ref type="bibr" target="#b30">(Lake et al., 2011</ref>) is a few-shot learning dataset consisting of 1623 handwritten characters (each with 20 instances) derived from 50 alphabets. We follow a pre-processing and training procedure akin to that defined in <ref type="bibr" target="#b53">(Vinyals et al., 2016)</ref>. First the images are resized to 28 ? 28 pixels and then character classes are augmented with rotations of 90 degrees.  <ref type="bibr" target="#b23">(Kingma and Ba, 2015)</ref> optimizer with a constant learning rate of 0.0001 with 16 tasks per batch to train all models.</p><p>The 5-way -5-shot and 5-way -1-shot models are trained for 80,000 iterations while the 20-way -5-shot model is trained for 60,000 iterations, and the 20-way -1-shot model is trained for 100,000 iterations. In addition, we use a Gaussian form for q and set the number of ? samples to L = 10.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2 miniIMAGENET FEW-SHOT CLASSIFICATION TRAINING PROCEDURE</head><p>miniImageNet <ref type="bibr" target="#b53">(Vinyals et al., 2016</ref>) is a dataset of 60,000 color images that is sub-divided into 100 classes, each with 600 instances. The images have dimensions of 84 ? 84 pixels. For our experiments, we use the 64 training, 16 validation, and 20 test class splits defined by <ref type="bibr" target="#b42">(Ravi and Larochelle, 2017)</ref>. Training proceeds in the same episodic manner as with Omniglot. We use the Adam <ref type="bibr" target="#b23">(Kingma and Ba, 2015)</ref> optimizer and a Gaussian form for q and set the number of ? samples to L = 10. For the 5-way -5-shot model, we train using 4 tasks per batch for 100,000 iterations and use a constant learning rate of 0.0001. For the 5-way -1-shot model, we train with 8 tasks per batch for 50,000 iterations and use a constant learning rate of 0.00025.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.3 FEW-SHOT CLASSIFICATION NETWORK ARCHITECTURES</head><p>Tables D.1 to D.4 detail the neural network architectures for the feature extractor ?, amortization network ?, and linear classifier ?, respectively. The feature extraction network is very similar to that used in <ref type="bibr" target="#b53">(Vinyals et al., 2016)</ref>. The output of the amortization network yields mean-field Gaussian parameters for the weight distributions of the linear classifier ?. When sampling from the weight distributions, we employ the local-reparameterization trick , that is we sample from the implied distribution over the logits rather than directly from the variational distribution.</p><p>To reduce the number of learned parameters, we share the feature extraction network ? with the pre-processing phase of the amortizaion network ?. Omniglot Shared Feature Extraction Network (?):x ? h ? (x) Output size Layers 28 ? 28 ? 1 Input image 14 ? 14 ? 64 conv2d (3 ? 3, stride 1, SAME, RELU), dropout, pool (2 ? 2, stride 2, SAME) 7 ? 7 ? 64 conv2d (3 ? 3, stride 1, SAME, RELU), dropout, pool (2 ? 2, stride 2, SAME) 4 ? 4 ? 64 conv2d (3 ? 3, stride 1, SAME, RELU), dropout, pool (2 ? 2, stride 2, SAME) 2 ? 2 ? 64 conv2d (3 ? 3, stride 1, SAME, RELU), dropout, pool (2 ? 2, stride 2, SAME) 256 flatten <ref type="table">Table D</ref>.2: Feature extraction network used for miniImageNet few-shot learning. Batch Normalization and dropout with a keep probability of 0.5 used throughout.</p><p>miniImageNet Shared Feature Extraction Network (?):x ? h ? (x) Output size Layers 84 ? 84 ? 1 Input image 42 ? 42 ? 64 conv2d (3 ? 3, stride 1, SAME, RELU), dropout, pool (2 ? 2, stride 2, VALID) 21 ? 21 ? 64 conv2d (3 ? 3, stride 1, SAME, RELU), dropout, pool (2 ? 2, stride 2, VALID) 10 ? 10 ? 64 conv2d (3 ? 3, stride 1, SAME, RELU), dropout, pool (2 ? 2, stride 2, VALID) 5 ? 5 ? 64 conv2d (3 ? 3, stride 1, SAME, RELU), dropout, pool (2 ? 2, stride 2, VALID) 2 ? 2 ? 64 conv2d (3 ? 3, stride 1, SAME, RELU), dropout, pool (2 ? 2, stride 2, VALID) 256 flatten E SHAPENET EXPERIMENTATION DETAILS E.1 VIEW RECONSTRUCTION TRAINING PROCEDURE AND NETWORK ARCHITECTURES ShapeNetCore v2 <ref type="bibr" target="#b5">(Chang et al., 2015)</ref> is an annotated database of 3D objects covering 55 common object categories with ?51,300 unique objects. For our experiments, we use 12 of the largest object categories. Refer to <ref type="table">Table E</ref>.1 for a complete list. We concatenate all instances from all 12 of the  object categories together to obtain a dataset of 37,108 objects. This concatenated dataset is then randomly shuffled and we use 70% of the objects (25,975 in total) for training, 10% for validation (3,710 in total) , and 20% (7423 in total) for testing. For each object, we generate V = 36, 128 ? 128 pixel image views spaced evenly every 10 degrees in azimuth around the object. We then convert the rendered images to gray-scale and reduce their size to be 32 ? 32 pixels. Again, we train our model in an episodic manner. Each training iteration consists a batch of one or more tasks. For each task an object is selected at random from the training set. We train on a single view selected at random from the V = 36 views associated with each object and use the remaining 35 views to evaluate the objective function. We then generate 36 views of the object with a modified version of our amortization network which is shown diagrammatically in <ref type="figure" target="#fig_3">Fig. 3</ref>. To evaluate the system, we generate views and compute quantitative metrics over the entire test set. Tables E.2 to E.4 describe the network architectures for the encoder, amortization, and generator networks, respectively. To train, we use the Adam <ref type="bibr" target="#b23">(Kingma and Ba, 2015)</ref> optimizer with a constant learning rate of 0.0001 with 24 tasks per batch for 500,000 training iterations. In addition, we set d ? = 256, d ? = 256 and number of ? samples to 1.  <ref type="table" target="#tab_0">airplane  02691156  4045  bench  02828884  1813  cabinet  02933112  1571  car  02958343  3533  phone  02992529  831  chair  03001627  6778  display  03211117  1093  lamp  03636649  2318  speaker  03691459  1597  sofa  04256520  3173  table  04379243  8436  boat  04530566  1939</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Object Category sysnet ID Instances</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>distinguished for each task t, as this is key for few-shot learning.x ..., Mt n = 1, ..., Nt t=1,...</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Directed graphical model for multi-task learning.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>classifier are task specific and inferred through an amortization network with parameters ?. Right: Amortization network that maps the extracted features of the k training examples of a particular class to the corresponding weight vector of the linear classifier. and {? (t) m ,x (t) m , D (t) } ? p(?,x, D), where p represents the data distribution (e.g., sampling tasks and splitting them into disjoint training data D and test</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Computational flow of VERSA for few-shot view reconstruction. Left: A set of training images and angles {to a stochastic input ? (t) through the amortization network q ? . ? (t) is then concatenated with a test anglex and mapped onto a new image through the generator ?. Right: Amortization network that maps k image/angle examples of a particular object-instance to the corresponding stochastic input. single softmax output instead of the entire weight matrix directly. To reduce the number of learned parameters, the amortization network operates directly on the extracted features h ? (x):</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>True posteriors p(?|D) ( ) and approximate posteriors q ? (?|D) ( ) for unseen test sets ( ) in the experiment. In both cases (five and ten shot), the approximate posterior closely resembles the true posterior given the observed data.(2017) for miniImagenet, using equivalent architectures for h ? . Training is carried out in an episodic manner: for each task, k c examples are used as training inputs to infer q ? (? (c) |D, ?) for each class, and an additional set of examples is used to evaluate the objective function. Full details of data preparation and network architectures are provided in Appendix D.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :</head><label>6</label><figDesc>Results for ShapeNet view reconstruction for unseen objects from the test set (shown left). The model was trained to reconstruct views from a single orientation. Top row: images/views generated by a C-VAE model; middle row images/views generated by VERSA; bottom row: ground truth images. Views are spaced evenly every 30 degrees in azimuth.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>Visualizing the learned weights for d ? = 16. (a) Weight dimensionality is reduced using T-SNE (Maaten and Hinton, 2008). Weights are colored according to class. (b) Each weight represents one column of the image. Weights are grouped by class.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure B. 2 :</head><label>2</label><figDesc>Visualizing the task weights for d ? = 2. (a) All training tasks. (b) Only training tasks containing both '1's and '2's.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 3</head><label>3</label><figDesc>details few-shot classification performance for VERSA as well as competitive approaches. The tables include results for only those approaches with comparable training procedures and convolutional feature extraction architectures. Approaches that employ pre-training and/or residual networks</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Negative Log-likelihood (NLL) results for different few-shot settings on Omniglot and miniImageNet. The ? sign indicates the 95% confidence interval over tasks using a Student's t-distribution approximation.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Omniglot</cell><cell></cell><cell cols="2">miniImageNet</cell></row><row><cell></cell><cell cols="2">5-way NLL</cell><cell cols="2">20-way NLL</cell><cell cols="2">5-way NLL</cell></row><row><cell>Method</cell><cell>1-shot</cell><cell>5-shot</cell><cell>1-shot</cell><cell>5-shot</cell><cell>1-shot</cell><cell>5-shot</cell></row><row><cell>Amortized VI Non-Amortized VI VERSA</cell><cell>0.179 ? 0.009 0.144 ? 0.005 0.010 ? 0.005</cell><cell>0.137 ? 0.004 0.025 ? 0.001 0.007 ? 0.003</cell><cell>0.456 ? 0.010 0.393 ? 0.005 0.079 ? 0.009</cell><cell>0.253 ? 0.004 0.078 ? 0.002 0.031 ? 0.004</cell><cell>1.328 ? 0.024 1.183 ? 0.023</cell><cell>1.165 ? 0.010 0.859 ? 0.015</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Accuracy results for different few-shot settings on Omniglot and miniImageNet. The ? sign indicates the 95% confidence interval over tasks using a Student's t-distribution approximation. Bold text indicates the highest scores that overlap in their confidence intervals. ? 0.55 98.71 ? 0.22 90.56 ? 0.54 96.12 ? 0.23 44.13 ? 1.78 55.68 ? 0.91 Non-Amortized VI 98.77 ? 0.18 99.74 ? 0.06 95.28 ? 0.19 98.84 ? 0.09 VERSA (Ours) 99.70 ? 0.20 99.75 ? 0.13 97.66 ? 0.29 98.77 ? 0.18 53.40 ? 1.82 67.37 ? 0.86</figDesc><table><row><cell></cell><cell>Omniglot</cell><cell></cell><cell>miniImageNet</cell></row><row><cell>Method</cell><cell>5-way accuracy (%) 1-shot 5-shot</cell><cell>20-way accuracy (%) 1-shot 5-shot</cell><cell>5-way accuracy (%) 1-shot 5-shot</cell></row><row><cell>Siamese Nets (Koch et al., 2015) Matching Nets (Vinyals et al., 2016) Neural Statistician (Edwards and Storkey, 2017) Memory Mod (Kaiser et al., 2017) Meta LSTM (Ravi and Larochelle, 2017) MAML (Finn et al., 2017) Prototypical Nets a (Snell et al., 2017) mAP-SSVM (Triantafillou et al., 2017) mAP-DLM (Triantafillou et al., 2017) LLAMA (Grant et al., 2018) PLATIPUS (Finn et al., 2018) Meta-SGD (Li et al., 2017) SNAIL (Mishra et al., 2018) Relation Net (Yang et al., 2018) Reptile (Nichol et al., 2018) BMAML (Yoon et al., 2018)</cell><cell cols="3">97.3 98.1 98.1 98.4 98.7 ? 0.4 97.4 98.6 98.8 99.53 ? 0.26 99.93 ? 0.09 95.93 ? 0.38 98.97 ? 0.19 50.47 ? 1.87 64.03 ? 0.94 98.4 88.1 97.0 98.9 93.8 98.5 46.6 60.0 99.5 93.2 98.1 99.6 95.0 98.6 43.44 ? 0.77 60.60 ? 0.71 99.9 ? 0.1 95.8 ? 0.3 98.9 ? 0.2 48.7 ? 1.84 63.11 ? 0.92 99.3 95.4 98.7 46.61 ? 0.78 65.77 ? 0.70 99.6 95.2 98.6 50.32 ? 0.80 63.94 ? 0.72 99.6 95.4 98.6 50.28 ? 0.80 63.70 ? 0.70 49.40 ? 1.83 50.13 ? 1.86 99.07 ? 0.16 99.78 ? 0.09 97.64 ? 0.30 99.36 ? 0.18 45.1 55.2 99.6 ? 0.2 99.8 ? 0.1 97.6 ? 0.2 99.1 ? 0.1 50.44 ? 0.82 65.32 ? 0.70 97.68 ? 0.04 99.48 ? 0.06 89.43 ? 0.14 97.12 ? 0.32 49.97 ? 0.32 65.99 ? 0.58 53.8 ? 1.46</cell></row><row><cell>Amortized VI</cell><cell>97.77</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>The training, validation and test sets consist of a random split of 1100, 100, and 423 characters, respectively. When augmented this results in 4400 training, 400 validation, and 1292 test classes, each having 20 character instances. For C-way, k c -shot classification, training proceeds in an episodic manner. Each training iteration consists of a batch of one or more tasks. For each task C classes are selected at random from the training set. During training, k c character instances are used as training inputs and 15 character instances are used as test inputs. The validation set is used to monitor the progress of learning and to select the best model to test, but does not affect the training process. Final evaluation of the trained model is done on 600 randomly selected tasks from the test set. During evaluation, k c character instances are used as training inputs and k c character instances are used as test inputs. We use the Adam</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>Table D.1: Feature extraction network used for Omniglot few-shot learning. Batch Normalization and dropout with a keep probability of 0.9 used throughout.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>Table D.3: Amortization network used for Omniglot and miniImageNet few-shot learning.Amortization Network (?): x c 1 , ..., x c kc ? ? w (c) , ? 2</figDesc><table><row><cell></cell><cell></cell><cell>w (c)</cell></row><row><cell>Phase</cell><cell cols="2">Output size Layers</cell></row><row><cell>feature extraction instance pooling ? weight distribution</cell><cell>k ? 256 256 256</cell><cell>shared feature network (?) mean 2 ? fully connected, ELU + linear fully connected to ? w (c) , ? 2 w (c)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>Table D.4: Linear classifier used for Omniglot and miniImageNet few-shot learning.</figDesc><table><row><cell cols="2">Linear Classifier (?): h ? (x) ? p(?|x, ?, ? t ) Output size Layers</cell></row><row><cell>256 C</cell><cell>Input features fully connected, softmax</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>Table E.1: List of ShapeNet categories used in the VERSA view reconstruction experiments.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Note that the true predictive posterior p(y|D) is recovered regardless of the identifiability of p(?|D).</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Source code for the experiments is available at https://github.com/Gordonjo/versa.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">For discrete outputs the integral may be replaced with a summation.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENTS</head><p>We thank Ravi   </p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>8 ? 8 ? 64 conv2d (3 ? 3, stride 1, SAME, RELU), pool (2 ? 2, stride 2, VALID) 4 ? 4 ? 64 conv2d (3 ? 3, stride 1, SAME, RELU), pool (2 ? 2, stride 2, VALID) 2 ? 2 ? 64 conv2d (3 ? 3, stride 1, SAME, RELU), pool (2 ? 2, stride 2, VALID) d ? fully connected, RELU ShapeNet Amortization Network (?): ShapeNet Generator Network (?):x ? p(?|x, ?, ? (t) ) Output size Layers</p><p>fully connected, RELU 1024 fully connected, RELU 2 ? 2 ? 256 reshape 4 ? 4 ? 128 deconv2d (3 ? 3, stride 2, SAME, RELU) 8 ? 8 ? 64 deconv2d (3 ? 3, stride 2, SAME, RELU) 16 ? 16 ? 32 deconv2d (3 ? 3, stride 2, SAME, RELU) 32 ? 32 ? 1 deconv2d (3 ? 3, stride 2, SAME, sigmoid)</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Task clustering and gating for Bayesian multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Bakker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Heskes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="83" to="99" />
			<date type="published" when="2003-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Discriminative k-shot learning using probabilistic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rojas-Carulla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>?wiatkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sch?lkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Turner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.00326</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Statistical decision theory and Bayesian analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">O</forename><surname>Berger</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>Springer Science &amp; Business Media</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Weight uncertainty in neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cornebise</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1613" to="1622" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Statistical inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Casella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">L</forename><surname>Berger</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">2</biblScope>
			<pubPlace>Duxbury Pacific Grove, CA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">ShapeNet: An Information-Rich 3D Model Repository</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hanrahan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.03012</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
		<respStmt>
			<orgName>Stanford University -Princeton University -Toyota Technological Institute at Chicago</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
	<note>cs.GR</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Inference suboptimality in variational autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cremer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Duvenaud</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning</title>
		<editor>J. Dy and A. Krause</editor>
		<meeting>the 35th International Conference on Machine Learning<address><addrLine>Stockholmsmssan, Stockholm Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-07" />
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="10" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The geometry of proper scoring rules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Dawid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of the Institute of Statistical Mathematics</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="77" to="93" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Towards a neural statistician</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Storkey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR</title>
		<meeting>the International Conference on Learning Representations (ICLR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">One-shot learning of object categories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="594" to="611" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Model-agnostic meta-learning for fast adaptation of deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1126" to="1135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Probabilistic model-agnostic meta-learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/8161-probabilistic-model-agnostic-meta-learning.pdf" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 31</title>
		<editor>S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="9516" to="9527" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Garnelo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rosenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Maddison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ramalho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Saxton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shanahan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M A</forename><surname>Eslami</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v80/garnelo18a.html" />
		<title level="m">Conditional neural processes</title>
		<imprint>
			<date type="published" when="2018-07-15" />
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="1704" to="1713" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Garnelo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schwarz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rosenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.01622</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">Neural processes. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Dynamic few-shot visual learning without forgetting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4367" to="4375" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Recasting gradient-based meta-learning as hierarchical Bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Grant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Griffiths</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR)</title>
		<meeting>the International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hypernetworks</surname></persName>
		</author>
		<ptr target="https://openreview.net/pdf?id=rkpACe1lx" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Empirical bayes for learning to learn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Heskes</surname></persName>
		</author>
		<ptr target="http://dl.acm.org/citation.cfm?id=645529.658133" />
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="367" to="374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The&quot; wake-sleep&quot; algorithm for unsupervised neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dayan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">J</forename><surname>Frey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>Neal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">268</biblScope>
			<biblScope unit="issue">5214</biblScope>
			<biblScope unit="page" from="1158" to="1161" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Scoring rules, divergences and information in Bayesian machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huszar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
		<respStmt>
			<orgName>University of Cambridge</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Probability theory: the logic of science</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">T</forename><surname>Jaynes</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<publisher>Cambridge university press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning to remember rare events</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nachum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Aurko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Semi-amortized variational autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wiseman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sontag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning</title>
		<meeting>the 35th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR)</title>
		<meeting>the International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Auto-encoding variational Bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR)</title>
		<meeting>the International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Semi-supervised learning with deep generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3581" to="3589" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Variational dropout and the local reparameterization trick</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2575" to="2583" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Siamese neural networks for one-shot image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML Deep Learning Workshop</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lacoste</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Oreshkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Boquet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Rostamzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krueger</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.07528</idno>
		<title level="m">Uncertainty in multitask transfer learning</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Approximate inference for the loss-calibrated Bayesian</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lacoste-Julien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Husz?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics</title>
		<meeting>the Fourteenth International Conference on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="416" to="424" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">One shot learning of simple visual concepts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Meeting of the Cognitive Science Society</title>
		<meeting>the Annual Meeting of the Cognitive Science Society</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">33</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.09835</idno>
		<title level="m">Meta-sgd: Learning to learn quickly for few shot learning</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Visualizing data using t-SNE</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V D</forename><surname>Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
			<date type="published" when="2008-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">A simple neural attentive meta-learner</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohaninejad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=B1DmUzWAW" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Density ratio trick</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
		<ptr target="http://blog.shakirm.com/2018/01/machine-learning-trick-of-the-day-7-density-ratio-trick/" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Meta-neural networks that learn by learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">K</forename><surname>Naik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mammone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Networks, 1992. IJCNN., International Joint Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1992" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="437" to="442" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Learning disentangled representations with semi-supervised deep generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Narayanaswamy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">B</forename><surname>Paige</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-W</forename><surname>Van De Meent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5927" to="5937" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">On discriminative vs. generative classifiers: A comparison of logistic regression and naive bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="841" to="848" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">On first-order meta-learning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nichol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Achiam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.02999</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Tadam: Task dependent adaptive metric for improved few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Oreshkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">R</forename><surname>L?pez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lacoste</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="721" to="731" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Pointnet: Deep learning on point sets for 3d classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Few-shot image recognition by predicting parameters from activations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Optimization as a model for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR</title>
		<meeting>the International Conference on Learning Representations (ICLR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Stochastic backpropagation and approximate inference in deep generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1278" to="1286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Meta-learning with latent embedding optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sygnowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=BJgklhAcK7" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Few-shot learning with graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">G</forename><surname>Satorras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Estrach</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=BJj6qGbRW" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Evolutionary principles in self-referential learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1987" />
		</imprint>
		<respStmt>
			<orgName>Technische Universit?t M?nchen</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Prototypical networks for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4080" to="4090" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Density ratio estimation in machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Suzuki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanamori</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Learning to learn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thrun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pratt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>Springer Science &amp; Business Media</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Few-shot learning through an information retrieval lens</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Triantafillou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2255" to="2265" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Trippe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Turner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.06230</idno>
		<title level="m">Overpruning in variational bayesian neural networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Two problems with variational expectation maximisation for time-series models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Turner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sahani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bayesian Time series models</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">3.1</biblScope>
			<biblScope unit="page" from="3" to="4" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Matching networks for one shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3630" to="3638" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Graphical models, exponential families, and variational inference. Foundations and Trends R in Machine Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Wainwright</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1" to="305" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Image quality assessment: from error visibility to structural similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">R</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on image processing</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="600" to="612" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Bayesian model-agnostic meta-learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Dia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ahn</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/7963-bayesian-model-agnostic-meta-learning.pdf" />
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7332" to="7342" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Deep sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kottur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ravanbakhsh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Poczos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3394" to="3404" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
