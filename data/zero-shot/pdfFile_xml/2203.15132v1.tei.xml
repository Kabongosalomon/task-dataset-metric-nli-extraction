<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">LocalBins: Improving Depth Estimation by Learning Local Distributions</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shariq</forename><surname>Farooq Bhat</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">KAUST</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ibraheem</forename><surname>Alhashim</surname></persName>
							<email>ibraheem.alhashim@gmail.com</email>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">National Center for Artificial Intelligence (NCAI)</orgName>
								<orgName type="department" key="dep2">Kingdom of Saudi Arabia</orgName>
								<orgName type="laboratory">Saudi Data and Artificial Intelligence Authority (SDAIA)</orgName>
								<address>
									<settlement>Riyadh</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Wonka</surname></persName>
							<email>pwonka@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution">KAUST</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">LocalBins: Improving Depth Estimation by Learning Local Distributions</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T07:45+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>single image depth estimation</term>
					<term>encoder-decoder architec- ture</term>
					<term>deep learning</term>
					<term>dense regression</term>
					<term>histogram prediction</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose a novel architecture for depth estimation from a single image. The architecture itself is based on the popular encoderdecoder architecture that is frequently used as a starting point for all dense regression tasks. We build on AdaBins which estimates a global distribution of depth values for the input image and evolve the architecture in two ways. First, instead of predicting global depth distributions, we predict depth distributions of local neighborhoods at every pixel. Second, instead of predicting depth distributions only towards the end of the decoder, we involve all layers of the decoder. We call this new architecture LocalBins. Our results demonstrate a clear improvement over the state-of-the-art in all metrics on the NYU-Depth V2 dataset. Code and pretrained models will be made publicly available. 3</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In this paper, we propose a new architecture for learning to estimate depth values given a single input image. In this line of work there are two main approaches that have been followed recently. Combining to train on multiple datasets at once while factoring out scale, e.g. <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b27">28]</ref> and training on a single dataset with consistent scale, e.g. <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b3">4]</ref>. Our approach falls in the second category. There are multiple published competing architectures, with Ad-aBins <ref type="bibr" target="#b3">[4]</ref> currently being the most successful architecture on datasets such as NYU-Depth V2 <ref type="bibr" target="#b28">[29]</ref>. Our newly proposed architecture, called LocalBins aims to improve upon this work.</p><p>The main idea of AdaBins is to predict adaptive bins that estimate one "global" depth distribution per image. This prediction works both as auxiliary supervision of depth estimation, but also directly influences the depth prediction.</p><p>We initially formulated two objectives in evolving AdaBins. First, we wanted to see if predicting local distributions around each pixel can improve upon predicting one global distribution for the complete input image. Second, we wanted  to design the architecture such that depth distribution supervision can be injected earlier in the network, preferably in a multi-scale fashion. AdaBins needs a special architecture design to work. Estimating global adaptive bins needs a transformer at "high resolution". Estimation of bins is done close to the output layer and most of the work is delegated to a specialized module based on a transformer. Even though this improves performance significantly, this offload of work may prevent earlier layers to fully exploit the "distribution supervision" to learn better representations. We call this the 'late injection problem' in our arguments. Any attempts to estimate global adaptive bins earlier in the network (e.g. near the bottleneck) or without a transformer leads to unstable trainingdivergence or convergence to a suboptimal point.</p><p>We realize these ideas in the following manner. To perform local predictions of depth distributions, we propose to use bin estimation at every pixel, and impose regularization on bin predictions via a query and response training scheme. Our proposed module is regularized to predict the depth distributions within the randomly selected bounding boxes within the image.</p><p>To perform multi-scale predictions of depth distributions, we let the network predict local depth distributions in a gradual step-wise manner throughout the decoder. Starting with a small N seed number of bins, at the bottleneck each bin is subsequently split into two at every decoder layer, i.e., the i th layer of the decoder estimates 2 i N seed bins at every pixel position. Together with locality, this coarseto-fine construction lets us avoid unstable training and simultaneously solves the late injection problem.</p><p>Our proposed LocalBins module is lightweight (adding only ?1M params) and can be used in conjunction with any encoder-decoder network.</p><p>To summarize, we make the following contributions:</p><p>-We propose a new architecture for single image depth estimation that improves upon the state-of-the-art in all metrics on the NYU-Depth V2 <ref type="bibr" target="#b28">[29]</ref> dataset. Models and code will be made publicly available. -We propose two novel architecture ideas to single image depth estimation: 1) estimating local histograms instead of a single global histogram and 2) estimating histograms in a multi-scale fashion to benefit from distribution supervision earlier in the pipeline. Even beyond depth estimation, we are not aware of existing similar concepts and we believe that these ideas could be beneficial beyond depth estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>There are multiple categories of depth estimation methods. The first category are unsupervised methods <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b41">41,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b40">40,</ref><ref type="bibr" target="#b31">32]</ref>. These methods do not use ground truth depth data, but use self-supervision generally by some form of 3D reconstruction to learn depth values. These methods typically use videos or stereo videos as input. The second category of methods learn depth estimation from multiple datasets <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b26">27]</ref> jointly. Combining multiple datasets requires considering the different depth scales of the scenes. Therefore, methods that train on multiple datasets are generally not comparable to methods that train on a single dataset, because the test protocol is different. The third category of algorithms are domain transfer methods <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b38">38,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b0">1]</ref>. These techniques assume the availability of ground truth data in one domain during training, but the images in the target domain do not have ground truth depth (or only a few of them have ground truth depth <ref type="bibr" target="#b39">[39]</ref>). The fourth category are depth estimation methods that learn a depth estimation network for each dataset separately <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b3">4]</ref>. Our method belongs to this category of supervised monocular depth estimation. These methods formulate the task as the regression of a depth map from a single RGB input image. The current dominant architecture follows an encoder-decoder network. Most high performing methods apply such architecture with some variations on the process of extracting of relevant feature maps during encoding and the fusion of these features with the intermediate maps produced during the decoding stage. Finally, we mention two very recent arXiv submissions that are concurrent to our work for the sake of completeness. The first is GLP-depth <ref type="bibr" target="#b16">[17]</ref> which proposes a hierarchical transformer encoder that captures global features and a simple decoder that considers the local context. The second method <ref type="bibr" target="#b37">[37]</ref> employs a neural window fully-connected Conditional Random Fields (CRFs) module for the decoder and a vision transformer for the encoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Background</head><p>AdaBins <ref type="bibr" target="#b3">[4]</ref>   to image), and reflects the global depth distribution for the input image. Each bin can have a different size and the predicted bin centers are closer to each other near more frequently occurring depth values. AdaBins employs an encoderdecoder architecture followed by a transformer based module to predict the adaptive bins. The final depth estimation is obtained by predicting the pixelwise probability distribution over the bins and computing an expectation over the predicted global bin centers. This can also be seen as expressing the final depth value as a linear combination of bin centers. The reader is referred to <ref type="bibr" target="#b3">[4]</ref> for more details.</p><p>We build upon the basic idea of AdaBins to estimate depth distributions, but we change the architecture design to incorporate two novel ideas. First, instead of predicting a single global depth bin-division, our architecture estimates a bin-division at every pixel, reflecting the depth distribution in the local neighborhood. Thus, the bin-divisions not only can vary from image to image (adaptiveness) but also from pixel to pixel (locality). Second, we do not use a transformer as a subsequent separate architecture block but integrate the depth prediction more tightly in the decoder. We utilize all the layers of the convolutional decoder to gradually refine the bin-division proposed for each pixel. The details of our architecture are described in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Architecture</head><p>Our architecture has two major components (see <ref type="figure" target="#fig_2">Fig. 2</ref>): 1) a standard encoderdecoder block and 2) our proposed LocalBins module.</p><p>Encoder-decoder We use the same encoder-decoder architecture as Ad-aBins to facilitate a fair comparison (EfficientNet-B5 with skip connections).</p><p>LocalBins module The LocalBins module uses the bottleneck features and the decoder features from the encoder-decoder block to estimate the local distribution of depth values at every pixel. As in AdaBins, the estimated distributions are encoded as the adaptive bin-divisions of the depth range interval, with the density of resulting bin-centers directly reflecting the density of the depth values in the local neighborhood. In practice, the bin-divisions are formulated as a vector of normalized bin-widths at every pixel from which the bin-centers can be easily obtained via Eq. 5. To estimate the bin-divisions at every pixel, we employ a coarse-to-fine binning strategy. Starting with N seed number of bins for every pixel at the bottleneck, the number of bins doubles at every decoder layer. The LocalBins module consists of three main types of layers: a) Bin embedding layers b) Seed bin width estimators c) Bin splitters. All these layers are composed of pointwise MLPs (a.k.a 1 ? 1 convolutional blocks) with two hidden layers with hidden dimension h. a) Bin embedding layers All the feature blocks input to the LocalBins module (the bottleneck features from the encoder and the decoder features of all scales) are first fed to bin embedding layers. These layers project the features of varying channel dimensionality into the same space (dim=128) that we refer to as the 'bin embedding space'. The further layers in the LocalBins module 'decide' the bin-division for each pixel based on their bin-embeddings. b) Seed bin width estimator This layer takes bin embeddings from the bottleneck as input and predicts N seed number of bins at each pixel of the bottleneck. This bin-division estimate is taken as the seed and subsequently each bin is divided into two bins for every subsequent decoder layer by the bin splitters (along with the spatial 2x interpolation). c) Bin splitters These pointwise MLPs are used to realize our coarse-tofine binning strategy. Loosely speaking, bin splitters 'decide' where to put more bin-centers for each pixel based on their bin-embeddings. As illustrated in the <ref type="figure">Fig. 3</ref>, bin-embeddings and bin-widths from the previous layer are first bilinearly upsampled to match the spatial resolution of the current layer. A bin splitter MLP at layer k, denoted as S k , takes as input the 'current' layer bin-embeddings after a residual connection with the upsampled previous layer bin-embeddings. The output is then used to split each bin-width from the previous layer into two. Specifically, let b ij ? R m be the normalized m-bin-widths at pixel location (i, j) (after 2x interpolation). Then, the new 2m-bin-widths b ? ij ? R 2m are given by:</p><formula xml:id="formula_0">? ij = ?(S k (e k?1 ij + e k ij )) (1) b ? ij = {? 0 ij b 0 ij , (1 ? ? 0 ij )b 0 ij , ? 1 ij b 1 ij , (1 ? ? 1 ij )b 1 ij , . . . , ? m ij b m ij , (1 ? ? m ij )b m ij } (2)</formula><p>where, ?(?) represents the splitter activation function that outputs values ? ? (0, 1), e k ij is the bin-embedding of the pixel at (i, j) at the k th layer and v a represent the components of a vector v.</p><p>We explore three designs of the splitter activation function ?(?), namely:  </p><formula xml:id="formula_1">?(x 1 , x 2 ) = x 1 x 1 + x 2 + ?<label>(3)</label></formula><p>where ? = 1e ?4 is used for numerical stability.</p><p>Refer to Sec. 5.3 for their comparison.</p><p>Since the number of bins doubles every layer, we have 2 n N seed bins at the end of an n-layer decoder. Therefore we use output channels = 2 n N seed for the last convolutional layer in the decoder. We then use the hybrid regression as in AdaBins, to obtain the final depth map. The difference is that now the bins change from pixel-to-pixel:</p><formula xml:id="formula_2">c(b k ij ) = d min + (d max ? d min )(b k ij /2 + k?1 s=1 b s ij ) (4) d ij = N k=1 c(b k ij )p k ij<label>(5)</label></formula><p>whered ij is the final estimated depth value, b ij and p ij are the bin-widths at the output layer and softmax scores respectively at location (i, j).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Training</head><p>Our network needs supervision in two forms. First, we need a pixel-wise loss (L pixel ) to provide supervision for the final estimated depth values. For this we use the Scale-Invariant Loss as used in recent works <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b19">20]</ref>. Second, we need to supervise our network such that the bin predictions at a pixel actually reflect the density of depth values in its local neighborhood.</p><p>A naive way would be to just choose a fixed-sized local window around every pixel (say a 5 ? 5 window) and directly impose the regularization on binpredictions at a pixel location such that they reflect the corresponding ground truth depth distributions within the window. However, there are two major problems with this approach: 1) It is not clear how one would choose the size of the window. Empirical determination is not a scalable solution as the amount of detail within a fixed sized window varies with the spatial resolution of the image. Alternatively, choosing different sizes simultaneously would lead to inconsistent regularization (e.g. bin predictions at the same pixel location would be compared to GT distributions within, say, 5 ? 5 and 7 ? 7 windows at the same time) 2) Chamfer loss, which is used in AdaBins to implement the distribution loss, is not computationally scalable. In AdaBins, it is computationally feasible because there is only one bin-division proposal per image. While in our case, we have a bin-division proposal at every pixel. This would mean we would have to compute Chamfer loss between the point sets at ?300K locations per image (= total number of pixels) at the highest resolution, which is not feasible in terms of memory or computation. One can, in practice, subsample the number of locations. For example, for a batch size of 16, we could fit around ?2% of randomly selected pixel locations on four NVIDIA A100 GPUs. However, as expected, this leads to inferior performance (Sec. 5.3).</p><p>One obvious reason is the significant loss of the spatial coverage of locations at which the loss is computed. We initially identified two possible workarounds for this problem. Either investigate efficient ways for subsampling or let the gradients from the loss computation at a given pixel location directly flow to neighboring regions to increase the coverage. This work focuses on designing the latter solution. To increase the coverage, we propose to involve all the pixel locations within the window to compute the loss (instead of just the center one), while keeping the loss computation feasible. This means that instead of regularizing the bin-predictions at individual pixel locations, we propose to regularize the bin-predictions of the entire local window together, potentially solving the coverage problem. We achieve this via the following formulation that we call 'Query-Response' training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Query-Response training We train the network via the following locality constraint regularization:</head><p>Consider a bounding box B at any given location in the input image. The LocalBins module, when applied on the spatial average of the features within B, must predict the density of depth values within B. This is illustrated in <ref type="figure" target="#fig_3">Fig. 4</ref>. By using bounding boxes of different sizes, we can enforce the features to contain the local distributional information at multiple scales. Furthermore, since the spatial averaging operation covers the entire window, we can potentially achieve the complete coverage with a relatively smaller number of bounding boxes per-image.</p><p>In order to implement such a regularization we make use of ROIAlign aggregation <ref type="bibr" target="#b13">[14]</ref>, a popular operation used in object detection pipelines. ROIAlign aggregation allows us to extract and pool the features at different layers (bottleneck and decoder features) corresponding to a given bounding box. Details on how ROIAlign works is found in <ref type="bibr" target="#b13">[14]</ref>.</p><p>Given a bounding box B (a.k.a query), we apply ROIAlign aggregation and use the pointwise MLPs (Bin embbeding, seed bin width estimator and bin splitter layers from the LocalBins module) on the pooled features to get the bins b(B) (a.k.a the response). The resulting bins b(B) are then 'forced' to match the ground truth depth distribution within that bounding box. We use the 1D bidirectional Chamfer loss as in <ref type="bibr" target="#b3">[4]</ref> as the matching loss: chamf er(b(B), Depth(B)).</p><p>"Foveated" Loss We now have a few choices on how to aggregate losses from different bounding boxes to calculate the final loss. We choose to have smaller bounding boxes to have more influence than the larger ones. We therefore use the loss weights that exponentially decay with the bounding box size. Suppose we generate N different sets of bounding boxes Q 1 , . . . , Q N with different box sizes such that size(B a ? Q i ) &lt; size(B b ? Q j ) ?a, b and i &lt; j, the total loss is given by:</p><formula xml:id="formula_3">L bins = n L=1 ? n?L l N k=1 ? k?1 b B?Q k chamf er(b L (B), Depth(B))<label>(6)</label></formula><p>where n is the number of layers and b L (B) is the response at layer L (running from bottleneck to output layer). We use ? l = ? b = 0.3 in our experiments and use 5 different sizes of bounding boxes as described below. In summary, we follow the steps: Finally, we define the total loss as:</p><formula xml:id="formula_4">L total = L pixel + ?L bins ,<label>(7)</label></formula><p>where we set ? = 0.02 in all our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RGB GT AdaBins</head><p>Ours AdaBins ? Ours ? <ref type="figure">Fig. 5</ref>. Qualitative results on iBims-1 benchmark without fine-tuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Implementation Details</head><p>We implement the model in PyTorch <ref type="bibr" target="#b24">[25]</ref>. We use hidden dimension h = 256 for the Seed bin width MLP and h = 128 for Bin embedding MLPs and Bin splitter MLPs. We train the network with batch size of 16 and use the AdamW <ref type="bibr" target="#b23">[24]</ref> optimizer with weight decay of 10 ?1 . We use a learning rate of 3.57 ? 10 ?4 which is decayed by a factor of 10 4 in the last 30% iterations using a cosine decay schedule. We train the models for 10 epochs in all our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments and Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Comparison to state-of-the-art</head><p>NYU-Depth V2. We use NYU-Depth V2 <ref type="bibr" target="#b28">[29]</ref> as the most important dataset for evaluation. <ref type="table">Table.</ref> 1 presents the performance comparison on the official test set of NYU-Depth V2. Our proposed model shows the state-of-the-art performance across all metrics, with ?4% reduction in the absolute relative error metric. Qualitatively, as shown in <ref type="figure">Fig. 7</ref>, our model is better at predicting depths of thin objects as well as planar surfaces. Note that our final model is able to beat the current published state-of-the-art model AdaBins while having the same encoder-decoder backbone. In total, our model has even fewer parameters since we do not use global attention or transformers. We attribute this performance improvement to the proposed LocalBins design and better utilization of the depth statistics via our novel training scheme.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Zero-shot performance</head><p>To evaluate the generalization performance of our model, we use the model pretrained on the NYU-Depth V2 dataset and evaluate it on other datasets without fine-tuning. LocalBins (Ours) 0.56 0.88 0.97 0.21 0.88 0.10 <ref type="table">Table 2</ref>. Quantitative results on the iBims benchmark without fine-tuning.</p><p>iBims-1 benchmark <ref type="bibr" target="#b17">[18]</ref> (independent Benchmark images and matched scans version 1) is a high quality RGB-D dataset acquired using a digital singlelens reflex (DSLR) camera and a high-precision laser scanner. <ref type="table">Table.</ref> 2 lists the performance on this benchmark, with our proposed model outperforming prior state-of-the-art methods. In addition, we show qualitative results in <ref type="figure">Fig. 5</ref>. AdaBins noticeably underestimates the depth range of the scenes with relative error growing with distance, whereas LocalBins more consistently predicts scale-accurate depths across varying depth ranges. This further emphasizes the generalization capability of our model. SUN-RGBD <ref type="bibr" target="#b29">[30]</ref> is an indoor dataset characterized by high scene diversity. We evaluate our model without fine-tuning on the official test set of 5050 images and report the results in <ref type="table">Table.</ref> 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Analysis and Ablation Studies</head><p>Here, we present the results of the extensive experiments we performed to analyse the properties and the importance of various components in our proposed model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>LocalBins module.</head><p>We first evaluate the importance of our LocalBins module. We remove the LocalBins module from the network and evaluate our base encoder-decoder architecture. We use the pixel-wise loss (L pixel ) to train the network. We also evaluate our proposed model (with LocalBins module) without the Chamfer loss to study the capacity of our design in absence of extra supervision. Results are reported in <ref type="table">Table.</ref> 3.</p><p>Query-Response training and Foveated loss. We evaluate our proposed 'Query-Response' training scheme against the naive implementation discussed in Sec. 3.2. As listed in <ref type="table">Table.</ref> 3, Query-Response training gives a significant boost to the performance, improving the absolute relative error by ?7%. We believe the Query-Response training scheme is a general, powerful regularization technique that can be directly used in tasks beyond depth estimation. The foveated weighting scheme further improves the squared error based metrics.</p><p>In order to demonstrate the power of Query-Response training, we compare it with the Naive scheme in terms of computation of Chamfer loss against the coverage (total number of pixel locations involved in loss computation -higher the better). We define 'PSCI' as the total number of Point Set Comparisons performed per Image and use it as an indicator of computational efficiency. Note that in practice, the computations are batched, and PSCI indicates the total number of 'samples' in the batch contributed per image. For the Naive scheme, PSCI is equal to the total number of subsampled locations. For the Query-Response training scheme, PSCI is equal to the total number of bounding box queries per image. We take random bounding boxes of sizes {3 ? 3, 7 ? 7, 15 ? 15, 31 ? 31, 63 ? 63} and compute their average total area. The results are given in <ref type="table">Table.</ref> 4. Our proposed training scheme performs ?7.6% better with 8? fewer point set comparisons compared to the naive scheme.</p><p>Splitter activation function. We evaluate the three types of splitter activation functions as discussed in Sec. 3.2. The results are given in <ref type="table">Table.</ref> 6.</p><p>Effect of N seed . We analyse the effect of varying the N seed and hence the total number of bins, and compare with AdaBins. The results are plotted in <ref type="figure" target="#fig_5">Fig. 6</ref>. We find that our model is more robust to the total number of bins used and generally has better performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>We introduced a new network architecture for depth estimation from a single image. We build on an encoder-decoder architecture and evolve the current stateof-the-art model AdaBins in two aspects. First, we add three building blocks to estimate local depth distributions in the neighborhood of a pixel. These three building blocks, bin embedding layers, seed bin width estimator, and bin splitters are tightly integrated with the decoder in a multi-scale fashion. Second, we propose a query -response acceleration strategy for training, since a naive implementation of the idea would be highly time and memory consuming. In future work, we would like to adapt the LocalBins concept to other dense regression algorithms, such as image segmentation or inpainting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RGB GT AdaBins [4]</head><p>Ours AdaBins ? Ours ? <ref type="figure">Fig. 7</ref>. Qualitative results on NYU-Depth V2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Appendix</head><p>A.1 Visualizing bin predictions  Our main goal in this work was to have the LocalBins module predict the local depth distribution at each pixel. However, in Query-Response training, windows of various sizes are generated, and regularization is imposed on the bin predictions of the entire window together rather than for each individual pixel location separately. While being vital (refer to 'Query-Response training' section in the main paper), this renders the 'size' of the 'local neighborhood' rather indeterministic. The model may either end up using a fixed size and always predict the bins at each pixel that reflect the density of depth values within a fixed-sized neighborhood, or the model may as well choose to cover different sized neighborhoods depending upon the context. Visualizations (See <ref type="figure" target="#fig_8">Fig. 9</ref>) indicate the latter case.  We switch the EfficientNet-b5 encoder in our default model with various other backbones and list the performance in <ref type="table" target="#tab_6">Table. 7</ref> </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 .</head><label>1</label><figDesc>Illustration of global adaptive bins vs local adaptive bins. While AdaBins predicts a depth distribution for a complete image, LocalBins predicts a depth distribution for the neighborhood of each pixel.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>Architecture overview. LocalBins module contains pixel-wise operations and estimate local neighborhood bin density for each pixel location.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Illustration of Query-Response training. See Sec 3.3 for more details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>1 .</head><label>1</label><figDesc>Constant splitter: ?(x) = 0.5 ?x, that divides a bin in half irrespective of the splitter MLP output (and indirectly the bin-embeddings). 2. Sigmoid splitter: where ?(?) represents the sigmoid activation function. 3. Linear norm splitter: In this case, we let the splitter MLP S k output two positive values (x 1 , x 2 ) (via ReLU) for each bin. Then the linear norm split is given by:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 .</head><label>6</label><figDesc>REL vs #Bins.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 8 .</head><label>8</label><figDesc>Visualization of bin predictions, showing input RGB (first row) and density plots of local depth distributions at selected locations across the depth range interval. Density plots consisting of density of bin centers predicted by LocalBins module at the selected pixel location (top) and density of ground truth depth values (bottom) for various window sizes (indicated by the colorbar).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 8</head><label>8</label><figDesc>shows a qualitative comparison of the bin predictions of the LocalBins module against the ground truth depth distribution in the local neighborhoods of various sizes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 9 .</head><label>9</label><figDesc>How "local" are the bin predictions of the LocalBins module? Plots show the average absolute difference between the ground truth depth values and their nearest bin centers predicted by the LocalBins module. Top row shows the average differences plotted for 100 random locations for four randomly selected input images (columns). GT depth values are taken from the 'concentric' bounding boxes and compared against the bins predicted at the center. Bottom row shows the mean across the 100 locations for each image. 0.986 0.997 0.098 0.351 0.042</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>1 .</head><label>1</label><figDesc>Generate five sets of random bounding boxes of sizes, 3 ? 3, 7 ? 7, 15 ? 15, 31 ? 31, 63 ? 63, each containing M = 200 boxes. 2. Extract the depth values from the ground truth depth map corresponding to these bboxes to use in the Chamfer loss calculation. 3. Use ROIAlign with average pooling to get the corresponding pooled features at the bottleneck and decoder layers. 4. Use bin embedding MLPs to get the corresponding bin-embeddings. At this stage, we have one bin-embedding vector at each layer corresponding to each bbox. 5. Use seed bin estimator and bin splitter MLPs as usual to get the resulting bins. 6. Calculate the unweighted Chamfer loss for the predicted bins against their ground truth (step 2). 7. Calculate the weights and compute the weighted sum to get the final Chamfer loss, with the weighting scheme in Eq. 6.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 .Table 4 .</head><label>34</label><figDesc>Enc-Dec LBM Naive QR Lbins Foveated REL RMS Ablation experiments showing the importance of various components in our proposed model. Enc-Dec: Base encoder-decoder model, LBM: LocalBins module, Naive: Naive training strategy discussed in Sec. 3.2, QR: Query-Response training, L bins : Chamfer loss supervision, Foveated: Foveated weighting in Chamfer loss. Data in the 'Naive' column indicates the bbox size used on GT to compute density. Quantitative demonstration of the efficiency of Query-Response Training. PSCI: Point Set Comparisons per Image. #px: Total number of pixels covered for loss computation. Coverage: Percentage of #px with respect to image resolution.</figDesc><table><row><cell>1</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>0.111 0.419</cell></row><row><cell>2</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>0.106 0.375</cell></row><row><cell>3</cell><cell>?</cell><cell>?</cell><cell cols="2">3 ? 3 ?</cell><cell>?</cell><cell>?</cell><cell>0.108 0.381</cell></row><row><cell>4</cell><cell>?</cell><cell>?</cell><cell cols="2">5 ? 5 ?</cell><cell>?</cell><cell>?</cell><cell>0.107 0.375</cell></row><row><cell>5</cell><cell>?</cell><cell cols="3">? 15 ? 15 ?</cell><cell>?</cell><cell>?</cell><cell>0.108 0.377</cell></row><row><cell>8</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>0.099 0.364</cell></row><row><cell>9</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>0.099 0.357</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="5">PSCI #px Coverage (%) ? REL ?</cell></row><row><cell></cell><cell>Naive</cell><cell></cell><cell cols="2">4096 4096 8192 8192</cell><cell></cell><cell>1.33 2.67</cell><cell>0.1075 0.1071</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">250 52,130</cell><cell></cell><cell>16.97</cell><cell>0.1043</cell></row><row><cell></cell><cell cols="2">Query-Response</cell><cell cols="2">500 104,260</cell><cell></cell><cell>33.94</cell><cell>0.1002</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">1000 208,520</cell><cell></cell><cell>67.88</cell><cell>0.0992</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 .</head><label>6</label><figDesc>Types of splitters.</figDesc><table><row><cell>Method</cell><cell cols="2">?1 ? ?2 ? ?3 ? REL? RMS? log10 ?</cell></row><row><cell>Chen [6]</cell><cell cols="2">0.757 0.943 0.984 0.166 0.494 0.071</cell></row><row><cell>Yin [36]</cell><cell cols="2">0.696 0.912 0.973 0.183 0.541 0.082</cell></row><row><cell>BTS [20]</cell><cell cols="2">0.740 0.933 0.980 0.172 0.515 0.075</cell></row><row><cell cols="3">AdaBins [4] 0.771 0.944 0.983 0.159 0.476 0.068</cell></row><row><cell>Ours</cell><cell cols="2">0.777 0.949 0.985 0.156 0.470 0.067</cell></row><row><cell cols="3">Table 5. Results of models trained on the NYU-Depth V2 dataset and tested on the</cell></row><row><cell cols="2">SUN RGB-D dataset [30] without fine-tuning.</cell></row><row><cell></cell><cell cols="2">Splitter activation REL RMS</cell></row><row><cell></cell><cell>Constant</cell><cell>0.117 0.454</cell></row><row><cell></cell><cell>Sigmoid</cell><cell>0.100 0.361</cell></row><row><cell></cell><cell>Linear norm</cell><cell>0.099 0.364</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 .</head><label>7</label><figDesc>Performance of LocalBins with various backbone encoders</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://github.com/shariqfarooq123/LocalBins</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Self-supervised learning of domain invariant features for depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Akada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">F</forename><surname>Bhat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Alhashim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wonka</surname></persName>
		</author>
		<idno type="DOI">10.1109/WACV51458.2022.00107</idno>
		<ptr target="https://doi.org/10.1109/WACV51458.2022.00107" />
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Winter Conference on Applications of Computer Vision, WACV 2022</title>
		<meeting><address><addrLine>Waikoloa, HI, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2022" />
			<biblScope unit="page" from="997" to="1007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">High quality monocular depth estimation via transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Alhashim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wonka</surname></persName>
		</author>
		<idno>abs/1812.11941</idno>
		<ptr target="http://arxiv.org/abs/1812.11941" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Real-time monocular depth estimation using synthetic data with domain adaptation via image style transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Atapour-Abarghouei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">P</forename><surname>Breckon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2800" to="2810" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Adabins: Depth estimation using adaptive bins</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">F</forename><surname>Bhat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Alhashim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wonka</surname></persName>
		</author>
		<idno type="DOI">https:/doi.ieeecomputersociety.org/10.1109/CVPR46437.2021.00400</idno>
		<ptr target="https://doi.ieeecomputersociety.org/10.1109/CVPR46437.2021.00400" />
	</analytic>
	<monogr>
		<title level="m">2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting><address><addrLine>Los Alamitos, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2021-06" />
			<biblScope unit="page" from="4008" to="4017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Unsupervised monocular depth and ego-motion learning with structure and semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Casser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pirk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mahjourian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Angelova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshop on Visual Odometry and Computer Vision Applications Based on Location Cues</title>
		<imprint>
			<publisher>VOC-VALC</publisher>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Structure-aware residual pyramid network for monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">J</forename><surname>Zha</surname></persName>
		</author>
		<idno type="DOI">10.24963/ijcai.2019/98</idno>
		<ptr target="https://doi.org/10.24963/ijcai.2019/98" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence, IJCAI-19</title>
		<meeting>the Twenty-Eighth International Joint Conference on Artificial Intelligence, IJCAI-19</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="694" to="700" />
		</imprint>
	</monogr>
	<note>International Joint Conferences on Artificial Intelligence Organization</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Crdoco: Pixel-level domain transfer with cross-domain consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Depth map prediction from a single image using a multi-scale deep network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Puhrsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>NIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep ordinal regression network for monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Batmanghelich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2002" to="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Unsupervised monocular depth estimation with left-right consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Godard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">M</forename><surname>Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Brostow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6602" to="6611" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Digging into self-supervised monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Godard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">M</forename><surname>Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Brostow</surname></persName>
		</author>
		<idno>abs/1806.01260</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Depth from videos in the wild: Unsupervised monocular depth learning from unknown cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jonschkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Angelova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Detail preserving depth estimation from a single image using attention guided networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on 3D Vision (3DV)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="304" to="313" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCV.2017.322</idno>
		<ptr target="https://doi.org/10.1109/ICCV.2017.322" />
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Revisiting single image depth estimation: Toward higher resolution maps with accurate object boundaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ozay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Okatani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<imprint>
			<biblScope unit="page" from="1043" to="1051" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Guiding monocular depth estimation using depth-attention volume</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Huynh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Nguyen-Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rahtu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Heikkila</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.02760</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Global-local path networks for monocular depth estimation with vertical cutdepth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.07436</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Evaluation of cnn-based singleimage depth estimation methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liebel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Fraundorfer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>K?rner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings ECCV 2018 Workshops</title>
		<meeting>ECCV 2018 Workshops</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deeper depth prediction with fully convolutional residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rupprecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Belagiannis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tombari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Fourth International Conference on 3D Vision (3DV)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="239" to="248" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">From big to small: Multi-scale local planar guidance for monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">K</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">W</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">H</forename><surname>Suh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.10326</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Woo</surname></persName>
		</author>
		<title level="m">Depth-assisted real-time 3d object detection for augmented reality. ICAT&apos;11</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="126" to="132" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Unsupervised monocular depth learning in dynamic scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Casser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Angelova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.16404</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Megadepth: Learning single-view depth prediction from internet photos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Snavely</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=Bkg6RiCqY7" />
	</analytic>
	<monogr>
		<title level="m">7th International Conference on Learning Representations</title>
		<meeting><address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-05-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2019/file/bdbca288fee7f92f2bfa9f7012727740-Paper.pdf" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>Wallach, H., Larochelle, H., Beygelzimer, A., d&apos;Alch?-Buc, F., Fox, E., Garnett, R.</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="8026" to="8037" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Sharpnet: Fast and accurate recovery of occluding contours in monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ramamonjisoa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) Workshops</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV) Workshops</meeting>
		<imprint>
			<date type="published" when="2019-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Vision transformers for dense prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ranftl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bochkovskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2021-10" />
			<biblScope unit="page" from="12179" to="12188" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Towards robust monocular depth estimation: Mixing datasets for zero-shot cross-dataset transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ranftl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lasinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hafner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2020" />
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Indoor segmentation and support inference from rgbd images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2012</title>
		<imprint>
			<biblScope unit="page" from="746" to="760" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Sun rgb-d: A rgb-d scene understanding benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">P</forename><surname>Lichtenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2015.7298655</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2015.7298655" />
	</analytic>
	<monogr>
		<title level="m">2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="567" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Unsupervised domain adaptation for depth prediction from images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tonioni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Poggi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mattoccia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Di Stefano</surname></persName>
		</author>
		<idno>abs/1909.03943</idno>
		<ptr target="http://arxiv.org/abs/1909.03943" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">The temporal opportunist: Self-supervised multi-frame monocular depth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Watson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Mac Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Prisacariu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Brostow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Firman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2021-06" />
			<biblScope unit="page" from="1164" to="1174" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Deep3d: Fully automatic 2d-to-3d video conversion with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Multi-scale continuous crfs as sequential deep networks for monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ricci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5354" to="5362" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Structured attention guided convolutional neural fields for monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ricci</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3917" to="3925" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Enforcing geometric constraints of virtual normal for depth prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.01502</idno>
		<title level="m">NeW CRFs: Neural Window Fullyconnected CRFs for Monocular Depth Estimation</title>
		<imprint>
			<date type="published" when="2022-03" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Geometry-aware symmetric domain adaptation for monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9788" to="9798" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Domain decluttering: Simplifying images to mitigate synthetic-real domain shift and improve depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fowlkes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Self-supervised monocular depth estimation with internal feature fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Greenwood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference (BMVC)</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Unsupervised learning of depth and ego-motion from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Snavely</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6612" to="6619" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
