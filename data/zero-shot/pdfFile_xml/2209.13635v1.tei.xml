<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Rethinking Clustering-Based Pseudo-Labeling for Unsupervised Meta-Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-09-27">27 Sep 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Inception Institute of Artificial Intelligence</orgName>
								<address>
									<settlement>Abu Dhabi</settlement>
									<country key="AE">UAE</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">SKL-IOTSC, Computer and Information Science</orgName>
								<orgName type="institution">University of Macau</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="laboratory">Terminus Group</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Rethinking Clustering-Based Pseudo-Labeling for Unsupervised Meta-Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-09-27">27 Sep 2022</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T15:23+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Meta-learning</term>
					<term>Unsupervised learning</term>
					<term>Clustering-friendly ? Corresponding author: Jianbing Shen</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The pioneering method for unsupervised meta-learning, CAC-TUs, is a clustering-based approach with pseudo-labeling. This approach is model-agnostic and can be combined with supervised algorithms to learn from unlabeled data. However, it often suffers from label inconsistency or limited diversity, which leads to poor performance. In this work, we prove that the core reason for this is lack of a clusteringfriendly property in the embedding space. We address this by minimizing the inter-to intra-class similarity ratio to provide clusteringfriendly embedding features, and validate our approach through comprehensive experiments. Note that, despite only utilizing a simple clustering algorithm (k-means) in our embedding space to obtain the pseudolabels, we achieve significant improvement. Moreover, we adopt a progressive evaluation mechanism to obtain more diverse samples in order to further alleviate the limited diversity problem. Finally, our approach is also model-agnostic and can easily be integrated into existing supervised methods. To demonstrate its generalization ability, we integrate it into two representative algorithms: MAML and EP. The results on three main few-shot benchmarks clearly show that the proposed method achieves significant improvement compared to state-of-the-art models. Notably, our approach also outperforms the corresponding supervised method in two tasks. The code and models are available at https://github.com/xingpingdong/PL-CFE.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Recently, few-shot learning has attracted increasing attention in the machine learning and computer vision communities <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b41">42]</ref>. It is also commonly used to evaluate meta-learning approaches <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b15">16]</ref>. However, most of the existing literature focuses on the supervised few-shot classification task, which is built upon datasets with human-specified labels. Thus, most previous works cannot ...</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Label inconsistency</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Support set</head><p>Query set Diverse Samples ... (a) Images (b) PCA Space (c) Clusters (d) N-ways-K-shots task <ref type="figure">Fig. 1</ref>. Illustration of the label inconsistency and limited diversity issues in the clustering-based CACTUs <ref type="bibr" target="#b24">[25]</ref>. (a) The unlabeled images. (b) The 2D mapping space of the embedding features, generated via principal component analysis (PCA). Each mark (or color) represents one class, and the larger marks are the class centers (i.e. the average features in a class). (c) The noisy clustering labels generated by k-means, with many inconsistent or noisy samples in each cluster. For example, we use the green class label for cluster 3, since cluster 3 has the most green-samples. Thus, the samples with inconsistent labels, like the orange samples in cluster 3, are regarded as noisy samples. Further, in the unsupervised setting, the green samples in other regions (cluster 1 and cluster 2) cannot be used to build few-shot tasks. This leads to the unsupervised few-shot tasks being less diverse than the supervised tasks. We term this issue limited diversity and refer to these green samples as diverse samples. (d) The label inconsistency and limited diversity problems in the few-shot task, which are caused by the noisy clustering labels, as shown in (c). The red box highlights an incorrect sample in the few-shot task, and the green box illustrates the limited diversity problem, i.e., the diverse samples cannot be used to build few-shot tasks. make use of the rapidly increasing amount of unlabeled data from, for example, the internet.</p><p>To solve this issue, the pioneering work CACTUs <ref type="bibr" target="#b24">[25]</ref>, was introduced to automatically construct the few-shot learning tasks by assigning pseudo-labels to the samples from unlabeled datasets. This approach partitions the samples into several clusters using a clustering algorithm (k-means) on their embedding features, which can be extracted by various unsupervised methods <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b14">15]</ref>. Unlabeled samples in the same clusters are assigned the same pseudo-labels. Then, the supervised meta-learning methods, MAML <ref type="bibr" target="#b15">[16]</ref> and ProtoNets <ref type="bibr" target="#b45">[46]</ref>, are applied to the few-shot tasks generated by this pseudo-labeled dataset. It is worth mentioning that CACTUs is model-agnostic and any other supervised methods can be used in this framework.</p><p>However, this approach based on clustering and pseudo-label generation suffers from label inconsistency, i.e. samples with the same pseudo-label may have different human-specified labels in the few-shot tasks (e.g. <ref type="figure">Fig. 1(d)</ref>). This is caused by noisy clustering labels. Specifically, as shown in <ref type="figure">Fig. 1(c)</ref>, several samples with different human-specified labels are partitioned into the same cluster, which leads to many noisy samples (with different labels) in the few-shot tasks. This is one reason for performance degeneration. Besides, noisy clustering labels will also partition samples with the same label into different clusters, which results in a lack of diversity for the few-shot tasks based on pseudo-labels compared with supervised methods. As shown in <ref type="figure">Fig. 1</ref> (c)(d), CACTUs ignores the diversity among partitioned samples with the same human label. This is termed the limited diversity problem. How to utilize the diversity is thus one critical issue for performance improvement.</p><p>To overcome the above problems, we first analyze the underlying reasons for the noisy clustering labels in depth, via a qualitative and quantitative comparison bewteen the embedding features of CACTUs. As shown in <ref type="figure">Fig. 1(b)</ref>, we find that the embedding features extracted by unsupervised methods are not clustering-friendly. In other words, most samples are far away from their class centers, and different class centers are close to each other in the embedding space. Thus, a standard clustering algorithms cannot effectively partition samples from the same class into one cluster, leading to noisy clustering labels. For example, cluster 3 in <ref type="figure">Fig. 1</ref>(c) contains many noisy samples from different classes. Furthermore, we propose an inter-to intra-class similarity ratio to measure the clustering-friendly property. In the quantitative comparison, we observe that the accuracy of the few-shot task is inversely proportional to the similarity ratio. This indicates that reducing the similarity ratio is critical for performance improvement.</p><p>According to these observations, a novel pseudo-labeling framework based on a clustering-friendly feature embedding (PL-CFE) is proposed to construct few-shot tasks on unlabeled datasets, in order to alleviate the label inconsistency and limited diversity problems. Firstly, we introduce a new unsupervised training method to extract clustering-friendly embedding features. Since the similarity ratio can only be applied to labeled datasets, we simulate a labeled set via data augmentation and try to minimize the similarity ratio on this to provide a clustering-friendly embedding function. Given our embedding features, we can run k-means to generate several clusters for pseudo-labeling and build clean fewshot tasks, reducing both the label inconsistency and limited diversity problems. Secondly, we present a progressive evaluation mechanism to obtain more divisive samples and further alleviate the limited diversity, by utilizing additional clusters for the task construction. Specifically, for each cluster, which we call a base cluster, in the task construction, we choose its k-nearest clusters as candidates. We use an evaluation model based on previous meta-learning models to measure the entropy of the candidates, and select the one with the highest entropy as the additional cluster for building a hard task, as it contains newer information for the current meta-learning model.</p><p>To evaluate the effectiveness of the proposed PL-CFE, we incorporate it into two representative supervised meta-learning methods: MAML <ref type="bibr" target="#b24">[25]</ref> and EP <ref type="bibr" target="#b41">[42]</ref>, termed as PL-CFE-MAML and PL-CFE-EP, respectively. We conduct extensive experiments on Omniglot <ref type="bibr" target="#b29">[30]</ref>, mini ImageNet <ref type="bibr" target="#b38">[39]</ref>, tiered ImageNet <ref type="bibr" target="#b40">[41]</ref>. The results demonstrate that our approach achieves significant improvement compared with state-of-the-art model-agnostic unsupervised meta-learning methods. In particular, our PL-CFE-MAML outperforms the corresponding supervised MAML method on mini ImageNet in both the 5-ways-20-shots and 5-ways-50shots tasks. Notably, we achieve a gain of 1.75% in the latter task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Meta-Learning for Few-Shot Classification</head><p>Meta-learning, whose inception goes as far back as the 1980s <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b43">44]</ref>, is usually interpolated as fast weights <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b2">3]</ref>, or learning-to-learn <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr">1]</ref>. Recent meta-learning methods can be roughly split into three main categories. The first kind of approaches is metric-based <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b6">7]</ref>, which attempt to learn discriminative similarity metrics to distinguish samples from the same class. The second kind of approaches are memory-based <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b38">39]</ref>, investigating the storing of key training examples with effective memory architectures or encoding fast adaptation methods. The last kind of approaches are optimization-based methods <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17]</ref>, searching for adaptive initialization parameters, which can be quickly adjusted for new tasks. Most meta-learning methods are evaluated under supervised few-shot classification, which requires a large number of manual labels. In this paper, we explore a new approach of few-shot task construction for unsupervised meta-learning to reduce this requirement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Unsupervised Meta-Learning</head><p>Unsupervised learning aims to learn previously unknown patterns in a dataset without manual labels. These patterns learned during unsupervised pre-training can be used to more efficiently learn various downstream tasks, which is one of the more practical applications <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b14">15]</ref>. Unsupervised pre-training has achieved significant success in several fields, such as image classification <ref type="bibr" target="#b54">[55,</ref><ref type="bibr" target="#b18">19]</ref>, speech recognition <ref type="bibr" target="#b53">[54]</ref>, machine translation <ref type="bibr" target="#b36">[37]</ref>, and text classification <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b35">36]</ref>.</p><p>Hsu et al. <ref type="bibr" target="#b24">[25]</ref> proposed an unsupervised meta-learning method based on clustering, named CACTUs, to explicitly extract effective patterns from small amounts of data for a variety of tasks. However, as mentioned, this method suffers from the label inconsistency and limited diversity problems, which our approach can significantly reduce. Subsequently, Khodadadeh et al. <ref type="bibr" target="#b26">[27]</ref> proposed the UM-TRA method to build synthetic training tasks in the meta-learning phase, by using random sampling and augmentation. Stemming from this, several works have been introduced to synthesize more generative and divisive meta-training tasks via various techniques, such as introducing a distribution shift between the support and query set <ref type="bibr" target="#b34">[35]</ref>, using latent space interpolation in generative models to generate divisive samples <ref type="bibr" target="#b27">[28]</ref>, and utilizing a variational autoencoder with a mixture of Gaussian for producing meta tasks and training <ref type="bibr" target="#b30">[31]</ref>. Compared to these synthetic methods, our approach can obtain harder few-shot tasks.Specifically, these previous methods only utilize the differences between augmented samples to increase the diversity of tasks, while our method can use the differences inside the class. Besides, some researchers explore to apply clustering methods for meta-training, by prototypical transfer learning <ref type="bibr" target="#b32">[33]</ref> or progressive clustering <ref type="bibr" target="#b25">[26]</ref>. These methods are not model-agnostic, while our approach can be incorporated into any supervised meta-learning method. This will bridge the unsupervised and supervised methods in meta-learning.  <ref type="bibr" target="#b10">[11]</ref>, ACAI <ref type="bibr" target="#b4">[5]</ref>, and our clustering-friendly embedding (CFE) features. We randomly select three classes from Omniglot <ref type="bibr" target="#b29">[30]</ref> and map the embeddings of their samples in 2D space via principal component analysis (PCA). Each color represents one class, and large circles are the class centers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">In-Depth Analysis of Clustering-Based Unsupervised Methods</head><p>CACTUs <ref type="bibr" target="#b24">[25]</ref> is a clustering-based and model-agnostic algorithm, which is easily incorporated into supervised methods for the unsupervised meta-learning task. However, there is still a large gap in performance between CACTUs and the supervised methods. We believe that the main reason is that the unsupervised embedding algorithms, such as InfoGAN <ref type="bibr" target="#b7">[8]</ref>, BiGAN <ref type="bibr" target="#b10">[11]</ref>, ACAI <ref type="bibr" target="#b4">[5]</ref> and DC <ref type="bibr" target="#b5">[6]</ref>, in CACTUs are not suitable for the clustering task, which is a core step of CACTUs. This is because these unsupervised methods were originally designed for the pre-training stage, where the extracted features can be further fine-tuned in the downstream tasks. Thus, they do not need to construct a clusteringfriendly feature space, where the samples in the same class are close to their class center and each class center is far away from the samples in other classes. However, the clustering-friendly property is very important for CACTUs, since it directly clusters the embedding features without fine-tuning. We first provide an intuitive analysis by visualizing the embedding features. We collect samples from three randomly selected classes of a dataset and map them into 2D space with principal component analysis (PCA). For example, we observe the BiGAN and ACAI embedding features on Omniglot <ref type="bibr" target="#b29">[30]</ref>. As shown in <ref type="figure" target="#fig_1">Fig. 2</ref>(a)(b), many samples are far away from their class centers, and the class centers are close to each other. Thus, we can see that many samples in different classes are close to each other. These samples are difficult for a simple clustering method to partition.</p><p>In order to observe the embedding features in the whole dataset, we define two metrics, intra-similarity and inter-similarity, to measure the clustering performance. We define the intra-similarity for each class as follows:</p><formula xml:id="formula_0">s intra i = exp( Ns j=1 ?i ? zij/(? N )),<label>(1)</label></formula><p>where z ij is the embedding feature of the j-th sample in class i, ? i = 1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ns</head><p>Ns j=1 z ij is the class center, ? is the dot product, N s is the number of samples in a class,  <ref type="table">Table 1</ref>. Illustrations of the relationship between similarity ratio R and classification accuracy (Acc). We present the average intra-similarity s intra , intersimilarity s inter , and similarity ratio R of different embedding methods on Omniglot <ref type="bibr" target="#b29">[30]</ref>(Omni) and miniImageNet <ref type="bibr" target="#b38">[39]</ref>(Mini). We also report the accuracy of MAML <ref type="bibr" target="#b15">[16]</ref> based on these embeddings for the 5-ways-1-shot task. All Acc values except ours are sourced from CACTUs <ref type="bibr" target="#b24">[25]</ref>.</p><formula xml:id="formula_1">Embedding Dataset s inter ? s intra ? R ? Acc (%) ? BIGAN [11]</formula><p>and ? is a temperature hyperparameter <ref type="bibr" target="#b52">[53]</ref>. The intra-similarity is the average similarity between the samples and their class center. It is used to evaluate the compactness of a class in the embedding space. A large value indicates that the class is compact and most samples are close to the class center. The other metric, inter-similarity, is defined as follows:</p><formula xml:id="formula_2">s inter ij = exp(?i ? ?j/? ), j ? = i.<label>(2)</label></formula><p>A low value of s inter ij indicates that two classes are far away from each other. To combine the above two similarities, we use the inter-to intra-class similarity ratio r ij = s inter ij /s intra i to represent the clustering performance. Finally, the average similarity ratio R over the whole dataset is denoted as:</p><formula xml:id="formula_3">R = 1 C C i=1 C j? =i s inter ij (C ? 1)s intra i ,<label>(3)</label></formula><p>where C is the number of classes. The lower the value of R, the better the clustering performance. In addition to R, we also calculate the average intra-similarity s intra = s intra i /C and average inter-similarity s inter = s inter ij /(N s C) for complete analysis, where N s is the samples' number of a class.</p><p>As shown in <ref type="table">Table 1</ref>, we apply the above three criteria, R, s intra and s inter , to the different embedding features (BiGAN <ref type="bibr" target="#b10">[11]</ref> and ACAI <ref type="bibr" target="#b4">[5]</ref>) in the Omniglot <ref type="bibr" target="#b29">[30]</ref> dataset, and report the accuracy on the 5-ways-1-shot meta-task with the CACTUs-MAML <ref type="bibr" target="#b24">[25]</ref> method. We find that the accuracy is inversely proportional to the similarity ratio R, but has no explicit relationship with the individual similarities s intra or s inter . This indicates that minimizing R is critical for better accuracy.</p><p>Therefore, we propose a novel clustering-friendly embedding method (CFE) to reduce the similarity ratio in the following subsection ?4.1. In the visualization comparison of <ref type="figure" target="#fig_1">Fig. 2</ref>, the proposed method provides more compact classes than the previous BiGAN and ACAI. As shown in <ref type="table">Table 1</ref>, our CFE approach also has significantly reduced R on both the Omniglot and mini ImageNet. These results indicate that our method is more clustering-friendly. To investigate the relationship between the clustering-friendly property and the accuracy on the meta-task, </p><formula xml:id="formula_4">C1N - C12 - C11 - C2N - C22 - C21 - CKN - CK2 - CK1 -- - - * * * C1 -C2 - CN - .</formula><p>..</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pick Clusters</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Selecting</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>K-Nearest Randomly Select</head><p>Momentum Update (a) Clustering-Friendly Feature Embedding (b) Progressive Evaluation Mechanism <ref type="figure">Fig. 3</ref>. The frameworks of our clustering-friendly feature embedding and progressive evaluation mechanism. (a) The embedding feature extraction.</p><p>We first augment the samples in one mini-batch and feed them into the asynchronous embedding models to build a positive set. We also use the negative set storing the historical embedding features to obtain more varied features. Then we update our main embedding model (green) with backpropagation (green arrows), by minimizing the similarity ratio between the average features (purple ellipses), and the features in the positive and negative sets. The historical encoder is momentum updated with the main model. (b) The progressive evaluation mechanism. We randomly select N clusters as base clusters (Ci), and choose theK-nearest neighbors as the candidates. Then, we use an evaluation model to select the clusters (C * i ) with the highest entropy and filter out noisy samples.</p><p>we firstly use k-means to split the samples into different clusters, and assign the same pseudo-label to samples in one cluster. Then we run the supervised MAML method on the 5-ways-1-shot task constructed by pseudo-labeled samples. Compared with CACTUs, which is based on previous embedding methods, our approach achieves significant improvement on both Omniglot and mini ImageNet. Notably, the gain on Omniglot is more than 20%. The results clearly support our claim: a clustering-friendly embedding space can benefit clustering-based unsupervised meta-learning methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Our Approach</head><p>We propose a novel clustering-based pseudo-labeling for unsupervised metalearning, which includes a clustering-friendly feature embedding and a progressive evaluation mechanism. <ref type="figure">Fig. 3</ref> shows the frameworks of our approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Clustering-Friendly Feature Embedding</head><p>Optimization Objective. We aim to learn a feature extraction function to map each sample to a clustering-friendly embedding space (with low similarity ratio). In this space, samples with the same class label are close to their class center, and each class center is far away from samples with different class labels.</p><p>We can assign most samples from the same class into the same cluster, even with a simple clustering algorithm, such as k-means.</p><p>In the unsupervised setting, we do not have access to the class labels. Thus, we need to simulate a labeled dataset. To do so, we first randomly select N p ? N all samples from the unlabeled dataset D to build a positive set D p , where N all is the size of D. For each sample x i ? D p , we produce N a augmented samples {x + ij , j ? [1, N a ]} via data augmentation methods, such as random color jittering, random horizontal flipping, and random grayscale conversion. Samples augmented from the same original sample are regarded as belonging to the same class. To involve more samples for training, we also construct a negative set by randomly selecting N n samples from D, without including the N p positive samples. We augment each sample once to obtain the final negative set {x ? k , k ? [1, N n ]}. Given the positive ('labeled') set and negative set, we can reformulate the intra-similarity in Eq. (1) and inter-similarity in Eq. (2), for the final optimization objective. Maximizing the former will force the samples to be close to their class center, while minimizing the latter will pull the class centers far away from the other class center and negative samples.</p><p>We rewrite the intra-similarity for each class:</p><formula xml:id="formula_5">s intra i = exp( Na j=1 ?i ? z + ij /(? Na)),<label>(4)</label></formula><p>where z + ij = ?(x + ij ; ?) is the embedding feature and ? i = 1 Na Na j z + ij is the class center. ? represents the embedding function and ? is its parameter. This embedding function can be any learnable module, such as a convolutional network. The rewritten inter-similarity includes a negative-similarity measuring the similarity between the class center and negative sample, and a center-similarity calculating the similarity bewteen the class centers. The formulations are as follows:</p><formula xml:id="formula_6">s neg ik = exp(?i ? z ? k /? ), s cen ij = exp(?i ? ?j/? ),<label>(5)</label></formula><p>where z ? k = ?(x ? k ; ?) is the negative embedding feature. To utilize the common losses from standard deep learning tools, such as PyTorch <ref type="bibr" target="#b46">[47]</ref>, we incorporate the logarithm function with the similarity ratio R in Eq. (3) to obtain the minimization objective, as follows:</p><formula xml:id="formula_7">L = 1 Np Np i=1 log 1 No (1 + Np j? =i s cen ij + Nn k=1 s neg k s intra i ),<label>(6)</label></formula><p>where N o = N p + N n ? 1. Similar to R in Eq. (3), a low value of L in Eq. (6) means that the current embedding space is clustering-friendly. Thus, we can partition most samples from the same class into the same cluster, using a simple clustering algorithm. A low value of L can also reduce the label inconsistency problem in the clustering-based unsupervised meta-learning. Asynchronous Embedding. If we only use a single encoder function ? for embedding learning, it is difficult to produce various embedding features for the samples augmented from the same original sample. Thus, we propose to utilize two asynchronous encoders, ?(?; ?) and?(?;?) for training to avoid fast convergence caused by similar positive samples. The first function ? is the main encoder, which is updated with the current samples (in a mini-batch). In other words, we use gradient backpropagation to update its parameter ?. The latter ? is the history encoder, which collects information from the main encoders in previous mini-batches. To ensure that the history encoders on-the-fly are smooth, i.e., the difference among these encoders can be made small <ref type="bibr" target="#b18">[19]</ref>, we use a momentum coefficient m ? [0, 1) to update the encoder parameter? = m?+?.</p><p>To reduce the computational load, only one sample from each class is encoded by the main encoder ?, and the others are encoded by the history encoder?. Without loss of generality, we encode the first augmented sample in each class. Then, the embedding features of the positive dataset {x + ij , i ? [1, N p ], j ? [1, N a ]} are reformulated as follows:</p><formula xml:id="formula_8">z + ij = ?(x + ij ; ?), j = 1, ?(x + ij ;?), j ? = 1.<label>(7)</label></formula><p>For the negative set, a naive approach would be to randomly select samples from the unlabeled dataset and then encode them with the history encoder. However, this solution is time-consuming and does not make full use of the different history encoders from previous mini-batches. Inspired by <ref type="bibr" target="#b18">[19]</ref>, we use a queue to construct the negative set by inserting the embedding features encoded by the history encoders. Specifically, we only insert one historical embedding of each class into the queue to maintain the diversity of the negative set. We remove the features of the oldest mini-batch to maintain the size of the negative set, since the oldest features are most outdated and least inconsistent with the new ones. Using the queue mechanism can reduce the computational cost and also remove the limit on the mini-batch size. Clustering and Meta-Learning. With the above training strategy, we can provide embedding features for the samples in the unlabeled dataset. Similar to CACTUs <ref type="bibr" target="#b24">[25]</ref>, we can then apply a simple clustering algorithm, e.g kmeans,to these features to split the samples into different clusters, denoted as {C 1 , C 2 , ? ? ? , C Ns }. We assign the same pseudo-label to all samples in a cluster. Thus, we obtain the pseudo-labeled dataset {(x i ,? i ), x i ? D}, where? i is the pseudo-label obtained by the clusters, i.e.? i = k if x i ? C k . Then, we can use any supervised meta-learning algorithm on this dataset to learn an efficient model for the meta-tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Progressive Evaluation Mechanism</head><p>Although our embedding approach yields promising improvements, it still suffers from the limited diversity problem. In other words, we cannot generate metatasks using the divisive samples, which are far away from their class center, and easily assigned the wrong label by the simple clustering algorithm. However, the supervised methods can utilize these samples to generate hard meta-tasks to enhance the model's discrimination ability. Thus, we propose a progressive evaluation mechanism to produce similar hard meta-tasks with supervised methods.</p><p>As mentioned before, our embedding approach will push samples close to their class centers. For each cluster, denoted as the base cluster, we assume that the divisive samples are in itsK-nearest-neighbors, which are denoted as the candidate clusters. Thus, we can try to use the samples in the candidate clusters to generate hard meta-tasks. To do so, we build an evaluation model using the meta-learning models in previous iterations to evaluate the candidates. To obtain a stable evaluation model, only the meta-training models trained at the end of an epoch are utilized. We denote the evaluation model as f (x;?), where? is the corresponding model parameter. Then, we select those with high information entropy as the final clusters, and choose the samples from the base and final clusters to generate hard meta-tasks, by filtering out noise. Selecting Clusters with High Information Entropy. To construct an Nways-K-shots meta-task, we need to randomly choose N c clusters denoted as base clustersC i , i ? [1, N c ]. We use the cluster center similaritys ik =? i ?? k to obtain theK most similar neighbors as the candidate clustersC ik , k ? [1,K], wher? ? i is the average embedding feature of clusterC i , andK = 5 in this paper. We randomly select K samples from each base cluster to build the support set, which we then use to finetune the evaluation model f (?;?) with the same meta-training method.</p><p>For simplification, we first define the entropy of a cluster C based on the finetuned evaluation model. The N -ways classification label of each sample x i ? C is computed as l i = arg max N j=1 l i [j], where l i = f (x i ;?). Then we can provide the probability p j of selecting a sample with label j from the cluster C by computing the frequency of the label j, i.e., p j = N j /? l , where N j is the occurrence number of label j and? l is the length of cluster C. Then the entropy of C is formulated as:</p><formula xml:id="formula_9">H(C) = ? N j=1 pj log pj.<label>(8)</label></formula><p>According to Eq. 8, we choose the clusterC * i with the highest entropy as the final cluster to construct the query set. The formulation is as follows:</p><formula xml:id="formula_10">C * i =C ik * , k * = arg maxK k=1 H(C ik )<label>(9)</label></formula><p>In our case, a low entropy for a cluster indicates that it is certain or informationless for the evaluation model, since the label outcome of a cluster can be regarded as a variable based on our evaluation model. In other words, the information in a cluster with low entropy has already been learned in the previous training epochs and is thus not new for the current evaluation model. In contrast, a cluster with high entropy can provide unseen information for further improvement. Filtering Out Noisy Samples. Notice that the clusterC * i contains many noisy samples (with inconsistent labels compared with the support samples). Thus, we use the proposed evaluation model f (?;?) to filter out several noisy samples and build the query set. First, we run f (?;?) on each sample x ij inC * i to provide the probabilities l ij of N -ways classification, i.e., l ij = f (x ij ;?). We take the probability of the i-th classification l ij [i] as the evaluation score of the sample x ij . According to these scores, we re-sort the clusterC * i in descending order to obtain a new cluster? i . The noisy samples are placed at the end of the cluster.</p><p>Thus, we can filter out the noisy samples by removing the ones at the end of the new cluster? * i . The keep rate ? ? (0, 1) is used to control the number of samples removed. Specifically, we define the removing operation as? i =? i [1 : ??N l ?], whereN l is the length of? i , and ??? is the floor operation. Finally, we randomly select Q samples from the cluster? i as the query set.</p><p>In particular, during training, we employ a random value ? ? (0, 1) for each mini-batch. Only when ? &gt; 0.9, we use the progressive evaluation mechanism to build the meta-tasks. Since our pseudo-labels contain the most useful information for training, we need to utilize this information as much as possible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Datasets and Implementation Details</head><p>Datasets. We evaluate the proposed approach on three popular datasets: Omniglot <ref type="bibr" target="#b29">[30]</ref>, mini ImageNet <ref type="bibr" target="#b38">[39]</ref>, and tiered ImageNet <ref type="bibr" target="#b40">[41]</ref>. Following the setting in <ref type="bibr" target="#b24">[25]</ref>, for all three datasets, we only use the unlabeled data in the training subset to construct the unsupervised few-shot tasks. Embedding Functions and Hyperparameters. For Omniglot, we adopt a 4-Conv model (the same as the one in MAML <ref type="bibr" target="#b15">[16]</ref>), as the backbone, and add two fully connected (FC) layers with 64 output dimensions (64-FC). For mini ImageNet and tiered ImageNet, we use the same embedding function, which includes a ResNet-50 <ref type="bibr" target="#b19">[20]</ref> backbone and two 128-FC layers, and train it on the whole ImageNet. The other training details are the same for the two models. The number of training epochs is set to 200. We use the same data augmentation and cosine learning rate schedule as <ref type="bibr" target="#b8">[9]</ref>. The other hyperparameters for training are set as N p = 256, N a = 2, N n = 65536, ? = 0.2, m = 0.999. We use the same number of clusters as CACTUs for fair comparison, i.e. N c = 500. The keep rate ? in the progressive evaluation is set as 0.75 to filter out very noisy samples. Supervised Meta-Learning Methods. We combine the proposed pseudolabeling based on clustering-friendly embedding (PL-CFE) with two supervised methods including classical model-agnostic meta-learning (MAML) <ref type="bibr" target="#b15">[16]</ref>, and the recently proposed embedding propagation (EP) <ref type="bibr" target="#b41">[42]</ref>. The corresponding methods are denoted as PL-CFE-MAML and PL-CFE-EP, respectively. In the following experiments, we only train the meta-learning models (MAML and EP) on the one-shot task and directly test them on the other few-shot tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Ablation Study</head><p>To analyze the effectiveness of the key components in our framework, we conduct extensive experiments on Omniglot and mini ImageNet with MAML and EP. First, we choose the unsupervised method ACAI <ref type="bibr" target="#b4">[5]</ref>/DC <ref type="bibr" target="#b5">[6]</ref> as the baselines of the embedding function, which achieves the best accuracy for CACTUs. Specifically, we apply k-means to the ACAI/DC embedding features to provide the pseudo-labels, and then run MAML and EP over eight few-shot tasks. These Omniglot miniImageNet Pos Neg AE SC FN (5,1) (5,5) (20,1) (20,5) (5,1) <ref type="bibr" target="#b4">(5,</ref><ref type="bibr" target="#b4">5)</ref>   <ref type="table">Table 2</ref>. Influence of key components in our model. We evaluate different variants in terms of the accuracy for N-ways-K-shots (N,K) tasks. The A/D labels represent ACAI <ref type="bibr" target="#b4">[5]</ref> on Omniglot <ref type="bibr" target="#b29">[30]</ref> and DC <ref type="bibr" target="#b5">[6]</ref> on miniImageNet <ref type="bibr" target="#b38">[39]</ref>. MC and M are short names of MoCo-v2 <ref type="bibr" target="#b8">[9]</ref> and MAML <ref type="bibr" target="#b15">[16]</ref>. Similarly, CFE is our embedding method and E represents the EP <ref type="bibr" target="#b41">[42]</ref>. The results of A/D-M are taken from CACTUs <ref type="bibr" target="#b24">[25]</ref>. The key components in our clustering-friendly embedding (CFE) are the positive set (Pos), negative set (Neg), and asynchronous embedding (AE). We also investigate the main parts of our progressive evaluation, including selecting the cluster (SC) and filtering out noise (FN). First, we minimize the similarity ratio on the positive set. As shown in <ref type="table">Table 2</ref>, our approach achieves impressive improvement for both MAML and EP in terms of all four tasks on Omniglot, compared with the baselines. In particular, in the 5-ways-1-shot task, the highest accuracy gain reaches 21.15%. Then, we add the negative set for training. However, the performance drops dramatically. The reason may be that the rapidly changing embedding models reduces the consistency of negative embedding features. Thus, we add AE to alleviate this issue. The increased performance (Pos+Neg+AE vs. Pos) indicates that Neg+AE can obtain more divisive samples for better training. We also use AE on the positive set (Pos+AE) and achieve expected accuracy gains for the two supervised methods in terms of all tasks. Finally, we use the SC strategy to select diverse samples for constructing few-shot tasks, yielding performance gains in all tasks. Then, the FN strategy is added to filter out noisy samples (SC+FN), which further improves the accuracy scores of all tasks. We also directly use our FN strategy on the original clusters (Pos+Neg+AE+FN) and achieve promising improvement in all tasks. This indicates that the proposed FN strategy can reduce the label inconsistency issue for improved performance. Besides, we also compare the recent contrast learning method MoCo-v2 <ref type="bibr" target="#b8">[9]</ref>, since our model has a similar training strategy but different training loss. We apply the pretrained MoCo-v2 model (200 epochs) to extract embeddings and provide the pseudo-labels, and then run MAML and EP over four few-shot tasks on mini ImageNet. These are denoted as MC-M and MC-E, respectively. As shown in <ref type="table">Table 2</ref>, even our methods without progress evaluation (Pos+Neg+AE) can outperform the MoCo-based methods in all tasks. This clearly demonstrates the effectiveness of our clustering-friendly embedding. Notice that our progress evaluation (Pos+Neg+AE+SC+FN) can further improve the performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Comparison with Other Algorithms</head><p>Results on Omniglot and mini ImageNet. We compare our PL-CFE-MAML and PL-CFE-EP with model-agnostic methods: CACTUs <ref type="bibr" target="#b24">[25]</ref>, AAL <ref type="bibr" target="#b1">[2]</ref>, ULDA <ref type="bibr" target="#b34">[35]</ref>, and LASIUM <ref type="bibr" target="#b27">[28]</ref>.</p><p>As shown in <ref type="table">Table 3</ref>, our PL-CFE-MAML outperforms all model-agnostic methods in eight few-shot tasks on two datasets. Specifically, for the 5-ways-5-shots task on the Omniglot dataset, our method achieves a very high score of 98.46%, which is very close to the supervised result of 98.90%. Surprisingly, the proposed PL-CFE-MAML even outperforms the corresponding supervised method under the 5-ways-20-shots and 5-ways-50-shots settings on the mini ImageNet dataset. In addition, compared with the baseline CACTUs-MAML, we achieve significant accuracy gains, which reach 32.77% (for 20-ways-1-shot) on Omniglot and 6.8% (for 5-ways-20-shots) on mini ImageNet. <ref type="table">Table  4</ref>. Accuracy (%) of N-ways-K-shots (N,K) tasks on tiered ImageNet <ref type="bibr" target="#b40">[41]</ref>. We show the results of supervised methods MAML <ref type="bibr" target="#b24">[25]</ref> and EP <ref type="bibr" target="#b41">[42]</ref> for complete comparison. The best values are in bold.</p><p>In the experiments for our PL-CFE-EP, we first create the CACTUs-EP baseline by combining CACTUs with EP. Our PL-CFE-EP provides impressive improvements in performance compared with CACTUs-EP. In particular, in the 20-ways-1-shot task on Omniglot, we achieve a huge gain of 30.17%. On mini ImageNet, the gains reach 10.72% (in 5-ways-20-shots). Compared with the other existing methods, our PL-CFE-EP obtains superior performance in six tasks and comparable performance in the other two tasks. In particular, we achieve a significant accuracy gain of 8.42% in 5-ways-1-shot on mini ImageNet. Results on tiered ImageNet. We compare our PL-CFE-MAML and PL-CFE-EP with the recent ULDA <ref type="bibr" target="#b34">[35]</ref> on tiered ImageNet. As shown in <ref type="table">Table 4</ref>, our models outperform the compared methods in four few-shot tasks. Specifically, for the 5-ways-1-shots and 5-ways-5-shots tasks, our PL-CFE-EP achieves the highest scores. Compared with the best previous method, ULDA-MetaOptNet, our method obtains accuracy gains of 7.74% and 7.35% on these two tasks, respectively. Our PL-CFE-MAML outperforms all compared methods in the 5-ways-20shots and 5-ways-50-shots tasks. It also achieves significant improvements, with gains of 3.98% and 4.36%, respectively, compared with ULDA-MetaOptNet.</p><p>In summary, the results demonstrate the effectiveness of our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we introduce a new framework of pseudo-labeling based on clusteringfriendly embedding (PL-CFE) to automatically construct few-shot tasks from unlabeled datasets for meta-learning. Specifically, we present an unsupervised embedding approach to provide clustering-friendly features for few-shot tasks, which significantly reduces the label inconsistency and limited diversity problems. Moreover, a progressive evaluation is designed to build hard tasks to further alleviate limited diversity issue. We successfully integrate the proposed method into two representative supervised models to demonstrate its generality. Finally, extensive empirical evaluations clearly demonstrate and the effectiveness of our PL-CFE, which outperforms the corresponding supervised meta-learning methods in two few-shot tasks. In the future, we will utilize our model to more computer vision tasks, such as object tracking <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b44">45]</ref> and segmentation <ref type="bibr" target="#b51">[52,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b12">13]</ref>, to explore label-free or label-less solutions.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Illustrations of 2D mapping of different embeddings, including Bi-GAN</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>two baselines are denoted as A/D-M and A/D-E, respectively. The results of A/D-M come from the original CACTUs paper.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>shots Task Filter Samples Process Clusters</head><label></label><figDesc></figDesc><table><row><cell>...</cell><cell>Set Negative</cell><cell>...</cell><cell></cell><cell>C1 -C2 -</cell><cell>...</cell><cell>CN -</cell></row><row><cell>Augmentation</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>Update</cell><cell></cell><cell></cell><cell></cell></row><row><cell>... ...</cell><cell></cell><cell>Positive Set ... ...</cell><cell>Similarity Ratio</cell><cell cols="3">Evaluation Model</cell></row><row><cell></cell><cell></cell><cell>Averaging</cell><cell></cell><cell></cell><cell></cell><cell>Support set</cell><cell>Query set</cell></row><row><cell>Mini-Batch</cell><cell>Embedding Models Asynchronous</cell><cell>...</cell><cell></cell><cell cols="3">Candidate Clusters</cell></row></table><note>N-ways-K-</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>95.48 98.04 83.67 92.54 49.13 62.91 70.47 71.79</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>(5,20) (5,50)</cell></row><row><cell>A/D-M</cell><cell></cell><cell></cell><cell cols="4">68.84 87.78 48.09 73.36 39.90 53.97 63.84 69.64</cell></row><row><cell>MC-M</cell><cell></cell><cell></cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>39.83 55.62 67.27 73.63</cell></row><row><cell></cell><cell>?</cell><cell></cell><cell cols="4">89.97 97.50 74.33 92.44 39.75 57.31 69.68 75.64</cell></row><row><cell></cell><cell>? ?</cell><cell></cell><cell cols="4">69.88 89.68 40.50 72.50 25.35 34.91 47.80 55.92</cell></row><row><cell></cell><cell>?</cell><cell>?</cell><cell cols="4">90.67 97.90 75.18 92.75 41.04 57.86 69.62 75.67</cell></row><row><cell>CFE-M</cell><cell cols="2">? ? ?</cell><cell cols="4">91.32 97.96 76.19 93.30 41.99 58.66 69.64 75.46</cell></row><row><cell></cell><cell cols="2">? ? ? ?</cell><cell cols="4">92.01 98.27 78.29 94.27 43.39 59.70 70.06 75.38</cell></row><row><cell></cell><cell cols="2">? ? ?</cell><cell cols="4">? 91.40 97.94 78.61 94.46 42.85 59.31 69.61 75.25</cell></row><row><cell></cell><cell cols="6">? ? ? ? ? 92.81 98.46 80.86 95.05 43.60 59.84 71.19 75.75</cell></row><row><cell>A/D-E</cell><cell></cell><cell></cell><cell cols="4">78.23 88.97 53.50 72.37 39.73 52.69 59.75 62.06</cell></row><row><cell>MC-E</cell><cell></cell><cell></cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>43.26 56.67 63.19 65.07</cell></row><row><cell></cell><cell>?</cell><cell></cell><cell cols="4">93.31 97.31 78.59 90.09 43.44 56.98 64.06 66.65</cell></row><row><cell></cell><cell>? ?</cell><cell></cell><cell cols="4">60.70 73.37 37.93 49.33 26.68 32.36 37.17 38.86</cell></row><row><cell></cell><cell>?</cell><cell>?</cell><cell cols="4">93.48 97.40 79.75 90.67 43.55 57.73 64.17 66.17</cell></row><row><cell>CFE-E</cell><cell cols="2">? ? ?</cell><cell cols="4">93.88 97.46 79.55 91.13 47.55 62.13 68.89 71.11</cell></row><row><cell></cell><cell cols="2">? ? ? ?</cell><cell cols="4">95.31 98.01 83.17 92.50 48.25 62.54 69.76 71.58</cell></row><row><cell></cell><cell cols="2">? ? ?</cell><cell cols="4">? 94.93 98.00 82.57 91.97 48.02 62.56 69.85 72.31</cell></row><row><cell></cell><cell cols="3">? ? ? ? ?</cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning to learn by gradient descent by gradient descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andrychowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Denil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">W</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pfau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Schaul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shillingford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>De Freitas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>References 1.</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Assume, augment and learn: Unsupervised fewshot meta-learning via random labels and data augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Antoniou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Storkey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.09884</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Using fast weights to attend to the recent past</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Z</forename><surname>Leibo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ionescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Greedy layer-wise training of deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lamblin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Popovici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Understanding and improving interpolation in autoencoders via an adversarial regularizer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICLR</publisher>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Deep clustering for unsupervised learning of visual features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ECCV</publisher>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A closer look at few-shot classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Kira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">C F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Infogan: Interpretable representation learning by information maximizing generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Houthooft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.04297</idno>
		<title level="m">Improved baselines with momentum contrastive learning</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Semi-supervised sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Adversarial feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kr?henb?hl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Clnet: A compact latent network for fast adjusting siamese trackers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Porikli</surname></persName>
		</author>
		<editor>ECCV. Springer</editor>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Sub-markov random walk for image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE T-IP</title>
		<imprint>
			<biblScope unit="page">14</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Diversity with cooperation: Ensemble methods for few-shot classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dvornik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Why does unsupervised pre-training help deep learning? JMLR pp</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">A</forename><surname>Manzagol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Model-agnostic meta-learning for fast adaptation of deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML (2017) 1, 2, 4</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Probabilistic model-agnostic meta-learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Learning to fuse asymmetric feature maps in siamese trackers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR (2020)</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Delving deep into rectifiers: Surpassing humanlevel performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>ICCV</publisher>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A fast learning algorithm for deep belief nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Using fast weights to deblur old memories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">C</forename><surname>Plaut</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CCSS (1987)</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Learning to learn using gradient descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Younger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">R</forename><surname>Conwell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
			<publisher>ICANN</publisher>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Universal language model fine-tuning for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ruder</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ACL</publisher>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Unsupervised learning via meta-learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
	<note>In: ICLR (2019) 2, 3, 4, 5, 6, 9</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Unsupervised few-shot feature learning via self-supervised training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers Comput. Neurosci</title>
		<imprint>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Unsupervised meta-learning for few-shot image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khodadadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Boloni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Unsupervised meta-learning through latent-space interpolation in generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khodadadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zehtabian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vahidian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>B?l?ni</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>ICLR</publisher>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Siamese neural networks for one-shot image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML workshop</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">One shot learning of simple visual concepts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CogSci (2011) 3, 5</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">B</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Hwang</surname></persName>
		</author>
		<title level="m">Meta-GMVAE: Mixture of Gaussian VAE for Unsupervised Meta-Learning. In: ICLR (2021)</title>
		<imprint>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning to self-train for semi-supervised few-shot classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Chua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Self-supervised prototypical transfer learning for few-shot classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Medina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Devos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Grossglauser</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.11325</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Few-shot image recognition with knowledge transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCV</publisher>
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Unsupervised few-shot learning via distribution shift-based augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.05805</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Improving language understanding by generative pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note>Preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Unsupervised pretraining for sequence to sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ramachandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EMNLP</title>
		<imprint>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Efficient learning of sparse representations with an energy-based model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Poultney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">L</forename><surname>Cun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Optimization as a model for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR (2017) 3, 4</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Few-shot learning with embedded class models and shot-free meta training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ravichandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bhotika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Soatto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCV</publisher>
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Meta-learning for semi-supervised few-shot classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Triantafillou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In: ICLR</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">14</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Embedding propagation: Smoother manifold for few-shot classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Rodr?guez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laradji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Drouin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lacoste</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV (2020) 1, 3, 4</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Meta-learning with memory-augmented neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bartunov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Botvinick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lillicrap</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In: ICML</title>
		<imprint>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Evolutionary principles in self-referential learning, or on learning how to learn: the meta-meta-... hook</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1987" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
		<respStmt>
			<orgName>Technische Universit?t M?nchen</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Distilled siamese networks for visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Hoi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE T-PAMI</title>
		<imprint>
			<biblScope unit="page">14</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Prototypical networks for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS. pp</title>
		<imprint>
			<biblScope unit="page">8</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Learning to learn: Introduction and overview</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thrun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pratt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Learning to learn</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1998" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Extracting and composing robust features with denoising autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">A</forename><surname>Manzagol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICML</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Matching networks for one shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Inferring salient objects from human fixations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE T-PAMI</title>
		<imprint>
			<biblScope unit="page">14</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Multi-level representation learning with semantic alignment for referring video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Unsupervised feature learning via nonparametric instance discrimination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Roles of pre-training and fine-tuning in contextdependent dbn-hmms for real-world speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Dahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS Workshop</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Split-brain autoencoders: Unsupervised learning by cross-channel prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
