<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">OriNet: A Fully Convolutional Network for 3D Human Pose Estimation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxu</forename><surname>Luo</surname></persName>
							<email>chenxuluo@jhu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">The Johns Hopkins University Baltimore</orgName>
								<address>
									<postCode>21218</postCode>
									<region>MD</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Chu</surname></persName>
							<email>chuxiao@baidu.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Baidu Research (USA) Sunnyvale</orgName>
								<address>
									<postCode>94089</postCode>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
							<email>ayuille1@jhu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">The Johns Hopkins University Baltimore</orgName>
								<address>
									<postCode>21218</postCode>
									<region>MD</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">OriNet: A Fully Convolutional Network for 3D Human Pose Estimation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>LUO ET AL.: ORINET: A FULLY CONVOLUTIONAL NETWORK FOR 3D HUMAN POSE 1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T19:20+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we propose a fully convolutional network for 3D human pose estimation from monocular images. We use limb orientations as a new way to represent 3D poses and bind the orientation together with the bounding box of each limb region to better associate images and predictions. The 3D orientations are modeled jointly with 2D keypoint detections. Without additional constraints, this simple method can achieve good results on several large-scale benchmarks. Further experiments show that our method can generalize well to novel scenes and is robust to inaccurate bounding boxes. Figure 1: Modeling limb orientations. (a) is an input example. (b) is the 3D human pose represented with skeleton. The key-points are connected following the tree structure, (c) zooming into to see the orientation of the upper arm. ?x, ?y and ?z are the relative coordinate of the two keypoints, l is the length of the limb. (d) showing two examples where orientation vectors are binding with the segmentation of the limb.</p><p>joint) when predicting the coordinates of each joint. Given that the actual receptive fields of an FCN are much less than the fully connected networks. This makes it harder to regress 3D coordinates of joints far away from the root joint.</p><p>In this paper, we propose a novel fully convolutional network to address the above issues. Instead of directly dealing with joint coordinates, we model limb orientations as a new representation for 3D pose reconstruction. One advantage is that orientation is scale invariant and independent of dataset, which helps resolve the scale ambiguity issue and generalize to diverse data. Since the orientations of limbs are what truly differentiate one pose from another, it is natural and explainable to model the orientations of limbs. Also, it allows the network to focus on pose itself. This representation is flexible and is especially useful for applications such as character control and motion retargeting. Another motivation for using only orientation is that the limb length ratios of different subjects are very similar and are often used as an regularization <ref type="bibr" target="#b30">[30,</ref><ref type="bibr" target="#b38">38]</ref>. So decoupling them to the post-processing stage can let the network focus on each limb without the need to consider other limbs that may lie far away. In our experiments, we show that this is one of the key components to achieve good performance and generalize well to other datasets.</p><p>Instead of using a vector representation that lacks spatial association between images and predictions, we propose to combine the orientation with the approximate bounding box of limbs, as shown in <ref type="figure">Fig.1 (d)</ref>. Unlike semantic part segmentation <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b32">32]</ref>, we do not care about the detailed boundary of each body part, so a bounding box is a good enough approximate representation for each limb. On the orientation map, regions corresponding to limbs are filled with an orientation vectors, while other locations are set to zero to indicate background. The bounding box of each limb can be easily obtained from 2D joints annotations. This representation can preserve the spatial layout of each limb and explicitly tells the network where to focus when predicting each limb orientation.</p><p>Generally, the proposed method can be plugged into any networks for 2D pose estimation. In this paper, we adopt the Stacked Hourglass network [19] because of its superior performance. We simply consider the orientations between adjacent joints without incooperating additional prior knowledge, we show that our method can still achieve the comparable results on Human 3.6m dataset [12] and start-of-the-art results on MPI-INF-3DHP dataset <ref type="bibr" target="#b15">[16]</ref>. We expect further constraints such as bone structures <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b26">26]</ref> or joint-angle limits [1] can improve the performance, but they are beyond the scope of this paper.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Estimating 3D human pose from a single RGB image is a fundamental yet challenging problem. It is potentially useful in many real world applications, such as human-robot interaction, augmented reality and character control. Besides the inherent challenges in 2D pose estimation, 3D pose estimation from monocular images is considered more difficult due to the loss of depth and scale ambiguity.</p><p>With the advent of deep neural networks, we saw significant progress in monocular 3D human pose estimation. Currently, the end-to-end training method <ref type="bibr" target="#b26">[26,</ref><ref type="bibr" target="#b38">38]</ref> achieve superior results on standard benchmarks <ref type="bibr" target="#b11">[12]</ref>. However, there are several limitations that hinder them to real world settings. First, due to the scale and depth ambiguity from a single image, some works <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b38">38]</ref> even require a fixed image scale, making it less flexible to generalize to other datasets.</p><p>Second, currently most methods require a tightly cropped box around the subject. One reason is that most methods use a fully-connected layer to regress the joint coordinates directly, which makes the network sensitive to backgrounds. Another practical issue is the lack of diverse training data as for 2D human pose estimation <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b13">14]</ref>.</p><p>Recently, VNect <ref type="bibr" target="#b16">[17]</ref> shows promising results of fully convolutional network for 3D human pose estimation. In that work, they attach the 3D joint coordinates to the corresponding locations in the image. However, the performance is not as good as its fully-connected counterparts. One reason may lies on that the network need to localized another joint (the root Our contributions can be summarized as follow: <ref type="bibr" target="#b0">(1)</ref> We propose a fully convolutional network for 3D human pose estimation, which is less sensitive to backgrounds and inaccurate bounding boxes. <ref type="bibr" target="#b1">(2)</ref> We propose to use 3D orientations of limbs as a new way to represent 3D poses, which is arguably natural and interpretable. (3) Our proposed method, by only using limb orientations, achieves state-of-the-art results and can generalize well to novel scenes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Given the difficulties of estimating 3D pose from a single image, many works decouple the problem into two steps: first estimate 2D joints and then lift then into 3D. A typical approach uses sparse-based representation <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b34">34]</ref>. Recent works also use deep network to regress 3D poses from 2D positions directly <ref type="bibr" target="#b14">[15]</ref>. Thanks to the deep networks, we see huge improvements on 2D pose estimation in recent years <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b29">29,</ref><ref type="bibr" target="#b31">31]</ref>. The progress also benefits such two-stage methods <ref type="bibr" target="#b36">[36]</ref>. However, these two-step methods rely highly on the results of 2D estimation. Also, discarding image cues makes the problem ill-posed.</p><p>Recently, some works try to combine the two steps together. In <ref type="bibr" target="#b35">[35]</ref>, they treat the 2D estimation as latent variables and use the EM algorithm to update the 2D joints at the same time. Tome et al. <ref type="bibr" target="#b28">[28]</ref> refine both the 2D and the 3D estimation iteratively at each stage. However, 3D poses are still estimated merely from the intermediate 2D results.</p><p>Recent works <ref type="bibr" target="#b26">[26,</ref><ref type="bibr" target="#b27">27,</ref><ref type="bibr" target="#b38">38]</ref> combine the 2D heatmaps and image cues. This method also needs a lot of training data. Since most of the current datasets only contain indoor scene with limited number of subject and background, some methods <ref type="bibr" target="#b26">[26,</ref><ref type="bibr" target="#b38">38]</ref> mix 2D and 3D data during the training time. This makes it less flexible and often need manually correct the discrepancy <ref type="bibr" target="#b38">[38]</ref> between dataset annotations. <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b23">24]</ref> use ordinal depth as additional supervision.</p><p>Most of the above methods use 3D coordinates to represent 3D poses. The coordinates can be relative positions to a root joint <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b28">28]</ref>,to the adjacent joint <ref type="bibr" target="#b12">[13]</ref> or their combinations <ref type="bibr" target="#b15">[16]</ref>. Another way is to use 2D pixel coordinates and depth to represent each joint <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b26">26]</ref> and assuming known intrinsic parameter to recover the final 3D pose . A few works model limbs for pose estimation. In <ref type="bibr" target="#b26">[26]</ref> they define bone errors along the skeleton chain. Different from theirs, our method only consider each limb independently without long-range dependencies, making it more flexible to deal with rare poses. Zhou et al. <ref type="bibr" target="#b37">[37]</ref> also tries to deal with joint angles. They define a set of joint angles as intermediate representation and use a kinematic layer to reconstruct 3D poses. Here we only use the orientations as output. We show that this simple method can achieve better results.</p><p>To best of our knowledge, only two kinds of works use fully convolutional networks for 3D pose estimation. Pavlakos et al. <ref type="bibr" target="#b19">[20]</ref> introduces 3D heatmap to predict per-voxel likelihood by discretizing depth values. But they need the groundtruth depth of a root joint to finally recover the 3D pose, making it less practical. VNect <ref type="bibr" target="#b16">[17]</ref> attach the 3D coordinates to a neighbor of the joint on the heatmap. Both of them only deal with each joint separately, without considering the limb orientation which is the underlying factor that causes different poses. Given the flexibility of FCNs, in this paper we propose a more powerful methods for 3D pose estimation.</p><p>The orientation has only been explored in the context of 2D pose <ref type="bibr" target="#b2">[3]</ref> previously. However, it only serves as an auxiliary task to differentiate different instances, so there is no quantitative evaluation for that. Also it is nontrivial whether it can be extended to estimat-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>In this section, we introduce our fully convolutional network for 3D human pose estimation in detail. Our method model the 2D key-point positions together with the orientation of limbs. The orientation is formulated with an approximate bounding box of the corresponding limb region. An overview of our framework is shown in <ref type="figure">Fig. 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Orientation Representation</head><p>The orientation of each limb is the most discriminative property of 3D poses. It can be represented by a unit vector U = (?x, ?y, ?z)/l, where ?x, ?y and ?z are the relative positions of the two joints. l = ?x 2 + ?y 2 + ?z 2 is the length of the limb. The normalized orientations remove the influence of different human scales and image resolutions.</p><p>In order to preserve this spatial layout and explicitly tell the network where to focus, we propose to model the orientation together with the limbs region, as shown in <ref type="figure">Fig. 2</ref>(e). For each limb k, we generate a bounding box using the locations of two endpoint joints correspond to that limb on the label map. Specifically, the bounding box region L k contains pixels within a predefined width w k to the line segment p k 1 p k 2 of two joints k 1 and k 2 . This forms a good approximation for each limb region, see <ref type="figure">Fig. 2</ref>(e) and supplementary material for detail. Pixels inside the bounding box indicate the region of this limb, and are labeled with the orientation vectors. Other places are set to zero,indicating background. The orientation map is defined as:</p><formula xml:id="formula_0">O k (i, j) = U k (i, j) ? L k , 0 (i, j) / ? L k ,<label>(1)</label></formula><p>where (i, j) are the pixel locations on the output prediction map, L k are the regions corresponding to the limb k. Examples of feature map are shown in <ref type="figure">Fig. 2</ref>. The loss function for limb orientations is as follows,</p><formula xml:id="formula_1">L o = ? k O k ? O k 2 2 (2)</formula><p>where O k is the predicted orientation map for the k-th limb.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Model the 2D keypoint location</head><p>In order to encourage the connection between the 2D appearance and 3D orientations, we propose to predict the 2D keypoint at the same time. The 2D joints can help localize the limb region more precisely during both training and inference. Following the common practice, we represent 2D keypoints with heatmaps, as shown in <ref type="figure">Fig. 2</ref>. In each heatmap, a Gaussian centered at ground-truth location indicate the existence of that keypoint. We train 2D keypoints with sigmoid cross entropy loss:  <ref type="figure">Figure 2</ref>: Overview of our framework. We use both 2D heatmaps (d) and limb orientations (e) as the supervisions. The 3D orientation maps are derived from keypoints without extra annotations. In the test stage, the predicted keypoint locations are used to crop the limb regions on the orientation heatmap to get the orientation prediction.</p><formula xml:id="formula_2">L p = ? 1 N ? n W ? i=1 H ? j=1 p n i, j log( p n i, j ) + (1 ? p n i, j ) log(1 ? p n i, j )<label>(3)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hourglass</head><p>Hourglass where p n and p n are predicted and groundtruth heatmaps and N is the number of joints. The final loss function is</p><formula xml:id="formula_3">f img f h P O f p f o f ' h</formula><formula xml:id="formula_4">L = L o + ? L p<label>(4)</label></formula><p>where ? is a balancing parameter. We use ? = 0.2 in the experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Network Structure</head><p>We implemented the model based on the Stacked Hourglass network <ref type="bibr" target="#b18">[19]</ref>. The network learns low level image representations f img for the input image. In each stack, the hourglass module will learn its feature f h . After that, the network is split into two branches, one for 2D joint locations, the other for the 3D limb orientations (see <ref type="figure">Fig. 2</ref>). Supervision is applied after each stack. The predictions, hourglass features and the image features are combined as follows:</p><formula xml:id="formula_5">f img = f img + Conv(p) + Conv(O) + Conv(f h )<label>(5)</label></formula><p>Then the updated image feature f img is used as the input of next hourglass module. For simplicity, we draw the Hourglass network structure for 2 stacks in <ref type="figure" target="#fig_1">Fig. 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Inference</head><p>The inference process includes (1) estimating 2D keypoints, (2) using the estimated 2D keypoints to read off limb regions, (3) taking the averaged orientations in that region, (4) recovering the 3D pose using the estimated orientations, limb length ratio and scale information. The 2D keypoint locations can be obtained by finding the maximal location on the 2D key-point heatmap, for the n-th joint, (x n , y n ) = arg max p n</p><p>After getting predictions of all 2D keypoints, we can define the region for each limb. Taking a pair of adjacent joints p 1 and p 2 that corresponds to limb l, we use the coordinates to crop out the region between them on the limb 3D orientation O l . The cropped regions contain the predictions of the orientation of the l-th limb.</p><p>We average the value of the normalized predictions and then normalize it as the estimated orientation. Then the limb length ratio as used in many other works <ref type="bibr" target="#b30">[30,</ref><ref type="bibr" target="#b38">38]</ref> and scale information <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b26">26,</ref><ref type="bibr" target="#b35">35,</ref><ref type="bibr" target="#b38">38]</ref> are used to recover each limb vector. By choosing a root joint, we reconstruct each joint position along the tree structure iteratively. Indeed this simple representation could cause error accumulation along the path, however, we show that this can still achieve good results without adding additional constraints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets and Implementation Detail</head><p>We evaluate our method on two datasets: Human3.6m <ref type="bibr" target="#b11">[12]</ref> and MPI-INF-3DHP <ref type="bibr" target="#b15">[16]</ref>. Human3.6M. This is currently the largest 3D pose dataset, containing 11 subjects performing 15 actions. We adopt a commonly used protocol: five subjects <ref type="bibr" target="#b0">(1,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8)</ref> for training and two subjects (9,11) for testing. The videos are down sampled to 10fps. Mean per joint position error(MPJPE) is computed with the root joint aligned. We also report the results after Procrustes alignment, which only focus on the structure of the 3D poses. Following <ref type="bibr" target="#b35">[35]</ref>, we test our algorithm on all 17 joints defined in <ref type="bibr" target="#b11">[12]</ref>. MPI-INF-3DHP. This is a newly released dataset. It is more challenging, for it contains more diverse motions and aims for testing the generalization of various methods. Here, we only use the provided training data and do not use any background augmentation like <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17]</ref>. We report the PCK with a threshold of 150mm and AUC on all 2929 testing images. Implementation Detail. We adopt a 5-stack hourglass network <ref type="bibr" target="#b18">[19]</ref> and implement it in Torch7 <ref type="bibr" target="#b7">[8]</ref>. The initial learning rate is 2.5e-4 and decreases by a factor of 10 after every two epochs. We train the model for about 8 epochs, using RMSprop optimizer. We also add scale and color jittering as data augmentation. During the testing time, for fair comparison, we assume known bounding box of the person as in all the other works. We only use a single crop, without using flipping or multiple scale fusion. For the scale issue, we use training subjects to compute the limb-length ratio and scale. We also test our model directly with randomly shifted bounding box randomly as in <ref type="bibr" target="#b16">[17]</ref>. Our approach can run at 20fps on a Titan XP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results on Human3.6M Dataset</head><p>Results are shown in <ref type="table" target="#tab_1">Table 1</ref>. For model trained only on Human3.6m dataset, we achieve the state-of-the-art result. Especially for difficult poses like sitting down, which have fewer examples in the training set, our method outperforms other methods by a large margin. This shows that our method is more data efficient. As for models pretrained or mixed trained with 2D datasets (e.g. MPII), our method can still achieve comparable results. Compared with <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b26">26]</ref>, our method has lower error after alignments while slightly higher errors before alignment. Note that <ref type="bibr" target="#b26">[26]</ref> also use 2D data during training and camera parameters during testing. The result shows that our predicted poses have the most similar structure compared with the groundtruth. We also report results using the groundtruth limb length, similar to the skeleton-fitting step used in VNect <ref type="bibr" target="#b16">[17]</ref>. In <ref type="figure" target="#fig_4">Fig. 4</ref>   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Results on MPI-INF-3DHP</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Cross Datasets Test</head><p>Performance on Human3.6M seems to be saturating. However, the generalization problem largely remain unexplored. MPI-INF-3DHP provides a good testbed for such purpose. We apply models trained on Human3.6m directly to this dataset and results are shown in table 2.</p><p>Our method can achieve comparable results with <ref type="bibr" target="#b15">[16]</ref>, which is specifically designed for transfer learning. Even compared with mixed-training method <ref type="bibr" target="#b38">[38]</ref>, we still get comparable results. However, as pointed out in <ref type="bibr" target="#b38">[38]</ref>, they need to manually fix the position of some joints in an ad-hoc manner because of the discrepancy between different annotations.</p><p>We also test the direct regression method <ref type="bibr" target="#b14">[15]</ref> using 2D groundtruth as input and rescale the output to the universal skeleton. We see that it can not generalize well to different camera viewpoints. Our method can deal with different camera viewpoints and background better.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Training Data PCK AUC GS noGS Outdoor ALL ALL Meta <ref type="bibr" target="#b15">[16]</ref> H36m   We also test the model trained on Human3.6m that uses relative limb vector (to the length of the torso) instead of orientations. During the testing time, we rescaled each limb to its groundtruth length, only preserving the orientations. We show that it generalize worse than using orientation as representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Training on MPI-INF-3DHP Dataset</head><p>We finetune the network trained on the Human3.6m previous on this new dataset, without adding background augmentations. Results are shown in <ref type="table" target="#tab_4">Table 3</ref>. Compared with <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17]</ref>, which use extensive background and clothes augmentation, our model trained merely on the green-screen background can still outperform other methods. This shows that our method is more robust to background and can transfer well to different scenes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Ablation Study</head><p>In this part, we evaluate different components of our approach on the Human3.6m. For simplicity, we use another protocol used in <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b33">33]</ref>, where only one subject (S11) is used  for testing. The estimated 3D pose is first aligned with a rigid transformation, which ruling out factors such as scale and rotation. We trained the 1-stack networks from scratch and 5-stack networks pretrained on the MPII dataset. Results are shown in  <ref type="table" target="#tab_5">Table 4</ref>: The mean reconstruction errors on Human3.6M (S11). Errors are computed after Procrustes alignment. "fc" means sparse representation with a fully connected layer at the end. "len" means using the properly normalized limb vector instead of orientations. "Rescaled" means rescaled each bone to the groundtruth length during testing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.1">Orientation</head><p>First, we demonstrate the advantage of estimating orientations compared with the original limb vector. We train the same network using the original limb vector properly normalized by the length of torso. As we can see, using the limb orientation representation can perform significantly better than considering bone length at the same time for both 1-stack and 5stack networks (see 1-stack-len vs 1-stack and 5-stack-len vs 5-stack respectively). Even if we rescaled the bone vector to the groundtruth length preserving the orientation, the result is still worse than directly regressing the orientation. Furthermore, the 1-stack network using the orientation representation can even outperform 5-stack network with the original limb vector. This shows that decoupling the orientation and length of each limb is beneficial.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.2">Image to Prediction Association</head><p>In this part, we show the advantages of having the spatial associations between images and predictions. We train 1-stack networks from scratch, one with dense output as proposed, one with max pooling layer after the last several convolutional layers and the final convolutional layer replaced by a fully connected layer for direct regression. The dense representations achieves better results using orientations as representation. This show the benefits of our proposed image-to-prediction association. However, by considering limb length, the fully convolutional does not perform as well as the fully connected network. We conjecture that it may due to the relative small receptive field the network actually has. It also shows the necessary for using orientation for fully convolutional networks to achieve better performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.3">Robust to Bounding Box Jitter and Scale</head><p>Similar to <ref type="bibr" target="#b16">[17]</ref>, we carry out experiments on MPI-INF-3DHP dataset by jittering the bounding box at random in the range of ?40 px. Since during the testing, we need to resize the bounding boxes to a fixed scale(256 in our experiments and 224 in <ref type="bibr" target="#b16">[17]</ref>), we also add more noises by jittering the bounding box in the range of ?100 px and rescale it by a factor of ?0.2. Our method is less sensitive to inaccurate bounding boxes. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we propose a fully convolutional network that ties orientations with corresponding limb region to enhance the spatial relation between images and predictions. Our method is simple yet effective. Further experiments show that our model can generalize better to novel scenes and robust to inaccurate bounding boxes. In the future work, we expect adding additional constraints during both training and testing process would further improve the performance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Network structure. f h is the output of the hourglass module, P is the 2D heatmaps and O is orientation maps. f img is the image feature and f p , f o and f h are the intermediate features,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc><ref type="bibr" target="#b17">[18]</ref> 69.5 80.2 78.2 87.0 100.7 102.7 76.0 69.6 104.7 113.9 89.7 98.5 79.2 82.4 77.2 87.3 [26] 42.1 44.3 45.0 45.4 51.5 53.0 43.2 41.3 59.3 73.3 51.0 44.0 48.0 38.3 44.8 48.3 [15] 39.5 43.2 46.4 47.0 51.0 56.0 41.4 40.6 56.5 69.4 49.2 45.0 49.5 38.0 43.1 47.7 Ours 40.8 44.6 42.1 45.1 48.3 54.6 41.2 42.9 55.5 69.9 46.7 42.5 48.0 36.0 41.4 46.6</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Qualitative results on Human3.6M dataset. In each example, the first one is the input image, the second one is our estimated pose and the third on is the groundtruth.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Qualitative results on the MPI-INF-3DHP test set. Our model trained only on green-screen background images (see coloum 1) can still perform well on novel scenes (see column 2 and 3).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>, we show some qualitative results Direct Diss. Eat Greet Phone Photo Pose Purch. Sit SitD. Smoke Wait WalkD Walk WalkT Ave Train on Human3.6M from scratch(or pretrained on ImageNet [25]) [26] 90.2 95.5 82.3 85.0 87.1 94.5 87.9 93.4 100.3 135.4 91.4 87.3 90.4 78.0 86.5 92.4 [28] 64.9 73.5 76.8 86.4 86.3 110.7 68.9 74.8 110.2 173.9 84.9 85.8 86.3 71.4 73.1 88.4 Ours 68.4 77.3 70.2 71.4 75.1 86.5 69.0 76.7 88.2 103.4 73.8 72.1 83.9 58.1 65.4 76.0 Pretrained on 2D pose datasets(e.g. MPII) [16] 59.7 69.7 68.8 68.8 76.4 85.2 59.0 75.0 96.2 122.9 70.8 68.5 82.0 54.4 59.8 74.1 [27] 54.2 61.4 60.2 61.2 79.4 78.3 63.1 81.6 70.1 107.3 69.3 70.3 74.3 51.8 63.2 69.7 [15] 51.8 56.2 58.1 59.0 69.5 78.4 55.2 58.1 74.0 94.6 62.3 59.1 65.1 49.5 52.4 62.9 Ours 53.5 60.9 56.3 59.1 64.3 74.4 55.4 63.4 74.8 98.0 61.1 58.2 70.6 49.1 55.7 63.7 Ours * 49.2 57.5 53.9 55.4 62.2 73.9 52.1 60.9 73.8 96.5 60.4 55.6 69.5 46.6 52.4 61.3 Results after Procrustes alignment</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Mean per joint position errors (MPJPE) in mm on Human3.6M. * denotes using the groundtruth length for each limb.from Human3.6M dataset. For each example, we illustrate the image, our result and the groundtruth respectively.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Results on MPI-INF-3DHP test set by scene. All models are pretrained on MPII. Our method can achieve comparable results even with mixed training method<ref type="bibr" target="#b38">[38]</ref> .</figDesc><table><row><cell></cell><cell>data</cell><cell cols="7">Walk Exe. Sit Reach Floor Sport Misc</cell><cell></cell><cell cols="2">Total</cell></row><row><cell></cell><cell></cell><cell cols="10">PCK PCK PCK PCK PCK PCK PCK PCK AUC MPJPE</cell></row><row><cell></cell><cell>(MPII)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Mehta [16]</cell><cell>3DHP a</cell><cell>86.6</cell><cell>75.3</cell><cell>74.8</cell><cell>73.7</cell><cell>52.2</cell><cell>82.1</cell><cell>77.5</cell><cell>75.7</cell><cell>39.3</cell><cell>117.6</cell></row><row><cell></cell><cell>+H3.6M</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>76.5</cell><cell>40.8</cell><cell>-</cell></row><row><cell>Mehta [17]</cell><cell cols="11">(MPII+LSP) H3.6M+3DHP a 87.7 77.4 74.7 72.9 51.3 83.3 80.1 76.6 40.4 124.7</cell></row><row><cell cols="3">Dabral [9] 89.1  Ours MPII+ 3DHP a (MPII) 90.4 H3.6M+3DHP 90.5  *</cell><cell>79.1 80.9  *</cell><cell>88.5 90.0  *</cell><cell>81.6 85.6  *</cell><cell>66.3 70.2  *</cell><cell>91.9 93.0  *</cell><cell>92.2 92.9  *</cell><cell>81.8 83.8  *</cell><cell>45.2 47.7  *</cell><cell>89.4 85.0  *</cell></row></table><note>* 75.1* 73.6* 77.9* 49.2* 79.3* 80.8* 76.7* 39.1* 103.8*</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Activitywise performance on MPI-INF-3DHP test set. We report the PCK, AUC and MPJPE. Higher PCK and AUC are better and lower MPJPE is better. (MPII) means pretrained on MPII dataset."a" means adding background augmentation for training, "p" means Procrustes alignment, * denote using groundtruth limb length.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>table 4 .</head><label>4</label><figDesc></figDesc><table><row><cell>method</cell><cell>MPJPE method</cell><cell>MPJPE</cell></row><row><cell>Chen[4]</cell><cell>82.72 Chen[4]+2D GT</cell><cell>57.50</cell></row><row><cell>1-stack</cell><cell>50.68 1-stack-fc</cell><cell>56.09</cell></row><row><cell cols="2">1-stack-len 86.32 1-stack-len-fc</cell><cell>81.51</cell></row><row><cell cols="3">5-stack-len 52.72 5-stack-len(rescaled) 41.95</cell></row><row><cell>5stack</cell><cell>38.53</cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">LUO ET AL.: ORINET: A FULLY CONVOLUTIONAL NETWORK FOR 3D HUMAN POSE ing 3D orientations accurately, for 3D poses need reasoning beyond the image space. The motivation and application are totally different from ours.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">LUO ET AL.: ORINET: A FULLY CONVOLUTIONAL NETWORK FOR 3D HUMAN POSE</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Pose-conditioned joint angle limits for 3d human pose reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ijaz</forename><surname>Akhter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">2d human pose estimation: New benchmark and state of the art analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhaylo</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Realtime multi-person 2d pose estimation using part affinity fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-En</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Sheikh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">3d human pose estimation= 2d pose estimation+ matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ching-Hang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Structured feature learning for pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Crf-cnn: Modeling structured information in human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Multi-context attention for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Torch7: A matlab-like environment for machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cl?ment</forename><surname>Farabet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BigLearn, NIPS Workshop</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning 3d human pose from structure and motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rishabh</forename><surname>Dabral</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Mundhada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uday</forename><surname>Kusupati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Safeer</forename><surname>Afaque</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arjun</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning pose grammar to encode human body configuration for 3d pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanlu</forename><surname>Hao-Shu Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenguan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaobai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song-Chun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Look into person: Selfsupervised structure-sensitive learning and a new benchmark for human parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohui</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Human3.6m: Large scale datasets and predictive methods for 3d human sensing in natural environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catalin</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragos</forename><surname>Papava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vlad</forename><surname>Olaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1325" to="1339" />
			<date type="published" when="2014-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">3d human pose estimation from monocular images with deep convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sijin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoni</forename><forename type="middle">B</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">A simple yet effective baseline for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julieta</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rayat</forename><surname>Hossain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">J</forename><surname>Little</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Monocular 3d human pose estimation in the wild using improved cnn supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dushyant</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helge</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleksandr</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weipeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 Fifth International Conference on</title>
		<imprint/>
	</monogr>
	<note>3D Vision (3DV</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Vnect: Real-time 3d human pose estimation with a single rgb camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dushyant</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srinath</forename><surname>Sridhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleksandr</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helge</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Shafiei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans-Peter</forename><surname>Seidel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weipeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">3d human pose estimation from a single image via distance matrix regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesc</forename><surname>Moreno-Noguer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiyu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Coarse-to-fine volumetric prediction for single-image 3d human pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Konstantinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Derpanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Daniilidis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Ordinal depth supervision for 3D human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Reconstructing 3d human pose from 2d image landmarks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Ramakrishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeo</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Mocap-guided data augmentation for 3d pose estimation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gr?gory</forename><surname>Rogez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">It&apos;s all relative: Monocular 3d human pose estimation from weakly supervised data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oisin</forename><forename type="middle">Mac</forename><surname>Matteo Ruggero Ronchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Eng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
		<imprint>
			<pubPlace>Alexander C</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Compositional human pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxiang</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Fusing 2d uncertainty and 3d cues for monocular body pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Bugra Tekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>M?rquez-Neila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fua</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Lifting from the deep: Convolutional 3d pose estimation from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denis</forename><surname>Tome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lourdes</forename><surname>Agapito</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Deeppose: Human pose estimation via deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Robust estimation of 3d human poses from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhou</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhouchen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Convolutional pose machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shih-En</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeo</forename><surname>Ramakrishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Zoom better to see clearer: Human and object parsing with hierarchical auto-zoom net</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangting</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A dualsource approach for 3d pose estimation from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hashim</forename><surname>Yasin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Umar</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bjorn</forename><surname>Kruger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juergen</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Xiaoyan Hu, and Kostas Daniilidis. 3d shape estimation from 2d landmarks: A convex relaxation approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spyridon</forename><surname>Leonardos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Sparseness meets deepness: 3d human pose estimation from monocular video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spyridon</forename><surname>Leonardos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Konstantinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Derpanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Monocap: Monocular human motion capture using a cnn coupled with a geometric prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spyridon</forename><surname>Leonardos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kostantinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Derpanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Daniilidis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.02354</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Deep kinematic pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV 2016 Workshops</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Weaklysupervised transfer for 3d human pose estimation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyang</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
