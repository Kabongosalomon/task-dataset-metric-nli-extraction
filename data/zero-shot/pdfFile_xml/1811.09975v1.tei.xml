<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Sequential Variational Autoencoders for Collaborative Filtering</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noveen</forename><surname>Sachdeva</surname></persName>
							<email>noveen.sachdeva@research.iiit.ac.in</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giuseppe</forename><surname>Manco</surname></persName>
							<email>giuseppe.manco@icar.cnr.it</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ettore</forename><surname>Ritacco</surname></persName>
							<email>ettore.ritacco@icar.cnr.it</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vikram</forename><surname>Pudi</surname></persName>
							<email>vikram@iiit.ac.in</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">International Institute of Information Technology Hyderabad</orgName>
								<address>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">ICAR-CNR</orgName>
								<address>
									<settlement>Rende</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">ICAR-CNR</orgName>
								<address>
									<settlement>Rende</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">International Institute of Information Technology Hyderabad</orgName>
								<address>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Sequential Variational Autoencoders for Collaborative Filtering</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T03:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>CCS CONCEPTS ? Information systems ? Collaborative filtering</term>
					<term>Recommender systems</term>
					<term>? Computing methodologies ? Supervised learn- ing</term>
					<term>Neural networks</term>
					<term>Latent variable models</term>
					<term>Ranking</term>
					<term>KEYWORDS Variational Autoencoders, Recurrent Networks, Sequence modeling</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Variational autoencoders were proven successful in domains such as computer vision and speech processing. Their adoption for modeling user preferences is still unexplored, although recently it is starting to gain attention in the current literature. In this work, we propose a model which extends variational autoencoders by exploiting the rich information present in the past preference history. We introduce a recurrent version of the VAE, where instead of passing a subset of the whole history regardless of temporal dependencies, we rather pass the consumption sequence subset through a recurrent neural network. At each time-step of the RNN, the sequence is fed through a series of fully-connected layers, the output of which models the probability distribution of the most likely future preferences. We show that handling temporal information is crucial for improving the accuracy of the VAE: In fact, our model beats the current state-of-the-art by valuable margins because of its ability to capture temporal dependencies among the user-consumption sequence using the recurrent encoder still keeping the fundamentals of variational autoencoders intact.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>The growing diffusion of Web-based services allows an increasingly large number of users to access and consume online content. People access web pages, purchase items in e-commerce sites, watch online content through streaming services, or interact within social networks and media. Understanding the factors that characterize user preferences and shape their future behavior is crucial in order to provide users with a better experience through the recommendation of new content and data that users are likely to appreciate.</p><p>Collaborative filtering approaches to recommendation were extensively investigated by the current literature <ref type="bibr" target="#b1">[2]</ref>. Among these, latent variable models <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b47">48]</ref> gained substantial attention, due to their capabilities in modeling the hidden causal relationships * Work done while in internship at ICAR-CNR. that ultimately influence user preferences. Recently, however, new approaches based on neural architectures (see <ref type="bibr" target="#b51">[52]</ref> for a comprehensive survey) were proposed, achieving competitive performance with respect to the current state of the art. Also, new paradigms based on the combination of deep learning and probabilistic latent variable modeling <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b35">36]</ref> were proven quite successful in domains such as computer vision and speech processing. However, their adoption for modeling user preferences is still unexplored, although recently it is starting to gain attention <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b25">26]</ref>.</p><p>The aforementioned approaches rely on the "bag-of-word" assumption: when considering a user and her preferences, the order of such preferences can be neglected and all preferences are exchangeable. This assumption works with general user trends which reflect a long-term behavior. However, it fails to capture the shortterm preferences that are specific of several application scenarios, especially in the context of the Web. Sequential data can express causalities and dependencies that require ad-hoc modeling and algorithms. And in fact, efforts to capture this notion of causality have been made, both in the context of latent variable modeling <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b43">44]</ref> and deep learning <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b49">50]</ref>.</p><p>In this paper, we consider the task of sequence recommendation from the perspective of combining deep learning and latent variable modeling. Inspired by the approach in <ref type="bibr" target="#b25">[26]</ref>, we assume that at a given timestamp the choice of a given item is influenced by a latent factor that models user trends and preferences. However, the latent factor itself can be influenced by user history and modeled to capture both long-term preferences and short-term behavior. As a matter of fact, the recent studies in the recurrent neural network literature <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b45">46]</ref> demonstrate the capabilities of these architectures in capturing both long-term and short-term relationships. It is hence natural to exploit them in a variational setting.</p><p>Our contribution can be summarized as follows.</p><p>? We review the framework of Variational Autoencoders and discuss its adoption for the problem of modeling implicit preferences in a collaborative filtering scenario. ? We extend the framework to the case of sequential recommendation, where user's preferences exhibit temporal dependencies. In particular, we discuss different modeling alternatives for tackling this task through the adoption of recurrent neural network architectures that model latent dependencies at different abstraction levels.</p><p>? We evaluate the proposed framework on standard benchmark datasets, by showing that (a) approaches not considering temporal dynamics are not totally adequate to model user preferences, and (b) the combination of latent variable and temporal dependency modeling produces a substantial gain, even with regard to other approaches that only focus on temporal dependencies through recurrent relationships.</p><p>The rest of the paper is organized as follows. Section 2 discusses the recent contributions in the current literature and provides a systematic review of the approaches related to our task of interest. Sections 3 and 4 propose the modeling of user preferences in a variational setting, by illustrating how the general framework can be adapted to the case of temporally ordered dependencies. The effectiveness of the proposed modeling is illustrated in section 5, and pointers to future developments are discussed in section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Recommender systems have been extensively studied over the last two decades. Within the collaborative filtering framework, recommendation is essentially modeled as a prediction problem: Given a user and an item, we would like to predict the preference of the user for that item, exploiting the user's past choices, i.e. her history.</p><p>Most approaches disregard the temporal order of the preferences in a user's history. Among these, latent variable models <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b47">48]</ref> were proven extremely effective in modeling user preferences and providing reliable recommendations. Essentially, these approaches embed users and items into latent spaces that translate relatedness into geometrical closeness. The latent embeddings can be used to decompose the large sparse preference matrix <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b38">39]</ref>, to devise item similarity <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b28">29]</ref>, or more generally to parameterize probability distributions for item preference <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b47">48]</ref> and sharpen the prediction quality by means of meaningful priors.</p><p>The recent literature is currently focusing on deep learning, which shows substantial advantages over traditional approaches. For example, Neural Collaborative Filtering (NCF) <ref type="bibr" target="#b16">[17]</ref> generalizes matrix factorization to a non-linear setting, where users, items and preferences are modeled through a simple multilayer perceptron network that exploits latent factor transformations.</p><p>Notably, prominent deep learning approaches to collaborative filtering are based on the idea of autoencoding the features from the preference matrix. AutoRec <ref type="bibr" target="#b39">[40]</ref> exploits autoencoders to encode preference histories. Unseen preferences can be devised by looking at the reconstructed decoding, which is shaped to include scores for all possible items of interest. Autoencoders are also amenable to consider side information <ref type="bibr" target="#b40">[41]</ref> to mitigate the sparsity of the data and to tackle the cold start problem.</p><p>Hybrid approaches that integrate latent variable modeling and deep learning have also gained attention. Collaborative Deep Learning <ref type="bibr" target="#b48">[49]</ref> embeds a stacked denoising autoencoder <ref type="bibr" target="#b46">[47]</ref> into a Bayesian matrix factorization setting. Similarly, AutoSVD++ <ref type="bibr" target="#b52">[53]</ref> exploits the notion of contractive autoencoder <ref type="bibr" target="#b36">[37]</ref> to learn latent item representations that are integrated into the SVD++ model <ref type="bibr" target="#b23">[24]</ref>.</p><p>The introduction of the variational autoencoding framework <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b35">36]</ref> has suggested a tighter coupling between deep learning and latent variable modeling. Collaborative Variational Autoencoder (CVA) <ref type="bibr" target="#b24">[25]</ref> and Hybrid Variational Autoencoder (HVAE) <ref type="bibr" target="#b13">[14]</ref> exploit side information to feed a variational autoencoder whose goal is to produce a latent representation of the items. In CVA, the preference matrix is hence modeled by combining user and item embeddings with the item latent representations, while HVAE uses another variational autoencoder to reproduce the whole users' preference history. By contrast, <ref type="bibr" target="#b25">[26]</ref> proposes a neural generative model where a user's history is modeled through a multinomial likelihood conditioned to a latent user representation which in turn is modeled through a variational autoencoder.</p><p>Within the context of collaborative filtering, a strong effort has also been made to model temporal dynamics within the history of user preferences <ref type="bibr" target="#b30">[31]</ref>. The Factorizing Personalized Markov Chain model (FPMC) <ref type="bibr" target="#b34">[35]</ref>, for example, proposes a combination of matrix factorization and Markov chains. FPMC considers personalized first-order transition probabilities between items, which are modeled by decomposing the underlying tensor through user and item embeddings. Transition probabilities can also be measured by exploiting more sophisticated modeling <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16]</ref>, where users are mapped into translation (latent) vectors operating on item sequences and consequently a transition corresponds to a geometric affinity of these latent vectors. Orthogonally, Markov dependencies can also be exploited to model dependencies between latent variables <ref type="bibr" target="#b5">[6]</ref>, thus resulting in richer formalizations and more accurate recommendations.</p><p>Recently, a revamped interest for sequence-based recommendation has taken place, motivated by both the success of recurrent neural networks <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b12">13]</ref> in domains such as language modeling, and the need to focus on session-based recommendations <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b41">42]</ref>, i.e. recommendation that do not rely on a user model and instead can cope with single anonymous preference sessions.</p><p>GRU4Rec <ref type="bibr" target="#b17">[18]</ref> proposes a recurrent neural network model based on Gated Recurrent Units to predict the next item in a user session, based on the history seen so far. Since its introduction, this model has witnessed several evolutions, and similar architectures were proposed in <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b50">51]</ref>. Recurrent networks were also exploited to strengthen matrix factorization, by producing history-aware embeddings of users and items. The RRN approach proposed in <ref type="bibr" target="#b49">[50]</ref> combines two recurrent networks, whose output at any time-step, relative to a specific user and item, can be hence exploited to predict the current preference.</p><p>Finally, the CASER (Convolutional Sequence Embedding Recommendation) model <ref type="bibr" target="#b42">[43]</ref> proposes an approach that departs from RNN modeling and instead exploits a convolutional neural network, by transforming a sequence into a matrix built from the concatenation of the embeddings of the items appearing in the sequence. The matrix can hence feed convolutional layers that can extract relevant useful features for predicting the next items.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">BACKGROUND</head><p>The framework of variational autoencoders draws from the idea of latent variable modeling <ref type="bibr" target="#b27">[28]</ref>. Essentially, we can assume a Kdimensional latent variable space Z, upon which we can devise a probability density function P ? (z) for z ? Z, where ? represents a set of density parameters. The datapoints X = {x 1 , . . . , x M } we observe can be modeled through a dependency P ? (x|z), so that the overall likelihood of X can be specified by marginalizing over Z:</p><formula xml:id="formula_0">P(X) = i P(x i ) = i ? P ? (x i |z) P ? (z) dz .</formula><p>Within the classical VAE framework <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b35">36]</ref>, z is assumed to be distributed according to a standard normal distribution, that is ? = {0, I K } and consequently z ? N (0, I K ). The key intuition here is that even complex dependencies can be generated starting from normally distributed variables. Thus, P ? (x i |z) is parameterized by the function f ? (z), representing any (unknown) transformation of z that expresses such a dependency. We can model this transformation through a neural network, so that ? represents the set of network parameters to be optimized. Hence, we can model the inference problem for P(X) as optimization problem, where we aim at finding the optimal parameter set ? that maximize P(X).</p><p>However, P(X) is typically intractable and we need to find an approximation. Variational inference <ref type="bibr" target="#b7">[8]</ref> usually tackles this approximation by introducing a proposal distribution Q(z|x), which approximates the true posterior P(z|x). The relationship between the likelihood and the proposal distribution is given by the following equation:</p><formula xml:id="formula_1">log P(x) = ? Q(z|x) log P(x) dz = ? Q(z|x) log P(x|z)P(z) P(z|x) dz = ? Q(z|x) log P(X |z) dz + ? Q(z|x) log Q(z|x) P(z|x) dz ? ? Q(z|x) log Q(z|x) P(z) dz = E z?Q [log P(x|z)] + KL (Q(z|x)?P(z|x)) ? KL (Q(z|x)?P(z)) .</formula><p>By rearranging, we obtain</p><formula xml:id="formula_2">log P(x) ? KL [Q(z|x)?P(z|x)] = E z?Q [log P(x|z)] ? KL [Q(z|x)?P(z)] . (3.1)</formula><p>This equation is crucial to the development variational autoencoders. The left-hand side represents the term to maximize, plus an error term due to approximating the true posterior P(z|x) with Q(z|x). A good choice of Q gives a small (hopefully zero) approximation error, and consequently allows to directly optimize the likelihood. The right-hand side of the equation (called Evidence Lower Bound, ELBO) represents the equivalent of the left-hand side, but with the added bonus that, for a given choice of Q it becomes tractable and amenable to optimization. In particular, by noticing that</p><formula xml:id="formula_3">KL [N (?, ?)?N (m, S)] = 1 2 lo? |S| |?| ? K + tr S ?1 ? + (m ? ?) T S ?1 (m ? ?) ,</formula><p>we can choose to model Q(z|x) = N (z|? ? (x), ? ? (x)), so that the term KL [Q(z|x)?P(z)] has a closed form. Again, we can model the parameters ? ? (x) and ? ? (x) through a neural network trained on x and parameterized by ?. A particular case that we shall use throughout this paper is given by</p><formula xml:id="formula_4">? ? (x) = diag ? ?,1 (x), . . . , ? ?, K (x) .</formula><p>As a consequence, the modeling can resort to two neural networks: the first one for "encoding" an input x into a latent variable z by means of Q ? (z|x). The second one "decodes" the latent representation z into the corresponding x by means of P ? (x|z). The learning process is governed by the loss function given by the right-hand side of eq. 3.1, computed on the training data X.</p><p>A problem with this loss is that term E z?Q [log P(x|z)] is typically approximated by sampling the values z according to Q(z|x). However, the sampling is a nondeterministic function that depends on the parameters ? ? and ? ? (which in turn depend on ?) and is not differentiable. To overcome this, <ref type="bibr" target="#b21">[22]</ref> propose a reparametrization trick: instead of sampling z, we can sample an auxiliary noise variable ? according to a fixed distribution P(?), and obtain z by means of a differentiable transformation depending on ?, ? and x. Specifically, we can sample gaussian noise ? ? N (0, I ), and obtain</p><formula xml:id="formula_5">z ? (?, x i ) = ? ? (x i ) + ? ? (x i ) ? ?, normally distributed according to parameters ? ? and ? ? .</formula><p>To summarize, given the loss function</p><formula xml:id="formula_6">L(?, ?; X) = i 1 2 k ? ?,k (x i ) ? 1 ? log ? ?,k (x i ) + ? ?,k (x i ) 2 ? E ? ?N(0,I ) log P ? (x i |z ? (?, x i )) ,<label>(3.2)</label></formula><p>we can learn the parameters ? and ?, and consequently the encoder Q ? (z|x) and the decoder P ? (x|z).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">VARIATIONAL AUTOENCODERS FOR USER PREFERENCES</head><p>The general framework described in the previous section can be instantiated to the case of collaborative filtering by specifying the x variables, and consequently the probability density P ? (x|z). We analyze some alternative models here. We shall use the following shared notation: u ? U = {1, . . . , M } indexes a user and i ? I = {1, . . . , N } indexes an item for which the user can express a preference. We model implicit feedback, thus assuming a preference matrix X ? {0, 1} N ?M , so that x u represents the (binary) row with all the item preferences for user u. Given x u , we define</p><formula xml:id="formula_7">I u = {i ? I |x u,i = 1} (with N u = |I u |). Analogously, U i = |{u ? U |x u,i = 1}| and M i = |U i |.</formula><p>We also consider a precedence and temporal relationships within X. First of all, the preference matrix induces a natural ordering relationship between items: i ? u j has the meaning that x u,i &gt; x u, j in the rating matrix. Also, we assume the existence of timing information T ? IR M ?N + ? {?}, where the term t u,i represents the time when i was chosen by u (with t u,i = ? if x u,i = 0). Then, i &lt; u j denotes that t u,i &lt; t u, j , With an abuse of notation, we also introduce a temporal mark in the elements of x u : the term</p><formula xml:id="formula_8">x u(t ) (with 1 ? t ? N u ) represents the t-th item in I u in the sorting induced by &lt; u , whereas x u(1:t ) represents the sequence x u(1) , . . . , x u(t ) .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Multinomial model</head><p>The reference model is the Multinomial variational autoencoder (MVAE) proposed in <ref type="bibr" target="#b25">[26]</ref>. Within this framework, for a given user u it is possible to devise x u according to the generative setting:</p><formula xml:id="formula_9">z u ? N (0, I K ) , ? (z u ) ? exp f ? (z u ) , x u ? Multi (N u , ? (z u )) . (4.1)</formula><p>The underlying "decoder" is modeled by</p><formula xml:id="formula_10">log P ? (x u |z u ) = i x u,i log ? i (z u ) ,</formula><p>thus enabling a complete specification of the overall variational framework. Prediction for new items is accomplished by resorting to the learned functions f ? and Q ? : given a user history x, we compute z = ? ? (x) and then devise the probabilities for the whole item set through ? (z). Unseen items can then be ranked according to their associated probabilities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Ranking Models</head><p>A different formulation is inspired by the Bayesian Personalized Ranking (BPR) model introduced in <ref type="bibr" target="#b33">[34]</ref>. Here, we explicitly model an order among observations, so that for each triplet u, i, j we can devise whether i ? u j. In classical BPR, this is modeled by associating a factorization rank p T u q i for each pair (u, i), and requiring that p T u q i &lt; p T u q j whenever i ? u j. In a VAE setting, p T u q i is replaced by a score based on the latent gaussian variable z u,i :</p><formula xml:id="formula_11">z u,i ? N (0, I K ) , i ? u j ? Bernoulli ? (z u,i , z u, j ) ,<label>(4.2)</label></formula><p>Two different specifications for ? (z u,i , z u, j ) are possible: either we let the neural network implicitly models the comparison between i and j ? (z u,i ,</p><formula xml:id="formula_12">z u, j ) ? ? f ? (z u,i , z u, j ) ,<label>(4.3)</label></formula><p>or alternatively, we can assume that the network simply provides the score upon which to compare:</p><formula xml:id="formula_13">? (z u,i , z u, j ) ? ? f ? (z u, j ) ? f ? (z u,i ) . (4.4)</formula><p>Here, ? represents the standard logistic function ? (z) = 1/(1 + exp{?z}). These alternatives represent two different instantiations of the Ranking Variational Autoencoder. The difference between them is substantial at prediction time: given u and a set J ? I of unseen items, in both models we need to devise z u,i = ? ? (x u |x u,i = 1) for each i ? J . However, the instance of eq. 4.3 requires to organize the data as triplets u, i, j and to compute P ? (i ? u j) for each triplet. These probabilities can be then exploited to devise an order among the items in U . Inconsistencies can arise in this setting, which can in principle prevent to produce a global rank order.</p><p>Conversely, the instance of eq. 4.4 can directly sort items according to f ? (z u,i ) and does not produce inconsistencies, thus being better suited for ranking items. In the following we shall only focus on this instance, that we shall refer to as RVAE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Sequential Models</head><p>The basic framework proposed in section 3 can also be exploited to model time-aware user preferences. Ideally, latent variable modeling should be able to express temporal dynamics and hence causalities and dependencies among preferences in a user's history. In the following we elaborate on this intuition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Modeling</head><p>History-aware User Preferences. Within a probabilistic framework, we can model temporal dependencies by conditioning each event to the previous events: given a sequence x (1:T ) , we have</p><formula xml:id="formula_14">P x (1:T ) = T ?1 t =0 P x (t +1) |x (1:t ) .</formula><p>This specification suggests two key aspects:</p><p>? There is a recurrent relationship between x (t +1) and x (1:t ) devised by P x (t +1) |x <ref type="bibr">(1:t )</ref> , upon which the modeling can take advantage, and ? each time-step can be handled separately, and in particular it can be modeled through a conditional VAE.</p><p>Let us consider the following (simple) generative model</p><formula xml:id="formula_15">z u(t ) ? N (0, I K ) , ? z u(t ) ? exp f ? z u(t ) , x u(t ) ? Multi 1, ? z u(t ) ,<label>(4.5)</label></formula><p>which results in the joint likelihood</p><formula xml:id="formula_16">P(x u(1:T ) , z u(1:T ) ) = t P(x u(t ) |z u(t ) )P(z u(t ) )</formula><p>.</p><p>Here, we can approximate the posterior P(z u(1:T ) |x u(1:T ) ) with the factorized proposal distribution</p><formula xml:id="formula_17">Q ? (z u(1:T ) |x (1:T ) ) = t q ? (z u(t ) |x (1:t ?1) ) ,</formula><p>where q ? (z u(t ) |x (1:t ?1) ) is a gaussian distribution whose parameters ? ? (t) and ? ? (t) depend upon the current history x u(1:t ?1) , by means of a recurrent layer h t :</p><formula xml:id="formula_18">? ? (t), ? ? (t) = ? ? (h t ) h t = RNN ? h t ?1 , x u(t ?1) . (4.6)</formula><p>The resulting loss function follows directly from eq. 3.2:</p><formula xml:id="formula_19">L(?, ?, X) = u N u t =1 1 2 k ? ?,k (t) ? 1 ? log ? ?,k (t) + ? ?,k (t) 2 ? E ? ?N(0,I ) log P ? x u(t ) |z ? (?, t) .</formula><p>The proposal distribution introduces a dependency of the latent variable from a recurrent layer, which allows to recover the information from the previous history. We call this model SVAE. <ref type="figure" target="#fig_0">Figure 1</ref> shows the main architectural difference with respect to the models proposed so far. In SVAE, we can observe the recurrent relationship occurring in the layer upon which z u(t ) depends.</p><p>Notably, the prediction step can be easily accomplished in a similar way as for MVAE: given a user history x u(1:t ?1) , we can resort to eq. 4.6 and set z = ? ? (t), upon which we can devise the probability for the x u(t ) by means of ? (z).   </p><formula xml:id="formula_20">x (t) ht ht 1 z (t) x (t) ht ht 1 z (t) x (t) ht ht 1 z (t) x (t) ht ht 1 z (t) 1 P (x (1:T ) )= ? P (z (1:T ) ) T ?1 t =1 P ? (x (t +1) |z (t +1) , x (1:t ) ) dz (1:T ) (a) Single latent dependency x (t) ht ht 1 z (t) x (t) ht ht 1 z (t) x (t) ht ht 1 z (t) x (t) ht ht 1 z (t) 1 P (x (1:T ) )= ? P (z (1:T ) ) T ?1 t =1 P ? (x (t +1) |z (1:t ) , x (1:t ) ) dz (1:T ) (b) Multiple latent dependencies x (t) ht ht 1 z (t) x (t) ht ht 1 z (t) x (t) ht ht 1 z (t) x (t) ht ht 1 z (t) 1 P (x (1:T ) )= ? T ?1 t =1 P ? (x (t +1) |z (t +1) )P(z (t +1) |x (1:t ) ) dz (1:T ) (c) Recurrent latent dependency x (t) ht ht 1 z (t) x (t) ht ht 1 z (t) x (t) ht ht 1 z (t) x (t) ht ht 1 z (t)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">A taxonomy of sequential variational autoencoders.</head><p>We already discussed that within a sequence modeling framework, the core of the approach is on modeling P x (t +1) |x (1:t ) through a conditional variational autoencoder. SVAE describes just one of several possible modeling choices. In fact, four alternate formalizations can take place, as illustrated in <ref type="figure" target="#fig_2">fig. 2</ref>:</p><p>? The simplest modeling, considers a single gaussian variable z and a parameterization of the conditional distribution</p><formula xml:id="formula_21">P ? x (t +1) |z, x (1:t ) with a function f ? (z, h t +1 ), where h t</formula><p>represents the hidden state of a recurrent neural function</p><formula xml:id="formula_22">h t = RNN h t ?1 , x (t ) .</formula><p>(4.7) <ref type="figure" target="#fig_2">Figure 2a</ref> illustrates the graphical model and the sequence likelihood. ? Following <ref type="bibr" target="#b6">[7]</ref>, we can alternatively introduce t independent gaussian variables and parameterize the conditional likelihood P ? x (t +1) |z (1:t ) , x (1:t ) by means of a function f ? (h t +1 ), where again h t represents the hidden state of a recurrent neural function</p><formula xml:id="formula_23">h t = RNN h t ?1 , x (t ) , z (t ) .</formula><p>(4.8) <ref type="figure" target="#fig_2">Figure 2b</ref> illustrates the graphical model and the sequence likelihood. ? So far, the modeling combines history and latent variables to define the conditional distribution. An alternative consists in assuming that history affects the latent variable z instead. In practice, the conditional likelihood P ? x (t +1) |z (t +1) would only depend on the latent variable, which exhibits a prior distribution P z (t +1) |x (1:t ) , modeled as a gaussian with parameters depending on the current state h t of the network, devised as in eq. 4.7. The graphical model and sequence likelihood are shown in <ref type="figure" target="#fig_2">fig. 2c</ref> and it resembles the SVAE model discussed above. ? Finally, <ref type="bibr" target="#b10">[11]</ref> propose a comprehensive model, where the gaussian latent variables t + 1 also depend on each other through the Markovian dependency P z (t +1) |z (1:t ) , x (1:t ) .</p><p>Both this dependency and P ? x (t +1) |z (1:t +1) , x (1:t ) can be specified by the hidden state h t of a recurrent network, devised as in eq. 4.8. The graphical model and sequence likelihood are shown in <ref type="figure" target="#fig_2">fig. 2d</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.3">Extending SVAE.</head><p>The generative model of eq. 4.5 only focuses on the next item in the sequence. The base model however is flexible enough to extend its focus on the next k items, regardless of the time:</p><formula xml:id="formula_24">x u(t :t +k ?1) ? Multi k, ? z u(t ) ,</formula><p>Again, the resulting joint likelihood can be modeled in different ways. The simplest way consists in considering x as a time-ordered multi-set,</p><formula xml:id="formula_25">P(x u(1:T ) , z u(1:T ) ) = t P(x u(t :t +k ?1) |z u(t ) )P(z u(t ) ) . (4.9)</formula><p>Alternatively, we can consider the probability of an item as a mixture relative to all the time-steps where it is considered:</p><formula xml:id="formula_26">P(x u(1:T ) , z u(1:T ) ) = t P(z u(t ) ) t ?k +1?j ?t P(x u(t ) |z u(j) ), (4.10)</formula><p>where P(x u(t ) |z u(j) ) is the probability of observing x u(t ) according to ? (z u(j) ). In both cases, the variational approximation is modeled exactly as shown above, and the only difference lies in the second component of the loss function, which has to be adapted according to the above equations.</p><p>There is an interesting analogy between eq. 4.10 and the attention mechanism <ref type="bibr" target="#b45">[46]</ref>. In fact, it can be noticed in the equation that the prediction of x u(t ) depends on the latent status of the k previous steps in the sequence. In practice, this enables to capture short-term dependencies and to encapsulate them in the same probabilistic framework by weighting the likelihood based on z u(t ?k +1) , . . . , z u(t ) .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EVALUATION</head><p>We evaluate SVAE on some benchmark datasets, by comparing with various baselines and the current state-of-the-art competitors, in order to assess its capabilities in modeling preference data. Additionally, we provide a sensitivity analysis relative to the configurations/contour conditions upon which SVAE is better suited. The main highlight from our experiments is that SVAE provides a huge edge over the current state-of-the-art for the task of top-N recommendation across various metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Datasets</head><p>We evaluate our model along with the competitors on two popular publicly available datasets, namely Movielens-1M and Netflix. Movielens-1M is a time series dataset containing user-item ratings pairs along with the corresponding timestamp. Since we work on implicit feedback, we binarize the data, by considering only the user-item pairs where the rating provided by the user was strictly greater than 3 on a range 1:5. Netflix has the same data format as Movielens-1M and the same technique is used to binarize the ratings. We use a subset of the full dataset that matches the userdistribution with the full dataset. The subset is built by stratifying users according to their history length, and then sampling a subset of size inversely proportional to the size of the strata. <ref type="figure" target="#fig_4">Figure 3</ref> compares the distributions of the full and the sampled dataset: We</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ML-1M</head><p>Netflix-Full Netflix- <ref type="table" target="#tab_1">Subset  # of users  6K  463K  75K  # of items  3,533  17,</ref>  can notice that the distributions share the same shape, but in the sample users with small history length are undersampled whereas users with large histories are kept. <ref type="table" target="#tab_1">Table 1</ref> shows the basic statistics of the data. For illustration purposes, we also show the basic statistics of the full Netflix dataset. We can see that the average length of sequences in the Netflix subset is significantly increased, as a result of downsampling users with small history length. Also, notice that the sampling procedure does not affect the number of items. To preprocess the data, we first group the interacted items for each user, and ignore the users who have interacted with less than five items. After preprocessing, we split the remaining users into train, validation and test sets.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Evaluation Metrics and protocol</head><p>Since we are considering implicit preferences, the evaluation is done on top-n recommendation, and it relies on the following metrics.</p><p>Normalized Discounted Cumulative Gain. Also abbreviated as NDCG@n, the metric gives more weight to the relevance of items on top of the recommender list and is defined as</p><formula xml:id="formula_27">NDCG@n = DCG@n IDCG@n where DCG@n = n i=1 r i lo? 2 (i + 1) and IDCG@n = |R| i=1 1 lo? 2 (i + 1)</formula><p>.</p><p>Here, r i is the relevance (either 1 or 0 within the implicit feedback scenario) of the i-th recommended items in the recommendation list, and R is the set of relevant items. Precision. By defining Hits = i r i as the number of items occurring in the recommendation list that were actually preferred by the user, we have Precision@n = Hits@n n Recall, defined as the percentage of items actually preferred by the user that were present in the recommendation list:</p><formula xml:id="formula_28">Recall@n = Hits@n |R|</formula><p>In our experiments, we use the above metrics with two values of n, respectively 10 and 100. The evaluation protocol works as follows. We partition users into training, validation and test set. The model is trained using the full histories of the users in the training set. During evaluation, for each user in the validation/test set we split the time-sorted user history into two parts, fold-in and fold-out splits. The fold-in split is used as a basis to learn the necessary representations and provide a recommendation list which is then evaluated with the fold-out split of the user history using the metrics defined above. We believe that this strategy is more robust when compared to other methodologies wherein the same user can be in both the training as well as testing sets. <ref type="table" target="#tab_1">Table 1</ref> shows the number of heldout users for each datasets.</p><p>It is worth noticing that Liang et. al <ref type="bibr" target="#b25">[26]</ref> follow a similar strategy but with a major difference: they do not consider any sorting for user histories. That is, for the validation/test users, the fold-in set doesn't precede the fold-out with respect to time. By contrast, we keep the fold-in set to be the first 80% of the time-sorted user history, and the last 20% represents the fold-out set. We shall see in the following sections that this difference is substantial in the evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Competitors</head><p>We compare our model with various baselines and current state-ofthe-art models including recently published neural architectures and we now present a brief summary about our competitors to provide a better understanding of these models.</p><p>? POP is a simple baseline where users are recommended the most popular items in the training set. ? BPR, already mentioned in section 4.2, is a state of the art model based on Matrix Factorization, which ranks items differently for each user <ref type="bibr" target="#b33">[34]</ref>. There is a subtle issue concerning BPR: by separating users on training/validation/test as discussed above, the latent representation of users in the validation/test is not meaningful. That, is, BPR is only capable of providing meaningful predictions for users that were already exploited in the training phase. To solve this, we extended the training set to include the partial history in fold-in for each user in the validation/test. The evaluation still takes place on their corresponding fold-out sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>? FPMC [35] is a model which clubs both Matrix Factorization</head><p>and Markov Chains together using personalized transition graphs over underlying Markov chains. ? CASER <ref type="bibr" target="#b42">[43]</ref>, already discussed in section 2, is a convolutional model that uses vertical and horizontal convolutional layers to embed a sequence of recent items thereby learning sequential patterns for next-item recommendation. The authors have shown that this model outperforms other approaches based on recurrent neural network modeling, such as GRU4Rec. We use the implementation provided by the authors and tune the network by keeping the number of horizontal filters to be 16, and vertical filters to be 4. ? MVAE, discussed in section 4.1, from which the SVAE model draws heavily. We use the implementation provided by the authors, with the default hyperparameter settings.</p><p>We also include the RVAE model proposed in section 4.2, that we consider a baseline here. Notably, despite being considered a simple extension of the BPR model, RVAE relies on a neural layer for embedding users: as a consequence, it does a better job in ranking items for the users which the model has never seen before, contrary to BPR. In practice, RVAE upgrades BPR to session-based recommendations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Training Details</head><p>The experiments only consider the SVAE model illustrated in subsection 4.3.1 and the extensions of subsection 4.3.3. We reserve a more detailed analysis of the extensions discussed in subsection 4.3.2 to future work. The model is trained end-to-end on the full histories of the training users. Model hyperparameters are set using the evaluation metrics obtained on validation users.</p><p>The SVAE architecture includes an embedding layer of size 256, a recurrent layer realized as a GRU with 200 cells, and two encoding layers (of size 150 and 64) and finally two decoding layers (again, of size 64 and 150). We set the number K of latent factors for the variational autoencoder to be 64. Adam <ref type="bibr" target="#b22">[23]</ref> was used to optimize the loss function coupled with a weight decay of 0.01. As for RVAE, the architecture includes user/item embedding layers (of size 128), two encoding layers (size 100 and 64), and a final layer that produces the score f ? (z u,i ). Both SVAE and RVAE were implemented in PyTorch <ref type="bibr" target="#b29">[30]</ref> and trained on a single GTX 1080Ti GPU. The source code is available on GitHub 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Results</head><p>In a first set of experiments, we compare SVAE with all competitors described above. <ref type="table">Table 2</ref> shows the results of the comparison. SVAE consistently outperforms the competitors on both datasets with a significant gain on all the metrics. It is important here to highlight how the temporal fold-in/fold-out split is crucial for a fair evaluation of the predictive capabilities: MVAE was evaluated both on temporal and random split, exhibiting totally different performances. Our interpretation is that, with random splits, the prediction for an item is easier if the encoding phase is aware of forthcoming items in the same user history. This severely affects the performance and overrates the predictive capabilities of a model: In fact, the accuracy of MVAE drops substantially when a temporal split is considered.</p><p>By contrast, SVAE is trained to capture the actual temporal dependencies and ultimately results in better predictive accuracy. This is also shown in <ref type="figure" target="#fig_5">fig. 4</ref>, where we show that SVAE consistently outperforms the competitors irrespective of the size of fold-in. The only exception is with very short sequences (less than 10 items), where MVAE gets better results with respect to the sequential models. It is also worth noticing how the performance of both sequential models tend to degrade with increasing sequences, but SVAE maintains its advantage over CASER.  <ref type="table">Table 2</ref>: Results of the evaluation (in percentage). MVAE * considers random splits that disregards the temporal order of user history. BPR * relies on including the fold-in subsequences in the training phase. We discussed in section 4.3.3 how the basic SVAE framework can be extended to focus on predicting the next k items, rather then just the next item. We analyse this capability in <ref type="figure" target="#fig_6">fig. 5</ref>, where the accuracy for different values of k is considered according to the modeling in 4.9. On Movielens, the best value is achieved for k = 4, and acceptable values range within the interval 1 ? 10.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSIONS AND FUTURE WORK</head><p>Combining the representation power of latent spaces, provided by variational autoencoders, with the sequence modeling capabilities of recurrent neural networks is an effective strategy to sequence recommendation. To prove this, we devised SVAE, a simple yet robust mathematical framework capable of modeling temporal dynamics upon different perspectives, within the fundamentals of variational autoencoders. The experimental evaluation highlights the capability of SVAE to consistently outperform state-of-the-art models.</p><p>The framework proposed here is worth further extensions that we plan to accomplish in a future work. From a conceptual point of view, we need to perform a thorough analysis of the taxonomy defined in section 4.3.2. From an architectural point of view, the attention mechanism, outlined in section 4.10, requires a better understanding and a more detailed analysis of its possible impact in view of the recent developments <ref type="bibr" target="#b2">[3]</ref> in the literature. Also, the SVAE framework relies on recurrent networks. However, different architectures (e.g. based on convolution <ref type="bibr" target="#b42">[43]</ref> or translation invariance <ref type="bibr" target="#b14">[15]</ref>) are worth being investigated within a probabilistic variational setting.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Variational architectures. Terms with hat represent decoding reconstruction. Dotted boxes represent neural layers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>1 P? T ? 1 t</head><label>11</label><figDesc>(x (1:T ) )= =1 P ? (x (t +1) |z (1:t +1) , x (1:t ) )P(z (t +1) |z (1:t ) , x (1:t ) ) dz (1:T ) (d) Global recurrent dependency</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Recurrent Variational Autoencoders. Diamond boxes represent deterministic variables.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Comparison of the user distributions of the full Netflix dataset and the subsample produced by progressive stratified sampling.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Average NDCG@100 for MVAE, CASER &amp; SVAE across various history lengths.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>NDCG@100 values for SVAE on Movielens, across different sizes for the number of items forward in time to predict on.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Finally, in</head><label></label><figDesc>fig. 6we analyse the convergence rate of SVAE, in terms of NDCG (left y-axis and blue line) and loss function values(right y-axis and red line). The learning phase converges quickly and does not exhibit overfitting: on Movielens, a stable model is reached within 8 epochs, whereas Netflix requires 13 epochs. The average runtime per epoch is 197 seconds on Movielens and 2 hours on Netflix: in practice, the learning scales linearly with the number of interactions in the dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 6 :</head><label>6</label><figDesc>Learning curves on validation data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Basic statistics of the datasets used in the experiments.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>29.93 12.48 49.09 14.40 6.93 24.64 26.77 8.93 35.58 21.93 10.55</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="2">Movielens</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Netflix</cell><cell></cell><cell></cell></row><row><cell>Model</cell><cell cols="2">NDCG</cell><cell cols="2">Recall</cell><cell cols="2">Precision</cell><cell cols="2">NDCG</cell><cell cols="2">Recall</cell><cell cols="2">Precision</cell></row><row><cell></cell><cell>@10</cell><cell>@100</cell><cell>@10</cell><cell>@100</cell><cell>@10</cell><cell>@100</cell><cell>@10</cell><cell>@100</cell><cell>@10</cell><cell>@100</cell><cell>@10</cell><cell>@100</cell></row><row><cell>POP</cell><cell>4.24</cell><cell>11.30</cell><cell>3.41</cell><cell>22.15</cell><cell>3.98</cell><cell>3.14</cell><cell>3.95</cell><cell>7.61</cell><cell>1.29</cell><cell>12.02</cell><cell>4.26</cell><cell>3.95</cell></row><row><cell>BPR ?</cell><cell>8.51</cell><cell>16.66</cell><cell>5.20</cell><cell>29.38</cell><cell>7.27</cell><cell>4.51</cell><cell>8.94</cell><cell>12.68</cell><cell>2.63</cell><cell>19.45</cell><cell>8.45</cell><cell>5.89</cell></row><row><cell>RVAE</cell><cell>7.76</cell><cell>14.02</cell><cell>4.11</cell><cell>23.82</cell><cell>6.39</cell><cell>3.96</cell><cell>4.19</cell><cell>9.46</cell><cell>1.87</cell><cell>17.00</cell><cell>3.87</cell><cell>4.30</cell></row><row><cell>FPMC</cell><cell>5.65</cell><cell>12.06</cell><cell>3.46</cell><cell>23.10</cell><cell>4.77</cell><cell>3.46</cell><cell>11.01</cell><cell>13.55</cell><cell>3.49</cell><cell>20.82</cell><cell>10.12</cell><cell>6.04</cell></row><row><cell>CASER</cell><cell>16.32</cell><cell>27.55</cell><cell>11.35</cell><cell>46.01</cell><cell>13.15</cell><cell>6.45</cell><cell>16.46</cell><cell>19.79</cell><cell>6.33</cell><cell>28.33</cell><cell>14.38</cell><cell>7.54</cell></row><row><cell>MVAE</cell><cell>11.69</cell><cell>23.01</cell><cell>9.12</cell><cell>41.43</cell><cell>9.02</cell><cell>5.39</cell><cell>16.15</cell><cell>22.19</cell><cell>7.47</cell><cell>32.72</cell><cell>13.94</cell><cell>8.31</cell></row><row><cell cols="2">SVAE 17.81 MVAE ? 27.82</cell><cell>39.79</cell><cell>17.46</cell><cell>59.70</cell><cell>23.01</cell><cell>9.59</cell><cell>35.41</cell><cell cols="3">37.70 13.50 47.22</cell><cell>31.39</cell><cell>15.20</cell></row><row><cell>1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/noveens/svae_cf.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">fLDA: Matrix Factorization Through Latent Dirichlet Allocation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third ACM International Conference on Web Search and Data Mining (WSDM &apos;10)</title>
		<meeting>the Third ACM International Conference on Web Search and Data Mining (WSDM &apos;10)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="91" to="100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Recommender Systems: The Textbook</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>Springer Publishing Company</publisher>
		</imprint>
	</monogr>
	<note>Incorporated</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Neural Machine Translation by Jointly Learning to Align and Translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno>abs/1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">An Analysis of Probabilistic Methods for Top-N Recommendation in Collaborative Filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Barbieri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Manco</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joint European Conference on Machine Learning and Knowledge Discovery in Databases (ECML/PKDD &apos;11)</title>
		<meeting>the Joint European Conference on Machine Learning and Knowledge Discovery in Databases (ECML/PKDD &apos;11)</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="172" to="187" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Probabilistic Approaches to Recommendations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Barbieri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Manco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ritacco</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>Morgan &amp; Claypool Publishers</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Probabilistic topic models for sequence data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Barbieri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Manco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ritacco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Carnuccio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bevacqua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="page" from="1" to="25" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Learning Stochastic Recurrent Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Osendorfer</surname></persName>
		</author>
		<idno>abs/1411.7610</idno>
		<ptr target="http://arxiv.org/abs/1411.7610" />
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Variational Inference: A Review for Statisticians</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kucukelbir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Mcauliffe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Amer. Statist. Assoc</title>
		<imprint>
			<biblScope unit="volume">112</biblScope>
			<biblScope unit="page" from="859" to="877" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>G?l?ehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1724" to="1734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>G?l?ehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno>abs/1412.3555</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>Presented at the Deep Learning workshop at NIPS2014</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A Recurrent Latent Variable Model for Sequential Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kastner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Neural Information Processing Systems (NIPS&apos;15</title>
		<meeting>the 28th International Conference on Neural Information Processing Systems (NIPS&apos;15</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Long and Short-Term Recommendations with Recurrent Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Devooght</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bersini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th Conference on User Modeling, Adaptation and Personalization (UMAP &apos;17</title>
		<meeting>the 25th Conference on User Modeling, Adaptation and Personalization (UMAP &apos;17</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="13" to="21" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">LSTM: A Search Space Odyssey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">K</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Koutnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">R</forename><surname>Steunebrink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="2222" to="2232" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">A Hybrid Variational Autoencoder for Collaborative Filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yelahanka Raghuprasad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kumar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.01006</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Translation-based Recommendation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mcauley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM Conference on Recommender Systems (RecSys &apos;17</title>
		<meeting>the ACM Conference on Recommender Systems (RecSys &apos;17</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="161" to="169" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Fusing Similarity Models with Markov Chains for Sparse Sequential Recommendation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mcauley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE 16th International Conference on Data Mining (ICDM &apos;16</title>
		<meeting>the IEEE 16th International Conference on Data Mining (ICDM &apos;16</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="191" to="200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Neural Collaborative Filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th International Conference on World Wide Web (WWW &apos;17</title>
		<meeting>the 26th International Conference on World Wide Web (WWW &apos;17</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="173" to="182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Session-based Recommendations with Recurrent Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hidasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karatzoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Baltrunas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tikk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR &apos;16)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Latent semantic models for collaborative filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hofmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Information Systems</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="89" to="115" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">When Recurrent Neural Networks Meet the Neighborhood for Session-Based Recommendation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jannach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ludewig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM Conference on Recommender Systems (RecSys &apos;17</title>
		<meeting>the ACM Conference on Recommender Systems (RecSys &apos;17</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="306" to="310" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">FISM: Factored Item Similarity Models for top-N Recommender Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kabbur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Karypis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD &apos;13</title>
		<meeting>the 19th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD &apos;13</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="659" to="667" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Auto-Encoding Variational Bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd International Conference on Learning Representations (ICLR &apos;14)</title>
		<meeting>the 2nd International Conference on Learning Representations (ICLR &apos;14)</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Adam: A Method for Stochastic Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno>abs/1412.6980</idno>
		<ptr target="http://arxiv.org/abs/1412.6980" />
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Factorization Meets the Neighborhood: A Multifaceted Collaborative Filtering Model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Koren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD &apos;08</title>
		<meeting>the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD &apos;08</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="426" to="434" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Collaborative Variational Autoencoder for Recommender Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>She</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD &apos;17</title>
		<meeting>the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD &apos;17</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="305" to="314" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Variational Autoencoders for Collaborative Filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">G</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Jebara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 World Wide Web Conference (WWW &apos;18</title>
		<meeting>the 2018 World Wide Web Conference (WWW &apos;18</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="689" to="698" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Context-Aware Sequential Recommendation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Data Mining (ICDM &apos;16</title>
		<meeting>the IEEE International Conference on Data Mining (ICDM &apos;16</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1053" to="1058" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Machine Learning: A Probabilistic Perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">P</forename><surname>Murphy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>The MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">SLIM: Sparse Linear Methods for Top-N Recommender Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Karypis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE 11th International Conference on Data Mining (ICDM &apos;11</title>
		<meeting>the IEEE 11th International Conference on Data Mining (ICDM &apos;11</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="497" to="506" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Automatic differentiation in PyTorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Autodiff Workshop</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Sequence-Aware Recommender Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Quadrana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Cremonesi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jannach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Comput. Surv</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="page">36</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Personalizing Session-based Recommendations with Hierarchical Recurrent Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Quadrana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karatzoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hidasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Cremonesi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM Conference on Recommender Systems (RecSys &apos;17</title>
		<meeting>the ACM Conference on Recommender Systems (RecSys &apos;17</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="130" to="137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Factorization Machines with libFM</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rendle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Intell. Syst. Technol</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">22</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">BPR: Bayesian Personalized Ranking from Implicit Feedback</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rendle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Freudenthaler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gantner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Schmidt-Thieme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence (UAI &apos;09</title>
		<meeting>the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence (UAI &apos;09</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="452" to="461" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Factorizing Personalized Markov Chains for Next-basket Recommendation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rendle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Freudenthaler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Schmidt-Thieme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on World Wide Web (WWW &apos;10</title>
		<meeting>the International Conference on World Wide Web (WWW &apos;10</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="811" to="820" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Stochastic Backpropagation and Approximate Inference in Deep Generative Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31th International Conference on Machine Learning (ICML &apos;14</title>
		<meeting>the 31th International Conference on Machine Learning (ICML &apos;14</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1278" to="1286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Contractive Autoencoders: Explicit Invariance During Feature Extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rifai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on International Conference on Machine Learning (ICML&apos;11</title>
		<meeting>the International Conference on International Conference on Machine Learning (ICML&apos;11</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="833" to="840" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Bayesian Probabilistic Matrix Factorization Using Markov Chain Monte Carlo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mnih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning (ICML &apos;08</title>
		<meeting>the International Conference on Machine Learning (ICML &apos;08</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="880" to="887" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Probabilistic Matrix Factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mnih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Neural Information Processing Systems (NIPS &apos;08</title>
		<meeting>the International Conference on Neural Information Processing Systems (NIPS &apos;08</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1257" to="1264" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">AutoRec: Autoencoders Meet Collaborative Filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sedhain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sanner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on World Wide Web (WWW &apos;15</title>
		<meeting>the International Conference on World Wide Web (WWW &apos;15</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="111" to="112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Collaborative Filtering with Stacked Denoising AutoEncoders and Sparse Inputs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mary</surname></persName>
		</author>
		<ptr target="https://hal.inria.fr/hal-01256422" />
	</analytic>
	<monogr>
		<title level="m">NIPS Workshop on Machine Learning for eCommerce</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Improved Recurrent Neural Networks for Session-based Recommendations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">K</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st Workshop on Deep Learning for Recommender Systems (DLRS &apos;16</title>
		<meeting>the 1st Workshop on Deep Learning for Recommender Systems (DLRS &apos;16</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="17" to="22" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Personalized Top-N Sequential Recommendation via Convolutional Sequence Embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM International Conference on Web Search and Data Mining (WSDM &apos;18</title>
		<meeting>the ACM International Conference on Web Search and Data Mining (WSDM &apos;18</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="565" to="573" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Factored MDPs for Detecting Topics of User Sessions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tavakol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Brefeld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th ACM Conference on Recommender Systems (RecSys &apos;14</title>
		<meeting>the 8th ACM Conference on Recommender Systems (RecSys &apos;14</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="33" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Modelling Contextual Information in Session-Aware Recommender Systems with Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Twardowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM Conference on Recommender Systems (RecSys &apos;16</title>
		<meeting>the ACM Conference on Recommender Systems (RecSys &apos;16</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="273" to="276" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Attention is All you Need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Stacked Denoising Autoencoders: Learning Useful Representations in a Deep Network with a Local Denoising Criterion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Lajoie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">A</forename><surname>Manzagol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Collaborative Topic Modeling for Recommending Scientific Articles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 17th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Collaborative Deep Learning for Recommender Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">Y</forename><surname>Yeung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD &apos;15</title>
		<meeting>the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD &apos;15</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1235" to="1244" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Recurrent Recommender Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Beutel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM International Conference on Web Search and Data Mining (WSDM &apos;17</title>
		<meeting>the ACM International Conference on Web Search and Data Mining (WSDM &apos;17</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="495" to="503" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Personal recommendation using deep recurrent neural networks in NetEase</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Data Engineering (ICDE &apos;16</title>
		<meeting>the IEEE International Conference on Data Engineering (ICDE &apos;16</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1218" to="1229" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sun</surname></persName>
		</author>
		<idno>abs/1707.07435</idno>
		<ptr target="http://arxiv.org/abs/1707.07435" />
		<title level="m">Deep Learning based Recommender System: A Survey and New Perspectives. CoRR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">AutoSVD++: An Efficient Hybrid Collaborative Filtering Model via Contractive Auto-encoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR &apos;17</title>
		<meeting>the International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR &apos;17</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="957" to="960" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
