<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Stage-wise Fine-tuning for Graph-to-Text Generation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingyun</forename><surname>Wang</surname></persName>
							<email>qingyun4@illinois.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Semih</forename><surname>Yavuz</surname></persName>
							<email>syavuz@salesforce.com</email>
							<affiliation key="aff1">
								<orgName type="department">Salesforce Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><forename type="middle">Victoria</forename><surname>Lin</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Facebook AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Ji</surname></persName>
							<email>hengji@illinois.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nazneen</forename><forename type="middle">Fatema</forename><surname>Rajani</surname></persName>
							<email>nazneen.rajani@salesforce.com</email>
							<affiliation key="aff1">
								<orgName type="department">Salesforce Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Stage-wise Fine-tuning for Graph-to-Text Generation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T18:19+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Graph-to-text generation has benefited from pre-trained language models (PLMs) in achieving better performance than structured graph encoders. However, they fail to fully utilize the structure information of the input graph. In this paper, we aim to further improve the performance of the pre-trained language model by proposing a structured graph-to-text model with a two-step fine-tuning mechanism which first fine-tunes the model on Wikipedia before adapting to the graph-to-text generation. In addition to using the traditional token and position embeddings to encode the knowledge graph (KG), we propose a novel treelevel embedding method to capture the interdependency structures of the input graph. This new approach has significantly improved the performance of all text generation metrics for the English WebNLG 2017 dataset. 1</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In the graph-to-text generation task <ref type="bibr">(Gardent et al., 2017)</ref>, the model takes in a complex KG (an example is in <ref type="figure">Figure 1</ref>) and generates a corresponding faithful natural language description <ref type="table">(Table 1)</ref>. Previous efforts for this task can be mainly divided into two categories: sequence-to-sequence models that directly solve the generation task with <ref type="bibr">LSTMs (Gardent et al., 2017)</ref> or Transformer <ref type="bibr">(Castro Ferreira et al., 2019)</ref>; and graph-to-text models <ref type="bibr" target="#b11">(Trisedya et al., 2018;</ref><ref type="bibr">Marcheggiani and Perez-Beltrachini, 2018)</ref> which use a graph encoder to capture the structure of the KGs. Recently, Transformer-based PLMs such as GPT-2 <ref type="bibr" target="#b3">(Radford et al., 2019)</ref>, <ref type="bibr">BART (Lewis et al., 2020)</ref>, * This research was conducted during the author's internship at Salesforce Research. <ref type="bibr">1</ref> The programs, data and resources are publicly available for research purpose at: https://github.com/ EagleW/Stage-wise-Fine-tuning and T5 <ref type="bibr" target="#b4">(Raffel et al., 2020)</ref> have achieved stateof-the-art results on WebNLG dataset due to factual knowledge acquired in the pre-training phase <ref type="bibr">(Harkous et al., 2020;</ref><ref type="bibr">Ribeiro et al., 2020b;</ref><ref type="bibr">Kale, 2020;</ref><ref type="bibr">Chen et al., 2020a)</ref>. Despite such improvement, PLMs fine-tuned only on the clean (or labeled) data might be more prone to hallucinate factual knowledge (e.g., "Visvesvaraya Technological University" in <ref type="table">Table  1</ref>). Inspired by the success of domain-adaptive pre-training <ref type="bibr">(Gururangan et al., 2020)</ref>, we propose a novel two-step fine-tuning mechanism graph-totext generation task. Unlike <ref type="bibr">(Ribeiro et al., 2020b;</ref><ref type="bibr">Herzig et al., 2020;</ref><ref type="bibr">Chen et al., 2020a)</ref> which directly fine-tune the PLMs on the training set, we first fine-tune our model over noisy RDF graphs and related article pairs crawled from Wikipedia before final fine-tuning on the clean/labeled training set. The additional fine-tuning step benefits our model by leveraging triples not included in the training set and reducing the chances that the model fabricates facts based on the language model. Meanwhile, the PLMs might also fail to cover all relations in the KG by creating incorrect or missing facts. For example, in <ref type="table">Table 1</ref>, although the T5-large with Wikipedia fine-tuning successfully removes the unwanted contents, it still ignores the "sports Governing Body" relation and incorrectly The Acharya Institute of Technology is located in the state of Karnataka . It was given the Technical Campus status by the All India Council for Technical Education which is located in Mumbai . The institute offers tennis and has Telangana to its northeast and the Arabian Sea to its west. [International Tennis Federation]</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>T5-large + Position</head><p>The Acharya Institute of Technology is located in the state of Karnataka which has Telangana to its northeast and the Arabian Sea to its west. It was given the Technical Campus status by the All India Council for Technical Education in Mumbai . The Institute offers tennis which is governed by the International Tennis Federation .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>T5-large + Wiki + Position</head><p>The Acharya Institute of Technology in Karnataka was given the 'Technical Campus' status by the All India Council for Technical Education in Mumbai . Karnataka has Telangana to its northeast and the Arabian Sea to its west. One of the sports offered at the Institute is tennis which is governed by the International Tennis Federation . <ref type="table">Table 1</ref>: Human and System Generated Description in <ref type="figure">Figure 1</ref>. We use the color box to frame each entity out with the same color as the corresponding entity in <ref type="figure">Figure 1</ref>. We highlight fabricated facts, [missed relations], and incorrect relations with different color. links the university to both "Telangana" and "Arabian Sea". To better capture the structure and interdependence of facts in the KG, instead of using a complex graph encoder, we leverage the power of Transformer-based PLMs with additional position embeddings which have been proved effective in various generation tasks <ref type="bibr">(Herzig et al., 2020;</ref><ref type="bibr">Chen et al., 2020a,b)</ref>. Here, we extend the embedding layer of Transfomer-based PLMs with two additional triple role and tree-level embeddings to capture graph structure.</p><p>We explore the proposed stage-wise fine-tuning and structure-preserving embedding strategies for graph-to-text generation task on WebNLG corpus <ref type="bibr">(Gardent et al., 2017)</ref>. Our experimental results clearly demonstrate the benefit of each strategy in achieving the state-of-the-art performance on most commonly reported automatic evaluation metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head><p>Given an RDF graph with multiple relations G = {(s 1 , r 1 , o 1 ), (s 2 , r 2 , o 2 ), ..., (s n , r n , o n )}, our goal is to generate a text faithfully describing the input graph. We represent each relation with a triple (s i , r i , o i ) ? G for i ? {1, ..., n}, where s i , r i , and o i are natural language phrases that represent the subject, type, and object of the relation, respectively. We augment our model with addi-   <ref type="figure">Figure 1</ref> tional position embeddings to capture the structure of the KG. To feed the input for the large-scale Transformer-based PLM, we flatten the graph as a concatenation of linearized triple sequences:   <ref type="bibr">, 2020)</ref>. We extend the input layer with two positionaware embeddings in addition to the original position embeddings 3 as shown in the Figure 2:</p><formula xml:id="formula_0">|S s 1 |P r 1 |O o 1 ... |S s n |P r n |O o</formula><p>? Position ID, which is the same as the original position ID used in BART, is the index of the token in the flattened sequence |S s 1 |P r 1 |O o 1 ... |S s n |P r n |O o n .</p><p>? Triple Role ID takes 3 values for a specific triple (s i , r i , o i ): 1 for the subject s i , 2 for the relation r i , and 3 for the object o i .</p><p>? Tree level ID calculates the distance (the number of relations) from the root which is the source vertex of the RDF graph.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Results and Analysis</head><p>We use the standard NLG evaluation metrics to report results: BLEU <ref type="bibr" target="#b1">(Papineni et al., 2002)</ref>, <ref type="bibr">METEOR (Lavie and Agarwal, 2007)</ref>, and TER <ref type="bibr" target="#b9">(Snover et al., 2006)</ref> , as shown in <ref type="table" target="#tab_3">Table 2</ref>. Because Castro Ferreira et al. <ref type="formula">(2020)</ref> has found that BERTScore <ref type="bibr" target="#b17">(Zhang* et al., 2020)</ref> correlates with human evaluation ratings better, we use BERTscore to evaluate system results 5 as shown in <ref type="table" target="#tab_6">Table 4</ref>. When selecting the best models, we also evaluate each model with <ref type="bibr">PARENT (Dhingra et al., 2019)</ref> metric which measures the overlap between predictions and both reference texts and graph contents. Dhingra et al. <ref type="formula">(2019)</ref> show PARENT metric has better human rating correlations. <ref type="table" target="#tab_5">Table 3</ref> shows the pre-trained models with 2-step fine-tuning and position embeddings achieve better results. <ref type="bibr">6</ref> We conduct paired t-test between our proposed model and all the other baselines on 10 randomly sampled subsets. The differences are statistically significant with p ? 0.008 for all settings.</p><p>Results with Wikipedia fine-tuning. The Wikipedia fine-tuning helps the model handle unseen relations such as "inOfficeWhileVicePresident", and "activeYearsStartYear" by stating "His vice president is Atiku Abubakar." and "started playing in 1995" respectively. It also combines relations with the same type together with correct order, e.g., given two death places of a person, the model generates: "died in Sidcup, London" instead of generating two sentences or placing the city name ahead of the area name.</p><p>Results with positional embeddings. For the KG with multiple triples, additional positional embeddings help reduce the errors introduced by pro- <ref type="bibr">5</ref> We only use BERTScore to evaluate baselines which have results available online. <ref type="bibr">6</ref> For more examples, please check Appendix for reference.</p><p>noun ambiguity. For instance, for a KG which has "leaderName" relation to both country's leader and university's dean, position embeddings can distinguish these two relations by stating "Denmark's leader is Lars L?kke Rasmussen" instead of "its leader is Lars L?kke Rasmussen". The tree-level embeddings also help the model arrange multiple triples into one sentence, such as combining the city, the country, the affiliation, and the affiliation's headquarter of a university into a single sentence: "The School of Business and Social Sciences at the Aarhus University in Aarhus, Denmark is affiliated to the European University Association in Brussels".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Remaining Challenges</head><p>However, pre-trained language models also generate some errors as shown in <ref type="table" target="#tab_8">Table 5</ref>. Because the language model is heavily pre-trained, it is biased against the occurrence of patterns that would enable it to infer the right relation. For example, for the "activeYearsStartYear" relation, the model might confuse it with the birth year. For some relations that do not have a clear direction, the language model is not powerful enough to consider the deep connections between the subject and the object. For example, for the relation "doctoralStudent", the model mistakenly describes a professor as a Ph.D. student. Similarly, the model treats an asteroid as a person because it has an epoch date. For KGs with multiple triples, the generator still has a chance to miss relations or mixes the subject and the object of different relations, especially for the unseen category. For instance, for a soccer player with multiple clubs, the system might confuse the subject of one club's relation with another club.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Related Work</head><p>The WebNLG task is similar to Wikibio generation <ref type="bibr">(Lebret et al., 2016;</ref>, AMRto-text generation <ref type="bibr" target="#b10">(Song et al., 2018)</ref> and RO-TOWIRE <ref type="bibr" target="#b15">(Wiseman et al., 2017;</ref><ref type="bibr" target="#b2">Puduppully et al., 2019)</ref>. Previous methods usually treat the graphto-text generation as an end-to-end generation task. Those models <ref type="bibr" target="#b11">(Trisedya et al., 2018;</ref><ref type="bibr">Gong et al., 2019;</ref><ref type="bibr" target="#b8">Shen et al., 2020)</ref> usually first lineralize the knowledge graph and then use attention mechanism to generate the description sentences. While the linearization of input graph may sacrifice the inter-dependency inside input graph, some papers <ref type="bibr" target="#b5">(Ribeiro et al., 2019</ref><ref type="bibr">(Ribeiro et al., , 2020a</ref><ref type="bibr" target="#b18">Zhao et al., 2020)</ref>   use graph encoder such as <ref type="bibr">GCN (Duvenaud et al., 2015)</ref> and graph transformer <ref type="bibr" target="#b13">(Wang et al., 2020a;</ref><ref type="bibr">Koncel-Kedziorski et al., 2019)</ref> to encode the input graphs. Others <ref type="bibr" target="#b8">(Shen et al., 2020;</ref><ref type="bibr" target="#b14">Wang et al., 2020b)</ref> try to carefully design loss functions to control the generation quality. With the development of computation resources, large scale PLMs such as GPT-2 <ref type="bibr" target="#b3">(Radford et al., 2019)</ref>, BART (Lewis et al., 2020) and T5 <ref type="bibr" target="#b4">(Raffel et al., 2020)</ref> achieve state-ofthe-art results even with simple linearized graph input <ref type="bibr">(Harkous et al., 2020;</ref><ref type="bibr">Chen et al., 2020a;</ref><ref type="bibr">Kale, 2020;</ref><ref type="bibr">Ribeiro et al., 2020b)</ref>. Instead of directly fine-tuning the PLMs, we propose a two-step finetuning mechanism to get better domain adaptation ability. In addition, using positional embeddings as an extension for PLMs has shown its effectiveness in table-based question answering (Herzig et al., 2020), fact verification (Chen et al., 2020b), and graph-to-text generation (Chen et al., 2020a). We capture the graph structure by enhancing the input layer with the triple role and tree-level embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions and Future Work</head><p>We propose a new two-step structured generation task for the graph-to-text generation task based on a two-step fine-tuning mechanism and novel treelevel position embeddings. In the future, we aim to address the remaining challenges and extend the framework for broader applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>This work is partially supported by Agriculture and Food Research Initiative (AFRI) grant no. 2020-67021-32799/project accession no.1024178 from the USDA National Institute of Food and Agriculture, and by the Office of the Director of National Intelligence (ODNI), Intelligence Advanced Research Projects Activity (IARPA), via contract # FA8650-17-C-9116. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies, either expressed or implied of the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for governmental purposes notwithstanding any copyright annotation therein.  Our model is built based on the Huggingface framework <ref type="bibr" target="#b16">(Wolf et al., 2020)</ref>  <ref type="bibr">8</ref> . Because the average lengths for source and target text in the training set are 31 and 22 words respectively, we set the maximum length for both source and target to 100 words. For T5 preprocessing, we prepend "translate RDF to English:" before the input. For BART-base, distil-BART-xsum, and T5-base, we use a batch size of 32 and train the model. We use a batch size of 16 for Bart-large, and 6 for T5large. We use the Adam optimizer <ref type="bibr">(Kingma and Ba, 2015)</ref> to optimize each model with learning rate of 3 ? 10 ?5 with = 1 ? 10 ?8 for a maximum of 10 epochs. We run each experiment on one Nvidia Tesla V100 GPU with 16G DRAM. We first fine-tuned the PLMs on crawled Wikipedia pairs for 3 epochs. The Wikipedia Fine-tuning stage takes about 24 hours for T5-large and 10 hours for the rest of models. The final WebNLG fine-tuning stage takes less than 1 hour for all the models. We chose our best model based on multi-BLEU score 9 . For inference, we use beam search with beam size in the range {3,5}. <ref type="table" target="#tab_10">Table 6</ref> shows the number of the parameters for each pre-trained model.     </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>References</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Sample Generation Results</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Figure 1: Input RDF Knowledge Graph</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Position Embeddings for the KG in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>The</head><label></label><figDesc>American Civil War was fought in the Siege of Petersburg . Jefferson Davis was the commander of the war. Alfred Moore Scales was born in the United States where Native Americans are an ethnic group. [S| Alfred Moore Scales P| battles O| Siege of Petersburg] T5-large + Position Alfred Moore Scales was born in the United States , where Native Americans are an ethnic group. He fought in the American Civil War, which was led by Jefferson Davis . The Siege of Petersburg is part of the American Civil War . [S| Alfred Moore Scales P| battles O| Siege of Petersburg] T5-large + Wiki + Position Alfred Moore Scales is from the United States where Native Americans are one of the ethnic groups. He fought in the Siege of Petersburg which is part of the American Civil War . Jefferson Davis was the commander of the American Civil War.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>arXiv:2105.08021v2 [cs.CL] 30 May 2021 Category Output Reference The Acharya Institute of Technology in Karnataka state was given Technical Campus status by All India Council for Technical Education in Mumbai . The school offers tennis which is governed by the International Tennis Federation . Karnataka has the Arabian Sea to its west and in the northeast is Telangana . T5-large The state of Karnataka is located southwest of Telangana and east of the Arabian Sea . It is the location of the Acharya Institute of Technology which was granted the Technical Campus status by the All India Council for Technical Education in Mumbai . The Institute is affiliated with the Visvesvaraya Technological University and offers the sport of tennis .</figDesc><table><row><cell>[International Tennis Federation]</cell></row><row><cell>T5-large</cell></row><row><cell>+ Wiki</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>... ... ... ...</head><label></label><figDesc></figDesc><table><row><cell>Token Embeddings</cell><cell>[CLS]</cell><cell>S|</cell><cell>Karnataka</cell><cell>P|</cell><cell>Northeast</cell></row><row><cell>Position Embeddings</cell><cell>POS 0</cell><cell>POS 1</cell><cell>POS 2</cell><cell>POS 3</cell><cell>POS 4</cell></row><row><cell>Triple Role Embeddings</cell><cell>ROL 0</cell><cell>ROL 1</cell><cell>ROL 1</cell><cell>ROL 1</cell><cell>ROL 2</cell></row><row><cell>Tree-level Embeddings</cell><cell>LV 0</cell><cell>LV 2</cell><cell>LV 2</cell><cell>LV 2</cell><cell>LV 2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell>: System Results on WebNLG Test Set Evaluated by BLEU, METEOR, and TER with Official Scripts</cell></row><row><cell>embeddings to enhance the flattened input of pre-</cell></row><row><cell>trained Transformer-based sequence-to-sequence</cell></row><row><cell>models such as BART and TaPas (Herzig et al.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Results with both Wikipedia Fine-tuning and Positional Embedding for Various Pre-trained Models over All Categories on Development Set Evaluated by average of PARENT 4 precision, recall, F1 and BLEU (%)We use the original version of English WebNLG2017 (Gardent et al., 2017) dataset which contains 18,102/2,268/4,928 graph-description pairs for training, validation, and testing set respectfully. For this task, we investigate a variety of the BART and T5 models with our novel tree-level embeddings. The statistics and more details of those models are listed in Appendix A.</figDesc><table><row><cell>Model</cell><cell>P?</cell><cell>R?</cell><cell>F1?</cell></row><row><cell>Gardent et al. (2017)</cell><cell cols="3">88.35 90.22 89.23</cell></row><row><cell>Moryossef et al. (2019)</cell><cell cols="3">85.77 89.34 87.46</cell></row><row><cell>Nan et al. (2021)</cell><cell cols="3">89.49 92.33 90.83</cell></row><row><cell>Ribeiro et al. (2020b)</cell><cell cols="3">89.36 91.96 90.59</cell></row><row><cell>T5-large + Wiki + Position</cell><cell cols="3">96.36 96.13 96.21</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>System Results on WebNLG Test Set Evaluated by BERTScore precision, recall, F1 (%)</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>Andrew White (born in 2003) is a musician who is associated with the band Kaiser Chiefs and Marry Banilow. He is also associated with the label Polydor Records and is signed to B-Unique Records. S| Aleksandra Kova? P| activeYearsStartYear O| 1990 T5-large Walter Baade was born in the German Empire and graduated from the University of Gottingen. He was the doctoral student of Halton Arp and Allan Sandage and was the discoverer of 1036 Ganymed. S| Walter Baade P| doctoralStudent O| Halton Arp; S| Walter Baade P| doctoralStudent O| Allan Sandage T5-large +Wiki 11264 Claudiomaccone was born on the 26th of November, 2005. He has an orbital period of 1513.722 days, a periapsis of 296521000.0 kilometres and an apoapsis of 475426000.0 kilometres. S| 11264 Claudiomaccone P| epoch O| 2005-11-26; S| Aleksandr Prudnikov P| club O| FC Amkar Perm T5-large +Position The chairman of FC Spartak Moscow is Sergey Rodionov. Aleksandr Prudnikov plays for FC Spartak Moscow and manages FC Amkar Perm. [ S| FC Amkar Perm P| manager O| Gadzhi Gadzhiyev; S| Aleksandr Prudnikov P| club O| FC Amkar Perm ]</figDesc><table><row><cell>Category Output</cell></row><row><cell>T5-large</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 :</head><label>5</label><figDesc>System Error Examples. We highlight fabricated facts, [missed relations], incorrect relations, and ground truth relations with different color.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>Hong Wang, Shiyang Li, Xiyou Zhou, and William Yang Wang. 2020b. Tabfact: A large-scale dataset for table-based fact verification. In Proceedings of the 8th International Conference on Learning Representations. Mihir Kale. 2020. Text-to-text pre-training for data-totext tasks. Computation and Language Repository, arXiv:2005.10433. Version 2.</figDesc><table><row><cell>Wenhu Chen, Hongmin Wang, Jianshu Chen, Yunkai</cell><cell>Meeting of the Association for Computational Lin-</cell></row><row><cell>Zhang, Bhuwan Dhingra, Manaal Faruqui, Ankur Parikh, Ming-Wei Chang, Dipanjan Das, and William Co-hen. 2019. Handling divergent reference texts when evaluating table-to-text generation. In Proceedings of the 57th Annual Meeting of the Association for</cell><cell>guistics, pages 4320-4333, Online. Association for Computational Linguistics. Diederik P Kingma and Jimmy Ba. 2015. Adam: A method for stochastic optimization. In Proceedings of the 3rd International Conference on Learning Representations.</cell></row><row><cell>Computational Linguistics, pages 4884-4895, Flo-rence, Italy. Association for Computational Linguis-tics. A Hyperparameters and Statistics of the Model</cell><cell>Rik Koncel-Kedziorski, Dhanush Bekal, Yi Luan, Mirella Lapata, and Hannaneh Hajishirzi. 2019. Text Generation from Knowledge Graphs with</cell></row><row><cell>David K Duvenaud, Dougal Maclaurin, Jorge Ipar-raguirre, Rafael Bombarell, Timothy Hirzel, Alan Origin 7 + Position Aspuru-Guzik, and Ryan P Adams. 2015. Convolu-BART-base 139.42M 139.43M tional networks on graphs for learning molecular fin-distil-BART-xsum 305.51M 305.53M gerprints. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors, Advances in BART-large 406.29M 406.31M T5-base 222.88M 222,90M Neural Information Processing Systems 28, pages 2224-2232. Curran Associates, Inc. T5-large 737.64M 737.65M</cell><cell>Graph Transformers. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 2284-2293, Minneapolis, Minnesota. Association for Computational Linguistics. automatic metric for MT evaluation with high levels Alon Lavie and Abhaya Agarwal. 2007. METEOR: An</cell></row><row><cell>Claire Gardent, Anastasia Shimorina, Shashi Narayan, and Laura Perez-Beltrachini. 2017. The WebNLG challenge: Generating text from RDF data. In Pro-ceedings of the 10th International Conference on</cell><cell>of correlation with human judgments. In Proceed-ings of the Second Workshop on Statistical Machine Translation, pages 228-231, Prague, Czech Repub-lic. Association for Computational Linguistics.</cell></row><row><cell>Natural Language Generation, pages 124-133, San-tiago de Compostela, Spain. Association for Compu-tational Linguistics.</cell><cell>R?mi Lebret, David Grangier, and Michael Auli. 2016. Neural text generation from structured data with application to the biography domain. In Proceed-</cell></row><row><cell>Heng Gong, Xiaocheng Feng, Bing Qin, and Ting Liu. 2019. Table-to-text generation with effective hier-archical encoder on three dimensions (row, column and time). In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Process-ing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 3143-3152, Hong Kong, China. Association for Computational Linguistics. Suchin Gururangan, Ana Marasovi?, Swabha Swayamdipta, Kyle Lo, Iz Beltagy, Doug Downey, and Noah A. Smith. 2020. Don't stop pretraining: Adapt language models to domains and tasks. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 8342-8360, Online. Association for Computational Linguistics. Hamza Harkous, Isabel Groves, and Amir Saffari. 2020. Have your text and use it too! end-to-end neural data-to-text generation with semantic fidelity. In Proceedings of the 28th International Conference</cell><cell>Thiago Castro Ferreira, Claire Gardent, Nikolai Ilinykh, Chris van der Lee, Simon Mille, Diego ings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1203-1213, Austin, Texas. Association for Computational Lin-guistics. Moussallem, and Anastasia Shimorina. 2020. The 2020 bilingual, bi-directional WebNLG+ shared task: Overview and evaluation results (WebNLG+ 2020). In Proceedings of the 3rd International Work-shop on Natural Language Generation from the Se-Mike Lewis, Yinhan Liu, Naman Goyal, Mar-jan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. 2020. BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, mantic Web (WebNLG+), pages 55-76, Dublin, Ire-land (Virtual). Association for Computational Lin-guistics. Thiago Castro Ferreira, Chris van der Lee, Emiel and comprehension. In Proceedings of the 58th An-nual Meeting of the Association for Computational Linguistics, pages 7871-7880, Online. Association for Computational Linguistics. van Miltenburg, and Emiel Krahmer. 2019. Neu-Diego Marcheggiani and Laura Perez-Beltrachini. ral data-to-text generation: A comparison between 2018. Deep graph convolutional encoders for struc-pipeline and end-to-end architectures. In Proceed-tured data to text generation. In Proceedings of ings of the 2019 Conference on Empirical Methods the 11th International Conference on Natural Lan-in Natural Language Processing and the 9th Interna-guage Generation, pages 1-9, Tilburg University, tional Joint Conference on Natural Language Pro-The Netherlands. Association for Computational cessing (EMNLP-IJCNLP), pages 552-562, Hong Linguistics. Kong, China. Association for Computational Lin-guistics. Amit Moryossef, Yoav Goldberg, and Ido Dagan. 2019.</cell></row><row><cell>on Computational Linguistics, pages 2410-2424,</cell><cell>Step-by-step: Separating planning from realization</cell></row><row><cell>Barcelona, Spain (Online). International Committee</cell><cell>Wenhu Chen, Yu Su, Xifeng Yan, and William Yang in neural data-to-text generation. In Proceedings of</cell></row><row><cell>on Computational Linguistics.</cell><cell>Wang. 2020a. KGPT: Knowledge-grounded pre-the 2019 Conference of the North American Chap-</cell></row><row><cell></cell><cell>training for data-to-text generation. In Proceed-ter of the Association for Computational Linguistics:</cell></row><row><cell>Jonathan Herzig, Pawel Krzysztof Nowak, Thomas</cell><cell>ings of the 2020 Conference on Empirical Methods Human Language Technologies, Volume 1 (Long</cell></row><row><cell>M?ller, Francesco Piccinno, and Julian Eisenschlos.</cell><cell>in Natural Language Processing (EMNLP), pages and Short Papers), pages 2267-2277, Minneapolis,</cell></row><row><cell>2020. TaPas: Weakly supervised table parsing via</cell><cell>8635-8648, Online. Association for Computational Minnesota. Association for Computational Linguis-</cell></row><row><cell>pre-training. In Proceedings of the 58th Annual</cell><cell>Linguistics. tics.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 6 :</head><label>6</label><figDesc></figDesc><table /><note># of Model Parameters</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head></head><label></label><figDesc>Massachusetts and started performing in 1995 . He formerly played with the band Twilight and is now in the Lotus Eaters . He is is closely associated with the Sludge metal genre.T5-largeAaron Turner, who is from Massachusetts , is a member of the Twilight band and the Lotus Eaters band.</figDesc><table><row><cell>Category</cell><cell>Output</cell></row><row><cell>Input</cell><cell>S| Aaron Turner P| associatedBand/associatedMusicalArtist O| Twilight (band)</cell></row><row><cell></cell><cell>S| Aaron Turner P| associatedBand/associatedMusicalArtist O| Lotus Eaters (band)</cell></row><row><cell></cell><cell>S| Aaron Turner P| genre O| Sludge metal</cell></row><row><cell></cell><cell>S| Aaron Turner P| origin O| Massachusetts</cell></row><row><cell></cell><cell>S| Aaron Turner P| activeYearsStartYear O| 1995</cell></row><row><cell>Reference</cell><cell>Aaron Turner was born in He is also associated with the Sludge metal genre and started playing in 1995 .</cell></row><row><cell>T5-large</cell><cell>Aaron Turner was born in Massachusetts and started playing in 1995 . He is associated with the band</cell></row><row><cell>+ Wiki</cell><cell>Twilight and the band Lotus Eaters . His music style is Sludge metal .</cell></row><row><cell>T5-large +</cell><cell>Aaron Turner, who was born in Massachusetts , is associated with Twilight and Lotus Eaters . He is a</cell></row><row><cell>Position</cell><cell>member of Sludge metal band and has also been a musical artist. [1995]</cell></row><row><cell>T5-large</cell><cell>Aaron Turner was born in Massachusetts and started playing Sludge metal in 1995 . He is also associated</cell></row><row><cell>+ Wiki + Position</cell><cell>with Twilight and Lotus Eaters .</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 7 :</head><label>7</label><figDesc>Human and System Generated Description We use the color box to frame each entity out with the same color as the corresponding entity. We highlight fabricate facts, [missed relations], and incorrect relations with different color. Siege of Petersburg involved U.S. born Alfred Moore Scales. The American Civil War was commanded by Jefferson Davis . An ethnic group of the United States includes Native Americans . T5-large Alfred Moore Scales is from the United States where Native Americans are one of the ethnic groups. He fought in the Siege of Petersburg which was part of the American Civil War commanded by</figDesc><table><row><cell>Category</cell><cell>Output</cell></row><row><cell>Input</cell><cell>S| American Civil War P| commander O| Jefferson Davis</cell></row><row><cell></cell><cell>S| Alfred Moore Scales P| country O| United States</cell></row><row><cell></cell><cell>S| United States P| ethnicGroup O| Native Americans in the United States</cell></row><row><cell></cell><cell>S| Alfred Moore Scales P| battles O| Siege of Petersburg</cell></row><row><cell></cell><cell>S| Siege of Petersburg P| isPartOfMilitaryConflict O| American Civil War target</cell></row><row><cell>Reference</cell><cell>American Civil War , Jefferson Davis .</cell></row><row><cell>T5-large</cell><cell></cell></row><row><cell>+ Wiki</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 8 :</head><label>8</label><figDesc>Human and System Generated Description We use the color box to frame each entity out with the same color as the corresponding entity. We highlight fabricate facts, [missed relations], and incorrect relations with different color. Helena P| discoverer O| James Craig Watson S| James Craig Watson P| almaMater O| University of Michigan S| 101 Helena P| discovered O| 1868-08-15 S| James Craig Watson P| nationality O| Canada S| James Craig Watson P| deathPlace O| Madison, Wisconsin Reference James Craig Watson, who discovered 101 Helena on 15th August 1868 , is a Canadian national who attended the University of Michigan . He died in Madison, Wisconsin . T5-large James Craig Watson is a Canadian who graduated from the University of Michigan . He was the discoverer of 101 Helena which was discovered on 15 August 1868 . He died in Madison, Wisconsin . Canadian , graduated from the University of Michigan and discovered 101 Helena on August 15th, 1868 . He died in Madison, Wisconsin . Canadian , graduated from the University of Michigan and was the discoverer of 101 Helena on August 15, 1868 . He died in Madison, Wisconsin . Canadian , graduated from the University of Michigan and discovered 101 Helena on August 15th, 1868 . He died in Madison, Wisconsin .</figDesc><table><row><cell>Category</cell><cell>Output</cell></row><row><cell cols="2">Input S| 101 T5-large + Wiki James Craig Watson, a T5-large + Position James Craig Watson, a T5-large James Craig Watson, a</cell></row><row><cell>+ Wiki +</cell><cell></cell></row><row><cell>Position</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 9 :</head><label>9</label><figDesc>Human and System Generated Description We use the color box to frame each entity out with the same color as the corresponding entity. We highlight fabricate facts, [missed relations], and incorrect relations with different color.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">For this baseline, we use the results reported from<ref type="bibr" target="#b18">Zhao et al. (2020)</ref> who also use official evaluation scripts.3 For T5 models, we only keep the Triple Role and Treelevel embeddings.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">https://github.com/KaijuML/parent</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8">https://github.com/huggingface/transformers 9 https://gitlab.com/webnlg/webnlg-baseline/-/blob/master/multi-bleu.perl 9 # of parameters are slightly different because we add special tokens to the vocabulary</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">DART: Open-domain structured data record to text generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linyong</forename><surname>Nan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Radev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amrit</forename><surname>Rau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinand</forename><surname>Sivaprasad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chiachun</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangru</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aadit</forename><surname>Vyas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neha</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangxiaokang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nadia</forename><surname>Irwanto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jessica</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faiaz</forename><surname>Rahman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmad</forename><surname>Zaidi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mutethia</forename><surname>Mutuma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasin</forename><surname>Tarabar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankit</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Chern Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Victoria Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nazneen Fatema</forename><surname>Rajani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="432" to="447" />
		</imprint>
	</monogr>
	<note>Online. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Jing</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="DOI">10.3115/1073083.1073135</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 40th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Philadelphia, Pennsylvania, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Data-to-text generation with content selection and planning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ratish</forename><surname>Puduppully</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="6908" to="6915" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">OpenAI blog</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Exploring the limits of transfer learning with a unified text-totext transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">140</biblScope>
			<biblScope unit="page" from="1" to="67" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Enhancing AMR-to-text generation with dual graph representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">R</forename><surname>Leonardo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Ribeiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iryna</forename><surname>Gardent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gurevych</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1314</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3183" to="3194" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Claire Gardent, and Iryna Gurevych. 2020a. Modeling global and local node contexts for text generation from knowledge graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">R</forename><surname>Leonardo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Ribeiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.1162/tacl_a_00332</idno>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="589" to="604" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">R</forename><surname>Leonardo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Ribeiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schmitt</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.08426</idno>
		<title level="m">Hinrich Sch?tze, and Iryna Gurevych. 2020b. Investigating pretrained language models for graph-to-text generation</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Neural data-to-text generation via jointly learning the segmentation and correspondence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyu</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ernie</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dietrich</forename><surname>Klakow</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.641</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="7155" to="7165" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A study of translation error rate with targeted human annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Snover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bonnie</forename><surname>Dorr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linnea</forename><surname>Micciulla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralph</forename><surname>Weischedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Association for Machine Transaltion in the Americas</title>
		<meeting>the Association for Machine Transaltion in the Americas</meeting>
		<imprint>
			<publisher>AMTA</publisher>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A graph-to-sequence model for AMRto-text generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linfeng</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiguo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Gildea</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P18-1150</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1616" to="1626" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">GTR-LSTM: A triple encoder for sentence generation from RDF data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianzhong</forename><surname>Bayu Distiawan Trisedya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P18-1151</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Melbourne, Australia. As</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1627" to="1637" />
		</imprint>
	</monogr>
	<note>sociation for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Describing a knowledge base</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingyun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoman</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lifu</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boliang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiying</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Heng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W18-6502</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th International Conference on Natural Language Generation</title>
		<meeting>the 11th International Conference on Natural Language Generation</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="10" to="21" />
		</imprint>
	</monogr>
	<note>Tilburg University, The Netherlands. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">AMR-to-text generation with graph transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianming</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojun</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanqi</forename><surname>Jin</surname></persName>
		</author>
		<idno type="DOI">10.1162/tacl_a_00297</idno>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="19" to="33" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Towards faithful neural table-to-text generation with content-matching constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenyi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bang</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changyou</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.101</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1072" to="1086" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Challenges in data-to-document generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Wiseman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stuart</forename><surname>Shieber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Rush</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D17-1239</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2253" to="2263" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Transformers: State-of-the-art natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clement</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierric</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Rault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Remi</forename><surname>Louf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Morgan</forename><surname>Funtowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joe</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Shleifer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clara</forename><surname>Patrick Von Platen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yacine</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Canwen</forename><surname>Plu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Teven</forename><forename type="middle">Le</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Scao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mariama</forename><surname>Gugger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Drame</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-demos.6</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations<address><addrLine>Quentin Lhoest, and Alexander Rush</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="38" to="45" />
		</imprint>
	</monogr>
	<note>Online. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Bertscore: Evaluating text generation with bert</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">*</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varsha</forename><surname>Kishore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">*</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">*</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Artzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Bridging the structural gap between encoding and decoding for data-to-text generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marilyn</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Snigdha</forename><surname>Chaturvedi</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.224</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2481" to="2491" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
