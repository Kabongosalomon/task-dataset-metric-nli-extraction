<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">n-CPS: Generalising Cross Pseudo Supervision to n Networks for Semi-Supervised Semantic Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dominik</forename><surname>Filipiak</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">AI Clearing, Inc</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Semantic Technology Institute</orgName>
								<orgName type="department" key="dep2">Department of Computer Science</orgName>
								<orgName type="institution">University of Innsbruck</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Tempczyk</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">AI Clearing, Inc</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Institute of Informatics</orgName>
								<orgName type="institution">University of Warsaw</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marek</forename><surname>Cygan</surname></persName>
							<email>cygan@mimuw.edu.pl</email>
							<affiliation key="aff2">
								<orgName type="department">Institute of Informatics</orgName>
								<orgName type="institution">University of Warsaw</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">n-CPS: Generalising Cross Pseudo Supervision to n Networks for Semi-Supervised Semantic Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T11:43+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The recent cross pseudo supervision (CPS) approach is a state-of-the-art method for semi-supervised semantic segmentation, which trains two neural networks with a custom cross supervision. As we observe that only one of those networks is used in the inference phase, we suggest using both networks using voting and generalising it to more than two networks. As a result, we present n-CPS, a generalisation of CPS that uses n simultaneously trained subnetworks that learn from each other through one-hot encoding perturbation and consistency regularisation, which together with ensembling of the trained subnetworks significantly improves the performance over the prior method. To the best of our knowledge, n-CPS paired with CutMix outperforms CPS and sets the new state-of-the-art for Pascal VOC 2012 with (1/16, 1/8, 1/4, and 1/2 supervised regimes) and Cityscapes (1/16 supervised). The code is available on GitHub 1 .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>An intense research effort can be observed in data-and label-efficient machine learning. The latter can be tackled using semi-supervised learning methods. Semantic segmentation can significantly benefit from semi-supervised methods due to the relatively high cost of labelling every pixel. Among numerous techniques, consistency regularisation is proven to boost performance in such settings. The recent cross pseudo supervision (abbreviated as CPS) approach <ref type="bibr" target="#b1">[2]</ref> has set the new state-of-the-art in the task of semi-supervised semantic segmentation. It simultaneously trains two networks, which are penalised for discrepancies between them. This consistency regularisation mechanism is enriched with a specific one-hot encoding data perturbation.</p><p>While in the original CPS approach two networks are used in training, the inference is performed only on the output of the first network. This observation constituted a question that led us to this study -why can't we use the outputs of both networks to improve the results? This produced another question -can this approach use more than two networks? The result of this investigation is n-CPS -a generalisation of the CPS approach.</p><p>This paper proposes a generalised cross pseudo supervision approach. Our contribution is two-fold:</p><p>? we generalise the CPS to handle more than two networks in the training process,</p><p>? we propose an evaluation approach inspired by ensemble learning, which treats these networks similarly to a blend of weak learners to form a better model.</p><p>To the best of our knowledge, n-CPS paired with CutMix <ref type="bibr" target="#b16">[17]</ref> outperforms CPS and sets the new state-of-the-art for Pascal VOC 2012 for 1/16, 1/8, 1/4, and 1/2 supervised regimes and Cityscapes for 1/16 supervised data. <ref type="figure">Figure 1</ref> presents results for the Pascal VOC 2012 dataset. The paper is structured as follows. In the Section 2, we present n-CPS, the proposed method. The results of evaluation are presented Section 3 -the experiment setup in Section 3.1, comparison with the state-of-the art in Section 3.2, and ablation studies in Section 3.3. Section 4 offers a comprehensive literature review of the literature on the topic. The paper is concluded with a short summary in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Method</head><p>This section describes the proposed method. Firstly, we present plain n-CPS. Then, we introduce a variant paired with the CutMix algorithm. Finally, we present ensemble learning techniques, which increase the evaluation performance. n-CPS. The proposed method generalises the cross pseudo supervision (CPS) approach for semi-supervised semantic segmentation. Similarly to CPS, a key feature of n-CPS is consistency regularisation between the output of one network and the output of another one perturbed with pixel-wise one-hot encoding. This perturbation (denoted as pmax hereafter) sets 1 for the class with the highest probability and 0 for the others. However, instead of using two networks (with the same architecture but with differently initialised weights) as in the original CPS algorithm, our method can use n networks (setting n to 2 results in the original CPS approach). Let D L and D U denote labelled and unlabelled training data sets respectively, both containing images of size W ? H. The n-CPS architecture consists of a set of n networks f (x; ? j ) with j ? {1, . . . , n}, each with the same architecture but initialised with different set of parameters from ? = {? 1 , ? 2 , . . . , ? n }. The pixel-wise class probability vector predicted by the j-th network f (x; ? j ) on the i-th pixel of the image x is represented by p ij . The supervised loss is calculated in a standard way:</p><formula xml:id="formula_0">L L = 1 |D L | x?DL 1 W ? H W ?H i=1 n j=1 l(p ij , y * i ),<label>(1)</label></formula><p>where l is the standard cross-entropy loss function. The ground truth for the labelled data is marked as y * i . For the n-CPS loss for labelled and unlabelled data, we defined them respectively:</p><formula xml:id="formula_1">L L CPS = 1 |D L | x?DL 1 W ? H W ?H i=1 n j=1 1 n ? 1 n k =j l(p ij , y ik ),<label>(2)</label></formula><formula xml:id="formula_2">L U CPS = 1 |D U | x?DU 1 W ? H W ?H i=1 n j=1 1 n ? 1 n k =j l(p ij , y ik ).<label>(3)</label></formula><p>This time the loss is calculated with y ik , which is the onehot encoded output of the k-th network on the unlabelled data. Finally, the overall loss L is defined as:</p><formula xml:id="formula_3">L = L L + ? L L CPS + L U CPS ,<label>(4)</label></formula><p>where ? is the weight of CPS loss. In equations <ref type="bibr" target="#b1">(2)</ref> and <ref type="formula" target="#formula_2">(3)</ref> we scale the consistency loss for each pair by 1/(n ? 1) to maintain a balance between supervised and CPS part of the loss regardless of the value of n. This scaling factor has this form because each of the n networks in our algorithm can be paired with n ? 1 other networks to calculate CPS loss. <ref type="figure" target="#fig_0">Figure 2</ref> depicts calculating the CPS loss. Details are presented in Algorithm 1. In each batch, we select images from the labelled and unlabelled data set, which are denoted as x L , x U respectively. Then, we perform the forward passes on all the data using all the networks separately. The n-CPS model consists of n networks, and the output of each network is the pixel-wise probability of each class. The i-th one is represented by f ( ? ; ? i ). Then, we perform the cross pseudo supervision for each unique pair of networks n 2 times. The one-hot encoded outputs of the l-th and r-th networks are denoted as Y l and Y r . In this case, pmax is a pixel-wise maximum function that selects each pixel's maximum value from the two masked images. It is calculated without passing the gradient (denoted as ? in the algorithm). The CPS loss is performed both on the labelled and unlabelled data. Then, we calculate the standard supervised loss for each network. The overall loss consists </p><formula xml:id="formula_4">x f (x; ? 1 ) f (x; ? 2 ) ? ? ? f (x; ? n ) P 1 P 2 ? ? ? P n Y 1 Y 2 ? ? ?</formula><formula xml:id="formula_5">Algorithm 1 n-CPS for each batch do L ? 0, L L ? 0, L L CPS ? 0, L U CPS ? 0 x L , x U , Y * ? dataloader.iter() for j = 1, . . . , n do P L j ? f (x L ; ? j ) P U j ? f (x U ; ? j ) end for for (l, r) ? (l, r) ? {1, . . . , n} 2 : l &lt; r do Y U l ? pmax(P U l ) // ? Y U r ? pmax(P U r ) // ? Y L l ? pmax(P L l ) // ? Y L r ? pmax(P L r ) // ? L U CPS ? L U CPS + L CPS P U l , Y U r + L CPS P U r , Y U l L L CPS ? L L CPS + L CPS P L l , Y L r + L CPS P L r , Y L l end for for j = 1, . . . , n do L L ? L L + L L P L j , Y * end for L ? L L + ? (n?1) L U CPS + L L CPS L.backward() end for</formula><p>of the standard labelled loss L L and the CPS losses L L CPS and L U CPS (for labelled and unlabelled data weighted by ? and normalised by the factor of 1 n?1 ). While n-CPS needs n 2 CPS loss calculations per batch, the most computationally expensive forward calls are hit only 2n times per batch, which effectively makes the time complexity of the algorithm linear in n.</p><p>n-CPS with CutMix. We also used the CutMix augmentation algorithm <ref type="bibr" target="#b16">[17]</ref> adapted to semantic segmentation <ref type="bibr" target="#b5">[6]</ref>, as it was proven to be very effective in the original CPS approach <ref type="bibr" target="#b1">[2]</ref>. Algorithm 2 presents our method. In each batch, we select images from the labelled and unlabelled data set, and we randomly select a binary mask M . Regarding the data, x L , x U , x m represent the labelled, unlabelled and mixed data respectively. Notice that there are two different subsets of unlabelled data per batch -x U 1 and x U 2 . They are sampled differently from the labelled data and both are used to generate the mixed data x m , which is the result of the CutMix applied with batch-wise mask M. The CutMix algorithm is defined as follows:</p><formula xml:id="formula_6">CutMix(x U 1 , x U 2 , M) = (1 ? M) x U 1 + M x U 2 . (5)</formula><p>The mask is the same size as the image and is randomly generated to satisfy CutMix constraints (such as leaving out the rectangular area). Then, we perform the forward passes on all the data using all the networks separately, similarly to n-CPS. Cross pseudo supervision for each unique pair of networks is performed slightly differently regarding one-hot encoding vectors. They are obtained by combining images in a CutMixlike approach and applying the maximum function. The overall loss consists of the standard labelled loss L L and the CPS loss L U CPS weighted by ? and normalised by the factor of 1 n?1 . Similarly to the original CPS with CutMix augmentation, the CPS loss is not calculated on supervised data and therefore L L CPS is not calculated. Once again, forward calls are linear in n and setting n = 2 results in the original CPS+CutMix approach.</p><p>Ensemble learning techniques. In the language of ensemble learning, an ensemble is a set of weak learners (models trained independently, each of them with relatively low performance), which together forms a strong model (which should display better performance) <ref type="bibr" target="#b11">[12]</ref>. During the n-CPS</p><formula xml:id="formula_7">Algorithm 2 n-CPS+CutMix for each batch do L ? 0, L L ? 0, L U CPS ? 0 x L , x U 1 , x U 2 , M, Y * ? dataloader.iter() x m ? CutMix(x U 1 , x U 2 , M) for j = 1, . . . , n do P L j ? f (x L ; ? j ) P m j ? f (x m ; ? j ) P U j,1 ? f (x U 1 ; ? j ) P U j,2 ? f (x U 2 ; ? j ) end for for (l, r) ? (l, r) ? {1, . . . , n} 2 : l &lt; r do Y l ? pmax(P U l,1 (1 ? M) + P U l,2 M) // ? Y r ? pmax(P U r,1 (1 ? M) + P U r,2 M) // ? L U CPS ? L U CPS + L CPS (P m l , Y r ) + L CPS (P m r , Y l ) end for for j = 1, . . . , n do L L ? L L + L L P L j , Y * end for L ? L L + ? (n?1) L U CPS L.backward() end for</formula><p>training process, there are n networks trained separately. In the original approach, the evaluation used only the results of the first network and discarded the other ones. While this approach alone was proven to generate state-of-the-art results, our empirical results show that including all the information from the trained networks is beneficial for performance. In the original CPS paper, the experiment showed that even in the last steps of learning, approximately 5% of pixels are labelled differently by the trained networks.</p><p>Therefore, we use the results of all the networks. It can be realised by taking the pixel-wise softmax of the output of each network and then combining the results in several ways. In this paper, we test two of them: max confidence (denoted as mc) and soft voting (sv). In the max confidence approach, we choose the result with the highest score. In other words, for each pixel, we choose the class from the most confident network. In the PyTorch-like notation, that max confidence voting is defined as: softmax(y, dim=2).max(dim=0), where the concatenated output of all networks y is of shape (n, b, c, w, h) -representing consecutively number of the networks, batch size, number of classes, width and height. The soft voting approach is similar, but instead of choosing the class with the highest score, we sum the probabilities of all the networks. This is proportional to the weighted mean of the output of all networks. In the PyTorch-like notation that would translate to softmax(y, dim=2).sum(dim=0). While the networks are not necessarily weak learners in the en-semble learning theory sense, both of the approaches are proven to be effective and improve the performance of n-CPS (see sections 3.2 and 3.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Evaluation</head><p>This section describes the evaluation of the proposed approach. First, we present the setup of our experiments and the training details. We evaluate n-CPS on PASCAL VOC 2012 and Cityscapes data sets. Ablation studies are also performed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Experiment setup</head><p>Datasets. The datasets used in this experiment are the same as in the original CPS paper <ref type="bibr" target="#b1">[2]</ref> -PASCAL VOC 2012 <ref type="bibr" target="#b3">[4]</ref> and Cityscapes <ref type="bibr" target="#b2">[3]</ref>. The PASCAL VOC 2012 contains 21 classes (including the background class). Regarding Cityscapes, it comprises 30 classes. We also follow GCT <ref type="bibr" target="#b7">[8]</ref> protocols regarding the ratio of supervisedto-unsupervised images in the dataset (1/16, 1/8, 1/4, 1/2). The sampling scheme is taken from the CPS paper <ref type="bibr" target="#b1">[2]</ref> to provide a fair comparison with other methods. Both data sets are evaluated using the standard mean intersection over union (mIoU) metric in % over val sets <ref type="bibr" target="#b0">(1,</ref><ref type="bibr">456</ref> images in PASCAL VOC 2012 and 500 in Cityscapes).</p><p>Training details. We use the same training setup as in the original CPS paper <ref type="bibr" target="#b1">[2]</ref>. We extend the original Py-Torch codebase with our n-CPS approach, although training details (such as augmentations or hyperparameters) stays the same. This means that we use DeepLabv3+ <ref type="bibr" target="#b0">[1]</ref> as our network with ResNet-50/101 backbones <ref type="bibr" target="#b6">[7]</ref> paired with mini-batch SGD with momentum (0.9) and weight decay (0.0005). Similarly to the authors of CPS, we also use the poly learning rate policy, in which the initial learning rate is multiplied by 1 ? iter max iter 0.9 . For Pascal VOC, both supervised and CPS loss is computed using standard cross-entropy loss. Regarding Cityscapes, OHEM loss <ref type="bibr" target="#b13">[14]</ref> is used for supervised loss and cross-entropy loss for CPS loss. VOC models were trained on 4?V100 GPUs, whereas Cityscapes models on 8?V100 GPUs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Results</head><p>We report mIoU results from the network with the highest-scoring step (not necessarily the last one). These results use n = 3 as the number of networks and mc and sv ensembling techniques 2 . The training regime is taken from the CPS paper, and it consists of different supervision ratios (1/16, 1/8, 1/4, 1/2) trained for 32/34/40/60 and 128/137/160/240 steps for Pascal VOC and Cityscapes, respectively. We report the best test results during the evaluation (not necessarily the result from the last step).</p><p>Pascal VOC 2012. The first part of <ref type="table">Table 1</ref> presents the results on the Pascal VOC dataset compared to other recent methods (the non-our results are taken from the CPS paper <ref type="bibr" target="#b1">[2]</ref>). For different supervision regimes (1/16, 1/8, 1/4, 1/2), the version without CutMix outperforms the mIoU results reported in the original CPS paper (previous state-of-the-art) by +0.15/+0.49/+1.57/+1.16 percentage points for ResNet-50 and +1.33/+0.75/+1.06/+1.38 pp for ResNet-101. Regarding the version with Cut-Mix algorithm, the n-CPS approach is better than CPS by +0.05/+0.54/+0.95/+0.50 pp for ResNet-50 and +1.38/+1.55/+1.29/+1.62 pp for ResNet-101. Mean confidence and soft voting ensembles behave similarly, and usually, the difference is slight (no more than ?0.1 mIoU). <ref type="figure" target="#fig_1">Figure 3</ref> shows masks from the 3-CPS model on this dataset.</p><p>Interestingly, for ResNet-50, the version without CutMix outperforms the version with CutMix in the 1/2 supervised scenario. The observed trend suggests that CutMix is vital for small supervision ratios, but there is a visible decrease in its effectiveness with the increasing share of supervised data. Since CutMix can be perceived as a heavy augmentation technique, we hypothesise that it can even deteriorate the training for higher shares of supervised data. However, this effect is not observed with ResNet-101 -at least up to 1/2 supervision scheme.</p><p>Cityscapes. The second part of <ref type="table">Table 1</ref> presents the results on the Cityscapes dataset compared with other recent methods. Similarly, the non-our results are taken from the CPS paper <ref type="bibr" target="#b1">[2]</ref>. We do not report results for the ResNet-101 backbone network due to the unavailability of the appropriate hardware (8?V100 GPUs were not enough in terms of RAM). On ResNet-50 and without CutMix, n-CPS achieved ?0.01/+0.49/?0.11/+0.65 pp change of mIoU compared to CPS. With CutMix on, our approach outperforms the current state of the art by +1.61/+1.00/+0.58/+0.43 pp. Even without testing our approach on ResNet-101, the n-CPS on ResNet-50 outperforms CPS on ResNet-101 on 1/16 supervision (+1.36 mIoU). Interestingly, the model based on ResNet-50 with the CutMix algorithm showed a slightly worse performance 1/2 supervision than the model without it. This behaviour is consistent with the PASCAL VOC 2012 results and possibly it can share a similar explanation.</p><p>Variance of the results. Results reported in <ref type="table">Table 1</ref> are collected from single runs. We decided to run the experiment several times to assess the method's variance and col- lect the results. <ref type="table" target="#tab_2">Table 2</ref> shows the results of such experiment for n-CPS (n = 3, ? = 1.5) on Pascal VOC 1/8 supervised (ResNet-50, without CutMix) with five runs. We report the best models for single network inference, max confidence, <ref type="table">Table 1</ref>. Comparison of the semi-supervised segmentation methods (mIoU under different supervision regimes, DeepLabv3+).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ResNet-50</head><p>ResNet-101  <ref type="table">Table 1</ref> and <ref type="table" target="#tab_3">Table 3</ref>, as they were collected in a separate run. All the methods have their standard deviation less than 0.25 pp. The means for ensemblingbased evaluation are almost identical, although the standard deviations are slightly smaller for 3-CPS-sv.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Ablations</head><p>This section presents the ablation experiments on the Pascal VOC 2012 dataset. We try to assess the importance of generalising CPS by controlling the number of networks n. Then, we address the influence of the max confidence ensembling technique without cross-pseudo supervision. Finally, we study how the choice of ensembling technique influences the performance.</p><p>Importance of n-CPS. To assess the importance of n-CPS, we run a series of experiments on the PASCAL VOC 2012 dataset with different values. The results are reported in <ref type="table" target="#tab_3">Table 3</ref> in terms of the mIoU under different supervision regimes. We report the results of the original CPS <ref type="bibr" target="#b1">[2]</ref>, as well as the results of our implementation (n-CPS, where n ? {2, 3}). All networks were trained on DeepLabv3+ with ResNet-50/101 as a backbone and using ? = 1.5. Most importantly, we did not use any ensembling techniques here. Note that while the original CPS is equivalent to ours reproduced with 2-CPS, these results are slightly different due to randomness. Nevertheless, one can observe that evaluating only the first network f 1 (as in the original CPS paper), which is not necessarily the best performing one, does not show the immediate advantage of using n-CPS (except ResNet-101 without CutMix).</p><p>Importance of ensemble networks. The previous paragraph showed that the choice of increased n does not improve the performance alone. However, in Section 3.2 we have already shown that the method combined with proper ensembling technique yields state-of-the-art results. To isolate the effect of ensembling, we performed a series of experiments with ? set to zero. This disabled cross-pseudo supervision loss from the learning process and effectively turned the task to standard supervised ensemble learning. <ref type="table">Table 4</ref> presents the results of this ablation study. As it turns out, for 2-CPS, ensembling resulted in +1.57 pp to Importance of the ensemble learning method. We also investigate how the choice of ensembling technique influences the performance during the whole training process. Apart from the best results reported in <ref type="table">Table 1</ref>, we also control it in a whole training procedure. To do so, we trained n-CPS (n = 3, ? = 1.5) with the 1/8 supervised regime with ResNet-50 on the Pascal VOC dataset and control evaluation results for different types of ensembling techniques presented in Section 2: no ensembling (i.e. only the first network is evaluated), max confidence and soft voting. <ref type="figure" target="#fig_3">Figure 4</ref> shows the results of the ablation study. The highest mIoU scores were 73.85 for soft voting and 73.71 for max confidence. In this case, soft voting was better than max confidence by 0.11 mIoU points on average. Soft voting was also better on the majority of learning steps. Notice that these results are slightly different from the results reported in <ref type="table">Table 1</ref>, as they were collected in a separate run.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Related Work</head><p>This section briefly describes other work in the areas of semantic segmentation and semi-supervised learning. Step mIoU no ensembling max confidence softmax soft voting Semantic segmentation. Semantic segmentation is a fundamental problem in computer vision, which consider assigning labels to each pixel in an image. Modern deep neural were networks successfully adapted to this problem, with fully-convolutional networks (FCN) <ref type="bibr" target="#b9">[10]</ref> being one of the first and most influential approaches. Many solutions follow the encoder-decoder architecture, such as U-Net <ref type="bibr" target="#b12">[13]</ref>. Numerous techniques have been developed, such as pyramid scene parsing in PSPNet <ref type="bibr" target="#b17">[18]</ref>, or dilated convolutions and atrous spatial pyramid pooling as in DeepLabv3+ <ref type="bibr" target="#b0">[1]</ref>. Another architecture, HRnet <ref type="bibr" target="#b15">[16]</ref> is focused on keeping high-resolution representations during the training.</p><p>Semi-supervised learning. A recent line of research has shown that deep neural networks can be successfully used for semantic segmentation, given the right amount of data. From the practical point of view, this is a challenging problem due to the high cost of labelling. The goal of semisupervised learning is to learn a model on a dataset, for which the labels are known on a certain percentage of the data. Semi-supervised learning combines semantic segmentation and semi-supervised learning. Recently, an intense effort can be observed in developing two families of techniques: contrastive learning and consistency regularisation. Contrastive learning is built around learning representations which are close for the samples of the same class and far otherwise. Consistency regularisation assumes that the same samples should yield the same labels under different -often heavy -augmentations and perturbations. The disagreement between models is later used for the training. Apart from CPS <ref type="bibr" target="#b1">[2]</ref>, there are several architectures dedicated to semi-supervised learning. A somewhat similar apporaches are cross-consistency training (CCT) <ref type="bibr" target="#b10">[11]</ref> or GCT <ref type="bibr" target="#b7">[8]</ref>, which uses cross-confidence consistency for feature perturbation. Mean Teacher <ref type="bibr" target="#b14">[15]</ref> considers a setting in which two models (student and teacher) are trained on the same dataset but using different augmentations. The student model is trained in a standard way, while the teacher model is an exponential moving average of student models from previous steps. The latter is also responsible for generating pseudo labels. The Mean Teacher framework combined with CutMix <ref type="bibr" target="#b16">[17]</ref> was used for semi-supervised segmentation in CutMix-Seg <ref type="bibr" target="#b5">[6]</ref>. In Dynamic Mutual Training (abbreviated as DMT) <ref type="bibr" target="#b4">[5]</ref>, two neural networks are trained using a dynamically re-weighted loss function. The authors of DMT leverages the disagreement between the models, which indicates a possible error and lowers the loss value. ReCo (an abbreviation from Regional Contrast) <ref type="bibr" target="#b8">[9]</ref> is a pixel-level contrastive learning framework, which incorporates memory-efficient sampling strategies. The framework proved to be very effective in few-supervision scenarios, reaching 50% mIoU on Cityscapes while requiring only 20 labelled images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Summary</head><p>In this paper, we presented n-CPS -a generalisation of the consistency regularisation framework CPS. We also proposed to utilise all the learned subnetworks for evaluation purposes using the ensemble learning techniques. Evaluation of our approach on the Pascal VOC dataset showed that it sets the new state-of-the-art in its category. An unavoidable limitation of such a study stems from the large number of parameters involved in the models. In practical settings, this means relatively large GPU memory requirements. Further work should consider the evaluation of ResNet-101 on the Cityscapes dataset, as the results on ResNet-50 are promising. It should also consider evaluating the behaviour of models with n ? 4 for both ResNet-50 and ResNet-101. Finally, higher values of n might also be tested using smaller networks (such as ResNet-18) to explore the behaviour of the models and the influence of ensembling techniques in such conditions.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>The architecture of n-CPS during the CPS loss calculation. The slashed line (with //) represents not passing the gradient, whereas the dashed line (--) denotes cross-pseudo supervision.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Example results for 3-CPS-sv (DeepLabv3+, ResNet50, 1/8 supervision, no CutMix) on Pascal VOC 2012 val dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Ablation study of different ensembling methods for 3-CPS on Pascal VOC 1/8 supervised (ResNet-50, without CutMix).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Statistics of the multiple runs of the same training (3-CPS, 1/8 supervised, ResNet50 on Pascal VOC, ? = 1.5).</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>run</cell><cell></cell><cell>mean</cell><cell>std</cell><cell>min</cell><cell>max</cell></row><row><cell></cell><cell>#1</cell><cell>#2</cell><cell>#3</cell><cell>#4</cell><cell>#5</cell></row><row><cell>3-CPS</cell><cell cols="5">72.91 72.57 72.94 72.75 73.17 72.87 0.23 72.57 73.17</cell></row><row><cell cols="6">3-CPS-mc 73.54 73.42 73.60 73.16 73.55 73.45 0.18 73.16 73.60</cell></row><row><cell>3-CPS-sv</cell><cell cols="5">73.50 73.40 73.62 73.22 73.50 73.45 0.15 73.22 73.62</cell></row><row><cell cols="4">mIoU, while ensembling and CPS (? = 1.5) provided an-</cell><cell></cell></row><row><cell cols="4">other +2.78 pp for 1/16 supervised data. The trend is con-</cell><cell></cell></row><row><cell cols="4">sistent in different supervision regimes, though it dimin-</cell><cell></cell></row><row><cell cols="4">ishes with larger shares of supervised data (+1.47/+2.04</cell><cell></cell></row><row><cell cols="4">pp for 1/8 supervised, +1.13/+1.44 pp for 1/4 supervised,</cell><cell></cell></row><row><cell cols="4">+0.59/+0.42 pp for 1/2 supervised). Regarding 3-CPS,</cell><cell></cell></row><row><cell cols="4">one can observe similar behaviour (+2.09 pp with ensem-</cell><cell></cell></row><row><cell cols="4">bling and +2.08 pp with ? = 1.5 for 1/16 supervised,</cell><cell></cell></row><row><cell cols="4">+1.79/+1.88 pp for 1/8 supervised, +2.04/+0.72 pp for</cell><cell></cell></row><row><cell cols="4">1/4 supervised, +1.53/+0.02 pp for 1/2 supervised). This</cell><cell></cell></row><row><cell cols="4">shows that the choice of ensembling and CPS positively af-</cell><cell></cell></row><row><cell cols="4">fects the performance, especially compared to the ensem-</cell><cell></cell></row><row><cell cols="4">bling alone. Regarding the n with ensembling, we observe</cell><cell></cell></row><row><cell cols="4">that the performance of the 3-CPS-ens is better than 2-CPS</cell><cell></cell></row><row><cell cols="4">(+0.11 pp for 1/8 supervised, +0.10 pp for 1/4 supervised,</cell><cell></cell></row><row><cell cols="4">+0.27 pp for 1/2 supervised) except 1/16 supervised (?0.60</cell><cell></cell></row><row><cell cols="4">pp). This justifies the generalisation of the original CPS</cell><cell></cell></row><row><cell cols="4">combined with ensembling for larger shares of supervised</cell><cell></cell></row><row><cell>data.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Ablation study on the number of networks (no ensemble techniques, DeepLabv3+ with ResNet-50/101 backbone, ? = 1.5).</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">ResNet-50</cell><cell></cell><cell>ResNet-101</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>1/16</cell><cell>1/8</cell><cell>1/4</cell><cell>1/2</cell><cell>1/16</cell><cell>1/8</cell><cell>1/4</cell><cell>1/2</cell></row><row><cell cols="3">CPS (Chen et al.)</cell><cell></cell><cell cols="5">68.21 73.20 74.24 75.91 72.18 75.83 77.55 78.64</cell></row><row><cell cols="3">2-CPS (ours)</cell><cell></cell><cell cols="5">68.64 73.30 75.34 76.33 73.15 76.07 77.32 78.64</cell></row><row><cell cols="3">3-CPS (ours)</cell><cell></cell><cell cols="5">67.5 72.67 75.12 76.36 73.32 76.03 78.25 78.87</cell></row><row><cell cols="9">CPS+CutMix (Chen et al.) 71.98 73.67 74.90 76.15 74.48 76.44 77.68 78.64</cell></row><row><cell cols="4">2-CPS+CutMix (ours)</cell><cell cols="3">71.43 73.99 75.37</cell><cell cols="2">75.6 74.59 77.11 77.64 78.65</cell></row><row><cell cols="4">3-CPS+CutMix (ours)</cell><cell cols="5">71.11 73.56 74.68 75.86 74.98 76.98 77.95 79.67</cell></row><row><cell cols="6">Table 4. Ablation study on importance of CPS (? ? {0, 1.5}),</cell><cell></cell><cell></cell></row><row><cell cols="6">n-CPS (n ? {2, 3}) and ensembling (none, max confidence) on</cell><cell></cell><cell></cell></row><row><cell cols="4">Pascal VOC (ResNet-50, without CutMix).</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>?</cell><cell>1/16</cell><cell>1/8</cell><cell>1/4</cell><cell>1/2</cell><cell></cell><cell></cell></row><row><cell>2-CPS</cell><cell cols="5">0 64.61 69.83 73.08 75.72</cell><cell></cell><cell></cell></row><row><cell>2-CPS-mc</cell><cell cols="2">0 66.18</cell><cell cols="3">71.3 74.21 76.13</cell><cell></cell><cell></cell></row><row><cell cols="6">2-CPS-mc 1.5 68.96 73.34 75.65 76.73</cell><cell></cell><cell></cell></row><row><cell>3-CPS</cell><cell cols="5">0 64.19 69.78 72.63 75.45</cell><cell></cell><cell></cell></row><row><cell>3-CPS-mc</cell><cell cols="5">0 66.28 71.57 75.03 76.98</cell><cell></cell><cell></cell></row><row><cell cols="6">3-CPS-mc 1.5 68.36 73.45 75.75 77.00</cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">The code will be released after the publication. For reviewers, it is available as a supplementary material.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Our evaluation was primarily meant to be done on max confidence voting. Soft voting was added later and was tested only on the models that were best performing with the max confidence voting. This means that there is a chance that reported soft voting results might be slightly improved if tested on all steps.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>Some experiments were performed using the Entropy cluster at the Institute of Informatics, University of Warsaw, funded by NVIDIA, Intel, the Polish National Science Center grant UMO2017/26/E/ST6/00622 and ERC Starting Grant TOTAL. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CRediT author statement</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Encoder-decoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Liang-Chieh Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Semi-supervised semantic segmentation with cross pseudo supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaokang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhui</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="2613" to="2622" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uwe</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3213" to="3223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">I</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Dmt: Dynamic mutual training for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengyang</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qianyu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiqi</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangliang</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuequan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lizhuang</forename><surname>Ma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.08514</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Semi-supervised semantic segmentation needs strong, high-dimensional perturbations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoff</forename><surname>French</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Mackiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Finlayson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Guided collaborative training for pixel-wise semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanghan</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaican</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiong</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rynson Wh</forename><surname>Lau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2020: 16th European Conference</title>
		<meeting><address><addrLine>Glasgow, UK</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="429" to="445" />
		</imprint>
	</monogr>
	<note>Proceedings, Part XIII 16</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Bootstrapping semantic segmentation with regional contrast</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shikun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuaifeng</forename><surname>Zhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Johns</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew J</forename><surname>Davison</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.04465</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Semisupervised semantic segmentation with cross-consistency training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yassine</forename><surname>Ouali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C?line</forename><surname>Hudelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myriam</forename><surname>Tami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="12674" to="12684" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Ensemble-based classifiers. Artificial intelligence review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lior</forename><surname>Rokach</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Unet: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical image computing and computer-assisted intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Training region-based object detectors with online hard example mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="761" to="769" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antti</forename><surname>Tarvainen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harri</forename><surname>Valpola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.01780</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Deep high-resolution representation learning for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianheng</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Borui</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaorui</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yadong</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingkui</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<idno>2020. 7</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Cutmix: Regularization strategy to train strong classifiers with localizable features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangdoo</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyoon</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyuk</forename><surname>Seong Joon Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsuk</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjoon</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="6023" to="6032" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2881" to="2890" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
