<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Feature Erasing and Diffusion Network for Occluded Person Re-Identification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhikang</forename><surname>Wang</surname></persName>
							<email>zkwang00@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Monash University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">SenseTime Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Zhu</surname></persName>
							<email>zhufeng@sensetime.com</email>
							<affiliation key="aff1">
								<orgName type="laboratory">SenseTime Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shixiang</forename><surname>Tang</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">The University of Sydney</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Zhao</surname></persName>
							<email>zhaorui@sensetime.com</email>
							<affiliation key="aff1">
								<orgName type="laboratory">SenseTime Research</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution" key="instit1">Qing Yuan Research Institute</orgName>
								<orgName type="institution" key="instit2">Shanghai Jiao Tong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihuo</forename><surname>He</surname></persName>
							<email>lihuo.he@gmail.com</email>
							<affiliation key="aff4">
								<orgName type="institution">Xidian University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangning</forename><surname>Song</surname></persName>
							<email>jiangning.song@monash.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Monash University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Feature Erasing and Diffusion Network for Occluded Person Re-Identification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T10:54+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Occluded person re-identification (ReID) aims at matching occluded person images to holistic ones across different camera views. Target Pedestrians (TP) are often disturbed by Non-Pedestrian Occlusions (NPO) and Non-Target Pedestrians (NTP). Previous methods mainly focus on increasing the model's robustness against NPO while ignoring feature contamination from NTP. In this paper, we propose a novel Feature Erasing and Diffusion Network (FED) to simultaneously handle challenges from NPO and NTP. Specifically, aided by the NPO augmentation strategy that simulates NPO on holistic pedestrian images and generates precise occlusion masks, NPO features are explicitly eliminated by our proposed Occlusion Erasing Module (OEM). Subsequently, we diffuse the pedestrian representations with other memorized features to synthesize the NTP characteristics in the feature space through the novel Feature Diffusion Module (FDM). With the guidance of the occlusion scores from OEM, the feature diffusion process is conducted on visible body parts, thereby improving the quality of the synthesized NTP characteristics. We can greatly improve the model's perception ability towards TP and alleviate the influence of NPO and NTP by jointly optimizing OEM and FDM. Furthermore, the proposed FDM works as an auxiliary module for training and will not be engaged in the inference phase, thus with high flexibility. Experiments on occluded and holistic person ReID benchmarks demonstrate the superiority of FED over state-of-theart methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Person Re-Identification (ReID) aims at retrieving the same pedestrians captured by different cameras with differ-* Zhikang Wang did this work as an intern in SenseTime Research. ? Corresponding author. ? This research was supported partially by the National Natural Science Foundation of China (Grant Nos. 61876146).  <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b42">43]</ref>. Most of these approaches assume that a holistic body of each pedestrian is available for feature extraction. However, in real-world scenarios, e.g., railway stations, schools, hospitals, and shopping malls, pedestrians are inevitably disturbed by non-pedestrian occlusions (NPO) and non-target pedestrians (NTP). Therefore, designing a powerful network for the occluded person ReID is essential. Methods assisted by human key points <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b33">34]</ref> and human parsing information <ref type="bibr" target="#b29">[30]</ref> dominate the state-of-the-art performance of the occluded ReID task. Generally, an auxiliary model extracts the body information first, and then the extracted information will assist the training of models. The strategy can greatly avoid mistakenly treating NPO as human parts. However, such methods have many caveats. Firstly, due to the domain gap between the training and testing data, the performance of the auxiliary models can not be consistent. In <ref type="figure" target="#fig_0">Fig.1</ref>, we adopt official pose estimation model <ref type="bibr" target="#b4">[5]</ref> and retrained human parsing model <ref type="bibr" target="#b5">[6]</ref> to extract body information. It is clear that both models perform well on holistic and object occluded pedestrian images but fail on multi-pedestrian ones, which means that noise from NTP will contaminate the final representations. Compared with object occlusion, the characteristics of NTP will result in a higher mismatching probability because of the semantic guidance. Secondly, the human parsing model can not recognize some person belonging, e.g., backpacks, umbrellas, which may lead to the deficiency of valuable information. At last, the enormous computation brought by the auxiliary models makes it unacceptable for real-time video surveillance.</p><p>To tackle the challenges above, we propose the feature erasing and diffusion network (FED) to simulate NPO on images and NTP in the feature space for increasing the model's perception ability towards TP. Specifically, we aim at the NPO feature erasing by proposing the NPO augmentation strategy along with an occlusion erasing module (OEM). The augmentation strategy will generate object occluded data of pedestrians by pasting cropped patches with a specific strategy. At the same time, by analyzing the pixellevel value differences, we can get precise part labels, indicating whether object occlusion or not. We refer to the part labels as occlusion masks. Sequentially, the occlusion masks will guide the OEM to analyze the semantic information and generate the final occlusion scores for part features. For alleviating the distractions from NTP, a straightforward way is pasting other pedestrians onto the image for data augmentation. However, pedestrian images with diversified background information can destroy the globality of the original images by simple pasting. Besides, the resize operation needs designing carefully for maintaining aspect ratio. Therefore, image-level augmentation for NTP is challenging and complex. Here, we propose a learnable structure named feature diffusion module (FDM), which will simulate multi-pedestrian images by diffusing characteristics of NTP to the original features. With the guidance of occlusion scores from OEM, the feature diffusion operation will be conducted only on body parts, guaranteeing the simulated features are more realistic. By optimizing the model through diffused features, we can indirectly improve the model's perception ability towards TP and robustness towards NTP.</p><p>In summary, we propose the feature erasing and diffusion network (FED) to tackle the distractions from NPO and NTP for occluded person ReID. FED consists of three innovative components: NPO augmentation strategy, occlusion erasing module (OEM), and feature diffusion module (FDM). These components enable the network to precisely perceive the TP regardless of the NPO and NTP. At the same time, extensive experiments on both occluded datasets (Occluded-DukeMTMC <ref type="bibr" target="#b8">[9]</ref>, Partial-REID <ref type="bibr" target="#b28">[29]</ref>, and Occluded-REID <ref type="bibr" target="#b13">[14]</ref>) and holistic datasets (Market-1501 <ref type="bibr" target="#b11">[12]</ref> and DukeMTMC-reID <ref type="bibr" target="#b12">[13]</ref>) demonstrate the effectiveness of our proposed method. Especially on the Occluded-DukeMTMC and Occluded-REID dataset, our Rank-1 and mAP accuracy surpass other state-of-the-art methods by a large margin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head><p>In this section, we briefly overview the existing methods of holistic person ReID and occluded person ReID.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Holistic Person Re-Identification</head><p>Person re-identification (ReID) aims to retrieve a person of interest in other camera views and great progress has been made in recent years. Existing ReID methods can be summarized into three categories, including handcrafted descriptor methods <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20]</ref>, metric learning methods <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22]</ref>, and deep learning methods <ref type="bibr" target="#b22">[23]</ref><ref type="bibr" target="#b23">[24]</ref><ref type="bibr" target="#b24">[25]</ref>. Due to the publishing of large-scale datasets and the development of Graphics Processing Unit (GPU), deep learning based methods have become dominant in the person reidentification area nowadays. Recent works utilizing partbased features have achieved state-of-the-art performance for the holistic person ReID. Zhang et al. <ref type="bibr" target="#b25">[26]</ref> perform an automatic part feature alignment through the shortest path loss during the learning, without requiring extra supervision or explicit pose information. Sun et al. <ref type="bibr" target="#b23">[24]</ref> propose a general part-level feature learning method, which can accommodate various part partitioning strategies. The attention mechanism has also been adopted to ensure the model focus on human areas, which extracts more effective features <ref type="bibr" target="#b26">[27]</ref><ref type="bibr" target="#b27">[28]</ref><ref type="bibr" target="#b28">[29]</ref>. However, these methods fail to retrieve persons with high accuracy when occlusions happen. The shortcoming limits the utility of the methods, especially in the common crowd scenes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Occluded Person Re-Identification</head><p>The study of the occluded person ReID is proposed by Zhou et al. <ref type="bibr" target="#b13">[14]</ref>. The training set and gallery set are constructed by holistic pedestrian images, and the query set is constructed by occluded pedestrian images. Recent study methods in this topic can be divided into two categories: assisted by pose estimation <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18]</ref> and human parsing <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b40">41]</ref> part regions and performing comparisons with consideration of visibility, the method not only reduces background noise but also achieves body alignment.</p><p>Different from the above methods, our approach does not rely on extra models and can be trained in an end-to-end fashion. We simulate NPO and NTP on both image and feature levels and thus greatly improve the model robustness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Feature Erasing and Diffusion Network</head><p>In this section, we introduce the proposed feature erasing and diffusion network (FED) in detail. The overall architecture of the network is illustrated in <ref type="figure" target="#fig_1">Fig.2</ref>. It begins with the NPO augmentation strategy that produces image pairs and occlusion masks. Following <ref type="bibr" target="#b2">[3]</ref>, we simply adopt the Vision Transformer (ViT) <ref type="bibr" target="#b37">[38]</ref> as the feature extractor. Position embeddings and a classification [cls] token are prepended to the input image. The output feature for each image is f ? R (n+1)?c , where n + 1 indicates the images tokens and one [cls] token, and c is the channel dimension. Under our settings, n and c are 128 and 768, respectively. Next, we conduct the part pooling operation on image tokens and obtain N local features, which will be fed into the occlusion erasing module (OEM). Here, we set N as 4 in accordance with NPO augmentation strategy. Two memory banks will be initialized at the beginning and updated with training processing. The auxiliary feature diffusion module (FDM) takes the image features and the first memory bank as input for multi-pedestrian simulation. Details of each module will be presented in the following section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">NPO Feature Erasing</head><p>Typically, NPO feature erasing needs auxiliary information for guidance. In this section, we propose the NPO augmentation strategy and occlusion erasing module to explicitly learn NPO-robust features.</p><p>NPO Augmentation Strategy. Occlusion augmentation strategies are effective in occluded ReID. Typically, there are two categories: (1) Zhong et al. <ref type="bibr" target="#b1">[2]</ref> randomly select a rectangle region in an image and erase its pixels with random values; (2) Chen et al. <ref type="bibr" target="#b0">[1]</ref> paste the selected objects or backgrounds onto images. The first method helps to reduce the risk of over-fitting and makes the model robust to occlusion. However, when facing the diversified occlusions, the trained model fails to identify them due to weak generalization. The second method implicitly learns NPO-robust features by simulating the occlusion scenes. However, it fails to fully utilize the potential information, e.g., precise occlusion region, brought by the augmentation.</p><p>Inspired by the methods above, we propose the NPO augmentation strategy. The strategy consists of occlusion augmentation and mask generation, which will generate augmented images for occlusion simulation and occlusion masks for further semantic analysis, respectively.</p><p>Empirically, occlusions happen at four locations (top, bottom, left, right) with a quarter to half areas. Our augmentation strategy is similar to Chen et al. <ref type="bibr" target="#b0">[1]</ref>, but with particular modifications. For occlusion augmentation, one important step is the patch set collection. To avoid extra body parts included in the patch set, we manually crop the backgrounds and occlusion objects from the chosen images in the training set and refer to these patches as the occlusion set. We formally describe the occlusion augmentation process as follows. Firstly, given an input image, we do common augmentations, e.g., resize, padding, and random crop, on it and get x ? R 3?h?w , where h and w represent the height and width, respectively. Secondly, we select a patch p ? R 3?p h ?pw from the occlusion set, where p h and p w are the height and width. Rather than randomly paste the patch onto x, we believe that only reasonable occlusions for pedestrians can generate valuable data for training. Therefore, we calculate the aspect ratio of the patch: ? = p h /p w . When ? is larger than 3, it implies the patch is more like a vertical occlusion, otherwise horizontal occlusion. Common augmentations, e.g., random crop, and colorjitter, are also applied on the patch for increasing its varieties. We resize the patch according to the occlusion type (horizontal or vertical) to R (H/4?H/2,W ) and R (H,W/4?W/2) , respectively. Thirdly, we randomly select one corner of x as the starting point and past the augmented patch on it. The augmented image is named x ? .</p><p>Mask generation is a fine-to-coarse process. Firstly, we get the pixel differences by subtraction and absolute function d = |x ? x ? |. Considering the subsequent part-based occlusion erasing module, each position of the occlusion mask should correspond to specific body parts. However, there are mis-alignments of semantics (body parts) between different images, fine-grained occlusion masks will have many false labels. Therefore, we roughly split the image into 4 stripes horizontally and aim at labeling them. As said before, there are vertical and horizontal occlusions in realworld scenarios. Vertical occlusion only damages parts of the symmetric characteristics. Usually, ReID models can easily distinguish between pedestrians and vertical occlusions and get discriminative representations without referring to further information. Therefore, the vertical occlusion is ignored while mask generation and stripes are regarded as a human part (valued 1). For the horizontal occlusion augmentation, we conduct the soft binarization operation. We take stripes covered more than three-quarters as occlusions (value 0), otherwise as human parts (value 1). In this way, we get the precise occlusion masks for the image pair.</p><p>Occlusion Erasing Module. Although the augmentation strategy is employed while training, the NPO may still contaminate representations. To further eliminate the influence of NPO, we propose the occlusion erasing module (OEM) for part feature erasing. As shown in <ref type="figure" target="#fig_1">Fig.2</ref>, the module is constructed by 4 sub-modules corresponding to each body part. For each sub-module, it is constructed by two fully connected (FC) layers, one layer normalization <ref type="bibr" target="#b30">[31]</ref>, and one Sigmoid function. The layer normalization is placed between the FC layers, and the Sigmoid function is located at the end. The first FC layer compresses the channel dimension to the quarter of the original one, aiming to wipe off the characteristic information and reserve the  semantic ones. The final Sigmoid function will output the regressed occlusion scores s i for each part feature. We refer to the multiplication between the occlusion scores and part features as f ? . Functionally the progress can be represented by</p><formula xml:id="formula_0">f ? i = Sigmoid(W rg LN (W cp f i )) ? f i ,<label>(1)</label></formula><p>where W cp ? R c/4?c , W rg ? R 1?c/4 , LN is the layer normalization and i indicates i th part feature.</p><p>Here, the occlusion masks from the NPO augmentation strategy are adopted to supervise the training of OEM. We calculate the Mean Square Error (MSE) Loss between occlusion masks and occlusion scores, and the function can be expressed as</p><formula xml:id="formula_1">L M SE = 1 N 4 i=1 (s i , mask i ).<label>(2)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Feature Diffusion Module</head><p>Previous works have not focused on the challenges of NPO. Apart from destroying the feature integrity of the TP, NTP also contaminates representations with realistic semantic noise. To solve this issue, we propose a learnable structure named feature diffusion module (FDM) to simulate multi-pedestrian images in the feature space. By optimizing the diffused features, we aim at indirectly enhancing the model's perception ability towards TP and robustness towards NTP. As shown in <ref type="figure" target="#fig_3">Fig.3</ref>, apart from the image features, an extra memory bankM, which is a collection of characteristics, is taken as the input. In the following session, we will introduce M and FDM, respectively.</p><p>Memory Bank. The generation of M includes memory initialization and memory update. We follow the same strategy as <ref type="bibr" target="#b3">[4]</ref>. The memory is initialized with the ID centers in the training set. We get the extracted features by performing forward computation, and average features with identical identities to get ID centers. Note that the memory initialization is only operated at the beginning of the algorithm and memory update is processed at each iteration in each mini-batch during training. The k-th center c k is updated by the mean of the encoded features belonging to identity k in the mini-batch as:</p><formula xml:id="formula_2">c k = mc k + (1 ? m) 1 |B k | f ? i ?B k f ? i ,<label>(3)</label></formula><p>where B k denotes the feature set belonging to identity k in the mini-batch, m is the momentum coefficient for updating, f ? is the flattened features after OEM. Apart from acting as the characteristic set, the memory bank M is also adopted for calculating the Contrastive Loss which will be introduced in the following section. We set m as 0.2 in our experiments. Feature Diffusion Module. Essentially, FDM is a modified cross attention module based on the standard architecture of the transformer <ref type="bibr" target="#b6">[7]</ref>. Given the feature vector, queries Q arise from the f ? , and keys K and values V arise from the memory bank M. The input feature is f ? ? R 1?(N ?c) , where N corresponds to the previous part pooling operation and is 4. Firstly, we conduct Memory Searching Operation between f ? and M. It finds K nearest centers M K ? R K?(N ?c) with different identities from the input image. Cosine distance is adopted for measurement. Here, we discard the center with an identical identity for avoiding polarization of the attention matrix which is calculated through cross-product. Formally,</p><formula xml:id="formula_3">Q = f ? W 1 , K i = M K i W 2 , V i = M K i W 3 ,<label>(4)</label></formula><p>where i ? 1, 2, ..., K, and</p><formula xml:id="formula_4">W 1 ? R d?d ? , W 2 ? R d?d ? , W 3 ? R d?d ?</formula><p>are linear projections. Then we calculate the attention matrix and corresponding part features. Formally,</p><formula xml:id="formula_5">m i = exp(? i ) K j=1 exp(? j ) , ? i = QK i ? d k ,<label>(5)</label></formula><p>where ? d k is a scaling factor. Each element of the attention matrix indirectly indicates the connections between Q and K i , and the cross-product operation between V and the attention matrix will generate the diffused features. The aggregation process can be defined as:</p><formula xml:id="formula_6">f d = Att(Q, K, V ) = K i=1 m i V i ,<label>(6)</label></formula><p>The multi-head attention operation is of great significance in this module. Since M K has many similar patterns with the input image and these patterns are distributed randomly in K feature centers. The multi-head operation will split each center into multi parts and generate attention weights for each part individually, thus ensuring more patterns similar to TP and sufficient unique patterns of NTP can be aggregated. In this way, we can simulate the multi-pedestrian images on feature level. After the cross attention operation, we utilize the post-layer normalization feed forward network (F F N 1 ) [8] to conduct non-linear transformation. F F N 1 (?) is a simple neural network with two fully connected layers and one activation function. The residual connection before the layer normalization is applied. Next, the occlusion scores generated by OEM are adopted for weighted summation between the transformed features and f ? . This ensures the characteristics of NTP are only added on human parts rather than pre-recognized object occlusion parts, improving the realness and quality of the diffused features. Besides, the weighted residual operation can stabilize the training process. Then, we utilize another F F N 2 <ref type="bibr" target="#b7">[8]</ref> for generating the final diffused representation of each image. Formally,</p><formula xml:id="formula_7">f ? d = F F N 2 (mask ? F F N 1 (f d ) + f ? ),<label>(7)</label></formula><p>where F F N 2 has the same structure as F F N 1 . Since the FDM is just an auxiliary module for simulation during training, it will be removed in the inference phase. This makes our model more concise and flexible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Loss Functions</head><p>There are three varieties of loss functions in our method, including Mean Square Error (MSE) Loss, Cross Entropy Loss, and Contrastive Loss. We refer to Cross Entropy Loss as ID Loss in this paper. As shown in <ref type="figure" target="#fig_1">Fig.2</ref>, we calculate the ID Loss on the output features of the classification [cls] token, flattened features after the OEM, and features after the FDM. Therefore, there are three additional fully connected layers on the top of the features to calculate the ID probabilities. Functionally, ID Loss can be presented as:</p><formula xml:id="formula_8">L ID = ?y i log( exp(W i f i ) IDs j=1 exp(W j f j ) ),<label>(8)</label></formula><p>where W is a linear projection matrix, y i is the corresponding label and IDs is the total number of identities. As for the Contrative Loss, the key components are the negative and positive samples. There are two memory banks in our algorithm, the first is generated after the OEM and the second is generated after FDM. The initialization and update strategies have been introduced in Sec 3.2. Functionally, the Contrative Loss is:</p><formula xml:id="formula_9">L C = ?log exp(&lt; f, c i &gt; /? ) IDs j exp(&lt; f, c j &gt; /? ) ,<label>(9)</label></formula><p>where ? is a predefined temperature parameter and c i represents the feature center with an identical identity. Although the training strategy is a parallel architecture, the lower branch does not involve in the memory initialization and update due to the characteristic deficiency caused by the NPO augmentation. In <ref type="figure" target="#fig_1">Fig.2</ref>, we utilize the solid lines to represent jointly memory update and loss calculation and dashed lines to represent loss calculation only. Therefore, the final loss function can be expressed: Occluded-REID <ref type="bibr" target="#b13">[14]</ref> is an occluded person ReID dataset captured by mobile cameras. It consists of 2,000 images belonging to 200 identities. Each identity has five full-body person images and five occluded person images with different viewpoints and different types of severe occlusions.</p><formula xml:id="formula_10">L F inal = 1 2 2 i=1 L i M SE + 1 2 6 i=1 L i ID + 1 2 4 i=1 L i C .<label>(</label></formula><p>Partial-REID [10] is a specially designed ReID dataset that consists of occluded, partial, and holistic pedestrian images. It involves 600 images of 60 persons. We take the occluded query set and holistic galley set for experiments.</p><p>Market-1501 <ref type="bibr" target="#b11">[12]</ref> is a famous holistic person ReID dataset. It contains 12,936 training images of 751 persons, 19,732 query images and 3,368 gallery images of 750 persons captured from 6 cameras. Few images in this dataset are occluded.</p><p>DukeMTMC-reID <ref type="bibr" target="#b12">[13]</ref> consists of 16,522 training images of 702 persons, 2,228 queries of 702 persons, and 17,661 gallery images of 702 persons. The images are captured by 8 different cameras, making it more challenging. As it contains more holistic images than occluded ones, this dataset can be treated as a holistic ReID dataset.</p><p>Evaluation Protocol. To guarantee a fair comparison with existing person ReID methods, all methods are evaluated under the Cumulative Matching Characteristic (CMC) and mean Average Precision (mAP). All experiments are performed in the single query setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation Details</head><p>Unless otherwise specified, all images are resized to 256 ? 128. We train our network in an end-to-end fashion through the SGD optimizer with a momentum of 0.9 and weight decay of 1e-4. We initialize the learning rate </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Comparison with State-of-the-art Methods</head><p>Comparisons on Occluded Datasets. The results on Occluded-DukeMTMC (O-Duke), Occluded-REID (O-REID), and Partial-REID (P-REID) are shown in <ref type="table">Table 1</ref>.</p><p>Since O-REID and P-REID don't have corresponding train-ing set, we simply adopt the model trained on Market-1501 for testing. PAT <ref type="bibr" target="#b35">[36]</ref> makes great improvement on accuracy. They adopt ResNet50 <ref type="bibr" target="#b36">[37]</ref> as the backbone and conduct diverse part discovery through the transformer encoderdecoder structure. The prototypes in the network are like specific feature detectors, which are important to improve the network performance on occluded data. TransReID <ref type="bibr" target="#b2">[3]</ref> is the first pure transform-based architecture for person ReID. For a fair comparison, we present the results of TransReID that adopts the Vision Transformer <ref type="bibr" target="#b37">[38]</ref> without the sliding window setting as the backbone and images resized to 256 ? 128. Since He et al. <ref type="bibr" target="#b2">[3]</ref> do not provide performance on O-REID and P-REID datasets, we retrain their official code on Market-1501 dataset and test on the two occluded datasets. The ViT Baseline performs better than TransReID on O-REID and P-REID datasets, this is because TransReID employs many dataset-specific tokens, which reduces the model's cross-domain generalization and increases the overfitting risk.</p><p>When comparing our FED (augmented by OS 1 ) with state-of-the-art methods, we achieve the highest Rank-1 and mAP on both O-Duke and O-REID datasets. Especially on O-REID dataset, we achieve 86.3%/79.3% on Rank-1/mAP, surpassing others by at least 4.7%/2.6%. On O-Duke, we achieve 68.1%/56.4% on Rank-1/mAP, surpassing others by at least 3.6%/0.7%. On the P-REID dataset, we achieve the highest mAP accuracy, reaching 80.5% and surpassing other methods by 3.9%. We fail to achieve the highest Rank-1 accuracy on this dataset due to the low generalization of ViT backbone trained on a small dataset. Meanwhile, to further demonstrate the flexibility and scalability of the FED, we add more diversified patches (combining OS 1 and OS 2 ) for NPO augmentation. As we can see from the table, FED* improves Rank-1/mAP on O-REID and P-REID by at least 0.7% by simply improving the diversity of the occlusion set. In conclusion, we achieve great performance on the occluded ReID datasets.</p><p>Comparisons on Holistic Datasets. We also experiment on holistic person ReID datasets, including Market-1501 and DukeMTMC-reID. While training on the DukeMTMC-reID dataset, MSE Loss is not calculated. It is because huge amounts NPO exist in the training set and we are unable to get precise occlusion masks. The results are shown in <ref type="table">Table.</ref>2. We achieve comparable performance compared with other state-of-the-art methods. The same as Section 4.3.1, the TransReID is without the sliding window setting and with 256 ? 128 image size. It is clear that TransReID gets better performance than our method on the holistic datasets. This is because TransReID is specifically designed for holistic ReID and encodes camera information during the training process. Besides, our proposed three components, which aim at tackling the occlusion issues, are not fully functional on holistic ReID datasets. However, we also </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Ablation Studies</head><p>Analysis of Each Component. In <ref type="table">Table.</ref>3, we present the ablation studies of random erasing (RE), NPO augmentation strategy (NPO Aug), occlusion erasing module (OEM), and feature diffusion module (FDM). The indexes from 0 to 5 represent baseline, baseline + RE, baseline + NPO Aug, baseline + NPO Aug + OEM, baseline + NPO Aug + FDM and FED, respectively. All the models adopt ViT as the feature extractor. M odel 0 and M odel 1 are both optimized by ID Loss and Triplet Loss <ref type="bibr" target="#b14">[15]</ref>. By comparing M odel 0?2 , we can see that RE <ref type="bibr" target="#b1">[2]</ref> is effective in improving discrimination of representations, however the improvement is not comparable with our NPO Aug (4.9% higher on Rank 1). We can conclude that the augmented images through NPO Aug are realistic and valuable. By comparing M odel 2 with M odel 3 , the proposed OEM can further improve the representations and improve mAP by 1.9% by removing the potential NPO information. By comparing M odel 2 with M odel 4 , FDM helps the model with 1.7% and 2.4% improvements on Rank-1 and mAP. It means that optimizing the network with diffused features can greatly improve the model's perception ability towards TP. Finally,  FED achieves the highest accuracy, demonstrating that each component can work individually and cooperatively. Analysis of the K in Memory Searching. Here, we analyze the searching number K in the memory searching operation. In <ref type="table">Table.</ref>4, we set K as 2, 4, 6, and 8 and conduct experiments on both holistic and occluded datasets. As we can see, the performance on holistic ReID datasets appears stably on the various K, with a float within 0.5%. For the Market-1501, there are few NPO and NTP, failing to highlight the effectiveness of FDM. For the DukeMTMC-reID, huge amounts of training data are with NPO and NTP, and loss constraints can enable the network with high accuracy. As for the Occluded-DukeMTMC, since all the training data are holistic pedestrians, the introduction of FDM can greatly simulate the multi-pedestrian conditions in the testing set. With increasing K, FDM can better maintain the characteristics of TP and introduce realistic noise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Qualitative Analysis</head><p>In this section, we present qualitative experimental results and demonstrate the superiority of our proposed FED.</p><p>In <ref type="figure">Fig.4</ref>, we present the occlusion scores from OEM for some pedestrian images. Images with NPO and NTP are presented. As can be seen, vertical object occlusions ( <ref type="figure">Fig.4  a, b)</ref> can hardly affect the occlusion scores, since occluding less than half of symmetric pedestrians is not a critical issue for person ReID. For horizontal occlusions ( <ref type="figure">Fig.4 c,  d)</ref>, our OEM can precisely identify NPO and label them with smaller values. For multi-pedestrian images ( <ref type="figure">Fig.4</ref> e,f), OEM identifies each stripe as valuable. Taken together, the subsequent FDM is essential for improving the model.</p><p>In <ref type="figure" target="#fig_4">Fig.5</ref>, we present the retrieval results of TransReID and our FED. The first two examples are object occluded images. It is obvious that our network has a better recognition ability on NPO and accordingly can retrieve target pedestrians precisely. Another two examples provided are the multi-pedestrian images. Our proposed FED has a stronger perception ability on TP and achieves a much higher retrieval accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, to tackle the NPO and NTP challenges for occluded person ReID, we propose a novel Feature Erasing and Diffusion network (FED). Specifically, guided by the image-level NPO augmentation strategy, the occlusion erasing module (OEM) is trained to eliminate NPO features based on the predicted occlusion scores. Subsequently, the feature diffusion module (FDM) performs feature diffusion between NPO-feature-erased pedestrian representations and memorized features, synthesizing NTP characteristics in the feature space. Jointly optimizing OEM and FDM in our proposed FED network significantly improves the model's perception ability on TP, which is demonstrated through comprehensive experiments and comparisons with state-ofthe-art algorithms on various person ReID benchmarks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Illustration of pose estimation and human parsing on pedestrian images. Both models perform well on holistic and object occluded pedestrians but fail on multi-pedestrian images. Meanwhile, human parsing models have difficulty in identifying personal belongings, e.g., backpacks, and umbrellas. ent viewpoints, lighting conditions, and locations. With the rapid development of deep learning area and publication of large-scale image and video ReID datasets, ReID methods based on deep neural networks have achieved remarkable performance</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>. Gao et al. propose a Pose-guided Visible Part Matching (PVPM) method that jointly learns the discriminative features with pose-guided attention and selfmines the part visibility in an end-to-end framework. He et al. [18] introduce a novel method named Pose-Guided Feature Alignment (PGFA), exploiting pose landmarks to disentangle the useful information from the occlusion noise. Huang et al. propose a model named HPNet to extract partlevel features and predict the visibility of each part, based on human parsing. By extracting features from semantic Overview of the proposed feature erasing and diffusion network for occluded person re-Identification. The two branches share the same parameters and the network consists of the feature extractor, occlusion erasing module (OEM), and feature diffusion module (FDM). The 'NPO Aug' indicates the NPO augmentation strategy. The solid lines connected to the Memory Banks indicate that the features participate in the memory update and loss calculation. The dashed lines indicate only loss calculation. The FDM is an auxiliary module for simulating NTP on feature level and will not be engaged in the inference phase.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Memory Bank ? !"#?%?('?()</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>Illustration of the feature diffusion module. The module diffuses characteristics of memory bank M to the features f ? for simulating NTP on feature level.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>TransReIDFEDFigure 5 .</head><label>5</label><figDesc>Retrieval results of TransReID and our proposed FED on Occluded-DukeMTMC dataset. The top 2 rows show images with NPO and the bottom 2 rows show images with NTP.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Performance analysis of each component in FED. achieve 84.9% Rank-1 accuracy on DukeMTMC-reID, surpassing other CNN-based methods and close to TransReID.</figDesc><table><row><cell>(a)</cell><cell>(b)</cell><cell>(c)</cell></row><row><cell></cell><cell>[0.9823]</cell><cell></cell><cell>[0.9833]</cell></row><row><cell></cell><cell>[0.9814]</cell><cell></cell><cell>[0.9856]</cell></row><row><cell></cell><cell>[0.9798]</cell><cell></cell><cell>[0.9863]</cell></row><row><cell></cell><cell>[0.9872]</cell><cell></cell><cell>[0.9732]</cell></row><row><cell>(d)</cell><cell>(e)</cell><cell>(f)</cell></row><row><cell cols="4">Figure 4. Occlusion scores of OEM on horizontal occluded, ver-</cell></row><row><cell cols="4">tical occluded and multi-pedestrian images. The OEM has the ca-</cell></row><row><cell cols="3">pacity to identify crucial NPO and fails on NTP.</cell></row><row><cell></cell><cell>Occluded-DukeMTMC</cell><cell></cell></row><row><cell cols="4">Index RE NPO Aug OEM FDM R@1 mAP</cell></row><row><cell>0</cell><cell></cell><cell>59.1</cell><cell>49.1</cell></row><row><cell>1</cell><cell></cell><cell>60.3</cell><cell>53.1</cell></row><row><cell>2</cell><cell></cell><cell>65.4</cell><cell>53.5</cell></row><row><cell>3</cell><cell></cell><cell>66.5</cell><cell>55.4</cell></row><row><cell>4</cell><cell></cell><cell>67.1</cell><cell>55.9</cell></row><row><cell>5</cell><cell></cell><cell>68.1</cell><cell>56.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Analysis of the K in memory searching on both occluded and holistic datasets. CMC curve and mAP are presented for evaluation.</figDesc><table><row><cell></cell><cell cols="3">Occlude-DukeMTMC</cell><cell cols="3">DukeMTMC-reID</cell><cell></cell><cell>Market-1501</cell><cell></cell></row><row><cell cols="10">Model Rank-1 Rank-5 mAP Rank-1 Rank-5 mAP Rank-1 Rank-5 mAP</cell></row><row><cell>K=2</cell><cell>67.4</cell><cell>78.4</cell><cell>55.8</cell><cell>89.4</cell><cell>94.7</cell><cell>77.6</cell><cell>95.0</cell><cell>98.6</cell><cell>85.7</cell></row><row><cell>K=4</cell><cell>67.7</cell><cell>79.9</cell><cell>56.2</cell><cell>89.2</cell><cell>94.3</cell><cell>78.0</cell><cell>94.8</cell><cell>98.5</cell><cell>86.3</cell></row><row><cell>K=6</cell><cell>67.3</cell><cell>79.8</cell><cell>56.2</cell><cell>88.9</cell><cell>94.2</cell><cell>77.3</cell><cell>94.8</cell><cell>98.4</cell><cell>86.0</cell></row><row><cell>K=8</cell><cell>68.1</cell><cell>79.3</cell><cell>56.4</cell><cell>89.0</cell><cell>94.3</cell><cell>77.1</cell><cell>94.8</cell><cell>98.4</cell><cell>85.9</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Occlude them all: Occlusion-aware attention network for occluded person re-id</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">842</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Random erasing data augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<idno>pp. 13 001-13 008. 3</idno>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Transreid: Transformer-based object re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Jiang</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Self-paced contrastive learning with hybrid memory for domain adaptive object re-id</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deep highresolution representation learning for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5693" to="5703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2881" to="2890" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">On layer normalization in the transformer architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML. PMLR, 2020</title>
		<imprint>
			<biblScope unit="page" from="10" to="524" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Pose-guided feature alignment for occluded person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Partial person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4678" to="4686" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep spatial feature reconstruction for partial person re-identification: Alignmentfree approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7073" to="7082" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Scalable person re-identification: A benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Unlabeled samples generated by gan improve the person re-identification baseline in vitro</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Occluded person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICME</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">In defense of the triplet loss for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.07737</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Bag of tricks and a strong baseline for deep person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Jiang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPRW</publisher>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Foreground-aware pyramid reconstruction for alignmentfree occluded person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Guided saliency feature learning for person re-identification in crowded scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="357" to="373" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Salient color names for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="536" to="551" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Covariance descriptor based on bio-inspired features for person re-identification and face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Jurie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">6-7</biblScope>
			<biblScope unit="page" from="379" to="390" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Beyond triplet loss: a deep quadruplet network for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="403" to="412" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Re-ranking person re-identification with k-reciprocal encoding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1318" to="1327" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Robust person reidentification through contextual mutual boosting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.07491</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Beyond part models: Person retrieval with refined part pooling (and a strong convolutional baseline)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ECCV</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Attentionaware compositional network for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in CVPR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="2119" to="2128" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Alignedreid: Surpassing human-level performance in person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.08184</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Harmonious attention network for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in CVPR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="2285" to="2294" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Aanet: Attribute attention network for person re-identifications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-P</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-H</forename><surname>Yap</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7134" to="7143" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Attention driven person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="143" to="155" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Human parsing based alignment with multi-task learning for occluded person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICME</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Layer normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Fd-gan: Pose-guided feature distilling gan for robust person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in NIPS</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1229" to="1240" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Recognizing partial biometric patterns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.07399</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Pose-guided visible part matching for occluded person reid</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR, 2020</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">High-order information matters: Learning relation and topology for occluded person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CVPR</publisher>
			<biblScope unit="page" from="6449" to="6458" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Diverse part discovery: Occluded person re-identification with partaware transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Robust video-based person re-identification by hierarchical mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TCSVT</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Pose transferrable person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in CVPR</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="4099" to="4108" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Neighbourhood-guided feature reconstruction for occluded person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.07345</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Person transfer gan to bridge domain gap for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in CVPR</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="79" to="88" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Multi-scale spatialtemporal network for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="2052" to="2056" />
			<date type="published" when="2019" />
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
