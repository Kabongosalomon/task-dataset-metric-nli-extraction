<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Robust Face Alignment by Multi-order High-precision Hourglass Network</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="20151">AUGUST 2015 1</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Journal Of L A T E X Class</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Files</surname></persName>
						</author>
						<title level="a" type="main">Robust Face Alignment by Multi-order High-precision Hourglass Network</title>
					</analytic>
					<monogr>
						<imprint>
							<biblScope unit="volume">14</biblScope>
							<biblScope unit="issue">8</biblScope>
							<date type="published" when="20151">AUGUST 2015 1</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T08:53+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Heatmap regression</term>
					<term>face alignment</term>
					<term>geometirc constraints</term>
					<term>heavy occlusions</term>
					<term>large poses</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Heatmap regression (HR) has become one of the mainstream approaches for face alignment and has obtained promising results under constrained environments. However, when a face image suffers from large pose variations, heavy occlusions and complicated illuminations, the performances of HR methods degrade greatly due to the low resolutions of the generated landmark heatmaps and the exclusion of important high-order information that can be used to learn more discriminative features. To address the alignment problem for faces with extremely large poses and heavy occlusions, this paper proposes a heatmap subpixel regression (HSR) method and a multi-order cross geometry-aware (MCG) model, which are seamlessly integrated into a novel multi-order high-precision hourglass network (MHHN). The HSR method is proposed to achieve high-precision landmark detection by a well-designed subpixel detection loss (SDL) and subpixel detection technology (SDT). At the same time, the MCG model is able to use the proposed multi-order cross information to learn more discriminative representations for enhancing facial geometric constraints and context information. To the best of our knowledge, this is the first study to explore heatmap subpixel regression for robust and high-precision face alignment. The experimental results from challenging benchmark datasets demonstrate that our approach outperforms state-of-theart methods in the literature.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>F ACE alignment, also known as facial landmark detection, refers to locating predefined landmarks (eye corners, nose tip, mouth corners, etc.) of a face. As a typical issue in computer vision, face alignment provides rich geometric information for other face analysis tasks, including facial recognition, face frontalization, human-computer interaction, 3D face reconstruction, etc.</p><p>In recent decades, scholars have made great progress in   <ref type="figure">1. (a)</ref>. The state-of-the-art method HGs <ref type="bibr" target="#b0">[1]</ref>; (b). HGs+MCG; (c). HGs+MCG+HSR (MHHN). NME denotes the normalized mean error. The predicted landmarks of HGs, HGs+MCG and HGs+MCG+HSR are denoted by cyan, blue and green colors, respectively, and the ground-truth landmarks are denoted by red color. HGs fails to accurately predict landmarks under heavy occlusions (NME: 10.12) or large poses (NME: 9.82), while the proposed MCG and HSR acheives much better results, i.e., lower NME.</p><p>face alignment under constrained environments, even by using traditional model-based face alignment algorithms, such as active shape model (ASM) <ref type="bibr" target="#b1">[2]</ref>, active appearance model (AAM) <ref type="bibr" target="#b2">[3]</ref>, constrained local model (CLM) <ref type="bibr" target="#b3">[4]</ref> and Gauss-Newton deformable part model (GN-DPM) <ref type="bibr" target="#b4">[5]</ref>. However, the performances of these methods degrade greatly when facing enormous challenges, such as differences in face shapes and facial appearances, i.e., facial expressions, head poses, partial occlusions and illuminations. To enhance the robustness of face alignment in the wild <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, coordinate regression methods have been widely used, in which the mappings from facial appearance features to shape increments are learned by using different kinds of regressors <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>. Owing to their discriminative features and favorable regression abilities, all these methods are more robust to variations in facial poses and occlusions. However, the coordinate regression method usually regresses to landmark coordinates with a fully connected output layer, which ignores the spatial correlations of features and thus suffers from large poses and partial occlusions. More recently, heatmap regression (HR) methods, such as hourglass networks (HGs <ref type="bibr" target="#b0">[1]</ref>), style aggregated network (SAN) <ref type="bibr" target="#b10">[11]</ref> and others <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>, have been proposed as more powerful alternative methods in a wide range of computer vision and pattern recognition tasks, including face alignment. HR methods can effectively drive the model to focus on parts of interest and better encode part constraints and context so that their robustness to large poses and partial occlusions can be enhanced. arXiv:2010.08722v1 [cs.CV] 17 Oct 2020 However, as shown in <ref type="figure">Fig. 1</ref>, for faces with severe occlusions or extremely large poses, these algorithms fail to accurately predict landmarks because 1) heatmap regression methods are limited by the resolutions of the generated landmark heatmaps and 2) occlusions or large poses may mislead convolutional neural networks (CNNs) in feature representation learning and shape/geometric constraint learning.</p><p>To address these general problems, in this paper, we propose a novel multi-order high-precision hourglass network (MHHN) (see <ref type="figure">Fig. 2</ref> for its structure) to achieve heatmap subpixel face alignment by learning more powerful feature representations and feature correlations (i.e., the geometric constraints). Specifically, a subpixel detection loss (SDL) and subpixel detection technology (SDT) are designed to achieve high-precision face alignment via a heatmap subpixel regression (HSR) method. The HSR method can not only help achieve heatmap subpixel-level facial landmark detection but can also make heatmap pixel-level corrections to the detected landmarks. Moreover, we propose a multi-order cross geometry-aware (MCG) model to enhance the feature representation and the geometric constraints by introducing multi-order cross information. The MCG model can also be updated and propagated in stacked hourglass networks, which helps generate more effective landmark heatmaps. Therefore, our method obtains better robustness and accuracy for face alignment under extremely large poses and heavy occlusions.</p><p>The main contributions of this work are summarized as follows:</p><p>1) By incorporating the SDL and SDT, we propose an HSR method that can achieve heatmap subpixel landmark detection. Moreover, the HSR method can be viewed as a generalized framework that can be applied to any heatmap-regressionlike tasks such as foreground-background segmentation <ref type="bibr" target="#b13">[14]</ref>, object segmentation <ref type="bibr" target="#b14">[15]</ref>, human pose estimation <ref type="bibr" target="#b11">[12]</ref>, etc.</p><p>2) With the well-designed multi-order cross information, a novel MCG model is proposed to enhance the feature representations and geometric constraints for face alignment with extremely large poses and heavy occlusions.</p><p>3) A novel algorithm called MHHN is developed to seamlessly integrate the proposed MCG model and HSR method into a multi-order high-precision hourglass network to handle face alignment under challenging scenarios. To the best of our knowledge, this is the first study to explore heatmap subpixel regression for robust and high-precision face alignment. With the proposed MHHN, our algorithm outperforms state-of-theart methods on benchmark datasets such as COFW <ref type="bibr" target="#b15">[16]</ref>, 300W <ref type="bibr" target="#b16">[17]</ref>, AFLW <ref type="bibr" target="#b17">[18]</ref> and WFLW <ref type="bibr" target="#b9">[10]</ref>.</p><p>The rest of the paper is organized as follows. In Section II, we review related works on face alignment. In Section III, we show the proposed method, including the HSR method and MCG model. In Section IV, we conduct a series of experiments to evaluate our proposed method. Finally, we conclude the paper in Section V.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>Research on face alignment can be traced back to the 1990s, and since then, rapid development has transpired. In general, existing methods can be categorized into three groups: modelbased methods, coordinate regression methods and heatmap regression methods.</p><p>Model-based methods. The performances of model-based methods <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref> depend on the design of the loss function. AAM <ref type="bibr" target="#b2">[3]</ref> addresses face alignment by simultaneously matching shape and texture information, which leads to a rapid and accurate algorithm. To reduce the influences of the variations in facial poses and partial occlusions, CLM <ref type="bibr" target="#b3">[4]</ref> first constructs a shape model and a patch model, then searches and matches the predicted landmark around each landmark in the initial shape. GN-DPM <ref type="bibr" target="#b4">[5]</ref> improves performance and reduces the computational cost of the algorithm by jointly optimizing a global shape model and a part-based appearance model with an efficient and robust Gauss-Newton optimization. However, these methods are still sensitive to large poses and partial occlusions.</p><p>Coordinate Regression methods. This category of methods directly learns the mapping from facial appearance features to the landmark coordinate vectors with different kinds of regressors <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b20">[21]</ref>. In explicit shape regression (ESR) <ref type="bibr" target="#b7">[8]</ref>, the fern is used to learn a regressor in a cascaded way by minimizing the alignment errors over the training data, and a correlation-based feature selection method is proposed to ensure its accuracy and efficiency. In local binary features (LBF) <ref type="bibr" target="#b18">[19]</ref>, the random forest is used to learn the local binary features and landmark regressor at the same time, and it can achieve 3000FPS in testing. With these effective linear models, robustness and accuracy of face alignment are enhanced. Other methods <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b20">[21]</ref> use deep models to predict landmark coordinates. In tasks-constrained deep convolutional network (TCDCN) <ref type="bibr" target="#b19">[20]</ref>, CNNs are used to construct a shared representation by jointly learning face alignment with subtly correlated tasks (such as appearance attributes, expressions, demographics and head poses), and a task-wise early stopping scheme is proposed to ensure its convergence. In cascaded regression and deocclusion (CRD) <ref type="bibr" target="#b21">[22]</ref>, the generative adversarial networks (GANs) are used to locate the facial occlusions and recover the occluded regions. Then the recovered faces can be utilized to improve the robustness of face alignment under occlusions. In look-at-boundary (LAB) <ref type="bibr" target="#b9">[10]</ref>, the stacked hourglass network and the GANs are combined to generate facial boundary heatmaps that can effectively help enhance the shape constraints and improve alignment accuracy. In occlusion-adaptive deep network (ODN) <ref type="bibr" target="#b20">[21]</ref>, the Resnet is utilized to construct the geometry-aware module, the distillation module and the low-rank learning module to overcome the occlusion problem in face alignment. Due to their more powerful non-linearity, the performances of coordinate regression methods can be further enhanced. However, coordinate regression methods usually regress landmark coordinates with a fully connected output layer, which ignores the spatial correlations of features. Hence, these methods do not perform as well as heatmap regression models.</p><p>Heatmap regression methods. HR methods <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b12">[13]</ref> directly predict landmark coordinates from the generated landmark heatmaps. Compared to coordinate regression methods, <ref type="figure">Fig. 2</ref>. The proposed multi-order high-precision hourglass network (MHHN). The MCG model is proposed to explore more discriminative representations for enhancing geometric constraints and context information by introducing multi-order cross information, and the HSR method can help to achieve heatmap subpixel landmark detection by combining subpixel detection loss and subpixel detection technology. Then, by integrating the MCG model and HSR method via a seamless formulation, our MHHN is able to achieve more robust FLD.</p><p>HR methods can better encode the part constraints and context information and effectively drive the network to focus on the important parts in facial landmark detection, thus achieving state-of-the-art accuracy. Yang et al. <ref type="bibr" target="#b0">[1]</ref> use a supervised transformation to normalize faces and then a stacked hourglass network to predict landmark heatmaps. By paying more attention to features with high confidence in an explicit manner, the robustness of this method is enhanced. By transforming the original face image into style-aggregated images, style aggregated network (SAN) <ref type="bibr" target="#b10">[11]</ref> is able to address the face alignment problems caused by variations in image styles. Liu et al. <ref type="bibr" target="#b12">[13]</ref> propose finding the semantically consistent annotations by a novel latent variable optimization strategy; then, the ground-truth shape can be updated, and the predicted shape becomes more accurate.</p><p>Until now, almost all HR methods <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b20">[21]</ref> have been limited to the low resolutions of the generated landmark heatmaps, reducing the accuracy of face alignment. Moreover, HR methods often ignore feature inter-dependencies and important higher-order information <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref>; thus, they cannot fully explore the discriminative abilities of the features in neural networks. Therefore, we propose a multi-order high-precision hourglass network by exploiting the heatmap subpixel regression method and multi-order cross information for robust face alignment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. MULTI-ORDER HIGH-PRECISION HOURGLASS NETWORK</head><p>In this section, we first elaborate on the proposed HSR method and then present the MCG model. Specifically, the SDL and SDT are proposed to generate more effective landmark heatmaps and achieve high-precision face alignment. Moreover, multi-order cross information is introduced by the MCG model to explore more discriminative features for enhancing facial geometric constraints and context information.</p><p>Finally, by fusing the MCG model and the HSR method with a seamless formulation via a multi-order high-precision hourglass network, our MHHN can achieve heatmap subpixel accuracy and is more robust to faces with extremely large poses and heavy occlusions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Heatmap Subpixel Regression</head><p>Most HR methods <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b10">[11]</ref> utilize the classical (mean squared error) MSE loss to generate landmark heatmaps and then estimate the landmarks by traversing the corresponding landmark heatmaps, which causes the following problems: 1) optimizing the MSE loss to generate landmark heatmaps often makes the generated landmark heatmaps blurry and implausible, reducing the accuracy of face alignment, and 2) traversing the generated landmark heatmaps to predict landmarks limits the accuracy of predicted landmarks to the generated low-resolution heatmaps (see <ref type="figure" target="#fig_1">Fig. 3</ref>). Transforming the estimated landmarks into the original image size to obtain the final landmark coordinates further increases the error. Hence, in this paper, we propose a heatmap subpixel regression method to handle the above problems. Our HSR method is able to achieve heatmap subpixel accuracy by integrating the SDL and SDT. The proposed SDL mainly contains two parts: the Jensen-Shannon divergence loss and the fine detection loss. The training process of the neural network is as follows. First, by initializing the neural network, the landmark heatmap can be generated. Then, we calculate the Jensen-Shannon divergence loss between the generated and ground-truth landmark heatmaps and predict the heatmap pixel landmark coordinates by traversing the generated heatmaps (corresponding to coarse locating). Next, the heatmap subpixel landmark coordinates can be further estimated by introducing the SDT (corresponding to fine locating), and the fine detection loss can be calculated and regarded as effective feedback for designing the SDL. Finally, by optimizing the SDL, the  <ref type="bibr" target="#b23">(24,</ref><ref type="bibr" target="#b23">24)</ref>. However, the practical landmark in (e) is close to <ref type="bibr" target="#b23">(24,</ref><ref type="bibr" target="#b23">24)</ref> that cannot be detected precisely by the HR methods. Hence, the HR methods are limited to the generated low resolution landmark heatmaps. The practical landmark in (e) can be represented as (u, v) which can be accurately estimated by the proposed HSR method.</p><p>parameters of the neural networks can be updated. Due to this coarse-to-fine structure, the HSR method can achieve heatmap subpixel landmark detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) Jensen-Shannon Divergence Loss:</head><p>The ground-truth landmark heatmap for face alignment is generated by a twodimensional Gaussian distribution, which can be illustrated as:</p><formula xml:id="formula_0">H * (x, y) = exp ? (x ? u * ) 2 + (y ? v * ) 2 2? 2 *<label>(1)</label></formula><p>where (u * , v * ) ? S * , S * denotes the ground-truth face shape and (u * , v * ) represents the coordinates of a landmark in S * . H * is the ground-truth landmark heatmap, while (x, y) is used to denote a pixel's location in heatmap H * . In most existing HR methods <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b10">[11]</ref>, the MSE loss is utilized to fit neural networks and generate landmark heatmaps. However, the MSE loss treats every location on the heatmap equally, and it is difficult to generate effective landmark heatmaps for high precision facial landmark detection. Here, our proposed HSR method is able to generate more accurate and effective landmark heatmaps because 1) the Jensen-Shannon divergence loss can pay more attention to the foreground area of heatmaps rather than treat the whole heatmap equally; 2) the Jensen-Shannon divergence loss can accurately measure the difference between two distributions and moreover, by optimizing the Jensen-Shannon divergence loss, the generated landmark heatmaps can be further utilized to estimate the heatmap subpixel coordinates. The Jensen-Shannon divergence loss can be stated as:</p><formula xml:id="formula_1">min JS ( p H * p H ) = 1 2 KL p H (x, y) p H * (x,y)+p H (x,y) 2 + 1 2 KL p H * (x, y) p H * (x,y)+p H (x,y) 2<label>(2)</label></formula><p>where p H * and p H denote the distributions of the ground-truth heatmap and the generated heatmap, respectively. p H (x, y) represents a pixel value on the heatmap H corresponding to a location (x, y) and works as the confidence of one particular landmark at that pixel. Both p H * and p H are based on discrete probability distributions, and we can calculate the Jensen-Shannon divergence loss by a sum operation. By minimizing the Jensen-Shannon divergence loss, we can generate heatmaps with the same distributions and convex points as the groundtruth heatmaps.</p><p>2) Subpixel Detection Technology: In existing works <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b20">[21]</ref>, by traversing the generated landmark heatmap, the pixel with the maximum value on the heatmap is often regarded as the location of the predicted landmark (the suboptimal landmark). However, these suboptimal landmarks can only achieve heatmap pixel-level accuracy and are not precise enough due to the generated low-resolution landmark heatmaps. Moreover, the transformation of the suboptimal landmarks into the original image size will further increase the errors. Therefore, it is very beneficial to predict the coordinates of the heatmap subpixel landmarks from the generated landmark heatmaps, and we propose the SDT to achieve this. Specifically, SDT first models the continuous distribution of the region centered on the suboptimal landmarks in the heatmaps by using a two-dimensional Gaussian distribution. Then, SDT estimates the parameters of this distribution, and the center of this two-dimensional Gaussian distribution is the predicted heatmap subpixel landmark (the optimal landmark). Because of this continuous distribution, the predicted optimal landmark can achieve heatmap subpixel accuracy, i.e., highprecision facial landmark detection. The modeling of the continuous distribution of the region centered on the suboptimal landmarks in the heatmaps can be formulated as follows:</p><formula xml:id="formula_2">M (x, y) = G exp ? (x ? u) 2 2? 2 x ? (y ? v) 2 2? 2 y<label>(3)</label></formula><p>where ? x and ? y denote the standard deviations in the xand y-directions, respectively. G represents the actual maximum confidence in the foreground of the heatmap (not the maximum value obtained by traversing the heatmap). M denotes the confidence matrix of one particular landmark. (G, ? x , ? y , u, v) are the parameters of the two-dimensional Gaussian distribution. The solution for (G, ? x , ? y , u, v) can be obtained by minimizing the following objective function:</p><formula xml:id="formula_3">min G,?x,?y,u,v E (x, y) = x,y?H M (x, y) ? H (x, y) 2 2<label>(4)</label></formula><p>where H denotes the generated landmark heatmap. Optimization of Eq. (4) can be performed using the least square method or gradient descent method. By minimizing Eq. (4), we can obtain the optimal landmark (u, v).</p><p>3) Subpixel Detection Loss: By optimizing the JSDL, landmark heatmaps can be generated, and suboptimal landmarks can be estimated. Then, with the proposed SDT, we can obtain the optimal landmarks and heatmap subpixel face alignment can be achieved. However, optimizing the JSDL or MSE loss leads to less accurate landmark heatmaps, e.g. the maximum value of the generated landmark heatmap is much smaller <ref type="figure">Fig. 4</ref>. The network structure of the proposed multi-order cross geometry-aware (MCG) model. With the proposed MCG model, the multi-order cross information containing both cross-layer information (i.e., the autocorrelation of intra-layer features and the cross-correlation of inter-layer features) and crossorder information (the first-order, second-order and third-order information) can be utilized to explore more discriminative representations for enhancing geometric constraints and further generating more effective heatmaps. than 1 or the flat hat problem <ref type="bibr" target="#b12">[13]</ref> arises, which reduces the accuracy of face alignment. Hence, a subpixel detection loss is proposed to address the above problems by combining the Jensen-Shannon divergence loss and fine detection loss. The fine detection loss is defined as follows:</p><formula xml:id="formula_4">F DL = k (u, v) k ? (u * , v * ) k 2 2 (5)</formula><p>where (u * , v * ) k denotes the coordinates of the kth landmark in the ground-truth face shape S * . (u, v) k represents the kth predicted landmark coordinates by using the SDT. To ensure that the maximum value of the generated landmark heatmap is equal to 1 and solve the flat hat problem <ref type="bibr" target="#b12">[13]</ref>, we set G = 1 and ? x = ? y = ? * . With these settings, the Jensen-Shannon divergence loss and fine detection loss are integrated into the final SDL, which can drive the network to generate more accurate and effective landmark heatmaps and improve the accuracy of face alignment. Moreover, in <ref type="table">Table V</ref>, we also show the corresponding experimental results from the 300W challenging subset without those settings, i.e., we do not specify the values of G, ? x and ? y . The SDL is formulated as follows: min</p><formula xml:id="formula_5">W,G,?x,?y,u,v SDL (W, G, ? x , ? y , u, v) = j k JS p j H * k ||p j H k +? j k (u, v) j k ? (u * , v * ) j k 2 2<label>(6)</label></formula><p>where j denotes the image index in the training set, k represents the landmark index in the face shape and ? is the weight coefficient. W represents the parameters of the deep neural networks. With the additional fine detection loss, the SDL can be used to generate more accurate and effective landmark heatmaps. Moreover, the SDT can not only be used to achieve heatmap subpixel-level facial landmark detection but can also make heatmap pixel-level corrections to the detected landmarks. Hence, by integrating the SDL and SDT, heatmap subpixel face alignment can be achieved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Multi-order Cross Geometry-aware Model</head><p>Besides proposing the heatmap subpixel regression method to help achieve heatmap subpixel face alignment, we also explore more effective and discriminative representations for enhancing facial geometric constraints in this paper. HR methods <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b12">[13]</ref> achieve state-of-the-art accuracy because they can effectively encode the part constraints and context information. However, these methods suffer from performance degradation for faces with extremely large poses and heavy occlusions, because, in these cases, the extracted features are not robust enough and the facial geometric constraints (e.g. part constraints and global constraints) among landmarks are missing. More recently, second-order information <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref>, cross-layer features <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b29">[30]</ref> and feature pyramids <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b31">[31]</ref> have been shown to be useful for obtaining more discriminative and effective representations and are beneficial to many vision tasks. However, how to fuse the higher-order information and cross-layer information to obtain more discriminative representations for robust face alignment is still an open question. Therefore, in this paper, a multi-order cross geometry-aware model is proposed to enhance facial geometric constraints by introducing well-designed multiorder cross information. Multi-order cross information mainly contains cross-order and cross-layer information. Cross-order information can fully explore the useful information contained in features themselves and has a larger receptive field while preserving local details, and cross-layer information can effectively capture landmark geometric constraints at different scales, so multi-order cross information is able to help obtain more discriminative representations for enhancing the geometric constraints, which further helps generate more effective landmark heatmaps and achieve robust face alignment.</p><p>1) Preliminaries: The proposed MCG model aims to model the spatial correlations between features (i.e., geometric constraints) by introducing multi-order cross information. As shown in <ref type="figure">Fig. 4</ref>, the MCG model is a modified hourglass network unit. The input of the hourglass network unit is denoted as P , and the output is denoted as O. Due to their bottom-up and top-down structures, P and O have the same size, i.e., P, O ? R N ?d , where N = w ? h, w and h denote the width and height of the feature map, respectively, and d denotes the channel number of the feature map. We use P i to denote the ith row of P , and P i represents the feature corresponding to the ith location in feature map P . The pairwise feature correlations can be represented as their inner product P i P T j . 2) Construction of the MCG model: Recently, bilinear CNNs <ref type="bibr" target="#b32">[32]</ref> and ODN <ref type="bibr" target="#b20">[21]</ref> have used the matrix outer product of the outputs from two CNN streams to model the intrinsic geometric structures of feature maps. Inspired by this work, we use the outer product of P and P T to represent the autocorrelations of intra-layer features that are able to capture geometric correlations between facial landmark regions. P O T is used to represent the cross-correlation of inter-layer features that can capture landmark geometric correlations at different scales (the deep layer and shallow layer). As the outer product operation is similar to a quadratic kernel expansion and is indeed a non-local operation, it can be used to effectively model local pairwise feature correlations for capturing longrange dependencies. Therefore, P P T , OO T and P O T can effectively preserve long-range geometric constraints, where P O T denotes the cross-layer information. As shown in <ref type="figure">Fig. 4</ref>, the MCG model computes the sum of P O T , OO T and P O T :</p><formula xml:id="formula_6">Z = P P T + OO T + P O T<label>(7)</label></formula><p>where Z ? R N ?N denotes the pairwise feature correlations of all the pixel locations in the feature maps and Z is the second-order information. The second-order information can fully explore the useful information contained in the features themselves and has a larger receptive field while preserving local details, thus enhancing the geometric constraints of facial landmarks under heavy occlusions. According to <ref type="bibr" target="#b33">[33]</ref>, OP T and P O T have similar information, so we discard OP T to reduce the computational cost. Z is followed by a Softmax function. Building on this, we multiply? and O to obtain the third-order information, which can be regarded as high-order cross-layer attention feature maps, i.e., we transform the pairwise feature correlations to the high-order cross-layer attention that can be imposed on the original output feature maps O. The whole process can be illustrated as follows:?</p><formula xml:id="formula_7">= sof tmax (Z) (8) P =??O+O<label>(9)</label></formula><p>where ? is a learnable scalar that is initialized as 0.?O and O are the third-order information and first-order information, respectively. Moreover,? contains the second-order information and cross-layer information. P is then called the multiorder cross information, i.e., it includes both cross-order information (including first-order, second-order and thirdorder information) and cross-layer information. Then, by integrating the multi-order cross information into an hourglass network unit, we construct a multi-order cross geometryaware (MCG) model. Specifically, the multi-order cross information is first introduced into the MCG model, and then, the learnable weight ? with respect to the third-order information can be updated gradually during training. Therefore, with the fusion of high-order information and cross-layer information, multi-order cross information can fully explore the useful information contained in feature maps and possesses stronger representation and discrimination capabilities, so it helps the MCG model to better model the facial geometric constraints for robust face alignment with extremely large poses and heavy occlusions. The proposed MCG model has the following three advantages: 1) with the fusion of cross-order information and crosslayer information, the MCG model can obtain more representative and discriminative multi-order cross information; 2) by integrating multi-order cross information into the hourglass network unit, the MCG model can mine useful information inherent in different convolutional layers and different orders of information and 3) multi-order cross information can be updated and propagated in stacked hourglass networks, which can help obtain more accurate and effective landmark geometric constraints for robust face alignment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Multi-order High-precision Hourglass Network</head><p>The proposed MCG model can effectively enhance facial geometric constraints by introducing multi-order cross information, i.e., cross-order information and cross-layer information. Then, the HSR method is proposed to incorporate the well-designed SDL with the SDT for high-precision heatmap regression. Finally, by integrating the MCG model and the HSR method into a novel MHHN via a seamless formulation, we can generate more accurate and effective landmark heatmaps and achieve high-precision face alignment under extremely large poses and heavy occlusions. The overall network structure of the MHHN is shown in <ref type="figure">Fig. 2</ref>. The objective function of the MHHN can be reformulated as follows:</p><formula xml:id="formula_8">min W,G,?x,?y,u,v SDL (W, G, ? x , ? y , u, v) = j k JS p j H * k ||p j H k + ? j k (u, v) k ? S * k 2 2 = j k JS p j H * k ||M HHN I j , W k +? j k (u, v) k ? (u * , v * ) j k 2 2<label>(10)</label></formula><p>where M HHN denotes the proposed multi-order highprecision hourglass network, and its parameters are denoted by W . The input of the MHHN is a face image, and the outputs are landmark heatmaps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Optimization</head><p>The objective function (i.e., Eq. (10)) of the proposed MHHN contains two terms. The first term corresponds to the Jensen-Shannon divergence loss between the predicted and ground-truth landmark heatmaps. The second term corresponds to the fine detection loss, which can be solved by optimizing the following problem:</p><formula xml:id="formula_9">min G,?x,?y,u,v (x,y)?M M j k (x, y) ? M HHN I j , W k 2 2<label>(11)</label></formula><p>Eq. (11) is a classical two-dimensional Gaussian surface fitting problem. Let</p><formula xml:id="formula_10">f (x, y) = G exp ? (x ? u) 2 2? 2 x ? ? (y ? v) 2 2? 2 y<label>(12)</label></formula><p>f ? ln f can be expressed as follows:</p><formula xml:id="formula_11">f ? ln f = ln G ? u 2 2? 2 x ? v 2 2? 2 y f + u ? 2 x xf + v ? 2 y yf ? u 2? 2 x x 2 f ? v 2? 2 y y 2 f<label>(13)</label></formula><p>Assuming that there are L data points involved in the fitting, the L data points can be expressed in the form of a matrix, i.e., A = BC, where a = f ? ln f , B = [b ] = f , f x , f y , f x 2 , f y 2 and = 1, 2, ..., L. C T = [c 0 , c 1 , c 2 , c 3 , c 4 ] denotes a vector consisting of Gaussian parameters and can be illustrated as follows:</p><formula xml:id="formula_12">C T = ln G ? u 2 2? 2 x ? v 2 2? 2 y , u ? 2 x , v ? 2 y , ? 1 2? 2 x , ? 1 2? 2 y<label>(14)</label></formula><p>Then, the least square method is used to fit the errors of the L data points, the fitted model is formulated as follows:</p><formula xml:id="formula_13">arg min C 1 L E 2 2 = E T E L = (A ? BC) T (A ? BC) L<label>(15)</label></formula><p>With the QR decomposition, B can be reformulated as B = QR. Q is an orthogonal matrix and Q ? R L?L . R is an upper triangular matrix and R ? R L?5 . Based on these, Eq. (15) can be reformulated as follows:</p><formula xml:id="formula_14">arg min C 1 L E 2 2 = 1 L Q T E 2 2 = 1 L Q T A ? RC 2 2 = 1 L S ? R 1 C 2 2 + T 2 2 (16) Q T A = S T , R = R 1 0<label>(17)</label></formula><p>where S denotes a 5-dimensional column vector and T represents a (L ? 5)-dimensional column vector. R 1 is an upper triangular matrix and R 1 ? R 5?5 . Hence, Eq. (16) obtains its minimum value when S = R 1 C, i.e., C = R ?1 1 S. (u, v) can be computed as follows:</p><formula xml:id="formula_15">u = ? c 1 2c 3 , v = ? c 2 2c 4<label>(18)</label></formula><p>For Eq. <ref type="formula" target="#formula_0">(12)</ref>, to ensure that the maximum value of the generated landmark heatmap is equal to 1 and solve the flat hat problem <ref type="bibr" target="#b12">[13]</ref>, we expect to add constraints G = 1 and ? x = ? y = ? * . However, the constraint ? x = ? y = ? * does not always hold, so we firstly only add constraints G = 1 and ? x = ? y , and then calculate (u, v) by further minimizing Eq. (19.1), <ref type="bibr">(19.2)</ref> and <ref type="bibr">(19.3)</ref>, which are shown as follows:</p><formula xml:id="formula_16">? ? ? ? ? c 1 ? ln G + u 2 +v 2 2? 2 x (19.1) c 2 ? u ? 2 x (19.2) c 3 ? v ? 2 x (19.3)<label>(19)</label></formula><p>The final solution of (u, v) can be expressed as follows:</p><formula xml:id="formula_17">u = ? 2 * c 2 , v = ? 2 * c 3 if c 1 ? 0 u = ?2c1 c 2 2 +c 2 3 c 2 , v = ?2c1 c 2 2 +c 2 3 c 3 c 1 &lt; 0<label>(20)</label></formula><p>Eq. (20) means that 1) when c 1 ? 0, G equals to 1 and both ? x and ? y can be set to ? * and 2) when c 1 &lt; 0, we can only assign G = 1 and ? x = ? y = ?2c1</p><formula xml:id="formula_18">c 2 2 +c 2 3</formula><p>. The optimization is a typical network training process under the supervision of the SDL. With the proposed MCG model, we can obtain more representative and discriminative features. By integrating the MCG model and HSR method into the MHHN, the optimal landmark can be easily predicted and heatmap subpixel face alignment is achieved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Datasets</head><p>We evaluate our proposed MHHN method on four challenging datasets: COFW <ref type="bibr" target="#b15">[16]</ref>, 300W <ref type="bibr" target="#b16">[17]</ref>, AFLW <ref type="bibr" target="#b17">[18]</ref> and WFLW <ref type="bibr" target="#b9">[10]</ref>.</p><p>300W (68 landmarks): With a total of 3148 pictures, the training set is made up of the AFW <ref type="bibr" target="#b34">[34]</ref>, LFPW <ref type="bibr" target="#b35">[35]</ref> and Helen <ref type="bibr" target="#b36">[36]</ref> training sets, while the testing set includes 689 images with the IBUG <ref type="bibr" target="#b16">[17]</ref>, LFPW and Helen testing sets.</p><p>COFW <ref type="formula" target="#formula_5">(68 landmarks</ref> WFLW (98 landmarks): It contains 10000 faces (7500 for training and 2500 for testing) with 98 landmarks. Apart from landmark annotation, the WFLW also possesses rich attribute annotations (such as occlusions, poses, make-up, illuminations, blurs and expressions) that can be used for a comprehensive analysis of existing algorithms.</p><p>Evaluation metric: Normalized mean error (NME) is commonly used to evaluate face alignment algorithms. For the 300W, the NME normalized by the inter-pupil distance is used. For the AFLW, we use the NME normalized by the face size given by the AFLW. For the WFLW and COFW, the NME normalized by inter-ocular distance is adopted, which is the same as the evaluation criterion of LAB <ref type="bibr" target="#b9">[10]</ref>. Moreover, we also use the standard deviation of the NME to evaluate these four datasets.</p><p>Implementation Details: In our experiments, all the training and testing images are cropped and resized to 256 ? 256 according to the provided bounding boxes. To perform data augmentation, we randomly sample the angle of rotation and the bounding box scale from a Gaussian distribution. We use four stacked hourglass networks as our backbone to construct the proposed MHHN. During training, we use the staircase function. The initial learning rate is 1?10 ?3 , which is decayed to 1?10 ?5 after 150 epochs. The learning rate is divided by 2, 5, 10 and 10 at epoch 20, 50, 100 and 150, respectively. During the search for the optimal landmark, we predict this landmark from a 9 ? 9 region centered on the suboptimal landmark in </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Com.</p><p>Chal. Full LBF <ref type="bibr" target="#b18">[19]</ref> 4.95 (3.97) 11.98 (7.07) 6.32 (6.07) 3DDFA(CVPR16) <ref type="bibr" target="#b37">[37]</ref> 6.15 (5.58) <ref type="bibr" target="#b9">10</ref>.59 (6.51) 7.01 (7.97) RAR(ECCV16) <ref type="bibr" target="#b38">[38]</ref> 4  <ref type="figure">Fig. 5</ref>. Comparison of CED curves between our method and state-of-the-art methods including LBF <ref type="bibr" target="#b18">[19]</ref>, CFSS <ref type="bibr" target="#b42">[42]</ref>, RAR <ref type="bibr" target="#b38">[38]</ref>, Wing <ref type="bibr" target="#b40">[40]</ref>, LAB <ref type="bibr" target="#b9">[10]</ref> and ODN <ref type="bibr" target="#b20">[21]</ref> on 300W Challenging subset (68 landmarks). Our approach is more robust to partial occlusions and large poses than other methods. the generated heatmap. The MHHN is trained with Pytorch on 8 Nvidia Tesla V100 GPUs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Comparison with state-of-the-art methods</head><p>300W. We compare our approach against state-of-the-art methods on the 300W dataset in <ref type="table" target="#tab_1">Table I</ref>. The baseline (HGs <ref type="bibr" target="#b0">[1]</ref> in <ref type="table" target="#tab_1">Table I</ref>) uses the original hourglass networks and achieves 4.43% NME on the 300W Common subset and 7.56% on the 300W Challenging subset. From <ref type="table" target="#tab_1">Table I</ref> and <ref type="figure">Fig. 5</ref>, we can see that 1) HGs+HSR achieves 3.69% NME on the 300W Common subset and 6.72% NME on the 300W Challenging subset, which are better than HGs alone and indicate that the proposed HSR is helpful for improving the accuracy of face alignment; 2) HGs+MCG achieves 3.54% NME on the 300W Common subset and 6.51% NME on the 300W Challenging subset, which also greatly exceed HGs alone and indicate that the proposed MCG model can effectively utilize the multiorder cross information to enhance the geometric constraints and thus generate more robust and effective heatmaps; and 3) HGs+MCG+HSR (MHHN) outperforms state-of-the-art meth- ods, which indicates that the proposed HSR method and MCG model can be seamlessly integrated into a multi-order highprecision hourglass network, thus achieving robust and highprecision face alignment for challenging scenarios. COFW. Our method is able to outperform state-of-the-art methods on the COFW dataset, as shown in <ref type="table" target="#tab_1">Table II</ref>. The failure rate is defined by the percentage of test images with more than 10% detection error. As shown in <ref type="table" target="#tab_1">Table II</ref>, the accuracy of our proposed HGs+MCG+HSR (MHHN) greatly exceeds that of other methods <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b44">[44]</ref>, <ref type="bibr" target="#b45">[45]</ref>, <ref type="bibr" target="#b46">[46]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b20">[21]</ref>. From the view of another evaluation criterion, the failure rate of our method (1.78%) is better than those of the other methods. Hence, we can conclude that the proposed method is more robust to heavy occlusions than the other methods.</p><p>AFLW. Compared with the 300W dataset (68 landmarks), the AFLW has only 19 landmarks, most of which possess challenging shape variations and significant view changes. <ref type="table" target="#tab_1">Table III</ref> and <ref type="figure">Fig. 6</ref> suggest that our method outperforms stateof-the-art methods on the AFLW dataset, indicating that the proposed algorithm has strong robustness over the variations in facial expressions and head poses.</p><p>WFLW. Compared with other datasets such as the 300W dataset (68 landmarks), AFLW dataset (19 landmarks), WFLW dataset has more landmark annotations, i.e., 98 landmarks. Moreover, images in WFLW dataset are collected from more complicated scenarios. So, experiments on WFLW dataset are more challenging. <ref type="table" target="#tab_1">Table IV</ref> suggests that our method outperforms state-of-the-art methods on the WFLW dataset, which indicates the proposed HGs+MCG+HSR (MHHN) has strong robustness over the variations in challenging scenarios, i.e., the variations in occlusion, pose, make-up, illumination, blur and expression.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Self Evaluations</head><p>Loss function. To generate heatmaps with the same distributions and convex points as the ground-truth heatmaps, we conduct experiments on the 300W challenging subset by separately using the mean squared error (MSE) loss, the Kullback-Leibler divergence loss (KLDL), the Jensen-  <ref type="figure">Fig. 6</ref>. Comparisons of CED curves of our method and state-of-the-art methods like SDM <ref type="bibr" target="#b48">[48]</ref>, CCL <ref type="bibr" target="#b17">[18]</ref>, DAC-CSR <ref type="bibr" target="#b46">[46]</ref>, SAN <ref type="bibr" target="#b10">[11]</ref> and ODN <ref type="bibr" target="#b20">[21]</ref> on AFLW-full dataset <ref type="bibr">(19 landmarks)</ref>. Our approach outperforms the other methods.</p><p>Shannon divergence loss (JSDL) and SDL. MSE involves applying the mean squared error loss to generated landmark heatmaps and then traversing the landmark heatmaps to obtain the corresponding landmarks. MSE+SDT requires first applying the mean squared error loss to generate landmark heatmaps and then using the SDT to predict the optimal landmarks as a post-processing method. SDL represents the subpixel detection loss with constraints G = 1 and ? x = ? y = ? * , and SDL1 denotes the subpixel detection loss without those constraints. From the experimental results in <ref type="table">Table V</ref>, we can conclude that 1) the SDT can be used to improve the accuracy of heatmap regression face alignment as a post-processing method; 2) compared to the MSE loss and KLDL, the JSDL can help generate more effective heatmaps, which can be combined with the SDT to further improve the performance of the face alignment. 3) by integrating the JSDL and fine detection loss into the well-designed SDL via the proposed HSR method, heatmap subpixel face alignment can be achieved; and 4) by constraining G = 1 and ? x = ? y = ? * , the SDL can generate more accurate landmark heatmaps, and the performance of face alignment can be further improved.</p><p>Analyses of ? and ?. ? denotes the standard deviations in the xand y-directions of the distribution of the groundtruth landmark heatmaps, and it also determines the size of the foreground region of the landmark heatmap. In general, the value of ? is affected by the height and width of the landmark heatmap. As our landmark heatmap size is 64?64, we conduct the corresponding experiment by using different ? values on the 300W challenging subset. From the experimental results in <ref type="table" target="#tab_1">Table VI</ref>, we find that ? = 3 is a good choice. ? represents the weight coefficient of fine detection loss in the SDL. If ? is set to 0, then it means the JSDL is used to optimize the proposed MHHN. If ? is set to a large integer, then the MHHN is difficult to converge. <ref type="table" target="#tab_1">Table VII</ref> shows the experimental results of different ? values on the 300W challenging subset, which indicates that the MHHN can achieve good results when ? is set to 1/16.</p><p>Search patch. From the generated heatmaps, we can first predict the suboptimal landmarks. Then, by utilizing the SDT, we can obtain the optimal landmarks. In general, the search patch is a 9 ? 9 rectangular region centered on the suboptimal landmark. The distribution (quality) of the heatmap can be affected by heavy occlusions and large poses. By introducing the SDL, our method can generate more effective heatmaps under complicated scenarios. Moreover, we can further expand the search radius to obtain more accurate landmarks. When we increase the search radius to 15 ? 15, the NME of the MHHN on 300W challenging subset can be reduced from 6.01% to 5.98%, which further indicates that the proposed SDT can help achieve high-precision face alignment.</p><p>Computational cost. Since our method can achieve heatmap subpixel accuracy, reducing the sizes of the input and output should not significantly affect the performance of our method. To verify this, we reduce the input of the MCG model from 64 ? 64 ? 256 to 32 ? 32 ? 256 and the output landmark heatmap size from 64 ? 64 to 32 ? 32. The NME on the 300W challenging dataset only increases from 6.01% to 6.07%, which still outperforms state-of-the-art methods. This finding indicates that the proposed HSR method can be used to reduce computational costs in heatmap-regression-like methods without significantly degrading their performances. The MCG model also increases the computational costs. To further evaluate its effectiveness, we conduct the following experiment. We replace the last hourglass network unit of the four stacked hourglass networks with the MCG model. The NME on the 300W challenging dataset will only increase from 6.01% to 6.13%, which still achieves state-of-the-art accuracy. These two experiments further demonstrate the effectiveness of the MCG model and the HSR method.</p><p>Heatmap quality. Comparisons of the generated heatmaps of the GT (ground-truth), HGs, HGs+MCG+JSDL and our method are shown in <ref type="figure">Fig. 7. Fig. 7 (a)</ref> illustrates some landmark heatmaps under severe occlusions. The first row, second row, third row and fourth row represent the landmark heatmaps of the ground-truth, HGs, HGs+MCG+JSDL and our method, respectively. In each row, the first four columns represent the fusion of landmark heatmaps and face images, and the last four columns represent the corresponding landmark heatmaps. From <ref type="figure">Fig. 7 (a)</ref>, we can see that HGs' heatmaps usually have long tails (the 5th, 6th and 7th heatmaps in the second row) and double centroids (the 8th heatmap in the second row) under severe occlusions, thus resulting in poor performance. <ref type="figure">Fig.  7</ref> (b) illustrates some landmark heatmaps (left outer mouth corner, right outer mouth corner, upper outer lip center and lower outer lip center) under large poses. From <ref type="figure">Fig. 7 (b)</ref>, we can see that all the methods can generate high-quality landmark heatmaps, but our method can obtain more accurate landmarks. As shown in <ref type="figure">Fig. 7</ref>, with the proposed MHHN, our heatmaps are more robust to heavy occlusions and extremely large poses, thus outperforming state-of-the-art methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Ablation Study</head><p>Our proposed multi-order high-precision hourglass network contains two pivotal components. From <ref type="table" target="#tab_1">Table I</ref>, we can find that each proposed module plays an essential role in improving performance. The performance of the baseline HGs in <ref type="table" target="#tab_1">Table I</ref> can achieve 4.43% NME on the 300W Common subset and 7.56% NME on the 300W Challenging subset. When HGs are combined with the HSR method or the MCG model, the performances of the corresponding algorithms are greatly improved (6.72% NME and 6.51% NME on 300W challenging subset, respectively). When integrating the HSR method and MCG model via a multi-order high-precision hourglass network, the final results of our method can be further improved (3.18% NME on 300W Common subset and 6.01% NME on 300W Challenging subset). The experimental results from Tables I-IV indicate that 1) the MCG model can effectively utilize multi-order cross information to enhance geometric constraints for faces under extremely large poses and heavy occlusions; 2) the SDL and SDT can be combined by the HSR method to achieve heatmap subpixel regression; and 3) the combination of the MCG model and HSR method works better than any one of them on their own, showing the complementarity of these two components.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Experimental results and discussions</head><p>From the experimental results listed in Tables I -VII and the figures presented in previous subsections, we state the following observations and corresponding analyses.</p><p>(1) MHHN, HGs <ref type="bibr" target="#b0">[1]</ref>, SAN <ref type="bibr" target="#b10">[11]</ref> and Liu et al. <ref type="bibr" target="#b12">[13]</ref> are heatmap regression face alignment methods. However, from Tables I -V, we know that the MHHN performs better than the other methods, mainly because 1) the MCG model can effectively enhance facial geometric constraints; 2) the HSR method can further help achieve high-precision facial landmark detection; and 3) by integrating the MCG model and HSR method into a multi-order high-precision hourglass network, more effective landmark heatmaps can be generated and heatmap subpixel landmark coordinates can be detected.</p><p>(2) Both the MHHN and ODN <ref type="bibr" target="#b20">[21]</ref> enhance the robustness of the model to partial occlusion by capturing facial geometric constraints among different components with the matrix outer product operation. However, from the experimental results on occluded datasets, such as the 300W challenging set and the COFW dataset (see <ref type="table" target="#tab_1">Tables I and II)</ref>, we know that our MHHN greatly exceeds the ODN, which indicates that by introducing multi-order cross information (i.e., high-order information and cross-layer information), our MCG model can obtain more discriminative representations for enhancing the geometric constraints, which leads to more robust face alignment.</p><p>(3) The proposed HSR method can help generate more accurate and effective landmark heatmaps by optimizing the <ref type="figure">Fig. 7</ref>. Comparisons of generated heatmaps of GT (ground-truth), HGs, HGs+MCG+JSDL and our method. With the proposed MHHN, we can find that our heatmaps are more robust to partial occlusions (a) and large poses (b). SDL, and then, the SDT can be further used to estimate the heatmap subpixel landmark coordinates. As shown in <ref type="table">Table  V</ref>, if we replace the SDL with other loss functions or do not use the SDT, the errors on the 300W challenging set will rise significantly, which indicates that 1) fine detection loss can help generate more effective landmark heatmaps; 2) the SDT can be used to achieve heatmap subpixel landmark detection; and 3) by incorporating the SDL with the SDT, we can achieve high-precision face alignment.</p><p>(4) As shown in <ref type="table">Table V</ref>, when combining the MSE with the SDT, the error of face alignment on 300W challenging subset can be effectively reduced, which also indicates that the proposed SDT can not only achieve heatmap subpixellevel facial landmark detection but can also make the heatmap pixel-level corrections to the detected landmarks.</p><p>V. CONCLUSION Unconstrained face alignment is still a very challenging topic due to the presence of large poses and partial occlusions. In this work, we present a multi-order high-precision hourglass network to address face alignment under extremely large poses and heavy occlusions. By fusing the multi-order cross geometry-aware model and the heatmap subpixel regression method with a seamless formulation, our MHHN is able to achieve more robust FLD. It is shown that the MCG model can effectively enhance geometric constraints and context information for face alignment by introducing multi-order cross information. The heatmap subpixel regression method can further improve the accuracy of face alignment by utilizing the subpixel detection loss and the subpixel detection technology, which help achieve heatmap subpixel face alignment. Experiments on four benchmark datasets of face alignment demonstrate that our method outperforms state-of-the-art methods. It can also be found from the experimental results that generating heatmaps with the distribution constraints is more effective for enhancing the accuracy and robustness of heatmap regression face alignment than generating heatmaps with pure pixel difference constraints.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig.</head><label></label><figDesc>Fig. 1. (a). The state-of-the-art method HGs[1]; (b). HGs+MCG; (c). HGs+MCG+HSR (MHHN). NME denotes the normalized mean error. The predicted landmarks of HGs, HGs+MCG and HGs+MCG+HSR are denoted by cyan, blue and green colors, respectively, and the ground-truth landmarks are denoted by red color. HGs fails to accurately predict landmarks under heavy occlusions (NME: 10.12) or large poses (NME: 9.82), while the proposed MCG and HSR acheives much better results, i.e., lower NME.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>(a) a landmark heatmap sample. (b) and (c) are the heatmap foreground regions of landmark (24, 24) and (24.2, 23.8), respectively. (d) and (e) are the related digital data of (b) and (c). From (d) and (e), the HR methods can estimate the same landmark coordinates</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>): It contains 1345 training images in which 845 images come from the LFPW [35] dataset and the other images are heavily occluded. The testing set contains 507 face images with heavy occlusions, large pose variations and expression variations. AFLW (19 landmarks): It contains 25993 face images that are characterized by relatively large differences in poses and expressions. AFLW-full divides the 24386 images into two parts: 20000 for training and 4386 for testing. AFLW-frontal selects 1165 images out of the 4386 testing images to evaluate the alignment algorithm on frontal faces.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>This work is supported by the National Natural Science Foundation of China(Grant No. 62076164, 62002233, 61802267, 61976145 and 61806127), the Natural Science Foundation of Guangdong Province (Grant No. 2019A1515111121, 2018A030310451 and 2018A030310450), the Shenzhen Municipal Science and Technology Innovation Council (Grant No. JCYJ20180305124834854) and the China Postdoctoral Science Fundation (Grant No. 2020M672802). Corresponding author: Zhihui Lai. J. Wan is with the College of Computer Science and Software Engineering, Shen zhen University, Shenzhen, 518060, China, and with the Information Systems Technology and Design Pillar, Singapore University of Technology and Design, Singapore, 487372 (e-mail: junwan2014@whu.edu.cn). Z. Lai, J. Zhou and C. Gao are with the College of Computer Science and Software Engineering, Shen zhen University, Shenzhen, 518060, China, and the Shenzhen Institute of Artificial Intelligence and Robotics for Society, Shenzhen, 518060, China.(e-mail: lai zhi hui@163.com, jie jpu@163.com, 2005gaocan@163.com) J. Liu is with the Information Systems Technology and Design Pillar, Singapore University of Technology and Design, Singapore, 487372 (e-mail: jun liu@sutd.edu.sg).</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I COMPARISONS</head><label>I</label><figDesc>WITH STATE-OF-THE-ART METHODS ON 300W DATASET. THE ERROR (NME) NORMALIZED BY THE INTER-PUPIL DISTANCE AND ITS STANDARD DEVIATION (IN BRACKET) ARE GIVEN. (-NOT COUNTED)</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE II COMPARISONS</head><label>II</label><figDesc>WITH STATE-OF-THE-ART METHODS ON COFW DATASET. THE ERROR (NME) NORMALIZED BY THE INTER-OCULAR DISTANCE AND ITS STANDARD DEVIATION (IN BRACKET) ARE GIVEN. (-NOT COUNTED)</figDesc><table><row><cell>Method</cell><cell>Error</cell><cell>Failure</cell></row><row><cell>human</cell><cell>5.6 (-)</cell><cell>-</cell></row><row><cell>PCPR[16]</cell><cell>8.50 (7.49)</cell><cell>20.00</cell></row><row><cell>HPM[43]</cell><cell>7.50 (5.88)</cell><cell>13.00</cell></row><row><cell>CCR[44]</cell><cell>7.03 (3.53)</cell><cell>10.9</cell></row><row><cell>DRDA[45]</cell><cell>6.46 (3.21)</cell><cell>6.00</cell></row><row><cell>RAR[38]</cell><cell>6.03 (2.84)</cell><cell>4.14</cell></row><row><cell>DAC-CSR(CVPR17)[46]</cell><cell>6.03 (3.07)</cell><cell>4.73</cell></row><row><cell>CAM[47]</cell><cell>5.95 (2.67)</cell><cell>3.94</cell></row><row><cell>CRD[22]</cell><cell>5.72 (2.44)</cell><cell>3.76</cell></row><row><cell>LAB(CVPR18)[10]</cell><cell>5.58 (2.17)</cell><cell>2.76</cell></row><row><cell>ODN(CVPR19)[21]</cell><cell>5.30 (1.94)</cell><cell>-</cell></row><row><cell>HGs</cell><cell>6.21 (3.29)</cell><cell>5.52</cell></row><row><cell>HGs+MCG</cell><cell>5.31 (1.96)</cell><cell>3.16</cell></row><row><cell>HGs+HSR</cell><cell>5.62 (2.27)</cell><cell>3.75</cell></row><row><cell>HGs+MCG+HSR (MHHN)</cell><cell>4.95 (1.61)</cell><cell>1.78</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE III COMPARISONS</head><label>III</label><figDesc>WITH STATE-OF-THE-ART METHODS ON AFLW DATASET. THE ERROR (NME) NORMALIZED BY FACE SIZE AND ITS STANDARD DEVIATION (IN BRACKET) ARE GIVEN.</figDesc><table><row><cell cols="3">Method</cell><cell></cell><cell>full</cell><cell></cell><cell>frontal</cell></row><row><cell cols="3">SDM[48]</cell><cell></cell><cell cols="2">4.05 (3.91)</cell><cell cols="2">2.94 (3.47)</cell></row><row><cell cols="3">PCPR[16]</cell><cell></cell><cell cols="2">3.73 (3.68)</cell><cell cols="2">2.87 (3.16)</cell></row><row><cell cols="3">ERT[49]</cell><cell></cell><cell cols="2">4.35 (4.78)</cell><cell cols="2">2.75 (3.21)</cell></row><row><cell cols="3">LBF[19]</cell><cell></cell><cell cols="2">4.25 (4.43)</cell><cell cols="2">2.74 (3.12)</cell></row><row><cell cols="3">CCL[18]</cell><cell></cell><cell cols="2">2.72 (2.41)</cell><cell cols="2">2.17 (2.28)</cell></row><row><cell cols="4">DAC-CSR(CVPR17)[46]</cell><cell cols="2">2.27 (2.25)</cell><cell cols="2">1.81 (1.69)</cell></row><row><cell cols="4">SAN[11](CVPR18)</cell><cell cols="2">1.91 (1.94)</cell><cell cols="2">1.85 (1.77)</cell></row><row><cell cols="3">CRD[22]</cell><cell></cell><cell cols="2">1.80 (1.78)</cell><cell cols="2">1.59 (1.29)</cell></row><row><cell cols="4">ODN[21](CVPR19)</cell><cell cols="2">1.63 (1.34)</cell><cell cols="2">1.38 (1.21)</cell></row><row><cell cols="2">HGs</cell><cell></cell><cell></cell><cell cols="2">2.47 (2.33)</cell><cell cols="2">1.92 (2.01)</cell></row><row><cell cols="3">HGs+MCG</cell><cell></cell><cell cols="2">1.59 (1.29)</cell><cell cols="2">1.31 (1.01)</cell></row><row><cell cols="3">HGs+HSR</cell><cell></cell><cell cols="2">1.76 (1.53)</cell><cell cols="2">1.48 (1.37)</cell></row><row><cell cols="4">HGs+MCG+HSR(MHHN)</cell><cell cols="2">1.38 (1.17)</cell><cell cols="2">1.19 (0.81)</cell></row><row><cell></cell><cell>1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Fraction of Test Faces (4386 in Total)</cell><cell>0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>SDM CCL DAC?CSR SAN</cell></row><row><cell></cell><cell>0.1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>ODN</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Ours</cell></row><row><cell></cell><cell>0</cell><cell>0</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">Error Normalized by Face Size</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE IV COMPARISONS</head><label>IV</label><figDesc>WITH STATE-OF-THE-ART METHODS ON WFLW DATASET. THE ERROR (NME) NORMALIZED BY THE INTER-OCULAR DISTANCE AND ITS STANDARD DEVIATION (IN BRACKET) ARE GIVEN.</figDesc><table><row><cell>Method</cell><cell></cell><cell></cell><cell>Testset</cell><cell></cell><cell>Pose Subset</cell><cell>Expression</cell><cell>Illumination</cell><cell>Make-Up</cell><cell>Occlusion</cell><cell>Blur Subset</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Subset</cell><cell>Subset</cell><cell>Subset</cell><cell>Subset</cell></row><row><cell></cell><cell></cell><cell cols="2">TABLE V</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="7">THE EFFECT OF DIFFERENT LOSS FUNCTIONS ON THE 300W</cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="3">CHALLENGING SUBSET.</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Loss</cell><cell>MSE</cell><cell>MSE</cell><cell>KLDL</cell><cell>JSDL</cell><cell>SDL1</cell><cell>SDL</cell><cell></cell></row><row><cell></cell><cell></cell><cell>+SDT</cell><cell>+SDT</cell><cell>+SDT</cell><cell>+SDT</cell><cell>+SDT</cell><cell></cell></row><row><cell cols="2">NME(%) 7.56</cell><cell>7.31</cell><cell>7.02</cell><cell>6.83</cell><cell>6.77</cell><cell>6.72</cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">TABLE VI</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="7">THE EFFECT OF DIFFERENT ? VALUES OF THE HGS+HSR ON THE 300W</cell><cell></cell></row><row><cell></cell><cell cols="5">CHALLENGING SUBSET. (-NOT CONVERGED)</cell><cell></cell><cell></cell></row><row><cell>?</cell><cell>2</cell><cell>3</cell><cell></cell><cell>4</cell><cell>5</cell><cell></cell><cell></cell></row><row><cell cols="2">NME(%) -</cell><cell cols="2">6.72</cell><cell>6.90</cell><cell>7.17</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE VII THE</head><label>VII</label><figDesc>EFFECT OF DIFFERENT ? VALUES OF THE MHHN ON 300W</figDesc><table><row><cell></cell><cell></cell><cell cols="2">CHALLENGING SUBSET.</cell><cell></cell><cell></cell></row><row><cell>?</cell><cell>1/2</cell><cell>1/4</cell><cell>1/8</cell><cell>1/16</cell><cell>1/32</cell></row><row><cell cols="2">NME(%) 7.01</cell><cell>6.47</cell><cell>6.21</cell><cell>6.01</cell><cell>6.13</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements We thank the reviewers for the suggestions and the NSCC for computing resources.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Stacked hourglass network for robust facial landmark localisation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2025" to="2033" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Active shape models-their training and application</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">F</forename><surname>Cootes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">H</forename><surname>Cooper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Graham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer vision and image understanding</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="38" to="59" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Active appearance models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">F</forename><surname>Cootes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="681" to="685" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Feature detection and tracking with constrained local models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cristinacce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">F</forename><surname>Cootes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Gauss-newton deformable part models for face alignment in-the-wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1851" to="1858" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Face alignment in-the-wild: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">162</biblScope>
			<biblScope unit="page" from="1" to="22" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Facial feature point detection: A comprehensive survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">275</biblScope>
			<biblScope unit="page" from="50" to="65" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Face alignment by explicit shape regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">107</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="177" to="190" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Robust facial landmark detection via a fully-convolutional local-global context network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Merget</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Rigoll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="781" to="790" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Look at boundary: A boundary-aware face alignment algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in CVPR</title>
		<imprint>
			<biblScope unit="page" from="2129" to="2138" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Style aggregated network for facial landmark detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in CVPR</title>
		<imprint>
			<biblScope unit="page" from="379" to="388" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Human pose estimation via convolutional part heatmap regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bulat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="717" to="732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Semantic alignment: Finding semantically consistent ground-truth for facial landmark detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3467" to="3476" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A deep convolutional neural network for background subtraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Babaee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">T</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Rigoll</surname></persName>
		</author>
		<idno>abs/1702.01731</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Multi-scale context aggregation by dilated convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<idno>abs/1511.07122</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Robust face landmark estimation under occlusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">P</forename><surname>Burgosartizzu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1513" to="1520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">300 faces in-the-wild challenge: database and results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sagonas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Antonakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">P</forename><surname>Zafeiriou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image Vision Comput</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="3" to="18" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Unconstrained face alignment via cascaded compositional learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3409" to="3417" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Face alignment at 3000 fps via regressing local binary features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1685" to="1692" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Facial landmark detection by deep multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="94" to="108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Robust facial landmark detection via occlusion-adaptive deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sadiq</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3486" to="3496" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Robust face alignment by cascaded regression and de-occlusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">123</biblScope>
			<biblScope unit="page" from="261" to="272" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Global second-order pooling convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in CVPR</title>
		<imprint>
			<biblScope unit="page" from="3024" to="3033" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Second-order attention network for single image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-T</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">P</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3024" to="3033" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deep global generalized gaussian networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5080" to="5088" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Towards highly accurate and stable face alignment for high-resolution videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>AAAI</publisher>
			<biblScope unit="page" from="8893" to="8900" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Hypernet: Towards accurate region proposal generation and joint object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="845" to="853" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A unified multi-scale deep convolutional neural network for fast object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Feris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="354" to="370" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-N</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Cross-x learning for fine-grained visual categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8242" to="8251" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2117" to="2125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Bilinear convolutional neural networks for fine-grained visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Roychowdhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="1309" to="1322" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Hyperlayer bilinear pooling with application to fine-grained categorization and image retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">282</biblScope>
			<biblScope unit="page" from="174" to="183" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Localizing parts of faces using a consensus of exemplars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">N</forename><surname>Belhumeur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">W</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Kriegman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="545" to="552" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Face detection, pose estimation, and landmark localization in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2879" to="2886" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Interactive facial feature localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Brandt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="679" to="692" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Face alignment across large poses: A 3d solution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="146" to="155" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Robust facial landmark detection via recurrent attentive-refinement networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kassim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="57" to="72" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">A deeplyinitialized coarse-to-fine ensemble of regression trees for face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Valle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Buenaposada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vald?s</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Baumela</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ECCV</title>
		<imprint>
			<biblScope unit="page" from="146" to="155" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Wing loss for robust facial landmark localisation with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-H</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kittler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Awais</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Huber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2235" to="2245" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Supervision-by-registration: An unsupervised approach to improve the precision of facial landmark detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-I</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in CVPR</title>
		<imprint>
			<biblScope unit="page" from="360" to="368" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Face alignment by coarseto-fine shape searching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4998" to="5006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Occlusion coherence: Localizing occluded faces with a hierarchical deformable part model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ghiasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Fowlkes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1899" to="1906" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Cascaded collaborative regression for robust facial landmark detection trained using a mixture of synthetic and real images with dynamic weighting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">H</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kittler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Christmas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">J</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="3425" to="3440" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Occlusion-free face alignment: Deep regression networks coupled with de-corrupt autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3428" to="3437" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Dynamic attention-controlled cascaded shape regression exploiting training data augmentation and fuzzy-set sample weighting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-H</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kittler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">J</forename><surname>Christmas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Huber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3681" to="3690" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Face alignment by component adaptive mechanism</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">329</biblScope>
			<biblScope unit="page" from="227" to="236" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Supervised descent method and its applications to face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>De La</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Torre</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="532" to="539" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">One millisecond face alignment with an ensemble of regression trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kazemi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sullivan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1867" to="1874" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Leveraging intra and inter-dataset variations for robust face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR workshops</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="150" to="159" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
