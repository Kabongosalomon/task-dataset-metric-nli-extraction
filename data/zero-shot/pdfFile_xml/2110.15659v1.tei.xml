<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Amendable Generation for Dialogue State Tracking</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Tian</surname></persName>
							<email>tianxin06@baidu.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Baidu Inc</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liankai</forename><surname>Huang</surname></persName>
							<email>huangliankai@baidu.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Baidu Inc</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingzhan</forename><surname>Lin</surname></persName>
							<email>linyingzhan01@baidu.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Baidu Inc</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siqi</forename><surname>Bao</surname></persName>
							<email>baosiqi@baidu.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Baidu Inc</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huang</forename><surname>He</surname></persName>
							<email>hehuang@baidu.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Baidu Inc</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunyi</forename><surname>Yang</surname></persName>
							<email>yangyunyi01@baidu.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Baidu Inc</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Wu</surname></persName>
							<email>wu_hua@baidu.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Baidu Inc</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Wang</surname></persName>
							<email>wang.fan@baidu.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Baidu Inc</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuqi</forename><surname>Sun</surname></persName>
							<email>sunshuqi01@baidu.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Baidu Inc</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Amendable Generation for Dialogue State Tracking</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T11:19+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In task-oriented dialogue systems, recent dialogue state tracking methods tend to perform one-pass generation of the dialogue state based on the previous dialogue state. The mistakes of these models made at the current turn are prone to be carried over to the next turn, causing error propagation. In this paper, we propose a novel Amendable Generation for Dialogue State Tracking (AG-DST), which contains a two-pass generation process: (1) generating a primitive dialogue state based on the dialogue of the current turn and the previous dialogue state, and (2) amending the primitive dialogue state from the first pass. With the additional amending generation pass, our model is tasked to learn more robust dialogue state tracking by amending the errors that still exist in the primitive dialogue state, which plays the role of reviser in the double-checking process and alleviates unnecessary error propagation. Experimental results show that AG-DST significantly outperforms previous works in two active DST datasets (MultiWOZ 2.2 and WOZ 2.0), achieving new state-of-the-art performances.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Dialogue state tracking (DST) is a crucial task in task-oriented dialogue systems, as it affects database query results as well as the subsequent policy prediction <ref type="bibr">(Chen et al., 2017)</ref>. It extracts users' goals at each turn of the conversation and represents them in the form of a set of (slot, value) pairs, i.e., dialogue state.</p><p>Traditional methods of DST mainly rely on a predefined ontology which includes all possible slots and corresponding values. These models predict the value for each slot as a classification problem <ref type="bibr" target="#b2">(Mrk?i? et al., 2017;</ref><ref type="bibr" target="#b8">Zhong et al., 2018;</ref><ref type="bibr">Ramadan et al., 2018)</ref>. However, in practical applications, some slot values appearing in the conversations cannot be predefined, and it is infeasible to acquire a fully predefined ontology or transfer to other domains with fixed predefined ontology. To address such challenges, open-vocabulary DST has been proposed, where the value of each slot is directly generated or extracted from the dialogue history <ref type="bibr">(Chao and Lane, 2019;</ref><ref type="bibr">Hosseini-Asl et al., 2020;</ref><ref type="bibr">Ham et al., 2020;</ref><ref type="bibr">Heck et al., 2020)</ref>. Although this approach offers scalability and is capable of handling unseen slot values, many of the previous models are not efficient enough as they need to predict the dialogue state from scratch based on the dialogue history.</p><p>On the merits of utilizing the previous dialogue state as a compact representation of the previous dialogue history, some recent methods choose to take the previous dialogue state into consideration when generating the slot values. One direction is to decompose DST into two explicit sub-tasks: State Operation Prediction and Value Generation <ref type="bibr">(Kim et al., 2020;</ref><ref type="bibr" target="#b6">Zeng and Nie, 2020)</ref>. At each turn, whether or how to modify the value in the previous dialogue state is determined by the discrete operations from the state operation prediction, so the accuracy of state operation prediction holds back the overall DST performance <ref type="bibr">(Kim et al., 2020)</ref>. Another direction of recent works recasts dialogue state tracking into a single causal language model by using the dialogue of the current turn and the previous dialogue state as input sequence <ref type="bibr">(Lin et al., 2020;</ref><ref type="bibr" target="#b4">Yang et al., 2021)</ref>, where the current dialogue state is generated by jointly modeling the state operation prediction and value generation in a implicit fashion. While it is more effective and reasonable to use the previous dialogue state under the Markov assumption, the mistakes of these models made during the prediction of the current turn are prone to be carried over to the next turn, causing error propagation. These carried-over mistakes are unlikely to be fixed in the next turn. Essentially, these models perform a one-pass generation process and lack a double-checking process to amend the mistakes of <ref type="figure">Figure 1</ref>: An example of AG-DST with two-pass generation process. In this example, the user wants to change restaurant-people from 2 to 1 by "... a change in plans, I will be dining alone". In the two-pass generation process, the amending generation obtains corresponding information to correct mistake in the basic generation. the current turn. Missing such amending process would result in some potential mistakes being left unfixed.</p><p>To nip the mistakes in the bud and alleviate the error propagation problem, we propose Amendable Generation for Dialogue State Tracking (AG-DST), a pretrained language model that generates the dialogue state based on the dialogue of the current turn and previous dialogue state. In contrast to previous one-pass generation process <ref type="bibr">(Kim et al., 2020;</ref><ref type="bibr" target="#b6">Zeng and Nie, 2020;</ref><ref type="bibr">Lin et al., 2020;</ref><ref type="bibr" target="#b4">Yang et al., 2021)</ref>, AG-DST employs a two-pass generation process consisting of a basic generation and an amending generation, where the first pass uses the dialogue of the current turn and the previous dialogue state to generate a primitive dialogue state, and second pass utilizes the dialogue of the current turn to amend the primitive dialogue state. With the additional amending generation pass, our model is tasked to learn more robust dialogue state tracking by amending the errors that still exist in the primitive dialogue state. These errors are more challenging to fix and relatively scarce during training. Therefore, we further design a negative sampling mechanism to exploit more challenging errors and facilitate the effective learning of the amending generation pass. With such two-pass generation process, AG-DST is less likely to generate false dialogue state for the next turn, and thus reduces error propagation. <ref type="figure">Figure 1</ref> illustrates a complete dialogue state generation process of AG-DST.</p><p>Experimental results show that AG-DST consistently outperforms all prior works on Multi-WOZ 2.2 and WOZ 2.0. Especially on MultiWOZ 2.2, AG-DST achieves 57.26% joint goal accuracy, 2.86% higher than the previous state-of-the-art performance. Besides, we provide ablation study and the attention visualization to demonstrate the effec-tiveness of the amending generation, and analyze the types of mistakes that can be corrected by the amending generation. Our models and code will be released for further research. 1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methodology</head><p>In this section, we introduce AG-DST in the following aspects: the basic generation, the amending generation and the training objective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Basic Generation</head><p>A dialogue with T turns can be represented as D = {D 1 , D 2 , . . . , D T }, where D t is the dialogue at turn t consisting of system response R t and user utterance U t . We denote the dialogue states at every turn as B = {B 1 , B 2 , . . . , B T }. For multidomain DST, the dialogue state at turn t is denoted</p><formula xml:id="formula_0">as B t = {(S i , V i t )|1 ? i ? I},</formula><p>in which S i is the slot and V i t is its corresponding value (I is the number of all slots in different domains). Particularly, S i is represented as a special token concatenated by domain and slot (i.e. &lt;domain-slot&gt;) following most previous works. We use special tokens &lt;nm&gt; and &lt;dc&gt; to indicate not mentioned and don't care in slot value respectively.</p><p>We leverage the dialogue of the current turn and the previous dialogue state to generate the current dialogue state. The dialogue of the current turn D t is used in the input tokens under the Markov assumption. To a certain extent the previous dialogue state B t?1 could be viewed as a compact representation of the previous dialogue history <ref type="bibr">(Kim et al., 2020)</ref>. Specifically, we denote the dialogue at turn t as:</p><formula xml:id="formula_1">D t = [R t ; U t ]<label>(1)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>AG-DST Basic Generation</head><p>Transformer Block 1 1</p><p>Transformer Block 2 2 ? Transformer Block L L</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Amending Generation</head><p>Transformer Block 1 1 <ref type="figure">Figure 2</ref>: The overview of the proposed AG-DST. In the basic generation, AG-DST takes the dialogue of the current turn D t and the previous dialogue stateB t?1 as input and generates the primitive dialogue stateB t . In the amending generation, AG-DST takes the dialogue of the current turn D t and the primitive dialogue stateB t as input and outputs the amended dialogue stateB t .</p><formula xml:id="formula_2">Transformer Block 2 2 ? Transformer Block L L B t B tBt B t D t D tBt?1 B t?1 D t D tBt B t Dialogue Last DS Amended DS Primitive DS Dialogue</formula><p>where R t and U t are system response and user utterance accordingly. Special tokens &lt;con/&gt; and &lt;/con&gt; are added around D t for marking the boundary of the dialogue context, and special tokens &lt;sys&gt; and &lt;usr&gt; are added before R t and U t respectively to indicate the role. The dialogue state at turn t is denoted as</p><formula xml:id="formula_3">B t = [B 1 t ; B 2 t ; . . . ; B I t ]<label>(2)</label></formula><p>where</p><formula xml:id="formula_4">B i t = [S i ; V i t ]</formula><p>is the concatenation of i-th slot and its value. Similar to the dialogue context, two special tokens &lt;ds/&gt; and &lt;/ds&gt; are added around the whole dialogue state.</p><p>The overview of AG-DST is illustrated in <ref type="figure">Figure</ref> 2. AG-DST is a generation model based on transformer <ref type="bibr" target="#b1">(Vaswani et al., 2017;</ref><ref type="bibr">Dong et al., 2019)</ref>. In the basic generation of AG-DST, the input sequence is composed of the current turn dialogue and the previous dialogue state, and the primitive dialogue state is predicted by:</p><formula xml:id="formula_5">B t = T ransf ormer(D t , B t?1 )<label>(3)</label></formula><p>where &lt;gen/&gt; and &lt;/gen&gt; are added around the whole input sequence to indicate the first pass generation process. As shown in <ref type="figure" target="#fig_0">Figure 3</ref>, the input embedding of each token is the sum of token embedding, position embedding, role embedding and segment embedding. Among them, position embedding is added to discriminate input token positions; role embedding is employed to distinguish the characters of the speaker in the dialogue; segment embedding is used for different types of sequence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Amending Generation</head><p>To amend the potential errors in the primitive dialogue state, we propose a novel amending generation that takes the dialogue of the current turn and the primitive dialogue state predicted by the basic generation as input, and generates the amended dialogue state:</p><formula xml:id="formula_6">B t = T ransf ormer(D t ,B t )<label>(4)</label></formula><p>where the new input sequence of amending generation is consisted of the current turn dialogue and the primitive dialogue state,B t is the amended dialogue state. The amending generation shares the same parameters with the basic generation model. To differentiate this two-pass generation process, the special tokens &lt;amend/&gt; and &lt;/amend&gt; are added around the new input sequence in Equation <ref type="formula" target="#formula_6">4</ref> as opposed to the &lt;gen/&gt; and &lt;/gen&gt; in Equation 3.</p><p>Negative Sampling To facilitate effective learning of the amending generation process, we propose a negative sampling strategy that actively mines the examples on which the model is prone to make mistakes (i.e., generating the wrong slot values, or failing to update some slots). Specifically, we performs negative sampling on the dialogue state with changed slot values between turns  </p><formula xml:id="formula_7">&lt;gen/&gt; &lt;utt/&gt; &lt;sys&gt; ? [SEP] &lt;usr&gt; ? &lt;/utt&gt; [SEP] &lt;ds/&gt; S 1 S 1 V 1 t?1 V 1 t?1 ? &lt;/ds&gt; &lt;/gen&gt; &lt;ds/&gt; S 1 S 1 V 1 t V 1 t ? &lt;/ds&gt; &lt;gen/&gt; &lt;utt/&gt; &lt;sys&gt; ? [SEP] &lt;usr&gt; ? &lt;/utt&gt; [SEP] &lt;ds/&gt; S 1 S 1 V 1 t?1 V 1 t?1 ? &lt;/ds&gt; &lt;/gen&gt; &lt;ds/&gt; S 1 S 1 V 1 t V</formula><formula xml:id="formula_8">?B t = B t ? B t ? B t?1 ,</formula><p>where these slot values are randomly replaced by &lt;nm&gt;, &lt;dc&gt; or some wrong values. During training, the dialogue state after negative sampling is used as the primitive dialogue stateB t in the amending generation to encourage the model to lay more emphasis on error correction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Training Objective</head><p>The training objective of the basic generation is the negative log-likelihood loss given the dialogue of the current turn and the previous dialogue state:</p><formula xml:id="formula_9">L basic = ? log P (B t |D t , B t?1 )<label>(5)</label></formula><p>Similar to the basic generation, the loss of the amending generation is also the negative loglikelihood loss:</p><formula xml:id="formula_10">L amending = ? log P (B t |D t ,B t )<label>(6)</label></formula><p>The total objective of AG-DST is to minimize the sum of the above two losses:</p><formula xml:id="formula_11">L = L basic + L amending<label>(7)</label></formula><p>3 Experiments</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Datasets</head><p>We conduct our experiments on both multi-domain dataset MultiWOZ 2.2 <ref type="bibr" target="#b5">(Zang et al., 2020)</ref>   <ref type="table">Table 1</ref> summarizes the statistics of the above two datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Implementation Details</head><p>Our AG-DST approach can be easily deployed with many pre-trained generation models (such as GPT-2 (Radford et al., 2019), T5 (Raffel et al., 2020), PLATO-2 <ref type="bibr" target="#b0">(Bao et al., 2021)</ref>). In this paper, we employ the GPT-2 and PLATO-2 to initialize our model parameters. GPT-2 is a large causal language model, and PLATO-2 is a largescale open-domain dialogue model trained on the Reddit comments. Specifically, GPT-2 has 117M parameters containing 12 transformer blocks, 12 attention heads and 768 hidden units, and PLATO-2 has 310M parameters containing 24 transformer blocks, 16 attention heads and 1024 hidden units. <ref type="bibr">2</ref> Adam optimizer (Kingma and Ba, 2015) is employed for optimization in all experiments. For the hyper-parameters we used in the best model, the PLATO-2 is fine-tuned with a dynamic batch size of 8192 tokens, a learning rate of 1e ? 5, warmup steps of 1000 and a learning rate decay rate of 0.01. For GPT-2, we set the batch size of 6, a learning rate of 5e ? 5 with no warmup and learning rate decay. All the models are trained on 4 Nvidia Telsa V100 32G GPU cards for 40 epochs and early stop according to the performance on the validation set.</p><p>All the reported results of AG-DST are averages over five runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Baselines</head><p>We compare our approach with the following methods:</p><p>Neural Belief Tracker (Mrk?i? et al., 2017) learns the distributed representation of system responses and user utterances from pre-trained word vectors, and decides which slot-value pairs are required by the user.</p><p>Belief Tracking (Ramadan et al., 2018) proposes a model that utilizes semantic similarity between ontology terms and utterances and shares the information across domains.</p><p>GLAD <ref type="bibr" target="#b8">(Zhong et al., 2018)</ref> uses global modules to share parameters across slots and local modules to learn the features which take into account the specific slot.</p><p>StateNet (Ren et al., 2018) proposes a model that composes a representation of the dialogue history and measures the distances between the representation and the value vectors.</p><p>TRADE <ref type="bibr" target="#b3">(Wu et al., 2019)</ref> uses a state operation with an utterance encoder and a dialogue state generator to handle the cross domain phenomenon.  proposes a dual strategy: picklist-based and span-based. The spanbased strategy includes a slot-gate classifier and span-based slot-value prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DS-DST</head><p>BERT-DST (Chao and Lane, 2019) adopts BERT to encode dialogue context and extracts slot values by the state operation prediction and the span prediction. <ref type="bibr">(Kim et al., 2020)</ref> proposes an efficient decoder by updating dialogue state as a memory to reduce the burden of the decoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SOM-DST</head><p>COMER (Ren et al., 2019) adopts two sequential decoders to generate dialogue state. The first decoder is used to generate state sketch (i.e. domains and slots), and the second one is used to generate slot values by conditioning on the dialogue history and state sketch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SGD-baseline</head><p>(Rastogi et al., 2020) adapts BERT to obtain the schema embeddings (including intents, slots and possible values of categorical slots) and utterance embeddings and uses different strategies for non-categorical and categorical slots.</p><p>SimpleTOD (Hosseini-Asl et al., 2020) changes sub-tasks of task-oriented dialogue into a single causal language model which generates dialogue state, system action and system response successively.</p><p>Seq2Seq-DU (Feng et al., 2021) applies schema descriptions to deal with unseen domains with a sequence-to-sequence framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Experimental Results</head><p>We use joint goal accuracy (Joint Acc) as our main evaluation metric for dialogue state tracking. Joint goal accuracy measures the percentage of correct in all dialogue turns, where a turn is considered as correct if and only if all the slot values are correctly predicted. We also show the slot accuracy for each domain which measures the accuracy of all the slot values in that specific domain. <ref type="table" target="#tab_3">Table 2</ref> shows the performance of the AG-DST in comparison to the baselines. We can observe that the AG-DST consistently outperforms all baselines on both MultiWOZ 2.2 and WOZ 2.0 datasets, achieving a new state-of-the-art performance. On MultiWOZ 2.2, AG-DST achieve 57.26% joint goal accuracy, by 2.86% significant improvement on the top of the Seq2Seq-DU <ref type="bibr">(Feng et al., 2021)</ref>, the latest sequence-to-sequence generation model. In addition, the domain-specific results of our approach are also provided in <ref type="table" target="#tab_4">Table 3</ref>. On the singledomain dataset, WOZ 2.0, we obtain joint goal accuracy of 91.37%, which indicates that our model is also effective in a relatively simple scenario. We   will analyse the strengths of AG-DST in the subsequent section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Amending Generation</head><p>As shown in  heuristic negative sampling, the model achieves the best joint goal accuracy of 57.35%. Essentially, the amending generation process amends the challenging mistakes that still exists after the basic generation and provides a more accurate prediction of the dialogue state of the current turn, which effectively alleviate the error propagation problem. To the best of our knowledge, AG-DST is the first DST method that involves a two-pass generation process to amend the primitive dialogue state.</p><p>Visualization <ref type="figure" target="#fig_1">Figure 4</ref> shows the attention visualization of the basic generation and the amending  generation on an example of MultiWOZ 2.2. In the basic generation, the value of slot restaurant-area is copied from previous dialogue state and the value of slot restaurant-people is predicted from user utterance accurately. However, for the restaurantname, although the model pays attention to the corresponding tokens in the dialogue of the current turn, it generates a wrong slot value. In the amending generation, the restaurant-name and its value attend to both the corresponding tokens in the user utterance and slot value in the primitive dialogue state with high weight, which indicates that the amending generation can utilize both the dialogue of the current turn and the primitive dialogue state for reference to correct the mistakes in the primitive dialogue state.</p><p>Error Analysis In our analysis, we found three frequent errors in a one-pass generation model of DST.</p><p>(1) The slot value is not updated, as the model fails to obtain the key information in the dialogue, an example can be seen in <ref type="figure">Figure 1.</ref> (2) Some common mistakes occur in generation model. For example, cambridge lodge is predicted to cambridge lodge lodge, and camboats is predicted to cambots.</p><p>(3) There is confusion between correlated slots, such as leave at and arrive by, departure and destination. As reported in <ref type="figure">Figure 5</ref>, we make the statistics of error types by random sampling on MultiWOZ 2.2, which indicates that 51% of the error comes from the first type, 4% comes from the second type, 3% comes from the third type, 36% is due to the inconsistent annotation, and 6% is due to others. Furthermore, we show that the proposed amendable generation can greatly correct these errors and improve the overall performance of DST with concrete examples of the amending generation are shown in <ref type="table" target="#tab_13">Table 9</ref> of Appendix B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Effect of Previous Dialogue State</head><p>As shown in <ref type="table" target="#tab_8">Table 5</ref>, we compare three types of input sequence formations: only dialogue history, dialogue history and previous dialogue state, current turn dialogue and previous dialogue state. We find that using the previous dialogue state (i.e. dialogue state memory) performs better than using only dialogue history, which confirms that the previous dialogue state can be served as a compact representation of the dialogue history and utilizing the previous dialogue state is more effective than performing DST from scratch. The results also show that using the current turn dialogue instead of <ref type="figure">Figure 5</ref>: Error type statistics of the basic generation. first type: slot value not updated; second type: common mistakes of generation; third type: confusion between correlated slots.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Structure</head><p>Joint Acc</p><formula xml:id="formula_12">(B t |D) ? 54.02 (B t |D, B t?1 ) 56.27 (B t |D t , B t?1 )</formula><p>56.20 the whole dialogue history barely hurts the performance, yet it greatly improves the efficiency, which justifies our use of the current turn dialogue and previous dialogue state as the input sequence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Effect of Pre-training</head><p>In prior works, GPT-2 is often the pre-trained backbone in generation model <ref type="bibr">(Hosseini-Asl et al., 2020;</ref><ref type="bibr">Ham et al., 2020;</ref><ref type="bibr" target="#b4">Yang et al., 2021)</ref>. We also analyze the ability of GPT-2 on MultiWOZ 2.2 (see <ref type="table" target="#tab_10">Table 6</ref>), where only token embedding and position embedding are added as input embedding. Results show that our approach initialized with GPT-2 surpasses the SimpleTOD (a generation model initialized by GPT-2) and Seq2Seq-DU (the prior state-of-the-art), and obtains similar results   with the PLATO-2 initialization. This indicates our approach's ability to universally improve the performances of other pretrained models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Effect of Special Tokens</head><p>Special tokens are important for identifying different input components (Hosseini-Asl et al., 2020).</p><p>In our experiments, similar to prior works, we use the utterance special token and dialogue state special token to differentiate each part. Besides, to boost the extraction of entity names on Multi-WOZ 2.2, we add special tokens &lt;name/&gt; and &lt;/name&gt; around the candidate entity names. <ref type="table" target="#tab_11">Table 7</ref> shows the joint goal accuracy reduction caused by the absence of various special tokens, which further confirms the neccesity of using special tokens.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>Traditional methods regard DST as a classification task, in which an encoder is employed for obtaining a representation of utterances and a classifier is utilized for predicting a slot value from a pre-defined ontology <ref type="bibr" target="#b2">(Mrk?i? et al., 2017;</ref><ref type="bibr" target="#b8">Zhong et al., 2018;</ref><ref type="bibr">Ramadan et al., 2018)</ref>. However, it is difficult to obtain a full ontology in a real scenario. Some open-vocabulary DST models extract or generate the dialogue state from the dialogue history at each turn <ref type="bibr">(Chao and Lane, 2019;</ref><ref type="bibr">Hosseini-Asl et al., 2020;</ref><ref type="bibr">Ham et al., 2020;</ref><ref type="bibr">Heck et al., 2020)</ref>. These methods predict the dialogue state from scratch at each turn, which undoubtedly puts a great burden on the model. To address this problem, some recent methods consider to utilize the previous dialogue state and recent dialogue history as input information. Specifically, the DST is separated into two components, where a component named State Operation Prediction is used to encode utterance and previous dialogue state, as well as predict the oper-ation of state at each turn, and the other component named Value Generation predicts the value for each slot <ref type="bibr">(Kim et al., 2020;</ref><ref type="bibr" target="#b6">Zeng and Nie, 2020)</ref>. <ref type="bibr">Kim et al. (2020)</ref> encodes the dialogue history of the last two turns and the previous dialogue state by BERT and predicts four kinds of state operations: carryover, delete, dontcare and update. For update operation, a GRU-based value generation is used to decode the slot value. <ref type="bibr" target="#b6">Zeng and Nie (2020)</ref> reuses a fraction of the hidden states of the encoder in the decoder to build a flat model for effective parameter updating. Besides, some methods treat dialogue state tracking as a causal language model by using the dialogue of the current turn and previous dialogue state as input sequence <ref type="bibr">(Lin et al., 2020;</ref><ref type="bibr" target="#b4">Yang et al., 2021)</ref>. <ref type="bibr">Lin et al. (2020)</ref> utilizes an encoder-decoder framework to generate dialogue state and system response sequentially, where minimal slot value pairs are generated for efficiently tracking. <ref type="bibr" target="#b4">Yang et al. (2021)</ref> models task-oriented dialogs on a dialog session level, which generates dialogue state, dialogue action and system response sequentially based on the whole previous dialogue context, including the generated dialogue states and dialogue actions. Moreover, some schemaguided DST methods leverage schema descriptions to deal with unseen schemas with new domains and slots <ref type="bibr" target="#b9">(Zhu et al., 2020;</ref><ref type="bibr">Feng et al., 2021;</ref><ref type="bibr">Noroozi et al., 2020)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we propose a novel amendable generative approach for dialogue state tracking, which learns implicit state operation prediction and value generation jointly in a single model to reduce the error propagation. Meanwhile, our model offers an opportunity to amend the primitive dialogue state in the amending generation. Experimental results show that our model outperforms previous works on both MultiWOZ 2.2 and WOZ 2.0 datasets, achieving state-of-the-art performance. The ablation study and attention visualization demonstrate that the proposed amending generation is significantly effective. Moreover, we analyze the types of mistakes that can be resolved by the amending generation and provide some examples to illustrate them in the Appendix. In the future, we will try to integrate schema descriptions into our architecture and explore a generative approach to support end-to-end task-oriented dialogue system. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Error Propagation Analysis</head><p>Our model learns simultaneously the implicit state operation prediction and value generation by a single generation model with dialogue state memory. Indeed, such joint learning can boost the performance of the update gate. Because the traditional method needs to predict the state operation firstly, and then use another component to predict the corresponding value, which leads to error propagation. The update gate accuracy and the value generation F1 under the update gate are given in <ref type="table" target="#tab_14">Table 8</ref>. <ref type="bibr">3</ref> Compared with the SOM-DST, our AG-DST attains better results of both gate accuracy and value F1. This implies that the coupling structure we used is beneficial to learning update gate and corresponding value prediction. 3 Note that for our model, the state operation can be concluded from the difference between last and current dialogue states.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Case Study</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Acc F1</p><p>SOM-DST 70.50 78.58 AG-DST 72.77 79.84  <ref type="figure">Figure 6a</ref> and 6b, we can easily find that the slot values attend to the corresponding tokens in the utterance with a highest weight. <ref type="figure">Figure 6c</ref> indicates that an implicit copy mechanism is used for the carryover gate. <ref type="figure">Figure 7</ref> shows the attention visualization of delete gate and coreference phenomenon. For delete gate in <ref type="figure">Figure 7a</ref>, it is easily seen that the model can grasp the information in the user utterance to generate appropriate &lt;nm&gt; slot value. As shown in <ref type="figure">Figure 7b,</ref>  taxi-departure-&lt;nm&gt; taxi-departure-&lt;nm&gt; taxi-departure-alexander bed and breakfast U t : i am sorry , i do not want a second hotel . i need a taxi between the bed and breakfast and the restaurant that arrives to the restaurant by the booked time .</p><p>R t : to clarify you wanted me to book a table on friday ?</p><p>train-day-&lt;nm&gt; train-day-&lt;nm&gt; train-day-tuesday U t : i am sorry . my train was supposed to be for tuesday as well as my restaurant reservation .</p><p>R t : there are 5 choices . to narrow down you need to choose the side of town you prefer hotel-area-&lt;nm&gt; hotel-area-&lt;nm&gt; hotel-area-&lt;dc&gt; U t : area does not matter , but it should be a guest house with a 4 star rating .</p><p>R t : the entrance fee is unknown . can i help you with anything else today ?</p><p>taxi-destination-&lt;nm&gt; taxi-destination-tajione restaurant and coffee bar taxi-destination-stazione restaurant and coffee bar U t : i would also like a taxi to arrive at the restaurant on time . can i have the taxis phone number and vehicle type ? R t : okay ! they are at king's parade , cb21rl . their phone number is 01223338300 . what time would you like to depart in the taxi ? taxi-destination-&lt;nm&gt; taxi-destination-saint catharine's college taxi-destination-saint catharines college U t : i need a taxi to commute between the 2 place -s . i need to leave the restaurant by 18:15 and need the contact # and car type R t : i have got you a reservation for 6 at hobson's house for 2 nights . your reference number is 4wngilmf .</p><p>hotel-name-&lt;nm&gt; hotel-name-hobson's house hotel-name-hobsons house U t : thank you so much ! that should be all i need .</p><p>R t : is there anything i can do for you ? taxi-destination-&lt;nm&gt; taxi-destination-caffe jello gallery taxi-destination-cafe jello gallery U t : i would like a taxi to the cafe jello gallery please . R t : &lt;name/&gt; camboats &lt;/name&gt; is located at the plough , green end , fen ditton , in the east side of town . taxi-destination-&lt;nm&gt; taxi-destination-cambots taxi-destination-camboats U t : ok ! thank you ! can you get me a taxi to cambots ?</p><p>R t : the postcode is cb28rj for &lt;name/&gt; bridge guest house &lt;/name&gt; . what time do you want to be picked up at the guest house or to arrive at eraina ? taxi-destination-eraina taxi-destination-eraina taxi-destination-bridge guest house U t : okay i need a taxi too . <ref type="table" target="#tab_13">Table 9</ref>: Examples of the amending generation. The key information in the dialogue is shown in blue.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 :</head><label>3</label><figDesc>An example of input representation. The input embedding of each token is the sum of token embedding, position embedding, role embedding and segment embedding.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>The attention visualization of (a) basic generation and (b) amending generation on an example from MultiWOZ 2.2. Note that in both figures, the abscissa and ordinate are a fraction of whole input and output sequence respectively because of limited space.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Joint goal accuracy of AG-DST and baselines on MultiWOZ 2.2 and WOZ 2.0 datasets. AG-DST consistently outperforms all baselines. ? : the results borrowed from<ref type="bibr" target="#b5">Zang et al. (2020)</ref>. ? : our reproduction results by official code.</figDesc><table><row><cell>Domain</cell><cell cols="2">Joint Acc Slot Acc</cell></row><row><cell>multi-domain</cell><cell>57.26</cell><cell>97.48</cell></row><row><cell>attraction</cell><cell>89.91</cell><cell>96.26</cell></row><row><cell>hotel</cell><cell>82.31</cell><cell>97.16</cell></row><row><cell>restaurant</cell><cell>89.00</cell><cell>97.97</cell></row><row><cell>taxi</cell><cell>96.09</cell><cell>98.31</cell></row><row><cell>train</cell><cell>89.15</cell><cell>97.51</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Joint goal accuracy and slot accuracy of AG-DST on MultiWOZ 2.2 by domains.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4</head><label>4</label><figDesc>, we conduct ablation study to investigate the effectiveness of the proposed amending generation. The results show that the amending generation has a significant improvement on the basic generation, where the model gets extra joint goal accuracy of 0.87%. When we perform negative sampling to facilitate effective learning of the amending generation process, it will lead to an increase by 1.06%. Furthermore, we implement heuristic negative sampling which exchanges some correlated slot values (such as leave at and arrive by, departure and destination, area in different domains) on MultiWOZ 2.2. While using this further</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.3</cell><cell>0.2</cell><cell>0.1</cell></row><row><cell>current dialogue + previous dialogue state</cell><cell>&lt;nm&gt; &lt;restaurant-people&gt; &lt;nm&gt; &lt;restaurant-name&gt; west &lt;restaurant-area&gt; expensive &lt;restaurant-pricerange&gt; . please , aturday s on 30 : 17 at 8 for book to like would i . lodge bridge cam with go 's let &lt;usr&gt;</cell><cell>0.30</cell><cell>0.25</cell><cell>0.20</cell><cell>0.15</cell><cell>0.10</cell><cell>0.05</cell><cell>current dialogue + primitive dialogue state</cell><cell>8 &lt;restaurant-people&gt; lodge lodge bridge cam &lt;restaurant-name&gt; west &lt;restaurant-area&gt; expensive &lt;restaurant-pricerange&gt; . please , aturday s on 30 : 17 at 8 for book to like &lt;usr&gt; let 's go with cam bridge lodge . i would</cell></row><row><cell></cell><cell></cell><cell cols="6">&lt;restaurant-area&gt; west &lt;restaurant-name&gt; cam bridge lodge lodge &lt;restaurant-people&gt; 8</cell><cell></cell><cell cols="2">&lt;restaurant-area&gt; west &lt;restaurant-name&gt; cam bridge lodge &lt;restaurant-people&gt; 8</cell></row><row><cell></cell><cell cols="6">(a) Basic Generation</cell><cell></cell><cell cols="3">(b) Amending Generation</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 :</head><label>4</label><figDesc>The ablation study of the amending generation on MultiWOZ 2.2 with joint goal accuracy. NS means the negative sampling mentioned in Section 2.2, as well as NS + performs a heuristic negative sampling.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table /><note>Joint goal accuracy of different structures of generation model on MultiWOZ 2.2. The structures with dialogue state memory perform better than the rest which uses only dialogue context.? : we reuse the result of SimpleTOD in Table 2.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 6 :</head><label>6</label><figDesc></figDesc><table><row><cell>Model</cell><cell>Joint Acc</cell></row><row><cell>basic generation</cell><cell>56.20</cell></row><row><cell>-name special token</cell><cell>55.34 (-0.86)</cell></row><row><cell cols="2">-utterance special token 55.70 (-0.50)</cell></row><row><cell>-DS special token</cell><cell>54.50 (-1.70)</cell></row><row><cell>-above three</cell><cell>54.41 (-1.79)</cell></row></table><note>The ablation study of the pre-trained model on MultiWOZ 2.2 with joint goal accuracy.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 7</head><label>7</label><figDesc></figDesc><table><row><cell>: The ablation study of the absence of different</cell></row><row><cell>special tokens on MultiWOZ 2.2 with joint goal accu-</cell></row><row><cell>racy.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head></head><label></label><figDesc>Milica Gasic. 2020. TripPy: A triple copy strategy for value independent neural dialog state tracking. In Proceedings of the 21th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 35-44, 1st virtual meeting. Association for Computational Linguistics.</figDesc><table><row><cell cols="2">pages 432-437, Melbourne, Australia. Association</cell><cell></cell></row><row><cell>for Computational Linguistics.</cell><cell></cell><cell></cell></row><row><cell cols="2">Abhinav Rastogi, Xiaoxue Zang, Srinivas Sunkara,</cell><cell></cell></row><row><cell cols="2">Raghav Gupta, and Pranav Khaitan. 2020. To-</cell><cell></cell></row><row><cell cols="2">wards scalable multi-domain conversational agents:</cell><cell></cell></row><row><cell cols="2">The schema-guided dialogue dataset. Proceedings</cell><cell></cell></row><row><cell cols="2">of the AAAI Conference on Artificial Intelligence,</cell><cell>Ehsan Hosseini-Asl, Bryan McCann, Chien-Sheng Wu,</cell></row><row><cell>34(05):8689-8696.</cell><cell></cell><cell>Semih Yavuz, and Richard Socher. 2020. A sim-</cell></row><row><cell cols="2">Pawe? Budzianowski, Tsung-Hsien Wen, Bo-Hsiang Liliang Ren, Jianmo Ni, and Julian McAuley. 2019. Tseng, I?igo Casanueva, Stefan Ultes, Osman Ra-Scalable and accurate dialogue state tracking via hi-madan, and Milica Ga?i?. 2018. MultiWOZ -a erarchical sequence generation. In Proceedings of large-scale multi-domain Wizard-of-Oz dataset for the 2019 Conference on Empirical Methods in Nat-task-oriented dialogue modelling. In Proceedings of ural Language Processing and the 9th International the 2018 Conference on Empirical Methods in Nat-Joint Conference on Natural Language Processing ural Language Processing, pages 5016-5026, Brus-sels, Belgium. Association for Computational Lin-guistics. (EMNLP-IJCNLP), pages 1876-1885, Hong Kong, China. Association for Computational Linguistics.</cell><cell>ple language model for task-oriented dialogue. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Pro-cessing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual. Sungdong Kim, Sohee Yang, Gyuwan Kim, and Sang-Woo Lee. 2020. Efficient dialogue state tracking by selectively overwriting memory. In Proceedings of the 58th Annual Meeting of the Association for Com-</cell></row><row><cell cols="2">Liliang Ren, Kaige Xie, Lu Chen, and Kai Yu. 2018. Guan-Lin Chao and Ian Lane. 2019. BERT-DST: Towards universal dialogue state tracking. In Pro-Scalable End-to-End Dialogue State Tracking with Bidirectional Encoder Representations from Trans-former. In Proc. Interspeech 2019, pages 1468-1472. Tang. 2017. A survey on dialogue systems: Re-Hongshen Chen, Xiaorui Liu, Dawei Yin, and Jiliang ceedings of the 2018 Conference on Empirical Meth-ods in Natural Language Processing, pages 2780-2786, Brussels, Belgium. Association for Computa-tional Linguistics.</cell><cell>putational Linguistics, pages 567-582, Online. As-sociation for Computational Linguistics. Conference Track Proceedings. Diederik P. Kingma and Jimmy Ba. 2015. Adam: A method for stochastic optimization. In 3rd Inter-national Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015,</cell></row><row><cell cols="2">cent advances and new frontiers. Acm Sigkdd Ex-</cell><cell>Zhaojiang Lin, Andrea Madotto, Genta Indra Winata,</cell></row><row><cell cols="2">plorations Newsletter, 19(2):25-35.</cell><cell>and Pascale Fung. 2020. MinTL: Minimalist trans-</cell></row><row><cell cols="2">Li Dong, Nan Yang, Wenhui Wang, Furu Wei, Xi-aodong Liu, Yu Wang, Jianfeng Gao, Ming Zhou, and Hsiao-Wuen Hon. 2019. Unified language model pre-training for natural language understand-ing and generation. In Advances in Neural Infor-</cell><cell>fer learning for task-oriented dialogue systems. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 3391-3405, Online. Association for Computa-tional Linguistics.</cell></row><row><cell cols="2">mation Processing Systems 32: Annual Conference</cell><cell>Nikola Mrk?i?, Diarmuid ? S?aghdha, Tsung-Hsien</cell></row><row><cell cols="2">on Neural Information Processing Systems 2019,</cell><cell>Wen, Blaise Thomson, and Steve Young. 2017. Neu-</cell></row><row><cell cols="2">NeurIPS 2019, December 8-14, 2019, Vancouver,</cell><cell>ral belief tracker: Data-driven dialogue state track-</cell></row><row><cell cols="2">BC, Canada, pages 13042-13054.</cell><cell>ing. In Proceedings of the 55th Annual Meeting of</cell></row><row><cell cols="2">Mihail Eric, Rahul Goel, Shachi Paul, Abhishek Sethi, Sanchit Agarwal, Shuyag Gao, and Dilek Hakkani-Tur. 2019. Multiwoz 2.1: Multi-domain dialogue</cell><cell>the Association for Computational Linguistics (Vol-ume 1: Long Papers), pages 1777-1788, Vancouver, Canada. Association for Computational Linguistics.</cell></row><row><cell cols="2">state corrections and state tracking baselines. arXiv</cell><cell>Vahid Noroozi, Yang Zhang, Evelina Bakhturina, and</cell></row><row><cell>preprint arXiv:1907.01669.</cell><cell></cell><cell>Tomasz Kornuta. 2020. A fast and robust bert-based</cell></row><row><cell cols="2">Yue Feng, Yang Wang, and Hang Li. 2021. A sequence-to-sequence approach to dialogue state tracking. In</cell><cell>dialogue state tracker for schema-guided dialogue dataset. arXiv preprint arXiv:2008.12335.</cell></row><row><cell cols="2">Proceedings of the 59th Annual Meeting of the</cell><cell>Alec Radford, Jeffrey Wu, Rewon Child, David Luan,</cell></row><row><cell cols="2">Association for Computational Linguistics and the</cell><cell>Dario Amodei, and Ilya Sutskever. 2019. Language</cell></row><row><cell cols="2">11th International Joint Conference on Natural Lan-</cell><cell>models are unsupervised multitask learners. OpenAI</cell></row><row><cell cols="2">guage Processing (Volume 1: Long Papers), pages</cell><cell>blog, 1(8):9.</cell></row><row><cell cols="2">1714-1725, Online. Association for Computational</cell><cell></cell></row><row><cell>Linguistics.</cell><cell></cell><cell>Colin Raffel, Noam Shazeer, Adam Roberts, Kather-</cell></row><row><cell></cell><cell></cell><cell>ine Lee, Sharan Narang, Michael Matena, Yanqi</cell></row><row><cell cols="2">Donghoon Ham, Jeong-Gwan Lee, Youngsoo Jang,</cell><cell>Zhou, Wei Li, and Peter J. Liu. 2020. Exploring</cell></row><row><cell>and Kee-Eung Kim. 2020.</cell><cell>End-to-end neural</cell><cell>the limits of transfer learning with a unified text-to-</cell></row><row><cell cols="2">pipeline for goal-oriented dialogue systems using</cell><cell>text transformer. Journal of Machine Learning Re-</cell></row><row><cell cols="2">GPT-2. In Proceedings of the 58th Annual Meet-</cell><cell>search, 21(140):1-67.</cell></row><row><cell cols="2">ing of the Association for Computational Linguis-</cell><cell></cell></row><row><cell cols="2">tics, pages 583-592, Online. Association for Com-putational Linguistics.</cell><cell>Osman Ramadan, Pawe? Budzianowski, and Milica Ga?i?. 2018. Large-scale multi-domain belief track-ing with knowledge sharing. In Proceedings of the</cell></row><row><cell cols="2">Michael Heck, Carel van Niekerk, Nurul Lubis, Chris-</cell><cell>56th Annual Meeting of the Association for Com-</cell></row><row><cell cols="2">tian Geishauser, Hsien-Chin Lin, Marco Moresi, and</cell><cell>putational Linguistics (Volume 2: Short Papers),</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 9</head><label>9</label><figDesc></figDesc><table><row><cell>indicates the the amendable ability of the</cell></row><row><cell>AG-DST. On the test set of MultiWOZ 2.2, the</cell></row><row><cell>total number of improved examples for the three</cell></row><row><cell>error types is 313. AG-DST was able to amend the</cell></row><row><cell>slot value during the amending generation, when</cell></row><row><cell>the basic generation decoded nothing or a wrong</cell></row><row><cell>value.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 8 :</head><label>8</label><figDesc>The comparison of update gate accuracy</figDesc><table><row><cell>and corresponding value generation F1 between SOM-</cell></row><row><cell>DST (Kim et al., 2020) and our basic generation on</cell></row><row><cell>MultiWOZ 2.2.</cell></row><row><cell>C Attention Visualization of State</cell></row><row><cell>Operation Prediction</cell></row><row><cell>Figure 6 shows the attention visualization of differ-</cell></row><row><cell>ent state operations on test set of MultiWOZ 2.2.</cell></row><row><cell>From the update gate and dontcare gate attention</cell></row><row><cell>visualizations in</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head></head><label></label><figDesc>when the user only said "from the restaurant to the hotel", the model can extract the corresponding restaurant and hotel names from DS memory. This indicates that our model is able to handle the coreference operation. Most examples of this coreference phenomenon appear in taxi domain. The attention visualization of state operation on MultiWOZ 2.2. The attention visualization of delete gate and coreference phenomenon on MultiWOZ 2.2. Primitive Dialogue StateB t Amended Dialogue StateB t R t : no , i am sorry . i am searching for a 4 star hotel in the centre for 2 nights on saturday . is that correct ?U t : i need the hotel for 3 nights starting on saturday please . : i would like to make a reservation for saturday at 11:45 . and there has been a change in plans , i will be dining alone . : i want a place to go in the south , a swimmingpool . or another type of entertainment , if there is no pool ?R t : &lt;name/&gt; caffe uno &lt;/name&gt; is a very nice , expensive italian restaurant in the center of town . would you like a table : actually , i change my mind . i think i want to stick with british food after all . can you suggest any 1 thats in the centre of town ? R t : &lt;name/&gt; primavera &lt;/name&gt; is a museum in the center of town . their address is 10 king s parade . what else can i help with ? t : oh , i made a mistake . i really need that table for 16:15 , not 17:45 . can you change it , do you thing ?R t : there are 5 museums in the west . i recommend kettle's yard . would you like the address and phone ? : yes , i would love the address . thank you so much ! R t : you would like a second hotel , for which night and location ?</figDesc><table><row><cell>can R t : ok , and what day and time would you like that reservation ? you help me find a 4 star expensive hotel ? current dialogue &lt;hotel-pricerange&gt; expensive &lt;hotel-stars&gt; 4 (a) update gate &lt;hotel-parking&gt; &lt;nm&gt; &lt;hotel-pricerange&gt; &lt;nm&gt; &lt;hotel-stars&gt; &lt;nm&gt; &lt;hotel-internet&gt; 0.2 0.4 0.6 yes &lt;hotel-parking&gt; &lt;dc&gt; &lt;hotel-type&gt; gu est house i am looking for a place to stay in the south of town . it does not need to have free parking current dialogue (b) dontcare gate &lt;hotel-area&gt; &lt;nm&gt; &lt;hotel-name&gt; &lt;nm&gt; &lt;hotel-people&gt; &lt;nm&gt; &lt;hotel-stay&gt; &lt;nm&gt; &lt;hotel-day&gt; &lt;nm&gt; last dialogue state &lt;hotel-internet&gt; yes &lt;hotel-type&gt; gu est house 0.2 0.4 (c) carryover gate Figure 6: &lt;usr&gt; what other cu is ines are available for a moderate price range in the centre of town ? &lt;/utt&gt; &lt;accu_ds/&gt; &lt;attraction-type&gt; &lt;nm&gt; &lt;attraction-area&gt; &lt;nm&gt; &lt;attraction-name&gt; &lt;nm&gt; &lt;hotel-parking&gt; &lt;nm&gt; &lt;hotel-pricerange&gt; &lt;nm&gt; &lt;hotel-stars&gt; &lt;nm&gt; &lt;hotel-internet&gt; &lt;nm&gt; &lt;hotel-type&gt; &lt;nm&gt; &lt;hotel-area&gt; &lt;nm&gt; &lt;hotel-name&gt; &lt;nm&gt; &lt;hotel-people&gt; &lt;nm&gt; &lt;hotel-stay&gt; &lt;nm&gt; &lt;hotel-day&gt; &lt;nm&gt; &lt;restaurant-food&gt; v iet namese current dialogue and last dialogue state &lt;restaurant-food&gt; &lt;nm&gt; 0.025 0.050 0.075 (a) delete gate &lt;usr&gt; i will also need a taxi to go from the restaurant to the hotel , i want to leave the restaurant at 02 : 15 &lt;/utt&gt; &lt;accu_ds/&gt; &lt;attraction-type&gt; &lt;nm&gt; &lt;attraction-area&gt; &lt;nm&gt; &lt;attraction-name&gt; &lt;nm&gt; &lt;hotel-parking&gt; &lt;nm&gt; &lt;hotel-pricerange&gt; moderate &lt;hotel-stars&gt; 0 &lt;hotel-internet&gt; &lt;nm&gt; &lt;hotel-type&gt; &lt;nm&gt; &lt;hotel-area&gt; cent re &lt;hotel-name&gt; city room z &lt;hotel-people&gt; 3 &lt;hotel-stay&gt; 1 &lt;hotel-day&gt; s und ay &lt;restaurant-food&gt; &lt;dc&gt; &lt;restaurant-pricerange&gt; moderate &lt;restaurant-area&gt; cent re &lt;restaurant-name&gt; y ipp ee nood le bar &lt;restaurant-people&gt; &lt;nm&gt; current dialogue and last dialogue state &lt;taxi-departure&gt; y ipp ee nood le bar &lt;taxi-destination&gt; city room z 0.05 0.10 0.15 0.20 0.25 (b) coreference phenomenon Previous Dialogue Stat? B t?1 hotel-stay-2 hotel-stay-2 hotel-stay-3 restaurant-people-2 restaurant-people-2 restaurant-people-1 U R t : hello . restaurant-food-&lt;nm&gt; restaurant-food-indian restaurant-food-south indian 0.1 0.2 0.3 U t : i am looking for info on expensive south indian restaurant -s in cambridge . R t : ok , so you would like a taxi from the restaurant to the park ? could you please let me know your desired departure and arrival times ? taxi-departure-byard art taxi-departure-byard art taxi-departure-wandlebury coun-try park U t : i am sorry , i would like a taxi from wandlebury country park to taj tandoori . i would like the taxi to pick me up at 10:15 . R t : yes , &lt;name/&gt; wandlebury country park &lt;/name&gt; is in the south . in order to help you book a taxi between the park and your hotel , i need to know what hotel you are at . taxi-destination-kohinoor taxi-destination-kohinoor taxi-destination-wandlebury country park U t : i want a taxi from the restaurant that i am at R t : i am sorry , i am experiencing a system error . could you please restate your request ? attraction-type-swimmingpool attraction-type-entertainment attraction-type-swimmingpool U there ? restaurant-name-clowns restaurant-name-clowns restaurant-name-&lt;nm&gt; U restaurant-time-17:45 restaurant-time-17:45 restaurant-time-16:15 attraction-name-cambridge punter attraction-name-cambridge punter attraction-name-kettles yard Figure 7: Current Turn Dialogue U</cell></row></table><note>tttUt</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Unless otherwise specified in subsequent experiments, the pre-trained backbone we used is PLATO-2 as it carries out better results.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">PLATO-2: Towards building an open-domain chatbot via curriculum learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siqi</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huang</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenquan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhibin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinchao</forename><surname>Xu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.findings-acl.222</idno>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="2513" to="2525" />
		</imprint>
	</monogr>
	<note>Online</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-12-04" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A networkbased end-to-end trainable task-oriented dialogue system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Tsung-Hsien Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikola</forename><surname>Vandyke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milica</forename><surname>Mrk?i?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lina</forename><forename type="middle">M</forename><surname>Ga?i?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pei-Hao</forename><surname>Rojas-Barahona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Ultes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the 15th Conference of the European Chapter of the Association for Computational Linguistics<address><addrLine>Valencia, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="438" to="449" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Transferable multi-domain state generator for task-oriented dialogue systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chien-Sheng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Madotto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehsan</forename><surname>Hosseini-Asl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascale</forename><surname>Fung</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1078</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="808" to="819" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Ubar: Towards fully end-to-end task-oriented dialog system with gpt-2</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunyi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojun</forename><surname>Quan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="14230" to="14238" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">MultiWOZ 2.2 : A dialogue dataset with additional annotation corrections and state tracking baselines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxue</forename><surname>Zang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Rastogi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srinivas</forename><surname>Sunkara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raghav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianguo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jindong</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.nlp4convai-1.13</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd Workshop on Natural Language Processing for Conversational AI</title>
		<meeting>the 2nd Workshop on Natural Language Processing for Conversational AI</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="109" to="117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Multi-domain dialogue state tracking-a purely transformer-based generative approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian-Yun</forename><surname>Nie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.14061</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Find or classify? dual strategy for slot-value predictions on multi-domain dialog state tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianguo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuma</forename><surname>Hashimoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chien-Sheng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Ninth Joint Conference on Lexical and Computational Semantics</title>
		<meeting>the Ninth Joint Conference on Lexical and Computational Semantics<address><addrLine>Barcelona, Spain (Online</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="154" to="167" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Global-locally self-attentive encoder for dialogue state tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P18-1135</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1458" to="1467" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Efficient context and schema fusion networks for multidomain dialogue state tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Su</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jieyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Yu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.findings-emnlp.68</idno>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: EMNLP 2020</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="766" to="781" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
