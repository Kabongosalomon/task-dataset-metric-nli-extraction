<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">KonIQ-10K: TOWARDS AN ECOLOGICALLY VALID AND LARGE-SCALE IQA DATABASE</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanhe</forename><surname>Lin</surname></persName>
							<email>hanhe.lin@uni-konstanz.de</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer and Information Science</orgName>
								<orgName type="institution">University of Konstanz</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vlad</forename><surname>Hosu</surname></persName>
							<email>vlad.hosu@uni-konstanz.de</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer and Information Science</orgName>
								<orgName type="institution">University of Konstanz</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dietmar</forename><surname>Saupe</surname></persName>
							<email>dietmar.saupe@uni-konstanz.de</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer and Information Science</orgName>
								<orgName type="institution">University of Konstanz</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">KonIQ-10K: TOWARDS AN ECOLOGICALLY VALID AND LARGE-SCALE IQA DATABASE</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T19:39+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Image database</term>
					<term>image quality assess- ment</term>
					<term>diversity sampling</term>
					<term>crowdsourcing</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The main challenge in applying state-of-the-art deep learning methods to predict image quality in-the-wild is the relatively small size of existing quality scored datasets. The reason for the lack of larger datasets is the massive resources required in generating diverse and publishable content. We present a new systematic and scalable approach to create large-scale, authentic and diverse image datasets for Image Quality Assessment (IQA). We show how we built an IQA database, KonIQ-10k 1 , consisting of 10,073 images, on which we performed very large scale crowdsourcing experiments in order to obtain reliable quality ratings from 1,467 crowd workers (1.2 million ratings). We argue for its ecological validity by analyzing the diversity of the dataset, by comparing it to stateof-the-art IQA databases, and by checking the reliability of our user studies.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Objective Image Quality Assessment (IQA) is important in a broad range of applications, from image compression to display technology and more. To further develop and evaluate objective IQA methods, in particular deep learning methods, large and diverse IQA databases are needed. "In research, the ecological validity of a study means that the methods, materials and setting of the study must approximate the real-world that is being examined" (Wikipedia). The ecological validity of an IQA database refers to the representativeness of the image collection for the wide range of public Internet photos.</p><p>Conventionally, creating an IQA database has followed the same typical procedure: collect pristine images and artificially degrade them. Next ask a few volunteers, usually students or naive participants, to assess the quality of the distorted images. The first drawback of the approach is that the diversity of image content is limited since all the distorted images are degraded from a small set of pristine images. Second, the distortions are applied in very limited combinations, *Hanhe Lin and Vlad Hosu contributed equally. <ref type="bibr" target="#b0">1</ref> Database is available at http://database.mmsp-kn. <ref type="bibr">de.</ref> whereas ecologically valid distortions are caused by combinations of distortions, also of types that may differ from those in the databases. Last, but not the least, the conventional approach for creating IQA datasets results in small databases, since assessing the quality of a large number of images in a lab setting is too costly.</p><p>To address these limitations, we have designed a scalable approach that allowed us to create the largest IQA database to date (images and subjective scores). It consists of 10,073 images that were selected from around 10 million YFCC100M <ref type="bibr" target="#b0">[1]</ref> entries. To ensure the diversity in content and distortions, our sampling algorithm makes use of seven quality indicators (sharpness, colorfulness, ...) and one content indicator (deep features). For each image, 120 reliable quality ratings were obtained by crowdsourcing, performed by 1,467 crowd workers. In comparison to existing IQA databases, ours contains a vastly larger number of images, with a much broader content diversity and authentic distortions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">RELATED WORK</head><p>A number of IQA databases have been released in recent years, aiming to help the development and evaluation of objective IQA methods, see <ref type="table" target="#tab_0">Table 1</ref>.</p><p>An early conventionally build IQA database, IVC <ref type="bibr" target="#b1">[2]</ref>, was released in 2005. LIVE <ref type="bibr" target="#b2">[3]</ref>, TID2008 <ref type="bibr" target="#b3">[4]</ref>, and CSIQ <ref type="bibr" target="#b4">[5]</ref> are the most common databases that researchers use to develop, improve, and evaluate their objective IQA methods. TID2008 was further extended to TID2013 <ref type="bibr" target="#b5">[6]</ref> by including seven more distortion types. The aforementioned databases, are all smallscale, contain limited content types, and consider few types of artificial distortions.</p><p>Virtanen et al. <ref type="bibr" target="#b6">[7]</ref> were first to introduce more authentic distortions, created from 480 images of 8 different scenes captured by 79 different cameras. However, the creation method is time-consuming and expensive and thus impractical for large-scale databases. Ghadiyaram et al. <ref type="bibr" target="#b7">[8]</ref> asked a few photographers to capture 1,162 images by a variety of mobile device cameras. Their visual quality was assessed by crowdsourcing experiments. Although this method provides an alternative way to reduce time and cost for IQA subjective study, the database size as well as the content diversity are still relatively low. Ma et al. <ref type="bibr" target="#b8">[9]</ref> created a database with 4,744 pristine images and 94,880 distorted images to validate their proposed mechanism called group MAximum Differentiation (gMAD) competition. Their database is meant to provide an alternative evaluation for the performance of IQA models, by means of paired comparisons. Although the Waterloo Exploration database is the largest available in the field, its images are artificially distorted, thus non-authentic, and due to the lack of subjective ratings it cannot be used for developing new IQA methods that rely on them.</p><p>In comparison to lab-based studies which are timeconsuming and expensive, crowdsourcing has been successfully employed to conduct Quality of Experience (QoE) assessment for images <ref type="bibr" target="#b7">[8]</ref> and videos <ref type="bibr" target="#b9">[10]</ref>. Although it has been believed that data collected by crowdsourcing is less reliable, Redi et al. <ref type="bibr" target="#b10">[11]</ref> verified that crowd workers can generate reliable results under certain experimental setups.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">DATABASE CREATION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Overview</head><p>We started from a large public multimedia database, YFCC-100m <ref type="bibr" target="#b0">[1]</ref>, from which we randomly selected approximately 10 million (9,974,030) image records. Then, we filtered them down in two stages to the final database of 10,073 images.</p><p>In the first stage we selected images with an appropriate Creative Commons (CC) license that allows editing and redistribution, and chose those with available machine tags (from YFCC100m) and a resolution between 960 ? 540 and 6000?6000. From this set of 4,807,816 images, we proposed a new tag-based sampling procedure that was used to select one million images such that their machine tag distribution covers the larger set well, see <ref type="figure">Fig. 1</ref>.</p><p>In the second stage, all images in the set of one million, that were larger than 1024 ? 768 were downloaded and rescaled to 1024 ? 768 pixels, while cropping was applied to maintain the pixel aspect ratio. In order to keep faces in the frame, as well as salient parts of the image we designed our own cropping method. It relied on the Viola-Jones face detector and the saliency method of Hou et al. <ref type="bibr" target="#b11">[12]</ref>. 13,000 images were then sampled while enforcing a uniform distribu- <ref type="figure">Fig. 1</ref>. Sampling 1.0 from 4.8 million images. The tags were sorted according to increasing frequency in the pre-sample set (red). The two histograms start diverging at the minimum quota Q = 4000 images per tag. Ideally, the rest of the (green) histogram should be flat, however this is not achieved as an image can have multiple tags. tion across eight image indicators. Duplicates were removed, using a sampling strategy that accounts for content and indicators. This collection was manually filtered for inappropriate content resulting in our KIQ-10 dataset of 10,073 images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Initial tag-based content sampling</head><p>Downloading 4.8 million images consumes much bandwidth and storage space. Hence, we devise a way to subset 1 million images such that not to reduce their content diversity. We aim at full coverage of content, i.e., having at least one image for each of the 1,570 different machine tags available. To assure a "uniform coverage, our sample should provide a similar number of images for each tag. This is generally not precisely possible as images have more than one tag (9.2 on average). Therefore, we devised a simple and computationally efficient sampling heuristic, with the above objectives in mind.</p><p>Considering the scale of the problem, we propose a computationally efficient method to find an approximate solution. Let ?(t, S O ) be the number of images that contain tag t in the set S O of 4.8 million. We choose a tag quota Q such that all images that contain a tag t with ?(t, S O ) &lt; Q are added to the sampled set S S . Let T (S) be the set of tags in a set of images S. For remaining tags T R = T (S O )\T (S S ), we include images in S S such that at least each tag's quota Q is reached. This procedure is as follows. For each tag t ? T R , in order of increasing counts ?(t, S O ), we generate an ordered </p><formula xml:id="formula_0">list of candidate images, O(S O \S S , K t ),</formula><p>where the list of images is sorted in decreasing order of K t , the machine confidence in the presence of the tag t ? T R , which is provided by YFCC100m. Then we add the top</p><formula xml:id="formula_1">Q ? ?(t, S S ) images from O(S O \S S , K t ) to S S .</formula><p>To assure that |S S | ? 1, 000, 000, one can apply the bisection method to choose the tag quota Q. We ran the above algorithm with Q = 4000 and stopped adding images to S S when |S S | = 1, 000, 000.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Uniform sampling</head><p>To ensure the content diversity and distortion authenticity, we sampled a subset of images while enforcing the uniform distribution across a number of indicators that have impact on image quality and content diversity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1.">Indicator selection</head><p>We collected a number of image quality indicators relating to brightness, colorfulness, contrast, noise, sharpness, and No-Reference (NR) IQA measures. Each indicator has at least one implementation. Since we have about 1 million images to evaluate, we dis-considered slow implementations. For the rest of the measures, we conducted preliminary subjective studies and kept four measures that are well correlated with human perception, namely brightness, colorfulness <ref type="bibr" target="#b12">[13]</ref>, Root Mean Square (RMS) contrast, and sharpness <ref type="bibr" target="#b13">[14]</ref>. Besides these, we considered three other indicators: image bitrate, resolution (height?width), and JPEG compression quality; these are highly correlated with image quality. Until this point, we had ensured content diversity by sampling 1 million images based on pre-existing machine tags from YFCC100m. These had been assigned using an existing deep architecture for classification, and represent a few most likely categories per image. To further improve the content description, we rely on the more comprehensive 4096dimensional deep features extracted by the pretrained VGG-16 model (FC7) <ref type="bibr" target="#b14">[15]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2.">Sampling strategy</head><p>Each quality indicator identifies an image attribute, measuring its magnitude or presence as a scalar value. Extreme values for an indicator relate to severe distortion, either due to the absence or abnormal emphasis on that particular aspect. If we were to randomly sample our image database, it is unlikely that images having "abnormal" attribute values would be selected. Therefore, we employed a sampling strategy which generates more images with a wide range of indicator values, and thus more distortions and content types.</p><p>Nonetheless, the absolute extremes of the indicator ranges are distorted to an excessive degree, not being informative, e.g., overly dark or bright, overly colorful, etc. Before performing the sampling procedure, we therefore trimmed the extreme ends of each indicator distribution by removing all images with an absolute z-scored indicator value greater than 3. The dataset size shrank from 1 million to 866,976.</p><p>For the actual sampling, we applied the method proposed by Vonikakis et al. <ref type="bibr" target="#b15">[16]</ref>, enforcing a uniform target distribution for each indicator. The method quantizes each indicator value into N bins. The sampling procedure jointly optimizes the shape of the histograms along all indicator dimensions, using Mixed Integer Linear Programming (MILP).</p><p>We used N = 200 bins for all seven scalar indicators. Since the deep features are 4096-dimensional vectors, we applied a bag-of-words model to quantize them. That is, we ran k-means to compute 200 centroids, mapping each deep feature to the nearest cluster. We ran the sampling procedure generating 13,000 images, with uniformly sampled indicators. The set is larger than the target of 10,000 to allow for removing duplicates and other post-filtering.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Removal of duplicates and inappropriate content</head><p>The uniform sampling as described ensures the diversity of the image database at a broad scale. However, due to the binning procedure, identical copies or near-duplicate images can be sampled together, e.g., photos of a scene taken from slightly different points of view.</p><p>We devise a way to remove near-duplicates. First, the values of each indicator were remapped to the interval [0, 1]. We computed all pair-wise euclidean distances D(i, j) between images i, j from the source dataset in the 8-dimensional indicator plus content space. The distance in the content space is 0 if two images are part of the same cluster, and 1 otherwise. Duplicate and near-duplicate images i, j are expected to correspond to small distances D(i, j). Thus, by iteratively removing a member of the closest pair, we can effectively remove near-duplicates. We removed 2,000 images in this way.</p><p>To ensure the quality of our database we manually removed images showing too little content, namely text screen shots, text scans, heavily under-exposed, or inappropriate im-ages showing mature content. At the end 10,073 images remained which make up our KonIQ-10k database, see <ref type="figure" target="#fig_0">Fig. 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">SUBJECTIVE IMAGE QUALITY ASSESSMENT</head><p>In order to assess the visual quality of the 10,073 selected images we performed a large scale crowdsourcing experiment on CrowdFlower.com. The experiment first presented workers with a set of instructions, including the definition of technical image quality, considerations when giving ratings, examples of often encountered distortion types, and images with different ratings. The subjects were instructed to consider the following types of degradations: noise, JPEG artifacts, aliasing, lens and motion blur, over-sharpening, wrong exposure, color fringing, and over-saturation. We used a standard 5-point Absolute Category Rating (ACR) scale, i.e., bad (1), poor, fair, good, and excellent <ref type="bibr" target="#b4">(5)</ref>. Before starting the actual experiment, workers would take a quiz, all questions of which had labeled answers (known as test questions). Only those with an accuracy surpassing 70% were eligible to continue. Hidden test questions were presented throughout the rest of the experiment, to encourage contributors to always pay full attention.</p><p>The opinions of domain experts are generally more reliable, and thus provide a good source of information for setting test questions. We involved 11 freelance photographers, who had on average more than 3 years of professional experience. We asked them to rate the quality of 240 images: 29 were pristine high quality images, carefully selected beforehand, 21 were artificially degraded using 12 types of distortions and the remaining 190 images were randomly selected from Flickr (not part of our 10k dataset). The distortions included blur, artifacts, contrast, and color degradation. Based on this set of images and the mean opinion score from the freelancers, we generated test questions for our crowdsourcing experiment. The correct answers were based on the rounded values of the freelancers' MOS ? one standard deviation. All images had at most three valid answer choices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">RESULTS AND ANALYSIS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Diversity analysis</head><p>We selected LIVE In the Wild and TID2013 to compare their diversity with KonIQ-10k in some aspects. Here LIVE In the Wild and TID2013 are the most representative authentic distorted and artificial distorted databases, respectively. Their distributions in brightness, colorfulness, contrast, and sharpness are depicted in <ref type="figure" target="#fig_1">Fig. 3(a)-(d)</ref>, respectively. Obviously, KonIQ-10k features more diversity in each of those indicators. To compare the content diversity, we embedded the 4,096-dimensional VGG-16 deep features from the databases into a 2D subspace by t-SNE <ref type="bibr" target="#b16">[17]</ref>. The visualization is shown in <ref type="figure" target="#fig_1">Fig. 3(f)-(h)</ref>. Clearly, since LIVE In the Wild images were captured by a few photographers, their content only covered a small region of KonIQ-10k, not to mention TID 2013, generated from only 25 reference images. Their MOS distributions are illustrated in <ref type="figure" target="#fig_1">Fig. 3</ref>(e) after rescaling to 1-100 range.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Crowdsourcing experiment</head><p>The experiment took more than two weeks to complete. Of 2,302 crowd workers taking the quiz, 1,749 passed it (76%). For those who passed the quiz and started work, 6% (101 contributors) failed to meet the 70% pass rate on test questions during work. This indicates the quality of our test questions, which were effectively filtering unqualified workers. As a result, to annotate the entire database of 10,073 images, with at least 120 scores each, more than 1.2 million trusted judgments were submitted (over 70% accuracy).</p><p>In <ref type="bibr" target="#b17">[18]</ref>, the authors have shown that screening users based on image quality test questions improves the intra-class correlation coefficient (ICC), leading to an increased reliability. They have found an improvement from an ICC of 0.37 before screening to 0.5 when users are screened on 70% accuracy on quality based test questions. The approach in our paper has a similar effect, leading to an ICC of 0.46 on the entire database.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Reliability of the crowd</head><p>In order to better study the reliability of the crowd data, we screened workers for unwanted behavior. First, we removed those that had a low agreement with the global mean opinion scores (MOS). Workers that had a PLCC of their votes and the crowd MOS lower than 0.5 (68 users) were removed.</p><p>Second, we detected line-clickers, workers that answered the same too often. We computed the scores' counts of each worker, for all five answer choices. We then took the ratio between the maximum count, and the sum of the four lower counts. Workers with a ratio larger than 2.0 were removed (121 workers). The MOS for all images was recomputed, after mapping the individual worker scores to <ref type="bibr" target="#b0">[1,</ref><ref type="bibr">100]</ref>. To check the reliability of the crowd MOS, we compare them with those obtained from a group of 11 experts. We have 187 images which have each been rated by 11 experts and at least 592 crowd workers. We compensate for difference in the range of the MOS between the two data sources by fitting a linear model: M OS experts = 1.12 ? M OS crowd ? 10.43. Relying on this model, the crowd MOS is re-mapped such that relative errors are more indicative of actual performance, and are less affected by changes in scale.  We compare the two data sources: experts and crowd. To do so, we calculate the relative errors between bootstrapped groups of users, by sampling with replacement. We compare the MOS of bootstrapped expert groups of size 11 against the MOS of all 11 experts, and differently sized groups of crowd workers against the MOS of all 11 experts. The crowd sample size varies beyond 120, which is the minimum number of votes we have collected for each of the 10,073 images in our database. In <ref type="figure" target="#fig_2">Fig. 4 (a)</ref> we show that with respect to errors, crowd workers converge to an agreed MOS quickly (around 30 participants). The crowd opinion is slightly different from that of the expert group, with an RMSE of 11.35 on a 100 point scale. The bootstrapped standard deviation of the experts is 6.63, meaning they also exhibit some inherent disagreement.</p><p>In <ref type="figure" target="#fig_2">Fig. 4 (b)</ref>, we point to the source of the errors by showing their distribution over all 187 images. We note that for a large number of images the errors are within ?2 standard deviations of the experts' MOS (95% confidence interval). A crowd MOS value that falls within this interval is likely to have been a result of the votes of 11 experts. We have that 137 of 187 (73%) images are sufficiently well rated by the crowd so that they can be confused with the ratings of experts. The crowd MOS on the remaining 50 images diverges more from the experts. A preliminary inspection shows that many of the items that have been rated lower by the crowd in comparison to the experts, represent shallow depth of field images (11 of 27). Crowd workers consider the large amount of blur an important degradation, whereas professional photographers understand it as an artistic effect, which doesn't reduce the quality as much. The observed disagreement is at least in part a consequence of diverging domain knowledge between the expert (freelancers) and novice (crowd) groups. Thus, we cannot conclude that the errors, however small, are an indicator for a lower reliability of the crowd.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">NR-IQA evaluation</head><p>We have compared five state-of-the-art NR-IQA <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23]</ref> methods on KonIQ-10k, LIVE In the Wild, and TID2013. We cross-validated a Support Vector Regression model (RBF kernel) on each database using 80% training / 20% test set, with 100 repetitions. The average Spearman Rank Order Correlation Coefficient (SROCC) and Pearson Linear Correlation Coefficient (PLCC) are reported in <ref type="table" target="#tab_1">Table 2</ref>. We observe a wide gap in performance between the two naturally distorted datasets (KonIQ-10k and LIVE In the Wild). An experiment w.r.t. size of the database showed that size matters, meaning that larger training sets improve quality predictions which explains the better performance on KonIQ-10k.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">SUMMARY</head><p>We proposed a new systematic and scalable approach to create an ecologically valid IQA database, KonIQ-10k. To ensure the diversity in content and quality factors, 10,073 images were sampled from around 4.8 million YFCC100m images by enforcing a roughly uniform distribution across seven quality indicators, one content indicator and machine tags. Experimental analysis demonstrated KonIQ-10k is far more diverse than state-of-the-art databases. For each image 120 quality ratings were obtained via crowdsourcing performed by a total of 1,467 crowd workers. We established the quality of the scoring procedure and the reliability of our results with respect to expert ratings. For a more detailed study on this issue see <ref type="bibr" target="#b17">[18]</ref>. Blind IQA is still a challenging task, especially for natural, not artificially distorted images, which calls for IQA databases with natural images like ours. We hope our approach will enable the scientific community to design better and larger databases in the future. Moreover, our dataset KonIQ-10k already has facilitated the design of new blind IQA methods using deep learning <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b24">25]</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>Indicator distributions in 866,976 YFCC100m images (blue curves) and sampled 10,073 images (red curves). While the original distributions are far from uniform, the sampling procedure enforces a more uniform distribution on each indicator.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Diversity comparison between TID2013, Live In the Wild, and KonIQ-10k. (a) -(d) distribution comparison in brightness, colorfulness, contrast, and sharpness, respectively. (f) -(h) deep feature embedding in 2D via t-SNE. (e) MOS distribution. (a) average errors (b) per image errors</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>(a) Top red line: bootstrapped RMSE of crowd MOS against 11 experts MOS; Bottom blue line: bootstrapped standard deviation of MOS of 11 experts; gray ribbon is the 95% CI of the RMSE. (b) Distributions of errors of crowd MOS against experts' MOS, expressed in multiples of the standard deviation of the bootstrapped MOS of 11 experts.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Comparison of existing IQA databases with KonIQ-10k.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>No. of</cell><cell></cell><cell>No. of</cell><cell>No. of</cell><cell>Ratings</cell><cell></cell></row><row><cell>Database</cell><cell>Year</cell><cell cols="6">Content distorted images Distortion type distortion types rated images per image</cell><cell>Environment</cell></row><row><cell>IVC [2]</cell><cell>2005</cell><cell>10</cell><cell>185</cell><cell>artificial</cell><cell>4</cell><cell>185</cell><cell>15</cell><cell>lab</cell></row><row><cell>LIVE [3]</cell><cell>2006</cell><cell>29</cell><cell>779</cell><cell>artificial</cell><cell>5</cell><cell>779</cell><cell>23</cell><cell>lab</cell></row><row><cell>TID2008 [4]</cell><cell>2009</cell><cell>25</cell><cell>1,700</cell><cell>artificial</cell><cell>17</cell><cell>1,700</cell><cell>33</cell><cell>lab</cell></row><row><cell>CSIQ [5]</cell><cell>2009</cell><cell>30</cell><cell>866</cell><cell>artificial</cell><cell>6</cell><cell>866</cell><cell>5?7</cell><cell>lab</cell></row><row><cell>TID2013 [6]</cell><cell>2013</cell><cell>25</cell><cell>3,000</cell><cell>artificial</cell><cell>24</cell><cell>3,000</cell><cell>9</cell><cell>lab</cell></row><row><cell>CID2013 [7]</cell><cell>2013</cell><cell>8</cell><cell>474</cell><cell>authentic</cell><cell>12?14</cell><cell>480</cell><cell>31</cell><cell>lab</cell></row><row><cell>LIVE In the Wild [8]</cell><cell>2016</cell><cell>1,169</cell><cell>1,169</cell><cell>authentic</cell><cell>N/A</cell><cell>1,169</cell><cell cols="2">175 crowdsourcing</cell></row><row><cell cols="2">Waterloo Exploration [9] 2016</cell><cell>4,744</cell><cell>94,880</cell><cell>artificial</cell><cell>4</cell><cell>0</cell><cell>0</cell><cell>lab</cell></row><row><cell>KonIQ-10k</cell><cell>2017</cell><cell>10,073</cell><cell>10,073</cell><cell>authentic</cell><cell>N/A</cell><cell>10,073</cell><cell cols="2">120 crowdsourcing</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Performance of IQA methods.</figDesc><table><row><cell></cell><cell cols="2">KonIQ-10k</cell><cell cols="2">LIVE In the Wild</cell><cell cols="2">TID2013</cell></row><row><cell></cell><cell cols="6">SROCC PLCC SROCC PLCC SROCC PLCC</cell></row><row><cell>BIQI</cell><cell>0.545</cell><cell>0.619</cell><cell>0.291</cell><cell>0.388</cell><cell>0.346</cell><cell>0.422</cell></row><row><cell>BLIINDS-II</cell><cell>0.575</cell><cell>0.583</cell><cell>0.447</cell><cell>0.483</cell><cell>0.529</cell><cell>0.615</cell></row><row><cell>BRISQUE</cell><cell>0.700</cell><cell>0.704</cell><cell>0.597</cell><cell>0.630</cell><cell>0.473</cell><cell>0.537</cell></row><row><cell>DIIVINE</cell><cell>0.585</cell><cell>0.622</cell><cell>0.430</cell><cell>0.468</cell><cell>0.513</cell><cell>0.605</cell></row><row><cell>SSEQ</cell><cell>0.596</cell><cell>0.615</cell><cell>0.456</cell><cell>0.500</cell><cell>0.510</cell><cell>0.578</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgment</head><p>The authors would like to thank the German Research Foundation (DFG) for financial support within project A05 of SFB/Transregio 161.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Yfcc100m: The new data in multimedia research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Thomee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerald</forename><surname>Shamma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Friedland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Elizalde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douglas</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Damian</forename><surname>Poland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Borth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="64" to="73" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Subjective quality assessment irccyn/ivc database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Callet</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florent</forename><surname>Autrusseau</surname></persName>
		</author>
		<ptr target="http://www.irccyn.ec-nantes.fr/ivcdb/" />
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A statistical evaluation of recent full reference image quality assessment algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hamid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Muhammad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">C</forename><surname>Sabir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="3440" to="3451" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Tid2008-a database for evaluation of fullreference visual quality assessment metrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolay</forename><surname>Ponomarenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Lukin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Zelensky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Egiazarian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Carli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federica</forename><surname>Battisti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. of Modern Radioelectr</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="30" to="45" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Most apparent distortion: full-reference image quality assessment and the role of strategy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Eric</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Damon M</forename><surname>Larson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chandler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Electronic Imaging</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="11006" to="011006" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Image database tid2013: Peculiarities, results and perspectives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolay</forename><surname>Ponomarenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lina</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleg</forename><surname>Ieremeiev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Lukin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Egiazarian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaakko</forename><surname>Astola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benoit</forename><surname>Vozel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kacem</forename><surname>Chehdi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Carli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federica</forename><surname>Battisti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Signal Processing: Image Communication</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="57" to="77" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Cid2013: A database for evaluating no-reference image quality assessment algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toni</forename><surname>Virtanen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikko</forename><surname>Nuutinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikko</forename><surname>Vaahteranoksa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pirkko</forename><surname>Oittinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jukka</forename><surname>H?kkinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="390" to="402" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Massive online crowdsourced study of subjective and objective picture quality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepti</forename><surname>Ghadiyaram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="372" to="387" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Group mad competition-a new methodology to compare objective image quality models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kede</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingbo</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengfang</forename><surname>Duanmu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongwei</forename><surname>Yong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongliang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conf. on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conf. on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1664" to="1673" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The konstanz natural video database (konvid-1k)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vlad</forename><surname>Hosu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franz</forename><surname>Hahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohsen</forename><surname>Jenadeleh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanhe</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Men</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tam?s</forename><surname>Szir?nyi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shujun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dietmar</forename><surname>Saupe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intern. Conf. on Quality of Multimedia Experience</title>
		<imprint>
			<publisher>QoMEX</publisher>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A reliable methodology to collect ground truth data of image aesthetic appeal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ernestasia</forename><surname>Siahaan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Hanjalic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judith</forename><surname>Redi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1338" to="1350" />
			<date type="published" when="2016-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Image signature: Highlighting sparse salient regions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodi</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Harel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christof</forename><surname>Koch</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="194" to="201" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Measuring colorfulness in natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Hasler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sabine</forename><forename type="middle">E</forename><surname>Suesstrunk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Electronic Imaging. Intern. Society for Optics and Photonics</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="87" to="95" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A fast waveletbased algorithm for global and local image sharpness estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Phong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Damon M</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chandler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="423" to="426" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno>abs/1409.1556</idno>
	</analytic>
	<monogr>
		<title level="j">Computing Research Repository (CoRR)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Shaping datasets: Optimal data selection for specific target distributions across dimensions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vassilios</forename><surname>Vonikakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramanathan</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Winkler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Intern. Conf. on Image Processing (ICIP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3753" to="3757" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Expertise screening in crowdsourcing image quality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vlad</forename><surname>Hosu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanhe</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dietmar</forename><surname>Saupe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">QoMEX 2018: Tenth International Conference on Quality of Multimedia Experience</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A two-step framework for constructing blind image quality indices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krishna</forename><surname>Anush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">Conrad</forename><surname>Moorthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="513" to="516" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Blind image quality assessment: A natural scene statistics approach in the dct domain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Michele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">C</forename><surname>Saad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christophe</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Charrier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="3339" to="3352" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">No-reference image quality assessment in the spatial domain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anish</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krishna</forename><surname>Anush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">Conrad</forename><surname>Moorthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="4695" to="4708" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Blind image quality assessment: From natural scene statistics to perceptual quality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krishna</forename><surname>Anush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">Conrad</forename><surname>Moorthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="3350" to="3364" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">No-reference image quality assessment based on spatial and spectral entropies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lixiong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">Conrad</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Signal Processing: Image Communication</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="856" to="863" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">DeepRN: A content preserving deep architecture for blind image quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Domonkos</forename><surname>Varga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamas</forename><surname>Szir?nyi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dietmar</forename><surname>Saupe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Multimedia and Expo (ICME)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Disregarding the big picture: Towards local image quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Wiedemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vlad</forename><surname>Hosu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanhe</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dietmar</forename><surname>Saupe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">QoMEX 2018: Tenth International Conference on Quality of Multimedia Experience</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
