<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Maximum Likelihood Training of Score-Based Diffusion Models</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Songc</surname></persName>
							<email>yangsong@cs.stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="department">omputer Science Department</orgName>
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Conor</forename><surname>Durkans</surname></persName>
							<email>conor.durkan@ed.ac.uk</email>
							<affiliation key="aff1">
								<orgName type="department">chool of Informatics</orgName>
								<orgName type="institution">University of Edinburgh</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iain</forename><surname>Murray</surname></persName>
							<email>i.murray@ed.ac.uk</email>
							<affiliation key="aff2">
								<orgName type="department">School of Informatics</orgName>
								<orgName type="institution">University of Edinburgh</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
							<email>ermon@cs.stanford.edu</email>
							<affiliation key="aff3">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Maximum Likelihood Training of Score-Based Diffusion Models</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T03:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Score-based diffusion models synthesize samples by reversing a stochastic process that diffuses data to noise, and are trained by minimizing a weighted combination of score matching losses. The log-likelihood of score-based diffusion models can be tractably computed through a connection to continuous normalizing flows, but log-likelihood is not directly optimized by the weighted combination of score matching losses. We show that for a specific weighting scheme, the objective upper bounds the negative log-likelihood, thus enabling approximate maximum likelihood training of score-based diffusion models. We empirically observe that maximum likelihood training consistently improves the likelihood of score-based diffusion models across multiple datasets, stochastic processes, and model architectures. Our best models achieve negative log-likelihoods of 2.83 and 3.76 bits/dim on CIFAR-10 and ImageNet 32?32 without any data augmentation, on a par with state-of-the-art autoregressive models on these tasks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Score-based generative models <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b47">48]</ref> and diffusion probabilistic models <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b18">19]</ref> have recently achieved state-of-the-art sample quality in a number of tasks, including image generation <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b10">11]</ref>, audio synthesis <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b36">37]</ref>, and shape generation <ref type="bibr" target="#b2">[3]</ref>. Both families of models perturb data with a sequence of noise distributions, and generate samples by learning to reverse this path from noise to data. Through stochastic calculus, these approaches can be unified into a single framework <ref type="bibr" target="#b47">[48]</ref> which we refer to as score-based diffusion models in this paper.</p><p>The framework of score-based diffusion models <ref type="bibr" target="#b47">[48]</ref> involves gradually diffusing the data distribution towards a given noise distribution using a stochastic differential equation (SDE), and learning the time reversal of this SDE for sample generation. Crucially, the reverse-time SDE has a closed-form expression which depends solely on a time-dependent gradient field (a.k.a., score) of the perturbed data distribution. This gradient field can be efficiently estimated by training a neural network (called a score-based model <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b44">45]</ref>) with a weighted combination of score matching losses <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b45">46]</ref> as the objective. A key advantage of score-based diffusion models is that they can be transformed into continuous normalizing flows (CNFs) <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b14">15]</ref>, thus allowing tractable likelihood computation with numerical ODE solvers. <ref type="bibr">Equal</ref>  Compared to vanilla CNFs, score-based diffusion models are much more efficient to train. This is because the maximum likelihood objective for training CNFs requires running an expensive ODE solver for every optimization step, while the weighted combination of score matching losses for training score-based models does not. However, unlike maximum likelihood training, minimizing a combination of score matching losses does not necessarily lead to better likelihood values. Since better likelihoods are useful for applications including compression <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b50">51]</ref>, semi-supervised learning <ref type="bibr" target="#b9">[10]</ref>, adversarial purification <ref type="bibr" target="#b46">[47]</ref>, and comparing against likelihood-based generative models, we seek a training objective for score-based diffusion models that is as efficient as score matching but also promotes higher likelihoods.</p><p>We show that such an objective can be readily obtained through slight modification of the weighted combination of score matching losses. Our theory reveals that with a specific choice of weighting, which we term the likelihood weighting, the combination of score matching losses actually upper bounds the negative log-likelihood. We further prove that this upper bound becomes tight when our score-based model corresponds to the true time-dependent gradient field of a certain reverse-time SDE. Using likelihood weighting increases the variance of our objective, which we counteract by introducing a variance reduction technique based on importance sampling. Our bound is analogous to the classic evidence lower bound used for training latent-variable models in the variational autoencoding framework <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b38">39]</ref>, and can be viewed as a continuous-time generalization of <ref type="bibr" target="#b42">[43]</ref>.</p><p>With our likelihood weighting, we can minimize the weighted combination of score matching losses for approximate maximum likelihood training of score-based diffusion models. Compared to weightings in previous work <ref type="bibr" target="#b47">[48]</ref>, we consistently improve likelihood values across multiple datasets, model architectures, and SDEs, with only slight degradation of Fr?chet Inception distances <ref type="bibr" target="#b16">[17]</ref>. Moreover, our upper bound on negative log-likelihood allows training with variational dequantization <ref type="bibr" target="#b17">[18]</ref>, with which we reach negative log-likelihood of 2.83 bits/dim on CIFAR-10 <ref type="bibr" target="#b27">[28]</ref> and 3.76 bits/dim on ImageNet 32?32 <ref type="bibr" target="#b54">[55]</ref> with no data augmentation. Our models present the first instances of normalizing flows which achieve comparable likelihood to cutting-edge autoregressive models.</p><p>2 Score-based diffusion models Score-based diffusion models are deep generative models that smoothly transform data to noise with a diffusion process, and synthesize samples by learning and simulating the time reversal of this diffusion. The overall idea is illustrated in <ref type="figure" target="#fig_7">Fig. 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Diffusing data to noise with an SDE</head><p>Let ppxq denote the unknown distribution of a dataset consisting of D-dimensional i.i.d. samples. Score-based diffusion models <ref type="bibr" target="#b47">[48]</ref> employ a stochastic differential equation (SDE) to diffuse ppxq towards a noise distribution. The SDEs are of the form dx " f px, tq dt`gptq dw,</p><p>where f p?, tq : R D ? R D is the drift coefficient, gptq P R is the diffusion coefficient, and w P R D denotes a standard Wiener process (a.k.a., Brownian motion). Intuitively, we can interpret dw as infinitesimal Gaussian noise. The solution of an SDE is a diffusion process txptqu tPr0,T s , where r0, T s is a fixed time horizon. We let p t pxq denote the marginal distribution of xptq, and p 0t px 1 | xq denote the transition distribution from xp0q to xptq. Note that by definition we always have p 0 " p when using an SDE to perturb the data distribution.</p><p>The role of the SDE is to smooth the data distribution by adding noise, gradually removing structure until little of the original signal remains. In the framework of score-based diffusion models, we choose f px, tq, gptq, and T such that the diffusion process txptqu tPr0,T s approaches some analytically tractable prior distribution ?pxq at t " T , meaning p T pxq ? ?pxq. Three families of SDEs suitable for this task are outlined in <ref type="bibr" target="#b47">[48]</ref>, namely Variance Exploding (VE) SDEs, Variance Preserving (VP) SDEs, and subVP SDEs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Generating samples with the reverse SDE</head><p>Sample generation in score-based diffusion models relies on time-reversal of the diffusion process. For well-behaved drift and diffusion coefficients, the forward diffusion described in Eq. (1) has an        associated reverse-time diffusion process <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b15">16]</ref> given by the following SDE</p><formula xml:id="formula_1">v / D V + f 4 M w x G K A f G K B j V v b X 5 s w f Z O t r q H G 3 9 r C N H 3 t r I L V j Z M u V D w t 0 V o V Q 0 r V 0 9 f f q g T P D q u l u Q g X G 6 S R u 7 n d G j A z V c L Y x 0 Q 7 v a G H C l B a / P h 4 P o f D 2 + G Z x b n T Z + t t P C p B</formula><formula xml:id="formula_2">R m D U A c F G V N R A s B E S d U i w E Q 9 1 R L A R D P W M Y C M S 6 o p g I w z q z w Q b M V D f E G w E Q J 0 Q n B h w S n B q w M Z E m z O c E 2 y I u S 4 I N p R c f y H Y k H F d E m x o u G Y E M 3 N R C e b D c 2 J K t y b Y 0 G 1 9 S 7 A h 2 v q O Y E O</formula><formula xml:id="formula_3">K i q k D U F I l R L y E o C E W Y s q k K o b q j b p T c W v l Y I 1 Q i y Q k C E K g N Z F y B C 9 Y C s B h C h K k D W A I g M V b x 2 q V u 7 S T E T 6 y 0 / O 5 X X X i t A q T 0 N 0 u 0 c P f 1 o q U 5 m W m K a 0 N J s Z a l h K X s h e Q 2 g Q 0 T w f 4 J Y H K W i q / w k W A d H G x j d Q J r G v P 5 G i F W 3 U K w + t V C o g T G o R g h U t 1 C g I b V Q n B G 1 U J g z a u H l G t e K g v x M L R T j j T E 3 j R B h N / J G C F C 3 c D K N W U T x</formula><formula xml:id="formula_4">b 4 f G V Q 2 b M U P 9 f F N j p H + + p T G K A L 6 t M Y o B v q M x i g K + q z G K A 7 6 n M Y o E v q 8 x i g V + o D G K B v 5 G Y x Q P / K 3 G K C L 4 R G M U E / x Q Y x Q V / E h j F B f 8 W G M U G f x E Y x Q b / J 3 G K D r 4 q c Y o P v i Z x i h C + L n G K E b 4 h c Y o S v i l x i h O + J X G K F L 4 t c Y o V v h 7 j V G 0 8 A 8 a U 1 U f C n l P v N S k 2 E j f S / v W L U 2 0 a c C k i 2 j L g E k a 0 b Y B k z q i H Q M m g U S 7 B k w a i f Y M m G Q S 7 R s w K S U 6 M G A S S / T G g E k v 0 V s D J s l E E w M m 1 U S H B k z C i Y 4 M m L Q</formula><formula xml:id="formula_5">Q v x D q t Z E A F V R F Y 1 1 J 6 e d x q Q h g u F I M h y C X r 1 E s i C C X o V E 8 i S C X o 1 E 8 i i C X p V E 8 i y C X p 1 E 8 j C C X q V E 8 j S C X q 1 E 8 j i C X r V E 8 j y C X r 1 E 8 g C C n o V F M g S C n o 1 F M g i C n p V F M g y C n p 1 F M h C C n q V F M h S C n q 1 F M h i C n r V F M h y C n r 1 F M i C C n o V F c i S C n o 1 F c i i C n p V F c i y C n p 1 F c j C C n q V F c j S C n q 1 F c j i C n r V F</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>v y C U F I j v y S U x M i v C C U t 8 t e E k h T 5 G 0 I X j 3 U K r P t A 3 U L 4 + o F O V w Q C V Q A T t / i X 5 e F T a q F U t 6 i F E t 2 m F k p z h 1 q 4 2 e 1 S C 0 W 0 R y 0 U z z N q o W j 2 q Y V i O a A W i u Q 5 t V A c L 6 i F o p h Q C 8 V w S C 0 U w R G 1 c P F f U g s X / Z h a u N i v q I W L f E I t X N x T a u G i n l E L F / O c W r i I F 9 T C x b u k F i 7 a F b V w s V 5 T C x f p j X W + r t L q q i y 5 Z G A v m d A V F 2 4 p M n 7 l i 6 0 y i D U 6 8 m 5 S M S t r 4 W G 5 4 8 k 3 h S v 5 D q t d E A F V R E 4 1 1 J 1 e L D S g D J c K Q V D l E v T q J V A F E / Q q J l A l E / R q J l B F E / S q J l B l E / T q J l C F E / Q q J 1 C l E / R q J 1 D F E / S q J 1 D l E / T q J 1 A F F P Q q K F A l F P R q K F B F F P S q K F B l F P T q K F C F F P Q q K V C l F P R q K V D F F P S q K V D l F P T q K V A F F f Q q K l A l F f R q K l B F F f S q K l B l F f T q K l C F F f Q q K 1 C l F f R q K 1 D F F f S q K 1 D l F f T q K 1 A F F l g V F t 4 p Y M o R r</head><formula xml:id="formula_6">dx " " f px, tq?gptq 2 ? x log p t pxq ? dt`gptq dw,<label>(2)</label></formula><p>wherew is now a standard Wiener process in the reverse-time direction. Here dt represents an infinitesimal negative time step, meaning that the above SDE must be solved from t " T to t " 0. This reverse-time SDE results in exactly the same diffusion process txptqu tPr0,T s as Eq. <ref type="formula" target="#formula_0">(1)</ref>, assuming it is initialized with xpT q " p T pxq. This result allows for the construction of diffusion-based generative models, and its functional form reveals the key target for learning: the time-dependent score function ? x log p t pxq. Again, see <ref type="figure" target="#fig_7">Fig. 1</ref> for a helpful visualization of this two-part formulation.</p><p>In order to estimate ? x log p t pxq from a given dataset, we fit the parameters of a neural network s ? px, tq, termed a score-based model, such that s ? px, tq ? ? x log p t pxq for almost all x P R D and t P r0, T s. Unlike many likelihood-based generative models, a score-based model does not need to satisfy the integral constraints of a density function, and is therefore much easier to parameterize.</p><p>Good score-based models should keep the following least squares loss small</p><formula xml:id="formula_7">J SM p?; ?p?qq :" 1 2 ? T 0 E ptpxq r?ptq ? x log p t pxq?s ? px, tq 2 2 s dt,<label>(3)</label></formula><p>where ? : r0, T s ? R ?0 is a positive weighting function. The integrand features the well-known score matching <ref type="bibr" target="#b22">[23]</ref> objective E ptpxq r ? x log p t pxq?s ? px, tq 2 2 s. We therefore refer to Eq. (3) as a weighted combination of score matching losses.</p><p>With score matching techniques <ref type="bibr" target="#b55">[56,</ref><ref type="bibr" target="#b45">46]</ref>, we can compute Eq. (3) up to an additive constant and minimize it for training score-based models. For example, we can use denoising score matching <ref type="bibr" target="#b55">[56]</ref> to transform J SM p?; ?p?qq into the following, which is equivalent up to a constant independent of ?:</p><formula xml:id="formula_8">J DSM p?; ?p?qq :" 1 2 ? T 0 E ppxqp0tpx 1 |xq r?ptq ? x 1 log p 0t px 1 | xq?s ? px 1 , tq 2 2 s dt.<label>(4)</label></formula><p>Whenever the drift coefficient f ? px, tq is linear in x (which is true for all SDEs in <ref type="bibr" target="#b47">[48]</ref>), the transition density p 0t px 1 | xq is a tractable Gaussian distribution. We can form a Monte Carlo estimate of both the time integral and expectation in J DSM p?; ?p?qq with a sample pt, x, x 1 q, where t is uniformly drawn from r0, T s, x " ppxq is a sample from the dataset, and x 1 " p 0t px 1 | xq. The gradient ? x 1 log p 0t px 1 | xq can also be computed in closed form since p 0t px 1 | xq is Gaussian.</p><p>After training a score-based model s ? px, tq with J DSM p?; ?p?qq, we can plug it into the reverse-time SDE in Eq. <ref type="bibr" target="#b1">(2)</ref>. Samples are then generated by solving this reverse-time SDE with numerical SDE solvers, given an initial sample from ?pxq at t " T . Since the forward SDE Eq. (1) is designed such that p T pxq ? ?pxq, the reverse-time SDE will closely trace the diffusion process given by Eq. (1) in the reverse time direction, and yield an approximate data sample at t " 0 (as visualized in <ref type="figure" target="#fig_7">Fig. 1</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Likelihood of score-based diffusion models</head><p>The forward and backward diffusion processes in score-based diffusion models induce two probabilistic models for which we can define a likelihood. The first probabilistic model, denoted as p SDE ? pxq, is given by the approximate reverse-time SDE constructed from our score-based model s ? px, tq. In particular, suppose tx ? ptqu tPr0,T s is a stochastic process given by</p><p>dx "</p><p>" f px, tq?gptq 2 s ? px, tq ? dt`gptq dw,x ? pT q " ?.</p><p>We define p SDE ? as the marginal distribution ofx ? p0q. The probabilistic model p SDE ? is jointly defined by the score-based model s ? px, tq, the prior ?, plus the drift and diffusion coefficients of the forward SDE in Eq. <ref type="bibr" target="#b0">(1)</ref>. We can obtain a samplex ? p0q " p SDE ? by numerically solving the reverse-time SDE in Eq. (5) with an initial noise vectorx ? pT q " ?. The other probabilistic model, denoted p ODE ? pxq, is derived from the SDE's associated probability flow ODE <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b47">48]</ref>. Every SDE has a corresponding probability flow ODE whose marginal distribution at each time t matches that of the SDE, so that they share the same p t pxq for all time. In particular, the ODE corresponding to the SDE in Eq. (1) is given by</p><formula xml:id="formula_10">dx dt " f px, tq?1 2 gptq 2 ? x log p t pxq.<label>(6)</label></formula><p>Unlike the SDEs in Eq. (1) and Eq. <ref type="formula" target="#formula_6">(2)</ref>, this ODE describes fully deterministic dynamics for the process. Notably, it still features the same time-dependent score function ? x log p t pxq. By approximating this score function with our model s ? px, tq, the probability flow ODE becomes Many applications benefit from models which achieve high likelihood. One example is lossless compression, where log-likelihood directly corresponds to the minimum expected number of bits needed to encode a message. Popular likelihood-based models such as variational autoencoders and normalizing flows have already found success in image compression <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21]</ref>. Despite some known drawbacks <ref type="bibr" target="#b49">[50]</ref>, likelihood is still one of the most popular metrics for evaluating and comparing generative models.</p><p>Maximizing the likelihood of score-based diffusion models can be accomplished by either maximizing the likelihood of p SDE ? or p ODE ? . Although p ODE ? is a continuous normalizing flow (CNF) and its loglikelihood is tractable, training with maximum likelihood is expensive. As mentioned already, it requires solving an ODE at every optimization step in order to evaluate the log-likelihood on a batch of training data. In contrast, training with the weighted combination of score matching losses is much more efficient, yet in general it does not directly promote high likelihood of either p SDE ? or p ODE ? . In what follows, we show that with a specific choice of the weighting function ?ptq, the combination of score matching losses J SM p?; ?p?qq actually becomes an upper bound on D KL pp } p SDE ? q, and can therefore serve as an efficient proxy for maximum likelihood training. In addition, we provide a related lower bound on log p SDE ? pxq that can be evaluated efficiently on any individual datapoint x.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Bounding the KL divergence with likelihood weighting</head><p>It is well-known that maximizing the log-likelihood of a probabilistic model is equivalent to minimizing the KL divergence from the data distribution to the model distribution. We show in the following theorem that for the model p SDE ? , this KL divergence can be upper bounded by J SM p?; ?p?qq when using the weighting function ?ptq " gptq 2 , where gptq is the diffusion coefficient of SDE in Eq. (1). </p><formula xml:id="formula_11">D KL pp } p SDE ? q ? J SM p?; gp?q 2 q`D KL pp T } ?q.<label>(8)</label></formula><p>Sketch of proof. Let ? and ? denote the path measures of SDEs in Eq. (1) and Eq. (5) respectively. Intuitively, ? is the joint distribution of the diffusion process txptqu tPr0,T s given in Section 2.1, and ? represents the joint distribution of the process tx ? ptqu tPr0,T s defined in Section 3. Since we can marginalize ? and ? to obtain distributions p and p SDE ? , the data processing inequality gives D KL pp } p SDE ? q ? D KL p? } ?q. From the chain rule for the KL divergence, we also have D KL p? } ?q " D KL pp T } ?q`E p T pzq rD KL p?p?| xpT q " zq } ?p?|x ? pT q " zqqs, where the KL divergence in the final term can be computed by applying the Girsanov theorem <ref type="bibr" target="#b33">[34]</ref> to Eq. (5) and the reverse-time SDE of Eq. (1).</p><p>When the prior distribution ? is fixed, Theorem 1 guarantees that optimizing the weighted combination of score matching losses J SM p?; gp?q 2 q is equivalent to minimizing an upper bound on the KL divergence from the data distribution p to the model distribution p SDE ? . Due to well-known equivalence between minimizing KL divergence and maximizing likelihood, we have the following corollary. Corollary 1. Consider the same conditions and notations in Theorem 1. When ? is a fixed prior distribution that does not depend on ?, we hav? E ppxq rlog p SDE ? pxqs ? J SM p?; gp?q 2 q`C 1 " J DSM p?; gp?q 2 q`C 2 , where C 1 and C 2 are constants independent of ?.</p><p>In light of the result in Corollary 1, we henceforth term ?ptq " gptq 2 the likelihood weighting. The original weighting functions in <ref type="bibr" target="#b47">[48]</ref> are inspired from earlier work such as <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b44">45]</ref> and <ref type="bibr" target="#b18">[19]</ref>, which are motivated by balancing different score matching losses in the combination, and justified by empirical performance. In contrast, likelihood weighting is motivated from maximizing the likelihood of a probabilistic model induced by the diffusion process, and derived by theoretical analysis. There are three types of SDEs considered in <ref type="bibr" target="#b47">[48]</ref>: the Variance Exploding (VE) SDE, the Variance Preserving (VP) SDE, and the subVP SDE. In <ref type="table" target="#tab_1">Table 1</ref>, we summarize all these SDEs and contrast their original weighting functions with our likelihood weighting. For VE SDE, our likelihood weighting incidentally coincides with the original weighting used in <ref type="bibr" target="#b47">[48]</ref>, whereas for VP and subVP SDEs they differ from one another. Theorem 1 leaves two questions unanswered. First, what are the conditions for the bound to be tight (become an equality)? Second, is there any connection between p SDE ? and p ODE ? under some conditions? We provide both answers in the following theorem. Theorem 2. Suppose ppxq and qpxq have continuous second-order derivatives and finite second moments. Let txptqu tPr0,T s be the diffusion process defined by the SDE in Eq. (1). We use p t and q t to denote the distributions of xptq when xp0q " p and xp0q " q, and assume they satisfy the same assumptions in Appendix A. Under the conditions q T " ? and s ? px, tq " ? x log q t pxq for all t P r0, T s, we have the following equivalence in distributions</p><formula xml:id="formula_12">p SDE ? " p ODE ? " q.<label>(9)</label></formula><p>Moreover, we have</p><formula xml:id="formula_13">D KL pp } p SDE ? q " J SM p?; gp?q 2 q`D KL pp T } ?q.<label>(10)</label></formula><p>Sketch of proof. When s ? px, tq matches ? x log q t pxq, they both represent the time-dependent score of the same stochastic process so we immediately have p SDE ? " q. According to the theory of probability flow ODEs, we also have p ODE ? " q " p SDE ? . To prove Eq. (10), we note that</p><formula xml:id="formula_14">D KL pp } p SDE ? q " D KL pp}qq " D KL pp T }q T q?? T 0 d dt D KL pp t } q t q dt " D KL pp T }?q?? T 0 d dt D KL pp t } q t q dt.</formula><p>We can now complete the proof by simplifying the integrand using the Fokker-Planck equation of p t and q t followed by integration by parts.</p><p>In practice, the conditions of Theorem 2 are hard to satisfy since our score-based model s ? px, tq will not exactly match the score function ? x log q t pxq of some reverse-time diffusion process with the initial distribution q T " ?. In other words, our score model may not be a valid time-dependent score function of a stochastic process with an appropriate initial distribution. Therefore, although score matching with likelihood weighting performs approximate maximum likelihood training for p SDE ? , we emphasize that it is not theoretically guaranteed to make the likelihood of p ODE ? better. That said, p ODE ? will closely match p SDE ? if our score-based model well-approximates the true score such that s ? px, tq ? ? x log p t pxq for all x and t P r0, T s. Moreover, we empirically observe in our experiments (see <ref type="table">Table 2</ref>) that training with the likelihood weighting is actually able to consistently improve the likelihood of p ODE ? across multiple datasets, SDEs, and model architectures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Bounding the log-likelihood on individual datapoints</head><p>The bound in Theorem 1 is for the entire distributions of p and p SDE ? , but we often seek to bound the log-likelihood for an individual data point x. In addition, J SM p?; ?p?qq in the bound is not directly computable due to the unknown quantity ? x log p t pxq, and can only be evaluated up to an additive constant through J DSM p?; ?p?qq (as we already discussed in Section 2.2). Therefore, the bound in Theorem 1 is only suitable for training purposes. To address these issues, we provide the following bounds for individual data points. Theorem 3. Let p 0t px 1 | xq denote the transition distribution from p 0 pxq to p t pxq for the SDE in Eq. (1). With the same notations and conditions in Theorem 1, we hav?</p><formula xml:id="formula_15">log p SDE ? pxq ? L SM ? pxq " L DSM ? pxq,<label>(11)</label></formula><p>where L SM ? pxq is defined a?</p><formula xml:id="formula_16">E p0T px 1 |xq rlog ?px 1 qs`1 2 ? T 0 E p0tpx 1 |xq " 2gptq 2 ? x 1?s ? px 1 , tq`gptq 2 s ? px 1 , tq 2 2?2 ? x 1?f px 1 , tq ? dt,</formula><p>and L DSM ? pxq is given b?</p><formula xml:id="formula_17">E p 0T px 1 |xq rlog ?px 1 qs`1 2 ? T 0 E p 0t px 1 |xq " gptq 2 s ? px 1 , tq?? x 1 log p0tpx 1 | xq 2 2 ? dt 1 2 ? T 0 E p 0t px 1 |xq " gptq 2 ? x 1 log p0tpx 1 | xq 2 2`2 ? x 1?f px 1 , tq ? dt.</formula><p>Sketch of proof. For any continuous data distribution p, we have?E ppxq rlog p SDE ? pxqs " D KL pp } p SDE ? q`Hppq, where Hppq denotes the differential entropy of p. The KL term can be bounded according to Theorem 1, while the differential entropy has an identity similar to Theorem 2 (see Theorem 4 in Appendix A). Combining the bound of D KL pp } p SDE ? q and the identity of Hppq, we obtain a bound on?E ppxq rlog p SDE ? pxqs that holds for all continuous distribution p. Removing the expectation over p on both sides then gives us a bound on?log p SDE ? pxq for an individual datapoint x. We can simplify this bound to L SM ? pxq and L DSM ? pxq with similar techniques to <ref type="bibr" target="#b22">[23]</ref> and <ref type="bibr" target="#b55">[56]</ref>.</p><p>We provide two equivalent bounds L SM ? pxq and L DSM ? pxq. The former bears resemblance to score matching while the second resembles denoising score matching. Both admit efficient unbiased estimators when f p?, tq is linear, as the time integrals and expectations in L SM ? pxq and L DSM ? pxq can be estimated by samples of the form pt, x 1 q, where t is uniformly sampled over r0, T s, and x 1 " p 0t px 1 | xq. Since the transition distribution p 0t px 1 | xq is a tractable Gaussian when f p?, tq is linear, we can easily sample from it as well as evaluating ? x 1 log p 0t px 1 | xq for computing L DSM ? pxq.</p><p>Moreover, the divergences ? x?s? px, tq and ? x?f px, tq in L SM ? pxq and L DSM ? pxq have efficient unbiased estimators via the Skilling-Hutchinson trick <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b21">22]</ref>.</p><p>We can view L DSM ? pxq as a continuous-time generalization of the evidence lower bound (ELBO) in diffusion probabilistic models <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b18">19]</ref>. Our bounds in Theorem 3 are not only useful for optimizing and estimating log p SDE ? pxq, but also for training the drift and diffusion coefficients f px, tq and gptq jointly with the score-based model s ? px, tq; we leave this avenue of research for future work. In addition, we can plug the bounds in Theorem 3 into any objective that involves minimizin? log p SDE ? pxq to obtain an efficient surrogate. Section 5.2 provides an example, where we perform variational dequantization to further improve the likelihood of score-based diffusion models.</p><p>Similar to the observation in Section 4.1, L SM ? pxq and L DSM ? pxq are not guaranteed to upper bound log p ODE ? pxq. However, they should become approximate upper bounds when s ? px, tq is trained sufficiently close to the ground truth. In fact, we empirically observe that?log p ODE ? pxq ? L SM ? pxq " L DSM ? pxq holds true for x sampled from the dataset in all experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Numerical stability</head><p>So far we have assumed that the SDEs are defined in the time horizon r0, T s in all theoretical analysis. In practice, however, we often face numerical instabilities when t ? 0. To avoid them, we choose a small non-zero starting time ? 0, and train/evaluate score-based diffusion models in the time horizon r , T s instead of r0, T s. Since is small, training score-based diffusion models with likelihood weighting still approximately maximizes their model likelihood. Yet at test time, the likelihood bound as computed in Theorem 3 is slightly biased, rendering the values not directly comparable to results reported in other works. We use Jensen's inequality to correct for this bias in our experiments, for which we provide a detailed explanation in Appendix B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Related work</head><p>Our result in Theorem 2 can be viewed as a generalization of De Bruijin's identity ( <ref type="bibr" target="#b48">[49]</ref>, Eq. 2.12) from its original differential form to an integral form. De Bruijn's identity relates the rate of change of the Shannon entropy under an additive Gaussian noise channel to the Fisher information, a result which can be interpreted geometrically as relating the rate of change of the volume of a distribution's typical set to its surface area. Ref. <ref type="bibr" target="#b1">[2]</ref> (Lemma 1) builds on this result and presents an integral and relative form of de Bruijn's identity which relates the KL divergence to the integral of the relative Fisher information for a distribution of interest and a reference standard normal. More generally, various identities and inequalities involving the (relative) Shannon entropy and (relative) Fisher information have found use in proofs of the central limit theorem <ref type="bibr" target="#b23">[24]</ref>. Ref. <ref type="bibr" target="#b30">[31]</ref> (Theorem 1) covers similar ground to the relative form of de Bruijn's identity, but is perhaps the first to consider its implications for learning in probabilistic models by framing the discussion in terms of the score matching objective ( <ref type="bibr" target="#b22">[23]</ref>, Eq. 2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Improving the likelihood of score-based diffusion models</head><p>Our theoretical analysis implies that training with the likelihood weighting should improve the likelihood of score-based diffusion models. To verify this empirically, we test likelihood weighting with different model architectures, SDEs, and datasets. We observe that switching to likelihood weighting increases the variance of the training objective and propose to counteract it with importance sampling. We additionally combine our bound with variational dequantization <ref type="bibr" target="#b17">[18]</ref> which narrows the gap between the likelihood of continuous and discrete probability models. All combined, we observe consistent improvement of likelihoods for both p SDE ? and p ODE ? across all settings. We term the model p ODE ? trained in this way ScoreFlow, and show that it achieves excellent likelihoods on CIFAR-10 <ref type="bibr" target="#b27">[28]</ref> and ImageNet 32?32 <ref type="bibr" target="#b54">[55]</ref>, on a par with cutting-edge autoregressive models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Variance reduction via importance sampling</head><p>As mentioned in Section 2.2, we typically use Monte Carlo sampling to approximate the time integral in J DSM p?; ?p?qq during training. In particular, we first uniformly sample a time step t " Ur0, T s, and then use the denoising score matching loss at t as an estimate for the whole time integral. This Monte Carlo approximation is much faster than computing the time integral accurately, but introduces additional variance to the training loss.</p><p>We empirically observe that this Monte Carlo approximation suffers from a larger variance when using our likelihood weighting instead of the original weightings in <ref type="bibr" target="#b47">[48]</ref>. Leveraging importance sampling, we propose a new Monte Carlo approximation that significantly reduces the variance of learning curves under likelihood weighting, as demonstrated in <ref type="figure" target="#fig_8">Fig. 2</ref>. In fact, with importance sampling, the loss variance (after convergence) decreases from 98.48 to 0.068 on CIFAR-10, and decreases from 0.51 to 0.043 on ImageNet.</p><p>Let ?ptq " ?ptq 2 denote the weightings in <ref type="bibr" target="#b47">[48]</ref> (reproduced in <ref type="table" target="#tab_1">Table 1</ref>), and recall that our likelihood weighting is ?ptq " gptq 2 . Since ?ptq 2 empirically leads to lower variance, we can use a proposal distribution pptq :" gptq 2 {?ptq 2 Z to change the weighting in J DSM p?; gp?q 2 q from gptq 2 to ?ptq 2 with importance sampling, where Z is a normalizing constant that ensures ? pptq dt " 1. Specifically, for any function hptq, we estimate the time integral</p><formula xml:id="formula_18">? T 0 gptq 2 hptq dt with ? T 0 gptq 2 hptq dt " Z ? T 0 pptq?ptq 2 hptq dt ? T Z?ptq 2 hptq,<label>(12)</label></formula><p>wheret is a sample from pptq. When training score-based models with likelihood weighting, hptq corresponds to the denoising score matching loss at time t.</p><p>Ref. <ref type="bibr" target="#b32">[33]</ref> also observes that optimizing the ELBO for diffusion probabilistic models has large variance, and proposes to reduce it with importance sampling. They build their proposal distribution based on historical loss values stored at thousands of discrete time steps. Despite this similarity, our method is easier to implement without needing to maintain history, can be used for evaluation, and is particularly suited to the continuous-time setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Variational dequantization</head><p>Digital images are discrete data, and must be dequantized when training continuous density models like normalizing flows <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13]</ref> and score-based diffusion models. One popular approach to this is uniform dequantization <ref type="bibr" target="#b52">[53,</ref><ref type="bibr" target="#b49">50]</ref>, where we add small uniform noise over r0, 1q to images taking values in t0, 1,???, 255u. As shown in <ref type="bibr" target="#b49">[50]</ref>, training a continuous model p ? pxq on uniformly dequantized data implicitly maximizes a lower bound on the log-likelihood of a certain discrete model P ? pxq.</p><p>Due to the gap between p ? pxq and P ? pxq, comparing the likelihood of continuous density models to models which fit discrete data directly, such as autoregressive models <ref type="bibr" target="#b54">[55]</ref> or variational autoencoders, naturally puts the former at a disadvantage.</p><p>To minimize the gap between p ? pxq and P ? pxq, ref. <ref type="bibr" target="#b17">[18]</ref> proposes variational dequantization, where a separate normalizing flow model q ? pu | xq is trained to produce the dequantization noise by optimizing the following objective</p><formula xml:id="formula_19">max ? E x"ppxq E u"q ? p?|xq rlog p ? px`uq?log q ? pu | xqs.<label>(13)</label></formula><p>Plugging in the lower bound on log p ? pxq from Theorem 3, we can optimize Eq. (13) to improve the likelihood of score-based diffusion models. <ref type="table">Table 2</ref>: Negative log-likelihood (bits/dim) and sample quality (FID scores) on CIFAR-10 and ImageNet 32?32. Abbreviations: "NLL" for "negative log-likelihood"; "Uni. deq." for "Uniform dequantization"; "Var. deq." for "Variational dequantization"; "LW" for "likelihood weighting"; and "IS" for "importance sampling". Bold indicates best result in the corresponding column. Shaded rows represent models trained with both likelihood weighting and importance sampling. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Experiments</head><p>We empirically test the performance of likelihood weighting, importance sampling and variational dequantization across multiple architectures of score-based models, SDEs, and datasets. In particular, we consider DDPM++ ("Baseline" in <ref type="table">Table 2</ref>) and DDPM++ (deep) ("Deep" in <ref type="table">Table 2</ref>) models with VP and subVP SDEs <ref type="bibr" target="#b47">[48]</ref> on CIFAR-10 <ref type="bibr" target="#b27">[28]</ref> and ImageNet 32?32 <ref type="bibr" target="#b54">[55]</ref> datasets. We omit experiments on the VE SDE since (i) under this SDE our likelihood weighting is the same as the original weighting in <ref type="bibr" target="#b47">[48]</ref>; (ii) we empirically observe that the best VE SDE model achieves around 3.4 bits/dim on CIFAR-10 in our experiments, which is significantly worse than other SDEs. For each experiment, we report?Erlog p ODE ? pxqs ("Negative log-likelihood" in <ref type="table">Table 2</ref>), and the upper bound ErL DSM have no obvious difference in visual quality (see <ref type="figure" target="#fig_10">Figs. 3 and 4)</ref>. To trade likelihood for FID, we can use weighting functions that interpolate between likelihood weighting and the original weighting functions in <ref type="bibr" target="#b47">[48]</ref>. Our FID scores are still much better than most other likelihood-based models. We term p ODE ? a ScoreFlow when its corresponding score-based model s ? px, tq is trained with likelihood weighting, importance sampling, and variational dequantization combined. It can be viewed as a continuous normalizing flow, but is parameterized by a score-based model and trained in a more efficient way. With variational dequantization, we show ScoreFlows obtain competitive negative log-likelihoods (NLLs) of 2.83 bits/dim on CIFAR-10 and 3.76 bits/dim on ImageNet 32?32. Here the ScoreFlow on CIFAR-10 is trained without horizontal flipping (different from the setting in <ref type="table">Table 2</ref>). As shown in <ref type="table" target="#tab_3">Table 3</ref>, our results are on a par with the stateof-the-art autoregressive models on these tasks, and outperform all existing normalizing flow models. The likelihood on CIFAR-10 can be significantly improved by incorporating advanced data augmentation, as demonstrated in <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b40">41]</ref>. While we do not compare against them, we believe that incorporating the same data augmentation techniques can also improve the likelihood of ScoreFlows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We propose an efficient training objective for approximate maximum likelihood training of scorebased diffusion models. Our theoretical analysis shows that the weighted combination of score matching losses upper bounds the negative log-likelihood when using a particular weighting function which we term the likelihood weighting. By minimizing this upper bound, we consistently improve the likelihood of score-based diffusion models across multiple model architectures, SDEs, and datasets. When combined with variational dequantization, we achieve competitive likelihoods on CIFAR-10 and ImageNet 32?32, matching the performance of best-in-class autoregressive models.</p><p>Our upper bound is analogous to the evidence lower bound commonly used for training variational autoencoders. Aside from promoting higher likelihood, the bound can be combined with other objectives that depend on the negative log-likelihood, and also enables joint training of the forward and backward SDEs, which we leave as a future research direction. Our results suggest that scorebased diffusion models are competitive alternatives to continuous normalizing flows which enjoy the same tractable likelihood computation but with more efficient maximum likelihood training.</p><p>Limitations and broader impact Despite promising experimental results, we would like to emphasize that there is no theoretical guarantee that improving the SDE likelihood will improve the ODE likelihood, and this is explicitly a limitation of our work. Score-based diffusion models also suffer from slow sampling. In our experiments, the ODE solver typically need around 550 and 450 evaluations of the score-based model for generation and likelihood computation on CIFAR-10 and ImageNet respectively, which is considerably slower than alternative generative models like VAEs and GANs. In addition, the current formulation of score-based diffusion models only supports continuous data, and cannot be naturally adapted to discrete data without resorting to dequantization. Same as other deep generative models, score-based diffusion models can potentially be used to generate harmful media contents such as "deepfakes", and might reflect and amplify undesirable social bias that could exist in the training dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Author Contributions</head><p>Yang Song wrote the code, ran the experiments, proposed and proved Theorems 1 and 3, and wrote most of the paper. Conor Durkan proposed and proved a first version of Theorem 2, and wrote the paper. Iain Murray and Stefano Ermon co-advised the project and provided helpful edits to the draft. which is equivalent to the following when q T " ? and s ? px, tq " ? x log q t pxq, dx dt " f ? px, tq?1 2 gptq 2 ?x log q t px, tq,x ? pT q " q T .</p><p>The theory of probability flow ODEs <ref type="bibr" target="#b47">[48]</ref> guarantees that Eq. (21) and Eq. (23) share the same set of marginal distributions, tq t u tPr0,T s , which implies thatx ? p0q " q. Since by definitionx ? p0q " p ODE ? , we have p ODE ? " q.</p><p>The next part of the theorem can be proved by first rewriting the KL divergence from p to q in an integral form:</p><formula xml:id="formula_21">D KL pppxq } qpxqq piq " D KL pp 0 pxq } q 0 pxqq?D KL pp T pxq } q T pxqq`D KL pp T pxq } q T pxqq piiq " ? 0 T BD KL pp t pxq } q t pxqq Bt dt`D KL pp T pxq } q T pxqq,<label>(24)</label></formula><p>where (i) holds due to our definition p 0 pxq " ppxq and q 0 pxq " qpxq; (ii) is due to the fundamental theorem of calculus.</p><p>Next, we show how to rewrite Eq. (24) as a mixture of score matching losses. The Fokker-Planck equation for the SDE in Eq. (1) describes the time-evolution of the stochastic process's associated probability density function, and is given by Bp t pxq Bt " ? x??1 2 g 2 ptqp t pxq? x log p t pxq?f px, tqp t pxq?" ? x?p h p px, tqp t pxqq,</p><p>where for simplified notations we define h p px, tq :" 1 2 g 2 ptq? x log p t pxq?f px, tq. Similarly,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Bqtpxq Bt</head><p>" ? x?p h q px, tqq t pxqq. Since we assume log p t pxq and log q t pxq are smooth functions with at most polynomial growth at infinity (assumption (xii)), we have lim x?8 h p px, tqp t pxq " 0 and lim x?8 h q px, tqq t pxq " 0 for all t. Then, the time-derivative of D KL pp t } q t q can be rewritten in the following way:</p><p>BD KL pp t pxq } q t pxqq Bt " B Bt ? p t pxq log p t pxq q t pxq dx " ? Bp t pxq Bt log p t pxq q t pxq dx`? Bp t pxq Bt dx loooooomoooooon "0?? p t pxq q t pxq Bq t pxq Bt dx " ? ? x?p h p px, tqp t pxqq log p t pxq q t pxq dx?? p t pxq q t pxq ? x?p h q px, tqq t pxqq dx piq "?? p t pxqrh T p px, tq?h T q px, tqsr? x log p t pxq?? x log q t pxqs dx "?1 2 ? p t pxqgptq 2 ? x log p t pxq?? x log q t pxq Theorem 4. Let Hpppxqq be the differential entropy of the initial probability density ppxq. Under the same conditions in Theorem 2, we have</p><p>Hpppxqq " Hpp T pxqq`1 2</p><formula xml:id="formula_22">? T 0 E x"ptpxq "</formula><p>2f px, tq T ? x log p t pxq?gptq 2 ? x log p t pxq 2 2 ? dt.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(27)</head><p>" Hpp T pxqq?1 2</p><formula xml:id="formula_23">? T 0 E x"ptpxq "</formula><p>2??f px, tq`gptq 2 ? x log p t pxq 2 2 ? dt.</p><p>Proof. Once more we proceed analogously to the proofs of Theorem 2. We have where again (i) follows from integration by parts and the limiting behaviour of h p given by assumption (xii). Plugging this expression in for the integrand in Eq. (29) then completes the proof for Eq. <ref type="bibr" target="#b26">(27)</ref>. For Eq. (28), we can once again perform integration by parts and leverage the limiting behavior of p t pxq in assumption (xii) to get E ptpxq rf px, tq T ? x log p t pxqs " ? f px, tq T ? x p t pxq dx "?? p t pxq??f px, tq dx, which establishes the equivalence between Eq. (28) and Eq. <ref type="bibr" target="#b26">(27)</ref>.</p><formula xml:id="formula_25">Hpppxqq?Hpp T pxqq " ? 0 T B Bt Hpp t pxqq dt.<label>(29)</label></formula><p>Remark The formula in Theorem 4 provides a new way to estimate the entropy of a data distribution from i.i.d. samples. Specifically, given tx 1 , x 2 ,???, x N u i.i.d.</p><p>" ppxq and an SDE like Eq. (1), we can first apply score matching to train a time-dependent score-based model such that s ? px, tq ? ? x log p t pxq, and then plug s ? px, tq into Eq. (27) to obtain the following estimator of Hpppxqq:</p><formula xml:id="formula_26">Hpp T pxqq`1 2N N ? i"1 ? T 0 "</formula><p>2f px i , tq T s ? px i , tq?gptq 2 s ? px i , tq Both estimators can be computed from a score-based model alone, and do not require training a density model. Training We follow the same training procedure for score-based models in <ref type="bibr" target="#b47">[48]</ref>. We also use the same hyperparameters for training the variational dequantization model, except that we train it for only 300000 iterations while fixing the score-based model. All models are trained on Cloud TPU v3-8 (roughly equivalent to 4 Tesla V100 GPUs). The baseline DDPM++ model requires around 33 hours to finish training, while the deep DDPM++ model requires around 44 hours. The variational dequantization model for the former requires around 7 hours to train, and for the latter it requires around 9.5 hours.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Confidence intervals</head><p>All likelihood values are obtained by averaging the results on around 50000 datapoints, sampled with replacement from the test dataset. We can compute the confidence intervals with Student's t-test. On CIFAR-10, the radius of 95% confidence intervals is typically around 0.006 bits/dim, while on ImageNet it is around 0.008 bits/dim.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sample quality</head><p>All FID values are computed on 50000 samples from p ODE ? , generated with numerical ODE solvers as in <ref type="bibr" target="#b47">[48]</ref>. We compute FIDs between samples and training/test data for CIFAR-10/ImageNet. Although likelihood weighting + importance sampling slightly increases FID scores, their samples have comparable visual quality, as demonstrated in <ref type="figure">Figs</ref>  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>&lt; l a t e x i t s h a 1 _ b a s e 6 4 =&lt; l a t e x i t s h a 1 _ b a s e 6 4 = " b p 6 Q 6 k 3 F</head><label>141463</label><figDesc>" 7 u q 7 D I I z a F J h J X w u u m g Y / 3 L 6 I d w = " &gt; A A A 6 d X i c l V t p c 9 x I c u X u + l j T 1 6 z 3 o 8 M R G F P y z m 6 0 e k l K W s 1 O h C P E m 5 S a h 3 h L 0 x o F j g Q a I i 6 h C i A p R P s / + K v 9 y / x L / N V Z V S h k F R r k h B k h N e q 9 r A S q 6 m V l A o 3 2 i i R m f H X 1 f 3 7 x y 1 / 9 x V / + 1 V / / + m + W / / b v / v 4 f / v G b 3 / z T J c u r 0 o c L P 0 / y 8 t p z G S R x B h c 8 5 g l c F y W 4 q Z f A l X e z J f i r G k o W 5 9 k 5 v y / g Y + p G W R z G v s s R u p q W 9 d 1 3 q 7 / / 9 M 3 K 6 n h V / j m L B 2 v t w c p S + 3 f y 6 T d / v p 4 G u V + l k H E / c R n 7 c W 2 9 4 B 8 b t + S x n 8 B 8 + a n 4 c 4 5 2 r p z D j f N 9 Z 3 t n 9 + D o 4 P z g + O j M k d T y 8 r R i U L j + j R t B 4 6 Y s d f l s h J 9 h n n E 2 8 l J x z O 5 T b y Q I n u c J E w i f p X O r p 5 y B 0 Z 3 8 m N t O Z z j c s o R w 7 q i / p 4 5 E c K Z u 2 P J U H j P g V d E s O 4 7 s L 5 k R t s S B R P 6 9 K S H 4 9 u X q t 1 6 C T u e C 8 2 M O L e c l F d h k V S Y m 9 7 3 m k B L / p g G E u E 7 S p M l L N 4 t g 3 u y f H 0 7 m T R i + C l d h w U i 4 0 S Z r 4 a t X 3 g t h Y g 0 T z 9 k b u J t E e R n L q X p q 4 D 9 m O W T B R 0 E X D C p c v j y A e b / n I 5 R y G v t 3 v f M h V O Z C X H a P J C 5 Y 1 V u u q H S L m f R g X l n D K i + M o 6 r s e U D Y d w s h 1 B 4 e B 4 B E T w l D l l 6 e 3 3 D X Y 3 N c / T A v n a L M Q 2 A i G t z E Q S I B 1 h e i F C B p R i 6 g l 7 t l 4 A g l O k K T K E a r V x b 7 E J a u T 7 3 8 P E W S a 2 t 5 7 r U / r o 8 c 4 P 7 4 g c n T f T W S y 8 m 6 7 8 k 9 F S F n D x K y K k V V 9 q b 6 F n v j p I 5 4 f P O 1 t z B 5 F s m x j 0 I 3 8 + 9 n Q d l b 6 u B z x b i X 3 4 0 c L 4 4 Y L y s + E p N V J W 6 J W F o l P C 7 z W z x y b 8 C H J B k 5 Q e x G 2 G F 4 s U d h k r u 9 a 2 Y c D 0 p I B q J 5 A O J e z 5 D P I C 8 h F d H d E w E D 1 j 9 T 6 p b 3 Z W C j Y i 1 d r 3 f B g V x 8 W 9 0 / o q 5 i 7 u J W D B 8 b 3 N p w M x U n R R s x e h c 3 X M 6 h t N z c M T z C 9 V w W o T x 9 n W c Q 5 L y Z x u H d 9 D W a v + b 5 D W T j K S Q M x t O s S p J p G E / b T s v T b c A N t Y T T 3 M N F 2 M L 1 d r N g q n 2 E F c c o Q S e G J 3 0 G c d 3 9 v W N t 3 p z u b c 6 b 1 d H a + v e j 9 Z c v 5 4 s 2 z z s b a d W z C N z y p n A Z h y Q q A b J 5 U 0 Y e 2 o 5 X 1 0 a Y I V 6 9 F P + v v + h 1 8 q G s E n A N a 2 H 2 4 n v x / 5 / E N W R w 6 6 u h N V M R n j + u f W y m C c 4 t X 1 n D n I S f l t E f m i k v h b u f m m l 6 A 2 X 2 b G 3 8 M q 2 m Y h V Z 2 J z P 5 z 2 f q R c q n 8 L C C 5 u V t U U L z 7 D w h i x 8 N y E T b A z Z C A k r A y l m t e Q Q N V N I i 1 k D 4 2 g + 1 a s j q R 1 N 7 S x Q M b R U P I Y e d a C p g w X K D 1 v K D 3 v M l m a 2 + g x u Q v o C u d / j a t Z S N e s x t y V v b s f l m N t w k I d N M M 7 H o Q 2 7 N 2 7 j j m / G b m + Q c Y D j i 8 d B / 4 p w e v U l O T j v 0 2 F N C z V a O h y h o E a r a P b U O U T O Y e C L L M C c P H T a j K B 2 X 4 x a r A b i L H J 4 j t t V H T N t p j I P s x c W w Q R C P m / E V T n f T f D 4 9 / 3 F R x s f N 2 M o t d W W b A 3 Z l X E 0 6 5 y d i s a Q F c 8 L b X O e F 0 M W X s 5 5 n m q j T d l a s G v H 7 W o z 9 y E L T 1 t 4 D 1 n 4 2 s J / y C L Q F o G w w G X Y x 9 E l Y o S O 6 6 B 9 c u + o B Q w c n J v U 9 o H H A p R R 1 k y 9 0 J E R h k 5 2 5 Z q o R Y P M h 5 G T 5 L d Q P h P 7 + 1 h J R s w q h C t r j V q / / 5 h i S 0 X o U P d 2 F x 8 7 u 6 g F T D 4 l F 0 v P x I I h r z z u a o + 7 f Y + S 5 r e 5 P u f K e n t W 5 m g j B w f U N t Z 1 j y + V G 1 C X l e c r L x a 6 j b o + + u i 5 6 e q F H M 6 Z E v W j 0 4 H C V x f f R o A 1 H w M O 9 I S o 3 m e 6 9 9 l A 7 1 P d S 4 Q O z k I X Z O N u Y t T Z m Z y Z L g Q f m J q + Q 9 w 5 o e / S 8 L f y f N E j z Z r h + / m i b z d z A B d B d B 6 Y M v i i x q x N H h 6 0 5 a c q 8 M 7 B E X 7 a L b 1 1 s z P k Z s M p 3 V u a 9 5 6 z Z 8 + e u X U e B 0 7 F x M 4 U h 0 6 R Y 3 W K p Z l y X S Q u R k 7 r / + G r c / w Z B i O U A 2 M U j O r e 2 v y / B 9 k 6 2 u o c b f 2 s I 0 f e 2 s g t W N k y 5 U P C 3 R W h V D S t X T 1 7 9 q B M 8 O q 6 W 5 C B c b p J G 7 u d 0 a M D N V w t j H R D u 9 o Y c K U F r 8 + H g + h 8 P b 4 Z n F u d N n 6 2 0 8 K k F r h p t S M 3 1 C d Q d b n i 6 L F F U f 3 7 6 j 3 p + p / Y / f V I u x P g V Y v j B y + 4 F R z E i R B r I g 5 w P 0 c D c d T 6 w z u B v J S 0 P F K 8 P G w N k P L S Z q H O w u o v z u Z U i W 3 3 D W q s 0 A P T 4 F N b l a W N o h Z K R A 6 M D 3 e Q z L w b E R R M p L i C x X j j J F A x t e g i T 5 3 a L e P 2 J l L q G 8 u Y R n i + 4 1 l e p u j 1 i a h s n s z 1 d J Y 9 2 i X G s x m P G N 9 m f G I C m w m I AZ s B Y k K b C Y m J b C Y i Z m Y z M 2 J i m 4 m J + W w z n 4 m 5 s Z k b Y h K b S e Z S x m X q x A w j t g Q 3 u B e b n V r B k S P u U p 0 g z 3 7 H n c x N A e V 4 L 3 Y e a 2 G c t P W d 2 b 4 z O m t u M z k x h c 0 U x H y x m S / E l D Z T E s N s h h H D b Y Y T U 9 l M R U x t M z U x t z Z z S 8 y d z d w R c 2 8 z 9 8 R 8 t Z m v c 1 W g 6 Q D A z J x 3 2 3 v d B k n T 3 X d R 2 H T X j b f u M k q 0 h W w b v M G Z s E e w E R u 1 T 7 A R G H V A s B E V N R B s h E Q d E m z E Q x 0 R b A R D P S P Y i I S 6 I t g I g / o z w U Y M 1 D c E G w F Q J w Q n B p w S n B q w M d H m D O c E G 2 K u C 4 I N J d d f C D Z k X J c E G x q u G c H M X F S C + f C c m N K t C T Z 0 W 9 8 S b I i 2 v i P Y U Gx 9 T 7 A h 1 / o r w V q r O w m I h 2 b y Z q 8 c 0 C 0 o 0 Q 3 u y 6 C U N 7 g z g 5 L f 4 N 4 M S o O D u z M o I Q 7 u z 6 D U O L h D g 5 L k 4 B 4 N S p e D u z R y D + 7 T o B Q 6 u F O D k u n g X g 1 K q / 3 d W n O p z a U m 9 + B O D E q 6 g 3 s x K P 0 O 7 s a g R D y 4 H 4 N S 8 u C O D E r O g 3 s y K E 0 P 7 s q g h D 2 4 L 4 N S 9 + D O D E r i g 3 s z K J 0 P 7 s 6 g x D 6 4 P 4 N S / M M 7N M Z C G f t d h Z J u U H x s U N i k m w R v G v A W w V s G v E 3 w t g H v E L x j w L s E 7 x r w H s F 7 B r x P 8 L 4 B H x B 8 Y M B v C H 5 j w G 8 J f m v A E 4 I n B n x I 8 K E B H x F 8 Z M D H B B 8 b 8 A n B J w b 8 j u B 3 B n x K 8 K k B n x F 8 Z s D n B J 8 b 8 A X B F w Z 8 S f C l A V 8 R f G X A 1 w R f G / B7 g t 8 b 8 A e C P z y 8 v d q i A 6 U 6 Q 6 M b h n 6 l 9 A x u 0 + S 2 b G 7 L 5 L Z t b t v k d m x u x + R 2 b W 7 X 5 P Z s b s / k 9 m 1 u 3 + Q O b O 7 A 5 N 7 Y 3 B u T e 2 t z b 0 1 u Y n M T k z u 0 u U O T O 7 K 5 I 5 M 7 t r l j k z u x u R O T e 2 d z 7 0 z u 1 O Z O T e 7 M 5 s 5 M 7 t z m z k 3 u w u Y u T O 7 S 5 i 5 N 7 s r m r k z u 2 u a u T e 6 9 z b 0 3 u Q 8 2 p 2 V / a Z Y Q 9 V e Q 9 x F 4 7 7 r a 9 a 3 z D B p 9 P 6 u x t F L Q N K W k 0 d X E A r f r 4 b q I W 7 y I D c c p R N p e H h O V u K k X a E 4 1 D G e z z t v M c N e a G 4 a e Q q j W k Z U O I l T h y P o G E a p r 6 n a 0 V M 3 I W g Y R q m F k B Y M I V S 6 y b k G E 6 p W 6 v U j j C j 8 r h G o T W Z k g c m O M X C G J M d c K o e p D 1 h 6 I Z M Z U K i Q 3 J k k h V F / I 6 g I R q i p k T Y E I 1 R K y k k C E G Y u q E K o b 6 n b p j Y W v F U I 1 g q w Q E K H K Q N Y F i F A 9 I K s B R K g K k D U A I k M V r 1 3 q 1 m 5 S z M R 6 y 8 9 O 5 b X X C l B q T 4 N 0 O 0 d P P 1 q q k 5 m W m C a 0 N F t Z a l j K X k h e A + g Q E f y f I B Z H q e g q P w n W w d E G R j e Q p j G v v x F i 1 S 0 U q 0 8 t F G p g D K o R A t U t F G h I L R R n R C 0 U 5 o x a e L n G t a I g P 1 M L x X h j z E 0 j R N i N v B E C 1 C 2 c T G M W U X y 5 M S W N E J 1 u o e i + U A s F V x o z 1 Q i h d R P U C J H p F k 6 0 M c 0 o s J p a K K 5 b a q G w 7 q i F o r q n F g r q 6 7 z 9 W g 1 z + Z 3 C Z R 5 H n V H + l t k b E c r a M m c j Q r l a Z m p E K E P L / I w I 5 W W Z l R G h b C x z M S K U g 2 U G R o Q y r 8 y 7 i F C + l d k W E c q y M s c i Q r l V Z l Z E K K P K f I o I 5 V G Z R R G h 7 C l z J y K U M 2 X G R I Q y p c y T i F B + l N k R E c q K M i c i Q r l Q Z k J E K A P K / I c I 5 T 2 Z 9 R C h b C d z H S K U 4 2 S G Q 4 Q y m 8 x r i F A + k 9 k M k Q / G C l L u 8 c z U k 5 5 0 2 e L E y B b p x M w w E z v D p G d t D A v u T M W x V N E 5 Z C w v u 7 c b U F S z D b E D 4 R m b 9 m v 8 W D y O h c z P g z i L 0 J l b J Q J h Y X e c z h s m n i S f A X / I g Z c n w c + 5 8 e 7 m j X h P y H 4 a n L V v I c j c 3 P q T T 8 L b o X F V w 2 b M U D / f 1 B j p n 2 9 p j C K A b 2 u M Y o D v a I y i g O 9 q j O K A 7 2 m M I o H v a 4 x i g R 9 o j K K B v 9 E Y x Q N / q z G K C D 7 R G M U E P 9 Q Y R Q U / 0 h j F B T / W G E U G P 9 E Y x Q Z / p z G K D n 6 q M Y o P f q Y x i h B + r j G K E X 6 h M Y o S f q k x i h N + p T G K F H 6 t M Y o V / l 5 j F C 3 8 g 8 Z U 1 Y d C 3 h M v N S k 2 0 v f S v n V L E 2 0 a M O k i 2 j J g k k a 0 b c C k j m j H g E k g 0 a 4 B k 0 a i P Q M m m U T 7 B k x K i Q 4 M m M Q S v T F g 0 k v 0 1 o B J M t H E g E k 1 0 a E B k 3 C i I w M m 7 U T H B k z y i U 4 M m B Q U v T N g E l F 0 a s C k o + j M g E l K 0 b k B k 5 q i C w M m Q U W X B k y a i q 4 M m G Q V X R s w K S t 6 b 8 A k r u i D A e u 7 C t z a 2 l K N d U 9 q P E N c b J N Q 0 h b b I p S k x b Y J l c p 6 6 m z L b 0 s q B o 7 r M O A O n j q B w N k Z O R 7 4 r s D 5 L G b O b V 4 l A U L Y A o f J 7 1 a w l q z K 9 n 3 G Z f U y J d w V W F v K L 4 z 1 1 / a 7 d E Z S J 9 s j l M T J 9 g k l bb I D Q k m a 7 A 2 h p E z 2 l l A S J p s Q S r p k h 4 S S L N k R o a R K d k w o i Z K d E E q a Z O 8 I J U m y U 0 J J k e y M U B I k O y e U 9 M g u C C U 5 s k t C S Y 3 s i l A S I 7 s m l L T I 3 h N K U m Q f C O 0 e 6 2 R Y 9 4 G 8 h X D V A 5 2 2 C A S q A C Z 2 8 S / K w w 1 q o V Q 3 q Y U S 3 a I W S n O b W r j Z 7 V A L R b R L L R T P H r V Q N P v U Q r E c U A t F 8 o Z a K I 6 3 1 E J R T K i F Y j i k F o r g i F q 4 + M f U w k U / o R Y u 9 j t q 4 S K f U g s X 9 4 x a u K j n 1 M L F v K A W L u I l t X D x r q i F i 3 Z N L V y s 9 9 T C R f p g n K + t t N o q S y w Z m E v G V c W F W 4 q I X / F i q w h i h Y 6 c 2 5 j P 8 o o 7 W O 4 4 4 k 3 h Q r z D a h Z E Q B W R V Q 2 1 p + e d B q T h Q i E I s l y C X r 0 E s m C C X s U E s m S C X s 0 E s m i C X t U E s m y C X t 0 E s n C C X u U E s n S C X u 0 E s n i C X v U E s n y C X v 0 E s o C C X g U F s o S C X g 0 F s o i C X h U F s o y C X h 0 F s p C C X i U F s p S C X i 0 F s p i C X j U F s p y C X j 0 F s q C C X k U F s q S C X k 0 F s q i C X l U F s q y C X l 0 F s r C C X m U F s r S C X m 0 F s r i C X n U F s r y C X n 0 F s s A C o 8 L C O w V M O b y s w K m y A M r k X r w Y F b j c d S L I o M R s I 9 o x Q 6 V 7 l U g 9 t m w L Y T p v i k / N t E w b 2 Z C J T 3 i F t I j L G F O e 1 b 9 7 P d G 7 l + l O v m o i T o L 5 s e d b v 4 U y c z n e q d u n s C x P T M u T + d D F p H k A y W M D k Q b d S F R r 4 T y t 0 c l j R g WP k w B a y 6 l s d F f f 9 c B t g u f + z G U 8 9 h 2 3 4 r m 8 g 4 L S u s L e O 7 K F s u m u s e 2 y e A E B W H a q O W B X I o G b j r Z T T d S C L 5 + h 2 c a J W y T i h f n u r Z 1 J C 4 i f e r T H 9 v T a / X f m X c b b 6 V / I h B k v B k 3 6 7 O n c z O 2 9 X T M t j D n u k U k 5 1 8 / d b K K E a N 4 9 S e t T P q c x i l Y c x l D 2 X b M 8 5 K l 7 R 5 Y a 6 N t h s s j l i 1 L q I d u i l y K p x O i / i i c B N v t 2 M j f f k n o 7 W V j A S 7 e k K x C N v n + O H 2 6 J a 1 / m h u X Z w g J s 5 T X R o i E E e p U n Y e m m 4 o H U 7 D Y v s U B l 7 j 1 z n k x + W n 8 i 3 h A S J W l Y Z e p N V / l r C i b f U H s y h S Q x b P Q D 0 a f O J i Z A D P l M / H e P 8 Q 6 p e F N O V M H K q W E t X l P N q 0 j m T F k U x x x G 0 j 3 L n S A H 4 e 4 2 v o k L C G J 3 3 H v L O S / T R H x B M G 8 m P 6 3 O B 8 g 8 A 8 G t D X H 8 V v Z b H + I K w R Q D j N T C 5 K d p n I X 8 v h 8 6 h V u K h 8 O 4 b b g i W M 4 A 9 1 r m R u D E m Z P l b U H P 4 W 7 s b M 1 y J q Y n F w W g P 3 O 2 8 d 4 3 g 9 8 x R / y u a r x s P c 4 5 L s T u n J d / Q I 2 X k b w A / J y O x N F j h m K f V I Z x 9 p g h q 4 r W U B w 9 Y o h j b g 3 F 0 a C h l D + e V / 7 / g M U 5 K v S 8 7 M 9 d p V 8 y z I t m P v 1 W a z R Y 2 J / E K r Q / a 4 G Q T 5 P L 9 r c t 4 s 3 4 a X k p f + G C X c R P e V w P d 4 U k v / V K c G + W P 3 2 z s t b / 5 e X i w e X 6 e O 1 P 4 / V 3 L 1 Z e b 7 a / y v z 1 0 j 8 v / e v S d 0 t r S 6 + W X i / t L 5 0 s X S z 5 S z d L / 7 n 0 X 0 v / / e f / / e F f f n j y w 7 8 p 0 1 / + o u 3 z 2 y X r 7 4 c / / h 9 p + I 1 F &lt; / l a t e x i t &gt; xp0q N e u P 4 S v J c / T 0 E c G 9 L j I = " &gt; A A A 6 n X i c l V v p c t x I c u b u + l j T 1 + z 6 p 3 8 Y Y 4 5 i Z 9 e t N k l J q 7 E i N k K 8 S a l 5 i L c 0 r V H g S K A h 4 h K q A J J C t F / C T + O / 9 l P 4 b Z x V h U J W o U F O L C O k R n 1 f V g J V 9 W V l A o 3 2 i i R m f H X 1 / 3 7 x y 1 / 9 x V / + 1 V / / + m + W / / b v / v 4 f / v G b 3 / z 2 k u V V 6 c O F n y d 5 e e 2 5 D J I 4 g w s e 8 w S u i x L c 1 E v g y r v Z E v x V D S W L 8 + y c 3 x f w M X W j L A 5 j 3 + U I f f r m 6 b Q K n G l Z 3 z l / c q Z 1 + L 0 4 H D n 8 9 4 7 A u f N v T v R 9 2 0 D m 9 t M 3 K 6 v j V f n n L B 6 s t Q c r S + 3 f y a f f / M f 1 N M j 9 K o W M + 4 n L 2 I 9 r 6 w X / 2 L g l j / 0 E 5 s t P x J 9 z t H P l H G 6 c 7 z v b O 7 s H R w f n B 8 d H Z 4 6 k l p e n F Y P C 9 W / c C B o 3 Z a n L Z y P 8 D P O M s 5 G X i m N 2 n 3 o j Q f A 8 T 5 h A + C y d W z 3 l T I 3 u 5 M f c d j r D a S l L C O e O + n v i S A R n 9 I Y t T + U x A 1 4 V z b L j y P 6 S G W F L H E j k T 0 0 J w b c v V r / 1 E n Q 6 F 5 w f c 2 g 5 L 6 n A J q s y M b k f N I e U + D c N I M T 1 l C Z N X r p Z B P N m / / x w M m / C 8 G W 4 C g t G w o 0 2 W Q t f v v S e C x N r m H j O 3 s D d J M r L W E 7 V E w P / M c s h C z 4 K u m B Q 4 f L l A c z 7 P R + h l N P Y v + u d D 6 E y F y K 0 e y R x w a r e c k W l W 8 y k B / P K G l Z 5 Y R x V Z c 8 D w r 5 b C E H 3 8 D g A J H p K G L L 0 8 v y G u x 6 b 4 + q H e e k U Z R 4 C E 1 H j J g 4 S C b C + E K U A S T N y A b 3 c L Q N H K N E R m k Q x W r 2 y 2 I e w d H 3 q 5 e c p k l x b y 3 O v / f v 6 y A H u j x + Y P N 1 X I 7 m c r P u e 3 F M R c v Y g I a t S V G V v q m + x N 0 7 q i M c 3 X 3 s L k 2 e R H P s o d D P / f h a U v a U O P l e M e z l u G F 4 c M V 5 W f C Q m q 0 r c E r G 0 S n h c 5 r d 4 5 N 6 A D 0 k y c o L Y j b D D 8 G K P w i R 3 e 9 f M O B 6 U k A x E 8 w D E v Z 4 h n 0 F e Q i q i u y c C B q x / p t Q t 7 8 v A R s V a u l 7 v g g O 5 + L a 6 f 0 R d x d z F L R s + N r i 1 4 a Y r T o o 2 Y v Q u b s y c Q 2 m 5 u W N 4 h O u 5 L E J 5 + j r P I M h 5 M 4 3 D u + l r N H / N 8 x v I x l N I G I y n W Z U k 0 z C e t p 2 W p 9 u A G 2 o J p 7 m H i 7 C F 6 + 1 m w V T 7 C C u O U Y J O D E / 6 D O K 6 + 3 v H 2 r w 5 3 d u c N 6 u j t f U f R u s v X s w X b Z 5 1 N t K q Z x G 4 5 U 3 h M g 5 J V A J k 8 6 a M P L Q d r 6 6 N M E O 8 f C H + X 3 / e 6 + R D W S X g G t b C 7 P k P 4 v 8 / i m v I 4 N Z X Q 2 u m I j x / X P v Y T B O c W 7 6 y h g k J P y 2 j P z R T X g p 3 P z X T 9 A b K 7 O n a + E V a T c U q s r A 5 n 8 9 7 P l M v V D 6 F h R c 2 K 2 u L F p 5 h 4 Q 1 Z + G 5 C J t g Y s h E S V g Z S z G r J I W q m k B a z B s b R f K p X R 1 I 7 m t p Z o G J o q X g M P e p A U w c L l B + 2 l B / 2 m C 3 N b P U Z 3 I T 0 B X K / x 9 W s p W r W Y 2 5 L 3 t y O y z G 3 4 S A P m 2 C c j 0 M b d m / c x h 3 f j N 3 e I O M A x x e P g / 4 V 4 f T q S 3 J w 3 q f D m h Z q t H Q 4 Q k G N V t H s i X O I n M P A F 1 m A O X n o t B l B 7 b 4 Y t V g N x F n k 8 B y 3 q z p m 2 k x l H m Y v L I I J h H z e i K t y v p / g 8 e / 7 i 4 8 2 P m 7 G U G q r L d k a s i v j a N Y 5 O x W N I S u e F 9 r m P C + G L L y c 8 z z V R p u y t W D X j t v V Z u 5 D F p 6 2 8 B 6 y 8 L W F / 5 B F o C 0 C Y Y H L s I + j S 8 Q I H d d B + + T e U Q u I Z S e U q e 0 D j w U o o 6 y Z e q E j I w y d 7 M o 1 U Y s G m Q 8 j J 8 l v o X w q 9 v e x k o y Y V Q h X 1 h q 1 f v 8 5 x Z a K 0 K H u 7 S 4 + d n Z R C 5 h 8 S i 6 W n o k F Q 1 5 5 3 N U e d / s e J c 1 v c 3 3 O l f X 2 r M z R R g 4 O q G 2 s 6 x 5 f K j e g L i v P V p 4 v d B t 1 f f T R M 9 P V c z m c M y X q R 6 c D h a 8 u v o 0 A a z 4 G H O g J U b 3 P d O + z g d 6 n u p c I H Z y F L s j G 3 c S o s z M 5 M 1 0 I P j A 1 f Y e 4 c 0 L f p e F v 5 d m i R 5 o 1 w / e z R d 9 u 5 g A u g u g 8 M G X w R Y 1 Z m z w 8 a M t P V e C d g y P 8 t F t 6 6 2 Z n y M 2 G U 7 q 3 N O 8 9 Z 0 + f P n X r P A 6 c i o m d K Q 6 d I s f q F E s z 5 b p I X I y c 1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>a 4 a b U j N 9 Q n U H W 5 4 u i x R V H 9 + + o 9 6 f q f 2 P 3 1 S L s T 4 F W L 4 w c v u B U c x I k Q a y I O c D 9 H A 3 H U + s M 7 g b y U t D x S v D x s D Z D y 0 m a h z s L q L 8 7 m V I l t 9 w 1 q r N A D 0 + B T W 5 W l j a I W S k Q O j A 9 3 k M y 8 G x E U T K S 4 g s V 4 4 y R Q M b X o I k + d 2 i 3 j 9 i Z S 6 h v L m E Z 4 v u N Z X q b o 9 T t R 2 X w 3 1 9 N Z 9 m i X G M 9 m P G J 8 m / G J C W w m I A Z s B o g J b S Y k J r K Z i J i Z z c y I i W 0 m J u a z z X w m 5 s Z m b o h J b C a Z S x m X q R M z j N g S 3 O B e b H Z q B U e O u E t 1 g j z 7 H X c y N w W U 4 7 3 Y e a y F c d L W d 2 b 7 z u i s u c 3 k x B Q 2 U x D z x W a + E F P a T E k M s x l G D L c Z T k x l M x U x t c 3 U x N z a z C 0 x d z Z z R 8 y 9 z d w T 8 9 V m v s 5 V g a Y D A D N z 3 m 3 v d R s k T X f f R W H T X T f e u s s o 0 R a y b f A G Z 8 I e w U Z s 1 D 7 B</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>x 9 T 3 B h l z r r w R r r e 4 k I B 6 a y Z u 9 c k C 3 o E Q 3 u C + D U t 7 g z g x K f o N 7 M y g N D u 7 O o I Q 4 u D + D U u P g D g 1 K k o N 7 N C h d D u 7 S y D 2 4 T 4 N S 6 O B O D U q m g 3 s 1 K K 3 2 d 2 v N p T a X m t y D O z E o 6 Q 7 u x a D 0 O 7 g b g x L x 4 H 4 M S s m D O z I o O Q / u y a A 0 P b g r g x L 2 4 L 4 M S t 2 D O z M o i Q / u z a B 0 P r g 7 g x L 7 4 P 4 M S v E P 7 9 A Y C 2 X s d x V K u k H x s U F h k 2 4 S v G n A W w R v G f A 2 w d s G v E P w j g H v E r x r w H s E 7 x n w P s H 7 B n x A 8 I E B v y H 4 j Q G / J f i t A U 8 I n h j w I c G H B n x E 8 J E B H x N 8 b M A n B J 8 Y 8 D u C 3 x n w K c G n B n x G 8 J k B n x N 8 b s A X B F 8 Y 8 C X B l w Z 8 R f C V A V 8 T f G 3 A 7 w l + b 8 A f C P 7 w 8 P Z q i w 6 U 6 g y N b h j 6 l d I z u E 2 T 2 7 K 5 L Z P b t r l t k 9 u x u R 2 T 2 7 W 5 X Z P b s 7 k 9 k 9 u 3 u X 2 T O 7 C 5 A 5 N 7 Y 3 N v T O 6 t z b 0 1 u Y n N T U z u 0 O Y O T e 7 I 5 o 5 M 7 t j m j k 3 u x O Z O T O 6 d z b 0 z u V O b O z W 5 M 5 s 7 M 7 l z m z s 3 u Q u b u z C 5 S 5 u 7 N L k r m 7 s y u W u b u z a 5 9 z b 3 3 u Q + 2 J y W / a V Z Q t R f Q d 5 H 4 L 3 r a t e 3 z j N o 9 P 2 s x t J K Q d O U k k Z X E w v c r o f r I m 7 x I j Y c p x B p e 3 l M V O K m X q A 5 1 T C c z T p v M 8 N d a 2 4 Y e g q h W k d W O o h Q h S P r G 0 S o r q n b 0 V I 1 I 2 s Z R K i G k R U M I l S 5 y L o F E a p X 6 v Y i j S v 8 r B C q T W R l g s i N M X K F J M Z c K 4 S q D 1 l 7 I J I Z U 6 m Q 3 J g k h V B 9 I a s L R</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>5 c a U N E J 0 u o W i + 0 I t F F x p z F Q j h N Z N U C N E p l s 4 0 c Y 0 o 8 B q a q G 4 b q m F w r q j F o r q n l o o q K / z 9 m s 1 z O V 3 C p d 5 H H V G + V t m b 0 Q o a 8 u c j Q j l a p m p E a E M L f M z I p S X Z V Z G h L K x z M W I U A 6 W G R g R y r w y 7 y J C + V Z m W 0 Q o y 8 o c i w j l V p l Z E a G M K v M p I p R H Z R Z F h L K n z J 2 I U M 6 U G R M R y p Q y T y J C + V F m R 0 Q o K 8 q c i A j l Q p k J E a E M K P M f I p T 3 Z N Z D h L K d z H W I U I 6 T G Q 4 R y m w y r y F C + U x m M 0 Q + G C t I u c c z U 0 9 6 0 m W L E y N b p B M z w 0 z s D J O e t T E s u D M V x 1 J F 5 5 C x v O z e b k B R z T b E D o R n b N q v 8 W P x O B Y y P w / i L E J n b p U I h I X d c T p v m H i S f A b 8 I Q d e n g Q / 5 8 a 7 m z f i P S H 7 a X D W v o U g c 3 P r T z 4 J</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>T H R s w y S c 6 M W B S U P T O g E l E 0 a k B k 4 6 i M w M m K U X n B k x q i i 4 M m A Q V X R o w a S q 6 M m C S V X R t w K S s 6 L 0 B k 7 i i D w a s 7 y p w a 2 t L N d Y 9 q f E M c b F N Q k l b b I t Q k h b b J l Q q 6 4 m z L b 8 t q R g 4 r s O A O 3 j q B A J n Z + R 4 4 L s C 5 7 O Y O b d 5 l Q Q I Y Q s c J r 9 b w V q y K t v 3 G Z f V y 5 R w V 2 B t K b 8 w 1 l / b 7 9 I Z S Z 1 s j 1 A S J 9 s n l L T J D g g l a b I 3 h J I y 2 V t C S Z h s Q i j p k h 0 S S r J k R 4 S S K t k x o S R K d k I o a Z K 9 I 5 Q k y U 4 J J U W y M 0 J J k O y c U N I j u y C U 5 M g u C S U 1 s i t C S Y z s m l D S I n t P K E m R f S C 0 e 6 y T Y d 0 H 8 h b C V Q 9 0 2 i I Q q A K Y 2 M W / K A 8 3 q I V S 3 a Q W S n S L W i j N b W r h Z r d D L R T R L r V Q P H v U Q t H s U w v F c k A t F M k b a q E 4 3 l I L R T G h F o r h k F o o g i N q 4 e I f U w s X / Y R a u N j v q I W L f E o t X N w z a u G i n l M L F / O C W r i I l 9 T C x b u i F i 7 a N b V w s d 5 T C x f p g 3 G + t t J q q y y x Z G A u G V c V F 2 4 p I n 7 F i 6 0 i i B U 6 c m 5 j P s s r 7 m C 5 4 4 g 3 h</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>c j y C n r 1 F c g C C 4 w K C + 8 U M O X w s g K n y g I o k 3 v x Y l T g c t e J I I M S s 4 1 o x w y V 7 l U i 9 d i y L Y T p v C k + N d M y b W R D J j 7 h F d I i L m N M e V b / 7 v V E 7 1 6 m O / m q i T g J 5 s e e b / 0 W y s z l e K d u n 8 K y P D E t T + Z D F 5 P m A S S P D U Q a d C N R r Y X z t E Y n j x k V P E 4 C a C 2 n s t F d f d c D t w m e + z O X 8 d h 3 3 I r n 8 g 4 K S u s K e + / I F s q m u 8 a 2 y + I F B G D Z q e a A X Y k E b j r a T j V R C 7 5 8 h m Y b J 2 6 R i B f m u 7 d 2 J i 0 g f u r R H t v T a / f f m X c Z b 6 d / I R N m v B g 0 6 b O n c z O 3 9 3 b N t D D m u E c m 5 V w / d 7 O J E q J 5 9 y S t T / m c x i h a c R h D 2 X f N 8 p C n 7 h 1 Z a q B v h 8 k i l y 9 K q Y d s i 1 6 K p B K j / y q e B N j s 2 8 n c f E v q 7 W R h A S / d k q 5 A N P r + O X 6 4 J a 5 9 m R u W Z w s L s J X X R I u G E O h V n o S l m 4 o H U r P b v M Q C l b n 3 z P l u 8 t P 6 d + I N I V G S h l W m 3 n S V v 6 Z g 8 g 2 1 7 6 a Q J I a N f i D 6 x N n E B I g h n 4 n / 7 j H e I R V v y o k q W D k 1 r M V r q n k V y Z w p i + K Y w 0 i 6 Z 7 k T 5 C D c 3 c Y 3 c Q F B 7 I 5 7 b z n n Z Z q I L w j m z e S n 1 f k A m W c g u L U h j t / K f u t D X C G Y Y o C R W p j 8 N I 2 z k N / 3 Q 6 d w S / F w G L c N V w T L G e B e y 9 w I n D h z s r w t 6 D n c j Z 2 t W c 7 E 9 O S i A P R n z j b e + 2 b w O + a I 3 1 W N l 6 3 H O c e F 2 J 3 z 8 g + o 8 T K S F 4 C f 0 5 E 4 e s x Q 7 J P K M M 4 e M 2 R V 0 R q K o 0 c M c c y t o T g a N J T y x / P K / x + w O E e F n p f 9 u a v 0 S 4 Z 5 0 c y n 3 2 q N B g v 7 k 1 i F 9 m c t E P J p c t n + t k W 8 G T 8 t L + U v X L C L + C m P 6 + G u k O S 3 X g n u z f K n b 1 b W + r + 8 X D y 4 X B + v / X G 8 / u 7 5 y u v N 9 l e Z v 1 7 6 5 6 V / X f p + a W 3 p 5 d L r p f 2 l k 6 W L J X / p v 5 b + e + l / l v 7 3 1 b + 8 2 n k 1 e X W k T H / 5 i 7 b P P y 1 Z f 6 + u / h + 7 z J n 5 &lt; / l a t e x i t &gt; dx " f px, tq dt`gptq dw &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " h C S A 3 Z C 7 t 7 f M t k X u o 6 O 4 N v r b 7 z M = " &gt; A A A 6 y 3 i c l V v b c t x I c u W u b 2 v 6 N m s / + g V j j m J n d l t t k p J 2 x h O x E S P e R E p N i u J d U k s K X B J o i L i p q g C S g n v e / G H + D H + B X + 0 / c F Y V q r M K D W r C j J A a d U 5 W A l V 1 s j K B R g d V l n K x v v 5 f v / r 1 n / 3 5 X / z l X / 3 m r 1 f / 5 m / / 7 u / / 4 a v f / u M F L 2 s W w n l Y Z i W 7 C n w O W V r A u U h F B l c V A z 8 P M r g M r r c l f 9 k A 4 2 l Z n I m 7 C t 7 l f l K k c R r 6 A q E P X 7 2 d 1 p E 3 Z c 2 t 9 y f v 7 b S J v 5 X H I 0 9 8 5 z 3 0 k m / F d + 8 3 f / 5 5 W v h B 5 n 9 Q V t O s T L z q g 1 B 2 3 / 3 8 8 z v Z X 3 h / U M a e c h b 4 r E X 2 Z v 7 h q 7 X 1 8 b r 6 8 5 Y P N r q D t Z X u 7 / j D b / / t a h q V Y Z 1 D I c L M 5 / z t x m Y l 3 r U + E 2 m Y w X z 1 g f z z j n Y v v c O n Z / v e z u 7 e w d H B 2 c H L o 1 N P U a u r 0 5 p D 5 Y f X f g K t n / P c F 7 M R f s Z l I f g o y O U x v 8 u D k S R E W W Z c I m K W z 5 2 e a l 5 H t + p j 7 j q d 4 S Q y B v H c 0 3 8 P P I X g / F / z 1 a k 6 5 i D q q l 3 1 P N V f M S N s y Q O F / K l l E H 3 9 Z P 1 r n N X w e i 6 5 M B X Q c U F W g 0 v W L L O 5 H w y H l P w 3 j S D G 1 V c m b c n 8 I o F 5 u 3 9 2 O J m 3 c f x 9 v A 5 L R t K N M d m I v / 8 + e C x N n G H i O X s D 9 7 O k Z K m a q g c W / r Y o o Y j e S b r i U O P y l R H M + z 2 / Q G m n a X j b O x 9 C r J S S d X t k a c X r 3 n I l z K 9 m y o N 9 Z S 2 v g z h N a t b z g H D o V 1 L + P T y N A I m e E o Y s g 7 K 8 F n 7 A 5 7 j 6 c c m 8 i p U x c B l j f u Y h k Q H v C 1 E J k D S j F j A o f R Z 5 U o m e 1 C S K 0 e l V p C H E z A + p V 1 j m S A p j r c 6 9 8 a + b I w 9 E O L 5 n 8 k x f g 5 R q s u 5 6 c s 9 l y L m D h K L O U Z W 9 q b 7 B 3 j i p I 5 F e f + 4 t T F k k a u y j 2 C / C u 1 n E e k s d f a y 5 C E r c X Y I 0 4 Y L V Y i Q n q 8 5 8 h l h e Z y J l 5 Q 0 e + d c Q Q p a N v C j 1 E + w w v N i j O C v 9 3 j V z g Q c M s o F o H o B E 0 D M U M y g Z 5 D K 6 e y L g w P t n y n 1 2 x y I X l W v p B 7 0 L j t T i u + p + i 7 p K h Y 8 b P L x r c W v D L V q e F G 3 k 6 H 3 c x o U A 5 r i 5 5 X i E 6 7 k q Q 3 n 6 U 1 l A V I p 2 m s a 3 0 5 / Q / C d R X k M x n k L G Y T w t 6 i y b x u m 0 6 7 Q 6 3 Q H c U B m c l A E u w j a u t 1 9 E U + M j r g V G C T q x P J k z y O v u 7 x 0 b 8 / b k 2 d a 8 X R 9 t b P 4 w 2 n z y Z L 5 s 8 2 h h o 6 x 6 F p H P r i u f C 8 g S B l D M W 5 Y E a D t e 3 x h h h v j + i f x / 8 3 G v U w i s z s C 3 r K X Z 4 x / k / 3 + U 1 1 D A T a i H 1 k 5 l e L 7 d e N d O M 5 x b s b a B G Q k / H a P f t 1 P B p L v 3 7 T S / B l Y 8 3 B g / y e u p X E U e t 2 f z e c 9 n H s T a p 7 Q I 4 n Z t Y 9 k i s C y C I Y v Q z 8 g E G 0 M 2 U s L a Q I l Z L z k k 7 R T y a t b C O J l P z e o o a t d Q u 0 t U C h 2 V j q F H H R j q Y I k K 4 4 4 K 4 x 6 z b Z j t P o O b k L l A E f a 4 h n d U w 3 v M D R P t z Z i N h Q t H Z d x G 4 3 I c u 7 B / 7 b f + + H r s 9 w a Z R j i + d B z 1 r w i n 1 1 y S h / M + H d a 0 V K O j w x E K a r S O Z g + 8 Q + Q 8 D q H M A t w r Y 6 / L C H r 3 x a j F a i A t E k + U u F 0 1 K T d m O v N w d 2 E R z C A W 8 1 Z e l f f t B I + / 6 y 8 + 2 o S 4 G Q M z V t u q N W T H 0 m S 2 c H Y i G 0 N W o q y M z V l Z D V k E p R B l b o y 2 V G v J r h u 3 b 8 z 8 + y w C Y x H c Z x E a i / A + i 8 h Y R N I C l 2 E f R 5 f J E X q + h / b Z n a c X E I t P Y L n r A 4 8 l q K K s n Q a x p y I M n e y p N d G L B k U I I y 8 r b 4 A 9 l P v 7 W E t G z i r E a x u t X r + f p 9 j S E T r U v d v F x 9 4 e a g G T D x N y 6 b l c M O S 1 x z 3 j c a / v U d H i p j T n X N v s z s o 9 Y + T h g L r G p u n x q f Y j 6 r L 2 a O 3 x U r f R o o 8 5 e m S 7 e q y G c 6 p F / c X p Q O H r i + 8 i w J m P A Q d m Q n T v U 9 P 7 d K D 3 i e k l Q w d n Y R F k 4 8 X E 6 L N z N T O L E L x n a v o O c e e E v k v L 3 9 q j Z Y 8 0 a 5 b v R 8 u + / c I D X A T Z e W D K 4 J M e s z G 5 f 9 C O n 7 r C O w d P + u m 2 9 M 7 N 7 p C b p x 7 z b 2 j e e 8 4 e P n z o N 2 U a e T W X O 1 M a e 1 W J 1 S m W Z t p 1 l f k Y O Z 3 / + 6 / O C 2 c Y j M A G x i g Z 3 b 2 z + X 8 P s n O 0 v X C 0 / Y u O P H V r o 7 Z g b c u 1 D w U v r g i l Y m j j 6 u H D e 2 W C V 7 e 4 B R k Y p 5 9 1 s b s w + u J A L V d L I 3 1 q X D 0 d c G U E b 8 6 H g 1 j 4 + v J m c O Z 0 e v q L n Z Y m t c J N q x u 5 p T 6 J 6 s u V R 1 9 a F N 2 / r 9 7 j R f 9 j t 7 8 Z 6 e I E e N X y + N 4 L 7g Q H a S b F m s k D 3 M / R Q B 5 1 / v B O o G S K V k e a V 4 e d A V J B 3 i 7 V W V j 9 p c W c K r G d v k G D F X p k G 3 z o q r K 8 1 d R S i S i A i + E O i p k v R g Q V l y m u 4 i n e O E l U T i 2 6 K H O v 8 V n a 3 U Q q f W M Z 0 0 r P t 6 I o W Y 5 e v 5 G V z T d z M 5 2 s R / v E B C 4 T E B O 6 T E h M 5 D I R M e A y Q E z s M j E x i c s k x M x c Z k Z M 6 j I p M R 9 d 5 i M x 1 y 5 z T U z m M t l c y Z j l X s o x Y h n 4 0 Z 3 c 7 P Q K j j x 5 l + p F Z f E 7 4 R V + D i j H O 7 n z O A v j 5 Z 3 v w v V d 0 F l L l y m J q V y m I u a T y 3 w i h r k M I 4 a 7 D C d G u I w g p n a Z m p j G Z R p i b l z m h p h b l 7 k l 5 s 5 l 7 o j 5 7 D K f 5 7 p A M w G A m b l c b O 9 N F y T t 4 r 6 L w m Z x 3 X j r r q L E W K i 2 x V u c D Q c E W 7 H R h A R b g d F E B F t R 0 Q D B V k g 0 M c F W P D Q J w V Y w N D O C r U h o a o K t M G g + E m z F Q H N N s B U A T U Z w Z s E 5 w b k F W x N t z 3 B J s C X m p i L Y U n L z i W B L x g 0 j 2 N J w w w n m 9 q I S L I b n x J Z u Q 7 C l 2 + a G Y E u 0 z S 3 B l m K b O 4 I t u T a f C T Z a 3 c 1 A P j R T N 3 t s Q L e g R T e 4 L 4 N W 3 u D O D F p + g 3 s z a A 0 O 7 s 6 g h T i 4 P 4 N W 4 +A O D V q S g 3 s 0 a F 0 O 7 t L I 3 b t P g 1 b o 4 E 4 N W q a D e z V o r f Z 3 a 8 P l L p f b 3 L 0 7 M W j p D u 7 F o P U 7 u B u D F v H g f g x a y Y M 7 M m g 5 D + 7 J o D U 9 u C u D F v b g v g x a 3 Y M 7 M 2 i J D + 7 N o H U + u D u D F v v g / g x a 8 f f v 0 B g L L A 0 X F U r + l O L j K Y V N v k X w l g V v E 7 x t w T s E 7 1 j w L s G 7 F r x H 8 J 4 F P y P 4 m Q X v E 7 x v w Q c E H 1 j w c 4 K f W / A L g l 9 Y 8 I T g i Q U f E n x o w U c E H 1 n w S 4 J f W v A x w c c W / I r g V x Z 8 Q v C J B Z 8 S f G r B Z w S f W f A 5 w e c W f E H w h Q V f E n x p w V c E X 1 n w a 4 J f W / A b g t / c v 7 2 6 o g O t O k u j T y 3 9 K u l Z 3 J b N b b v c t s 3 t u N y O z e 2 6 3 K 7 N 7 b n c n s 0 9 c 7 l n N r f v c v s 2 d + B y B z b 3 3 O W e 2 9 w L l 3 t h c x O X m 9 j c o c s d 2 t y R y x 3 Z 3 E u X e 2 l z x y 5 3 b H O v X O 6 V z Z 2 4 3 I n N n b r c q c 2 d u d y Z z Z 2 7 3 L n N X b j c h c 1 d u t y l z V 2 5 3 J X N v X a 5 1 z b 3 x u W M 7 C / s E q L 5 D O o + A u 9 d 1 x d 9 m 7 K A 1 t z P G i y v N T T N K W k s a m K J u / V w U 6 U d X q W W 4 x w S Y 6 + O i c r 8 P I g M p x u W s 9 n C 2 8 x y 1 5 l b h o F G q N Z R l Q 4 i V O G o + g Y R q m u a b r R U z a h a B h G q Y V Q F g w h V L q p u Q Y T q l a a 7 S O s K P 2 q E a h N V m S B y b Y 1 c I 5 k 1 1 x q h 6 k P V H o g U 1 l R q p L Q m S S N U X 6 j q A h G q K l R N g Q j V E q q S Q I R b i 6 o R q h u a b u m t h W 8 0 Q j W C q h A Q o c p A 1 Q W I U D 2 g q g F E q A p Q N Q A i Q x W v W + o 2 f l b N 5 H q r z 4 X K m 6 A T o N K e A e l 2 j p 5 + d N R C Z k Z i h j D S 7 G R p Y C V 7 K X k D o E N E 8 H + C e J r k s q v 6 J N g E R x c Y i 4 G 0 r X 3 9 r R S r a a F Y Q 2 q h U C N r U K 0 U q G m h Q G N q o T g T a q E w Z 9 T C y 7 W u F Q X 5 k V o o x m t r b l o p w s X I W y l A 0 8 L J t G Y R x V d a U 9 J K 0 Z k W i u 4 T t V B w z J q p V g p t M U G t F J l p 4 U R b 0 4 w C a 6 i F 4 r q h F g r r l l o o q j t q o a A + z 7 u v 1 T C X 3 2 p c 5 X H U G e V v l b 0 R o a y t c j Y i l K t V p k a E M r T K z 4 h Q X l Z Z G R H K x i o X I 0 I 5 W G V g R C j z q r y L C O V b l W 0 R o S y r c i w i l F t V Z k W E M q r K p 4 h Q H l V Z F B H K n i p 3 I k I 5 U 2 V M R C h T q j y J C O V H l R 0 R o a y o c i I i l A t V J k S E M q D K f 4 h Q 3 l N Z D x H K d i r X I U I 5 T m U 4 R C i z q b y G C O U z l c 0 Q e W O t I O W e w E 4 9 + f E i W x x b 2 S K f 2 B l m 4 m a Y / L S L Y c m d 6 j h W K j q D g p d s 8 X Y D i m r 2 V O 5 A e M a 2 + x o / l Y 9 j o Q j L K C 0 S d O b X m U R 4 v D j O 5 y 2 X T 5 J P Q d z n I C i z 6 J f c B L f z V r 4 n 5 D 4 N L r q 3 E F R u 7 v y p J + H d 0 I S u Y Q t u q V 9 s G Y z 0 L 7 Y N R h E g d g x G M S B 2 D U Z R I P Y M R n E g n h m M I k H s G 4 x i Q R w Y j K J B P D c Y x Y N 4 Y T C K C D E x G M W E O D Q Y R Y U 4 M h j F h X h p M I o M c W w w i g 3 x y m A U H e L E Y B Q f 4 t R g F C H i z G A U I + L c Y B Q l 4 s J g F C f i 0 m A U K e L K Y B Q r 4 r X B K F r E G 4 P p q g + F / E y + 1 K T Z x N x L h 8 4 t T b J l w a S L Z N u C S R r J j g W T O p J d C y a B J H s W T B p J n l k w y S T Z t 2 B S S n J g w S S W 5 L k F k 1 6 S F x Z M k k k m F k y q S Q 4 t m I S T H F k w a S d 5 a c E k n + T Y g k l B y S s L J h E l J x Z M O k p O L Z i k l J x Z M K k p O b d g E l R y Y c G k q e T S g k l W y Z U F k 7 K S 1 x Z M 4 k r e W L C 5 q 8 C t r S v V + O J J T W C J i 2 8 R S t r i 2 4 S S t P g O o U p Z D 7 w d 9 W 1 J z c H z P Q 7 C w 1 N n E H m 7 I y + A 0 J e 4 m K X c u y n r L E I I W + B x 9 d 0 K 1 p I 1 6 9 5 n X N U v U 8 J t h b W l + s L Y f G 2 / R 2 c k d f J n h J I 4 + T 6 h p E 1 + Q C h J k z 8 n l J T J X x B K w u Q T Q k m X / J B Q k i U / I p R U y V 8 S S q L k x 4 S S J v k r Q k m S / I R Q U i Q / J Z Q E y c 8 I J T 3 y c 0 J J j</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>A a v L i J g 2 Z 1 8 M S r y h e 8 l U A D D b C P b K U e l B 7 V M P a 5 s K 2 k 6 b 6 s P 7 Z T l r W q o x C e 9 Q l 6 l L M W U 5 / R f v J 4 Y 3 K l 0 p 1 4 1 k S f B / N j z b d 5 C m f k C 7 9 T d U z i W x 7 b l 8 X z o Y v I y g u x L A 1 E G i 5 H o 1 t J 5 O q P j L x l V I s 0 i 6 C y n q r G 4 + k U P 3 C Z E G c 5 8 L t L Q 8 2 t R q j s o Y M 4 V 9 t 6 R r b T N 4 h q 7 L s s X E I F j p 5 s D d g w J 3 H S M n W 6 i F k L 1 D M 0 1 z v w q k y / M L 9 7 a m X S A / K l H d + x O r 9 t / d 7 7 I e L v 9 C 5 l w 6 8 W g S Z 8 9 m d u 5 v b d r 5 p U 1 x z 0 y Y 3 P z 3 M 0 l G C T z x Z O 0 P h U K G q N s p X E K r O + a l 7 H I / V u y N E D f D p N F q V 6 U 0 g / Z l r 1 U W S 1 H / 1 k + C X D Z F 5 O 5 / Z b U i 8 n S A l 7 4 j K 5 A N v r + B X 7 4 D N e e l Z b l 6 d I C b J c N 0 b I h B X p Z Z j H z c / l A a n Z T M i x Q u X / H v W 8 m 7 z e / k W 8 I y Z I 0 r g v 9 p q v 6 N Q V X b 6 h 9 M 4 U s s 2 z M A 9 E H 3 h Y m Q A z 5 Q v 5 3 h / E O u X x T T l b B 2 q l l L V 9 T L e t E 5 U x V F K c C R s o 9 L 7 2 o B O n u J r 1 O K 4 h S f 9 x 7 y 7 l k e S a / I J i 3 k / f r 8 w G y L E B y G 0 O c u F H 9 N o e 4 S j L V A K O 0 M H k / T Y t Y 3 P V D p / K Z f D i M 2 4 Y v g + U U c K / l f g J e W n h F 2 R X 0 A m 7 H 3 v a s 5 H J 6 S l k A h j N v B + 9 9 C / g d 9 + T v q s a r z u O c l 5 X c n U v 2 e 9 Q 4 S 9 Q F 4 O d 0 J I + + Z C j 3 S W 2 Y F l 8 y 5 H X V G c q j L x j i m D t D e T R o q O S P 5 1 X / 3 2 N x h g o 9 Y / 2 5 q 8 1 L h m X V z q d f G 4 1 G S / u T X I X u Z y 0 Q i 2 l 2 0 f 2 2 R b 4 Z P 2 U X 6 h c u 2 E X + l M c P c F f I y p u A g X + 9 + u G r t Y 3 + L y + X D y 4 2 x x t / H G + + e r z 2 0 1 b 3 q 8 z f r P z z y r + s f L u y s f L 9 y k 8 r + y v H K + c r 4 c p / r v z 3 y v + s / O + P h z / y H z / / + O / a 9 N e / 6 v r 8 0 4 r z 9 + N / / B + W t a y q &lt; / l a t e x i t &gt; dx " rf px, tq?gptq 2 r x log p t pxq sdt`gptq dw</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 1 :</head><label>1</label><figDesc>We can use an SDE to diffuse data to a simple noise distribution. This SDE can be reversed once we know the score of the marginal distribution at each intermediate time step, ? x log p t pxq.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 2 :</head><label>2</label><figDesc>Learning curves with the likelihood weighting on the CIFAR-10 dataset (smoothed with exponential moving average). Importance sampling significantly reduces the loss variance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>" 1 ? T 0 "</head><label>10</label><figDesc>into Eq.<ref type="bibr" target="#b27">(28)</ref> to obtain the following alternative estimatorHpp T pxqq?1 2N N ? i2??f px i , tq`gptq 2 s ? px i , tq 2 2 ? dt.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 3 :</head><label>3</label><figDesc>Samples on CIFAR-10. (a) Model with the best FID. (b) ScoreFlow trained with likelihood weighting + importance sampling + VP SDE. Samples of both models are generated with the same random seed.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>. 3 and 4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 4 :</head><label>4</label><figDesc>Samples on ImageNet 32?32. (a) Model with the best FID. (b) ScoreFlow trained with likelihood weighting + importance sampling + VP SDE. Samples of both models are generated with the same random seed.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>contribution. 35th Conference on Neural Information Processing Systems (NeurIPS 2021). arXiv:2101.09258v4 [stat.ML] 21 Oct 2021</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>SDEs and their corresponding weightings for score matching losses. Theorem 1. Let ppxq be the data distribution, ?pxq be a known prior distribution, and p SDE ? be defined as in Section 3. Suppose txptqu tPr0,T s is a stochastic process defined by the SDE in Eq. (1) with xp0q " p, where the marginal distribution of xptq is denoted as p t . Under some regularity conditions detailed in Appendix A, we have</figDesc><table><row><cell>SDE</cell><cell>Formula</cell><cell></cell><cell cols="3">?ptq in [48]</cell><cell>likelihood weighting</cell></row><row><cell>VE</cell><cell>dx " ?ptq dw</cell><cell></cell><cell cols="3">? 2 ptq</cell><cell>? 2 ptq</cell></row><row><cell>VP</cell><cell cols="2">dx "?1 2 ?ptqx dt`a?ptq dw</cell><cell cols="2">1?e??</cell><cell>t 0 ?psq ds</cell><cell>?ptq</cell></row><row><cell cols="2">subVP dx "?1 2 ?ptqx dt`b?ptqp1?e?2</cell><cell cols="2">? t 0 ?psq ds q dw p1?e??</cell><cell cols="2">t 0 ?psq ds q 2 ?ptqp1?e?2</cell><cell>? t 0 ?psq ds q</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>NLLs on CIFAR-10 and ImageNet 32x32.</figDesc><table><row><cell>Model</cell><cell cols="2">CIFAR-10 ImageNet</cell></row><row><cell>FFJORD [15]</cell><cell>3.40</cell><cell>-</cell></row><row><cell>Flow++ [18]</cell><cell>3.08</cell><cell>3.86</cell></row><row><cell>Gated PixelCNN [35]</cell><cell>3.03</cell><cell>3.83</cell></row><row><cell>VFlow [4]</cell><cell>2.98</cell><cell>3.83</cell></row><row><cell>PixelCNN++ [40]</cell><cell>2.92</cell><cell>-</cell></row><row><cell>NVAE [54]</cell><cell>2.91</cell><cell>3.92</cell></row><row><cell>Image Transformer [36]</cell><cell>2.90</cell><cell>3.77</cell></row><row><cell>Very Deep VAE [8]</cell><cell>2.87</cell><cell>3.80</cell></row><row><cell>PixelSNAIL [7]</cell><cell>2.85</cell><cell>3.80</cell></row><row><cell>?-VAE [38]</cell><cell>2.83</cell><cell>3.77</cell></row><row><cell>Sparse Transformer [9]</cell><cell>2.80</cell><cell>-</cell></row><row><cell>ScoreFlow (Ours)</cell><cell>2.83</cell><cell>3.76</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>"?? ? x?p h p px, tqp t pxqq log p t pxq dx piq " ? p t pxqh T p px, tq? x log p t pxq dx " 1 2 E x"ptpxq rgptq 2 ? x log p t pxq 2 2?2 f px, tq T ? x log p t pxqs,</figDesc><table><row><cell cols="3">Expanding the integrand, we have</cell><cell></cell><cell></cell></row><row><cell>B Bt</cell><cell>Hpp t pxqq "?B Bt</cell><cell>?</cell><cell cols="2">p t pxq log p t pxq dx</cell></row><row><cell></cell><cell>"??</cell><cell cols="2">Bp t pxq Bt</cell><cell>log p t pxq`B</cell><cell>p t pxq Bt</cell><cell>dx</cell></row><row><cell></cell><cell>"??</cell><cell cols="2">Bp t pxq Bt</cell><cell cols="2">log p t pxq dx?B Bt</cell><cell>? looooomooooon p t pxq dx</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>"1</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments and Disclosure of Funding</head><p>The authors would like to thank Sam Power, George Papamakarios, Adji Dieng for helpful feedback, and Duoduo for providing her photos in <ref type="figure">Fig. 1</ref>. This research was supported by NSF (#1651565, #1522054, #1733686), ONR (N000141912145), AFOSR (FA95501910024), ARO (W911NF-21-1-0125), Sloan Fellowship, and Google TPU Research Cloud. This research was also supported by the EPSRC Centre for Doctoral Training in Data Science, funded by the UK Engineering and Physical Sciences Research Council (grant EP/L016427/1), and the University of Edinburgh. Yang Song was supported by the Apple PhD Fellowship in AI/ML. Theorem 5. Let p 0t px 1 | xq denote the transition kernel from p 0 pxq to p t pxq for any t P p0, T s. With the same conditions and notations in Theorem 1, we hav?</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Proofs</head><p>We first summarize the notations and assumptions used in our theorems.</p><p>Notations The drift and diffusion coefficients of the SDE in Eq. <ref type="formula">(1)</ref> are denoted as f : R Dr 0, T s ? R D and g : r0, T s ? R respectively, where r0, T s represents a fixed time horizon, andd enotes the Cartesian product. The solution to Eq. (1) is a stochastic process txptqu tPr0,T s . We use p t to represent the marginal distribution of xptq, and p 0t px 1 | xq to denote the transition distribution from xp0q to xptq. The data distribution and prior distribution are given by p and ?. We use C to denote all continuous functions, and let C k denote the family of functions with continuous k-th order derivatives. For any vector-valued function h : R D?r 0, T s ? R D , we use ??hpx, tq to represent its divergence with respect to the first input variable.</p><p>Assumptions We make the following assumptions throughout the paper:</p><p>(iv) DC ? 0, @x, y P R D : f px, tq?f py, tq 2 ? C x?y 2 .</p><p>(v) g P C and @t P r0, T s, |gptq| ? 0. (xii) @t P r0, T s Dk ? 0 : p t pxq " Ope? x k 2 q as x 2 ? 8.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(vi) For any open bounded set</head><p>Below we provide all proofs for our theorems. Theorem 1. Let ppxq be the data distribution, ?pxq be a known prior distribution, and p SDE ? be defined as in Section 3. Suppose txptqu tPr0,T s is a stochastic process defined by the SDE in Eq. (1) with xp0q " p, where the marginal distribution of xptq is denoted as p t . Under some regularity conditions detailed in Appendix A, we have</p><p>Proof. We denote the path measure of txptqu tPr0,T s and tx ? ptqu tPr0,T s as ? and ? respectively. Due to assumptions (i) (ii) (iii) (iv) (v) (ix) and (x), both ? and ? are uniquely given by the corresponding SDEs. Consider a Markov kernel Kptzptqu tPr0,ts , yq :" ?pzp0q " yq. Since xp0q " p 0 and x ? p0q " p ? , we have the following result ? Kptxptqu tPr0,T s , xq d?ptxptqu tPr0,T s q " p 0 pxq ? Kptx ? ptqu tPr0,T s , xq d?ptx ? ptqu tPr0,T s q " p ? pxq.</p><p>Here the Markov kernel K essentially performs marginalization of path measures to obtain "sliced" distributions at t " 0. We can use the data processing inequality with this Markov kernel to obtain </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Reverse-Time Diffusion Equation Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">D</forename><surname>Anderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Stochastic Processes and their Applications</title>
		<imprint>
			<date type="published" when="1982" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="313" to="326" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Entropy and the Central Limit Theorem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Barron</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Probability</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="336" to="342" />
			<date type="published" when="1986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning Gradient Fields for Shape Generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Averbuch-Elor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Snavely</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Vflow: More expressive generative flows with variational data augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chenli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1660" to="1669" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wavegrad</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.00713</idno>
		<title level="m">Estimating Gradients for Waveform Generation</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Neural Ordinary Differential Equations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rubanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bettencourt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">K</forename><surname>Duvenaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6571" to="6583" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Pixelsnail: An improved autoregressive generative model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohaninejad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="864" to="872" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Very deep VAEs generalize autoregressive models and can outperform them on images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Generating long sequences with sparse transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.10509</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Good semi-supervised learning that requires a bad GAN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Neural Information Processing Systems</title>
		<meeting>the 31st International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6513" to="6523" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nichol</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.05233</idno>
		<title level="m">Diffusion models beat GANs on image synthesis</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1410.8516</idno>
		<title level="m">NICE: Non-Linear Independent Components Estimation</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Density estimation using real NVP</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">5th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Toulon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-04-24" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings. OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A family of embedded runge-kutta formulae</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Dormand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Prince</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of computational and applied mathematics</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="19" to="26" />
			<date type="published" when="1980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">FFJORD: Free-form Continuous Dynamics for Scalable Reversible Generative Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Grathwohl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">T Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bettencourt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Duvenaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Time reversal of diffusions. The Annals of Probability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><forename type="middle">G</forename><surname>Haussmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Pardoux</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1986" />
			<biblScope unit="page" from="1188" to="1205" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">GANs trained by a two time-scale update rule converge to a local Nash equilibrium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Nessler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems</title>
		<editor>I. Guyon, U. von Luxburg, S. Bengio, H. M. Wallach, R. Fergus, S. V. N. Vishwanathan, and R. Garnett</editor>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-12-04" />
			<biblScope unit="page" from="6626" to="6637" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Flow++: Improving flow-based generative models with variational dequantization and architecture design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2722" to="2730" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Denoising Diffusion Probabilistic Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Compression with flows via local bits-back coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Lohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems</title>
		<editor>H. M. Wallach, H. Larochelle, A. Beygelzimer, F. d&apos;Alch?-Buc, E. B. Fox, and R. Garnett</editor>
		<meeting><address><addrLine>NeurIPS; Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-12-08" />
			<biblScope unit="page" from="3874" to="3883" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Integer discrete flows and lossless compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hoogeboom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W T</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems</title>
		<editor>H. M. Wallach, H. Larochelle, A. Beygelzimer, F. d&apos;Alch?-Buc, E. B. Fox, and R. Garnett</editor>
		<meeting><address><addrLine>NeurIPS; Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-12-08" />
			<biblScope unit="page" from="12134" to="12144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A stochastic estimator of the trace of the influence matrix for laplacian smoothing splines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>Hutchinson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications in Statistics-Simulation and Computation</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1059" to="1076" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Estimation of Non-Normalized Statistical Models by Score Matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hyv?rinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="695" to="709" />
			<date type="published" when="2005-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Barron</surname></persName>
		</author>
		<title level="m">Fisher Information inequalities and the Central Limit Theorem. Probability Theory and Related Fields</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">129</biblScope>
			<biblScope unit="page" from="391" to="409" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Distribution augmentation for generative modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5006" to="5019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Auto-Encoding Variational Bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ping</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Catanzaro</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.09761</idno>
		<title level="m">DiffWave: A Versatile Diffusion Model for Audio Synthesis</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<ptr target="http://www.cs.toronto.edu/kriz/cifar.html" />
		<title level="m">The CIFAR-10 Dataset</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">55</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Some properties of path measures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>L?onard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">S?minaire de Probabilit?s XLVI</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="207" to="230" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Scalable Gradients for Stochastic Differential Equations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-K</forename><forename type="middle">L</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">T Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Duvenaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd International Conference on Artificial Intelligence and Statistics</title>
		<meeting>the 23rd International Conference on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Interpretation and Generalization of Score Matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lyu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th Conference on Uncertainty in Artificial Intelligence</title>
		<meeting>the 25th Conference on Uncertainty in Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="359" to="366" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Interacting particle solutions of Fokker-Planck equations through gradient-log-density estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Maoutsa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Opper</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.00702</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Improved denoising diffusion probabilistic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nichol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename></persName>
		</author>
		<idno type="arXiv">arXiv:2102.09672</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Stochastic differential equations: an introduction with applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>Springer Science &amp; Business Media</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Conditional image generation with pixelcnn decoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">V</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th International Conference on Neural Information Processing Systems</title>
		<meeting>the 30th International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4797" to="4805" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Image transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4055" to="4064" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Grad-TTS: A diffusion probabilistic model for text-to-speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Popov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Vovk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Gogoryan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sadekova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kudinov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.06337</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Preventing posterior collapse with delta-vaes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Razavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">7th International Conference on Learning Representations</title>
		<meeting><address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-05-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Stochastic Backpropagation and Approximate Inference in Deep Generative Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Machine Learning</title>
		<meeting>the 31st International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Pixelcnn++: Improving the pixelcnn with discretized logistic mixture likelihood and other modifications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">5th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Toulon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-04-24" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings. OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B</forename><surname>Dieng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.14859</idno>
		<title level="m">Consistency regularization for variational auto-encoders</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">The eigenvalues of mega-dimensional matrices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Skilling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Maximum Entropy and Bayesian Methods</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1989" />
			<biblScope unit="page" from="455" to="466" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Deep Unsupervised Learning Using Nonequilibrium Thermodynamics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Maheswaranathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ganguli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2256" to="2265" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Generative Modeling by Estimating Gradients of the Data Distribution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ermon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="11918" to="11930" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Improved Techniques for Training Score-Based Generative Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ermon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Sliced Score Matching: A Scalable Approach to Density and Score Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ermon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-Fifth Conference on Uncertainty in Artificial Intelligence</title>
		<meeting>the Thirty-Fifth Conference on Uncertainty in Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">204</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Pixeldefend: Leveraging generative models to understand and defend against adversarial examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ermon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kushman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Score-Based Generative Modeling Through Stochastic Differential Equations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ermon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Poole</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Some Inequalities Satisfied by the Quantities of Information of Fisher and Shannon</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Stam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information and Control</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="101" to="112" />
			<date type="published" when="1959-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">A note on the evaluation of generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bethge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">4th International Conference on Learning Representations</title>
		<editor>Y. Bengio and Y. LeCun</editor>
		<meeting><address><addrLine>San Juan, Puerto Rico</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-05-02" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Practical lossless compression with latent variables using bits back coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Townsend</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Bird</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Barber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Tzen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Raginsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.09883</idno>
		<title level="m">Neural Stochastic Differential Equations: Deep Latent Gaussian Models in the Diffusion Limit</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Rnade: the real-valued neural autoregressive densityestimator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Uria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th International Conference on Neural Information Processing Systems</title>
		<meeting>the 26th International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="2175" to="2183" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">NVAE: A deep hierarchical variational autoencoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vahdat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz ; H. Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Balcan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Pixel recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1747" to="1756" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">A Connection Between Score Matching and Denoising Autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1661" to="1674" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.06191</idno>
		<title level="m">A study of face obfuscation in imagenet</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Recall that by definition xpT q &quot; p T andx ? pT q &quot; ?. Leveraging the chain rule of KL divergences (see, for example, Theorem 2</title>
		<imprint/>
	</monogr>
	<note>4 in [29]), we have (a) DDPM++ (deep, subVP) [48], FID = 2.86</note>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Scoreflow</surname></persName>
		</author>
		<idno>FID = 5.34</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">DDPM++ (VP) [48], FID = 8.34 (b) ScoreFlow</title>
		<idno>FID = 10.18</idno>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
