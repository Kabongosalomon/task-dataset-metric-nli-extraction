<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multi-Temporal Land Cover Classification with Sequential Recurrent Encoders</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Ru?wurm</surname></persName>
							<email>marc.russwurm@tum.de</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Chair of Remote Sensing Technology</orgName>
								<orgName type="department" key="dep2">TUM Department of Civil</orgName>
								<orgName type="department" key="dep3">Geo and Environmental Engineering</orgName>
								<orgName type="institution">Technical University of Munich</orgName>
								<address>
									<addrLine>Arcisstra?e 21</addrLine>
									<postCode>80333</postCode>
									<settlement>Munich</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>ID</roleName><forename type="first">Marco</forename><surname>K?rner</surname></persName>
							<email>marco.koerner@tum.de</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Chair of Remote Sensing Technology</orgName>
								<orgName type="department" key="dep2">TUM Department of Civil</orgName>
								<orgName type="department" key="dep3">Geo and Environmental Engineering</orgName>
								<orgName type="institution">Technical University of Munich</orgName>
								<address>
									<addrLine>Arcisstra?e 21</addrLine>
									<postCode>80333</postCode>
									<settlement>Munich</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Multi-Temporal Land Cover Classification with Sequential Recurrent Encoders</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.3390/ijgi7040129</idno>
					<note type="submission">Received: 22 January 2018; Accepted: 17 March 2018; Published: 21 March 2018; Updated on ArXiv: 7 April 2018</note>
					<note>Article * Correspondence:</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T10:40+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>deep learning</term>
					<term>multi-temporal classification</term>
					<term>land use and land cover classification</term>
					<term>recurrent networks</term>
					<term>sequence encoder</term>
					<term>crop classification</term>
					<term>sequence-to-sequence</term>
					<term>Sentinel 2</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Earth observation (EO) sensors deliver data at daily or weekly intervals. Most land use and land cover classification (LULC) approaches, however, are designed for cloud-free and mono-temporal observations. The increasing temporal capabilities of today's sensors enable the use of temporal, along with spectral and spatial features.Domains such as speech recognition or neural machine translation, work with inherently temporal data and, today, achieve impressive results by using sequential encoder-decoder structures. Inspired by these sequence-to-sequence models, we adapt an encoder structure with convolutional recurrent layers in order to approximate a phenological model for vegetation classes based on a temporal sequence of Sentinel 2 (S2) images.</p><p>In our experiments, we visualize internal activations over a sequence of cloudy and non-cloudy images and find several recurrent cells that reduce the input activity for cloudy observations. Hence, we assume that our network has learned cloud-filtering schemes solely from input data, which could alleviate the need for tedious cloud-filtering as a preprocessing step for many EO approaches. Moreover, using unfiltered temporal series of top-of-atmosphere (TOA) reflectance data, our experiments achieved state-of-the-art classification accuracies on a large number of crop classes with minimal preprocessing, compared to other classification approaches.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Land use and land cover classification (LULC) has been a central focus of Earth observation (EO) since the first air-and space-borne sensors began to provide data. For this purpose, optical sensors sample the spectral reflectivity of objects on the Earth's surface in a spatial grid at repeated intervals. Hence, LULC classes can be characterized by spectral, spatial and temporal features. Today, most classification tasks focus on spatial and spectral features <ref type="bibr">[? ]</ref>, while utilizing the temporal domain had long proven challenging. This is mostly due to limitations on data availability, the cost of data acquisition, infrastructural challenges regarding data storage and processing and the complexity of model design and feature extraction over multiple time frames.</p><p>Some LULC classes, such as urban structures, are mostly invariant to temporal changes and, hence, are suitable for mono-temporal approaches.</p><p>Others, predominantly vegetation-related classes, change their spectral reflectivity based on biochemical processes initiated by phenological events related to the type of vegetation and to environmental conditions. These vegetation-characteristic phenological transitions have been utilized for crop yield prediction and, to some extent, for classification <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>. However, to circumvent the previously-mentioned challenges, the dimensionality of spectral bands has often been compressed by calculating task-specific indices, such as the normalized difference vegetation index (NDVI), the normalized difference water index (NDWI) or the enhanced vegetation index (EVI).</p><p>Today, most of these temporal data limitations have been alleviated by technological advances. Reasonable spatial and temporal resolution data of multi-spectral Earth observation sensors are available at no cost. Moreover, new services inexpensively provide high temporal and spatial resolution imagery. The cost of data storage has decreased, and data transmission has become sufficiently fast to allow gathering and processing all available images over a large area and multiple years. Finally, new advances in machine learning, accompanied by GPU-accelerated hardware, have made it possible to learn complex functional relationships, solely from the data provided.</p><p>Since now data are available at high resolutions and processing is feasible, the temporal domain should be exploited for EO approaches. However, this exploitation requires suitable processing techniques utilizing all available temporal information at reasonable complexity. Other domains, such as machine translation <ref type="bibr" target="#b2">[3]</ref>, text summarization <ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref> or speech recognition <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8]</ref>, handle sequential data naturally. These domains have popularized sequence-to-sequence learning, which transforms a variable-length input sequence to an intermediate representation. This representation is then decoded to a variable-length output sequence. From this concept, we adopt the sequential encoder structure and extract characteristic temporal features from a sequence of Sentinel 2 (S2) images using a straightforward, two-layer network.</p><p>Thus, the main contributions of this work are:</p><p>(i) the adaptation of sequence encoders from the field of sequence-to-sequence learning to Earth observation (EO), (ii) a visualization of internal gate activations on a sequence of satellite observations and, (iii) the application of crop classification over two seasons.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>As we aim to apply our network to vegetation classes, we first introduce common crop classification approaches, to which we will compare our results in Section 6. Then, we motivate data-driven learning models and cover the latest work on recurrent network structures in the EO domain.</p><p>Many remote sensing approaches have achieved adequate classification accuracies for multi-temporal crop data by using multiple preprocessing steps in order to improve feature separability. Common methods are atmospheric correction <ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref>, calculation of vegetation indices <ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref> or the extraction of sophisticated phenological features <ref type="bibr" target="#b11">[12]</ref>. Additionally, some approaches utilize expert knowledge, for instance, by introducing additional agro-meteorological data <ref type="bibr" target="#b8">[9]</ref>, by selecting suitable observation dates for the target crop-classes <ref type="bibr" target="#b12">[13]</ref> or by determining rules for classification <ref type="bibr" target="#b9">[10]</ref>. Pixel-based <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b11">12]</ref> and object-based <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b12">13]</ref> approaches have been proposed. Commonly, decision trees (DTs) <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b12">13]</ref> or random forests (RFs) <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12]</ref> are used as classifiers, the rules of which are sometimes aided by additional expert knowledge <ref type="bibr" target="#b9">[10]</ref>.</p><p>These traditional approaches generally trade procedural complexity and the use of region-specific expert knowledge for good classification accuracies in the respective areas of interest (AOIs). However, these approaches are, in general, difficult to apply to other regions. Furthermore, the processing structure requires supervision to varying degrees (e.g., product selection, visual image inspection, parameter tuning), which impedes application at larger scales.</p><p>Today, we are experiencing a change in paradigm: away from the design of physically-interpretable, human-understandable models, which require task-specific expert knowledge, towards data-driven models, which are encoded in internal weight parameters and derived solely from observations. In that regard, hidden Markov models (HMMs) <ref type="bibr" target="#b13">[14]</ref> and conditional random fields (CRFs) <ref type="bibr" target="#b14">[15]</ref> have shown promising classification accuracies with multi-temporal data. However, the underlying Markov property limits long-term learning capabilities, as Markov-based approaches assume that the present state only depends on the current input and one previous state.</p><p>Deep learning methods have had major success in fields, such as target recognition and scene understanding <ref type="bibr">[? ]</ref>, and are increasingly adopted by the remote sensing community. These methods <ref type="table">Table 1</ref>. Update formulas of the convolutional variants of standard recurrent neural networks (RNNs), long short-term memory (LSTM) cells and gated recurrent units (GRUs). A convolution between matrices a and b is denoted by a * b, element-wise multiplication by the Hadamard operator a b, and concatenation on the last dimension is marked by [a b]. The activation functions sigmoid ?(x) and tangens hyperbolicus tanh(x) are used for non-linear scaling.</p><p>Gate Variant RNN LSTM <ref type="bibr" target="#b24">[25]</ref> GRU <ref type="bibr" target="#b25">[26]</ref> </p><formula xml:id="formula_0">h t ? x t , h t?1 h t , c t ? x t , h t?1 , c t?1 h t ? x t , h t?1</formula><p>Forget/Reset</p><formula xml:id="formula_1">f t ? ?([x t h t?1 ] * W f + 1) r t ? ?([x t h t?1 ] * W r ) Insert/Update i t ? ?([x t h t?1 ] * W i ) u t ? ?([x t h t?1 ] * W u ) j t ? ?([x t h t?1 ] * W j ) Output o t ? ?([x t h t?1 ] * W o )h t ? [x t r t h t?1 ] * Wh c t ? c t?1 f t + i t j t h t ? ?([x t h t?1 ] * W ) h t ? o t tanh(c t ) h t ? u t h t?1 + (1 ? u t ) tanh(h t )</formula><p>have proven particularly beneficial for modeling physical relationships that are complicated, cannot be generalized or are not well-understood <ref type="bibr" target="#b15">[16]</ref>. Thus, deep learning is potentially well suited to approximate models of phenological changes, which depend on complex internal biochemical processes of which only the change of surface reflectivity can be observed by EO sensors. A purely data-driven approach might alleviate the need to manually design a functional model for this complex relationship. However, caution is required, as external and non class-relevant factors, such as seasonal weather or observation configurations, are potentially incorporated into the model, which might remain undetected if these factors constantly bias the dataset. In remote sensing, convolutional networks have gained increasing popularity for mono-temporal observation tasks <ref type="bibr" target="#b16">[17]</ref><ref type="bibr" target="#b17">[18]</ref><ref type="bibr" target="#b18">[19]</ref><ref type="bibr" target="#b19">[20]</ref>. However, for sequential tasks, recurrent network architectures, which provide an iterative framework to process sequential information, are generally better suited. Recent approaches utilize recurrent architectures for change detection <ref type="bibr">[21?</ref> ? ], identification of sea level anomalies <ref type="bibr" target="#b21">[22]</ref> and land cover classification <ref type="bibr" target="#b22">[23]</ref>. For long-term dependencies, Jia et al. <ref type="bibr" target="#b20">[21]</ref> proposed a new cell architecture, which maintains two separate cell states for single-and multi-seasonal long-term dependencies. However, the calculation of an additional cell state requires more weights, which may prolong training and require more training samples.</p><p>In previous work, we have experimented with recurrent networks for crop classification <ref type="bibr" target="#b23">[24]</ref> and achieved promising results. Based on this, we propose a network structure using convolutional recurrent layers and the aforementioned adaptation of a many-to-one classification scheme with sequence encoders.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head><p>Section 3.1 incrementally introduces the concepts of artificial neural networks (ANNs), feed-forward networks (FNNs), and recurrent neural networks (RNNs) and illustrates the use of RNNs in sequence-to-sequence learning. We then describe details of the proposed network structure in Section 3.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Network Architectures and Sequential Encoders</head><p>Artificial neural networks approximate a function? = f (x; W ) of outputs? (e.g., class labels) from input data x given a large set of weights W. This approximation is commonly referred to as the inference phase. These networks are typically composed of multiple cascaded layers with hidden vectors h as intermediate layer outputs. Analogous to the biological neural cortex, single elements</p><formula xml:id="formula_2">f t i t j t o t + x t h t?1 c t c t?1 h t r t u th t + h t?1 x t h t a b [a b]</formula><p>concat a a a copy <ref type="figure">Figure 1</ref>. Schematic illustration of long short-term memory (LSTM) and gated recurrent unit (GRU) cells analog to the cell definitions in <ref type="table">Table 1</ref>. The cell output h t is calculated via internal gates and based on the current input x t combined with prior context information h t?1 , c t?1 . LSTM cells are designed to separately accommodate long-term context in the internal cell state c t?1 , from short-term context h t?1 .</p><p>GRU cells combine all context information in a single, but more sophisticated output h t?1 .</p><p>in these vectors are often referred to as neurons. The quality of the approximation? with respect to ground truth y is determined by the loss function L(?, y). Based on this function, gradients are back-propagated through the ANN and adjust network weights W at each training step. Popular feed-forward networks often utilize convolutional or fully-connected layers at which the input data are propagated through the network once. This is realized by an affine transformation (fully-connected) h = ?(W x) or a convolution h = ?(W * x) followed by an element-wise, non-linear transformation ? : R ? R.</p><p>However, domains like translation <ref type="bibr" target="#b2">[3]</ref>, text summarization <ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref> or speech recognition <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8]</ref> formulate input vectors naturally as a sequence of observations x = {x 0 , . . . , x T }. In these domains, individual samples are generally less expressive, and the overall model performance is based largely on contextual information.</p><p>Sequential data are commonly processed with recurrent neural network (RNN) layers, in which the hidden layer output h t is determined at time t by current input x t in combination with the previous output h t?1 . In theory, the iterative update of h t enables RNNs to simulate arbitrary procedures <ref type="bibr" target="#b26">[27]</ref>, since these networks are Turing complete <ref type="bibr" target="#b27">[28]</ref>. The standard RNN variant performs the update step h t = ?(Wx) by an affine transformation of the concatenated vectorx = [x t h t?1 ] followed by a non-linearity ?. Consequently, the internal weight matrix is multiplied at each iteration step, which essentially raises it to a high power <ref type="bibr" target="#b28">[29]</ref>. At gradient back-propagation, this iterative matrix multiplication leads to vanishing and exploding gradients <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b30">31]</ref>. While exploding gradients can be avoided with gradient clipping, vanishing gradients impede the extraction of long-term feature relationships. This issue has been addressed by Hochreiter and Schmidhuber <ref type="bibr" target="#b24">[25]</ref>, who introduced additional gates and an internal state vector c t in long short-term memory (LSTM) cells to control the gradient propagation through time and to enable long-term learning, respectively. Analogous to standard RNNs, the output gate o t balances the influence of the previous cell output h t?1 and the current input x t . At LSTMs, the cell output h t is further augmented by an internal state vector c t , which is designed to contain long-term information. To avoid the aforementioned vanishing gradients, reading and writing to the cell state is controlled by three additional gates. The forget gate f t decreases previously-stored information by element-wise multiplication c t?1 f t . New information is added by the product of input gate i t and modulation gate j t . Illustrations of the internal calculation can be seen in <ref type="figure">Figure 1</ref>, and the mathematical relations are shown in <ref type="table">Table 1</ref>. Besides LSTMs, gated recurrent units (GRUs) <ref type="bibr" target="#b25">[26]</ref> have gained increasing popularity, as these cells achieve similar accuracies to LSTMs with fewer trainable parameters. Instead of separate vectors for long-and short-term memory, GRUs formulate a single, but more sophisticated, output vector.   <ref type="bibr" target="#b23">[24]</ref> shown in <ref type="figure" target="#fig_1">Figure 2</ref>(a) creates a prediction y t at each observation t based on spectral input information x t and the previous context h t?1 , c t?1 . Sequence-to-sequence networks, as shown in <ref type="figure" target="#fig_1">Figure 2</ref>(b), aggregate sequential information to an intermediate state c T which is a representation of the entire series.</p><p>To account for the more complicated design, recurrent layers are conventionally referred to as a collection of cells with a single cell representing the set of elements at one vector-index.</p><p>The common output of recurrent layers provides a many-to-many relation by generating an output vector at each observation h t given previous context h t?1 and c t?1 , as shown in <ref type="figure">Figure ?</ref>?a. However, encoding information of the entire sequence in a many-to-one relation is favored in many applications. Following this idea, sequence-to-sequence learning, illustrated in <ref type="figure">Figure ?</ref>?b, has popularized the use of the cell state vector c T at the last-processed observation T as a representation of the entire input sequence. These encoding-decoding networks transform an input sequence of varying length to an intermediate state representation c of fixed size. Subsequently, the decoder generates a varying length output sequence from this intermediate representation. Further developments in this domain include attention schemes. These provide additional intermediate connections between encoder and decoder layers, which are beneficial for translations of longer sequences <ref type="bibr" target="#b2">[3]</ref>.</p><p>In many sequential applications, the common input form is x t ? R d with a given depth d. The output vectors h t ? R r are computed by matrix multiplication with internal weights W ? R (r+d)?r and r recurrent cells. However, other fields, such as image processing, commonly handle raster data x t ? R h?w?d of specific width w, height h and spectral depth d. To account for neighborhood relationships and to circumvent the increasing complexity, convolutional variants of LSTMs <ref type="bibr" target="#b31">[32]</ref> and GRUs have been introduced. These variants convolve the input tensors with weights W ? R k?k?(r+d)?r augmented by the convolutional kernel size k, which is a hyper-parameter determining the perceptive field.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Prior Work</head><p>Given recurrent networks as popular architectures for sequential data processing, we experimented with recurrent layers for multi-temporal vegetation classification prior to this work <ref type="bibr" target="#b23">[24]</ref>. In the conducted experiments, we used a network architecture similar to the illustration in <ref type="figure">Figure ?</ref>?a. Following the input dimensions of standard recurrent layers, an input sequence x ? {x 0 ,. . . , x T } of observations x t ? R d was introduced to the network. Based on contextual information from previous observations, a classification for each observation y t was produced. We evaluated the effect of this information gain by comparing the recurrent network with convolutional neural networks (CNNs) and a support vector machine (SVM). Standard RNNs and LSTMs outperformed their non-sequential SVMs and CNNs counterparts. Further, we observed an increase in accuracy at sequentially later observations, which were classified with more context information available. Overall, we concluded that recurrent network architectures are well suited for the extraction of temporal features from multi-temporal EO imagery, which is consistent with other recent findings <ref type="bibr" target="#b20">[21]</ref><ref type="bibr" target="#b21">[22]</ref><ref type="bibr" target="#b22">[23]</ref>.</p><p>However, the experimental setup introduced some limitations regarding applicability in real-world scenarios. We followed the standard formulation of recurrent networks, which process a d-dimensional input vector. This vector included the concatenated bottom-of-atmosphere (BOA) reflectances of nine pixels neighboring one point-of-interest. The point-wise classification was sufficient for quantitative accuracy evaluation, but could not produce areal classification maps. Since a class prediction was performed on every observation, we introduced additional covered classes for cloudy pixels at single images. These were derived from the scene classification of the SEN2COR atmospheric correction algorithm, which required additional preprocessing. A single representative classification for the entire time-series would have required additional post-processing to further aggregate the predicted labels for each observation. Finally, the mono-directional iterative processing introduced a bias towards last observations. With more contextual information available, later observation showed better classification accuracies compared to observations earlier in the sequence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">This Approach</head><p>To address the limitations of previous work, we redesigned and streamlined the network structure and processing pipeline. Inspired by sequence-to-sequence structures described in Section 3, the proposed network aggregates the information encoded in the cell state c t within the recurrent cell. Since one class prediction for the entire temporal series is produced, atmospheric perturbations can be treated as temporal noise. Hence, explicitly introduced cloud-related labels are not required, which alleviates the need for prior cloud classification. Without the need for prior scene classification to obtain these classes, the performance on atmospherically uncorrected top-of-atmosphere (TOA) reflectance data can be evaluated. We further implemented convolutional recurrent cell variants, as formulated in <ref type="table">Table 1</ref>, to process input tensors x t of given height h, width w and depth d. Hence, the proposed network produces areal prediction maps as shown in the qualitative results Section 5.3. Finally, we introduce the input sequence in a bidirectional manner to eliminate any bias towards the later elements in the observation sequence.</p><p>Overall, we employ a bidirectional sequential encoder for the task of multi-temporal land cover classification. As Earth observation data are gathered in a periodic manner, many observations of the same area at consecutive times are available, which may contribute to the classification decision. Inspired by sequence-to-sequence models, the proposed model encodes this sequence of images into a fixed-length representation. Compared to previous work, this is an elegant way to condense the available temporal dimension without further post-processing. A classification map for each class is derived from this sequence representation. Many optical observations are covered by clouds, and prior cloud classification is often required as additional preprocessing step. As clouds do not contribute to the classification decision, these observations can be treated as temporal noise and may be potentially ignored by this encoding scheme. In Section 5.1, we investigate this by visualizing internal activation states on cloudy and non-cloudy observations. <ref type="figure" target="#fig_3">Figure 3</ref> presents the proposed network structure schematically. The input image sequence x = {x t , . . . , x T } of observations x ? R h?w?d is passed to gated recurrent layers at each observation time t. The index T denotes the maximum length of the sequence and d the input feature depth. In practice, sequence lengths are often shorter than T, as the availability of satellite acquisitions is variable over larger scales. If less than T observations are present, sequence elements are padded with a constant value and are subsequently ignored at the iterative encoding steps. To eliminate bias towards the last observations in the sequence, the data are passed to the encoder in both sequential (seq) and reversed (rev) order. Network weights are shared between both passes. The initial cell states c . . </p><formula xml:id="formula_3">x T x T x T?1 . . . x 0 h seq 0 = 0 c seq 0 = 0 h rev T = 0 c rev T = 0 c seq T c rev 0 prediction label argmax H(y,?)</formula><formula xml:id="formula_4">? {x 0 , . . . , x T } of observations x t ? R h?w?d is encoded to a representation c T = [c seq T c inv 0 ]</formula><p>. The observations are passed in sequence (seq) and reversed (rev) order to the encoder to eliminate bias towards recent observations. The concatenated representation of both passes c T is then projected to softmax-normalized feature maps for each class using a convolutional layer. c T to softmax-normalized activation maps? for n classes: c T ? R h?w?2r ?? ? R h?w?n . This layer is composed of a convolution with a kernel size of k class , followed by batch normalization and a rectified linear unit (ReLU) <ref type="bibr" target="#b33">[34]</ref> or leaky ReLU <ref type="bibr" target="#b32">[33]</ref> non-linear activation function. At each training step, the cross-entropy loss</p><formula xml:id="formula_5">H(?, y) = ? ? i y i log(? i )<label>(1)</label></formula><p>between the predicted activations? and an one-hot representation of the ground truth labels y evaluates the prediction quality. Tunable hyper-parameters are the number of recurrent cells r and the sizes of the convolutional kernel k rnn and the classification kernel k class .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Dataset</head><p>For the evaluation of our approach, we defined a large area of interest (AOI) of 102 km ? 42 km north of Munich, Germany. An overview of the AOI at multiple scales is shown in <ref type="figure" target="#fig_4">Figure 4</ref>. The AOI was further subdivided into squared blocks of 3.84 km ? 3.84 km (multiples of 240 m and 480 m) to ensure dataset independence while maintaining similar class distributions. These blocks were then randomly assigned to partitions for network training, hyper-parameter validation and model evaluation in a ratio of 4:1:1 similar to previous work <ref type="bibr" target="#b23">[24]</ref>. The spatial extent of single samples x is determined by tile-grids of 240 m and 480 m. We bilinearly interpolated the 20 m and 60 m S2 bands to 10 m ground sampling distance (GSD) to harmonize the raster data dimensions. With ground truth labels of two growing seasons 2016 and 2017 available, we gathered 274 (108 in 2016; 166 in 2017) Sentinel 2 products at 98 (46 in 2017; 52 in 2017) observation dates between 3 January 2016 and 15 November 2017. The obtained time series represents all available S2 products labeled with cloud coverage less than 80%. In some S2 images, we noticed a spatial offset in the scale of one pixel. However, we did not perform additional georeferencing and treated the spatial offset as data-inherent observation noise. Overall, we relied on the geometrical and spectral reference as provided by the COPERNICUS ground segment.</p><p>Ground truth information was provided by the Bavarian Ministry of Food, Agriculture and Forestry (StMELF) in the form of geometry and semantic labels of 137 k field parcels. The crop-type is reported by farmers to the ministry as mandated by the European crop subsidy program. We selected and aggregated 17 crop-classes from approximately 200 distinct field labels, occurring at least 400 times in the AOI. With modern agriculture, centered on a few predominant crops, the distribution of classes is not uniform, as can be observed from <ref type="figure" target="#fig_5">Figure 5a</ref>. This non-uniform class distribution is generally not optimal for the classification evaluation as it skews the overall accuracy metric towards classes of high frequency. Hence, we additionally calculated kappa metrics <ref type="bibr" target="#b35">[36]</ref> for the quantitative evaluation in Section 5.2 to compensate for unbalanced distributions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Results</head><p>In this section, we first visualize internal state activations in Section 5.1 to gain a visual understanding of the sequential encoding process. Further findings on internal cloud masking are presented before the classification results on crop classes are quantitatively and qualitatively evaluated in Sections 5.2 and 5.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Internal Network Activations</head><p>In Section 3.1, we gave an overview of the functionality of recurrent layers and discussed the property of LSTM state vectors c t ? R h?w?r to encode sequential information over a series of observations. The cell state is updated by internal gates i t , j t , f t ? R h?w?r , which in turn are calculated based on previous cell output h t?1 and cell state c t?1 (see <ref type="table">Table 1</ref>). To assess prior assumptions regarding cloud filtering and to visually assess the encoding process, we visualized internal LSTM cell tensors for a sequence of images and show representative activations of three cells in <ref type="figure">Figure 6</ref>. The LSTM network, from which these activations are extracted, was trained on 24 px ? 24 px tiles with r = 256 recurrent cells and k rnn = k class = 3 px. Additionally, we inferred the network with tiles of height h and width w of 48 px. Experiments with the input size of 24 px show similar results and are included in the Supplementary Material to this work. In the first row, a 4? band-normalized RGB image represents the input satellite image x t ? R h=48 ? R w=48 ? R d=15 at each time frame t. The next rows show the activations of input gate i i t , modulation gate j i t , forget gate f i t and cell state c i t at three selected recurrent cells, which are denoted by the raised index i ? {3, 22, 47}. After iteratively processing the sequence, the final cell state c T=36 is used to produce activations for each class, as described in Section 3.3.</p><p>In the encoding process, the detail of structures at the cell state tensor increased gradually. This may be interpreted as additional information written to the cell state. It further appeared that the structures visible at the cell states resembled shapes, which were present in cloud-free RGB images (e.g., c <ref type="bibr" target="#b2">(3)</ref> t=15 or c <ref type="bibr" target="#b21">(22)</ref> t=28 ). Some cells (e.g., Cell 3 or Cell 22) changed their activations gradually over the span of multiple observations, while others (e.g., 48) changed more frequently. Forget gate f activations are element-wise multiplied with the previous cell state c t?1 and range between zero and one. Low values in this gate numerically reduce the cell state, which can be potentially interpreted as a change of decision. The input i and modulation gate j control the degree of new information written to the cell state. While the input gate is scaled between zero and one, the modulation gate j ? [?1, 1] determines the sign of change. In general, we found the activity of a majority of cells (e.g., Cell 3 or Cell 22) difficult to associate with distinct events in the current input. However, we assumed that classification-relevant features were expressed as a combination of cell activations similar to other neural network approaches. Nevertheless, we could identify a proportionally small number of cells, in which the shape of clouds visible in the image was projected on the internal state activations. One of these was cell i = 47. For cloudy observations, the input gate approached zero either over the entire tile (e.g., t = {10, <ref type="bibr">18, 19, 36})</ref> or over patches of cloudy pixels (e.g., t = {11, 13, 31, 33}). At some observation times (e.g., t = {13, 31, 32}), the modulation gate j (47) t additionally changed the sign. In a similar fashion, Karpathy <ref type="bibr" target="#b36">[37]</ref> evaluated cell activations for the task of text processing. He could associate a small number of cells with a set of distinct tasks, such as monitoring the lengths of a sentence or maintaining a state-flag for text inside and outside of brackets.</p><p>Summarizing this experiment, the majority of cells showed increasingly detailed structures when new information was provided in the input sequence. It is likely that the grammar of crop-characteristic phenological changes was encoded in the network weights, and we suspect that a certain amount of these cells was sensitive to distinct events relevant for crop identification. However, these events may be encoded in multiple cells and were difficult to visually interpret. A small set of cells could be visually associated with individual cloud covers and may be used for internal cloud masking. Based on these findings, we are confident that our network has learned to internally filter clouds without explicitly introducing cloud-related labels.</p><p>x f <ref type="bibr" target="#b2">(3)</ref> i <ref type="bibr" target="#b2">(3)</ref> j <ref type="bibr" target="#b2">(3)</ref> c <ref type="bibr" target="#b2">(3)</ref> f <ref type="bibr" target="#b21">(22)</ref> i <ref type="bibr" target="#b21">(22)</ref> j <ref type="bibr" target="#b21">(22)</ref> c <ref type="bibr" target="#b21">(22)</ref> f <ref type="bibr">(47)</ref> i <ref type="bibr">(47)</ref> j <ref type="bibr">(47)</ref>    <ref type="figure">Figure 6</ref>. Internal LSTM cell activations of input gate i (i) , forget gate f (i) , modulation gate j (i) and cell state c (i) at three (of r = 256) selected cells i ? {3, 22, 47} given the current input x t over the sequence of observations t = {1, .., 36}. The detail of features at the cell states increased gradually, which indicated the aggregation of information over the sequence. While most cells likely contribute to the classification decision, only some cells are visually interpretable with regard to the current input x t . One visually interpretable cell i = 47 has learned to identify cloud, as input and modulation gates show different activation patterns on cloudy and non-cloudy observations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Quanititative Classificaton Evaluation</head><p>For the quantitative evaluation of our approach, we trained networks with bidirectional convolutional LSTM and GRU cells with r ? {128, 256} recurrent cells. Kernel sizes of k rnn = k class = 3 were used for the evaluation since previous tests with larger kernel sizes showed similar accuracies. For these initial experiments, we predominantly tested network variants with r = 128 recurrent cells, as these networks could be trained within a reasonable time frame. We decided to use networks with r = 256 recurrent cells for the final accuracy evaluation, as we found that these variants achieved slightly better results in prior tests. The convolutional GRU and LSTM networks were trained on a P100 GPU for 60 epochs (3.51 Mio 24 px ? 24 px tiles seen) and took 58 h and 51 h, respectively. However, reasonable accuracies were achieved within the first twelve hours, and further training increased the accuracies on validation data only marginally. At each training step, a subset of 30 observations was randomly sampled from all available 46 (2016) and 52 (2017) observations to randomize the sequence while the sequential order was maintained. For all our tests, the performance of LSTM and GRU networks was similar. The fewer weights of GRU cells, however, allowed using a slightly larger batch size of 32 samples compared to 28 samples of the LSTM variant. This led to a seven-hour faster training compared to the LSTM variant.</p><p>For these reasons, we decided to report evaluation results of the GRU network in <ref type="table">Table 2</ref>. Precision and recall are common accuracy measures that normalize the sum of correctly-predicted samples with the total number of predicted and reference samples of a given class, respectively. These measures are equivalent to user's and producer's accuracies and inverse to errors of commission and omission, which are popular metrics in the remote sensing community. We further calculated the f -measure as the harmonic average of precision and recall and the overall accuracy as the sum of correctly-classified samples normalized by the total number of samples. These metrics weight each sample equally. This introduces a bias towards frequent classes in the dataset, such as maize or wheat. To compensate for the non-uniform class distribution, we additionally report the conditional <ref type="bibr" target="#b37">[38]</ref> and overall kappa <ref type="bibr" target="#b35">[36]</ref> coefficients, which are normalized by the probability of a hypothetical correct classification by chance. The kappa coefficient ? is a measure of agreement and typically ranges between ? = 0 for no and ? = 1 for complete agreement. McHugh <ref type="bibr" target="#b38">[39]</ref> provides an interpretative table in which values 0.4 ? ? &lt; 0.6 are considered 'weak', values 0.6 ? ? &lt; 0.8 'moderate', 0.8 ? ? ? 0.9 considered 'strong' and values beyond 0.9 'almost perfect'.</p><p>The provided table of accuracies shows precision, recall, f -measure and the conditional kappa coefficient for each class over the two evaluated seasons. Furthermore, overall accuracy and overall kappa coefficients indicate the quality for the classification and report good accuracies. The pixel-averaged achieved precision, recall and f -score accuracies were consistent and ranged between 89.3% and 89.9%.The kappa coefficients of 0.870 and overall accuracies of 89.7% and 89.5% show similar consistency. While these classification measures reported good performances, the class-wise accuracies varied largely between 41.5% (peas) and 96.8% (maize). For better visibility, we emphasized the best and worst metrics by boldface. The conditional kappa scores are similarly variable and range between 0.414 (peas) and 0.957 (rapeseed).</p><p>Frequent classes (e.g., maize, meadow) have been in general more confidently classified than less frequent classes (e.g., peas, summer oat, winter spelt, winter triticale). Nonetheless this relation has exceptions. The least frequent class, peas, performed relatively well on data of 2016, and other less frequent classes, such as asparagus or hop, showed good performances despite their underrepresentation in the dataset.</p><p>To investigate the causes of the varying accuracies, we calculated confusion matrices for both seasons as shown in <ref type="figure">Figure 7</ref>. These error matrices are two-dimensional histograms of classification samples aggregated by the class prediction and ground truth reference. To account for the non-uniform class distribution, the absolute number of samples for each row-column pair is normalized. We decided to normalize the confusion matrices by row to obtain recall (producer's) accuracies, due to their direct relation to available ground truth labels. The diagonal elements of the matrices represent <ref type="table">Table 2</ref>. Pixel-wise accuracies of the trained convolutional GRU sequential encoder network after training over 60 epochs on data of both growth seasons. The conditional kappa metrics <ref type="bibr" target="#b37">[38]</ref> for each class and the overall kappa <ref type="bibr" target="#b35">[36]</ref> measure are given for both growth seasons. correctly-classified samples with values equivalent to <ref type="table">Table 2</ref>. Structures outside the diagonal indicate systematic confusions between classes and may give insight into the reasoning behind varying classification accuracies. Some crops likely share common spectral or phenological characteristics. Hence, we expected some symmetric confusion between classes, which would be expressed as diagonal symmetric confusions consistent in both years. Examples of this were triticale and rye or oat and summer barley. However, these relations were not frequent in the dataset, which indicates that the network had sufficient capacity to separate the classes by provided features. In some cases, one class may share characteristics with another class. This class may be further distinguished by additional unique features, which would be expressed by asymmetric confusions between these two classes in both seasons. Relations of this type were more dominantly visible in the matrices and included confusions between barley and triticale, triticale and spelt or wheat confused with triticale and spelt. These types of confusion were consistent over both seasons and may be explained by a spectral or phenological similarity between individual crop-types.</p><p>More dominantly, many confusions were not consistent over the two growing seasons. For instance, confusions occurring only in the 2017 season were soybeans with potato or peas with meadow and potato. Since the cultivated crops are identical in these years and the class distributions were consistent, seasonally-variable factors were likely responsible for these relations. As reported in <ref type="table">Table 2</ref>, peas have been classified well in 2016, but poorly in 2017, due to the aforementioned confusions with meadow and potato. These results indicate that external and not crop-type-related factors had a negative influence on classification accuracies, which appeared unique to one season. One of these might be the variable onset of phenological events, which are indirectly observed by the change of reflectances by the sensors. These events are influenced by local weather and sun exposure, which may vary over large regional scales or multiple years.  <ref type="figure">Figure 7</ref>. Confusion matrix of the trained convolutional GRU network on data of the seasons 2016 and 2017. While the confusion of some classes was consistent over both seasons (e.g., winter triticale to winter wheat ), other classes are classified at different accuracies at consecutive years (e.g., winter</p><p>barley to winter spelt ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Qualitative Classification Evaluation</head><p>For the qualitative evaluation, we used the same network structure as in the previous section. We inferred the network with 48 px tiles from the evaluation dataset of 2017 for better visibility. In <ref type="figure" target="#fig_7">Figure 8</ref>, a series of good (A-D) and bad (E,F) classification examples are shown. The first column represents the input sequence x as band-normalized RGB images from one selected cloud-free observation x RGB,t . Further columns show the available ground truth labels y, predictions? and the cross-entropy loss H(y,?). Additionally, four selected softmax-normalized class activations are displayed in the last columns. These activations can be interpreted as classification confidences for each class. The prediction map contains the index of the most activated class at each pixel, which may be interpreted as the class of highest confidence. The cross-entropy loss is the measure the agreement between the one-hot representation of the ground truth labels and the activations per class. It is used as the objective function, as network training indicates disagreement between ground truth and prediction even if the final class prediction is correct. This relation can be observed in fields of several examples, such as peas in Example A, spelt in Example B and oat in Example C. However, most classifications for these examples were accurate, which is expressed by well-defined activation maps.</p><p>Often, classifiers use low-pass filters in the spatial dimensions to compensate for high-frequent noise. These filters typically limit the ability to classify small objects. To evaluate to what degree the network has learned to apply low-pass filtering, we show a tile with a series of narrow fields in Example D. Two thin wheat and maize fields have been classified correctly. However, some errors occurred on the southern end of an adjacent potato field, as indicated by the loss map. It appears that the network was able to resolve high-frequency spatial changes and did not apply smoothing of the class activations, as in Example F.</p><p>Two misclassified fields are shown in Example E. The upper wheat field has been confidently misclassified to summer barley. Underneath, the classification of a second rye field was uncertain between rye, wheat and triticale. While triticale, as the least activated class, was not present in the prediction map, the mixture of rye and wheat is visible in the class predictions.</p><p>Example F shows a mostly misclassified tile. Only a few patches of meadow and winter barley were correctly predicted. The activations of these classes were, compared to previous examples, generally more blurred and of lower amplitude. Similar to Example D, the most activated classes are also the most frequent in the dataset. In fact, the entire region around the displayed tile seemed to be classified  poorly. This region was located on the northwest border of the AOI. Further examination showed that for this region, fewer satellite images were available. The lack of temporal information likely explains the poor classification accuracies. However, this example illustrates that the class activations give an indication of the classification confidence independent of the ground truth information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Discussion</head><p>In this section, we compare our approach with other multi-temporal classifications. Unfortunately, to the best of our knowledge, no multi-temporal benchmark dataset is available to compare remote sensing approaches on equal footing. Nevertheless, we provide some perspective of the study domain by gathering multi-temporal crop classification approaches in <ref type="table" target="#tab_4">Table 3</ref> and categorizing these by their applied methodology and achieved overall accuracy. However, the heterogeneity of data sources, the varying extents of their evaluated areas and the number of classes used in these studies impedes a numerical comparison of the achieved accuracies. Despite this, we hope that this table will provide an overview of the state-of-the-art in multi-temporal crop identification. Earth observation (EO) data are acquired in periodic intervals at high spatial resolutions. From an information theoretical perspective, utilizing additional data should lead to better classification performance. However, the large quantity of data requires methods that are able to process this information and are robust with regard to observation noise. Optimally, these approaches are scalable with minimal supervision so that data of multiple years can be included over large regions. Existing approaches in multi-temporal EO tasks often use multiple separate processing steps, such as preprocessing, feature extraction and classification, as summarized by ?nsalan and Boyer <ref type="bibr" target="#b39">[40]</ref>. Generally, these steps require manual supervision or the selection of additional parameters based on region-specific expert knowledge, a process that impedes applicability at large scales. The cost of data acquisition is an additional barrier, as multiple and potentially expensive satellite images are required. Commercial satellites, such as RapidEye (RE), Satellite Pour l'Observation de la Terre (SPOT) or QuickBird (QB), provide images at excellent spatial resolution. However, predominantly inexpensive sensors, such as Landsat (LS), Sentinel 2 (S2), Moderate-resolution Imaging Spectroradiometer (MODIS) or Advanced Spaceborne Thermal Emission and Reflection Radiometer (ASTER), can be applied at large scales, since the decreasing information gain of additional observations must justify image acquisition costs. Many approaches use spectral indices, such as normalized difference vegetation index (NDVI), normalized difference water index (NDWI) or enhanced vegetation index (EVI), to extract statistical features from vegetation-related signals and are invariant to atmospheric perturbations. Commonly, decision trees (DTs) or random forests (RFs) are used for classification. The exclusive use of spectral indices simplifies the task of feature extraction. However, these indices utilize only a small number of available spectral bands (predominantly blue, red and near-infrared). Thus, methods that utilize all reflectance measurements, either at top-of-atmosphere (TOA), or atmospherically-corrected to bottom-of-atmosphere (BOA), are favorable, since all potential spectral information can be extracted.</p><p>In general, a direct numerical comparison of classification accuracies is difficult, since these are dependent on the number of evaluated samples, the extent of evaluated area and the number of classified categories. Nonetheless, we compare our method with the approaches of Siachalou et al. <ref type="bibr" target="#b13">[14]</ref> and Hao et al. <ref type="bibr" target="#b11">[12]</ref> in detail since their achieved classification accuracies are on a similar level as ours. Hao et al. <ref type="bibr" target="#b11">[12]</ref> used an RF classifier on phenological features, which were extracted from NDVI and NDWI time series of MODIS data. Their results demonstrate that good classification accuracies with hand-crafted feature extraction and classification methods can be achieved if data of sufficient temporal resolution are available. However, the large spatial resolution (500 m) of the MODIS sensor limits the applicability of this approach to areas of large homogeneous regions. On a smaller scale, Siachalou et al. <ref type="bibr" target="#b13">[14]</ref> report good levels of accuracy on small fields. For this, they used a hidden Markov models (HMMs) with a temporal series of four LS images combined with one single RapidEye (RE) image for field border delineation. Methodologically, HMMs and conditional random fields (CRFs) <ref type="bibr" target="#b14">[15]</ref> are closer to our approach since the phenological model is approximated with an internal chain of hidden states. However, these methods might not be applicable for long temporal series, since Markov-based approaches assume that only one previous state contains classification-relevant information.</p><p>Overall, this comparison shows that our proposed network can achieve state-of-the-art classification accuracy with a comparatively large number of classes. Furthermore, the S2 data of non-atmospherically-corrected TOA values can be acquired easily and does not require further preprocessing. Compared to previous work, we were able to process larger tiles by using convolutional recurrent cells with only a single recurrent encoding layer. Moreover, we neither required atmospheric correction, nor additional cloud classes, since one classification decision is derived from the entire sequence of observations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>In this work, we proposed an automated end-to-end approach for multi-temporal classification, which achieved state-of-the-art accuracies in crop classification tasks with a large number of crop classes. Furthermore, the reported accuracies were achieved without radiometric and geometric preprocessing. The trained and inferred data were atmospherically uncorrected and contained clouds. In traditional approaches, multi-temporal cloud detection algorithms utilize the sudden positive change in reflectivity of cloudy pixels and achieve better results than other traditional mono-temporal remote sensing classifiers <ref type="bibr" target="#b42">[43]</ref>. Results of this work indicate that cloud masking can be learned jointly together with classification. By visualizing internal gate activations in our network in Section 5.1, we found evidence that some recurrent cells were sensitive to cloud coverage. These cells may be used by the network to internally mask cloudy pixels similar to an external cloud filtering algorithm.</p><p>In Sections 5.2 and 5.3, we further evaluated the classification results quantitatively and qualitatively. Based on several findings, we derived that the network has approximated a discriminative crop-specific phenological model based on a raw series of TOA S2 observations. Further inspection revealed that some crops were inconsistently classified in both growing seasons. This may be caused by seasonally-variable environmental conditions, which may have been implicitly integrated into the encoded phenological model. We employed our network for the task crop classification since vegetative classes are well characterized by their inherently temporal phenology. However, the network architecture is methodologically not limited to vegetation modeling and may be employed for further tasks, which may benefit from the extraction of temporal features. We hope that our results encourage the research community to utilize the temporal domain for their applications. In this regard, we publish the TENSORFLOW source code of our network along with the evaluations and experiments from this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ArXiv version history</head><p>v1 submitted version to IJGI v2 added notice of submission date to history v3 revised manuscript version on review comments. Added section Section 3.2 Prior Work and improved English language and style.</p><p>v4 updated ArXiv version with published MDPI paper and added doi. Minor spelling and wording corrections and text layout. Minor modifications on cited papers. Updated Github link.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Illustration of a sequence-to-sequence network<ref type="bibr" target="#b6">[7]</ref> as often used in neural translation tasks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Illustrations of recurrent network architectures which inspired this work. The network of previous work</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>seq 0 , c rev T ? R h?w?r and output h seq 0 , h rev T ? R h?w?r are initialized with zeros. The concatenated final states c T = [c seq T c inv 0 ] are the representation of the entire sequence and are passed to a convolutional layer for classification. A second convolutional classification layer projects the sequence representation in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>Schematicial illustration of our proposed bidirectional sequential encoder network. The input sequence x</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>To provide additional temporal meta information, the year and day-of-year of the individual observations were added as matrices to the input tensor. Hence, the input feature depth d = 15 is composed of four 10 m (B4, B3, B2, B8), six 20 m (B5, B6, B7, B8A) and three 60 m (B1, B11, B12) bands combined with year and day-of-year. Area of interest (AOI) north of Munich containing 430 kha and 137 k field parcels. The AOI is further tiled at multiple scales into datasets for training, validation and evaluation and footprints of individual samples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 .</head><label>5</label><figDesc>Non-uniform distribution of field classes in the AOI Acquired Sentinel 2 (S2) observations of the twin satellites S2A and S2B Information of the area of interest containing location, division schemes, class distributions and dates of acquired satellite imagery.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>t 28 t 29 t 30 t 31 t 32 t 33 t 34 t 35 t 36</head><label>36</label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 .</head><label>8</label><figDesc>Qualitative results of the convolutional GRU sequential encoder. Examples A-D show good classification results. At example E the network misclassified one maize parcel with high confidence, which is indicated by incorrect, but well defined activations. At a second field the class activations reveal a confusion between wheat, meadow and maize. At example F most pixels are misclassified. However, the class activations show uncertainty in the classification decision.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>10 t 11 t 12 t 13 t 14 t 15 t 16 t 17 t 18 t 19 t 20 . . .</figDesc><table><row><cell>t 1</cell><cell>t 2</cell><cell>t 3</cell><cell>t 4</cell></row><row><cell></cell><cell></cell><cell></cell><cell>. . .</cell></row><row><cell></cell><cell></cell><cell></cell><cell>. . .</cell><cell>. . .</cell></row><row><cell></cell><cell></cell><cell></cell><cell>. . .</cell><cell>. . .</cell></row><row><cell></cell><cell></cell><cell></cell><cell>. . .</cell><cell>. . .</cell></row><row><cell></cell><cell></cell><cell></cell><cell>. . .</cell><cell>. . .</cell></row><row><cell></cell><cell></cell><cell></cell><cell>. . .</cell><cell>. . .</cell></row><row><cell></cell><cell></cell><cell></cell><cell>. . .</cell><cell>. . .</cell></row><row><cell></cell><cell></cell><cell></cell><cell>. . .</cell><cell>. . .</cell></row><row><cell></cell><cell></cell><cell></cell><cell>. . .</cell><cell>. . .</cell></row><row><cell></cell><cell></cell><cell></cell><cell>. . .</cell><cell>. . .</cell></row><row><cell></cell><cell></cell><cell></cell><cell>. . .</cell><cell>. . .</cell></row><row><cell></cell><cell></cell><cell></cell><cell>. . .</cell><cell>. . .</cell></row><row><cell>c (47)</cell><cell></cell><cell></cell><cell>. . .</cell><cell>. . .</cell></row></table><note>t</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 .</head><label>3</label><figDesc>Overview over recent approaches for crop classification.</figDesc><table><row><cell>Approach</cell><cell>Details</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Sensor</cell><cell>Preprocessing</cell><cell>Features</cell><cell>Classifier</cell><cell>Accuracy</cell><cell># Classes</cell></row><row><cell>this work</cell><cell>S2</cell><cell>none</cell><cell>TOA reflect.</cell><cell cols="2">ConvRNN 90</cell><cell>17</cell></row><row><cell>Ru?wurm and K?rner [24, 2017]</cell><cell>S2</cell><cell cols="2">atm. cor. (SEN2COR) BOA reflect.</cell><cell>RNN</cell><cell>74</cell><cell>18</cell></row><row><cell>Siachalou et al. [14, 2015]</cell><cell>LS, RE</cell><cell>geometric image registration correction,</cell><cell>TOA reflect.</cell><cell>HMM</cell><cell>90</cell><cell>6</cell></row><row><cell>Hao et al. [12, 2015]</cell><cell cols="2">MODIS image reprojection, atm. cor. [41]</cell><cell>statistical phen. features</cell><cell>RF</cell><cell>89</cell><cell>6</cell></row><row><cell>Conrad et al. [11, 2014]</cell><cell>SPOT, RE, QB</cell><cell>segmentation, atm. cor. [41]</cell><cell>vegetation indices</cell><cell>OBIA+RF</cell><cell>86</cell><cell>9</cell></row><row><cell>Foerster et al. [9, 2012]</cell><cell>LS</cell><cell>phen. normalization, atm. cor. [41]</cell><cell>NDVI statistics</cell><cell>DT</cell><cell>73</cell><cell>11</cell></row><row><cell>Pe?a-Barrag?n et al. [13, 2011]</cell><cell>ASTER</cell><cell>segmentation, atm. cor. [42]</cell><cell>vegetation indices</cell><cell>OBIA+DT</cell><cell>79</cell><cell>13</cell></row><row><cell>Conrad et al. [10, 2010]</cell><cell>SPOT ASTER</cell><cell>segmentation, atm. cor. [41]</cell><cell>vegetation indices</cell><cell>OBIA+DT</cell><cell>80</cell><cell>6</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments:</head><p>We would like to thank the Bavarian Ministry of Food, Agriculture and Forestry (StMELF)for providing ground truth data in excellent semantic and geometric quality. Furthermore, we thank the Leibnitz Supercomputing Centre (LRZ)for providing access to computational resources, such as the DGX-1 and P100servers and NVIDIA for providing one TITAN X GPU. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conflicts of Interest:</head><p>The authors declare no conflict of interest.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Materials:</head><p>The source code of the network implementation and further material is made publicly available at https://github.com/TUM-LMF/MTLCC.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Crop identification using Landsat temporal-spectral profiles. Remote Sensing of Environment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Odenweller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">I</forename><surname>Johnson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1984" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="39" to="54" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Measuring Phenological Variability from Satellite Imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">C</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Vanderzee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">R</forename><surname>Loveland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Merchant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">O</forename><surname>Ohlen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Vegetation Science</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="703" to="714" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Neural Machine Translation by Jointly Learning to Align and Translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno>1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">A Neural Attention Model for Sentence Summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Neural Headline Generation with Minimum Risk Training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.01904v1</idno>
	</analytic>
	<monogr>
		<title level="j">Arxiv</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Abstractive Text Summarization Using Sequence-to-Sequence RNNs and Beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">N</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiang</surname></persName>
		</author>
		<idno>1602.06023</idno>
	</analytic>
	<monogr>
		<title level="j">Arxiv</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Sequence to Sequence Learning with Neural Networks</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>1409.3215</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Attention-based models for speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chorowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Serdyuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno>1506.07503</idno>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="577" to="585" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Crop type mapping using spectral-temporal profiles and phenological information. Computers and Electronics in Agriculture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Foerster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kaden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Foerster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Itzerott</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">89</biblScope>
			<biblScope unit="page" from="30" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Per-Field Irrigated Crop Classification in Arid Central Asia Using SPOT and ASTER Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Conrad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fritsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zeidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>R?cker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dech</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1035" to="1056" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Derivation of temporal windows for accurate crop discrimination in heterogeneous croplands of Uzbekistan using multitemporal RapidEye images. Computers and Electronics in Agriculture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Conrad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Dubovyk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fritsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>L?w</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Schorcht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zeidler</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">103</biblScope>
			<biblScope unit="page" from="63" to="74" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Feature Selection of Time Series MODIS Data for Early Crop Classification Using Random Forest: A Case Study in</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shakir</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="5347" to="5369" />
			<pubPlace>Kansas, USA</pubPlace>
		</imprint>
	</monogr>
	<note>Remote Sensing</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Object-based crop identification using multiple vegetation indices, textural features and crop phenology. Remote Sensing of Environment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Pe?a-Barrag?n</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">K</forename><surname>Ngugi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Plant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Six</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="page" from="1301" to="1316" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">A hidden markov models approach for crop classification: Linking crop phenology to time series of multi-sensor remote sensing data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Siachalou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mallinis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tsakiri-Strati</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="3633" to="3650" />
		</imprint>
	</monogr>
	<note>Remote Sensing</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Conditional random fields for multitemporal and multiscale classification of optical satellite imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hoberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Rottensteiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">Q</forename><surname>Feitosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Heipke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="page" from="659" to="673" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Deep learning in remote sensing: a review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tuia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Fraundorfer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>1710.03959</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Transferring Deep Convolutional Neural Networks for the Scene Classification of High-Resolution Remote Sensing Imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="14680" to="14707" />
		</imprint>
	</monogr>
	<note>Remote Sensing</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Training Deep Convolutional Neural Networks for Land-Cover Classification of High-Resolution Imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Scott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>England</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">A</forename><surname>Starms</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Marcum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geoscience and Remote Sensing Letters</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="549" to="553" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep Supervised Learning for Hyperspectral Data Classification through Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Makantasis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Karantzalos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Doulamis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Doulamis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings GARSS 2015. 2015 IEEE International Geoscience and Remote Sensing Symposium</title>
		<meeting>GARSS 2015. 2015 IEEE International Geoscience and Remote Sensing Symposium</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4959" to="4962" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Castelluccio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Poggi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sansone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Verdoliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Land Use</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.00092</idno>
		<idno>1508.00092</idno>
		<title level="m">Remote Sensing Images by Convolutional Neural Networks</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Incremental Dual-memory LSTM in Land Cover Prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khandelwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Nayak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gerber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Carlson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>West</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="867" to="876" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Sea Level Anomaly Prediction using Recurrent Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Braakmann-Folgmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Roscher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wenzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Uebbing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kusche</surname></persName>
		</author>
		<idno>1710.07099</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Land Cover Classification from Multi-temporal, Multi-spectral Remotely Sensed Imagery using Patch-Based Recurrent Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<idno>1708.00813</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Temporal Vegetation Modelling using Long Short-Term Memory Networks for Crop Identification from Medium-Resolution Multi-Spectral Satellite Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ru?wurm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>K?rner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>IEEE/ISPRS Workshop on Large Scale Computer Vision for Remote Sensing Imagery (EarthVision</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Long Short-Term Memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.1078</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>1406.1078</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wayne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Danihelka</surname></persName>
		</author>
		<idno>1410.5401</idno>
	</analytic>
	<monogr>
		<title level="j">Neural Turing Machines. Arxiv</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">On the Computational Power of Neural Nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Siegelmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Sontag</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computer and System Sciences</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="page" from="132" to="150" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">An Empirical Exploration of Recurrent Network Architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rafal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wojciech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ilya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on International Conference on Machine Learning</title>
		<meeting>the 32nd International Conference on International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="2342" to="2350" />
		</imprint>
	</monogr>
	<note>1512.03385</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Gradient flow in recurrent nets: the difficulty of learning long-term dependencies. A Field Guide to Dynamical Recurrent Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Frasconi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1011.1669v3</idno>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="237" to="243" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Learning long-term dependencies with gradient descent is difficult</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yoshua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Patrice</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Paolo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="157" to="166" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">Y</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">K</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">C</forename><surname>Woo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Convolutional</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Network</surname></persName>
		</author>
		<title level="m">Machine Learning Approach for Precipitation Nowcasting. Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">28</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Rectifier Nonlinearities Improve Neural Network Acoustic Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Maas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Hannun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30 th International Conference on Machine Learning</title>
		<meeting>the 30 th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Digital selection and analogue amplification coexist in a cortex-inspired silicon circuit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hahnloser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sarpeshkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Mahowald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Douglas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Seung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">405</biblScope>
			<biblScope unit="page" from="947" to="951" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">A Method for Stochastic Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A coefficient of agreeement for nominal scales</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Educational and Psychological Measurement</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="37" to="46" />
			<date type="published" when="1960" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">The unreasonable effectiveness of recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">The Determination of Optimal Threshold Levels for Change Detection Using Various Accuracy Indices. Photogrammetric Engineering &amp; Remote Sensing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Fung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ledrew</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1988" />
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="page" from="1449" to="1454" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Interrater reliability: the kappa statistic</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">L</forename><surname>Mchugh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biochemia medica</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="276" to="82" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Review on Land Use Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>?nsalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">L</forename><surname>Boyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Multispectral Satellite Image Understanding: From Land Classification to Building and Road Detection</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="49" to="64" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">A spatially adaptive fast atmospheric correction algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Richter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="1201" to="1214" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Status of Atmospheric Correction using a MODTRAN4-Based Algorithm. SPIE proceeding. Algorithms for multispectral, hyperspectral, and ultra-spectral imagery VI</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">W</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Adler-Golden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Berk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Richtsmeier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">Y</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">K</forename><surname>Acharya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">P</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">W</forename><surname>Felde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">P</forename><surname>Hoke</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="199" to="207" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">A multi-temporal method for cloud detection, applied to FORMOSAT-2, VENuS, LANDSAT and SENTINEL-2 images. Remote Sensing of Environment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Hagolle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Huc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Villa Pascual</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Dedieu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">114</biblScope>
			<biblScope unit="page" from="1747" to="1755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Zupanc</surname></persName>
		</author>
		<ptr target="http://creativecommons.org/licenses/by/4.0/" />
	</analytic>
	<monogr>
		<title level="m">CC BY) license</title>
		<meeting><address><addrLine>Basel, Switzerland</addrLine></address></meeting>
		<imprint/>
	</monogr>
	<note>Improving Cloud Detection with Machine Learning, 2017. c 2018 by the authors</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
