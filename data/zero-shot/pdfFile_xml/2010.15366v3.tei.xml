<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Stabilizing Label Assignment for Speech Separation by Self-supervised Pre-training</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sung-Feng</forename><surname>Huang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">National Taiwan University</orgName>
								<address>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shun-Po</forename><surname>Chuang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">National Taiwan University</orgName>
								<address>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Da-Rong</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">National Taiwan University</orgName>
								<address>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Chen</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">National Taiwan University</orgName>
								<address>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gene-Ping</forename><surname>Yang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Edinburgh</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hung-Yi</forename><surname>Lee</surname></persName>
							<email>hungyilee@ntu.edu.tw</email>
							<affiliation key="aff0">
								<orgName type="institution">National Taiwan University</orgName>
								<address>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Stabilizing Label Assignment for Speech Separation by Self-supervised Pre-training</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T03:36+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms: Speech Enhancement</term>
					<term>Self-supervised Pre-train</term>
					<term>Speech Separation</term>
					<term>Label Permutation Switch</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Speech separation has been well developed, with the very successful permutation invariant training (PIT) approach, although the frequent label assignment switching happening during PIT training remains to be a problem when better convergence speed and achievable performance are desired. In this paper, we propose to perform self-supervised pre-training to stabilize the label assignment in training the speech separation model. Experiments over several types of self-supervised approaches, several typical speech separation models and two different datasets showed that very good improvements are achievable if a proper self-supervised approach is chosen.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Supervised learning has been extremely successful in recent years in machine learning, except the huge quantity of labeled data needed causes the major problem. On the other hand, self-supervised learning tries to train the model using only unlabeled data, such as reconstructing the original data from some transformed representations or leveraging some parts of data to predict the other parts, therefore becomes highly attractive. In natural language processing (NLP) <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3]</ref>, BERT <ref type="bibr" target="#b0">[1]</ref> learned powerful representations by self-supervised pretraining to encode contextual information. In computer vision (CV) <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8]</ref>, SimCLRv2 <ref type="bibr" target="#b4">[5]</ref> outperformed the previous state-of-the-art on ImageNet by self-supervised pretraining. Examples in NLP and CV have shown self-supervised pre-trained models are more label-efficient than previous semisupervised training methods. In the speech processing area, self-supervised learning also showed great advantages when labeled data are limited <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14]</ref>. CPC <ref type="bibr" target="#b5">[6]</ref> and APC <ref type="bibr" target="#b11">[12]</ref> learned to extract useful representations for speech using a probabilistic contrastive loss to capture information for predicting future samples. Wav2vec <ref type="bibr" target="#b8">[9]</ref> benefited from the idea of CPC and outperformed the state-of-the-art in character-based ASR with representations learned from 1000 hours of unlabeled speech. Wav2vec 2.0 <ref type="bibr" target="#b9">[10]</ref> further showed that 10 minutes of labeled data were enough for training an ASR system with 53k hours of unlabeled data. TERA <ref type="bibr" target="#b13">[14]</ref> pre-trained a Transformer model with a BERT-like objective. The learned representations were shown to be robust for a wide range of downstream tasks. The model could even outperform supervised learning when fine-tuned with only 0.1% of labeled data.</p><p>On the other hand, speech separation has long been a fundamental problem towards robust speech processing un-der the real-world acoustic environment, in which the considered speech signal is inevitably disturbed by some additional signals produced by other speakers. In general, deep learning techniques for single-channel speech separation can be divided into two categories: time-frequency (T-F) domain methods and end-to-end time-domain approaches. Based on T-F features obtained with short-time Fourier transform (STFT), T-F domain methods separate the T-F features for each source and then reconstruct the source waveforms by inverse STFT <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19]</ref>. Time-domain approaches then directly process the mixture waveform using an encode-decoder framework, and this line of research has achieved significant progress in recent years <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23]</ref>. But both the T-F domain and time-domain approaches suffer from the label ambiguity problem when evaluating the reconstruction errors by matching the ground truths with the estimated signals. Permutation-invariant training (PIT) <ref type="bibr" target="#b23">[24]</ref> has been very useful to handle this problem by dynamically choosing the best label assignment each time. However, the very unstable label assignment during the early training stage in PIT was shown to lead to slower convergence and lower performance <ref type="bibr" target="#b24">[25]</ref>.</p><p>In this paper, we made the following contributions:</p><p>? We point out the self-supervised pre-training is also extremely helpful to speech separation.</p><p>? We show the self-supervised pre-training can effectively stabilize the label assignment in PIT during training speech separation models, and the significantly reduced label assignment switching during training directly lead to faster convergence and improved performance.</p><p>? The proposed approach is shown to be equally useful to all different separation models over different datasets, because PIT has been widely used across almost all speech separation tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Label ambiguity problem and permutation invariant training (PIT)</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Label ambiguity problem</head><p>In single-channel speech separation, several speech signals are mixed: y = N n=1 xn, where N is the number of sources; the goal is to extract all individual speech signals {xn} N n=1 from the mixed signal y. For simplicity, we consider two sources only, y = x1 + x2, and employ a model with two outputs, o1 and o2. There exist two possible label assignments: (1) o1 regresses to x1 and o2 regresses to x2, or (2) o1 regresses to x2 and o2 regresses to x1. These two label assignments lead to two different loss functions to be used in model training. There are arXiv:2010.15366v3 [cs.SD] 22 Aug 2021 N ! possible label assignments for N ? 2. Incorrect label assignments naturally force the separation model to be updated to wrong direction, or even possibly destroy what has been learned before.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">PIT and label assignment switching problem</head><p>Permutation invariant training (PIT) <ref type="bibr" target="#b23">[24]</ref> was proposed to solve the above problem. Every time when the model parameters are to be updated, all possible label assignments as mentioned above are used to calculate the regression loss, and the one with minimum loss is chosen to update the model. Although such a dynamic label selection principle sounds reasonable, the selected labels can be very different for different training epochs giving a very rugged training path. A soft version of PIT was proposed to relax the label assignment switching problem between epochs <ref type="bibr" target="#b25">[26]</ref>, but restricted to those with L2-based objective functions only. A cascaded training strategy was then proposed <ref type="bibr" target="#b24">[25]</ref>, in which a good label assignment was first obtained with PIT, based on which the model parameters were better updated, to be used as a good initialization for the third stage of PIT training. This approach properly reduced the assignment switching during training, but made the training time several times longer compared to the original PIT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed training strategies</head><p>Considering the unstable label assignment problem during training as mentioned above, plus the fact that self-supervised pre-training was shown to be able to assist the model to learn structural information from large-scale unlabeled data and benefit in boosting the following training procedures <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8]</ref>, we propose a self-supervised pre-training and fine-tuning framework as below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Pre-train</head><p>In this work, we consider three different self-supervised approaches for pre-training here: speech enhancement (SE), Masked Acoustic Model with Alteration (MAMA) used in TERA <ref type="bibr" target="#b13">[14]</ref>, and continuous contrastive task (CC) used in wav2vec 2.0 <ref type="bibr" target="#b9">[10]</ref>. Speech enhancement (SE) simply tries to reconstruct the original signal when noise is added to the input. MAMA is a masked reconstruction task, where the input audio is disturbed by noise with some parts randomly picked up and masked, and the model is required to reconstruct the clean audio of the masked parts. CC is a contrastive task; we mask the spans of the input audio features, and the model is trained to predict the masked spans of features correctly. <ref type="figure" target="#fig_0">Fig. 1</ref>(a) (colored part) shows the flowchart of pre-training, where the input signal is probably mixed with random noise and masked, and the model is to reconstruct the original clean source.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Fine-tune</head><p>After pre-training, the model is then fine-tuned with the normal separation training objective to produce the desired individual signals. All model parameters for fine-tuning are loaded from the pre-trained model as long as available, but the parameters used to generate specific output channels are re-initialized, as shown in <ref type="figure" target="#fig_0">Fig. 1(b)</ref>. PIT is performed as usual.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Multi-task learning</head><p>To verify that whether the proposed framework really benefits from the "pre-train then fine-tune" procedure, jointly learning from the self-supervised training plus separation in a multi-task learning framework is also tested as a baseline for comparison, as in <ref type="figure" target="#fig_0">Fig. 1(c)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental setup 4.1. Dataset</head><p>In this work, speech separation was trained and evaluated on WSJ0-2mix <ref type="bibr" target="#b17">[18]</ref> and Libri2Mix <ref type="bibr" target="#b26">[27]</ref> train-100 set, and self-supervised approaches were trained using Libri1Mix <ref type="bibr" target="#b26">[27]</ref> train-360 set <ref type="bibr" target="#b26">[27]</ref>. The WSJ0-2mix dataset was derived from the WSJ0 data corpus <ref type="bibr" target="#b27">[28]</ref>. The training and validation data contained two-speaker mixtures generated by randomly selecting utterances from different speakers in the WSJ0 si tr s set, and the test set was similarly generated using utterances from unseen speakers in WSJ0 si dt 05 and si et 05 set. Libri2Mix is created based on the Librispeech dataset <ref type="bibr" target="#b28">[29]</ref> with a similar generating procedure as WSJ0-2mix. Libri2Mix train-100 set used speakers randomly selected from the train-clean-100 set of Librispeech, while the dev and test set used the utterances from unseen speakers in the dev and test sets of Librispeech respectively. Libri1Mix train-360 dataset was created with the same settings as Libri2Mix, while only one speaker was randomly selected from the train-clean-360 set of Librispeech, and mixed with a random noise sampled from WHAM! <ref type="bibr" target="#b29">[30]</ref>. The speaker groups of Libri1Mix train-360 and Libri2Mix train-100 set were disjoint.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation details</head><p>The proposed self-supervised pre-training can be used with any separation model, and the study here was mainly focused on the effectiveness of pre-training. In this work, we choose Conv-TasNet <ref type="bibr" target="#b19">[20]</ref> as our main baseline model, DPRNN <ref type="bibr" target="#b20">[21]</ref> and DPT-Net <ref type="bibr" target="#b21">[22]</ref> were also used in later experiments. All experiments were implemented with Asteroid <ref type="bibr" target="#b30">[31]</ref>, and the detailed training configurations are in the repository 1 .</p><p>The model was trained with three different strategies for comparison in our experiments: from scratch, pre-trained then fine-tune (PT-FT), and multi-task training. We purposely let the three strategies have the same number of update steps in training the separation task for fairness. Separation performance was evaluated in scale-invariant signal-to-noise ratio improvement (SI-SNRi) <ref type="bibr" target="#b31">[32]</ref> and signal-to-distortion ratio improvement (SDRi) <ref type="bibr" target="#b32">[33]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experimental results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Comparison between the self-supervised pre-training tasks</head><p>We first wished to find out which self-supervise pre-training approach was more helpful to the separation task. Speech enhancement (SE), Masked Acoustic Model with Alternation (MAMA) and continuous contrastive task (CC) as described in Section 3.1 were tested. We used Conv-TasNet as our separation model. After pre-trained with SE, MAMA and CC respectively, we fine-tuned the obtained models for 100 epochs for the speech separation task. The pre-training tasks were all trained on Libri1Mix train-360 set, and the fine-tuning task was trained on WSJ0-2mix. The results listed in <ref type="table" target="#tab_0">Table 1</ref> showed  that both SE and MAMA led to significant improvement, but not CC. Note that both SE and MAMA had input speech disturbed by noise, while the model was to reconstruct the whole utterance (SE) or only the masked parts (MAMA), as mentioned in Section 3.1. So we may conclude that approaches trying to reconstruct the clean input speech from the noisy and/or masked one are probably more effective for pre-training speech separation tasks. A possible explanation may be here SE and MAMA already learned to extract from disturbed signals the information about each individual speaker, so all the following Conv-TasNet model needed to learn is to separate the extracted information into two channels, therefore the learning process was more stable and efficient. This is why in the following tests we only used speech enhancement (SE) for self-supervised pre-training. <ref type="table" target="#tab_1">Table 2</ref> shows the results of three different training strategies: from scratch, pre-training and fine-tuning (PT-FT) and multitask, with the latter two using speech enhancement (SE) for selfsupervised learning, all trained with Conv-TasNet as the main separation model. As shown, both pre-training and multi-task learning improved the separation model on both WSJ0-2mix and Libri2Mix, while pre-training improved more significantly (0.7 -1.0 dB improvements for PT-FT (SE) compared to "from scratch" v.s. 0.1 -0.5 dB for multi-task (SE)). A good explana- tion for this is that, as mentioned above, the pre-trained model already learned to extract from disturbed signals the information about the individual target speakers, so the following separation model could focus on the construction of the two masks, for the two sources. In contrast, for multi-task learning, the two different tasks of speech enhancement and speech separation were learned jointly, while sharing the knowledge learned for the two very different tasks may not be easy. This further showed the effectiveness of learning the two different tasks sequentially instead of jointly (self-supervised for enhancement then fine-tuning for separation). Also noted that since the corpus used to train speech enhancement was Libri1Mix train-360 set, which was closer to Libri2Mix train-100 set but farther from WSJ0-2mix, which may be the simple reason why the results in <ref type="table" target="#tab_1">Table 2</ref> on Libri2Mix (upper half) showed more improvements than those on WSJ0-2mix (lower half). Validation SI-SNR results during training are reported in <ref type="figure" target="#fig_1">Figure 2</ref>(a)(b) for Libri2Mix and WSJ0-2mix respectively. As shown in the figure, improvements for multi-task learning gradually decreased while training on Libri2Mix and were nearly hard to see on WSJ0-2mix. The proposed pre-trained model led the baselines all the way and achieved the final result of the baselines in only 37 epochs for Libri2Mix and 66 epochs for WSJ0-2mix, which are about one-third to two-thirds of the baseline training epochs. <ref type="figure" target="#fig_1">Figure 2(c)(d)</ref> show the percentage of label assignment switches in total training data. Here we can see only the proposed pre-training with speech enhancement (orange bars) significantly reduced label assignment switches, while multi-task learning (green bars) not only failed to reduce the label assignment switches but sometimes increased them. Moreover, training from scratch (blue bars) and multitask learning (green bars) sometimes got very high switching percentages (e.g., roughly 15% -35 % of the label assignments were often switched over for both training from scratch and multi-task training in <ref type="figure" target="#fig_1">Figure 2</ref>(c), and most label assignments were switched at epoch 40 for multi-task training in <ref type="figure" target="#fig_1">Figure 2(d)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Effectiveness of pre-training and fine-tuning (PT-FT)</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">More Separation models tested on WSJ0-2mix</head><p>More test results on different separation models with different batch sizes (BS), utterance length (L), with pre-training (PT) or from scratch are listed in <ref type="table">Table 3</ref>, all trained and evaluated on WSJ0-2mix. The first rows (a)(d)(g)(l) for each model are those reported in their original papers. For speeding up the experiments, Conv-TasNet and DPRNN (Sec. (I)(II) in <ref type="table">Table 3</ref>) were trained with shorter utterance length (3 or 2 sec) and a larger batch size (24) with 200 epochs, which caused the slightly worse DPRNN results than those previously reported <ref type="bibr" target="#b20">[21]</ref>. In addition to those for Conv-TasNet discussed previously, the pretrained DPRNN (Sec. (II)) was improved significantly, even achieving comparable performance as the reported one (rows (f) v.s. (d)(e)), although with worse performance from scratch due to the hyper-parameters. DPTNet (Sec. (III)) was trained with batch size 1 and 4 with 100 epochs to speed up the training process. Setting batch size 4 instead of 1 gave 0.3 dB worse performance (rows (h) v.s. (j)). Nevertheless, the pre-trained DPTNet made up the gap, even doing slightly better (rows (i) v.s. (j)). Compared to the current state-of-the-art (Sandglasset <ref type="bibr" target="#b22">[23]</ref>), the pre-trained DPTNet with a batch size 1 actually <ref type="table">Table 3</ref>: Different separation models on WSJ0-2mix in SI-SNRi (dB) and SDRi (dB). BS: batch size. L: utterance length (sec). PT: pre-training, "-" means training from scratch. The first rows (a)(d)(g)(l) for each model are the reported results from original papers. The blank indicates unknown. *Row (g) are actually SI-SNR and SDR. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this paper, we propose to use self-supervised pre-training to stabilize the label assignment for speech separation. We show that pre-training with speech enhancement offers better training and consistently improves the separation performance across all different separation model architectures over two different datasets.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>The flowchart for the proposed training framework: (a) pre-training, (b) fine-tuning after pre-training, and (c) multi-task training for comparison. Gray blocks indicate the corresponding parts are not used during training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>(a)(b) validation SI-SNR (dB) and (c)(d) percentage of label assignment switches in total training data (%) at each epoch on two datasets Libri2Mix and WSJ0-2mix respectively. In (d), the green bars reach 89% at around epoch 40.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Comparison between different self-supervised pretraining approaches when fine-tuned with Conv-TasNet in SI-SNRi and SDRi. The first row is for training from scratch.</figDesc><table><row><cell cols="3">Pre-training task SI-SNRi (dB) SDRi (dB)</cell></row><row><cell>-</cell><cell>15.6</cell><cell>15.8</cell></row><row><cell>SE</cell><cell>16.3</cell><cell>16.5</cell></row><row><cell>MAMA [14]</cell><cell>16.2</cell><cell>16.5</cell></row><row><cell>CC [10]</cell><cell>15.5</cell><cell>15.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table><row><cell>Corpus</cell><cell>Training strategy</cell><cell>SI-SNRi</cell><cell>SDRi</cell></row><row><cell></cell><cell>from scratch</cell><cell>13.2</cell><cell>13.6</cell></row><row><cell>Libri2Mix</cell><cell>PT-FT (SE)</cell><cell cols="2">14.1 (0.9) 14.6 (1.0)</cell></row><row><cell></cell><cell>multi-task (SE)</cell><cell cols="2">13.7 (0.5) 14.1 (0.5)</cell></row><row><cell></cell><cell>from scratch</cell><cell>15.6</cell><cell>15.8</cell></row><row><cell>WSJ0-2mix</cell><cell>PT-FT (SE)</cell><cell cols="2">16.3 (0.7) 16.5 (0.7)</cell></row><row><cell></cell><cell>multi-task (SE)</cell><cell cols="2">15.7 (0.1) 16.0 (0.2)</cell></row></table><note>Comparison between different training strategies for Conv-TasNet on two datasets (WSJ0-2mix and Libri2Mix) in SI- SNRi (dB) and SDRi (dB). PT-FT: pre-trained then fine-tune.SE: with speech separation for self-supervised. Number in the parentheses are the improvements over "from scratch".</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/SungFeng-Huang/ SSL-pretraining-separation/tree/main/local</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Acknowledgements</head><p>We thank to professor Lin-shan Lee for the guidance and National Center for High-performance Computing (NCHC) of National Applied Research Laboratories (NARLabs) in Taiwan for providing computational and storage resources.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Bert: Pretraining of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.05365</idno>
		<title level="m">Deep contextualized word representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Improving language understanding by generative pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.05709</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Big self-supervised models are strong semi-supervised learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.10029</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Representation learning with contrastive predictive coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">V</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03748</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning representations by maximizing mutual information across views</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">D</forename><surname>Hjelm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Buchwalter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="15" to="535" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Data-efficient image recognition with contrastive predictive coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">J</forename><surname>H?naff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>De Fauw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Razavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">V D</forename><surname>Oord</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.09272</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">wav2vec: Unsupervised pre-training for speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.05862</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">wav2vec 2.0: A framework for self-supervised learning of speech representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.11477</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Audio word2vec: Unsupervised learning of audio segment representations using sequence-to-sequence autoencoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-A</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-H</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-Y.</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-S</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.00982</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">An unsupervised autoregressive model for speech representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-A</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-N</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Glass</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.03240</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Mockingjay: Unsupervised speech representation learning with deep bidirectional transformer encoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-H</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-Y</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6419" to="6423" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Tera: Self-supervised learning of transformer encoder representation for speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-Y</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.06028</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">On the improvement of singing voice separation for monaural recordings using the mir-1k dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-L</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-S</forename><forename type="middle">R</forename><surname>Jang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="310" to="319" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep learning for monaural speech separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hasegawa-Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Smaragdis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1562" to="1566" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Source separation with scattering non-negative matrix factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sprechmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1876" to="1880" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep clustering: Discriminative embeddings for segmentation and separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Hershey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Le</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="31" to="35" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep attractor network for single-microphone speaker separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mesgarani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="246" to="250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Conv-tasnet: Surpassing ideal timefrequency magnitude masking for speech separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mesgarani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">speech, and language processing</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="1256" to="1266" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Dual-path rnn: efficient long sequence modeling for time-domain single-channel speech separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yoshioka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="46" to="50" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Dual-path transformer network: Direct context-aware modeling for end-to-end monaural speech separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.13975</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Sandglasset: A light multi-granularity self-attentive network for time-domain speech separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">W</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.00819</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Permutation invariant training of deep models for speaker-independent multi-talker speech separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kolbaek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-H</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jensen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="241" to="245" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Interrupted and cascaded permutation invariant training for speech separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-P</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-W</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-S</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6369" to="6373" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Probabilistic permutation invariant training for speech separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yousefi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khorram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Hansen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4604" to="4608" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Librimix: An open-source dataset for generalizable speech separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cosentino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pariente</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cornell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Deleforge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Vincent</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.11262</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Csr-i (wsj0) complete ldc93s6a</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Garofolo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Graff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pallett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Web Download. Philadelphia: Linguistic Data Consortium</title>
		<imprint>
			<biblScope unit="volume">83</biblScope>
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Librispeech: an asr corpus based on public domain audio books</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Panayotov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="5206" to="5210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Wham!: Extending speech separation to noisy environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wichern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Antognini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Flynn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">R</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Mcquinn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Crow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Manilow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Roux</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.01160</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Asteroid: the pytorch-based audio source separation toolkit for researchers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pariente</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cornell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cosentino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sivasankaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzinis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Heitkaemper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Olvera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F.-R</forename><surname>St?ter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Mart?n-Do?as</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.04132</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Speaker-independent speech separation with deep attractor network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mesgarani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Speech, and Language Processing</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="787" to="796" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Performance measurement in blind audio source separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gribonval</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>F?votte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on audio, speech, and language processing</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1462" to="1469" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
