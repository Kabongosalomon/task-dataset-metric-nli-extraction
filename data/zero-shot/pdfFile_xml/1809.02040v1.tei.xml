<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Exploring Graph-structured Passage Representation for Multi-hop Reading Comprehension with Graph Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linfeng</forename><surname>Song</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Rochester</orgName>
								<address>
									<postCode>14623</postCode>
									<settlement>Rochester</settlement>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiguo</forename><surname>Wang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">IBM T.J. Watson Research Center</orgName>
								<address>
									<postCode>10598</postCode>
									<settlement>Yorktown Heights</settlement>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mo</forename><surname>Yu</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">IBM T.J. Watson Research Center</orgName>
								<address>
									<postCode>10598</postCode>
									<settlement>Yorktown Heights</settlement>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">School of Engineering</orgName>
								<orgName type="institution">Westlake University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Florian</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">IBM T.J. Watson Research Center</orgName>
								<address>
									<postCode>10598</postCode>
									<settlement>Yorktown Heights</settlement>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Gildea</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Rochester</orgName>
								<address>
									<postCode>14623</postCode>
									<settlement>Rochester</settlement>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Exploring Graph-structured Passage Representation for Multi-hop Reading Comprehension with Graph Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T16:35+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Multi-hop reading comprehension focuses on one type of factoid question, where a system needs to properly integrate multiple pieces of evidence to correctly answer a question. Previous work approximates global evidence with local coreference information, encoding coreference chains with DAG-styled GRU layers within a gated-attention reader. However, coreference is limited in providing information for rich inference. We introduce a new method for better connecting global evidence, which forms more complex graphs compared to DAGs. To perform evidence integration on our graphs, we investigate two recent graph neural networks, namely graph convolutional network (GCN) and graph recurrent network (GRN). Experiments on two standard datasets show that richer global information leads to better answers. Our method performs better than all published results on these datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Recent years have witnessed a growing interest in the task of machine reading comprehension. However, most existing work <ref type="bibr" target="#b9">(Hermann et al., 2015;</ref><ref type="bibr" target="#b27">Wang and Jiang, 2017;</ref><ref type="bibr" target="#b22">Seo et al., 2016;</ref><ref type="bibr" target="#b31">Wang et al., 2016;</ref><ref type="bibr" target="#b32">Weissenborn et al., 2017;</ref><ref type="bibr" target="#b6">Dhingra et al., 2017a;</ref><ref type="bibr" target="#b23">Shen et al., 2017)</ref> focuses on a factoid scenario where the questions can be answered by simply considering very local information, such as one or two sentences. For example, to correctly answer a question "What causes precipitation to fall?", a QA system only needs to refer to one sentence in a passage: "... In meteorology, precipitation is any product of the condensation of atmospheric water vapor that falls under gravity. ..."</p><p>A more challenging yet practical extension is the multi-hop reading comprehension (MHRC) <ref type="bibr" target="#b33">(Welbl et al., 2018)</ref>, where a system needs to properly integrate multiple evidence to correctly answer a question. <ref type="figure" target="#fig_0">Figure 1</ref> shows an example, which has three passages, a question and several candidate choices. In order to * Work done during an internship at IBM Research. ? Co-mentoring.  correctly answer the question, a system has to integrate the facts "The Hanging Gardens are in Mumbai" and "Mumbai is a city in India". There are also some irrelevant facts, such as "The Hanging Gardens provide sunset views over the Arabian Sea" and "The Arabian Sea is bounded by Pakistan and Iran", which make the task more challenging, as an MHRC model has to distinguish the relevant facts from the irrelevant ones. Despite being a practical task, so far MHRC has received relatively little research attention. One notable method, Coref-GRU <ref type="bibr" target="#b5">(Dhingra et al., 2018)</ref>, integrates multiple evidence associated with each entity mention by incorporating coreference information using a collection of GRU layers of a gated-attention reader <ref type="bibr" target="#b6">(Dhingra et al., 2017a)</ref>. However, the main disadvantage of Coref-GRU is that the coreferences it considers are usually local to a sentence, neglecting other useful global information. The top part of <ref type="figure" target="#fig_1">Figure 2</ref> shows a directed acyclic graph (DAG) with only coreference edges. In particular, the two coreference edges indicate the two facts: "The Hanging Gardens provide views over the Arabian Sea" and "Mumbai is a city in India", from which we cannot indicate the ultimate fact, "The Hanging Gardens are in India", for correctly answering this instance.</p><p>In this paper, we propose a new approach for evidence integration by considering two more useful types  of edges in addition to coreferences. The bottom part of <ref type="figure" target="#fig_1">Figure 2</ref> shows one example graph generated by our approach. In particular, we consider three types of edges. The first type of edges connect the mentions of the same entity appearing across passages or further apart in the same passage. Shown in <ref type="figure" target="#fig_1">Figure 2</ref>, one instance connects the two "Mumbai" across the two passages. Intuitively, same-typed edges help to integrate global evidence related to the same entity, which are not covered by pronouns. The second type of edges connect two mentions of different entities within a context window. They help to pass useful evidence further across entities. For example, in the bottom graph of <ref type="figure" target="#fig_1">Figure  2</ref>, both window-typed edges of 1 and 6 help to pass evidence from "The Hanging Gardens" to "India", the answer of this instance. Besides, window-typed edges enhance the relations between local mentions that can be missed by the sequential encoding baseline. Finally, coreference-typed edges are further complimentary to the previous two types.</p><p>With three types of edges, our generated graphs are complex and can have cycles, making it difficult to directly apply a DAG network (e.g. the structure of Coref-GRU). In addition, certain groups of nodes cannot be reached from each other through graph edges. Take <ref type="figure" target="#fig_1">Figure 2</ref> as an example. Information of "They" and "Arabian Sea" in the first passage cannot reach "Mumbai", "It" or "India" in the second passage, or vice versa. To handle these problems, we adopt graph neural networks <ref type="bibr" target="#b21">(Scarselli et al., 2009)</ref>, which can encode arbitrary graphs. In particular, we choose graph convolutional network (GCN) and graph recurrent network (GRN), as they have been shown successful on encoding semantic graphs <ref type="bibr" target="#b24">(Song et al., 2018a)</ref>, dependency graphs <ref type="bibr" target="#b1">(Bastings et al., 2017;</ref><ref type="bibr" target="#b25">Song et al., 2018b)</ref> and raw texts <ref type="bibr" target="#b36">(Zhang et al., 2018)</ref>.</p><p>Given an instance containing several passages and a list of candidates, we first use NER and coreference resolution tools to obtain entity mentions, and then create a graph out of the mentions and relevant pronouns. As the next step, evidence integration is executed on the graph by adopting a graph neural network on top of a sequential layer. The sequential layer learns local representation for each mention, while the graph network learns a global representation. The answer is decided by matching the representations of the mentions against the question representation.</p><formula xml:id="formula_0">! " ! # ! $ ? ? ? % " % # % &amp; ? BiLSTM</formula><p>Experiments on WikiHop <ref type="bibr" target="#b33">(Welbl et al., 2018)</ref> and ComplexWebQuestions <ref type="bibr" target="#b26">(Talmor and Berant, 2018)</ref> show that the additional types of edges we introduced are highly useful for MHRC. On the holdout testset of WikiHop, it achieves an accuracy of 65.4%, which is the best over all published results on the leaderboard 1 as of the paper submission time. On the testset of ComplexWebQuestions, it also achieves better numbers than all published results without additional annotations. To our knowledge, we are among the first to investigate graph neural networks on reading comprehension 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Baseline</head><p>As shown in <ref type="figure" target="#fig_2">Figure 3</ref>, we introduce two baselines, which are inspired by <ref type="bibr" target="#b5">Dhingra et al. (2018)</ref>. The first baseline, Local, uses a standard BiLSTM layer (shown in the green dotted box), where inputs are first encoded with a BiLSTM layer, then the representation vectors for the mentions in the passages are extracted, before being matched against the question for selecting an answer. The second baseline, Coref LSTM, differs from Local by replacing the BiLSTM layer with a DAG LSTM layer (shown in the orange dotted box) for encoding additional coreference information, as proposed by <ref type="bibr" target="#b5">Dhingra et al. (2018)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Local: BiLSTM encoding</head><p>Given a list of relevant passages, we first concatenate them into one large passage p 1 , p 2 . . . p N , where each p i is a passage word and x pi is the embedding of it. It adopts a Bi-LSTM to encode the passage:</p><formula xml:id="formula_1">? ? h i p = LSTM( ? ? h i+1 p , x pi ) ? ? h i p = LSTM( ? ? h i?1 p , x pi )</formula><p>Each hidden state contains the information of its local context. Similarly, the question words q 1 , q 2 . . . q M are first converted into embeddings x q1 , x q2 . . . x q M before being encoded by another BiLSTM:</p><formula xml:id="formula_2">? ? h j q = LSTM( ? ? h j+1 q , x qj ) ? ? h j q = LSTM( ? ? h j?1 q , x qj ) 2.2 Coref LSTM: DAG LSTM with conference</formula><p>Taking the passage word embeddings x p1 , . . . x p N and coreference information as the input, the DAG LSTM layer encodes each input word embedding (such as x pj ) with the following gated operations 3 :</p><formula xml:id="formula_3">i j = ?(W i x pj + U i i?N(j) ? ? h i p + b i ) o j = ?(W o x pj + U o i?N(j) ? ? h i p + b o ) f i,j = ?(W f x pj + U f ? ? h i p + b f ) u j = ?(W u x pj + U u i?N(j) ? ? h i p + b u ) ? ? c j p = i j u j + i?N(j) f i,j ? ? c i p ? ? h j p = o j tanh( ? ? c j p )</formula><p>N j represents all preceding words of p j in the DAG, i j , o j and f i,j are the input, output and forget gates, respectively. W x , U x and b x (x ? {i, o, f, u}) are model parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Representation extraction</head><p>After encoding both the passage and the question, we obtain a representation vector for each entity mention k , spanning from k i to k j , by concatenating the hidden states of its start and end positions, before they are correlated with a fully connected layer:</p><formula xml:id="formula_4">h k = W 1 [ ? ? h ki p ; ? ? h ki p ; ? ? h kj p ; ? ? h kj p ] + b 1 ,<label>(1)</label></formula><p>where W 1 and b 1 are model parameters for compressing the concatenated vector. Note that the current multihop reading comprehension datasets all focus on the situation where the answer is a named entity mention. Similarly, the representation vector for the question is generated by concatenating the hidden states of its first and last positions:</p><formula xml:id="formula_5">h q = W 2 [ ? ? h 1 q ; ? ? h 1 q ; ? ? h M q ; ? ? h M q ] + b 2<label>(2)</label></formula><p>where W 2 and b 2 are also model parameters.</p><p>3 Only the forward direction is shown for space consideration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Attention-based matching</head><p>After obtaining the representation vectors for the question and the entity mentions in the passages, an additive attention model <ref type="bibr" target="#b0">(Bahdanau et al., 2015)</ref> is adopted by treating all entity mention representations and the question representation as the memory and the query, respectively. In particular, the probability for an entity being the answer is calculated by summing up all the occurrences of across the input passages:</p><formula xml:id="formula_6">P r = k?N ? k k ?N ? k ,<label>(3)</label></formula><p>where N and N represent all occurrences of entity and all occurrences of all entities, respectively. Previous work  shows that summing the probabilities over all occurrences of the same entity mention is important for the multi-passage scenario. ? k is the attention score for the entity mention k , calculated by an additive attention model shown below:</p><formula xml:id="formula_7">e k 0 = v T a tanh(W a h k + U a h q + b a ) (4) a k = exp(e k 0 ) k ?N exp(e k 0 )<label>(5)</label></formula><p>where v a , W a , U a and b a are model parameters.</p><p>Comparison with <ref type="bibr" target="#b5">Dhingra et al. (2018)</ref> The Coref-GRU model <ref type="bibr" target="#b5">(Dhingra et al., 2018)</ref> is based on the gated-attention reader (GA reader) <ref type="bibr" target="#b6">(Dhingra et al., 2017a)</ref>, which is designed for the cloze-style reading comprehension task <ref type="bibr" target="#b9">(Hermann et al., 2015)</ref>, where one token is selected from the input passages as the answer for each instance. To adapt their model for the Wiki-Hop benchmark, where an answer candidate can contain multiple tokens, they first generate a probability distribution over the passage tokens with GA reader, and then compute the probability for each candidate c by aggregating the probabilities of all passage tokens that appear in c and renormalizing over the candidates. In addition to using LSTM instead of GRU 4 , the main difference between our two baselines and Dhingra et al. <ref type="formula" target="#formula_4">(2018)</ref> is that our baselines consider each candidate as a whole unit no matter whether it contains multiple tokens or not. This makes our models more effective on the datasets containing phrasal answer candidates. (GCN) <ref type="bibr" target="#b14">(Kipf and Welling, 2017)</ref>. <ref type="figure" target="#fig_3">Figure 4</ref> shows the overall procedure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Graph construction</head><p>As a first step, we create a graph from a list of input passages. The entity mentions within the passages are taken as the graph nodes. They are automatically generated by NER and coreference annotators, so that each graph node is either an entity mention or a pronoun representing an entity. We then create a graph by ensuring that edges between two nodes follow the situations below:</p><p>? They are occurrences of the same entity mention across passages or with a distance larger than a threshold ? L when being in the same passage.</p><p>? One is an entity mention and the other is its coreference. Our coreference information is automatically generated by a coreference annotator.</p><p>? Between two mentions of different entities in the same passage within a window threshold of ? S .</p><p>Between every two entities that satisfy the situations above, we make two edges in opposite directions. As a result, each generated graph can also be considered as an undirected graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Evidence integration with graph network</head><p>Tackling multi-hop reading comprehension requires inferring on global context. As the next step, we merge related information through the three types of edges just created. We investigate two recent graph networks: GRN and GCN.</p><p>Graph recurrent network (GRN) GRN models a graph as a single state, performing recurrent information exchange between graph nodes through graph state transitions. Formally, given a graph G = (V, E), a hidden state vector s k is created to represent each node v k ? V . The state of the graph can thus be represented as:</p><formula xml:id="formula_8">g = {s k }|v k ? V</formula><p>In order to integrate non-local evidence among nodes, information exchange between neighborhooding nodes is performed through recurrent state transitions, leading to a sequence of graph states g 0 , g 1 , . . . , g t , where g t = {s k t }|v k ? V and t is a hyperparameter representing the number of graph state transition decided by a development experiment. For initial state g 0 = {s k 0 }|v k ? V , we initialize each s k 0 by:</p><formula xml:id="formula_9">s k 0 = W 3 [h k ; h q ] + b 3 ,<label>(6)</label></formula><p>where h k is the corresponding representation vector of entity mention v k , calculated by Equation 1. h q is the question representation. W 3 and b 3 are model parameters.</p><p>A gated recurrent neural network is used to model the state transition process. In particular, the transition from g t?1 to g t consists of a hidden state transition for each node. At each step t, direct information exchange is conducted between a node and all its neighbors via the following LSTM <ref type="bibr" target="#b11">(Hochreiter and Schmidhuber, 1997)</ref> operations:</p><formula xml:id="formula_10">i k t = ?(W i m k t + b i ) o k t = ?(W o m k t + b o ) f k t = ?(W f m k t + b f ) u k t = ?(W u m k t + b u ) c k t = f k t c k t?1 + i k t u k t s k t = o k t tanh(c k t ),<label>(7)</label></formula><p>where c k t is the cell vector to record memory for s k t , and i k t , o k t and f k t are the input, output and forget gates, respectively. W x and b x (x ? {i, o, f, u}) are parameters. m k t is the sum of the neighborhood hidden states for the node v k 5 :</p><formula xml:id="formula_11">m k t = i?N(k) s i t?1<label>(8)</label></formula><p>N(k) represents the set of all neighbors of v k . Graph convolutional network (GCN) GCN is a convolution-based alternative to GRN for encoding graphs. Similar with GRN, a GCN model consists of two main steps: state initialization and state update. For state initialization, GCN adopts the same approach as with GRN by initializing from the representations vectors of entity mentions, as shown in Equation 6. The main difference between GCN and GRN is the way for updating node states. GRN adopts gated operations (shown in Equation 7), while GCN uses linear transportation with sigmoid:</p><formula xml:id="formula_12">s k t = ?(W g m k t + b g ),<label>(9)</label></formula><p>where m k t is also the sum of the neighborhood hidden states defined in Equation 8. W g and b g are model parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Matching and combination</head><p>After evidence integration, we match the hidden states at each graph encoding step with the question representation using the same additive attention mechanism introduced in the Baseline section. In particular, for each entity v k , the matching results for the baseline and each GRN step t are first generated, before being combined using a weighted sum to obtain the overall matching result:</p><formula xml:id="formula_13">e k t = v T at tanh(s k t W at + h q U at + b at ) (10) e k = w c [e k 0 , e k 1 , . . . , e k T ] + b c ,<label>(11)</label></formula><p>where e k 0 is the baseline matching result for v k , e k t is the matching results after t GRN steps and T is the number of graph encoding steps. W at , U at , v at , b at , w c and b c are model parameters. In addition, a probability distribution is calculated from the overall matching results using softmax, similar to Equations 5. Finally, probabilities that belong to the same entity mention are merged to obtain the final distribution, as shown in Equation 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Training</head><p>We train both the baseline and our models using the cross-entropy loss:</p><formula xml:id="formula_14">l = ? log p( * |X; ?),</formula><p>where * is ground-truth answer, X and ? are the input and model parameters, respectively. Adam <ref type="bibr" target="#b13">(Kingma and Ba, 2014)</ref> with a learning rate of 0.001 is used as the optimizer. Dropout with rate 0.1 and a l2 normalization weight of 10 ?8 are used during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments on WikiHop</head><p>We study the effectiveness of the three types of edges and the graph encoders using WikiHop <ref type="bibr" target="#b33">(Welbl et al., 2018)</ref> dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Data</head><p>The dataset contains around 51K instances, including 44K for training, 5K for development and 2.5K for held-out testing. Each instance consists of a question, a list of associated passages, a list of candidate answers and a correct answer. One example is shown in <ref type="figure" target="#fig_0">Figure  1</ref>. We use Stanford CoreNLP  to obtain coreference and NER annotations. Then the entity mentions, pronoun coreferences and the provided candidates are taken as graph nodes to create an evidence graph. The distance thresholds (? L and ? S , in Graph construction) for making same and window typed edges are set to 200 and 20, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Settings</head><p>We study the model behavior on the WikiHop devset, choosing the best hyperparameters for online system  For model hyperparameters, we set the graph state transition number as 3 according to development experiments. Each node takes information from at most 200 neighbors, where same and coref typed neighbors are kept first. The hidden vector sizes for both bidirectional LSTM and GRN layers are set to 300 6 . <ref type="figure" target="#fig_4">Figure 5</ref> shows the devset performances of our model using GRN or GCN with different transition steps. It shows the baseline performances when transition step is 0. The performances go up for both models when increasing the transition step to 3. Further increasing the transition step leads to a slight decrease in performance. One reason can be that executing more transition steps may also introduce more noise through richly connected edges. GRN shows better performances than GCN with large transition steps, indicating that GRN are better at capturing long-range dependency. This is likely because the gated operations of GRN is better at handling the vanishing/exploding gradient problem than the linear operations of GCN.  <ref type="table">Table 2</ref>: Ablation study on different types of edges using GRN as the graph encoder. <ref type="table">Table 1</ref> shows the main comparison results with existing work, where GA w/ GRU and GA w/ Coref-GRU correspond to <ref type="bibr" target="#b5">Dhingra et al. (2018)</ref>, and their reported numbers are copied. The former is their baseline, a gated-attention reader <ref type="bibr" target="#b6">(Dhingra et al., 2017a)</ref>, and the latter is their proposed method. Local is our baseline encoding input passages with a BiLSTM, which only captures local information for each mention. Coref LSTM is our baseline that encodes input passages with coreference annotations by using a bidirectional DAG LSTM. This can be considered as a reimplementation of <ref type="bibr" target="#b5">Dhingra et al. (2018)</ref> based on our framework. Coref GRN is another baseline that uses GRN for encoding coreference. It is an ablation study of our model on coreference DAGs, and is for contrasting a DAG network with a graph network. MHQA-GCN and MHQA-GRN correspond to our evidence integration approaches via graph encoding, adopting GCN and GRN for graph encoding, respectively. Our baselines and models show much higher accuracies compared with GA w/ GRU and GA w/ Coref-GRU, as our models are more compatible with the evaluated dataset. In particular, we consider each candidate answer as a single unit, while GA w/ GRU and GA w/ Coref-GRU calculate the probability for each candidate by summing up the probabilities of all tokens within the candidate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Development experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Main results</head><p>Coref LSTM only shows 0.4 points gains over Local. On the other hand, MHQA-GCN and MHQA-GRN are 1.4 and 1.8 points more accurate than Local, respectively. This is mainly because our graphs are better connected than coreference DAGs and are more suitable for integrating relevant evidence. Coref GRN gives a comparable performance with Coref LSTM, showing that graph networks may not necessarily be better than DAG networks on encoding DAGs. However, the former are more general on encoding arbitrary graphs.</p><p>Finally, MHQA-GRN shows a higher testing accuracy than all published results 7 . 7 At submission time we observe a recent short arXiv paper <ref type="bibr" target="#b2">(Cao et al., 2018)</ref>, available on August 28th, showing an accuracy of 67.6 using ELMo <ref type="bibr" target="#b20">(Peters et al., 2018)</ref>, which is the only result better than ours. ELMo has achieved dramatic performance gains of 3+ points over a broad range of tasks. Our main contribution is studying an evidence integration ap-  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Analysis</head><p>Effectiveness of edge types <ref type="table">Table 2</ref> shows the effectiveness of different types of edges that we introduce. The first group shows the ablation study, which indicates the importance of each type of edges. Among all these types, removing window-typed edges causes the least performance drop. One possible reason is that some information captured by window-typed edges has been well captured by sequential encoding. However, window-typed are still useful, as they can help passing evidence through to further nodes. Take <ref type="figure" target="#fig_1">Figure 2</ref> as an example, window-typed edges help to pass information from "The Hanging Gardens" to "India". The other two types of edges are more important than window-typed ones. Intuitively, they help to learn a better representation for an entity by integrating the contextual information from its co-references and occurrences.</p><p>The second group of <ref type="table">Table 2</ref> shows the model performances when only one type of edges are used. The numbers generally demonstrate the same patterns as the first group. In addition, only same is slightly better than only coref. It is likely because some coreference information can also be captured by sequential encoding. None of the results with a single edge type is significantly better than our strong baseline, whereas the combination of all three types achieves a much better result. This indicates the importance of evidence integration over multiple edge types.</p><p>Distance <ref type="figure" target="#fig_5">Figure 6</ref> shows the percentage distribution of distances between a question and its closet answer when either all types of edges are adopted or only coreference edges are used. The subject of each question 8 is used to locate the question on the corresponding graph.</p><p>When all types of edges are used, the instances with distances less than or equal to 3 count for around 70% of all the instances. On the other hand, the instances with distances longer than 4 only count for 10%. This  can be the reason why performances do not increase when more than 3 transition steps are performed in our model. The advantage of our graph construction method can be shown by contrasting the distance distributions over graphs generated by both the baseline and our method. We further evaluate both methods on a subset of the devset instances, where for each instance the distance between the answer and the question is at most 3 in our graph but is infinity on the coreference DAG. The performances of Coref LSTM and MHQA-GRN on this subset are 61.1 and 63.8, respectively. Comparing with the performances on the whole devset (61.4 vs 62.8), the performance gap is increased by 1.3 points on this subset, which further confirms our observation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experiments on ComplexWebQuestions</head><p>In additional to WikiHop, we conduct experiments on the newly released ComplexWebQuestions version 1.1 <ref type="bibr" target="#b26">(Talmor and Berant, 2018)</ref> for better evaluating our approach. Compared with WikiHop, where the complexity is implicitly specified in the passages, the complexity of this dataset is explicitly specified on the question side. One example question is "What city is the birthplace of the author of 'Without end"'. A two-step reasoning is involved, with the first step being "the author of 'Without end"' and the second being "the birthplace of x".</p><p>x is the answer of the first step. In this dataset, web snippets (instead of passages as in WikiHop) are used for extracting answers. The baseline of <ref type="bibr" target="#b26">Talmor and Berant (2018)</ref> (SimpQA) only uses a full question to query the web for obtaining relevant snippets, while their model (SplitQA) obtains snippets for both the full question and its sub-questions. With all the snippets, SplitQA models the QA process based on a computation tree 9 of the full question. In particular, they first obtain the answers for the sub-questions, and then integrate those answers based on the computation tree. In contrast, our approach creates a graph from all the snippets, thus the succeeding evidence integration process can join all associated evidence.</p><p>Main results As shown in <ref type="table" target="#tab_6">Table 3</ref>, similar to the observations in WikiHop, both MHQA-GRN and MHQA-GCN achieve large improvements over Local, and MHQA-GRN gives slightly better accuracy. Both the baselines and our models use all web snippets, but MHQA-GRN and MHQA-GCN further consider the structural relations among entity mentions. SplitQA achieves 0.5% improvement over SimpQA 10 . Our Local baseline is comparable with SplitQA and our graphbased models contribute a further 2% improvement over Local. This indicates that considering structural information on passages is important for the dataset.</p><p>Analysis To deal with complex questions that require evidence from multiple passages to answer, previous work <ref type="bibr" target="#b15">Lin et al., 2018;</ref><ref type="bibr" target="#b30">Wang et al., 2018c)</ref> collect evidence from occurrences of an entity in different passages. The above methods correspond to a special case of our method, i.e. MHQA with only the same-typed edges. From <ref type="table" target="#tab_6">Table 3</ref>, our method gives 1 point increase over MHQA-GRN w/ only same, and it gives more increase in WikiHop (comparing all types with only same in <ref type="table">Table 2</ref>). Both results indicate that our method could capture more useful information for multi-hop QA tasks, compared to the methods developed for previous multi-passage QA tasks. This is likely because our method integrates not only evidences for an entity but also these for other related entities.</p><p>The leaderboard reports SplitQA with additional sub-question annotations and gold answers for subquestions. These pairs of sub-questions and answers are used as additional data for training SplitQA. The above approach relies on annotations of ground-truth answers for sub-questions and semantic parses, thus is not practically useful in general. However, the results have additional value since it can be viewed as an upper bound of SplitQA. Note that the gap between this upper bound and our MHQA-GRN is small, which further proves that larger improvement can be achieved by introducing structural information on the passage side.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Related Work</head><p>Question answering with multi-hop reasoning Most existing work on multi-hop QA focuses on hopping over knowledge bases or tables <ref type="bibr" target="#b12">(Jain, 2016;</ref><ref type="bibr" target="#b18">Neelakantan et al., 2016;</ref><ref type="bibr" target="#b35">Yin et al., 2016)</ref>, thus the problem is reduced to deduction on a readily-defined structure with known relations. On the other hand, we study multi-hop QA on textual data and we introduce an effective approach on creating graph structures over the textual input for solving our problems. Previous work <ref type="bibr" target="#b10">(Hill et al., 2015;</ref><ref type="bibr" target="#b23">Shen et al., 2017)</ref> studying multi-hop QA on text does not create structures. In addition, they only eval-uate on a simple task  with a very limited vocabulary and passage length. Our work is fundamentally different from theirs by modeling structures over the input, and we evaluate our models on more challenging tasks.</p><p>Recent work starts to exploit ways for creating structures from inputs. <ref type="bibr" target="#b26">Talmor and Berant (2018)</ref> build a two-level computation tree over each question where the first-level nodes are sub-questions and the secondlevel node is a composition operation. The answers for the sub-questions are first generated, and then combined with the composition operation. They predefine two composition operations, so it is not general enough for other QA problems. On the other hand, <ref type="bibr" target="#b5">Dhingra et al. (2018)</ref> create DAGs over passages with coreference. The DAGs are then encoded with a DAG network. Our work follows the second direction by creating graphs on the passage side. However, we consider more types of relations than coreference, making a thorough study on relation types. Besides, we also investigate recent graph networks on this problem.</p><p>Question answering over multiple passages Recent efforts in open-domain QA start to generate answers from multiple passages instead of from a single passage. However, most existing work on multipassage QA selects the most relevant passage for answering the given question, thus reducing the problem to single-passage reading comprehension <ref type="bibr" target="#b8">Dunn et al., 2017;</ref><ref type="bibr" target="#b7">Dhingra et al., 2017b;</ref>. Our method is fundamentally different by truly leveraging multiple passages.</p><p>A few multi-passage QA approaches merge evidence from multiple passages before selecting an answer <ref type="bibr" target="#b15">Lin et al., 2018;</ref><ref type="bibr" target="#b30">Wang et al., 2018c)</ref>. Similar to our work, they combine evidences from multiple passages, thus they fully utilize the input passages. The key difference is that their approaches focus on how the contexts of a single answer candidate from different passages could cover different aspects of a complex question, while our approach studies how to properly integrate the related evidence of an answer candidate, some of which come from the contexts of different entity mentions. Specially, it increases the difficulty, since those contexts do not co-occur with the candidate answer nor the question. This is also demonstrated by our empirical comparison, where our approach shows much better performance than combining only the evidence of the same entity mentions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>We have introduced a new approach for tackling multihop reading comprehension (MHRC) with an evidence integration process. Given a question and a list of passages, we first use three types of edges to connect related evidence, and then adopt recent graph neural networks to encode resulted graphs for performing evidence integration. Results show that the three types of edges are useful on combining global evidence and that the graph neural networks are effective on encoding complex graphs resulted by the first step. Our approach shows the highest performance among all published results on two standard MHRC datasets.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>An example from WikiHop<ref type="bibr" target="#b33">(Welbl et al., 2018)</ref>, where some relevant entity mentions and their coreferences are highlighted.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>A DAG generated by<ref type="bibr" target="#b5">Dhingra et al. (2018)</ref> (top) and a graph by considering all three types of edges (bottom) on the example inFigure 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Baselines. The upper dotted box is a DAG LSTM layer with addition coreference links, while the bottom is a typical BiLSTM layer. Either layer is used.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Model framework.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>DEV performances of different transition steps.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Distribution of distances between a question and an answer on the DEVSET.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>[</head><label></label><figDesc>The Hanging Gardens], in [Mumbai], also known as Pherozeshah Mehta Gardens, are terraced gardens ... [They] provide sunset views over the [Arabian Sea] ... [Mumbai] (also known as Bombay, the official name until 1995) is the capital city of the Indian state of Maharashtra. [It] is the most populous city in [India] ... The [Arabian Sea] is a region of the northern Indian Ocean bounded on the north by [Pakistan] and [Iran], on the west by northeastern [Somalia] and the Arabian Peninsula, and on the east by [India] ...</figDesc><table /><note>Q: (The Hanging gardens, country, ?) Options: Iran, India, Pakistan, Somalia, ...</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 :</head><label>3</label><figDesc>Results on the ComplexWebQuestions dataset.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">http://qangaroo.cs.ucl.ac.uk/leaderboard.html 2 The concurrent unpublished work (Cao et al., 2018) also investigate the usage of graph convolution networks on Wik-iHop. Our work proposes a different model architecture, and focus more on the exploration and comparison of multiple edge types for building the graph-structured passage representation.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Evidence integration with graph networkAfter obtaining the representation vectors for a question and the corresponding entity mentions, we build a graph out of the entity mentions by connecting relevant mentions with edges, and then integrating relevant information for each graph node (entity mention) with a graph recurrent network (GRN)<ref type="bibr" target="#b36">(Zhang et al., 2018;</ref><ref type="bibr" target="#b24">Song et al., 2018a)</ref> or a graph convolutional network 4 Model architectures are selected according to dev results.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">We tried distinguishing the neighbors connected by different types of edges, but it does not improve the performance.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">We tried larger hidden sizes for our baselines, but did not observe further improvement.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">proach, which is orthogonal to the contribution of ELMo on large-scale training. For more fair comparison with existing work, we did not adopt ELMo, but we will conduct experiments with ELMo as well.8  As shown inFigure 1, each question is a three-element tuple of subject, relation and a question mark (asking for the object).</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9">A computation tree is a special type of semantic parse, which has two levels. The first level contains sub-questions and the second level is a composition operation.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10">Upon the submission time, the authors of ComplexWe-bQuestions have not reported testing results for the two methods. To make a fair comparison we compare the devset accuracy.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Graph convolutional encoders for syntax-aware neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joost</forename><surname>Bastings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wilker</forename><surname>Aziz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diego</forename><surname>Marcheggiani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khalil</forename><surname>Simaan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Question answering by reasoning across documents with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicola</forename><surname>De Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wilker</forename><surname>Aziz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Titov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.09920</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Reading wikipedia to answer opendomain questions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Fisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1870" to="1879" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Simple and effective multi-paragraph reading comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Neural models for reasoning over multiple mentions using coreference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhuwan</forename><surname>Dhingra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiao</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="42" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Gatedattention readers for text comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhuwan</forename><surname>Dhingra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1832" to="1846" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">QUASAR: Datasets for question answering by search and reading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhuwan</forename><surname>Dhingra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kathryn</forename><surname>Mazaitis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.03904</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Dunn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Levent</forename><surname>Sagun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Higgins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ugur</forename><surname>Guney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volkan</forename><surname>Cirik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.05179</idno>
		<title level="m">SearchQA: A new q&amp;a dataset augmented with context from a search engine</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Teaching machines to read and comprehend</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Moritz Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Kocisky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lasse</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mustafa</forename><surname>Suleyman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1693" to="1701" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">The goldilocks principle: Reading children&apos;s books with explicit memory representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.02301</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Question answering over knowledge base using factual memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarthak</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the NAACL Student Research Workshop</title>
		<meeting>the NAACL Student Research Workshop</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="109" to="115" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Semisupervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Denoising distantly supervised open-domain question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haozhe</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1736" to="1745" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The Stanford CoreNLP natural language processing toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenny</forename><surname>Finkel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><forename type="middle">J</forename><surname>Bethard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mc-Closky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="55" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Encoding sentences with graph convolutional networks for semantic role labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diego</forename><surname>Marcheggiani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Titov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1506" to="1515" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Neural programmer: Inducing latent programs with gradient descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">GloVe: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep contextualized word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">The graph neural network model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franco</forename><surname>Scarselli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ah</forename><surname>Chung Tsoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Hagenbuchner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriele</forename><surname>Monfardini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="61" to="80" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Bidirectional attention flow for machine comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minjoon</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aniruddha</forename><surname>Kembhavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01603</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Reasonet: Learning to stop reading in machine comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yelong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Po-Sen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SIGKDD</title>
		<meeting>SIGKDD</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1047" to="1055" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A graph-to-sequence model for AMR-to-text generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linfeng</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiguo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Gildea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1616" to="1626" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">N-ary relation extraction using graph state LSTM</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linfeng</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiguo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Gildea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">The web as a knowledge-base for answering complex questions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alon</forename><surname>Talmor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Machine comprehension using match-lstm and answer pointer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuohang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">R3: Reinforced ranker-reader for open-domain question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuohang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mo</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxiao</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiguo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Klinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyu</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerald</forename><surname>Tesauro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Evidence aggregation for answer re-ranking in open-domain question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuohang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mo</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxiao</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyu</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiguo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Klinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerald</forename><surname>Tesauro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Murray</forename><surname>Campbell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Joint training of candidate extraction and answer selection for reading comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiachen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyan</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yajuan</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1715" to="1724" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Multi-perspective context matching for machine comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiguo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haitao</forename><surname>Mi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wael</forename><surname>Hamza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Florian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.04211</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Making neural qa as simple as possible but not simpler</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Wiese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Seiffe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CoNLL</title>
		<meeting>CoNLL</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="271" to="280" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Constructing datasets for multi-hop reading comprehension across documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Welbl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pontus</forename><surname>Stenetorp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TACL</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="287" to="302" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Towards ai-complete question answering: A set of prerequisite toy tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merri?nboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.05698</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengcheng</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Kao</surname></persName>
		</author>
		<title level="m">Neural enquirer: Learning to query tables with natural language</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Sentencestate LSTM for text representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linfeng</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="317" to="327" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
