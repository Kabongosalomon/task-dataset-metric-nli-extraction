<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep Neural Network Augmentation: Generating Faces for Affect Analysis</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitrios</forename><surname>Kollias</surname></persName>
							<email>dimitrios.kollias15@imperial.ac.uk?shiyang.cheng11@imperial.ac.uk</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyang</forename><surname>Cheng</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evangelos</forename><surname>Ververas</surname></persName>
							<email>*e.ververas16@imperial.ac.uk</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irene</forename><surname>Kotsia</surname></persName>
							<email>1i.kotsia@mdx.ac.uk</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Middlesex University of London</orgName>
								<address>
									<postCode>NW4 4BT</postCode>
									<settlement>London</settlement>
									<country key="GB">U.K</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Stefanos</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zafeiriou</forename></persName>
							<email>2s.zafeiriou@imperial.ac.uk</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Department of Computing</orgName>
								<orgName type="institution">Imperial College London</orgName>
								<address>
									<addrLine>Queens Gate</addrLine>
									<postCode>SW7 2AZ</postCode>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Deep Neural Network Augmentation: Generating Faces for Affect Analysis</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note type="submission">Received: Sept 30th 2018 / Accepted: date</note>
					<note>Noname manuscript No. (will be inserted by the editor)</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T15:34+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>dimensional</term>
					<term>categorical affect</term>
					<term>valence</term>
					<term>arousal</term>
					<term>basic emotions</term>
					<term>facial affect synthesis</term>
					<term>4DFAB</term>
					<term>blendshape models</term>
					<term>3DMM fitting</term>
					<term>DNNs</term>
					<term>StarGAN</term>
					<term>Ganimation</term>
					<term>data augmentation</term>
					<term>affect recognition</term>
					<term>facial expression transfer</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper presents a novel approach for synthesizing facial affect; either in terms of the six basic expressions (i.e., anger, disgust, fear, joy, sadness and surprise), or in terms of valence (i.e., how positive or negative is an emotion) and arousal (i.e., power of the emotion activation). The proposed approach accepts the following inputs: i) a neutral 2D image of a person; ii) a basic facial expression or a pair of valencearousal (VA) emotional state descriptors to be generated, or a path of affect in the 2D VA Space to be generated as an image sequence. In order to synthesize affect in terms of VA, for this person, 600, 000 frames from the 4DFAB database were annotated. The affect synthesis is implemented by fitting a 3D Morphable Model on the neutral image, then deforming the reconstructed face and adding the inputted affect, and blending the new face with the given affect into the original image. Qualitative experiments illustrate the generation of realistic images, when the neutral image is sampled from thirteen well known lab-controlled or in-the-wild databases, including Aff-Wild, AffectNet, RAF-DB; comparisons with Generative Adversarial Networks (GANs) show the higher quality achieved by the proposed approach. Then, quantitative experiments are conducted, in which the synthesized images are used for data augmentation in training Deep Neural Networks to perform affect recognition over all databases; greatly improved performances are achieved when compared with state-of-theart methods, as well as with GAN-based data augmentation, in all cases.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Rendering photorealistic facial expressions from single static faces while preserving the identity information is an open research topic which has significant impact on the area of affective computing. Generating faces of a specific person with different facial expressions can be used in various applications, including face recognition <ref type="bibr" target="#b12">[12]</ref>  <ref type="bibr" target="#b45">[44]</ref>, face verification <ref type="bibr" target="#b59">[58]</ref>  <ref type="bibr" target="#b61">[60]</ref>, emotion prediction, expression database generation, facial expression augmentation and entertainment. This paper describes a novel approach that uses an arbitrary face image with a neutral expression and synthesizes a new face image of the same person, but with a different expression, generated according to a categorical or dimensional emotion representation model. This problem cannot be tackled using small databases with labeled facial expressions, as it would be really difficult to disentangle facial expressions and identity information through them. Our approach is based on the analysis of a large 4D facial database, the 4DFAB <ref type="bibr" target="#b14">[14]</ref>, which we appropriately annotated and used for facial expression synthesis on a given subject's face.</p><p>At first, a dimensional emotion model, in terms of the continuous variables, valence (i.e., how positive or negative is an emotion) and arousal (i.e., power of the emotion activation) <ref type="bibr" target="#b68">[67]</ref>  <ref type="bibr" target="#b54">[53]</ref>, has been used to annotate a large amount of 600,000 facial images. This model can represent, not only primary, extreme expressions, but also subtle expressions which are met in everyday human to human, or human to machine interactions. Additionally, a categorical emotion model, in terms of the six basic facial expressions, has been used, according to which 12,000 expressions from the 4DFAB were selected, including 2,000 cases for each of the six basic expressions.</p><p>The proposed approach accepts: i) a pair of valencearousal values and synthesize the respective facial affect, ii) a path of affect in the 2D VA Space and synthesize a temporal sequence showing it, iii) a value indicating the basic facial expression to be synthesized; a given neutral 2D image of a person is used in all cases to appropriately transfer the synthesized affect.</p><p>Section 2 refers to related work regarding facial expression synthesis, as well as data augmentation related methodologies. Section 3 presents materials and methods that are used in the current work. We describe the annotation and use of the 4DFAB database and the 3D Morphable Model that we utilize in our developments. Section 4 presents our approach, explaining in detail all steps used to synthesize affect on an image or image sequence. Section 5 mentions the categorical and dimensional databases, which are used by our approach.</p><p>An extensive experimental study is presented in Section 6. At first, a qualitative evaluation of the proposed approach is provided, also showing the achieved higher quality when compared to GAN-generated facial affect. Then, we use the synthesized facial images for data augmentation and train Deep Neural Networks over eight databases, annotated with either dimensional or categorical affect labels. We show that the achieved performance is much higher than i) that obtained by the respective state-of-the-art methods, ii) the performance of the same DNNs with data augmentation provided by the StarGAN and Ganimation networks. A further comparison with GANs is performed, with the synthesized facial images being used, together with the original images, as DNN training and/or test data respectively; this also verifies the improved performance of our approach. An ablation study is also presented, illustrating the effect of data granularity and subjects' age on the performance of the proposed method. Finally, conclusions and future work are presented in Section 6.</p><p>The proposed approach includes many novel contributions. To the best of our knowledge, it is the first time that the dimensional model of affect is taken into account when synthesizing face images. As verified in the experimental study, the generated images are of high quality and realistic. All other methods produce synthesized faces according to the six basic, or a few more, expressions. We further show that the proposed approach can accurately synthesize the six basic expressions.</p><p>Moreover, it is the first time that a 4D face database is annotated in terms of valence and arousal and is then used for affect synthesis. The fact that this a temporal database ensures that successive video frames' annotations are adjacent in the VA Space. Consequently, we generate temporal affect sequences on a given neutral face by using annotations that are adjacent in the VA Space. Results are presented in the qualitative experimental study that illustrate this novel capability.</p><p>It should be also mentioned that the proposed approach works well, when presented with a neutral face image, obtained either in a controlled environment, or in-the-wild, e.g., irrespective of the head pose of the person appearing in the image.</p><p>An extensive experimental study is provided, over most significant databases with affect, showing that the developed DNNs based on the proposed facial affect synthesis approach outperform the existing stateof-the-art, as well the same DNNs based on facial affect synthesis produced by GAN architectures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Facial expression transfer is a research field for mapping and generating desired images of specified subject and facial expression. Many methods achieved significant results for high-resolution images and are applied to a wide range of applications, such as facial animation, facial editing, and facial expression recognition.</p><p>There are mainly two categories of methods for facial expression transfer from a single image: traditional graphic-based methods and emerging generative methods. In the first case, some methods directly warp the input face to create the targeted expression, by either 2D warps <ref type="bibr" target="#b22">[22,</ref><ref type="bibr" target="#b23">23]</ref>, or 3D warps <ref type="bibr" target="#b7">[7,</ref><ref type="bibr" target="#b11">11,</ref><ref type="bibr" target="#b37">36]</ref>. Other methods construct parametric global models. In <ref type="bibr" target="#b42">[41]</ref>, a probabilistic model is learned, in which existing and generated images obey structural constraints. <ref type="bibr" target="#b4">[5]</ref> added finescale dynamic details, such as wrinkles and inner mouth, that are associated with facial expressions. Although these methods have achieved some positive results in high-resolution and one-to-many image synthesis, they are still limited due to their sophisticated design and expensive computation.</p><p>In <ref type="bibr" target="#b62">[61]</ref>, the authors developed a real-time face-toface expression transfer system, with an extra blending step for mouth. This 2D-to-3D approach shows promising results, but due to the nature of its formulation, it is unable to retrieve fine-details, and its applicability is limited to expressions lying in a linear shape subspace with known rank. The authors extended this system to human portrait video transfer <ref type="bibr" target="#b63">[62]</ref>. They captured facial expressions, eye gaze, rigid head pose, and motions of the upper body of a source actor and transferred them to a target actor in real time.</p><p>The second category of methods is based on datadriven generative models. At the beginning, some generative models, such as deep belief nets (DBN) <ref type="bibr" target="#b60">[59]</ref> and higher-order Boltzmann machines <ref type="bibr" target="#b51">[50]</ref>, had been applied to facial expression synthesis. However, these models faced problems such as blurry generated images, incapability of fine control of facial expression and lowresolution outputs.</p><p>With the recent development of Generative Adversarial Networks (GANs) <ref type="bibr" target="#b25">[25]</ref>, these networks have been applied to facial expression transfer; due to the fact that the generated images are of high-quality, these provided positive results. A generative model is trained according to a dataset, including all information about identity, expression, viewing angle, etc, while performing facial expression transfer. Generative modeling methods reduce the complicated design of the connection between facial textures and emotional states and encode intuitionistic facial features into parameters of data distribution. However, the main drawback of GANs is the training instability and the trade-off between visual quality and image diversity.</p><p>Since the original GAN could not generate facial images with a specific facial expression referring to a specific person, some methods conditioned on expression categories have been proposed. Conditional GANs (cGANs) <ref type="bibr" target="#b41">[40]</ref> (and conditional variational autoencoders (cVAEs) <ref type="bibr" target="#b57">[56]</ref>) can generate samples conditioned on attribute information, when this is available. Those networks require large training databases so that identity information can be properly disambiguated. Otherwise, when presented with an unseen face, the networks tend to generate faces which look like the closest subject in the training datasets. During training, those networks require the knowledge of the attribute labels; it is not clear how to adapt them to new attributes without retraining from scratch. Finally, these networks suffer from mode-collapse (e.g., the generator only outputs samples from a single mode, or with extremely low variety) and blurriness.</p><p>The conditional difference adversarial autoencoder (CDAAE) <ref type="bibr" target="#b75">[74]</ref> aims at synthesizing specific expressions for unseen persons with a targeted emotion or facial action unit label. However, such GAN-based methods are still limited to discrete facial expression synthesis, i.e., they cannot generate a face sequence showing a smooth transition from an emotion to another. <ref type="bibr" target="#b20">[20]</ref> proposed an Expression Generative Adversarial Network (ExprGAN) in which the expression intensity could be controlled in a continuous manner from weak to strong. The identity and expression representation learning were disentangled and there was no rigid requirement of paired samples for training. The authors developed a threestage incremental learning algorithm to train the model on small datasets.</p><p>In <ref type="bibr" target="#b48">[47]</ref>, the authors proposed a weakly supervised adversarial learning framework for automatic facial expression synthesis based on continuous action unit coefficients. In <ref type="bibr" target="#b49">[48]</ref>, the Ganimation was proposed that additionally controlled the generated expression by AU labels, and allowed a continuous expression transformation. In addition, the authors introduced an attentionbased generator to promote the robustness of their model for distracting backgrounds and illuminations.</p><p>Recently, <ref type="bibr" target="#b58">[57]</ref> utilized landmarks and proposed the geometry-guided GAN (G2GAN) to generate smooth image sequences of facial expressions. G2GAN uses geometry information based on dual adversarial networks to express face changes and synthesizes facial images. Through manipulating landmarks, smoothly changed images can also be generated. However, this method demands a neutral face of the targeted person as the intermediate of facial expression transfer. Although the expression removal network could generate a neutral expression of a specific person, this procedure brings additional artifacts and degrades the performance of expression transition.</p><p>[49] used geometry (facial landmarks) to control the expression synthesis with a facial geometry embedding network and proposed a Geometry-Contrastive Generative Adversarial Network (GC-GAN) to transfer continuous emotions across different subjects, even if there existed big difference in shapes. <ref type="bibr" target="#b70">[69]</ref> proposed a boundary latent space and boundary transformer. They mapped the source face into the boundary latent space, and transformed the source faces boundary to the targets boundary, which was the medium to capture facial geometric variances during expression transfer.</p><p>In <ref type="bibr" target="#b38">[37]</ref>, an unpaired learning framework was developed to learn the mapping between any two facial expressions in the facial blendshape space. This framework automatically transforms the source expression in an input video clip to a specified target expression. This work lacks the capability to generate personalized expressions; individual-specific expression characteristics, such as wrinkles and creases, are ignored. Also, the transitions between different expressions are not taken into consideration. Finally, this work is limited in the sense that it cannot produce highly exaggerated expressions.</p><p>Both the graphic-based methods and the genererative methods of facial expression transfer have been used to create synthetic data that are used as auxiliary data in network training, augmenting the training dataset. A synthetic data generation system with a 3D convolutional neural network (CNN) was created in <ref type="bibr" target="#b0">[1]</ref> to confidentially create faces with different levels of saturation in expression. <ref type="bibr" target="#b3">[4]</ref> proposed the Data Augmentation Generative Adversarial Network (DAGAN) which is based on cGAN and tested its effectiveness on vanilla classifiers and one shot learning. DAGAN is a basic framework for data augmentation based on cGAN.</p><p>[76] presented another basic framework for face data augmentation based on CycleGAN <ref type="bibr" target="#b76">[75]</ref>. Similar to cGAN, CycleGAN is also an general-purpose solution for imageto-image translation, but it learns a dual mapping between two domains simultaneously with no need for paired training examples, because it combines a cycle consistency loss with adversarial loss. The authors used this framework to generate auxiliary data for imbalanced datasets, where the data class with fewer samples was selected as transfer target and the data class with more samples was the reference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Materials &amp; Methods</head><p>In the following, we first describe the 4DFAB database, its annotation in terms of valence-arousal and the selection of expressive categorical sequences from it. The annotated 4DFAB database has been used for constructing the 3D facial expression gallery that is the basis of our affect synthesis pipeline described in the next Section. Then we describe the methods we have used: a) for registering and correlating all components of the 3D gallery into a universal coordinate frame; b) for constructing the 3D Morphable Model used in this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">The 4DFAB Database</head><p>The 4DFAB database <ref type="bibr" target="#b14">[14]</ref> is the first large scale 4D face database designed for biometric applications and facial expression analysis. It consists of 180 subjects (60 females, 120 males) aging from 5 to 75 years. 4DFAB was collected over a period of 5 years under four different sessions, with over 1,800,000 3D faces. The database was designed to capture articulated facial actions and spontaneous facial behaviors, the latter being elicited by watching emotional video clips. In each of the four sessions, different video clips were shown that stimulated different spontaneous behaviors. In this paper, we use all 1,580 spontaneous expression sequences (video clips) for dimensional emotion analysis and synthesis. The frame rate of 4DFAB database is 60 FPS and the average clip length for spontaneous expression sequences is 380 frames. Consequently the 1,580 expression sequences correspond to 600,000 frames, which we annotated in terms of valence and arousal (details follow in the next subsection). These sequences cover a wide range of expressions as shown in Figs. 2 and 3.</p><p>Moreover, to be able to develop the categorical emotion synthesis model, we used the 2,000 expressive 3D meshes per basic expression (12,000 meshes in total) that were provided along with 4DFAB. Those 3D meshes corresponded to (annotated) apex frames of posed expression sequences in 4DFAB. Such examples are shown in <ref type="figure" target="#fig_0">Fig. 1</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">4DFAB Dimensional Annotation</head><p>Targeting to develop the novel dimensional expression synthesis method, all 1,580 dynamic 3D sequences (i.e., over 600,000 frames) of 4DFAB have been annotated in terms of valence and arousal emotion dimensions. In total, three experts were chosen to perform the annotation task. Each expert performed a time-continuous annotation for both affective dimensions. The applicationtool described in <ref type="bibr" target="#b72">[71]</ref>, was used in the annotation process.</p><p>Each expert logged into the application-annotation tool using an identifier (e.g. his/her name) and selected an appropriate joystick; then the application showed a scrolling list of all videos and the expert selected a video to annotate; then a screen appeared that showed the selected video and a slider of valence or arousal values ranging in [?1, 1] ; the expert annotated the video by moving the joystick either up or down; finally, a file was created with the annotations. The mean interannotation correlation per annotator was 0.66, 0.70, 0.68 for valence and 0.59, 0.62, 0.59 for arousal. The average of those mean inter-annotation correlations was 0.68 for valence and 0.60 for arousal. Those values are high, indicating a very good agreement between annotators. As a consequence, the final label values were chosen to be the mean of those three annotations.</p><p>Examples of frames from the 4DFAB along with their annotations, are shown in <ref type="figure" target="#fig_1">Fig. 2</ref>. <ref type="figure" target="#fig_2">Fig. 3</ref> shows the 2D histogram of annotations of 4DFAB. In the rest of the paper, we refer to the 4DFAB database either as: i) the 600,000 frames with their corresponding 3D meshes, which have been annotated with 2D valence and arousal (VA) emotion values or ii) the 12,000 apex frames of posed expressions with their corresponding 3D meshes, which have categorical annotation. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Mesh Pre-Processing: Establishing Dense Correspondence</head><p>Each 3D mesh is first re-parameterized into a consistent form where the number of vertices, the triangulation and the anatomical meaning of each vertex are made consistent across all meshes. For example, if the vertex with index i in one mesh corresponds to the nose tip, it is required that the vertex with the same index in every mesh corresponds to the nose tip too. Meshes satisfying the above properties are said to be in dense correspondence with one another. So, correlating all these meshes with a universal coordinate frame (viz. a 3D face template) is a step to follow so as to establish dense correspondence.</p><p>In order to do so, we need to define a 2D UV space for each mesh, which in fact is a contiguous flattened atlas that embeds the 3D facial surface. Such a UV space is associated with its corresponding 3D surface through a bijective mapping; thus, establishing dense correspondence between two UV images implicitly establishes a 3D-to-3D correspondence for the mapped mesh. UV mapping is the 3D modelling process of projecting a 2D image to a 3D model's surface for texture mapping. The letters U and V denote the axes of the 2D texture, since X, Y and Z are already taken to denote the axes of the 3D object in model space.</p><p>We employ an optimal cylindrical projection method <ref type="bibr" target="#b10">[10]</ref> to synthetically create a UV space for each mesh. A UV map (which is an image I), with each pixel encoding both spatial information (X, Y, Z) and texture information (R, G, B), is produced, on which we perform non-rigid alignment. Non-rigid alignment is performed through the UV-TPS method that utilises key landmarks fitting and Thin Plate Spline (TPS) warping <ref type="bibr" target="#b17">[17]</ref>. Following <ref type="bibr" target="#b14">[14]</ref>, we perform several modifications to <ref type="bibr" target="#b17">[17]</ref>, to suit our data. Firstly, we build session-and-personspecific Active Appearance Models (AAMs) <ref type="bibr" target="#b40">[39]</ref> to automatically track feature points in the UV sequences. This means that 4 different AAMs are built and used separately for one subject. Main reasons behind this are: i) textures of different sessions differ due to several facts (i.e. aging, beards, make-ups, experiment lighting condition), ii) person-specific model is proven more accurate and robust in specific domains <ref type="bibr" target="#b15">[15]</ref>.</p><p>In total, 435 neutral meshes and 1047 expression meshes (1 neutral and 2-3 expressive meshes per person and session) in 4DFAB were selected; these contained annotations with 79 3D landmarks. They were unwrapped and rasterised to UV space, then grouped for building the corresponding AAMs. Each UV map was flipped to increase fitting robustness. Once all the UV sequences were tracked with 79 landmarks, they were then warped to the corresponding reference frame using TPS, thus achieving the 3D dense correspondence. For each subject and session, one specific reference coordinate frame from his/her neutral UV map was built. From each warped frame, we could uniformly sample the texture and 3D coordinates. Eventually, a set of non-rigidly corresponded 3D meshes under the same topology and density were obtained.</p><p>Given that meshes have been aligned to their designated reference frame, the last step was to establish dense 3D-to-3D correspondences between those reference frames and a 3D template face. This is a 3D mesh registration problem, solved by Non-rigid ICP <ref type="bibr" target="#b2">[3]</ref>. We employed it to register the neutral reference meshes to a common template, the Large Scale Facial Model (LSFM) <ref type="bibr" target="#b9">[9]</ref>. We brought all 600,000 3D meshes into full correspondence with the mean face of LSFM. As a result, we created a new set of 600,000 3D faces that share identical mesh topology, while maintaining their original facial expressions. In the following, this set constitutes the 3D facial expression gallery which we use for facial affect synthesis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Constructing a 3D Morphable Model</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.1">General Pipeline</head><p>A common 3DMM consists of three parametric models: the shape, the camera and the texture models.</p><p>To build the shape model, the training 3D meshes should be put in dense correspondence (similarly to the previous Mesh Pre-Processing subsection). Next, Generalized Procrustes Analysis is performed to remove any similarity effects, leaving only shape information. Finally, Principal Component Analysis (PCA) is applied to these meshes, which generates a 3D deformable model as a linear basis of shapes. This model allows for the generation of novel shape instances. The model can be expressed as:</p><formula xml:id="formula_0">S(p) =s + U s p<label>(1)</label></formula><p>wheres ? R 3N is the mean component of 3D shape (in our case it is the mean of shape models from the LSFM model described in the next subsection) with N denoting the number of vertices in the shape model; U s ? R 3N ?ns is the shape eigenbase (in our case it is the identity subspace of LSFM) with n s &lt;&lt; 3N being the number of principal components (n s is chosen to explain a percentage of the training set variance; generally, this percentage is 99.5%); and p ? R ns is a vector of parameters which allows for the generation of novel shape instances.</p><p>The purpose of camera model is to project the objectcentered Cartesian coordinates of a 3D mesh instance into 2D Cartesian coordinates in an image plane. At first, given that the camera is static, the 3D mesh is rotated and translated using a linear view transformation, which results in 3D rotation and translation components. Then, a nonlinear perspective transformation is applied. Note that quaternions <ref type="bibr" target="#b33">[32,</ref><ref type="bibr" target="#b67">66]</ref> are used to parametrise the 3D rotation, which ensures computational efficiency, robustness and simpler differentiation. In this manner we construct the camera parameters (i.e., 3D translation components, quaternions and parameter of linear perspective transformation). The camera model of the 3DMM applies the above transformations on the 3D shape instances generated by the shape model. Finally, the camera model can be written as:</p><formula xml:id="formula_1">W(p, c) = P(S(p), c),<label>(2)</label></formula><p>where S(p) is a 3D face instance; c ? R nc are the camera parameters (for rotation, translation and focal length; n c is 7); and P : R 3N ? R 2N is the perspective camera projection. For the texture model, large facial in-the-wild databases annotated for sparse landmarks are needed. Let us assume that the meshes have corresponding camera and shape parameters. These images are passed through a dense feature extraction function that returns featurebased representations for each image. These are then sampled from the camera model at each vertex location so as to build a texture sample, which will be nonsensical for some regions mainly due to self occlusions present in the mesh projected in the image space. To complete the missing information of the texture samples, Robust PCA (RPCA) with missing values <ref type="bibr" target="#b56">[55]</ref> is applied. This produces complete feature-based textures that can be processed with PCA to create the statistical model of texture, which can be written as:</p><formula xml:id="formula_2">T (?) =t + U t ?,<label>(3)</label></formula><p>where t ? R 3N is the mean texture component (in our case it is the mean of texture model from LSFM); U t ? R 3N ?nt and ? ? R nt are the texture subspace (eigenbase) and texture parameters, respectively, with n t &lt;&lt; 3N being the number of principal components. This model can be used to generate novel 3D featurebased texture instances. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.2">The Large Scale Facial Model (LSFM)</head><p>We have adopted the LSFM model constructed using the MeIn3D dataset <ref type="bibr" target="#b9">[9]</ref>. The construction pipeline of LSFM starts with a robust approach to 3D landmark localization resulting in generating 3D landmarks for the meshes. The 3D landmarks are then employed as soft constraints in Non-rigid ICP to place all meshes in correspondence with a template facial surface; the mean face of the Basel Face Model <ref type="bibr" target="#b46">[45]</ref> has been chosen. However, the large cohort of data could result in convergence failures. These are an unavoidable byproduct of the fact that both landmark localization and NICP are non-convex optimization problems sensitive to initialization.</p><p>A refinement post-processing step weeds out problematic subjects automatically, guaranteeing that the LSFM models are only constructed from training data for which there exist a high confidence of successful processing. Finally, the LSFM models are derived by applying PCA on the corresponding training sets, after excluding the shape vectors that have been classified as outliers. In total, 9,663 subjects are used to build LSFM, which covers a wide variety of age (from 5 to over 80s), gender (48% male, 52% female), and ethnicity (82% White, 9% Asian, 5% Mixed Heritage, 3% Black and 1% other).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">The Proposed Approach</head><p>In this Section, we present the fully automatic facial affect synthesis framework. The user needs to provide a neutral image and an affect, which can be a VA pair of values, a path in the 2D VA space, or one of the basic expression categories. Our approach: 1) performs face detection and landmark localization on the input neutral image, 2) fits a 3D Morphable Model (3DMM) on the resulting image <ref type="bibr" target="#b8">[8]</ref>, 3) deforms the reconstructed face and adds the input affect, and 4) blends the new face with the given affect into the original image. Here let us note that the total time needed for the first two steps is about 400ms; this has to be performed only once, if generating multiple images from the same input image. Specific details regarding the described steps of our approach follow. This procedure is shown in <ref type="figure" target="#fig_3">Fig. 4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Face Detection &amp; Landmark Localization</head><p>The first step to edit an image is to locate landmark points that will be used for fitting the 3DMM. We first perform face detection with the face detection model from <ref type="bibr" target="#b74">[73]</ref> and then utilize <ref type="bibr" target="#b18">[18]</ref> to localize 68 2D facial landmark points which are aware of the 3D structure of the face, in the sense that points on occluded parts of the face (most commonly part of the jawline) are correctly localized.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">3DMM-Fitting: Cost Function &amp; Optimization</head><p>The goal of this step is to retrieve a reconstructed 3D face with the texture sampled from the original image. In order to do so, we first need a 3DMM; we select the LSFM.</p><p>Fitting a 3DMM on face images is an inverse graphics approach to 3D reconstruction and consists of optimizing three parametric models of the 3DMM, the shape, texture and camera models. The optimization aims at rendering a 2D image which is as close as as possible to the input one. In our pipeline we follow the 3DMM fitting approach of <ref type="bibr" target="#b8">[8]</ref>. As is already noted, we employ the LSFM [9] S(p) to model the identity deformation of faces. Moreover, we adopt the robust, featurebased texture model T (?) of <ref type="bibr" target="#b8">[8]</ref>, built from in-the-wild images. The employed camera model is a perspective transformation W(p, c), which projects shape S(p) on the image plane.</p><p>Consequently, the objective function that we optimize can be formulated as:</p><formula xml:id="formula_3">argmin p,?,c F(W(p, c)) ? T (?) 2 + c l W l (p, c) ? s l 2 + c s p 2 ? ?1 s + c t ? 2 ? ?1 t ,<label>(4)</label></formula><p>where the first term denotes the pixel loss between the feature based image F sampled at the projected shape's locations and the model generated texture; the second term denotes a sparse landmark loss between the image 2D landmarks and the corresponding 2D projected 3D points, where the 2D shape, s l , is provided by <ref type="bibr" target="#b18">[18]</ref>; the rest two terms are regularization terms which serve as counter over-fitting mechanism, where ? s and ? t are diagonal matrices with the main diagonal being eigenvalues of the shape and texture models respectively; c l , c s and c t are weights used to regularize the importance of each term during optimization and were empirically set to 10 5 , 3?10 6 and 1, respectively, following <ref type="bibr" target="#b8">[8]</ref>. Note also, that the 2D landmarks term is useful as it drives the optimization to converge faster. Problem of Eq. 4 is solved by the Project-Out variation of Gauss-Newton optimization as formulated in <ref type="bibr" target="#b8">[8]</ref>.</p><p>From the optimized models, the optimal shape instance constitutes the neutral 3D representation of the input face. Moreover, by utilizing the optimal shape and camera models, we are able to sample the input image at the projected locations of the recovered mesh and extract a UV texture, that we later use for rendering.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Deforming Face &amp; Adding Affect</head><p>Given an affect and an arbitrary 2D image I, we first fit the LSFM to this image using the aforementioned 3DMM fitting method. After that, we can retrieve a reconstructed 3D face s orig with the texture sampled from the original image (texture sampling is simply extracting image pixel value for each projected 3D vertex in image plane). Let us assume that we have created an affect synthesis model M Af f that takes the affect as input and generates a new expressive face (denoted as s gen ), i.e., s = M Af f (af f ect) (specific details regarding the generation of the expressive face, can be found in subsection 4.5). Next, we calculate the facial deformation ?s by subtracting the synthesized face s gen from the LSFM templates, i.e., ?s = s gen ?s, and impose this deformation on the reconstructed mesh, i.e., s new = s orig + ?s. Therefore, we obtain a 3D face (dubbed s new ) with facial affect.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Synthesizing 2D Face</head><p>The final step in our pipeline is to render the new 3D face s new back to the original 2D image. To do that we employ the mesh that we have deformed according to the given affect, the extracted UV texture and the optimal camera transformation of the 3DMM. For rendering, we pass the three model instances to a renderer and we use as background the background of the input image. Lastly, the rendered image is fused with the original image via poisson blending <ref type="bibr" target="#b47">[46]</ref> to smooth the boundary between foreground face and image background so as to produce a natural and realistic result. In our experiments, we used both a CPU-based renderer <ref type="bibr" target="#b1">[2]</ref> and a GPU-based renderer <ref type="bibr" target="#b24">[24]</ref>. The GPU-based renderer greatly decreases the rendering time, as it needs 20ms to render a single image, while the CPU-based renderer needs 400ms. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Synthesizing Expressive Faces with Given Affect</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.1">VA &amp; Basic Expression cases: Building Blendshape Models &amp; Computing Mean Faces</head><p>Let us first describe the VA case. We have 600,000 3D meshes (established into dense correspondence) and their VA annotations. We want to appropriately discretize the VA Space into classes, so that each class contains a sufficient number of data. This is due to the fact that if classes contain only few examples, it is more likely to include identity information. However, the synthesized facial affect should only describe the expression associated with the VA pair of values, rather than information for the person's identity, gender, or age. We have chosen to perform agglomerative clustering <ref type="bibr" target="#b39">[38]</ref> on the VA values, using the euclidean distance as metric and the ward as linkage criterion (keeping the correspondence between VA values and 3D meshes). In this manner, we created 550 clusters, i.e., classes. Then we built blendshape models and computed the mean face per class. <ref type="figure" target="#fig_4">Fig. 5</ref> illustrates the mean faces of various classes. It should be mentioned that the majority of classes correspond to the first two quadrants of the VA Space, namely the regions of positive valence (as can be seen in the 2D histogram of <ref type="figure" target="#fig_2">Fig. 3</ref>).</p><p>As far as the basic expression case is concerned, based on the derived 12,000 3D meshes, 2,000 for each of the six basic expressions, we built six blendshape models and six corresponding mean faces.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.2">User Selection: VA/Basic Expr &amp; Static/Temporal Synthesis</head><p>The user first chooses the type of affect that our approach will generate. The affect could be either a point, or a path in the VA space, or one the six basic expression categories. If the user chooses the latter, then we retrieve the mean face of this category and add it on the 3D face reconstructed from the user's input neutral image. In this case, the only difference in <ref type="figure" target="#fig_3">Fig. 4</ref> would be for the user to input a basic expression, the happy one, instead of a VA pair of values. If the user chooses the former, then (s)he needs to additionally clarify if our approach should generate an image ('static synthesis') or a sequence of images ('temporal synthesis') with this affect.</p><p>Static synthesis If the user selects 'static synthesis', then the user should input a specific VA pair of values. Then, we retrieve the mean face of the class to which this VA value belongs. We use this mean face as the affect to be added on the 3D face reconstructed from the provided neutral image. <ref type="figure" target="#fig_3">Fig. 4</ref> shows the proposed approach for this specific case. <ref type="figure" target="#fig_5">Fig. 6</ref> illustrates the procedure described in 4.5.1 given that the 550 VA classes are already created.</p><p>Temporal synthesis If the user selects 'temporal synthesis', then, (s)he should provide a path in the VA space (for instance by drawing) that the synthesized sequence should follow. Then, we retrieve the mean faces of the classes to which the VA values of the path belong. We use each of these mean faces as the affect to be added on the 3D faces reconstructed from the provided neutral image. As a consequence, an expressive sequence is generated that shows the evolution of affect on the VA path specified by the user.</p><p>Here let us mention that the fact that the 4DFAB used in our approach is a temporal database, ensures that successive video frames' annotations are adjacent in the VA Space, since they generally show the same or slightly different states of affect. Thus, the 3D meshes of successive video frames will lie in the same and in adjacent classes in the 2-D VA space. Thus mean faces from adjacent classes can be used to show temporal evolution of affect as was above described.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.3">Expression Blendshape Models</head><p>Expression blendshape models provide an effective way to parameterize facial behaviors. The localized blendshape model <ref type="bibr" target="#b44">[43]</ref> has been used to describe the selected VA samples. To build this model, we first bring all meshes into full correspondence following the dense registration approach described in Section 3.3. As a result, we have a set of training meshes with the same number of vertices and identical topology. Note that we have also selected one neutral mesh for each subject, which should have full correspondence with the rest data. Next, we subtract each 3D mesh from the respective neutral mesh, and create a set of m difference vectors d i ? R 3N . We then stack them into a matrix</p><formula xml:id="formula_4">D = [d 1 , ..., d m ] ? R 3N ?m ,</formula><p>where N is number of vertices in the mesh. Finally, a variant of sparse Principal Component Analysis (PCA) is applied to the data matrix D, so as to identify sparse deformation components C ? R h?1 :</p><formula xml:id="formula_5">arg min D ? BC 2 F + ? (C) s.t. V (B) ,<label>(5)</label></formula><p>where, the constraint V can be either max (</p><formula xml:id="formula_6">|B k |) = 1, ?k or max (B k ) = 1, B ? 1, ?k, with B k ? R 3N ?1 denoting the k th components of sparse weight matrix B = [B 1 , ? ? ? , B h ].</formula><p>Selection of these two constraints depends on the actual usage; the major difference is that the latter one allows for negative weights and therefore enables deformation towards both directions, which is useful for describing shapes like muscle bulges. In this paper, we have selected the latter constraint, as we wish to enable bidirectional muscle movement and synthesise a rich variety of expressions. The regularization of sparse components C was performed with 1/ 2 norm <ref type="bibr" target="#b69">[68,</ref><ref type="bibr" target="#b5">6]</ref>. To permit more local deformations, additional regularization parameters were added into ? (C).</p><p>To compute optimal C and B, an iterative alternating optimization was employed (please refer to <ref type="bibr" target="#b44">[43]</ref> for more details).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Databases</head><p>To evaluate our facial affect synthesis method in different scenarios (e.g. controlled laboratory environment, uncontrolled in-the-wild setting), we utilized neutral facial images from as many as 13 databases (both small and large in terms of size).  <ref type="table" target="#tab_0">Table 1</ref> presents these databases by showing: i) the model of affect they use, their condition, their type (static images or audiovisual image sequences), the total number of frames and (male/female) subjects that they contain and the range of ages of the subjects, and ii) the total number of images that we synthesized using our approach (both in the valence-arousal and the six basic expressions cases).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experimental Study</head><p>This section describes the experiments performed so as to evaluate the proposed approach. At first, we provide a qualitative evaluation of our approach by showing many synthesized images or image sequences from all thirteen databases described in the previous Section; as well as by comparing images generated by state-of-theart GANs (StarGAN, Ganimation) and our approach. Next, a quantitative evaluation is performed by using the synthesized images as additional data to train Deep Neural Networks (DNNs); it is shown that the trained DNNs outperform current state-of-the-art networks and GAN-based methods on each database. Finally an ablation study is performed in which: i) the synthesized data are considered and used as a training (test) dataset, while the original data are respectively used as test (training) dataset, ii) the effect of the amount of synthesized data on network performance is studied, iii) an analysis is performed based on subjects' age.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Qualitative evaluation of achieved facial affect synthesis</head><p>We used all databases mentioned in Section 5 to supply the proposed approach with 'input' neutral faces. We then synthesized the emotional state corresponding to specific affects (both in VA case and in the six basic expressions one) for these images. At first we show many generated images (static synthesis) according to different VA values, then we illustrate examples of generated image sequences (temporal synthesis) and next we present some synthesized (static) images according to the six basic expressions. Finally, we visually compare images generated by our approach with synthesized images by StarGAN and Ganimation. <ref type="figure" target="#fig_6">Fig. 7</ref> shows representative results of facial affect synthesis, when user inputs a VA pair and selects to generate a static image. These results are organized in three age groups: <ref type="figure" target="#fig_6">Fig. 7(b)</ref> kids, <ref type="figure" target="#fig_6">Fig. 7</ref>(c) elderly people and <ref type="figure" target="#fig_6">Fig. 7(a)</ref> in-between ages. In each part, the first row illustrates neutral images sampled from each of the aforementioned databases, the second one shows the respective synthesized images and the third shows   the respective VA values that were synthesized. Moreover, <ref type="figure" target="#fig_7">Fig. 8</ref> shows neutral images on the left hand side (first column) and synthesized images, with various valence and arousal values, on the right hand side (following columns). It can be observed that the synthesized images are identity preserving, realistic and vivid. <ref type="figure">Fig.  9</ref> refers to the basic expression case; it shows neutral images on the left hand side of (a) and (b) and synthesized images with basic expressions on the right hand side. <ref type="figure" target="#fig_0">Fig. 10</ref> illustrates the VA case for temporal synthesis, as was described in Section 4.5.2. Neutral images are shown on the left hand side, while synthesized face sequences with time-varying levels of affect are shown on the right hand side. All these Figs. show that the proposed framework works well, when using images from either in-the-wild, or controlled databases. This indicates that we can ef-(a) (b) <ref type="figure">Fig. 9</ref> Basic Expression Case of facial synthesis: on the left hand side of (a) and (b) are the neutral 2D images and on the right the synthesized images with some basic expressions fectively synthesize facial affect irregardless of image conditions (e.g., occlusions, illumination and head poses).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.1">Results on Static &amp; Temporal Affect Synthesis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.2">Comparison with GANs</head><p>In order to characterize the value that the proposed approach imparts, we provide qualitative comparisons with two state-of-the-art GANs, namely StarGAN <ref type="bibr" target="#b16">[16]</ref> and Ganimation. Like CycleGAN (referenced in Section 2), Star-GAN performs image-to-image translation, but adopts a unified approach such that a single generator is trained to map an input image to one of multiple target domains, selected by the user. By sharing the generator weights among different domains, a dramatic reduction of the number of parameters is achieved. Ganimation was described in Section 2. We used these networks to fit the VA case, namely to generate expressions according to VA values. We trained them with the same 600,000 frames of 4DFAB that we used in our approach. Preprocessing also included face detection and alignment. For a fair comparison, in all presented comparisons (both qualitative and quantitative), the GANs generated samples, while provided with the same neutral images and the same VA values. <ref type="figure" target="#fig_0">Fig. 11</ref> presents a visual comparison between images generated by our approach, StarGAN and Ganimation. It shows the neutral images, the synthesized VA values and the resulting images. It is evident that our approach synthesizes samples that: i) look much more natural and realistic, ii) maintain the degree of sharpness of the original neutral image, and iii) combine visual accuracy with spatial resolution.</p><p>Some further deductions can be made from <ref type="figure" target="#fig_0">Fig. 11</ref>. StarGAN does not perform well when tested on different in-the-wild and controlled databases that include variations in illumination conditions and head poses. StarGAN is shown to not reflect detailed illumination; unnatural lighting changes were observed on the results. These can be explained because in the original Start-GAN paper <ref type="bibr" target="#b16">[16]</ref>, its capability to generate affect has not been tested on in-the-wild facial analysis (we refer only to the case of emotion recognition). In general, Star-GAN yields more realistic results when it is trained simultaneously with multiple datasets annotated for different tasks.</p><p>Additionally, in <ref type="bibr" target="#b16">[16]</ref>, when referring to emotion recognition, StarGAN was trained and evaluated on Radboud Faces Database (RaFD) <ref type="bibr" target="#b34">[33]</ref> which: i) is very small in terms of size (around 4,800 images) and ii) is a labcontrolled and posed expression database. Last but not least, StarGAN has been tested to change only a particular aspect of a face among a discrete number of attributes/emotions defined by the annotation granularity of the dataset. As can be seen in <ref type="figure" target="#fig_0">Fig. 11</ref>, StarGAN cannot accurately provide realistic results when tested in the much broader and more difficult task of valence and arousal generation (and estimation).</p><p>As far as Ganimation is concerned, its results are also worse than the results of our approach. In most cases, it shows artifacts and in some cases certain levels of blurriness. When compared to StarGAN, Ganimation seems more robust to changing backgrounds and lighting conditions; this is due to the attention and color masks that it contains. Nevertheless, in general, errors in the attention mechanism occur when the input contains extreme expressions. The attention mechanism does not seem to sufficiently weight the color transformation, causing transparencies. It is interesting to note that on the Leonardo DiCaprio image, the synthesized image by Ganimation shows open eyes, whereas on the neutral image (and the one synthesized by our approach) eyes are closed; this illustrates errors of the mask. For example, in <ref type="figure" target="#fig_0">Fig. 11</ref>, images produced by Ganimation in columns 1, 3, 4, 5, 6, 9 show the discussed problems. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Quantitative evaluation of the facial affect synthesis through data augmentation</head><p>It is generally accepted that using more training data -of good quality -leads to better results in supervised training. Data augmentation increases the effective size of the training dataset. In this Section we present a data augmentation strategy which uses the synthesized data produced by our approach, as additional data to train DNNs, for both valence-arousal prediction, as well as classification into the basic expression categories. In particular, we describe experiments performed on eight databases, presenting the adopted evaluation criteria, the networks we used and the obtained results. We also report the performances of the networks trained -in a data augmentation manner-with synthesized images from StarGAN and Ganimation. It is shown that the DNNs trained with the proposed data augmentation methodology outperform both the state-of-the-art techniques and the DNNs trained with StarGAN and Ganimation, in all experiments, validating the effectiveness of the proposed facial synthesis approach. Let us first explain some notations. In the followings, by reporting 'network name trained using StarGAN', 'network name trained using Ganimation' and 'network name trained using the proposed approach', we refer to networks trained with the specific database's training set augmented with data synthesized by StarGAN, Ganimation and the proposed approach, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.1">Leveraging synthesized data for training Deep Neural Networks: Valence-Arousal case</head><p>In this set of experiments we consider four facial affect databases annotated in terms of valence and arousal, the Aff-Wild, RECOLA, AffectNet and AFEW-VA databases. At first, we selected neutral frames from these databases, i.e., frames with zero valence and arousal values (human inspection was also conducted to make sure that they represented neutral faces). For every frame, we synthesized facial affect according to the methodology described in Section 4. We start by first describing the evaluation criteria used in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.1.1">The adopted evaluation criteria</head><p>The main evaluation criterion that we use is the Concordance Correlation Coefficient (CCC) <ref type="bibr" target="#b35">[34]</ref>, which has been widely used in related Challenges (e.g., <ref type="bibr" target="#b65">[64]</ref>); we also report the Mean Squared Error (MSE), since this has been also frequently used in related research. CCC evaluates the agreement between two time series by scaling their correlation coefficient with their mean square difference. CCC takes values in the range [?1, 1], where +1 indicates perfect concordance and ?1 denotes perfect discordance. Therefore high values are desired. CCC is defined as follows:</p><formula xml:id="formula_7">? c = 2s xy s 2 x + s 2 y + (x ??) 2 ,<label>(6)</label></formula><p>where s x and s y are the variances of the ground truth and predicted values respectively,x and? are the corresponding mean values and s xy is the respective covariance value. The Mean Squared Error (MSE) provides a simple comparative metric, with a small value being desirable. MSE is defined as follows:</p><formula xml:id="formula_8">M SE = 1 N N i=1 (x i ? y i ) 2 ,<label>(7)</label></formula><p>where x and y are the ground truth and predicted values respectively and N is the total number of samples.</p><p>In some cases we also report the Pearson-CC (P-CC) and the Sign Agreement Metric (SAGR), since they have been reported by respective state-of-the-art methods.</p><p>The P-CC takes values in the range [-1,1] and high values are desired. It is defined as follows:</p><formula xml:id="formula_9">? xy = s xy s x s y ,<label>(8)</label></formula><p>where s x and s y are the variances of the ground truth and predicted values respectively and s xy is the respective covariance value.</p><p>The SAGR takes values in the range [0,1], with high values being desirable. It is defined as follows:</p><formula xml:id="formula_10">SAGR = 1 N N n=1 ?(sign(x i ), sign(y i )),<label>(9)</label></formula><p>where N is the total number of samples, x and y are the ground truth and predicted values respectively, ? is the Kronecker delta function and ?(sign(x), sign(y)) is defined as: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.1.2">Experiments on Dimensional Affect</head><p>Aff-Wild We synthesized 60,135 images from the Aff-Wild database and added those images to the training set of the first Affect-in-the-wild Challenge. The employed network architecture was the AffWildNet (VGG-FACE-GRU) described in <ref type="bibr" target="#b29">[29,</ref><ref type="bibr" target="#b30">30]</ref>. <ref type="table" target="#tab_2">Table 2</ref> shows a comparison of the performance of: the VGG-FACE-GRU trained using: i) our approach, ii) StarGAN, and iii) Ganimation; the best performing network, AffWildNet, reported in <ref type="bibr" target="#b29">[29,</ref><ref type="bibr" target="#b30">30]</ref>; the winner of the Aff-Wild Challenge <ref type="bibr" target="#b13">[13]</ref> (FATAUVA-Net). From <ref type="table" target="#tab_2">Table 2</ref>, it can be verified that the network trained on the augmented dataset, with synthesized by our approach images, outperformed all other networks. It should be noted that the number of synthesized images (around 60K) was small compared to the size of Aff-Wild's training set (around 1M ), the latter being already sufficient for training the best performing DNN; consequently, the improvement was not large, about 2%. An interesting observation is that the network trained using StarGAN displayed worse performance than AffWildNet. This means that the 68 landmark points that were passed as additional input to the AffWildNet helped the network in reaching a better performance than just adding a small amount (compared to the training set size) of auxiliary synthesized data. The MSE error improvement on Valence and Arousal estimation provided by the augmented training vs the AffWildNet one, over the different areas of the VA space, is shown through the 2D histograms presented in <ref type="figure" target="#fig_0">Fig. 12</ref>. It can be seen that the improvement on MSE was better in areas in which a larger number of new samples was generated, i.e., in the positive valence regions. RECOLA We generated 46,455 images from RECOLA; this number corresponds to around 40% of its training data set size. The employed network architecture was the ResNet-GRU described in <ref type="bibr" target="#b30">[30]</ref>. <ref type="table" target="#tab_3">Table 3</ref> shows a comparison of the performance of: the ResNet-GRU network trained using: i) our approach, ii) StarGAN, and iii) Ganimation; the AffWildNet finetuned on the RECOLA, as reported in <ref type="bibr" target="#b30">[30]</ref>; a ResNet-GRU directly trained on RECOLA, as reported in <ref type="bibr" target="#b30">[30]</ref>.</p><p>From <ref type="table" target="#tab_3">Table 3</ref>, it can be verified that the network trained using the proposed approach outperformed all  <ref type="figure" target="#fig_0">Fig. 13</ref> The 2D histogram of valence and arousal RECOLA's test set annotations, along with the MSE per grid area, in the case of (a) ResNet-GRU and (b) ResNet-GRU trained using the proposed approach other networks. The above gains in performance can be justified by the fact that the number of synthesized images (around 46,500) was significant compared to the size of RECOLA's training set (around 120,000) and that the original training set size was not very sufficient to train the DNNs. It is worth mentioning that the GAN based methods have not managed to provide a sufficiently enriched dataset so that a similar boost in the achieved performances could be obtained. The MSE error improvement on Valence and Arousal estimation provided by the augmented training vs the original one (which was 0.045-0.100 vs 0.055-0.160), over the different areas of the VA space, is shown through the 2D histograms presented in <ref type="figure" target="#fig_0">Fig. 13</ref>. Big reduction of MSE value was achieved in all covered VA areas.</p><p>AffectNet The AffectNet database contains around 450,000 manually annotated images and around 550,000 automatically annotated images for valence-arousal. We only used the manually annotated images so as to be consistent with the state-of-the-art networks that were also trained using this set. Additionally, the manually annotated set ensures that the images used by our approach to synthesize new, are indeed neutral. We created 2,476,235 synthesized images from the AffectNet database, a number that is more than 5 times bigger than the training data size. The employed network architecture was VGG-FACE. For comparison purposes, we trained the network using the original training data set (let us call this network 'the VGG-FACE baseline'). <ref type="table" target="#tab_4">Table 4</ref> shows a comparison of the performance of: the VGG-FACE baseline; the VGG-FACE trained using: i) our approach, ii) StarGAN, and iii) Ganimation; AlexNet, which is the baseline network of the AffectNet database <ref type="bibr" target="#b43">[42]</ref>.</p><p>From <ref type="table" target="#tab_4">Table 4</ref>, it can be verified that the network trained by the proposed methodology outperformed all other networks. This boost in performance has been large, in all evaluation criteria, compared to the VGG-FACE baseline network, with spread of this improvement over the VA space shown in <ref type="figure" target="#fig_0">Fig. 14.</ref> The explanation arises from the large number of synthesized images that helped the network train and generalize better, since in the training set there existed a lot of ranges that were poorly represented. This is shown in the histogram of the -manually annotated-training set, for valence and arousal, in <ref type="figure" target="#fig_0">Fig. 15</ref>. Our network also outperformed the AffectNet's database baseline. For the arousal estimation, the performance gain was remarkable, mainly in CCC and SAGR evaluation criteria, whereas for the valence estimation the performance gain was also significant. AFEW-VA. We synthesized 108,864 images from the AFEW-VA database, a number that is more than 3.5 times bigger than its original size. For training, we used the VGG-FACE-GRU architecture described in <ref type="bibr" target="#b30">[30]</ref>. Similarly to <ref type="bibr" target="#b31">[31]</ref>, we used a 5-fold person-independent cross-validation strategy and at each fold we augmented the training set with the synthesized images of people appearing only in that set (preserving the person independence).  <ref type="table" target="#tab_5">Table 5</ref> shows a comparison of the performance of: the VGG-FACE-GRU network trained using: i) our approach, ii) StarGAN, and iii) Ganimation; the best performing network as reported in <ref type="bibr" target="#b31">[31]</ref>. From <ref type="table" target="#tab_5">Table 5</ref>, it can be verified that the network trained using the proposed approach outperformed all other networks. Great boost in performance was achieved. The general gain in performance can be justified by the fact that the number of synthesized images (around 109,000) is much greater than the number of images in the dataset (around 30,000), with the latter being rather small for effectively training the DNNs. The 2D histogram in <ref type="figure" target="#fig_0">Fig. 16</ref> shows the achieved MSE when using the proposed approach over the different areas of the VA space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.2">Leveraging synthesized data for training Deep Neural Networks: Basic Expressions case</head><p>In the following experiments we used the synthesized faces to train DNNs, for classification into the six basic expressions, over four facial affect databases, RAF-DB, <ref type="figure" target="#fig_0">Fig. 16</ref> The 2D histogram of valence and arousal AFEW-VA's test set annotations, along with the MSE per grid area, in the case of the VGG-FACE trained using the proposed approach AffectNet, AFEW and BU-3DFE. Our first step has been to select neutral frames from these four databases.</p><p>Then, for each frame, we synthesized facial affect according to the methodology described in Section 4. We start by first describing the evaluation criteria used in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.2.1">The adopted evaluation criteria</head><p>One evaluation criterion used in the experiments is total accuracy, defined as the total number of correct predictions divided by the total number of samples. Another criterion is the F 1 score, which is a weighted average of the recall (= the ability of the classifier to find all the positive samples) and precision (= the ability of the classifier not to label as positive a sample that is negative). The F 1 score reaches its best value at 1 and its worst score at 0. In our multi-class problem, F 1 score is the unweighted mean of the F 1 scores of the expression classes. F 1 score of each class is defined as:</p><formula xml:id="formula_12">F 1 = 2 ? precision ? recall precision + recall<label>(11)</label></formula><p>Another criterion that is used is the average of the diagonal values of the confusion matrix for the seven basic expressions.</p><p>One, or more of the above criteria are used in our experiments, so as to illustrate the comparison with other state-of-the-art methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.2.2">Experiments on Categorical Affect</head><p>RAF-DB. In this database we only considered the six basic expression categories, since our approach synthesizes images based on these categories; we ignored compound expressions that were included in the original dataset. We created 12,828 synthesized images, which <ref type="table">Table 6</ref> RAF-DB: The diagonal values of the confusion matrix for the seven basic expressions and their average, using the VGG-FACE trained using the proposed approach, as well as using other state-of-the-art networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Networks</head><p>Anger  <ref type="figure" target="#fig_0">Fig. 17</ref> The confusion matrix of (a) the VGG-FACE baseline and (b) the VGG-FACE trained using the proposed approach for the RAF-DB database; 0: Neutral, 1: Anger, 2: Disgust, 3: Fear, 4: Joy, 5: Sadness, 6: Surprise are slightly more than the training images <ref type="bibr" target="#b12">(12,</ref><ref type="bibr">271)</ref>. We employed the VGG-FACE network. For comparison purposes, we trained the network using the original training dataset (let us call this network the VGG-FACE baseline).</p><p>For further comparison purposes, we used the networks defined in <ref type="bibr" target="#b36">[35]</ref>: i) mSVM-VGG-FACE: first the VGG-FACE was trained on the RAF-DB database and then features from the penultimate fully connected layer were extracted and fed into a Support Vector Machine (SVM) that performed the classification, ii) LDA-VGG-FACE: same as before: LDA was applied on the features which were extracted from the penultimate fully connected layer and performed the final classification and iii) mSVM-DLP-CNN: the designed Deep Locality Preserving CNN network (we refer the interested reader for more details to <ref type="bibr" target="#b36">[35]</ref>) was first trained on the RAF-DB database and then a SVM performed the classification using the features extracted from the penultimate fully connected layer of this architecture. <ref type="table">Table 6</ref> shows a comparison of the performance of the above described networks. From <ref type="table">Table 6</ref>, it can be verified that the network trained using the proposed approach outperformed all state-of-the-art nets. When compared to the mSVM-VGG-FACE and LDA-VGG-FACE networks, the boost in performance has been significant. This can be explained by the fact that the disgust and fear classes, originally, did not contain a lot of training images, but after adding the synthesized data, they did. This resulted in obtaining a better performance in the other classes, as well. Interestingly, there was also a considerable performance gain in the neutral class, that did not contain any synthesized images. This can be explained by considering the fact that the network trained with the augmented data could distinguish better the classes, since it had more samples in the two above described categories. <ref type="figure" target="#fig_0">Fig. 17</ref> illustrates the whole confusion matrix of the VGG-FACE baseline and the VGG-FACE trained using the proposed approach, giving a better insight on the improved performance and verifying the above explanations.</p><p>AffectNet. We synthesized 176,425 images from the AffectNet database, a number that is almost 40% of its size. It should be mentioned that the AffectNet database contained the six basic expressions and another one, contempt. Our approach synthesized images only for the basic expressions, so for the contempt class we only kept the original training data. The network architecture that we employed here was VGG-FACE. For comparison purproses, we trained a VGG-FACE network  <ref type="table">Table 7</ref> shows a comparison of the performance of: i) the VGG-FACE baseline, ii) the VGG-FACE network trained using the proposed approach and iii) AlexNet, the baseline network of the AffectNet database <ref type="bibr" target="#b43">[42]</ref>. <ref type="table">Table 7</ref> AffectNet: Total accuracy and F 1 score of the VGG-FACE trained using the proposed approach vs state-of-the-art networks</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Networks</head><p>Total Accuracy F 1 score AlexNet <ref type="bibr" target="#b43">[42]</ref> 0.58 0.58 the VGG-FACE baseline 0.52 0.51 VGG-FACE trained using the proposed approach 0.60 0.59</p><p>From <ref type="table">Table 7</ref>, it can be verified that the network trained using the proposed approach outperformed all the other networks. In more detail, when compared to the VGG-FACE baseline network, the boost in performance was significant, as also shown in <ref type="figure" target="#fig_0">Fig. 18</ref> in terms of the confusion matrices obtained by the two networks. This can be explained by the big size of the added synthesized images. When compared to the Af-fectNet's baseline, a slightly improved performance was also obtained; this could be higher, if we had synthesized images for the contempt category as well. AFEW. We synthesized 56,514 images from the AFEW database; this number was almost 1.4 times bigger than its training set size <ref type="bibr" target="#b42">(41,</ref><ref type="bibr">406)</ref>. The employed network architecture was VGG-FACE. For comparison purposes, we first trained a baseline network on AFEW's training set, which we call the VGG-FACE baseline. For further comparisons, we used the following networks developed by the three winning methods of the EmotiW 2017 Grand Challenge: i) VGG-FACE-FER: the VGG-FACE was first fine-tuned on the FER2013 database <ref type="bibr" target="#b26">[26]</ref> and then trained on the AFEW as described in <ref type="bibr" target="#b28">[28]</ref>, ii) VGG-FACE-external: the VGG-FACE was trained on the union of the AFEW database and some external data as described in <ref type="bibr" target="#b66">[65]</ref> and iii) VGG-FACE-LSTM-external-augmentation: the VGG-FACE-LSTM was trained on the union of the AFEW database and some external data; then data augmentation was performed, as described in <ref type="bibr" target="#b66">[65]</ref>. <ref type="table">Table 8</ref> AFEW: Total accuracy of the VGG-FACE trained using the proposed approach vs state-of-the-art networks</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Networks</head><p>Total Accuracy the VGG-FACE baseline 0.379 VGG-FACE-external <ref type="bibr" target="#b66">[65]</ref> 0.414 VGG-FACE-FER <ref type="bibr" target="#b28">[28]</ref> 0.483 VGG-FACE-LSTM-external-augmentation <ref type="bibr" target="#b66">[65]</ref> 0.486 VGG-FACE trained using the proposed approach 0.484 <ref type="table">Table 8</ref> shows a comparison of the performance of the above described networks. From <ref type="table">Table 8</ref>, one can see that the VGG-FACE trained using the proposed approach performed much better than the same network trained on, either only the AFEW database, or the union of the AFEW database with some external data whose size in terms of videos was the same as that of AFEW. The boost in performance can be explained taking into account the fact that the fear, disgust and surprise classes contained few data in AFEW and that our approach augmented the data size of those classes; in total the large number of synthesized images assisted to improve the performance of the network. This is evident when comparing the confusion matrix of the VGG- FACE baseline to the one of VGG-FACE trained using the proposed approach, as can be seen in <ref type="figure" target="#fig_0">Fig.19</ref>. The diagonal of the two confusion matrices indicates that there is an increase in the performance in almost all basic categories. Additionally, performance of our network is slightly better than the performance of the same VGG-FACE network first fine-tuned on the FER2013 database and then trained on the AFEW. FER2013 is a database of around 35,000 still images and different identities, annotated with the six basic expressions. In this case, the network that was first fine-tuned on the FER2013 database has seen more faces, since the tasks were similar. However, still our network provided a slightly better performance. On the other hand, our network had a slightly worse performance than a VGG-FACE-LSTM network that was trained with the same external data mentioned before and was also trained with data augmentation. Here, it was the LSTM network, which due to the time recurrent nature could better exploit the fact that AFEW consists of video sequences. BU-3DFE. We synthesized 600 images from the BU-3DFE database. This number was almost one fourth of its size <ref type="bibr" target="#b1">(2,</ref><ref type="bibr">500)</ref>. BU-3DFE is a small database and is not really suited for training DNNs. The network architecture that we employed here was VGG-FACE, with a modification in the number of hidden units in the two first fully connected layers. Since we did not have a lot of data for training the network, we i) used 256 and 128 units in the two fully connected layers and ii) kept the convolutional weights fixed, training only the fully connected ones. For training the network on this database, we used a 10-fold person-independent cross-validation strategy; in each fold, we augmented the training set with the synthesized images of people appearing only in that set (preserving person independence). The reported total accuracy of the model has been the average of the total accuracies over the 10-folds.</p><p>At first, we trained the above described VGG-FACE network (let us call this network 'the VGG-FACE baseline'). Next, we trained the above described VGG-FACE network, but also applied on-the-fly data augmentation techniques, such as: small rotations, left and right flipping, first resize and then random crop to original dimensions, random brightness and saturation (let us call this network 'VGG-FACE-augmentation'). Finally, we trained the above described VGG-FACE network using the proposed approach. <ref type="table">Table 9</ref> BU-3DFE: Total accuracy of the VGG-FACE trained using the proposed approach vs the VGG-FACE baseline and the VGG-FACE trained with on-the-fly data augmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Networks</head><p>Total Accuracy the VGG-FACE baseline 0.528 VGG-FACE-augmentation 0.588 VGG-FACE trained using the proposed approach 0.768 <ref type="table">Table 9</ref> shows a comparison of the performance of those networks. From <ref type="table">Table 9</ref>, it can be verified that the network trained using the proposed approach greatly outperformed the networks trained without it. This indicates that the proposed approach for synthesizing images can be used for data augmentation in cases of small amount of DNN training data, being able to significantly improve the obtained performances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Quantitative evaluation of the facial affect synthesis used in testing or training tasks</head><p>Results in the previous section show that the data generated using our approach provide improvements in network performance in both valence-arousal and basic expressions settings, when used for data augmentation.</p><p>In the following, we perform further analysis (two different settings) to assess the quality of our generated data, compared to the data synthesized by StarGAN and Ganimation, focusing only on the synthesized data.</p><p>In the first setting, the synthesized data are evaluated as a test set, for each database, against models trained on real data/images. The AffWildNet that has been trained solely on Aff-Wild's training set, the ResNet-GRU trained on the RECOLA's training set and the VGG-FACE baseline trained on AffectNet's training set (all described in Section 6.2.1.2), have been used as emotion regressors and are being evaluated on each of the three afore-mentioned synthesized datasets. From <ref type="table" target="#tab_0">Table 10</ref> it is evident that the networks trained on the afore mentioned databases displayed a much better performance (in all databases) when tested on the synthesized data from the proposed approach in comparison to the synthesized data from StarGAN and Ganimation.</p><p>We further conducted a second setting, using the synthesized data to train respective DNN models. These models are then evaluated on the real test set of Aff-Wild, RECOLA and AffectNet. <ref type="table" target="#tab_0">Table 11</ref> shows the results of this setting. The performance in terms of both CCC and MSE is much higher in all databases when the networks are trained with the data synthesized by the proposed approach. This difference in the compared performances, along with the former results, reflect the direct value of our generated data in enhancing regression performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Effect of synthesized data granularity on performance improvement</head><p>In this subsection we performed experiments using a subset of our synthesized data for augmenting the databases. Our aim is to see if all synthesized data are needed for augmenting network training and more generally to see how the improvement in classification and regression scale with the granularity of synthesized data. In more detail, for each database used in our experiments, we used a subset of N synthesised data from this database to augment its training set. <ref type="table" target="#tab_0">Table 12</ref> shows the databases and its corresponding N values. <ref type="figure" target="#fig_1">Fig. 20</ref> shows the improvement in network performance when training using additionally auxiliary data; the improvement shown per database is the difference in the performances when training networks with only the database's training set and when training them with the union of the training set and auxiliary data. <ref type="figure" target="#fig_1">Fig. 20</ref> illustrates for each database the difference in network performance, when N synthesized data generated by our approach (N defined in <ref type="table" target="#tab_0">Table 12</ref>) are used as auxiliary data.</p><p>The performance measure for Aff-Wild, RECOLA, AffectNet and AFEW-VA is the average of valence CCC and arousal CCC. The performance measure for the rest databases depends on the database. More details follow.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dimensional affect generation</head><p>For the Aff-Wild database, we use the VGG-FACE-GRU network. When augmenting the dataset with 30K or less synthesized images, no performance improvement is seen, whereas when augmenting it with more than 30K, the performance is increasing, following the increase in the granularity of synthesized data. Adding synthesized data to the training set seems to be beneficial for improving the performance and thus the improvement would be much greater if we added more than 60K (if we had more neutral expressions), although probably at a given point, a plateau would be reached (considering the large training set that consists of around 1M images).</p><p>For the RECOLA database, we use the ResNet-GRU network. When augmenting the dataset with up to 30K synthesized images, there exists small performance improvement, whereas when augmenting it with more than 30K, the performance is continuously increasing following the increase in the granularity of synthesized data; this increase is large. This is expected, since 120K frames are not sufficient for training a network for regression and additionally, 170K frames are not either.</p><p>For the AffectNet database, we use the VGG-FACE network. After adding 10K synthesized images, the performance starts to increase. This increase continues to happen as more data are added until the training set has been augmented with 1.5M data. If more data are added, the performance does not change, implying that a plateau has been reached. The final performance improvement is large.</p><p>For the AFEW-VA database, we use the VGG-FACE-GRU network. The improvement is systematically very significant. When adding more than 30K data, the increase in performance is more rapid. The performance is expected to continue increasing while more data are added, as both the initial training set of around 23K frames and the augmented set of around 135K frames are not large enough to train a DNN for regression.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Categorical affect generation</head><p>For the RAF-DB database, we use the VGG-FACE network and the performance is measured in terms of the mean diagonal value of the confusion matrix. The increase in performance is almost linear as more data are used. The final performance gain is great. RAF-DB is a very small database (of size about 12K images) and therefore if we had more data to add, the performance would further improve.</p><p>In the AffectNet database, we use the VGG-FACE network and performance is measured in terms of the F1 score. Increasing the amount of added data provides a respective increase in the performance. After adding 60K images the performance is increasing at a lower rate. It should be mentioned that the results include erroneous classification of the contempt class. If we synthesized samples of the contempt class as well, the network would provide a higher performance; but this is beyond the scope of the current paper.</p><p>In the AFEW database, we use the VGG-FACE network; the performance measure is total accuracy. The performance is increasing with the addition of more data. The performance increase is significant. The AFEW database is a small database (of size about 40K images) and therefore adding data is expected to increment the performance.</p><p>In the BU-3DFE database, we use the VGG-FACE network; the performance measure is total accuracy. There is a huge and rapid increase in network performance with the addition of data. This is explained by the very small size of BU-3DFE (around 2K) which makes it impossible to train a neural network on it.</p><p>General deductions that can be made from <ref type="figure" target="#fig_1">Fig. 20</ref>: the smaller the size of the database, the bigger and faster the increase in performance would be, when augmenting it with synthesized data from our approach the improvement in performance is small if we augment the training set with few data in proportion to its size in dimensionally annotated databases, a plateau is reached and no further improvement is seen when a lot of data (about ? 1.5M in our case) are added the performance due to data augmentation does not increase commensurately; in the AffectNet database (mainly in the valence-arousal case) the gain yielded by data augmentation saturates as N increases generally, the performance increase is larger in categorically annotated databases in comparison to dimensionally annotated ones. This is an interesting result, since it indicates that synthesizing more data is needed in the latter case, to make the data distribution more dense.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5">Effect of subjects' age in classification &amp; regression results</head><p>It is interesting to quantitatively assess the effect of age on the performance of the proposed approach. However, not all databases contain age information about their subjects. To achieve this, we trained an age estimator on them. In more detail, we trained a Wide Residual Network (WideResNet) <ref type="bibr" target="#b73">[72]</ref> on the union of IMDB <ref type="bibr" target="#b53">[52]</ref> and Adience datasets <ref type="bibr" target="#b21">[21]</ref> (so that the training dataset contained an adequate number of images of people under the age of 25) and tested it on WIKI <ref type="bibr" target="#b53">[52]</ref>. Then we applied this estimator on the test sets of the examined databases.  <ref type="table" target="#tab_0">Table 13</ref> shows, for each dimensionally annotated database (Aff-Wild, RECOLA, AffectNet and AFEW-VA), the estimated age groups (we split the age values into appropriate groups so that each group contained a significant amount of samples), the number of test samples that are within the age groups, the number of synthesized by our approach samples for each age group, different evaluation metrics (CCC and MSE) for each age group in two cases: when a network trained only with the training set of each database was used (denoted as 'Network' in <ref type="table" target="#tab_0">Table 13</ref>) and when the same network was trained with the training set augmented with our approach's synthesized data (denoted as 'Network-Augmented' in <ref type="table" target="#tab_0">Table 13</ref>). For Aff-Wild and AFEW-VA, the VGG-FACE-GRU network was used, for RECOLA the ResNet-GRU and for AffectNet the VGG-FACE. <ref type="table" target="#tab_0">Table 14</ref> is similar to <ref type="table" target="#tab_0">Table 13</ref> with the difference being that it refers to categorically annotated databases (RAF-DB, AffectNet, AFEW and BU-3DFE). In this case, the evaluation metrics are the F1 score for RAF-DB and AffectNet, and the total accuracy for AFEW and BU-3DFE. The 'VGG-FACE-Augmented' refers to the case in which the VGG-FACE network is trained on the union of training set of each database and data synthesized by our approach.</p><p>By observing the two <ref type="table" target="#tab_0">Tables (13 and 14)</ref>, it is seen that augmenting the training dataset with the images generated by our approach is beneficial in all age groups, both for regression and classification. It would be interesting to focus on specific groups, such as very young (&lt;20 years old) in RAF-DB and AffectNet, each containing more than 150 subjects, or elderly (e.g., 70-79 years old) in AffectNet, also containing more than 150 subjects. In the former case, the F1 value improved from about 0.45 to 0.6; the F1 values over all categories improved from about 0.51 to 0.66. Although the F1 values in the very young category were lower than the mean F1 values over all ages, the improvement in both cases was similar. A similar observation can be made in the latter case, of elderly persons, with the F1 value in the category being improved from about 0.4 to 0.47. Although these values were lower than the total F1 values over all ages, which were 0.51 and 0.59 respectively, the improvement in these cases was similar as well. This verifies the above-mentioned observation that the proposed approach for data augmentation can be also beneficial in cases where the number of available samples is rather small.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusions and Future Work</head><p>A novel approach to generate facial affect in faces has been presented in this paper. It leverages a dimensional emotion model in terms of valence and arousal or the six basic expressions, and a large scale 4D face database, the 4DFAB. We performed dimensional annotation of the 4DFAB and used the facial images with their respective annotations to generate mean faces on a discretized 2-D affect space. A methodology has been proposed using these mean faces to synthesize faces with affect, both categorical or dimensional, static or dynamic. Using a given neutral image and the desired affect, which can be a Valence Arousal pair of values, a path in the 2D VA space, or one of the basic expression categories, the proposed approach performs face detection and landmark localization on the input neutral image, fits a 3D Morphable Model on the resulting image, deforms the reconstructed face, adds the input affect and blends the new face with the given affect into the original image.</p><p>An extensive experimental study has been conducted, providing both qualitative and quantitative evaluations of the proposed approach. The qualitative results show the achieved higher quality of the synthesized data compared to GAN-generated facial affect. The quantitative results are based on using the synthesized facial images for data augmentation and training of Deep Neural Networks over eight databases, annotated with either dimensional or categorical affect labels. It has been shown that, over all databases, the achieved performance is much higher than i) the performance of the respective state-of-the-art methods, ii) the performance of the same DNNs with data augmentation provided by the StarGAN and Ganimation networks.</p><p>In our future work we will extend this approach to synthesize, not only dimensional, as well as categorical, affect in faces, but also Facial Action Units. In this way a Global Local synthesis of facial affect will be possible, through a unified modeling of global dimensional emotion and local action unit based facial expression synthesis. Another future direction will be to generate faces of different genders and human races.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1</head><label>1</label><figDesc>Examples from the 4DFAB of apex frames with posed expressions for the six basic expressions: Anger (AN), Disgust (DI), Fear (FE), Joy (J), Sadness (SA), Surprise (SU)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2</head><label>2</label><figDesc>The 2D Valence-Arousal Space and some representative frames of 4DFAB</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3</head><label>3</label><figDesc>The 2D histogram of annotations of 4DFAB</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4</head><label>4</label><figDesc>The facial affect synthesis framework: the user inputs an arbitrary 2D neutral face and the affect to be synthesized (a pair of valence-arousal values in this case)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5</head><label>5</label><figDesc>Some mean faces of the 550 classes in the VA Space</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6</head><label>6</label><figDesc>Generation of new facial affect from the 4D face gallery; the user provides a target VA pair</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7 (</head><label>7</label><figDesc>a)-(c). VA Case of static (facial) synthesis across all databases; first rows show the neutral, second ones show the corresponding synthesized images and third rows show the corresponding VA values. Images of: (b) kids, (c) elderly people and (a) in-between ages, are shown.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 8</head><label>8</label><figDesc>VA case of facial synthesis: on the left hand side are the neutral 2D images and on the right the synthesized images with different levels of affect</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 10 VA</head><label>10</label><figDesc>Case of temporal (facial) synthesis: on the left hand side are the neutral 2D images and on the right the synthesized image sequences</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 11</head><label>11</label><figDesc>Generated results by our approach, StarGAN and Ganimation</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 12</head><label>12</label><figDesc>The 2D histogram of valence and arousal Aff-Wild's test set annotations, along with the MSE per grid area, in the case of (a) AffWildNet and (b) VGG-FACE-GRU trained using the proposed approach</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 14</head><label>14</label><figDesc>The 2D histogram of valence and arousal AffectNet's test set annotations, along with the MSE per grid area, in the case of (a) the VGG-FACE baseline, (b) the VGG-FACE trained using the proposed approach</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. 15</head><label>15</label><figDesc>The 2D histogram of valence and arousal AffectNet's annotations for the manually annotated training set</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Fig. 18</head><label>18</label><figDesc>The confusion matrix of (a) the VGG-FACE baseline and (b) the VGG-FACE trained using the proposed approach for the AffectNet database; 0: Neutral, 1: Anger, 2: Disgust, 3: Fear, 4: Joy, 5: Sadness, 6: Surprise, 7: Contempt using the training set of the AffectNet database (let us call this network 'the VGG-FACE baseline').</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Fig. 19</head><label>19</label><figDesc>The confusion matrix of (a) the VGG-FACE baseline and (b) the VGG-FACE trained using the proposed approach for the AFEW database; 0: Neutral, 1: Anger, 2: Disgust, 3: Fear, 4: Joy, 5: Sadness, 6: Surprise</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1</head><label>1</label><figDesc></figDesc><table><row><cell>briefly presents</cell></row><row><cell>the Multi-PIE [27], Aff-Wild [30, 71], AFEW 5.0 [19],</cell></row><row><cell>AFEW-VA [31], BU-3DFE [70], RECOLA [51], Affect-</cell></row><row><cell>Net [42], RAF-DB [35], KF-ITW [8], Face place, FEI [63],</cell></row><row><cell>2D Face Sets and Bosphorus [54] databases that we used</cell></row><row><cell>in our experimental study. Let us note that for Affect-</cell></row><row><cell>Net no test set is released and thus we use the released</cell></row><row><cell>validation set to test on and randomly divide the train-</cell></row><row><cell>ing set into a training and a validation subset (with a</cell></row><row><cell>85/15 split).</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1</head><label>1</label><figDesc>Databases used in our approach, along with their properties and the number of synthesized images in the valencearousal case and the six basic expressions one; 'static' means images, 'A/V' means audiovisual sequences, i.e., videos</figDesc><table><row><cell>Databases (DBs)</cell><cell>DB Type</cell><cell>Model of Affect</cell><cell>Condition</cell><cell>DB Size</cell><cell cols="2"># of Subjects Age Range</cell><cell cols="2">Total # of Synthesized Images</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>VA</cell><cell>Basic Expr</cell></row><row><cell>MULTI-PIE [27]</cell><cell>static</cell><cell>Neutral, Surprise, Disgust, Smile + Squint, Scream</cell><cell>controlled</cell><cell>755,370</cell><cell>337 Female: 102 Male: 235</cell><cell>-</cell><cell>52,254</cell><cell>5,520</cell></row><row><cell>Kinect Fusion ITW [8]</cell><cell>static</cell><cell>Neutral, Happiness , Surprise</cell><cell>in-the-wild</cell><cell>3,264</cell><cell>17</cell><cell>-</cell><cell>116,235</cell><cell>12,236</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>200</cell><cell></cell><cell></cell><cell></cell></row><row><cell>FEI [63]</cell><cell>static</cell><cell>Neutral, Smile</cell><cell>controlled</cell><cell>2,800</cell><cell>Male: 100</cell><cell>19-40</cell><cell>11,400</cell><cell>1,200</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Female: 100</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>235</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Face place 1</cell><cell>static</cell><cell>6 Basic Expr, Neutral, Confusion</cell><cell>controlled</cell><cell>6,574</cell><cell>Male: 143</cell><cell>-</cell><cell>59,736</cell><cell>6,288</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Female: 92</cell><cell></cell><cell></cell><cell></cell></row><row><cell>AFEW 5.0 [19]</cell><cell>A/V</cell><cell>6 Basic Expr, Neutral</cell><cell>in-the-wild</cell><cell>41,406</cell><cell>&gt;330</cell><cell>1-77</cell><cell>705,649</cell><cell>56,514</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>46</cell><cell></cell><cell></cell><cell></cell></row><row><cell>RECOLA[51]</cell><cell>A/V</cell><cell>VA</cell><cell>controlled</cell><cell>345,000</cell><cell>Male: 19</cell><cell>-</cell><cell>46,455</cell><cell>4,890</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Female: 27</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>100</cell><cell></cell><cell></cell><cell></cell></row><row><cell>BU-3DFE [70]</cell><cell>static</cell><cell>6 Basic Expr, Neutral</cell><cell>controlled</cell><cell>2,500</cell><cell>Male: 56</cell><cell>18-70</cell><cell>5,700</cell><cell>600</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Female: 44</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>105</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Bosphorus[54]</cell><cell>static</cell><cell>6 Basic Expr</cell><cell>controlled</cell><cell>4,666</cell><cell>Male: 60</cell><cell>25-35</cell><cell>17,018</cell><cell>1,792</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Female: 45</cell><cell></cell><cell></cell><cell></cell></row><row><cell>AffectNet[42]</cell><cell>static</cell><cell>VA + 6 Basic Expr, Neutral + Contempt</cell><cell>in-the-wild</cell><cell>450,000 annotated manually</cell><cell>-</cell><cell>0 to &gt;50</cell><cell>2,476,235</cell><cell>176,425</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>200</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Aff-Wild [30] [71]</cell><cell>A/V</cell><cell>VA</cell><cell>in-the-wild</cell><cell>1,224,094</cell><cell>Male: 130</cell><cell>-</cell><cell>60,135</cell><cell>6,330</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Female: 70</cell><cell></cell><cell></cell><cell></cell></row><row><cell>AFEW-VA [31]</cell><cell>A/V</cell><cell>VA</cell><cell>in-the-wild</cell><cell>30,050</cell><cell>&lt;600</cell><cell>-</cell><cell>108,864</cell><cell>11,460</cell></row><row><cell>RAF-DB[35]</cell><cell>static</cell><cell>6 Basic, Neutral + 11 Compound Expr</cell><cell>in-the-wild</cell><cell>15,339 + 3,954</cell><cell>-</cell><cell>0-70</cell><cell>121,866</cell><cell>12,828</cell></row><row><cell>2D Face Sets 2 : Pain</cell><cell>static</cell><cell>6 Basic, Neutral + 10 Pain Expr</cell><cell>controlled</cell><cell>599</cell><cell>23 Female: 10 Male: 13</cell><cell>-</cell><cell>2,736</cell><cell>288</cell></row><row><cell>2D Face Sets: Iranian</cell><cell>static</cell><cell>Neutral, Smile</cell><cell>controlled</cell><cell>369</cell><cell>34 Female: 34 Male: 0</cell><cell>-</cell><cell>2,679</cell><cell>282</cell></row><row><cell>2D Face Sets: Nottingham Scans</cell><cell>static</cell><cell>Neutral</cell><cell>controlled</cell><cell>100</cell><cell>100 Female: 50 Male: 50</cell><cell>-</cell><cell>5,700</cell><cell>600</cell></row><row><cell></cell><cell></cell><cell></cell><cell>(a)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>(b)</cell><cell></cell><cell></cell><cell></cell><cell>(c)</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2</head><label>2</label><figDesc>Aff-Wild: CCC and MSE evaluation of valence &amp; arousal predictions provided by the VGG-FACE-GRU trained using our approach vs state-of-the-art networks and methods. Valence and arousal values are in [?1, 1].</figDesc><table><row><cell>Networks</cell><cell cols="2">CCC</cell><cell cols="2">MSE</cell></row><row><cell></cell><cell cols="4">Valence Arousal Valence Arousal</cell></row><row><cell>FATAUVA-Net [13]</cell><cell>0.396</cell><cell>0.282</cell><cell>0.123</cell><cell>0.095</cell></row><row><cell>VGG-FACE-GRU trained using StarGAN</cell><cell>0.556</cell><cell>0.424</cell><cell>0.085</cell><cell>0.060</cell></row><row><cell>VGG-FACE-GRU trained using Ganimation</cell><cell>0.576</cell><cell>0.433</cell><cell>0.077</cell><cell>0.057</cell></row><row><cell>AffWildNet [29, 30]</cell><cell>0.570</cell><cell>0.430</cell><cell>0.080</cell><cell>0.060</cell></row><row><cell>VGG-FACE-GRU</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>trained using the</cell><cell>0.595</cell><cell>0.445</cell><cell>0.074</cell><cell>0.051</cell></row><row><cell>proposed approach</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3</head><label>3</label><figDesc>RECOLA: CCC evaluation of valence &amp; arousal predictions provided by the ResNet-GRU trained using the proposed approach vs other state-of-the-art networks and methods.</figDesc><table><row><cell>Networks</cell><cell cols="2">CCC</cell></row><row><cell></cell><cell cols="2">Valence Arousal</cell></row><row><cell>ResNet-GRU [30]</cell><cell>0.462</cell><cell>0.209</cell></row><row><cell>ResNet-GRU trained using StarGAN</cell><cell>0.503</cell><cell>0.245</cell></row><row><cell>ResNet-GRU trained using Ganimation</cell><cell>0.486</cell><cell>0.222</cell></row><row><cell>fine-tuned AffWildNet[30]</cell><cell>0.526</cell><cell>0.273</cell></row><row><cell>ResNet-GRU trained using the proposed approach</cell><cell>0.554</cell><cell>0.312</cell></row><row><cell>(a)</cell><cell></cell><cell></cell></row><row><cell>(b)</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4</head><label>4</label><figDesc>AffectNet: CCC, P-CC, SAGR and MSE evaluation of valence &amp; arousal predictions provided by the VGG-FACE trained using the proposed approach vs state-of-the-art networks and methods. Valence and arousal values are in [?1, 1].</figDesc><table><row><cell>Networks</cell><cell>CCC</cell><cell></cell><cell>P-CC</cell><cell></cell><cell cols="2">SAGR</cell><cell>MSE</cell><cell></cell></row><row><cell></cell><cell cols="8">Valence Arousal Valence Arousal Valence Arousal Valence Arousal</cell></row><row><cell>AlexNet [42]</cell><cell>0.60</cell><cell>0.34</cell><cell>0.66</cell><cell>0.54</cell><cell>0.74</cell><cell>0.65</cell><cell>0.14</cell><cell>0.17</cell></row><row><cell>the VGG-FACE baseline</cell><cell>0.50</cell><cell>0.37</cell><cell>0.54</cell><cell>0.48</cell><cell>0.65</cell><cell>0.60</cell><cell>0.19</cell><cell>0.18</cell></row><row><cell>VGG-FACE trained using StarGAN</cell><cell>0.55</cell><cell>0.42</cell><cell>0.58</cell><cell>0.49</cell><cell>0.74</cell><cell>0.73</cell><cell>0.17</cell><cell>0.16</cell></row><row><cell>VGG-FACE trained using Ganimation</cell><cell>0.56</cell><cell>0.45</cell><cell>0.59</cell><cell>0.51</cell><cell>0.74</cell><cell>0.74</cell><cell>0.15</cell><cell>0.16</cell></row><row><cell>VGG-FACE trained using the proposed approach</cell><cell>0.62</cell><cell>0.54</cell><cell>0.66</cell><cell>0.55</cell><cell>0.78</cell><cell>0.75</cell><cell>0.14</cell><cell>0.15</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5</head><label>5</label><figDesc>AFEW-VA: P-CC and MSE evaluation of valence &amp; arousal predictions provided by the VGG-FACE trained using the proposed approach vs state-of-the-art network and methods. Valence and arousal values are in [?1, 1].</figDesc><table><row><cell>Networks</cell><cell cols="2">Pearson CC</cell><cell cols="2">MSE</cell></row><row><cell></cell><cell cols="4">Valence Arousal Valence Arousal</cell></row><row><cell>best of [31]</cell><cell>0.407</cell><cell>0.450</cell><cell>0.484</cell><cell>0.247</cell></row><row><cell>VGG-FACE trained using StarGAN</cell><cell>0.512</cell><cell>0.489</cell><cell>0.262</cell><cell>0.097</cell></row><row><cell>VGG-FACE trained using Ganimation</cell><cell>0.491</cell><cell>0.453</cell><cell>0.308</cell><cell>0.151</cell></row><row><cell>VGG-FACE-GRU</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>trained using</cell><cell>0.562</cell><cell>0.614</cell><cell>0.226</cell><cell>0.075</cell></row><row><cell>the proposed approach</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 10 Table 11 Table 12</head><label>101112</label><figDesc>CCC and MSE evaluation of valence &amp; arousal predictions provided by the: i) AffWildNet (trained on Aff-Wild), ii) ResNet-GRU (trained on RECOLA) and iii) the VGG-FACE baseline (trained on AffectNet); these networks are tested on the synthesized images by StarGAN, Ganimation and our approach. Each score is shown in the format: Valence value-Arousal value CCC and MSE evaluation of valence &amp; arousal predictions provided by the: i) AffWildNet, ii) ResNet-GRU and iii) the VGG-FACE baseline; these networks are trained on the synthesized images by StarGAN, Ganimation and our approach; these networks are evaluated on the Aff-Wild, RECOLA and AffectNet test sets. Each score is shown in the format: Valence value-Arousal value Databases used in our approach and the different values of N for each one; N denotes a subset of the synthesized data (per database) by the proposed approach</figDesc><table><row><cell>Databases</cell><cell cols="2">Methods</cell><cell cols="2">Evaluation Metrics</cell><cell></cell><cell>Networks</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">AffWildNet[30] ResNet-GRU[30] the VGG-FACE baseline</cell></row><row><cell>Aff-Wild</cell><cell cols="2">StarGAN Ganimation</cell><cell></cell><cell>CCC MSE CCC MSE</cell><cell>0.33-0.26 0.21-0.19 0.35-0.28 0.19-0.16</cell><cell>--</cell><cell>--</cell></row><row><cell></cell><cell></cell><cell>Ours</cell><cell></cell><cell>CCC MSE</cell><cell>0.43-0.33 0.15-0.13</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell cols="2">StarGAN</cell><cell></cell><cell>CCC</cell><cell>-</cell><cell>0.29-0.23</cell><cell>-</cell></row><row><cell>RECOLA</cell><cell cols="2">Ganimation</cell><cell></cell><cell>CCC</cell><cell>-</cell><cell>0.28-0.22</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell>Ours</cell><cell></cell><cell>CCC</cell><cell>-</cell><cell>0.34-0.33</cell><cell>-</cell></row><row><cell>AffectNet</cell><cell cols="2">StarGAN Ganimation</cell><cell></cell><cell>CCC MSE CCC MSE</cell><cell>--</cell><cell>--</cell><cell>0.23-0.23 0.34-0.37 0.26-0.21 0.31-0.38</cell></row><row><cell></cell><cell></cell><cell>Ours</cell><cell></cell><cell>CCC MSE</cell><cell>-</cell><cell>-</cell><cell>0.39-0.31 0.27-0.28</cell></row><row><cell cols="2">Databases</cell><cell cols="2">Methods</cell><cell>Evaluation Metrics</cell><cell></cell><cell>Networks</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">AffWildNet ResNet-GRU VGG-FACE baseline</cell></row><row><cell cols="2">Aff-Wild</cell><cell cols="2">StarGAN Ganimation</cell><cell>CCC MSE CCC MSE</cell><cell>0.16-0.13 0.18-0.17 0.17-0.14 0.17-0.15</cell><cell>--</cell><cell>--</cell></row><row><cell></cell><cell></cell><cell>Ours</cell><cell></cell><cell>CCC MSE</cell><cell>0.21-0.20 0.15-0.12</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell cols="2">StarGAN</cell><cell>CCC</cell><cell>-</cell><cell>0.19-0.10</cell><cell>-</cell></row><row><cell cols="2">RECOLA</cell><cell cols="2">Ganimation</cell><cell>CCC</cell><cell>-</cell><cell>0.17-0.10</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell>Ours</cell><cell></cell><cell>CCC</cell><cell>-</cell><cell>0.23-0.14</cell><cell>-</cell></row><row><cell cols="2">AffectNet</cell><cell cols="2">StarGAN Ganimation</cell><cell>CCC MSE CCC MSE</cell><cell>--</cell><cell>--</cell><cell>0.37-0.29 0.23-0.21 0.40-0.31 0.20-0.19</cell></row><row><cell></cell><cell></cell><cell>Ours</cell><cell></cell><cell>CCC MSE</cell><cell>-</cell><cell>-</cell><cell>0.45-0.35 0.18-0.17</cell></row><row><cell></cell><cell cols="2">Databases</cell><cell></cell><cell></cell><cell cols="2">N synthesized data</cell></row><row><cell></cell><cell cols="2">Aff-Wild</cell><cell></cell><cell cols="3">N ? {10K, 20K, 30K, 40K, 50K, 60K}</cell></row><row><cell></cell><cell cols="2">RECOLA</cell><cell></cell><cell></cell><cell cols="2">N ? {10K, 20K, 30K, 40K, 50K}</cell></row><row><cell cols="3">AffectNet (VA)</cell><cell cols="4">N ? {10K, 20K, 30K, 40K, 50K, 60K, 70K, 80K, 90K, 100K, 110K, 300K, 600K, 1M, 1.5M, 2M, 2.5M }</cell></row><row><cell></cell><cell cols="2">AFEW-VA</cell><cell></cell><cell cols="3">N ? {10K, 20K, 30K, 40K, 50K, 60K, 70K, 80K, 90K, 100K, 110K}</cell></row><row><cell></cell><cell cols="2">RAF-DB</cell><cell></cell><cell cols="3">N ? {200, 400, 600, 3.5K, 6.5K, 9.5K, 12.5K}</cell></row><row><cell cols="3">AffectNet (Expressions)</cell><cell></cell><cell cols="3">N ? {6.5K, 12.5K, 25K, 38K, 56.5K, 75K, 100K, 150K, 180K}</cell></row><row><cell></cell><cell cols="2">AFEW</cell><cell></cell><cell cols="3">N ? {3.5K, 6.5K, 12.5K, 25K, 38K, 56.5K}</cell></row><row><cell></cell><cell cols="2">BU-3DFE</cell><cell></cell><cell></cell><cell cols="2">N ? {200, 400, 600}</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 13</head><label>13</label><figDesc>Age Analysis in terms of CCC and MSE for the dimensionally annotated databases</figDesc><table><row><cell>Databases</cell><cell>Ages</cell><cell cols="2"># Test Samples # Synthesized Samples</cell><cell cols="2">Network-Augmented</cell><cell cols="2">Network</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>CCC</cell><cell>MSE</cell><cell>CCC</cell><cell>MSE</cell></row><row><cell></cell><cell>20-29</cell><cell>29,013</cell><cell>5,301</cell><cell>0.61-0.38</cell><cell>0.101-0.063</cell><cell>0.59-0.37</cell><cell>0.102-0.066</cell></row><row><cell>Aff-Wild</cell><cell>30-39 40-49</cell><cell>99,962 44,727</cell><cell>23,427 21,831</cell><cell>0.66-0.47 0.50-0.48</cell><cell>0.077-0.054 0.048-0.033</cell><cell>0.61-0.44 0.46-0.44</cell><cell>0.088-0.066 0.054-0.044</cell></row><row><cell></cell><cell>50-59</cell><cell>41,748</cell><cell>9,120</cell><cell>0.58-0.40</cell><cell>0.074-0.054</cell><cell>0.57-0.38</cell><cell>0.075-0.057</cell></row><row><cell></cell><cell>total</cell><cell>215,450</cell><cell>59,679</cell><cell cols="4">0.60-0.45 0.074-0.051 0.57-0.43 0.080-0.060</cell></row><row><cell></cell><cell>30-39</cell><cell>90,000</cell><cell>11,001</cell><cell>0.61-0.38</cell><cell>-</cell><cell>0.60-0.34</cell><cell>-</cell></row><row><cell>RECOLA</cell><cell>40-49</cell><cell>15,000</cell><cell>16,188</cell><cell>0.43-0.24</cell><cell>-</cell><cell>0.36-0.19</cell><cell>-</cell></row><row><cell></cell><cell>50-59</cell><cell>7,500</cell><cell>11,742</cell><cell>0.49-0.20</cell><cell>-</cell><cell>0.44-0.10</cell><cell>-</cell></row><row><cell></cell><cell>total</cell><cell>112,500</cell><cell>38,931</cell><cell>0.55-0.31</cell><cell>-</cell><cell>0.53-0.27</cell><cell>-</cell></row><row><cell></cell><cell>0-19</cell><cell>172</cell><cell>118,902</cell><cell>0.67-0.55</cell><cell>0.105-0.156</cell><cell>0.61-0.41</cell><cell>0.127-0.181</cell></row><row><cell></cell><cell>20-29</cell><cell>1,179</cell><cell>714,232</cell><cell>0.60-0.53</cell><cell>0.128-0.159</cell><cell>0.51-0.36</cell><cell>0.170-0.193</cell></row><row><cell>AffectNet</cell><cell>30-39 40-49</cell><cell>1,218 762</cell><cell>814,588 452,504</cell><cell>0.64-0.54 0.64-0.61</cell><cell>0.139-0.145 0.149-0.134</cell><cell>0.50-0.39 0.49-0.44</cell><cell>0.193-0.169 0.202-0.166</cell></row><row><cell></cell><cell>50-59</cell><cell>569</cell><cell>229,938</cell><cell>0.58-0.53</cell><cell>0.161-0.149</cell><cell>0.47-0.34</cell><cell>0.216-0.181</cell></row><row><cell></cell><cell>60-89</cell><cell>600</cell><cell>146,091</cell><cell>0.62-0.44</cell><cell>0.145-0.167</cell><cell>0.51-0.29</cell><cell>0.200-0.195</cell></row><row><cell></cell><cell>total</cell><cell>4,500</cell><cell>2,476,235</cell><cell cols="4">0.62-0.54 0.141-0.150 0.50-0.37 0.190-0.180</cell></row><row><cell></cell><cell>20-29</cell><cell>766</cell><cell>17,466</cell><cell>0.46-0.60</cell><cell>0.192-0.084</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>30-39</cell><cell>1,990</cell><cell>36,388</cell><cell>0.51-0.62</cell><cell>0.254-0.080</cell><cell>-</cell><cell>-</cell></row><row><cell>AFEW-VA</cell><cell>40-49</cell><cell>1,558</cell><cell>34,906</cell><cell>0.59-0.47</cell><cell>0.211-0.076</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>50-59</cell><cell>946</cell><cell>15,102</cell><cell>0.74-0.85</cell><cell>0.215-0.045</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>60-79</cell><cell>396</cell><cell>4,102</cell><cell>0.63-0.45</cell><cell>0.236-0.100</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>total</cell><cell>5,646</cell><cell>108,864</cell><cell>0.57-0.59</cell><cell>0.226-0.075</cell><cell>-</cell><cell>-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 14</head><label>14</label><figDesc>Age Analysis for the categorically annotated databases; criterion for RAF-DB &amp; AffectNet is F1 score, for AFEW &amp; BU-3DFE is total accuracy; AFEW test samples refer to: number of videos (frames)</figDesc><table><row><cell>Databases</cell><cell>Ages</cell><cell cols="3"># Test Samples # Synthesized Samples VGG-FACE-Augmented</cell><cell>VGG-FACE</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Performance Metric</cell><cell>Performance Metric</cell></row><row><cell></cell><cell>10-19</cell><cell>168</cell><cell>210</cell><cell>0.631</cell><cell>0.446</cell></row><row><cell></cell><cell>20-29</cell><cell>911</cell><cell>2,250</cell><cell>0.813</cell><cell>0.556</cell></row><row><cell></cell><cell>30-39</cell><cell>998</cell><cell>4,320</cell><cell>0.739</cell><cell>0.498</cell></row><row><cell>RAF-DB</cell><cell>40-49</cell><cell>516</cell><cell>3,606</cell><cell>0.744</cell><cell>0.511</cell></row><row><cell></cell><cell>50-59</cell><cell>258</cell><cell>1,776</cell><cell>0.709</cell><cell>0.440</cell></row><row><cell></cell><cell>60-69</cell><cell>149</cell><cell>552</cell><cell>0.657</cell><cell>0.550</cell></row><row><cell></cell><cell>70-79</cell><cell>68</cell><cell>128</cell><cell>0.904</cell><cell>0.635</cell></row><row><cell></cell><cell>total</cell><cell>3,068</cell><cell>12,828</cell><cell>0.738</cell><cell>0.505</cell></row><row><cell></cell><cell>0-19</cell><cell>152</cell><cell>12,516</cell><cell>0.593</cell><cell>0.453</cell></row><row><cell></cell><cell>20-29</cell><cell>882</cell><cell>45,182</cell><cell>0.584</cell><cell>0.477</cell></row><row><cell></cell><cell>30-39</cell><cell>962</cell><cell>55,513</cell><cell>0.593</cell><cell>0.518</cell></row><row><cell>AffectNet</cell><cell>40-49 50-59</cell><cell>594 431</cell><cell>27,632 20,204</cell><cell>0.586 0.648</cell><cell>0.532 0.606</cell></row><row><cell></cell><cell>60-69</cell><cell>289</cell><cell>11,178</cell><cell>0.564</cell><cell>0.498</cell></row><row><cell></cell><cell>70-79</cell><cell>161</cell><cell>3,582</cell><cell>0.466</cell><cell>0.398</cell></row><row><cell></cell><cell>80-89</cell><cell>29</cell><cell>618</cell><cell>0.448</cell><cell>0.410</cell></row><row><cell></cell><cell>total</cell><cell>3,500</cell><cell>176,425</cell><cell>0.590</cell><cell>0.510</cell></row><row><cell></cell><cell>20-29</cell><cell>29 (1,536)</cell><cell>6,474</cell><cell>0.379</cell><cell>0.241</cell></row><row><cell></cell><cell>30-39</cell><cell>156 (8,568)</cell><cell>22,518</cell><cell>0.455</cell><cell>0.333</cell></row><row><cell>AFEW</cell><cell>40-49</cell><cell>132 (7,803)</cell><cell>17,934</cell><cell>0.553</cell><cell>0.439</cell></row><row><cell></cell><cell>50-59</cell><cell>57 (3,202)</cell><cell>7,482</cell><cell>0.474</cell><cell>0.456</cell></row><row><cell></cell><cell>60-79</cell><cell>16 (764)</cell><cell>2,106</cell><cell>0.438</cell><cell>0.313</cell></row><row><cell></cell><cell>total</cell><cell>390 (21,873)</cell><cell>56,514</cell><cell>0.484</cell><cell>0.379</cell></row><row><cell></cell><cell>20-29</cell><cell>115</cell><cell>192</cell><cell>0.800</cell><cell>0.600</cell></row><row><cell></cell><cell>30-39</cell><cell>100</cell><cell>240</cell><cell>0.820</cell><cell>0.570</cell></row><row><cell>BU-3DFE</cell><cell>40-49</cell><cell>100</cell><cell>120</cell><cell>0.800</cell><cell>0.550</cell></row><row><cell></cell><cell>50-59</cell><cell>100</cell><cell>30</cell><cell>0.790</cell><cell>0.490</cell></row><row><cell></cell><cell>60-70</cell><cell>85</cell><cell>18</cell><cell>0.600</cell><cell>0.400</cell></row><row><cell></cell><cell>total</cell><cell>500</cell><cell>600</cell><cell>0.768</cell><cell>0.528</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements The work of Stefanos Zafeiriou was partially funded by the FiDiPro program of Tekes with project number 1849/31/2015. The works of Dimitrios Kollias, as well as Evangelos Ververas were funded by Teaching Fellowships of Imperial College London. We also thank the NVIDIA Corporation for donating a Titan X GPU. Additionally, we would like to thank the reviewers for their valuable comments that helped us to improve this paper.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Using synthetic data to improve facial expression analysis with 3d convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Abbasnejad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sridharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Denman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fookes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lucey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1609" to="1618" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Menpo: A comprehensive platform for parametric image alignment and visual deformable models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Alabort-I-Medina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Antonakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Booth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Snape</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
		<idno type="DOI">http:/doi.acm.org/10.1145/2647868.2654890</idno>
		<ptr target="http://doi.acm.org/10.1145/2647868.2654890" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM International Conference on Multimedia, MM &apos;14</title>
		<meeting>the ACM International Conference on Multimedia, MM &apos;14<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="679" to="682" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Optimal step nonrigid icp algorithms for surface registration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Amberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Romdhani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Vetter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Antoniou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Storkey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Edwards</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.04340</idno>
		<title level="m">Data augmentation generative adversarial networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Bringing portraits to life</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Averbuch-Elor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cohen-Or</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">196</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Optimization with sparsity-inducing penalties</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jenatton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Obozinski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations and Trends in Machine Learning</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="106" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title/>
		<idno type="DOI">10.1561/2200000015</idno>
		<ptr target="http://dx.doi.org/10.1561/2200000015" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Reanimating faces in images and video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Blanz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Basso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Vetter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer graphics forum</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="641" to="650" />
			<date type="published" when="2003" />
			<publisher>Wiley Online Library</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">3d face morphable models &quot;in-the-wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Booth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Antonakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ploumpis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Trigeorgis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Panagakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1701.05360" />
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Large scale 3d morphable models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Booth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Roussos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ponniah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dunaway</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">126</biblScope>
			<biblScope unit="issue">2-4</biblScope>
			<biblScope unit="page" from="233" to="254" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Optimal uv spaces for facial morphable model construction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Booth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Image Processing (ICIP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="4672" to="4676" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Displaced dynamic expression regression for real-time facial tracking and animation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">43</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Vggface2: A dataset for recognising faces across pose and age</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">M</forename><surname>Parkhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Automatic Face &amp; Gesture Recognition (FG 2018)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="67" to="74" />
		</imprint>
	</monogr>
	<note>13th IEEE International Conference on</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Fatauva-net : An integrated deep learning framework for facial attribute recognition, action unit (au) detection, and valencearousal estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">Y</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">H</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Chien</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshop</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshop</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">4dfab: A large scale 4d database for facial expression analysis and biometric applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kotsia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Salt Lake City, Utah, US</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">W</forename><surname>Chew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lucey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lucey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Saragih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sridharan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">the pursuit of effective affective computing: The relationship between features and registration. IEEE Transactions on Systems, Man, and Cybernetics</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="1006" to="1016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Stargan: Unified generative adversarial networks for multi-domain image-to-image translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8789" to="8797" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A facs valid 3d dynamic action unit database with applications to 3d dynamic morphable facial modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cosker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Krumhuber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hilton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 International Conference on Computer Vision</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2296" to="2303" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Cascade multi-view hourglass model for robust 3d face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zaferiou</surname></persName>
		</author>
		<idno type="DOI">10.1109/FG.2018.00064</idno>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="399" to="403" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">From individual to group-level emotion recognition: Emotiw 5.0</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dhall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Goecke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gedeon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th ACM International Conference on Multimodal Interaction</title>
		<meeting>the 19th ACM International Conference on Multimodal Interaction</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="524" to="528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Exprgan: Facial expression editing with controllable expression intensity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sricharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Age and gender estimation of unfiltered faces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Eidinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Enbar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hassner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Forensics and Security</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2170" to="2179" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Perspective-aware manipulation of portrait photos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Fried</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">B</forename><surname>Goldman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Finkelstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">128</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Automatic face reenactment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Garrido</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Valgaerts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Rehmsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Thormahlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="4217" to="4224" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Unsupervised training for 3d morphable model regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Genova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Cole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Maschinot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sarna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Vlasic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Challenges in representation learning: A report on three machine learning contests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">L</forename><surname>Carrier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hamner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Cukierski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Thaler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Neural Information Processing</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="117" to="124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Multi-pie</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Baker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="807" to="813" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Knyazev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Shvetsov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Efremova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kuharenko</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.04598</idno>
		<title level="m">Convolutional neural networks pretrained on large face recognition datasets for emotion classification from video</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Recognition of affect in the wild using deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kollias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Nicolaou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kotsia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition Workshops</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1972" to="1979" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Deep affect prediction in-the-wild: Aff-wild database and challenge, deep architectures, and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kollias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tzirakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Nicolaou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Papaioannou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schuller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kotsia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">127</biblScope>
			<biblScope unit="issue">6-7</biblScope>
			<biblScope unit="page" from="907" to="929" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Afew-va database for valence and arousal estimation inthe-wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kossaifi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Todorovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kollias</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Kuipers</surname></persName>
		</author>
		<title level="m">Quaternions and rotation sequences</title>
		<meeting><address><addrLine>Princeton</addrLine></address></meeting>
		<imprint>
			<publisher>Princeton university press</publisher>
			<date type="published" when="1999" />
			<biblScope unit="volume">66</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Presentation and validation of the radboud faces database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Langner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Dotsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bijlstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">H</forename><surname>Wigboldus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">T</forename><surname>Hawk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Knippenberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognition and emotion</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1377" to="1388" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A concordance correlation coefficient to evaluate reproducibility</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrics</title>
		<imprint>
			<biblScope unit="page" from="255" to="268" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Reliable crowdsourcing and deep locality-preserving learning for expression recognition in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2017 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2584" to="2593" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Facial animation by optimized blendshapes from motion capture data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Animation and Virtual Worlds</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="235" to="245" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Real-time facial expression transformation for monocular rgb video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="470" to="481" />
			<date type="published" when="2019" />
			<publisher>Wiley Online Library</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Maimon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Rokach</surname></persName>
		</author>
		<title level="m">Data mining and knowledge discovery handbook</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">A unified framework for compositional fitting of active appearance models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alabort-I</forename><surname>Medina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zafeiriou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">121</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="26" to="64" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.1784</idno>
		<title level="m">Conditional generative adversarial nets</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Visio-lization: generating novel facial images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Mohammed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Prince</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">57</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Affectnet: A database for facial expression, valence, and arousal computing in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mollahosseini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hasani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Mahoor</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.03985</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Sparse localized deformation components</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Varanasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wenger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wacker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Magnor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">179</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">M</forename><surname>Parkhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">A 3d face model for pose and illumination invariant face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Paysan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Knothe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Amberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Romdhani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Vetter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Sixth IEEE International Conference on Advanced Video and Signal Based Surveillance</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="296" to="301" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Poisson image editing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>P?rez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gangnet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Blake</surname></persName>
		</author>
		<idno type="DOI">http:/doi.acm.org/10.1145/1201775.882269</idno>
		<ptr target="http://doi.acm.org/10.1145/1201775.882269" />
	</analytic>
	<monogr>
		<title level="m">ACM SIGGRAPH 2003 Papers, SIGGRAPH &apos;03</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2003" />
			<biblScope unit="page" from="313" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">X</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Pavlovic</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.07716</idno>
		<title level="m">Generative adversarial talking head: Bringing portraits to life with a weakly supervised neural network</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Ganimation: Anatomically-aware facial animation from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pumarola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agudo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sanfeliu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Moreno-Noguer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="818" to="833" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.01822</idno>
		<title level="m">Geometry-contrastive gan for facial expression transfer</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Learning to disentangle factors of variation with manifold interaction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1431" to="1439" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Introducing the recola multimodal corpus of remote collaborative and affective interactions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ringeval</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sonderegger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lalanne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">10th IEEE International Conference and Workshops on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
	<note>Automatic Face and Gesture Recognition (FG)</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Dex: Deep expectation of apparent age from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Rothe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision Workshops</title>
		<meeting>the IEEE International Conference on Computer Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="10" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Evidence of convergent validity on the dimensions of affect</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Russell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of personality and social psychology</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">1152</biblScope>
			<date type="published" when="1978" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Bosphorus database for 3d face analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Savran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Aly?z</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Dibeklioglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>G?kberk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sankur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Akarun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Workshop on Biometrics and Identity Management</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="47" to="56" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Robust principal component analysis with missing data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cheng</surname></persName>
		</author>
		<idno type="DOI">http:/doi.acm.org/10.1145/2661829.2662083</idno>
		<ptr target="http://doi.acm.org/10.1145/2661829.2662083" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd ACM International Conference on Conference on Information and Knowledge Management, CIKM &apos;14</title>
		<meeting>the 23rd ACM International Conference on Conference on Information and Knowledge Management, CIKM &apos;14<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1149" to="1158" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Learning structured output representation using deep conditional generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3483" to="3491" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Geometry guided adversarial facial expression synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 ACM Multimedia Conference on Multimedia Conference</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="627" to="635" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Deep learning face representation by joint identification-verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1988" to="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Generating facial expressions with deep belief nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Susskind</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Movellan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Anderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Affective Computing. IntechOpen</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Deepface: Closing the gap to human-level performance in face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Taigman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1701" to="1708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Face2face: Real-time face capture and reenactment of rgb videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Thies</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zollhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stamminger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nie?ner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2387" to="2395" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Headon: real-time reenactment of human portrait videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Thies</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zollh?fer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stamminger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nie?ner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">164</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">A new ranking method for principal components analysis and its application to face image analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">E</forename><surname>Thomaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">A</forename><surname>Giraldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="902" to="913" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Avec 2016: Depression, mood, and emotion recognition workshop and challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Valstar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gratch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schuller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ringeval</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lalanne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Torres Torres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Scherer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Stratou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cowie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th International Workshop on Audio/Visual Emotion Challenge</title>
		<meeting>the 6th International Workshop on Audio/Visual Emotion Challenge</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">Temporal multimodal fusion for video emotion classification in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vielzeuf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pateux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Jurie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.07200</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">Iterative estimation of rotation and translation using the quaternion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Wheeler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ikeuchi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995" />
		</imprint>
		<respStmt>
			<orgName>Carnegie-Mellon University. Department of Computer Science</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">The dictionary of affect in language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">M</forename><surname>Whissell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The measurement of emotions</title>
		<imprint>
			<publisher>Elsevier</publisher>
			<date type="published" when="1989" />
			<biblScope unit="page" from="113" to="131" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Sparse reconstruction by separable approximation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">D</forename><surname>Nowak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A T</forename><surname>Figueiredo</surname></persName>
		</author>
		<idno type="DOI">10.1109/TSP.2009.2016892</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="2479" to="2493" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Reenactgan: Learning to reenact faces via boundary transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="603" to="619" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">A 3d facial expression database for facial behavior research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Rosato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Automatic face and gesture recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="211" to="216" />
		</imprint>
	</monogr>
	<note>7th international conference on</note>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Aff-wild: Valence and arousal in-the-wildchallenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kollias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Nicolaou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Papaioannou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kotsia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1980" to="1987" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.07146</idno>
		<title level="m">Wide residual networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Joint face detection and alignment using multitask cascaded convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<idno type="DOI">10.1109/LSP.2016.2603342</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1499" to="1503" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Photorealistic facial expression synthesis by the conditional difference adversarial autoencoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">E</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Seventh International Conference on Affective Computing and Intelligent Interaction (ACII)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="370" to="376" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Unpaired image-to-image translation using cycle-consistent adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2223" to="2232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Emotion classification with data augmentation using generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pacific-Asia Conference on Knowledge Discovery and Data Mining</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="349" to="360" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
