<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multi-task Self-distillation for Graph-based Semi-Supervised Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yating</forename><surname>Ren</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junzhong</forename><surname>Ji</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingfeng</forename><surname>Niu</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of the Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minglong</forename><surname>Lei</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Beijing University of Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Multi-task Self-distillation for Graph-based Semi-Supervised Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T14:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Graph convolutional networks have made great progress in graph-based semi-supervised learning. Existing methods mainly assume that nodes connected by graph edges are prone to have similar attributes and labels, so that the features smoothed by local graph structures can reveal the class similarities. However, there often exist mismatches between graph structures and labels in many real-world scenarios, where the structures may propagate misleading features or labels that eventually affect the model performance. In this paper, we propose a multi-task self-distillation framework that injects self-supervised learning and self-distillation into graph convolutional networks to separately address the mismatch problem from the structure side and the label side. First, we formulate a self-supervision pipeline based on pre-text tasks to capture different levels of similarities in graphs. The feature extraction process is encouraged to capture more complex proximity by jointly optimizing the pre-text task and the target task. Consequently, the local feature aggregations are improved from the structure side. Second, self-distillation uses soft labels of the model itself as additional supervision, which has similar effects as label smoothing. The knowledge from the classification pipeline and the self-supervision pipeline is collectively distilled to improve the generalization ability of the model from the label side. Experiment results show that the proposed method obtains remarkable performance gains under several classic graph convolutional architectures.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Semi-supervised learning on graphs (GSSL) is a fundamental machine learning task with only limited labels for graph nodes available. The goal of GSSL is to leverage the small proportion of labeled nodes in conjunction with abundant graph structures to classify the rest of unlabeled nodes <ref type="bibr" target="#b33">(Zhu, Lafferty, and Rosenfeld 2005)</ref>. Successfully resolving GSSL problems provides support for many downstream applications, e.g., POI recommendations <ref type="bibr" target="#b27">(Yang et al. 2017)</ref>, hyperspectral image classification <ref type="bibr" target="#b21">(Shao et al. 2018)</ref>, phenotype classification (Doostparast Torshizi and Petzold 2018) and part-of-speech tagging <ref type="bibr" target="#b22">(Subramanya, Petrov, and Pereira 2010)</ref>.</p><p>Recently, graph convolutional networks (GCNs) show great potentials in GSSL problems <ref type="bibr" target="#b13">(Kipf and Welling 2017)</ref>, * leiml@bjut.edu.cn which mainly benefits from the local smoothing operations that aggregate attributes from neighbors to generate discriminative features <ref type="bibr" target="#b15">(Li, Han, and Wu 2018)</ref>. Many followed GCN models devote efforts to developing efficient aggregation functions to learn more powerful features from graph structures <ref type="bibr" target="#b23">(Veli?kovi? et al. 2018;</ref><ref type="bibr" target="#b14">Klicpera, Bojchevski, and G?nnemann 2019)</ref>. In general, the success of GSSL often requires efforts from both graph structures and labels. The fractional labels can be combined with graph structures to provide additional supervision. For example, label propagation algorithm (LPA) uses label smoothing to match the attributes and labels <ref type="bibr" target="#b16">(Li et al. 2019;</ref><ref type="bibr" target="#b10">Hongwei and Jure 2020)</ref>. Within the GCN framework, they proved that the feature aggregation gives theoretical guarantees for label propagation.</p><p>Notice that the combination of graph structures and labels only works effectively when the edges in graphs reveal the real feature or label similarities. Unfortunately, this assumption is often violated in real-world scenarios since there exist mismatches between graph structures and labels <ref type="bibr" target="#b26">(Yamaguchi and Hayashi 2017;</ref><ref type="bibr" target="#b4">Chien et al. 2021)</ref>. For example, in a citation network, a paper may cite papers from other fields. In other words, simply exploring the proximity exhibited by edges is insufficient to discover qualified levels of node similarities for inferring the labels. Besides, many methods that augment the limited labels by graphs are usually based on hard labels, where the distributional information that reveals the label similarities cannot be adequately captured during the training.</p><p>In this paper, we address these challenges by whittling down the mismatches between graph structures and label similarities, instead of directly using graph connections as vehicles to propagate hard label information. We cast our multi-task self-distillation framework by integrating graph-based self-supervised learning and self-distillation into GCNs (SDSS-GCN). From the structure side, graphbased self-supervised learning mines underlying structural similarities and is able to learn general features that facilitate semi-supervised learning. To this end, we propose to use different levels of similarities (e.g., node level, community level, and graph level) to design pre-text tasks, so that the proximity that may contribute to the prediction can be fully explored. From the label side, the soft predictions of the model itself are leveraged to capture the label distributions. Aside from the ability to improve model general-ization, self-distillation is also closely related to the label smoothing mechanism and is working as regularization for GCNs <ref type="bibr" target="#b31">(Zhang and Sabuncu 2020)</ref>.</p><p>To be concrete, we built a two-stage training architecture where self-distillation is implemented based on middle-layer outputs, classification outputs and self-supervision outputs. By jointly optimizing the self-supervision pipeline and the classification pipeline, the knowledge encapsulated in graph structures and labels is carefully explored. It is delightful to find that self-supervised learning and self-distillation are nicely assembled under the GCN framework. First, incorporating self-supervised learning may bring undesired guidance that affects the model training. Self-distillation can improve the stability of training in a teacher-free fashion. Second, the distillation of self-supervision outputs considers more information that can further improve the generalization of GCNs. The contributions of our model are summarized in four-fold:</p><p>? We propose a novel self-distilled multi-task GCN framework named SDSS-GCN for semi-supervised learning, which further mines the information within graphs and labels to resolve the mismatches between node proximity and label similarities. ? We resort to self-supervised learning based on four pretext tasks to extract different levels of proximity. The improved feature extraction process largely facilitates the local aggregation in GCNs. ? We propose to use self-distillation that is highly related to label smoothing to further improve the generalization of GCNs. The soft labels of the model itself provide distributional label information that can be matched with the structures more easily. ? Extensive experiments show that self-supervision and self-distillation are nicely incorporated in the GCN framework, and achieves impressive performance gains in several widely-used GCN-based frameworks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Background and Related Works</head><p>Graphs. We use G = (V, E) to denote a graph where V = {v 1 , v 2 , . . . , v n } is the set of n nodes, E is the set of edges describing the relations between nodes. The graph structure information can also be represented by an adjacency matrix A ? [0, 1] n?n where A i,j = 1 indicates that there exists a edge between nodes v i and v j , otherwise A i,j = 0. The feature matrix for all nodes is denoted as X, where x i is the feature vector of node v i .</p><p>Graph-based Semi-supervised Learning. In this paper, we focus on the semi-supervised node classification task where only a subset of nodes V L ? V with |V L | |V| are associated with labels drawn from a label set L = {1, ? ? ? , C}. We can also denote the labels of all nodes as a label matrix Y, where y i is the one-hot label vector for node v i . The graph-based semi-supervised learning aims at taking advantage of the graph G, node features X 1:|V L | , and node labels Y 1:|V L | to train a classifier that can infer the labels of nodes in unlabeled node set V U ? V (V U = V \ V L ). The objective to be optimized is the differences between the predictions and ground truth labels. Formally, the loss function can be represented as,</p><formula xml:id="formula_0">L NC = 1 |V L | vi?V L dist (F (A, X, N (v i )), y i ) , (1)</formula><p>where N (v i ) denotes the neighbors of v i , F is the mapping function from the input to the predictions, dist(?, ?) is the distance function that measures the differences (e.g., cross entropy), and L NC is the loss function for node classification. Traditional GSSL approaches are mainly based on graph regularizations <ref type="bibr" target="#b32">(Zhou et al. 2004;</ref><ref type="bibr" target="#b1">Belkin, Niyogi, and Sindhwani 2006)</ref>. Later approaches are mostly based on graph embedding methods <ref type="bibr" target="#b29">(Yang, Cohen, and Salakhutdinov 2016</ref>) that spur the development of advanced graph neural networks <ref type="bibr" target="#b13">(Kipf and Welling 2017;</ref><ref type="bibr" target="#b9">Hamilton, Ying, and Leskovec 2017;</ref><ref type="bibr" target="#b23">Veli?kovi? et al. 2018;</ref><ref type="bibr" target="#b6">Feng et al. 2020;</ref><ref type="bibr" target="#b17">Li, Li, and Wang 2020)</ref>.</p><p>Graph Convolutional Networks. GCN is a multi-layer neural network that iteratively aggregates features through the edges. The utilizing of local information makes it effective in graph-based semi-supervised learning. The vanilla GCN (Kipf and Welling 2017) is a two-layer neural network that can be formulated as,</p><formula xml:id="formula_1">Z = L ReLU(LXW (0) ) W (1) ,<label>(2)</label></formula><p>where Z is the output logits matrix, L =D ? 1 2?D ? 1 2 ,? is the adjacent matrix with self-loop,D is the degree matrix of A, W (0) and W (1) are parameter matrices.</p><p>The forward process of GCN in Eqn.</p><p>(2) can be generalized as the combination of a feature propagation process and a feature transformation process between l-th and (l + 1)-th layer . The feature propagation is to smooth the node features by the adjacent matrix: LH (l) ? H (l+ 1 2 ) while the feature transformation is to transform the features via parameter matrix W (l) : ReLU(H (l+ 1 2 ) W (l) ) ? H (l+1) . We use H (l+ 1 2 ) to denote the intermediate results after feature propagation.</p><p>Let ?(X, A) = L ReLU(LXW (0) ) in Eqn.</p><p>(2) be the feature extractor of GCN parameterized with ?. The graphbased semi-supervised learning based on GCN can be decomposed into a feature extraction function ?(?) and a linear transformer ?:</p><formula xml:id="formula_2">Z = ?(X, A)?, where ? = W (1) . Thus, Eqn.</formula><p>(1) can be crystallized as,</p><formula xml:id="formula_3">L NC = 1 |V L | vi?V L dist(z i , y i ),<label>(3)</label></formula><p>where z i is the output logits of node v i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>To resolve the mismatch problem between structures and labels, self-supervised learning and self-distillation strategies are used to mine the information from both structures and labels. The overall architecture is illustrated in <ref type="figure">Figure</ref> 1. Notice that our framework distills knowledge from the model itself in a teacher-free fashion and is more compatible with current graph convolutional models. Different The teacher and student network both consist of a classification pipeline and a self-supervision pipeline, where the same feature extraction backbone is used for the two pipelines. The distillation is accomplished from the classification outputs, self-supervision outputs, and middle-layer outputs.</p><p>levels of proximity are plugged into the structural information extraction backbone. The details of our framework are described in the following subsections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Graph-based Self-supervised Learning</head><p>Even graph convolutional networks are powerful feature extractors, the inner information of the data has not been fully utilized <ref type="bibr" target="#b24">(Wan et al. 2021)</ref>. In this subsection, we build a multi-task framework that leverages self-supervised learning to increase data efficiency by further mining the information of the data itself without any label information. At present, self-supervised learning has been widely used in situations with limited labels. We can formulate the general process of self-supervised learning based on GCNs similar to the node classification task,</p><formula xml:id="formula_4">Z = ?(X,?)?,<label>(4)</label></formula><p>where? is the linear transformation,X and? are the inputs of pre-text tasks,? is the self-supervision predictions. The corresponding self-supervision loss function L SS can be formulated as,</p><formula xml:id="formula_5">L SS = 1 |V L | vi?V L dist(? i ,? i ),<label>(5)</label></formula><p>where? i is the predicted logits for pre-text tasks,? i is the ground truth of v i from?, and dist(?, ?) is the distance function. We use the features of V L during the training under the inductive setting. Different from the target task, selfsupervised learning contains both classification tasks and regression tasks. We use the cross entropy as distance function for classification tasks and use smooth mean absolute error for regression tasks. Concretely, the smooth mean absolute error can be formulated as,</p><formula xml:id="formula_6">dist(?,?) = 0.5 * (? ??) 2 , if |? ??| &lt; 1, |? ??| ? 0.5. otherwise.<label>(6)</label></formula><p>The self-supervised task can be co-trained with the target task to formulate a multi-task framework. We jointly optimize the following loss function,</p><formula xml:id="formula_7">L = L NC + ?L SS ,<label>(7)</label></formula><p>where ? is a positive hyper-parameter. The multi-task training paradigm largely facilitates the feature extraction process of GCNs since it introduces knowledge from the data itself that can improve the generalization of the features <ref type="bibr" target="#b30">(You et al. 2020;</ref><ref type="bibr" target="#b12">Ke, Zhouchen, and Zhanxing 2020)</ref>. To capture different levels of graph similarities, we design several pre-text tasks for GCNs based on graph properties and prior knowledge.</p><p>Node Degrees. Node degree is an essential graph property where its distribution characterizes the network influence. Many phenomena in graphs, e.g., random walks and diffusion, are highly related to node degrees. Hence, we build the self-supervision task that predicts the node degrees based on node features and adjacent matrix. Notice that this task is unscathed since it requires no pre-processing procedure to the inputs. Consequently, the pre-text task and the target task have the same inputs during the training. Formally, we denote the continuous node degrees as labels d ? R n where d i = j A i,j . Then, predicting the node degrees is formulated as a regression problem that minimizes the distance between the predicted degrees ?(A, X)? and ground truth d.</p><p>Feature Clustering. Clustering is a classical unsupervised task that can capture the community-wise proximity in graphs. There are several attempts that use clustering to build pre-text tasks for self-supervised learning <ref type="bibr" target="#b12">(Ke, Zhouchen, and Zhanxing 2020;</ref><ref type="bibr" target="#b30">You et al. 2020)</ref>. Grouping nodes into dense clusters detects attributed similarities that are beneficial for classifications. To construct the pre-text task, we firstly run k-means on the input attributes and assign each node to a cluster C i where i ? {1, 2, ? ? ? , K} and K is the total number of clusters. Then, we can take the clustering assignments as labels and feed them into GCNs to assist the training. Notice that instead of using the features calculated by GCNs, we emphasize the role of initial attributes that have not been smoothed by structures to provide the complete attribute-perspective information.</p><p>Graph Partitioning. Graph partitioning is another community-wise pre-text task based on graph typologies. It aims to divide nodes into different subsets where inter-connections are sparse and intra-connections are dense. Unlike feature clustering, graph partitioning captures structural similarities only provided by connections. Additionally, graph partitioning considers global information, which is complementary for GCNs that focus on local information aggregation. To be concrete, we select METIS <ref type="bibr" target="#b11">(Karypis and Kumar 1998)</ref> to accomplish this goal. Given a graph G, METIS generates K distinct node sets C i where i ? {1, 2, ? ? ? , K}. To avoid extremely unbalanced partitioning, the sizes of partitioned sets are constrained by K maxi |Ci| |V| ? 1 + where is a small value between 0 and 1. Finally, the partitioning results are served as labels to guide the training of the self-supervised learning pipeline.</p><p>Graph Completion. Predicting the missing attributes in graphs brings deep understandings of the local smoothness in graphs. From the perspective of graph signal processing, the local continuity describes how signals distribute along with graph structures. Such an assumption also spurs the Laplacian regularizations in graph theory. Consequently, we build a pre-text task that aims to predict the attributes of missing nodes. The features of randomly selected nodes are firstly masked. Then, the goal turns to predict the unscathed graphs by the masked graphs. We notice that the attributes in graphs are high-dimensional, which brings additional computational burdens for the multi-task framework. To solve this problem, we employ Principle Component Analysis (PCA) to reduce the dimension of features. The key advantage of applying dimension reduction to attributes is that the self-supervised learning component is encouraged to focus on the salient information in the attributes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Graph-based Self-Distillation</head><p>After introducing the self-supervision, we harness the selfdistillation strategy into GCNs to distill knowledge from the multi-task framework. Self-distillation can transfer the knowledge from the model itself in a teacher-free fashion. In other words, the teacher network and the student network share the same structure. The knowledge from the "future" can stabilize the training process. Unlike previous knowledge distillation frameworks that only distill knowledge from the target task, we utilize supervisions from both the target and auxiliary tasks. As indicated in <ref type="figure" target="#fig_0">Figure 1</ref>, both the teacher and the student consist of three components: a feature extractor ?(?) with parameter ?, a linear transformation layer f (?) with parameter ? for the node classification, and a linear transformation layer g(?) with parameter? for the pre-text task.</p><p>The incorporating of self-distillation constitutes a twostage training scheme. In the first stage, the node classification and pre-text tasks are trained until its convergence by optimizing the objective in Eqn. 7. We can obtain the soft logits Z t for the node classification and the logits? t for the self-supervised learning . For the target task, the soft logits carry the distributional information of labels that are more adequate measurements for label similarities compared with hard labels <ref type="bibr" target="#b8">(Geoffrey, Oriol, and Jeffrey 2015)</ref>.</p><p>In the second stage, we train the student network with the supervision of the teacher. The knowledge of the multitask framework is transferred to the student by two selfdistillation losses, L SD-NC and L SD-SS , where the former is for the node classification while the latter is for pre-text tasks. Still, in most GCNs, the feature forward process in neural networks and the feature aggregation in graphs are highly intertwined. In other words, one forward GCN layer propagates features only from one-hop neighbors. This phenomenon limits the depths of many GCN-based models, which makes the information in each immediate layer important. We further propose to distill knowledge from the intermediate layer of GCNs by optimizing the loss L SD-M .</p><p>To sum up, the student network is trained with respect to the follow loss,</p><formula xml:id="formula_8">L SD = L SD-NC + L SD-SS + L SD-M ,<label>(8)</label></formula><p>where L SD is the overall self-distillation loss. We give the details of three losses as follows.</p><p>Distillation for Node Classification. To balance the hard labels and soft labels, the node classification pipeline in the student network is supervised by the combination of two terms. Using KL divergence as the distance measure, the optimization objective can be denoted as,</p><formula xml:id="formula_9">L SD-NC = ?? 1 p m j=1 p t j log p s j ? (1 ? ? 1 ) y m j=1 y j log y s j ,<label>(9)</label></formula><p>where superscripts t and s denote the teacher and the student, p = softmax(z/? ), ? is the temperature, y is the ground truth, m is the number of classes, and ? 1 is a hyperparameter that controls the ratio of supervision from the teacher.</p><p>Distillation for Self-supervision. We can distill the knowledge from self-supervised learning similar to the node classification task. The optimization objective is,</p><formula xml:id="formula_10">L SD-SS = ?? 2 p jp t j logp s j ? (1 ? ? 2 ) ? j? j log? s j ,<label>(10)</label></formula><p>wherep = softmax(?/? ),? is the ground truth, and ? 2 is the balance hyper-parameter.</p><p>Distillation for Intermediate Layer. To distill knowledge from shallow layers, we fetch the hidden outputs H t and H t from the feature extractor. It aligns the middle-layer outputs of the student network via the following function,</p><formula xml:id="formula_11">L SD-M = V L dist(H t , H s ),<label>(11)</label></formula><p>where dist(?, ?) is the smooth mean absolute error in Eqn. (6).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Overall Algorithm</head><p>The overall algorithm is listed in Algorithm 1. First, we train the teacher network which includes a node classification loss and a self-supervised learning loss. Second, the student network is trained with the supervision of the distillation from the teacher network. After the two-stage training, the student network is used for the inference. Obtain inputX for the pre-text task;</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4:</head><p>Obtain middle-layer outputs H t and? t via ? t (?);</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5:</head><p>Obtain soft logits Z t and? t via f t (?) and g t (?); 6:</p><p>Calculate the teacher loss via Eqn. (7); 7:</p><p>Update parameters ? t , ? t and? t for ? t , f t and g t ; 8: end for 9: [Training Student Network] 10: for epoch ? max epochs do 11:</p><p>Obtain inputX for the pre-text task;</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>12:</head><p>Obtain middle-layer outputs H s and? s via ? s (?);</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>13:</head><p>Obtain soft logits Z s and? s via f s (?) and g s (?);</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>14:</head><p>Calculate the distillation loss via Eqn. (8);</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>15:</head><p>Update parameters ? s , ? s and? s for ? s , f s and g s ; 16: end for 17: Return ? s and ? s .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments</head><p>To evaluate the effectiveness of the proposed framework, we conduct extensive experiments to demonstrate the improvements of graph convolutional networks after introducing the multi-task distillation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experimental Settings</head><p>Datasets. We use four public benchmark datasets for experiments. Cora, Citeseer and Pubmed <ref type="bibr" target="#b20">(Sen et al. 2008</ref>) are citation networks, where nodes represent papers and edges represent their citations. A-Comp <ref type="bibr" target="#b28">(Yang, Liu, and Shi 2021)</ref> is a co-purchase graph extracted from Amazon, where nodes represent products, edges represent the co-purchased relations of products, and features are bag-of-words vectors extracted from product reviews. For three citation datasets, we follow the public split with fixed 20 nodes per class in the training set. For A-Comp, 20 nodes are randomly sampled from each class for training, 30 nodes for validation, and the rest for test.</p><p>Parameter Settings. The experimental platform is Intel Core i7-8700K, 3.70GHz CPU, NVIDIA GeForce GTX 2080Ti GPU. We randomly initialize the parameters and employ early stopping with a patience of 50 epochs. Adam optimizer is used for optimization with default settings. We set the initial learning rate as 0.01 with weight decay 0.001. The balance hyper-parameter ? for the teacher network is tuned and set as 0.1, and the balance hyper-parameters ? 1 and ? 2 are set as 0.6 and 0.3 respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Case Study</head><p>We first carry out a case study to show how self-distillation from multi-tasks resolves the mismatches between node connections and labels. We run GCN and SDSS-GCN on Cora and show how they perform in some selected nodes that are connected by edges but with different labels.</p><p>As shown in <ref type="figure" target="#fig_1">Figure 2</ref>, the numbers within the nodes denote the node indices and their labels. For example, 2420(4) indicates that node 2420 belongs to class 4. The nodes are also colored to show their categories. After feature smoothing, GCN erroneously categories node 2420 and node 1979 into class 0 that is identical to their connected neighbor node 56. Similarly, node 299, node 1816 and node 651 are also put into class 6 by mistake. The results show that the original GCN cannot discriminate whether the labels are coinciding with connections. On the contrary, when introducing multitask self-distillation, SDSS-GCN can classify nodes <ref type="bibr">2420,</ref><ref type="bibr">1979,</ref><ref type="bibr">299,</ref><ref type="bibr">1816</ref> and 651 into the correct categories. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ablation Study</head><p>Since our framework is teacher-free and can be integrated into most GCN-based structures, we conduct the ablation study in this subsection to show the roles of different components in our framework. We gradually show that the two strategies used in our paper can largely improve the performance of these frameworks. We select five popular graph convolutional networks for comparison: vanilla GCN <ref type="bibr" target="#b13">(Kipf and Welling 2017)</ref>, GraphSAGE <ref type="bibr" target="#b9">(Hamilton, Ying, and Leskovec 2017)</ref>, GAT <ref type="bibr" target="#b23">(Veli?kovi? et al. 2018)</ref>, APPNP <ref type="bibr" target="#b14">(Klicpera, Bojchevski, and G?nnemann 2019)</ref> and GCNII   The results of vanilla GCN, GAT and GraphSAGE are reported in <ref type="table">Table 1</ref> while results of GCNII and APPNP are presented in Appendix. +SD, +SS, +SDSS denote the corresponding models with self-distillation, with selfsupervision, and with both strategies. It can be observed that the two strategies can largely improve the performance of original graph convolutional networks. Besides, the combination of self-distilling and self-supervision brings surprising performance gains. From the data perspective, the average improvements on Cora, Citeseer, Pubmed, A-Comp are 3.70%, 7.00%, 5.32%, 3.96%, among which Citeseer benefits most from our framework. From the model perspective, the average improvements for GCN, GAT and GraphSAGE are 5.55%, 4.85%, 7.77% respectively. The results are in accordance with our expectations. The improvements mainly come from our incorporating of information from both the data and model sides, which provides additional supervision that can fully utilize the graph structures and limited labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparison with the State-of-the-arts</head><p>In this subsection, we firstly give the overall comparison between our method and 12 graph neural networks on Cora, Citeseer, Pubmed and A-Comp. The baseline models are: GCN <ref type="bibr" target="#b13">(Kipf and Welling 2017)</ref>, GAT <ref type="bibr" target="#b23">(Veli?kovi? et al. 2018)</ref>, GraphSAGE <ref type="bibr" target="#b9">(Hamilton, Ying, and Leskovec 2017)</ref>, GCNII , APPNP <ref type="bibr" target="#b14">(Klicpera, Bojchevski, and G?nnemann 2019)</ref>, SGC , DropEdge <ref type="bibr" target="#b19">(Rong et al. 2019)</ref>, FastGCN <ref type="bibr" target="#b2">(Chen, Ma, and</ref><ref type="bibr">Xiao 2018), MixHop (Abu-El-Haija et al. 2019)</ref>, Graph U-Net <ref type="bibr" target="#b7">(Gao and Ji 2019)</ref>, GMNN <ref type="bibr" target="#b18">(Qu, Bengio, and Tang 2019)</ref>, and GRAND <ref type="bibr" target="#b6">(Feng et al. 2020)</ref>. We also provide the variations of our model based on different teacher networks. We report the results in <ref type="table" target="#tab_2">Table 2</ref>. It can be observed that our framework achieves the highest performance compared with baseline methods. Since our method trains the model in two stages, the knowledge from the first stage can be transferred to guide the training of the second stage. The multi-task training in each stage further mines the information within the data, which largely boosts the performance of GCN-based methods. It is surprising to find that simply grafting self-distillation and self-supervision in vanilla GCN can obtain impressive results that are even superior or competitive to advanced graph neural networks. The observations show that simple models may benefit more from the data and model knowledge. Secondly, we specially compare our framework with a recent work CPF <ref type="bibr" target="#b28">(Yang, Liu, and Shi 2021)</ref> which addresses the semi-supervised learning on graphs via knowledge distilling and label propagation. For simplicity, we report the improvements of our method (SDSS-) and CPF over five graph convolutional networks in <ref type="figure" target="#fig_2">Figure 3</ref>. The results for GAT, GCNII and APPNP are listed in Appendix. It shows that the combination of self-supervision and self-distillation achieves maximally 6.38% higher improvements compared with CPF. In addition, our framework is teacher-free and is applicable to most graph convolutional architectures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effects of Self-supervised Tasks</head><p>In this subsection, we further explore the effects of different self-supervised tasks on classification performance: predicting node degrees (Deg.), node clustering (Clu.), graph partitioning (Part.), and graph completion (Comp.). As shown in <ref type="table" target="#tab_4">Table 3</ref>   for different models and datasets. For example, In Citeseer, graph partitioning is more suitable for GraphSAGE, and predicting node degrees is more suitable for vanilla GCN. For larger graphs (e.g., Pubmed), node clustering is less competitive compared with other tasks. Graph partitioning is synchronized with GAT since they both focus on detecting the information from graph structures. Besides, the types of selfsupervised tasks can also affect the performance, e.g., the regression tasks (Deg. and Part.) are more suitable for Graph-SAGE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effects of Distillation Strategies.</head><p>In this subsection, we illustrate the effects of three kinds of knowledge distilled during the training: the knowledge from node classification (NC), the knowledge from node classification and middle-layer (NC+M), and the knowledge from node classification, middle-layer, and self-supervised learning (NC+SS+M). As shown in <ref type="table" target="#tab_5">Table 4</ref>, adding each kind of knowledge can gradually improve the performance. For GAT and GraphSAGE, the improvements of adding middlelayer information are marginal. For all methods and datasets, adding distillation and self-supervision can boost the performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effects of Training Ratios.</head><p>In this subsection, we analyze how SDSS strategy performs under different training ratios. Vanilla GCN is selected as the basic model, and the number of labeled nodes per class varies from 5 to 50. The results of different strategies (GCN, SS-GCN, SD-GCN, SDSS-GCN) on Cora are plotted in <ref type="figure" target="#fig_3">Figure 4</ref>. Under different training ratios, SDSS-GCN always achieves the best performance. Compared with selfsupervision, self-distillation is more effective in boosting classification performance. Our method also performs well with few labeled nodes. As evidence, the performance improvements of SDSS-GCN are 4.89%, 4.14%, 6.67% and 3.22% on average for 5, 10, 20 and 50 labeled nodes per class. Hence, our model is able to further mine the information in graph-based semi-supervised learning. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>In this paper, we studied the challenge of semi-supervised learning on graphs, and pointed out that using current graph structures is inadequate to capture the class similarities. The labels are shifted and can not be exactly revealed by node connections. From this perspective, we proposed a multitask self-distillation framework with a two-stage training to combine self-supervision and self-distillation. Compared with current graph-based semi-supervised learning methods, especially the knowledge distilling frameworks, we have the following two main improvements: First, we aimed at distilling knowledge from multi-tasks so that different levels of graph similarities are extracted and can be used to improve the feature aggregation in GCNs. The node features and labels can be aligned with the assist of multi-task distillation. Second, our framework is teacher-free, which relies on no specific GCN structures. As examples, we integrated our framework into several widely-used GCN structures and achieved impressive performance gains practically.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>The overview of our framework. The left side gives the process of building the self-supervision tasks. The right side describes the self-distillation process where the teacher network (top) and the student network (bottom) share the same structures.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Case study: the ground truth, the predictions of GCN and SDSS-GCN on Cora.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Comparison between SDSS-GCN and CPF in terms of performance improvements for GCN and Graph-SAGE.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Classification results under different ratios of labeled nodes on Cora.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Overall comparison with the state-of-the-arts.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>, the effects of self-supervised tasks are distinct 86.00 83.51 85.95 85.15 84.96 85.29 84.87 86.00 85.43 84.26 84.22 Citeseer 76.13 72.98 72.10 73.37 76.35 75.69 75.41 75.36 70.39 70.00 74.20 71.10 Pubmed 80.23 77.85 80.96 82.21 80.41 80.51 81.46 78.87 82.01 81.11 80.97 81.96 A-Comp 84.43 84.86 84.46 84.04 83.24 84.24 84.10 84.02 84.18 83.92 84.19 84.66</figDesc><table><row><cell>Dataset</cell><cell>Deg.</cell><cell>SDSS-GCN Clu. Part. Comp. Deg.</cell><cell>SDSS-GAT Clu. Part. Comp. Deg.</cell><cell>SDSS-SAGE Clu. Part. Comp.</cell></row><row><cell>Cora</cell><cell>85.53</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Comparison under four different auxiliary tasks.</figDesc><table><row><cell>Dataset</cell><cell>NC</cell><cell cols="2">GCN NC+M NC+SS+M</cell><cell>NC</cell><cell cols="2">GAT NC+M NC+SS+M</cell><cell>NC</cell><cell>SAGE NC+M NC+SS+M</cell></row><row><cell>Cora</cell><cell cols="2">84.22 85.10</cell><cell>86.00</cell><cell cols="2">83.56 83.56</cell><cell>85.29</cell><cell>83.57 83.89</cell><cell>86.00</cell></row><row><cell cols="3">Citeseer 71.93 73.70</cell><cell>76.13</cell><cell cols="2">74.09 74.14</cell><cell>76.35</cell><cell>70.00 70.06</cell><cell>74.20</cell></row><row><cell cols="3">Pubmed 75.88 77.52</cell><cell>82.21</cell><cell cols="2">79.26 79.28</cell><cell>81.46</cell><cell>80.52 80.52</cell><cell>82.01</cell></row><row><cell cols="3">A-comp 83.75 84.56</cell><cell>84.86</cell><cell cols="2">81.99 82.03</cell><cell>84.24</cell><cell>81.88 82.36</cell><cell>84.66</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Comparison under three different distillation strategies.</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Mixhop: Higher-order graph convolutional architectures via sparsified neighborhood mixing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Abu-El-Haija</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kapoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Alipourfard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lerman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Harutyunyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ver Steeg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Galstyan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="21" to="29" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Manifold Regularization: A Geometric Framework for Learning from Labeled and Unlabeled Examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Belkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Niyogi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sindhwani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">85</biblScope>
			<biblScope unit="page" from="2399" to="2434" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">FastGCN: Fast Learning with Graph Convolutional Networks via Importance Sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Simple and deep graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1725" to="1735" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Adaptive Universal Generalized PageRank Graph Neural Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Chien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Milenkovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Graphbased semi-supervised learning with genomic data integration using condition-responsive genes applied to phenotype classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Doostparast Torshizi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">R</forename><surname>Petzold</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Medical Informatics Association</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="99" to="108" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Graph Random Neural Networks for Semi-Supervised Learning on Graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kharlamov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="22092" to="22103" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Graph U-Nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2083" to="2092" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Distilling the Knowledge in a Neural Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">H</forename><surname>Geoffrey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Oriol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jeffrey</surname></persName>
		</author>
		<idno>abs/1503.02531</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Inductive Representation Learning on Large Graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1025" to="1035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Unifying Graph Convolutional Neural Networks and Label Propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hongwei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jure</surname></persName>
		</author>
		<idno>abs/2002.06755</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A Fast and High Quality Multilevel Scheme for Partitioning Irregular Graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Karypis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Scientific Computing</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="359" to="392" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Multi-Stage Self-Supervised Learning for Graph Convolutional Networks on Graphs with Few Labeled Nodes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhouchen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhanxing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5892" to="5899" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Predict then Propagate: Graph Neural Networks meet Personalized PageRank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Klicpera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>G?nnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deeper insights into graph convolutional networks for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-M</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3538" to="3545" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Label efficient semi-supervised learning via graph filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-M</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Guan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9582" to="9591" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Co-gcn for multi-view semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="4691" to="4698" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Gmnn: Graph markov neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5241" to="5250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">DropEdge: Towards Deep Graph Convolutional Networks on Node Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Collective Classification in Network Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Namata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bilgic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Getoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Galligher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Eliassi-Rad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AI Magazine</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">93</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Spatial and class structure regularized sparse representation graph for semi-supervised hyperspectral image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="page" from="81" to="94" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Efficient graph-based semi-supervised learning of structured tagging models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Subramanya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="167" to="176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Veli?kovi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<title level="m">Graph Attention Networks. In ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Contrastive and Generative Graph Convolutional Networks for Graphbased Semi-Supervised Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="10049" to="10057" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Simplifying Graph Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Souza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fifty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6861" to="6871" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">When Does Label Propagation Fail? A View from a Network Generative Model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yamaguchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hayashi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3224" to="3230" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Bridging collaborative filtering and semi-supervised learning: a neural approach for poi recommendation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1245" to="1254" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Extract the Knowledge of Graph Neural Networks and Go Beyond it: An Effective Knowledge Distillation Framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Web Conference</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1227" to="1237" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Revisiting Semi-Supervised Learning with Graph Embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="40" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">When Does Self-Supervision Help Graph Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">119</biblScope>
			<biblScope unit="page" from="10871" to="10880" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Self-Distillation as Instance-Specific Label Smoothing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sabuncu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="2184" to="2195" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning with local and global consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Bousquet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Lal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sch?lkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="321" to="328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Semi-Supervised Learning with Graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Rosenfeld</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
		<respStmt>
			<orgName>Carnegie Mellon University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. thesis</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
