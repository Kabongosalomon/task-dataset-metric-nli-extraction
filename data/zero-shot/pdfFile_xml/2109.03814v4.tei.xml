<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Panoptic SegFormer: Delving Deeper into Panoptic Segmentation with Transformers</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqi</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Nanjing University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhai</forename><surname>Wang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Shanghai AI Laboratory</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enze</forename><surname>Xie</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">The University of Hong</orgName>
								<address>
									<addrLine>Kong 4 NVIDIA 5 Caltech</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiding</forename><surname>Yu</surname></persName>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anima</forename><surname>Anandkumar</surname></persName>
							<email>aanandkumar@nvidia.comjosea@nvidia.compluo@cs.hku.hklutong@nju.edu.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><forename type="middle">M</forename><surname>Alvarez</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">The University of Hong</orgName>
								<address>
									<addrLine>Kong 4 NVIDIA 5 Caltech</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Lu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Nanjing University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Panoptic SegFormer: Delving Deeper into Panoptic Segmentation with Transformers</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T15:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Panoptic segmentation involves a combination of joint semantic segmentation and instance segmentation, where image contents are divided into two types: things and stuff. We present Panoptic SegFormer, a general framework for panoptic segmentation with transformers. It contains three innovative components: an efficient deeply-supervised mask decoder, a query decoupling strategy, and an improved post-processing method. We also use Deformable DETR to efficiently process multi-scale features, which is a fast and efficient version of DETR. Specifically, we supervise the attention modules in the mask decoder in a layer-wise manner. This deep supervision strategy lets the attention modules quickly focus on meaningful semantic regions. It improves performance and reduces the number of required training epochs by half compared to Deformable DETR. Our query decoupling strategy decouples the responsibilities of the query set and avoids mutual interference between things and stuff. In addition, our postprocessing strategy improves performance without additional costs by jointly considering classification and segmentation qualities to resolve conflicting mask overlaps. Our approach increases the accuracy 6.2% PQ over the baseline DETR model. Panoptic SegFormer achieves stateof-the-art results on COCO test-dev with 56.2% PQ. It also shows stronger zero-shot robustness over existing methods. The code is released at https://github.com/ zhiqi-li/Panoptic-SegFormer.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Semantic segmentation and instance segmentation are two important and related vision tasks. Their underlying connections recently motivated panoptic segmentation as a unification of both the tasks <ref type="bibr" target="#b5">[6]</ref>. In panoptic segmentation, image contents are divided into two types: things and stuff. 43.5 42.8 Max-Deeplab-S <ref type="bibr" target="#b1">[2]</ref> 48.4 62.0 Max-Deeplab-L <ref type="bibr" target="#b1">[2]</ref> 51.1 451.0 MaskFormer-T <ref type="bibr" target="#b2">[3]</ref> 47.4 42.0 MaskFormer-B <ref type="bibr" target="#b2">[3]</ref> 51.1 102.0 MaskFormer-L <ref type="bibr" target="#b2">[3]</ref> 52.7 212.0 K-Net-L <ref type="bibr" target="#b3">[4]</ref> 54.6 208.9 Panoptic SegFormer-B0 49. <ref type="bibr" target="#b4">5</ref> 24.2 Panoptic SegFormer-B2 52. <ref type="bibr" target="#b4">5</ref> 43.6 Panoptic SegFormer-B5 55.4 104.9 <ref type="figure" target="#fig_12">Figure 1</ref>. Comparison to the prior arts in panoptic segmentation methods on the COCO val2017 split. Panoptic SegFormer models outperform the other counterparts among different models. Panoptic SegFormer (PVTv2-B5 <ref type="bibr" target="#b4">[5]</ref>) achieves 55.4% PQ, surpassing previous methods with significantly fewer parameters.</p><p>Things refer to countable instances (e.g., person, car) and each instance has a unique id to distinguish it from the other instances. Stuff refers to the amorphous and uncountable regions (e.g., sky, grassland) and has no instance id <ref type="bibr" target="#b5">[6]</ref>. Recent works <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref> attempt to employ transformers to handle both things and stuff through a query set. For example, DETR <ref type="bibr" target="#b0">[1]</ref> simplifies the workflow of panoptic segmentation by adding a panoptic head on top of an end-to-end object detector. Unlike previous methods <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7]</ref>, DETR does not require additional handcrafted pipelines <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9]</ref>. While being simple, DETR also causes some issues: (1) It requires a lengthy training process to converge; (2) Because the computational complexity of self-attention is squared with the length of the input sequence, the feature resolution of DETR is limited. So that it uses an FPN-style <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b9">10]</ref> panoptic head to generate masks, which always suffer low-fidelity boundaries; (3) It handles things and stuff equally, yet representing them with bounding boxes, which may be suboptimal for stuff <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref>. Although DETR achieves excellent perfor-  <ref type="figure">Figure 2</ref>. Overview of Panoptic SegFormer. Panoptic SegFormer is composed of backbone, encoder, and decoder. The backbone and the encoder output and refine multi-scale features. Inputs of the location decoder are N th thing queries and the multi-scale features. We feed N th thing queries from the location decoder and Nst stuff queries to the mask decoder. The location decoder aims to learn reference points of queries, and the mask decoder predicts the final category and mask. Details of the decoder will be introduced below. We use a mask-wise merging method instead of the commonly used pixel-wise argmax method to perform inference. mance on the object detection task, its superiority on panoptic segmentation has not been well demonstrated. In order to overcome the defects of DETR on panoptic segmentation, we propose a series of novel and effective strategies that improve the performance of transformer-based panoptic segmentation models by a large margin. Our approach. In this work, we propose Panoptic Seg-Former, a concise and effective framework for panoptic segmentation with transformers. Our framework design is motivated by the following observations: 1) Deep supervision matters in learning high-qualities discriminative attention representations in the mask decoder. 2) Treating things and stuff with the same recipe <ref type="bibr" target="#b0">[1]</ref> is suboptimal due to the different properties between things and stuff <ref type="bibr" target="#b5">[6]</ref>. 3) Commonly used post-processing such as pixel-wise argmax <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref> tends to generate false-positive results due to extreme anomalies. We overcome these challenges in Panoptic SegFormer framework as follows:</p><p>? We propose a mask decoder that utilizes multi-scale attention maps to generate high-fidelity masks. The mask decoder is deeply-supervised, promoting discriminative attention representations in the intermediate layers with better mask qualities and faster convergence.</p><p>? We propose a query decoupling strategy that decomposes the query set into a thing query set to match things via bipartite matching and another stuff query set to process stuff with class-fixed assign. This strategy avoids mutual interference between things and stuff within each query and significantly improves the qualities of stuff segmentation. Kindly refer to Sec. 3.3.1 and <ref type="figure" target="#fig_11">Fig. 3</ref> for more details.</p><p>? We propose an improved post-processing method to generate results in panoptic format. Besides being more efficient than the widely used pixel-wise argmax method, our method contains a mask-wise merging strategy that considers both classification probability and predicted mask qualities. Our post-processing method alone renders a 1.3% PQ improvement to DETR <ref type="bibr" target="#b0">[1]</ref>. We conduct extensive experiments on COCO <ref type="bibr" target="#b10">[11]</ref> dataset. As shown in <ref type="figure" target="#fig_12">Fig. 1</ref>, Panoptic SegFormer significantly surpasses priors arts such as MaskFormer <ref type="bibr" target="#b2">[3]</ref> and K-Net <ref type="bibr" target="#b3">[4]</ref> with much fewer parameters. With deformable attention <ref type="bibr" target="#b11">[12]</ref> and our deeply-supervised mask decoder, our method requires much fewer training epochs than previous transformer-based methods (24 vs. 300+) <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b2">3]</ref>. In addition, our approach also achieves competitive performance with current methods <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14]</ref> on the instance segmentation task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Panoptic Segmentation. Panoptic segmentation becomes a popular task for holistic scene understanding <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b14">[15]</ref><ref type="bibr" target="#b15">[16]</ref><ref type="bibr" target="#b16">[17]</ref>. The panoptic segmentation literature mainly treats this problem as a joint task of instance segmentation and semantic segmentation where things and stuff are handled separately <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19]</ref>. Kirillov et al. <ref type="bibr" target="#b5">[6]</ref> proposed the concept of and benchmark of panoptic segmentation together with a baseline that directly combines the outputs of individual instance segmentation and semantic segmentation models. Since then, models such as Panoptic FPN <ref type="bibr" target="#b6">[7]</ref>, UPSNet <ref type="bibr" target="#b8">[9]</ref> and AUNet <ref type="bibr" target="#b19">[20]</ref> have improved the accuracy and reduced the computational overhead by combining instance segmentation and semantic segmentation into a single model. However, these methods approximate the target task by solving the surrogate sub-tasks, therefore introducing undesired model complexities and suboptimal performance.</p><p>Recently, efforts have been made to unify the framework of panoptic segmentation. Li et al. <ref type="bibr" target="#b20">[21]</ref> proposed Panoptic FCN where the panoptic segmentation pipeline is simplified with a "top-down meets bottom-up" two-branch design similar to CondInst <ref type="bibr" target="#b21">[22]</ref>. In their work, things and stuff are jointly modeled by an object/region-level kernel branch and an image-level feature branch. Several recent works represent things and stuff as queries and perform endto-end panoptic segmentation via transformers. DETR <ref type="bibr" target="#b0">[1]</ref> predicts the bounding boxes of things and stuff and combines the attention maps of the transformer decoder and the feature maps of ResNet <ref type="bibr" target="#b22">[23]</ref> to perform panoptic segmentation. Max-Deeplab <ref type="bibr" target="#b1">[2]</ref> directly predicts object categories and masks through a dual-path transformer regardless of the category being things or stuff. On top of DETR, MaskFomer <ref type="bibr" target="#b2">[3]</ref> used an additional pixel decoder to refine high spatial resolution features and generated the masks by multiplying queries and features from the pixel decoder. Due to the computational complexity of self attention <ref type="bibr" target="#b23">[24]</ref>, both DETR and MaskFormer use feature maps with limited spatial resolutions for panoptic segmentation, which hurts the performance and requires combining additional highresolution feature maps in final mask prediction. Unlike the methods mentioned above, our query decoupling strategy deals with things and stuff with separate query sets. Although thing and stuff queries are designed for different targets, they are processed by the mask decoder with the same workflow. Prediction results of these queries are in the same format so that we can process them in an equal manner during the post-processing procedure. One concurrent work <ref type="bibr" target="#b3">[4]</ref> employs a similar line of thinking to use dynamic kernels to perform instance and semantic segmentation, and it aims to utilize unified kernels to handle various segmentation tasks. In contrast to it, we aim to delve deeper into the transformerbased panoptic segmentation. Due to the different nature of various tasks, whether a unified pipeline is suitable for these tasks is still an open problem. In this work, we utilize an additional location decoder to assist things to learn location clues and get better results.</p><p>End-to-end Object Detection. The recent popular endto-end object detection frameworks have inspired many other related works <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b24">25]</ref>. DETR <ref type="bibr" target="#b0">[1]</ref> is arguably the most representative end-to-end object detector among these methods. DETR models the object detection task as a dictionary lookup problem with learnable queries and employs an encoder-decoder transformer to predict bounding boxes without extra post-processing. DETR greatly simplifies the conventional detection framework and removes many handcrafted components such as Non-Maximum Suppression (NMS) <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b26">27]</ref> and anchors <ref type="bibr" target="#b26">[27]</ref>. Zhu et al. <ref type="bibr" target="#b11">[12]</ref> proposed Deformable DETR, which further reduces the memory and computational cost through deformable attention layers. In this work, we adopt deformable attention <ref type="bibr" target="#b11">[12]</ref> for the improved efficiency and convergence over DETR <ref type="bibr" target="#b0">[1]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Overall Architecture</head><p>As illustrated in <ref type="figure">Fig. 2</ref>  <ref type="figure" target="#fig_11">Figure 3</ref>. (a) Methods <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref> adopt one query set to match things (purple squares) and stuff (green squares) jointly. (b) We use one thing query set (purple circles) to target things through bipartite matching and one stuff query set (green circles) to predict stuff by a class-fixed assign strategy. ? is assigned to not-matched queries. and mask decoder, where (1) the transformer encoder is applied to refine the multi-scale feature maps given by the backbone, (2) the location decoder is designed to capturing location clues of things, and (3) the mask decoder is for final classification and segmentation. Our architecture feeds an input image X ? R H?W ?3 to the backbone network, and obtains the feature maps C 3 , C 4 , and C 5 from the last three stages, of which the resolutions are 1/8, 1/16 and 1/32 compared to the input image, respectively. We project the three feature maps to the ones with 256 channels by a fully-connected (FC) layer, and flatten them into feature tokens C ? 3 , C ? 4 , and C ? 5 . Here, we define L i as H 2 i+2 ? W 2 i+2 , and the shapes of C ? 3 , C ? 4 , and C ? 5 are L 1 ?256, L 2 ?256, and L 3 ?256, respectively. Next, using the concatenated feature tokens as input, the transformer encoder outputs the refined features of size (L 1 +L 2 +L 3 )?256. After that, we use N th and N st randomly initialized things and stuff queries to describe things and stuff separately. Location decoder refines N th thing queries by detecting the bounding boxes of things to capture location information. The mask decoder then takes both things and stuff queries as input and predicts mask and category at each layer.</p><p>During inference, we adopt a mask-wise merging strategy to convert the predicted masks from final mask decoder layer into the panoptic segmentation results, which will be introduced in detail in Sec. 3.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Transformer Encoder</head><p>High-resolution and the multi-scale features maps are important for the segmentation tasks <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b27">28]</ref>. Since the high computational cost of self-attention layer, previous transformer-based methods <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b2">3]</ref> can only process lowresolution feature maps (e.g., ResNet C 5 ) in their encoders, which limits the segmentation performance. Different from these methods, we employ the deformable attention <ref type="bibr" target="#b11">[12]</ref> to implement our transformer encoder. Due to the low computational complexity of the deformable attention, our encoder can refine and involve positional encoding <ref type="bibr" target="#b23">[24]</ref> to high-resolution and multi-scale feature maps F .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Decoder</head><p>In this section, we introduce our query decoupling strategy firstly, and then we will explain the details of our loca-tion decoder and mask decoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Query Decoupling Strategy</head><p>We argue that using one query set to handle both things and stuff equally is suboptimal. Since there many different properties between them, things and stuff is likely to interfere with each other and hurt the model performance, especially for PQ st . To prevent things and stuff from interfering with each other, we apply a query decoupling strategy in Panoptic SegFormer, as shown in <ref type="figure" target="#fig_11">Fig. 3</ref>. Specifically, N th thing queries are used to predict things results, and N st stuff queries target stuff only. Using this form, things and stuff queries can share the same pipeline since they are in the same format. We can also customize private workflow for things or stuff according to the characteristics of different tasks. In this work, we use an additional location decoder to detect individual instances with thing queries, and this will assist in distinguishing between different instances <ref type="bibr" target="#b5">[6]</ref>. Mask decoder accepts both thing queries and stuff queries and generates the final masks and categories. Note that, for thing queries, ground truths are assigned by bipartite matching strategy. For stuff, We use a class-fixed assign strategy, and each stuff query corresponds to one stuff category.</p><p>Thing and stuff queries will output results in the same format, and we handle these results with a uniform postprocessing method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Location Decoder</head><p>Location information plays an important role in distinguishing things with different instance ids in the panoptic segmentation task <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b28">29]</ref>. Inspired by this, we employ a location decoder to introduce the location information of things into the learnable queries. Specifically, given N th randomly initialized thing queries and the refined feature tokens generated by transformer encoder, the decoder will output N th location-aware queries.</p><p>In the training phase, we apply an auxiliary MLP head on top of location-aware queries to predict the bounding boxes and categories of the target object, We supervise the prediction results with a detection loss L det . The MLP head is an auxiliary branch, which can be discarded during the inference phase. The location decoder follows Deformable DETR <ref type="bibr" target="#b11">[12]</ref>. Notably, the location decoder can learn location information by predicting the mass centers of masks instead of bounding boxes. This box-free model can still achieve comparable results to our box-based model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.3">Mask Decoder</head><p>As shown in <ref type="figure">Fig. 2 (d)</ref>, the mask decoder is proposed to predict the categories and masks according to the given queries. The queries Q of the mask decoder are the locationaware thing queries from the location decoder or the classfixed stuff queries. The keys K and values V of the mask decoder are projected from the refined feature tokens F from the transformer encoder. We first pass thing queries through the mask decoder, and then fetch the attention map A ? R N ?h?(L1+L2+L3) and the refined query Q refine ? R N ?256 from each decoder layer, where N = N th +N st is the whole query number, h is the number of attention heads, and L 1 +L 2 +L 3 is the length of feature tokens F .</p><p>Similar to methods <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>, we directly perform classification through a FC layer on top of the refined query Q refine from each decoder layer. Each thing query needs to predict probabilities over all thing categories. Stuff query only predicts the probability of its corresponding stuff category.</p><p>At the same time, to predict the masks, we first split and reshape the attention maps A into attention maps A 3 , A 4 , and A 5 , which have the same spatial resolution as C 3 , C 4 , and C 5 . This process can be formulated as:</p><formula xml:id="formula_0">(A 3 , A 4 , A 5 ) = Split(A), A i ? R H 2 i+2 ? W 2 i+2 ?h ,<label>(1)</label></formula><p>where Split(?) denotes the split and reshaping operation.</p><p>After that, as illustrated in Eq. <ref type="formula" target="#formula_1">(2)</ref>, we upsample these attention maps to the resolution of H/8?W/8 and concatenate them along the channel dimension,</p><formula xml:id="formula_1">A fused = Concat(A 1 , Up ?2 (A 2 ), Up ?4 (A 3 )),<label>(2)</label></formula><p>where Up ?2 (?) and Up ?4 (?) mean the 2 times and 4 times bilinear interpolation operations, respectively. Concat(?) is the concatenation operation. Finally, based on the fused attention maps A fused , we predict the binary mask through a 1?1 convolution. Previous literature <ref type="bibr" target="#b11">[12]</ref> argues that the reason for slow convergence of DETR is that attention modules equally pay attention to all the pixels in the feature maps, and learning to focus on sparse meaningful locations requires plenty of effort. We use two key designs to solve this problem in our mask decoder: (1) Using an ultra-light FC head to generate masks from the attention maps, ensuring attention modules can be guided by ground truth mask to learn where to focus on. This FC head only contains 200 parameters, which ensures the semantic information of attention maps is highly related to the mask. Intuitively, the ground truth mask is exactly the meaningful region on which we expect the attention module to focus. (2) We employ deep supervision in the mask decoder. Attention maps of each layer will be supervised by the mask, the attention module can capture meaningful information in the earlier stage. This can highly accelerate the learning process of attention modules.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Loss Function</head><p>During training, our overall loss function of Panoptic SegFormer can be written as:</p><formula xml:id="formula_2">L = ? things L things + ? stuff L stuff ,<label>(3)</label></formula><p>where L things and L stuff are loss for things and stuff, separately. ? things and ? stuff are hyperparameters.</p><p>Things Loss. Following common practices <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b29">30]</ref>, we search the best bipartite matching between the prediction set and the ground truth set. Specifically, we utilize Hungarian algorithm <ref type="bibr" target="#b30">[31]</ref> to search for the permutation with the minimum matching cost, which is the sum of the classification loss L cls , detection loss L det and the segmentation loss L seg . The overall loss function for the thing categories is accordingly defined as follows:</p><formula xml:id="formula_3">L things = ? det L det + Dm i (? cls L i cls + ? seg L i seg ),<label>(4)</label></formula><p>where ? cls , ? seg , and ? loc are the weights to balance three losses. D m is the number of layers in the mask decoder. L i cls is the classification loss that is implemented by Focal loss <ref type="bibr" target="#b26">[27]</ref>, and L i seg is the segmentation loss implemented by Dice loss <ref type="bibr" target="#b31">[32]</ref>. L det is the loss of Deformable DETR that used to perform detection.</p><p>Stuff Loss. We use a fixed matching strategy for stuff. Thus there is a one-to-one mapping between stuff queries and stuff categories. The loss for the stuff categories is similarly defined as:</p><formula xml:id="formula_4">L stuff = Dm i (? cls L i cls + ? seg L i seg ),<label>(5)</label></formula><p>where L i cls and L i seg are the same as those in Eq. (4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Mask-Wise Merging Inference</head><p>Panoptic Segmentation requires each pixel to be assigned a category label (or void) and instance id (ignored for stuff) <ref type="bibr" target="#b5">[6]</ref>. One challenge of panoptic segmentation is that it requires generating non-overlap results. Recent methods <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref> directly use pixel-wise argmax to determine the attribution of each pixel, and this can solve the overlap problem naturally. Although pixel-wise argmax strategy is simple and effective, we observe that it consistently produces false-positive results due to the abnormal pixel values.</p><p>Unlike pixel-wise argmax resolves conflicts on each pixel, we propose the mask-wise merging strategy by resolving the conflicts among predicted masks. Specifically, we use the confidence scores of masks to determine the attribution of the overlap region. Inspired by previous NMS methods <ref type="bibr" target="#b27">[28]</ref>, the confidence scores take into both classification probability and predicted mask qualities. The confidence score of the i-th result can be formulated as:</p><formula xml:id="formula_5">s i = p ? i ?average(1 {mi[h,w]&gt;0.5} m i [h, w]) ? ,<label>(6)</label></formula><p>where p i is the most likely class probability of i-th result. m i [h, w] is the mask logit at pixel [h, w], ?, ? are used to balance the weight of classification probability and segmentation qualities.</p><p>As illustrated in Algorithm 1, mask-wise merging strategy takes c, s, and m as input, denoting the predicted categories, confidence scores, and segmentation masks, respectively. It outputs a semantic mask SemMsk and an instance</p><formula xml:id="formula_6">Algorithm 1: Mask-Wise Merging def MaskWiseMergeing(c,s,m): # category c ? R N # confidence score s ? R N # mask m ? R N?H?W SemMsk = np.zeros(H,W) IdMsk = np.zeros(H,W) order = np.argsort(-s) id = 0 for i in order: mi = m[i]&gt;0.5 &amp; (SemMsk==0) if s[i]&lt; t cnf or m i m[i]&gt;0.5 &lt; t keep : continue SemMsk[mi] = c[i] IdMsk[mi] = id id += 1 return SemMsk,IdMsk</formula><p>id mask IdMsk, to assign a category label and an instance id to each pixel. Specifically, SemMsk and IdMsk are first initialized by zeros. Then, we sort prediction results in descending order of confidence score and fill the sorted predicted masks into SemMsk and IdMsk in order. Then we discard the results with confidence scores below t cls and remove the overlaps with lower confidence scores. Only remained non-overlap part with a sufficient fraction t keep to origin mask will be kept. Finally, the category label and unique id of each mask are added to generate non-overlap panoptic format results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We evaluate Panoptic SegFormer on COCO <ref type="bibr" target="#b10">[11]</ref> and ADE20K dataset <ref type="bibr" target="#b32">[33]</ref>, comparing it with several state-ofthe-art methods. We provide the main results of panoptic segmentation and instance segmentation. We also conduct detailed ablation studies to verify the effects of each module. Please refer to Appendix for implementation details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Dataset</head><p>We perform experiments on COCO 2017 datasets <ref type="bibr" target="#b10">[11]</ref> without external data. The COCO dataset contains 118K training images and 5k validation images, and it contains 80 things and 53 stuff. We further demonstrate the generality of our model on the ADE20K dataset <ref type="bibr" target="#b32">[33]</ref>, which contains 100 things and 50 stuff.   Instance segmentation. Panoptic SegFormer can be converted to an instance segmentation model by just discarding stuff queries. In Tab. 4, we report our instance segmentation results on COCO test-dev set. We achieve results comparable to the current state-of-the-art methods such as QueryInst <ref type="bibr" target="#b12">[13]</ref> and HTC <ref type="bibr" target="#b13">[14]</ref>, and 1.8 AP higher than K-Net <ref type="bibr" target="#b3">[4]</ref>. Using random crops during training boosts the AP by 1.3 percentage points.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Main Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation Studies</head><p>First, we show the effect of each module in Tab. 5. Compared to baseline DETR, our model achieves better performance, faster inference speed and significantly reduces the training epochs. We use Panoptic SegFormer (R50) to perform ablation experiments by default. Effect of Location Decoder. Location decoder assists queries to capture the location information of things. Tab. 6 shows the results with varying the number of layers in the location decoder. With fewer location decoder layers, our model performs worse on things, and it demonstrates that learning location clues through the location decoder is beneficial to the model to handle things better. * notes we predict mass centers rather than bounding boxes in our location decoder, and this box-free model achieves comparable results (49.2% PQ vs. 49.6% PQ).</p><p>Mask-wise Merging. As shows in Tab. 7, we compare our mask-wise merging strategy against pixel-wise argmax strategy on various models. We use both Mask PQ and Boundary PQ <ref type="bibr" target="#b40">[41]</ref> to make our conclusions more credible. Models with mask-wise merging strategy always performs better. DETR with mask-wise merging outperforms origin DETR by 1.3% PQ <ref type="bibr" target="#b0">[1]</ref>. In addition, our mask-wise merging is 20% less time-consuming than DETR's pixel-wise  <ref type="figure" target="#fig_6">Figure 4</ref>. While using pixel-wise argmax, the keyboard is covered on the laptop (noted by the red circle in (e). However, the laptop has a higher classification probability than the keyboard. The pixel-wise argmax strategy fails to use this important clue. Masks logits were generated through DETR-R50 <ref type="bibr" target="#b0">[1]</ref>.  <ref type="table" target="#tab_11">Table 7</ref>. Effect of mask-wise merging strategy. The table shows the results of models with different post-processing methods, and the backbone is ResNet-50. "(p)" refers to using pixel-wise argmax as the post-processing method. "(p*)" considers both class probability and mask prediction probability in its pixel-wise argmax strategy <ref type="bibr" target="#b2">[3]</ref>. Models with "(m)" that employ mask-wise merging always perform better in both Mask PQ and Boundary PQ <ref type="bibr" target="#b40">[41]</ref> than pixel-wise argmax method. argmax since DETR uses more tricks in its code, such as merging stuff with the same category and iteratively removing masks with small areas. <ref type="figure" target="#fig_6">Fig. 4</ref> shows one typical fail case of using pixel-wise argmax. Mask Decoder. Our proposed mask decoder converges faster since the ground truth masks guide the attention module to focus on meaningful regions. <ref type="figure">Fig. 5</ref> shows the convergence curves of several models. We only supervise the last layer of the mask decoder while not employing deep supervision. We can observe that our method achieves 49.6% PQ with training for 24 epochs, and longer training has little effect. However, D-DETR-MS needs at least 50 epochs to achieve better performance. Deep supervision is vital for our mask decoder to perform better and converge faster. <ref type="figure">Fig. 6</ref> shows the attention maps of different layers in the mask decoder, and the attention module focuses on the target car in the previous layer when using deep supervision. The attention maps are very similar to the final predicted masks, since masks are generated by attention maps with a lightweight FC head. Since our mask decoder can generate masks from each layer, we evaluate the performance of each layer in the mask decoder, see Tab. 10. During inference, using the first two layers of mask decoder will be on par with the whole mask decoder. It also inferences faster because the computational cost decreases. PQ th is hardly affected by the number of layers, PQ st performs a little poorly in the first layer. The reason is that the location decoder has made additional refinements to the thing queries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Effect of Query Decoupling Strategy. We compare our proposed query decoupling strategy with previous DETR's matching method (described here as "joint matching") <ref type="bibr" target="#b0">[1]</ref>, as shown in Tab. 8. Following DETR, joint matching uses a set of queries to target both things and stuff and feeds all queries to both location decoder and mask decoder. For our proposed query decoupling strategy, we use thing queries to detect things through bipartite matching and use location decoder to refine them. Stuff queries are assigned through class-fixed assign strategy. For a fair  <ref type="table" target="#tab_11">Table 11</ref>. Panoptic segmentation results on COCO-C. To ease the workload of the experiment, we use a subset of 2000 images from the COCO val2017. The third column is the average results on 16 types of corruption data.</p><p>comparison, both the joint matching strategy and our query decoupling strategy employ 353 queries. We can observe that our proposed strategy highly boost PQ st . In addition, panoptic segmentation model can perform instance segmentation by utilizing its thing results only. However, previous panoptic segmentation methods always perform poorly on instance segmentation task even though the two tasks are closely related. Tab. 8 shows both panoptic segmentation and instance segmentation performance of various methods. Our query decoupling strategy can achieve sota performance on panoptic segmentation task while obtaining a competitive instance segmentation performance. In short, query decoupling strategy achieves higher PQ st and AP seg compared to joint matching. We analyze the experimental results of joint matching and find that if one query prefers things more, the precision of stuff results detected by it will be lower, see <ref type="figure" target="#fig_3">Fig. 7</ref>. Each point represents the Thing-Preference and Stuff-Precision corresponding to each query, and the specific definitions are in Appendix. The red line is the linear regression of these points. When using one query set to detect things and stuff together, it will cause interference within each query. Our query decoupling strategy prevents things and stuff from interfering within the same query.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Robustness to Natural Corruptions</head><p>Panoptic segmentation has promising applications in many fields, such as autonomous driving. Model robustness is one of the top concerns of autonomous driving. In this experiment, we evaluate the robustness of our model to disturbed data. We follow <ref type="bibr" target="#b41">[42]</ref> and generate COCO-C, which extends the COCO validation set to include disturbed data generated by 16 algorithms from blur, noise, digital and weather categories. We compare our model to Panoptic FCN <ref type="bibr" target="#b20">[21]</ref>, D-DETR-MS and MaskFormer <ref type="bibr" target="#b2">[3]</ref>. The results are shown in Tab. 11. We calculated the mean results of disturbed data on COCO-C. Using the same backbone, our model always performs better than others. Previ-ous literature <ref type="bibr" target="#b42">[43]</ref><ref type="bibr" target="#b43">[44]</ref><ref type="bibr" target="#b44">[45]</ref> found that transformer-based model has stronger robustness on image classification and semantic segmentation tasks. Our experimental results also show that the transformer-based backbone (Swin-L and PVTv2-B5) can bring better robustness to the model. However, for tasks requiring a more complex pipeline, such as panoptic segmentation, we argue that the design of the task head also plays an important role for the robustness of the model. For example, Panoptic SegFormer (Swin-L) has an average result of 47.2% PQ on COCO-C, outperforming MaskFormer (Swin-L) by 5.5% PQ, higher than their gap (2.9% PQ) on clean data. We posit it is due to our transformer-based mask decoder having stronger robustness than the convolutionbased pixel decoder of MaskFormer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>Limitation. This work relies on deformable attention to process multi-scale features, and the speed is a little slow. Our model is still hard to handle features with a larger spatial shape and does not perform well for small targets.</p><p>Discussion. Recently, the segmentation field attempted to use a uniform pipeline to process various tasks, including semantic segmentation, instance segmentation, and panoptic segmentation. However, we think that complete unification is conceptually exciting but not necessarily a suitable strategy. Given the similarities and differences among the various segmentation tasks, "seek common ground while reserving differences" is a more reasonable guiding ideology. With query decoupling strategy, we can handle things and stuff in the same paradigm since they are represented as queries. In addition, we can also design customized pipelines for things or stuff. Such a flexible strategy is more suitable for various segmentation tasks. At present, taskspecific designs still bring better performance. We encourage the community to further explore the unified segmentation frameworks and expect that Panoptic SegFormer can inspire future works. Our settings mainly follow DETR <ref type="bibr" target="#b0">[1]</ref> and Deformable DETR <ref type="bibr" target="#b11">[12]</ref> for simplicity. The hyper-parameters in deformable attention are the same as Deformable DETR <ref type="bibr" target="#b11">[12]</ref>. We use Channel Mapper <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b46">46]</ref> to map dimensions of the backbone's outputs to 256. The location decoder contains 6 deformable attention layers, and the mask decoder contains 6 vanilla cross-attention layers <ref type="bibr" target="#b23">[24]</ref>. The spatial positional encoding is the commonly used fixed absolute encoding that is the same as DETR. The window size of Swin-L <ref type="bibr" target="#b33">[34]</ref> we used is 7. Since we equally treat each query. ? things and ? stuff are dynamically adjusted according to the relative proportion of things and stuff in each image, and their sum is 1. ? cls , ? seg , and ? det in Eq. (4) are set to 2, 1, 1, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Acknowledge</head><p>During the training phase, the predicted masks that be assigned ? will have a weight of zero in computing L seg . While using the mass center of instance to replace the bounding box, we only use L1 loss to supervise the mass center of predicted mask and mass center of ground truth. We employ a threshold 0.5 to obtain binary masks from soft masks. Threshold t cnf and t keep are 0.25 (0.3) and 0.6, respectively. ? and ? in Eq. (6) are 1 and 2, respectively. All experiments are trained on one NVIDIA DGX node with 8 Tesla V100 GPUs.</p><p>By default, for COCO dataset <ref type="bibr" target="#b10">[11]</ref>, We train our models with 24 epochs, a batch size of 1 per GPU, a learning rate of 1.4?10 ?4 (decayed at the 18th epoch by a factor of 0.1, learning rate multiplier of the backbone is 0.1). We use a multi-scale training strategy with the maximum image-side not exceeding 1333 and the minimum image size varying from 480 to 800, and random crop augmentations is applied during training. The number of thing queries N th is set to 300. Stuff queries have tge equal number of stuff classes, and it is 53 in COCO.</p><p>For the ADE20K dataset <ref type="bibr" target="#b32">[33]</ref>, we train our model with 100 epochs (decayed at 80th epoch), image size varying from 512 to 2048. Since ADE20K contains 50 stuff, we use 50 stuff queries. Other settings are the same to COCO dataset.</p><p>FPS and FLOPs. FPS in Tab.5 is measured on a V100 GPU with a batch size of 1. "DETR" and "DETR+mask wise merging" are from Detectron2 <ref type="bibr" target="#b47">[47]</ref> and DETR's implementation. Others are from Mmdet <ref type="bibr" target="#b46">[46]</ref> and our own implementation. Our framework is slightly more efficient than DETR. FLOPs of DETR are measured from Detectron2 on an average of 100 images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pixel-wise argmax</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mask-wise merging</head><p>Image &amp; GT Inference Results Masks Logits Set  <ref type="figure" target="#fig_12">Figure B.1</ref>. Pixel-wise Argmax vs. Mask-wise Merging. We use DETR-R50 to compare the results generated through pixel-wise armgax and mask-wise merging. Firstly, DETR-R50 detects a cup and a knife from the image. When using pixel-wise argmax, other pixels ( dining table) are incorrectly filled with "cup" or "knife". It mistakenly believes that the largest mask logit is the correct result, regardless of its value. However, our mask-wise merging strategy generates the correct results since we binarize each mask.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Deformable DETR for Panoptic Segmentation</head><p>Following DETR for panoptic segmentation, we transplanted the panoptic head of DETR to Deformable DETR. To ensure consistency, we only generate the attention maps with the spatial shape of 32s. When using single scale deformable DETR, the process of generating attention maps is the same as DETR. When using multi-scale deformable DETR, we only multiply queries and the features (from C5) to generate attention maps. Other settings of deformable DETR for object detection are kept unchanged. We apply iterative bounding box refinement as the default setting for Deformable DETR. We use 300 queries and this brings huge computation costs, although this model achieves pretty good performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Discussion</head><p>We will deliver more ablation studies, more detailed analysis in this section.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1. Post-processing Method</head><p>Defects of Pixel-wise Argmax. Pixel-wise argmax only considers the mask logits of each pixel. It has multiple issues that may lead to incorrect results. First of all, the <ref type="figure">Figure B</ref>.2. The joint distribution for classification probability and segmentation score. We can observe that segmentation scores can be high while the masks have low classification probability.  <ref type="table" target="#tab_11">Table B</ref>.3. The weights of the classification score and segmentation score determine the priority of masks. We can observe that employing both of them will perform better. According to the results on multiple models, we choose ? = 1, ? = 2 as our default setting.</p><p>pixel value generated from argmax may be extremely small, as shown in <ref type="figure" target="#fig_12">Fig. B.1</ref>, which will generate plenty of falsepositive results. The second issue is that the pixel with max mask logit may be the suboptimal result, as shown in <ref type="figure" target="#fig_6">Fig. 4</ref> of the paper. This kind of error frequently appears in the segmentation maps generated by pixle-wise argmax. Mask-Former <ref type="bibr" target="#b2">[3]</ref> alleviates this problem by multiplying the classification probability by the masks logits. But this kind of error will still exist. Heuristic Procedure. The heuristic procedure <ref type="bibr" target="#b5">[6]</ref> was the first proposed post-processing method of panoptic segmentation. It uses different strategies to handle things and stuff separately. Pixel-wise argmax was still used in its stuff workflow. One apparent defect of this method is that it solves the overlap problem of stuff and things by always preferring things. This is an unfair way of dealing with stuff. Tab. B.2 shows that PQ st of using heuristic procedure is lower than other methods because all stuff are treated unfairly.</p><p>Masks-wise Merging. Post-processing of panoptic segmentation aims to solve the overlap problem between masks. Although pixel-wise argmax uses an intuitive method to solve the overlap problem, it has defects mentioned above. We solve the overlap problem by giving different masks different priorities. Mask-wise merging guarantees that high-quality masks have higher priority by sorting the masks with confidence scores. This strategy ensures that low-quality instances will not cover highquality instances. In order to be able to effectively distinguish the quality of the masks, we consider both classification probability and segmentation score as the confidence score of each mask. The segmentation score average(1 {mi[h,w]&gt;0.5} m i [h, w]) represents the confidence of the overall segmentation quality of the mask. Tab. B.3 shows the results of varying ? and ? in Eq.6. Applying both classification probability and segmentation scores always have better performance. <ref type="figure">Fig. B.2</ref> shows the relationship of classification probability and segmentation score. While one mask has a low classification probability ([0, 0.4]), it may have a large segmentation score. Large segmentation score means it has many pixels with high logits and this may generate false-positive results through pixel-wise argmax since its classification probability is pretty low.</p><p>Our mask-wise merging needs two thresholds to filter out undesirable results. Tab. B.4 shows that our algorithm is not very dependent on the choice of threshold t cnf and t keep .  <ref type="table" target="#tab_11">Table B</ref>.6. We divide 353 queries into ten groups according to their P t . For each group, we calculate their precision on stuff and things. Queries with higher P t have very low precision when they predict stuff. This demonstrates that things may interfere with the prediction of stuff and using queries to target both things and stuff is suboptimal. Although our proposed mask-wise merging strategy has achieved better results than other post-processing methods, it also has several shortcomings. First of all, we binaries the mask through a fixed threshold. This may cause one pixel to be easily assigned a void label because the values of   <ref type="figure">Figure B</ref>.6. Visualization of multi-head attention maps and corresponding outputs from mask decoder. Different heads have different preferences. Head 4 and Head 1 pay attention to foreground regions, and Head 8 prefers regions that occlude foreground. Head 5 always pays attention to the background that is around the foreground. Through the collaboration of these heads, Panoptic SegFormer can predict accurate masks. The 3rd row shows an impressive result of a horse that is highly obscured by the other horse.  <ref type="figure">Figure B</ref>.7. The Joint Distribution for Queries and Category in DETR. We can observe that queries prefer either things or stuff. Although a few queries predict most of the stuff results ( within the red box), other queries still generate a considerable proportion of stuff results (within the yellow box). Our experimental results demonstrate that the results in the yellow box are usually of low quality. We sort the query ids for better visualization. Other literature <ref type="bibr" target="#b1">[2]</ref> reports similar phenomenon. confidence scores. If the confidence scores are not accurate, it will produce a low-quality panoptic format mask.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2. Location Decoder</head><p>Although we use the location decoder to detect the bounding boxes of things, our workflow is still very different from the previous box-based panoptic segmentation. For example, Panoptic FPN performs instance segmentation with Mask R-CNN style. The two-stage method usually needs to extract regions from the feature based on the bboxes and then use these regions to perform segmentation. The quality of segmentation is heavily dependent on the quality of detection. However, our location decoder is used to assist in learning the location clues of the query and distinguishing different instances. Mask will not have the wrong boundary due to the wrong boundary prediction of the bbox since the bbox does not constrain the mask. We also show that using mass centers of masks to replace bboxes can still learn location clues.</p><p>Another valuable function of the location decoder is to help filter out low-quality thing queries during the training and inference phase. This can greatly save memory. Current transformer-based panoptic segmentation methods always consume a lot of GPU memory. For example, MaskFormer takes up more than 20G of GPU memory with a batch size   of 1 and R50 backbone. Although these methods have achieved excellent results, they also require high hardware resources. However, our Panoptic SegFormer can be trained with taking up less than 12G memory by using a location decoder to filter out low-quality thing queries. In particular, we use bipartite matching for multiple rounds of matching in the detection phase. The thing query that already be matched will not participate in the next round of matching. After several rounds, we can select partial promising thing queries. Only these promising thing queries will be fed to the mask decoder. with this strategy, the mask decoder usually only needs to handle less than 100 thing and stuff queries.  <ref type="figure" target="#fig_6">Fig. B.4</ref> shows the model architecture of our mask decoder. <ref type="figure">Fig. B</ref>.5 shows the process of converting multi-scale multihead attention maps to mask. We found that discarding the self-attention in the decoder does not affect the effectiveness of the model. The computational cost of our mask decoder is around 30G FLOPs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3. Mask Decoder</head><p>Multi-head attention maps. <ref type="figure">Fig. B.6</ref> shows some samples of multi-head attention maps. Through a multi-head attention mechanism, different heads of one query learn their own attention preference. We observe that some heads pay attention to foreground regions, some prefer boundaries, and others prefer background regions. This shows that each mask is generated by considering various comprehensive information in the image. Tab. B.5 shows that utilizing a multi-head attention mechanism will outperform single-head attention by 0.4% PQ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.4. Advantage of Query Decoupling Strategy</head><p>DETR uses the same recipe to predict boxes of things and stuff (To facilitate the distinction between the query decoupling strategy we proposed, we refer to the DETR's strategy as a joint matching strategy.). However, detecting bboxes for stuff like DETR is suboptimal. We counted the ratio of the area of masks to the area of bboxes on the COCO train2017. The ratios of things and stuff are 52.5% and 9.2%, separately. This shows that bounding boxes can not represent stuff well since the stuff is amorphous and dispersed. We also observe that bbox AP of DETR drops from 42.0 to 38.8 after training on panoptic segmentation. This may be due to the interference of stuff on things, since predicting stuff bboxes needs to re-adapt the model. <ref type="figure" target="#fig_3">Fig. B.7</ref> shows that DETR seems to learn automatic segregation between things and stuff, and each query either prefers things or stuff. However, we argue that this automatically learned segregation is not ideal. If one query prefers things, it will perform poorly when it generates stuff results. This situation is very common, and our following experiments based on Panoptic SegFormer will give detailed data. Following DETR, we use 353 queries to predict things and stuff with the same recipe. Specifically, the input of the location decoder is 353 queries, which will detect both things and stuff. The refined queries are fed to the mask decoder to predict category labels and masks. We define a query's preference for things as P t , which can be calculated by:</p><formula xml:id="formula_7">P i t = N i things /(N i things + N i stuff ),<label>(7)</label></formula><p>where N things and N stuff are the number of things and stuff masks that i-th query predicted on COCO val set. P i t &gt; 0.5 means that i-th query prefers things more than stuff. The predicted mask is a true positive (TP) if IoU between it and one ground truth mask is larger than 0.5 and the category of them is the same. Then we can calculate the precision of queries' predicted masks. Tab. B.6 shows relevant statistical results. First, we can observe that the queries that own lower P t basically have higher precision. The stuff precision of the queries that have the highest P t ([0.9, 1.0]) only is 0.30, which is much lower than the average stuff precision (0.60) on all queries. These erroneous results are mainly due to errors in the predicted category. Queries that have no obvious preference for stuff and things( P t in [0.4, 0.6) ) performs poorly both on stuff and things. These results demonstrate that using one query set to predict things and stuff simultaneously is flawed. This joint matching strategy is suboptimal for stuff.</p><p>In order to avoid mutual interference between stuff and things, we propose the query decoupling strategy to handle things and queries with a separate query set. Compared to stuff query, thing query will go through an additional location decoder. However, all queries will produce the outputs in the same format. Things and stuff use the same loss for training, except that things use an additional detection loss. During inference, we can use our mask-wise merging strategy to merge them uniformly. This is different from the previous methods that modeled panoptic segmentation into instance segmentation and semantic segmentation. For example, Panoptic FPN uses one branch to generate things and one branch to generate stuff. The things and stuff generated by Panoptic FPN are in different formats and need different training strategies and post-processing methods. PQ st with query decoupling outperforms joint matching strategy by 2.9% PQ and experimental results verify the effectiveness of our method. The stuff precision by using query decoupling is 0.66, better than the joint matching strategy.</p><p>C. Visualization <ref type="figure">Fig. B.8</ref> shows our visualization result against DETR and MaskFormer. We use the original codes that they officially implemented. First of all, compared with other methods, we can observe that our results are more consistent with ground truths. Due to the defects of pixel-wise argmax we discussed in Appendix B.1, DETR always generates results with artifacts. MaskFormer performs better because they improved pixel-wise argmax by considering classification probabilities. However, it may still fail in hard cases. For example, it recognizes the billboard as a car in the fourth row. <ref type="figure" target="#fig_9">Fig. B.9</ref> shows some failure cases of our model. Firstly, our model may have lower recall when facing crowded scenarios filled with the same things, especially for the small targets. Another typical failure mode is that large stuff with a high confidence score occupies most of the space, causing other things not to be added to the canvas. <ref type="figure" target="#fig_10">Fig. B.10</ref> shows the results on some complex scenes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Various Backbones</head><p>We give all the panoptic segmentation results under various backbones, as shown in Tab. D.1. <ref type="figure" target="#fig_12">Fig. D.1</ref> shows two training curves with backbone ResNet-101 and Swin-L. With Swin-L, Panoptic SegFormer with training for 24 epochs even performs better than training for 50 epochs.   <ref type="bibr" target="#b22">[23]</ref> and Swin-L as the backbone, we train our model for 24 epochs and 50 epochs, separately. We can observe that our model that training for 24 epochs can achieve comparable or even higher results while comparing the models that training for 50 epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Code and Data</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>PQ (%) #Param (M) DETR-R50 [1]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 .Figure 6 .</head><label>56</label><figDesc>Convergence curves of Panoptic SegFormer and D-DETR-MS. We train models with different training schedules. "w/o ds" refers that we do not employ deep supervision in the mask decoder. The learning rate is reduced where the curves leap. Attention maps of different layers in the mask decoder. "ds" refers to deep supervision.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 7 .</head><label>7</label><figDesc>Things-Preference vs. Stuff-Precision.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>This work is supported by the Natural Science Foundation of China under Grant 61672273 and Grant 61832008. Ping Luo is supported by the General Research Fund of HK No.27208720 and 17212120. Wenhai Wang and Tong Lu are corresponding authors. Appendix A. Implementation Details A.1. Panoptic SegFormer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure B. 4 .</head><label>4</label><figDesc>Architecture of mask decoder. Attn-Maps notes attention maps.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure B. 9 .</head><label>9</label><figDesc>Failure case of Panoptic SegFormer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure B. 10 .</head><label>10</label><figDesc>Visualization results of some complex scenes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. B. 3</head><label>3</label><figDesc>shows the architecture of DETR's panoptic head. Although it only contains 1.2M parameters, it has a huge computational burden (about 150G FLOPs). DETR adds ResNet features to each attention map, and this process repeats 100 times since there are 100 attention maps.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure D. 1 .</head><label>1</label><figDesc>By using ResNet-101</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 .Table 2 .</head><label>12</label><figDesc>Panoptic segmentation. We conduct experiments on COCO val set and test-dev set. In Tab. 1 and Tab. 2, we report our main results, comparing with other state-ofthe-art methods. Panoptic SegFormer attains 49.6% PQ on COCO val with ResNet-50 as the backbone and singlescale input, and it surpasses previous methods K-Net [4] Method Backbone Epochs PQ PQ th PQ st #P #F Experiments on COCO val set. #P and #F indicate number of parameters (M) and number of FLOPs (G). Panoptic Seg-Former (R50) achieves 49.6% PQ on COCO val, surpassing previous methods such as DETR [1] and MaskFormer [3] over 6.2% PQ and 3.1% PQ respectively. ? notes that backbones are pretrained on ImageNet-22K. Panoptic SegFormer PVTv2-B5 ? 55.8 61.9 46.5 83.0 66.5 Experiments on COCO test-dev set. ? notes that backbones are pre-trained on ImageNet-22K.</figDesc><table><row><cell>Panoptic FPN [7]</cell><cell>R50</cell><cell>36</cell><cell cols="3">41.5 48.5 31.1</cell><cell>-</cell><cell>-</cell></row><row><cell>SOLOv2 [28]</cell><cell>R50</cell><cell>36</cell><cell cols="3">42.1 49.6 30.7</cell><cell>-</cell><cell>-</cell></row><row><cell>DETR [1]</cell><cell>R50</cell><cell cols="6">325 43.4 48.2 36.3 42.9 248</cell></row><row><cell>Panoptic FCN [21]</cell><cell>R50</cell><cell>36</cell><cell cols="5">43.6 49.3 35.0 37.0 244</cell></row><row><cell>K-Net [4]</cell><cell>R50</cell><cell>36</cell><cell cols="3">47.1 51.7 40.3</cell><cell>-</cell><cell>-</cell></row><row><cell>MaskFormer [3]</cell><cell>R50</cell><cell cols="6">300 46.5 51.0 39.8 45.0 181</cell></row><row><cell>Panoptic SegFormer</cell><cell>R50</cell><cell>12</cell><cell cols="5">48.0 52.3 41.5 51.0 214</cell></row><row><cell>Panoptic SegFormer</cell><cell>R50</cell><cell>24</cell><cell cols="5">49.6 54.4 42.4 51.0 214</cell></row><row><cell>DETR [1]</cell><cell>R101</cell><cell cols="6">325 45.1 50.5 37.0 61.8 306</cell></row><row><cell cols="2">Max-Deeplab-S [2] Max-S [2]</cell><cell>54</cell><cell cols="5">48.4 53.0 41.5 61.9 162</cell></row><row><cell>MaskFormer [3]</cell><cell>R101</cell><cell cols="6">300 47.6 52.5 40.3 64.0 248</cell></row><row><cell>Panoptic SegFormer</cell><cell>R101</cell><cell>24</cell><cell cols="5">50.6 55.5 43.2 69.9 286</cell></row><row><cell cols="2">Max-Deeplab-L [2] Max-L [2]</cell><cell>54</cell><cell cols="5">51.1 57.0 42.2 451.0 1846</cell></row><row><cell>Panoptic FCN [36]</cell><cell>Swin-L  ?</cell><cell>36</cell><cell cols="3">51.8 58.6 41.6</cell><cell>-</cell><cell>-</cell></row><row><cell>MaskFormer [3]</cell><cell>Swin-L  ?</cell><cell cols="6">300 52.7 58.5 44.0 212.0 792</cell></row><row><cell>K-Net [4]</cell><cell>Swin-L  ?</cell><cell>36</cell><cell cols="5">54.6 60.2 46.0 208.9 -</cell></row><row><cell cols="2">Panoptic SegFormer Swin-L  ?</cell><cell>24</cell><cell cols="5">55.8 61.7 46.9 221.4 816</cell></row><row><cell cols="2">Panoptic SegFormer PVTv2-B5  ?</cell><cell>24</cell><cell cols="5">55.4 61.2 46.6 104.9 349</cell></row><row><cell>Method</cell><cell>Backbone</cell><cell></cell><cell cols="5">PQ PQ th PQ st SQ RQ</cell></row><row><cell>Max-Deeplab-L [2]</cell><cell cols="7">Max-L [2] 51.3 57.2 42.4 82.5 61.3</cell></row><row><cell>Innovation [35]</cell><cell>ensemble</cell><cell></cell><cell cols="5">53.5 61.8 41.1 83.4 63.3</cell></row><row><cell>MaskFormer [3]</cell><cell>Swin-L  ?</cell><cell></cell><cell cols="5">53.3 59.1 44.5 82.0 64.1</cell></row><row><cell>K-Net [4]</cell><cell>Swin-L  ?</cell><cell></cell><cell cols="5">55.2 61.2 46.2 82.4 66.1</cell></row><row><cell>Panoptic SegFormer</cell><cell>R50</cell><cell></cell><cell cols="5">50.2 55.3 42.4 81.9 60.4</cell></row><row><cell>Panoptic SegFormer</cell><cell>R101</cell><cell></cell><cell cols="5">50.9 56.2 43.0 82.0 61.2</cell></row><row><cell>Panoptic SegFormer</cell><cell>Swin-L  ?</cell><cell></cell><cell cols="5">56.2 62.3 47.0 82.8 67.1</cell></row><row><cell>Method</cell><cell cols="2">Backbone</cell><cell cols="5">PQ PQ th PQ st SQ RQ</cell></row><row><cell>BGRNet [37]</cell><cell>R50</cell><cell></cell><cell>31.8</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="4">Auto-Panoptic [38] ShuffleNetV2 [39] 32.4</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>MaskFormer [3]</cell><cell>R50</cell><cell></cell><cell cols="5">34.7 32.2 39.7 76.7 42.8</cell></row><row><cell>MaskFormer [3]</cell><cell>R101</cell><cell></cell><cell cols="5">35.7 34.5 38.0 77.4 43.8</cell></row><row><cell>Panoptic SegFormer</cell><cell>R50</cell><cell></cell><cell cols="5">36.4 35.3 38.6 78.0 44.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .Table 4 .</head><label>34</label><figDesc>Panoptic segmentation results on ADE20K val set. and DETR [1] over 2.5% PQ and 6.2% PQ, respectively. Except for the remarkable performance, the training of Panoptic SegFormer is efficient. Under 1? training strategy (12 epochs), Panoptic SegFormer (R50) achieves 48.0% PQ that outperforms MaskFormer [3] that training 300 epochs by 1.5% PQ. Enhanced by vision transformer backbone Swin-L [34], Panoptic SegFormer attains a new record of 56.2% PQ on COCO test-dev without bells and whistles, surpassing MaskFormer [3] over 2.9% PQ. Our method even surpasses the previous competition-level method In-Instance segmentation on COCO test-dev set.</figDesc><table><row><cell>S</cell><cell>AP seg M</cell><cell>AP seg L</cell></row></table><note>novation [35] over 2.7 % PQ. We also obtain comparable performance by employing PVTv2-B5 [5], while the model parameters and FLOPs are reduced significantly compared to Swin-L. Panoptic SegFormer also outperforms Mask- Former by 1.7% PQ on ADE20K dataset [33], see Tab. 3.Method Backbone AP seg AP seg</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 .</head><label>5</label><figDesc>We increase the panoptic segmentation performance of DETR<ref type="bibr" target="#b0">[1]</ref> (R50<ref type="bibr" target="#b22">[23]</ref>) from 43.4% PQ to 49.6% PQ with fewer training epochs, less computation cost, and faster inference speed.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6</head><label>6</label><figDesc></figDesc><table><row><cell>#Layer</cell><cell>PQ</cell><cell cols="2">PQ th PQ st</cell></row><row><cell>0</cell><cell>47.0</cell><cell>50.0</cell><cell>42.5</cell></row><row><cell>1</cell><cell>47.7</cell><cell>51.1</cell><cell>42.5</cell></row><row><cell>2</cell><cell>48.1</cell><cell>51.8</cell><cell>42.5</cell></row><row><cell cols="2">6* (box-free) 49.2</cell><cell>53.5</cell><cell>42.6</cell></row><row><cell>6</cell><cell>49.6</cell><cell>54.4</cell><cell>42.4</cell></row></table><note>. Ablate location decoder.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 8 .</head><label>8</label><figDesc>PQ PQ th PQ st AP box AP seg Effect of query decoupling strategy. PQ and AP scores of various panoptic segmentation models on COCO val2017.</figDesc><table><row><cell>DETR [1]</cell><cell>43.4 48.2 36.3</cell><cell>38.8</cell><cell>31.1</cell></row><row><cell>D-DETR-MS [12]</cell><cell>47.3 52.6 39.0</cell><cell>45.3</cell><cell>37.6</cell></row><row><cell>Panoptic FCN [21]</cell><cell>43.6 49.3 35.0</cell><cell>36.6</cell><cell>34.5</cell></row><row><cell>Ours (Joint Matching)</cell><cell>48.5 54.5 39.5</cell><cell>44.1</cell><cell>37.7</cell></row><row><cell cols="2">Ours (Query Decoupling) 49.6 54.4 42.4</cell><cell>45.6</cell><cell>39.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>Glass Gauss Gauss Impul Shot Speck Bright Contr Satur JPEG Snow Spatt Fog Frost Panoptic FCN (R50) 43.8 26.8 22.5 23.7 14.1 25.0 28.2 20.0 28.3 31.9 39.4 24.3 38.0 22.9 20.0 29.6 35.3 25.3 MaskFormer (R50) 47.0 29.5 24.9 28.1 16.4 29.5 31.2 24.7 30.9 34.8 42.5 27.5 41.2 22.0 20.4 31.0 38.5 27.7 D-DETR (R50) 47.6 30.3 25.6 28.7 16.8 29.7 32.5 24.9 31.4 35.9 43.1 28.6 41.3 24.5 21.7 31.7 39.7 28.7 Ours (R50) 50.0 32.9 26.9 30.2 17.5 31.6 35.5 27.9 35.4 38.6 45.7 31.3 43.9 29.0 24.3 35.0 41.9 32.3</figDesc><table><row><cell cols="4">Method Motion Defoc MaskFormer (Swin-L) 52.9 41.7 Blur Clean Mean 37.3 38.0 30.4 39.3 42.3 42.5 42.8 45.3 49.7 43.9 49.4 39.7 35.2 45.2 48.8 37.9 Noise Digital Weather</cell></row><row><cell>Ours (Swin-L)</cell><cell>55.8 47.2</cell><cell>41.3</cell><cell>41.5 34.3 42.7 48.6 49.5 48.8 50.3 53.8 50.1 53.5 46.9 44.8 51.5 53.3 44.3</cell></row><row><cell>Ours (PVTv2-B5)</cell><cell>55.6 47.0</cell><cell>41.5</cell><cell>41.1 36.1 42.5 48.4 49.6 48.4 50.4 53.5 50.8 53.0 46.2 42.4 50.3 52.9 44.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table B</head><label>B</label><figDesc>For more implementation details, please refer to the Appendix A. As shown in Tab. B.1, multi-scale deformable attention improves 2.9% PQ compared to DETR. Multi-scale attention outperforms single-scale attention by 5.7% PQ, highlighting the important role of multi-scale features for segmentation task.</figDesc><table><row><cell>Effect of Deformation</cell><cell></cell><cell></cell></row><row><cell>Attention. To ablate the</cell><cell></cell><cell></cell></row><row><cell>effect of deformable atten-</cell><cell></cell><cell></cell></row><row><cell>tion, we extend Deformable</cell><cell>.1.</cell><cell>"D-", "SS"</cell></row><row><cell>DETR on panoptic segmen-</cell><cell cols="2">and "MS" refers to "De-</cell></row><row><cell>tation with the panoptic head</cell><cell cols="2">formable", single-scale and</cell></row><row><cell>of DETR.</cell><cell>multi-scale.</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head></head><label></label><figDesc>Comparing visualization results of Panoptic SegFormer with other methods on the COCO val set. For a fair comparison, all results are generated with ResNet-101<ref type="bibr" target="#b22">[23]</ref> backbone. The second and fourth row results show that our method still performs well in highly crowded or occluded scenes. Benefits from our mask-wise inference strategy, our results have few artifacts, which often appear in the results of DETR<ref type="bibr" target="#b0">[1]</ref> (e.g., dining table of the third row).</figDesc><table><row><cell>Original Image</cell><cell>Ours</cell><cell>DETR [1]</cell><cell>MaskFormer [3]</cell><cell>Ground Truth</cell></row><row><cell></cell><cell>50.6% PQ</cell><cell>45.1% PQ</cell><cell>47.6% PQ</cell><cell></cell></row><row><cell>Figure B.8.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head></head><label></label><figDesc>Table D.1. Panoptic segmentation results on COCO val with various backbones.</figDesc><table><row><cell>Backbone</cell><cell>PQ</cell><cell>SQ</cell><cell>RQ</cell><cell>PQ th</cell><cell>SQ th</cell><cell>RQ th</cell><cell>PQ st</cell><cell>SQ st</cell><cell>RQ st</cell></row><row><cell>ResNet-50 [23]</cell><cell>49.6</cell><cell>81.6</cell><cell>59.9</cell><cell>54.4</cell><cell>82.7</cell><cell>65.1</cell><cell>42.4</cell><cell>79.9</cell><cell>52.1</cell></row><row><cell>ResNet-101 [23]</cell><cell>50.6</cell><cell>81.9</cell><cell>60.9</cell><cell>55.5</cell><cell>83.0</cell><cell>66.3</cell><cell>43.2</cell><cell>80.1</cell><cell>52.9</cell></row><row><cell>PVTv2-B0 [5]</cell><cell>49.5</cell><cell>82.4</cell><cell>59.2</cell><cell>55.3</cell><cell>83.3</cell><cell>65.8</cell><cell>40.6</cell><cell>80.9</cell><cell>49.2</cell></row><row><cell>PVTv2-B2 [5]</cell><cell>52.5</cell><cell>82.7</cell><cell>62.7</cell><cell>58.5</cell><cell>83.6</cell><cell>69.5</cell><cell>43.4</cell><cell>81.4</cell><cell>52.4</cell></row><row><cell>PVTv2-B5 [5]</cell><cell>55.4</cell><cell>82.9</cell><cell>66.1</cell><cell>61.2</cell><cell>84.0</cell><cell>72.4</cell><cell>46.6</cell><cell>81.3</cell><cell>56.5</cell></row><row><cell>Swin-L [34]</cell><cell>55.8</cell><cell>82.6</cell><cell>66.8</cell><cell>61.7</cell><cell>83.7</cell><cell>73.3</cell><cell>46.9</cell><cell>80.9</cell><cell>57.0</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We use the official implementations of DETR 1 , Mask-Former 2 , Panoptic FCN 3 to perform additional experiments. The models they provide all can reproduce the same scores they reported in their literature. Deformable DETR is from Mmdet 4 .</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">End-toend object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Max-deeplab: End-to-end panoptic segmentation with mask transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huiyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Per-pixel classification is not all you need for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Towards unified image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangmiao</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen Change Loy. K-Net</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enze</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng-Ping</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaitao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ding</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.13797</idno>
		<title level="m">Pvtv2: Improved baselines with pyramid vision transformer</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Panoptic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr Doll?r. Panoptic feature pyramid networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Soft-nms-improving object detection with one line of code</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navaneeth</forename><surname>Bodla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharat</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rama</forename><surname>Chellappa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5561" to="5569" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Upsnet: A unified panoptic segmentation network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuwen</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renjie</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ersin</forename><surname>Yumer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Kaiming He, Bharath Hariharan, and Serge Belongie. Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deformable detr: Deformable transformers for end-to-end object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xizhou</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijie</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lewei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Instances as queries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shusheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyu</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)<address><addrLine>Oc</addrLine></address></meeting>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note>tober 2021</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Hybrid task cascade for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangmiao</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxiao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuyang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wansen</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Towards bounding-box free panoptic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ujwal</forename><surname>Bonde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><forename type="middle">F</forename><surname>Alcantarilla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Leutenegger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">DAGM German Conference on Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Unifying training and inference for panoptic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qizhu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip Hs</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="13320" to="13328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Panoptic-deeplab: A simple, strong, and fast baseline for bottom-up panoptic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Maxwell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tien-Ju</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Maxwell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jyh-Jing</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vivienne</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Sze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.05093</idno>
		<title level="m">Deeperlab: Single-shot image parser</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">SSAP: Single-shot instance segmentation with affinity pyramid</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naiyu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanhu</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yupei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiqi</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Attention-guided unified network for panoptic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinze</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingxi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dalong</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for panoptic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Conditional convolutions for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV, 2020</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Solq: Segmenting objects by learning queries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangao</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiancai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr Doll?r. Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">SOLOv2: Dynamic and fast instance segmentation. NeurIPS</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rufeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Solo: Segmenting objects by locations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuning</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">End-to-end people detection in crowded scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Russell</forename><surname>Stewart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhaylo</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">The hungarian method for the assignment problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Harold W Kuhn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Naval research logistics quarterly</title>
		<imprint>
			<date type="published" when="1955" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="83" to="97" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">V-net: Fully convolutional neural networks for volumetric medical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fausto</forename><surname>Milletari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nassir</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seyed-Ahmad</forename><surname>Ahmadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on 3D vision (3DV)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Scene parsing through ade20k dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Puig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adela</forename><surname>Barriuso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Swin transformer: Hierarchical vision transformer using shifted windows. ICCV</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baining</forename><surname>Guo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Joint coco and mapillary workshop at iccv 2019: Coco panoptic segmentation challenge track technical report: Panoptic htc with class-guided fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chongsong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daisheng</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongang</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cunjun</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bairun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinyi</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SHR</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="page" from="67" to="69" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Fully convolutional networks for panoptic segmentation with point-based supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.07682</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Bidirectional graph reasoning network for panoptic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gengwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiajun</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9080" to="9089" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Auto-panoptic: Cooperative multi-component architecture search for panoptic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gengwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Shufflenet v2: Practical guidelines for efficient cnn architecture design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ningning</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai-Tao</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="116" to="131" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Piotr Doll?r, and Ross Girshick. Mask R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Boundary iou: Improving object-centric image segmentation evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Alexander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kirillov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Benchmarking the robustness of semantic segmentation models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Kamann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Rother</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="8828" to="8838" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Segformer: Simple and efficient design for semantic segmentation with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enze</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiding</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anima</forename><surname>Anandkumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Luo</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Understanding robustness of transformers for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srinadh</forename><surname>Bhojanapalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ayan</forename><surname>Chakrabarti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Glasner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daliang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Veit</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.14586</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muzammal</forename><surname>Naseer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kanchana</forename><surname>Ranasinghe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salman</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Munawar</forename><surname>Hayat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fahad</forename><surname>Shahbaz Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Intriguing properties of vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename></persName>
		</author>
		<idno type="arXiv">arXiv:2105.10497</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangmiao</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxiao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuyang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wansen</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiarui</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dazhi</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenchen</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianheng</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qijie</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Buyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.07155</idno>
		<title level="m">MMDetection: Open MMLab detection toolbox and benchmark</title>
		<editor>Jifeng Dai, Jingdong Wang, Jianping Shi, Wanli Ouyang, Chen Change Loy, and Dahua Lin</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wan-Yen</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Detectron2</surname></persName>
		</author>
		<idno>2019. 1</idno>
		<ptr target="https://github.com/facebookresearch/detectron2" />
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
