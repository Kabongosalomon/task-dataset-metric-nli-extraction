<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Estimating and Exploiting the Aleatoric Uncertainty in Surface Normal Estimation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gwangbin</forename><surname>Bae</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Cambridge</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ignas</forename><surname>Budvytis</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Cambridge</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Cipolla</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Cambridge</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Estimating and Exploiting the Aleatoric Uncertainty in Surface Normal Estimation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T10:56+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Surface normal estimation from a single image is an important task in 3D scene understanding. In this paper, we address two limitations shared by the existing methods: the inability to estimate the aleatoric uncertainty and lack of detail in the prediction. The proposed network estimates the per-pixel surface normal probability distribution. We introduce a new parameterization for the distribution, such that its negative log-likelihood is the angular loss with learned attenuation. The expected value of the angular error is then used as a measure of the aleatoric uncertainty. We also present a novel decoder framework where pixel-wise multi-layer perceptrons are trained on a subset of pixels sampled based on the estimated uncertainty. The proposed uncertainty-guided sampling prevents the bias in training towards large planar surfaces and improves the quality of prediction, especially near object boundaries and on small structures. Experimental results show that the proposed method outperforms the state-of-the-art in Scan-Net [4] and NYUv2 <ref type="bibr" target="#b32">[33]</ref>, and that the estimated uncertainty correlates well with the prediction error. Code is available at https://github.com/baegwangbin/ surface_normal_uncertainty.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Image TiltedSN</head><p>Ours -Surface Normal Ours -Uncertainty 60?0?F igure 1. Comparison between our method and TiltedSN <ref type="bibr" target="#b5">[6]</ref>. The proposed network estimates the surface normal probability distribution, from which the expected angular error can be inferred. The prediction made by our method shows clearer object boundaries and preserves a higher level of detail. This is due to the proposed uncertainty-guided sampling which prevents the bias in training towards large planar surfaces. depth noise and to the algorithm used to compute the normal (see <ref type="figure">Fig. 2</ref> for examples of inaccurate ground truth). The network should be able to capture such aleatoric uncertainty, in order to be deployed in real-world applications.</p><p>(2) Lack of detail in the prediction. An indoor scene generally consists of large planar surfaces (e.g., walls and floors) and small objects with complex geometry. Therefore, if the training loss is applied to all pixels, the learning becomes biased to large surfaces, resulting in an oversmoothed output. Such bias can be solved by applying the loss on a carefully selected subset of pixels. For example, in <ref type="bibr" target="#b39">[40]</ref>, pair-wise ranking loss was applied to the pixels near instance boundaries to improve the quality of monocular depth estimation. However, such effort has not been made for surface normal estimation.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The ability to estimate surface normal from a single RGB image plays a crucial role in understanding the 3D scene geometry. The estimated normal can be used to build augmented reality (AR) applications <ref type="bibr" target="#b17">[18]</ref> or to control autonomous robots <ref type="bibr" target="#b40">[41]</ref>. In this work, we address two limitations shared by the state-of-the-art methods.</p><p>(1) Inability to estimate the aleatoric uncertainty. Stateof-the-art learning-based approaches <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b37">38]</ref> train deep networks by minimizing some distance metric (e.g., L 2 ) between the predicted normal and the ground truth. However, the ground truth normal, calculated from a measured depth map, can be sensitive to the <ref type="bibr">Figure 2</ref>. Ground truth surface normal of NYUv2 <ref type="bibr" target="#b32">[33]</ref>, generated by Ladicky et al. <ref type="bibr" target="#b21">[22]</ref>. The ground truth is unreliable especially near object boundaries and on small structures.</p><p>In this work, we estimate the aleatoric uncertainty by predicting the probability distribution of the per-pixel surface normal. While the von Mises-Fisher distribution <ref type="bibr" target="#b7">[8]</ref> can be used for this purpose, minimizing its negative loglikelihood (NLL) is equivalent to minimizing the L 2 distance between the predicted normal and the ground truth with learned loss attenuation. As the error metric of our interest is the angle between the two vectors, we introduce a new parameterization for the distribution such that its NLL is the angular loss with learned attenuation. At test time, the expected angular error is calculated from the estimated distribution, and used as a measure of the aleatoric uncertainty.</p><p>We also propose a novel decoder framework to improve the level of detail in the prediction. The network initially makes a coarse prediction for which the training loss is applied to all pixels. Then, the coarse prediction and featuremap are bilinearly upsampled by a factor of 2, and are passed through a pixel-wise multi-layer perceptron (MLP) to yield a refined output. This process is repeated until reaching the desired resolution. The MLPs are trained on a subset of pixels selected based on the uncertainty: Pixels with the highest uncertainty are selected and are complemented with uniformly sampled pixels. Such uncertaintyguided sampling prevents the bias in training towards large planar surfaces (for which the network estimates low uncertainty), thereby improving the quality of prediction near object boundaries and on small structures.</p><p>Our contributions can be summarized as follows:</p><p>? Estimation of the aleatoric uncertainty in surface normal. To the best of our knowledge, we are the first to estimate the aleatoric uncertainty in CNN-based surface normal estimation. We introduce a new parameterization for the surface normal probability distribution and show that the estimated uncertainty correlates well with the prediction error.</p><p>? Uncertainty-guided sampling for pixel-wise refinement. We introduce a novel decoder module where the loss is applied to a subset of pixels selected based on the uncertainty. We show that this module significantly improves the quantitative and qualitative performance.</p><p>? State-of-the-art performance. Experimental results show that the proposed method achieves state-of-theart performance on ScanNet <ref type="bibr" target="#b3">[4]</ref> and NYUv2 <ref type="bibr" target="#b32">[33]</ref>. Qualitatively, the prediction made by our method contains a higher level of detail (see <ref type="figure">Fig. 1</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Surface normal estimation. Surface normal estimation from a single RGB image has been studied extensively in literature <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b37">38]</ref>. The existing methods generally consist of a feature extractor followed by a prediction head. For example, Ladicky et al. <ref type="bibr" target="#b21">[22]</ref> extracted hand-crafted features (e.g., SIFT <ref type="bibr" target="#b25">[26]</ref>) and applied multi-class Ada-boost <ref type="bibr" target="#b35">[36]</ref> to regress the output as a linear combination of a discrete set of normals. Following the success of deep learning, recent methods replace both components with convolutional neural networks (CNNs). Wang et al. <ref type="bibr" target="#b38">[39]</ref> introduced two-stream CNNs to learn global and local cues, and fused them with another CNN. Eigen and Fergus <ref type="bibr" target="#b6">[7]</ref> proposed a multi-scale architecture to jointly predict depth, surface normals and semantic labels. Following these early attempts, contributions have been made by enforcing depth-normal consistency <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b31">32]</ref>, formulating the task as spherical regression <ref type="bibr" target="#b23">[24]</ref>, and introducing a spatial rectifier to handle tilted images <ref type="bibr" target="#b5">[6]</ref>. In this work, we address the aleatoric uncertainty in surface normal, which has not been studied in previous literature. Uncertainty in deep learning. Two major types of uncertainty are epistemic and aleatoric <ref type="bibr" target="#b4">[5]</ref>. Epistemic uncertainty (i.e. uncertainty in model) can be modelled by approximating the posterior over the model weights. For example, by applying dropout <ref type="bibr" target="#b34">[35]</ref> at test time, N networks can be sampled from the approximate posterior, and the variance of the outputs can be used as a measure of uncertainty <ref type="bibr" target="#b10">[11]</ref>. The posterior can also be approximated by training N networks on random subsets of data <ref type="bibr" target="#b22">[23]</ref>, or by taking N snapshots during a single training <ref type="bibr" target="#b16">[17]</ref>. The aforementioned approaches are task-independent and can easily be applied to surface normal estimation.</p><p>The focus of this paper is on the aleatoric uncertainty, which captures the noise inherent in the data. We assume that the uncertainty is heteroscedastic <ref type="bibr" target="#b19">[20]</ref> (i.e. certain pixels have higher uncertainty than the others). For such a scenario, a commonly used approach is to estimate the perpixel probability distribution over the output, and train the network by maximizing the likelihood of the ground truth <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b11">12]</ref>. This requires a task-specific formulation and has not been studied for CNN-based surface normal estimation. Distribution on a unit sphere. The surface normal probability distribution should be defined on a unit sphere. An example of such distribution is the von Mises-Fisher distribution <ref type="bibr" target="#b7">[8]</ref>, a rotationally symmetric uni-modal distribution defined on an n-sphere. In this paper, we introduce a variant of the von Mises-Fisher distribution, such that minimizing its negative log-likelihood is equivalent to minimizing the angle between the predicted normal and the ground truth, which is the error metric of our interest. Uncertainty-guided sampling. PointRend <ref type="bibr" target="#b20">[21]</ref> is a neural network module designed for instance/semantic segmentation. As making inference on a regular grid leads to undersampling of the pixels near object boundaries, PointRend uses a point-wise MLP to make inference on a subset of pixels with high uncertainty. Our decoder module is a novel extension of such a framework to surface normal estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>This section provides the details of our method. Firstly, we introduce a new parameterization for the surface normal probability distribution that can be used for uncertainty estimation. Secondly, we explain the network architecture and the uncertainty-guided sampling used for training the pixel-wise refinement networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Aleatoric Uncertainty in Surface Normal</head><p>Our goal is to learn the per-pixel surface normal probability distribution p i (n i |I), where i is the pixel index and I is the input image. In practice, we parameterize the distribution with a set of parameters ? i , which is estimated by a network of weights W. The network is trained by minimizing the negative log-likelihood (NLL) of the ground truth n gt i . The training loss can thus be written as</p><formula xml:id="formula_0">L = ? 1 N i log p i (n gt i |? i (I, W)),<label>(1)</label></formula><p>where N is the number of pixels with ground truth. Finding a suitable parameterization for the distribution is important as it determines which quantity will be minimized (or maximized) during training. von Mises-Fisher distribution. We use the von Mises-Fisher distribution <ref type="bibr" target="#b7">[8]</ref> (abbreviated hereafter as vonMF) as a baseline. It is a spherical analogue to the normal distribution, defined on a unit n-sphere in R n+1 <ref type="bibr" target="#b14">[15]</ref>. For n = 2, the probability density function (PDF) is given as</p><formula xml:id="formula_1">p vonMF,i (n i |? i , ? i ) = ? i exp(? i ? T i n i ) 4? sinh ? i ,<label>(2)</label></formula><p>where ? i is the mean direction and ? i is the concentration parameter. Both n i and ? i are unit vectors and ? i ? 0.</p><p>Higher value of ? i means that the distribution is more concentrated around ? i and that the uncertainty is low for that pixel (the distribution is uniform when ? i = 0). The pixelwise NLL loss can be written as  <ref type="figure">Figure 3</ref>. Each histogram shows the distribution of ground truth along the dashed line. The red and the blue lines show the direction that minimizes the L2 loss and the angular loss, respectively (the lines overlap for (a)). In both examples, the pixels along the dashed line have similar visual features and belong to the same plane. However, the pixels in (b) suffer from the noise caused by the neighboring pixels belonging to a different plane. The angular loss is more robust in the presence of such asymmetric noise.</p><formula xml:id="formula_2">L vonMF,i = ? log ? i + log sinh ? i ? ? i ? T i n gt i .<label>(3)</label></formula><p>Maximizing ? T i n gt i is equivalent to minimizing the L 2 distance |? i ? n gt i | 2 2 . The loss is attenuated for the pixels with high uncertainty. The first two terms in Eq. 3 prevent the network from predicting infinite ? for all pixels. To summarize, Eq. 3 is an L 2 loss with learned attenuation. Angular vonMF distribution. While Eq. 3 minimizes L 2 , we argue that the loss should minimize the angle between the predicted normal and the ground truth, cos ?1 ? T i n gt i . Firstly, this makes the loss consistent with the error metric. Secondly, this makes the network more robust against the asymmetric noise in the ground truth surface normal.</p><p>The ground truth surface normal of a pixel is obtained by fitting a plane to the point cloud defined by the pixel and its local neighborhood. If some of the neighboring pixels belong to a different plane (e.g., because the central pixel is close to the plane boundary), the ground truth will be affected accordingly and the noise in the ground truth will be asymmetric around the true normal. The mean direction, which minimizes the L 2 loss, is sensitive to such asymmetric noise. The angular loss, on the other hand, is minimized at the median direction, which is more robust against such noise (see <ref type="figure">Fig. 3</ref>). To this end, we introduce a distribution such that its NLL is the angular loss with learned attenuation. The PDF and the NLL loss are given as,</p><formula xml:id="formula_3">p AngMF,i (n i |? i , ? i ) = (? 2 i + 1) exp(?? i cos ?1 ? T i n i ) 2?(1 + exp(?? i ?))<label>(4)</label></formula><p>and L AngMF,i = ? log(? 2 i + 1) + log(1 + exp(?? i ?)) uncertainty-guided sampling <ref type="figure">Figure 4</ref>. Illustration of the proposed pipeline. Initially, a coarse prediction is made from the 1/8 resolution feature-map and the loss is applied to all pixels. Then, a refinement module upsamples the coarse feature-map and prediction by a factor of 2, and applies a pixel-wise MLP to yield a refined, higher resolution output. Full-resolution output is obtained by applying three refinement modules. The MLPs are trained on a subset of pixels selected based on the uncertainty, to prevent the bias in training towards low-uncertainty pixels.</p><formula xml:id="formula_4">+ ? i cos ?1 ? T i n gt i . (5)</formula><p>We call this the Angular vonMF distribution (abbreviated hereafter as AngMF). Eq. 4 is obtained by setting the NLL as L i = C(? i ) + ? i cos ?1 ? T i n i and finding the expression for C(? i ) via normalization (derivation in the supplementary material). Minimizing Eq. 5 is equivalent to minimizing the angular error, while attenuating the loss for the pixels with high uncertainty (i.e. low ?). We show in the experiments that using the proposed AngMF leads to higher accuracy than using the vonMF. Measure of uncertainty. In the proposed distribution (Eq. 4), ? i encodes the network's confidence in the predicted mean ? i . To translate this into an intuitive quantity, we calculate the expected value of the angular error</p><formula xml:id="formula_5">E[cos ?1 ? T i n i ] = 2? i ? 2 i + 1 + exp(?? i ?)? 1 + exp(?? i ?) ,<label>(6)</label></formula><p>and use it as a measure of the pixel-wise aleatoric uncertainty (derivation in the supplementary material).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Uncertainty-Guided Sampling for Pixel-Wise Refinement</head><p>The NLL losses (Eq. 3 and Eq. 5) are more robust against noisy data than their counterparts (L 2 and angular loss) as the loss is attenuated for high-uncertainty pixels. However, this also makes the training more biased to large planar surfaces that have low surface normal uncertainty.</p><p>Such bias leads to the lack of detail in the prediction, as the network is not encouraged to make accurate predictions for the challenging pixels, most of which are near object boundaries and on small structures. To this end, we propose a novel decoder framework, where pixel-wise multilayer perceptrons (MLPs) are trained on a subset of pixels selected based on the estimated uncertainty.</p><p>Feature extraction. The proposed pipeline is illustrated in <ref type="figure">Fig. 4</ref>. The input to the network is an RGB image of size (H ? W ). We first generate feature-maps of different resolutions, using a convolutional encoder-decoder with skip-connections. We use the same architecture as <ref type="bibr" target="#b1">[2]</ref>.</p><p>Coarse prediction. The network initially makes a coarse prediction from the 1/8 resolution feature-map, using a 3 ? 3 convolutional layer. The number of output channels is 4 (3 for ? and 1 for ?). The first three channels are L 2normalized to ensure ||?|| = 1. We apply the modified ELU function <ref type="bibr" target="#b2">[3]</ref>, f (x) = ELU(x) + 1, for the last channel to ensure that ? is positive. For the coarse prediction, the training loss (Eq. 5) is applied to all pixels.</p><p>Pixel-wise refinement modules. The coarse prediction is then passed through three pixel-wise refinement modules of the same architecture. The input to each module is a low-resolution feature-map and prediction of size (H/2n ? W/2n), and the output is a refined prediction of size (H/n ? W/n). The forward pass in each module consists of three steps. (1) Upsampling: Both the feature-map for most pixels except for those on the floor. If the NLL loss is applied to all pixels, the pixels on the floor will dominate the training as our loss is weighted by ?. (e-f) Uncertaintyguided sampling. We sample the pixels with high uncertainty (importance sampling) and add uniformly sampled pixels (coverage). Such sampling helps the network to focus on the challenging pixels. (g-h) Prediction made in full-resolution in the final epoch. The prediction is improved especially on the challenging pixels near object boundaries and on small structures. The network also becomes more confident about such pixels. and prediction are bilinearly upsampled by a factor of 2.</p><formula xml:id="formula_6">Predicted ! (1/8 res) Image Predicted " (1/8 res) Predicted ! (full res) Coverage Importance Sampling (a) (c) (d) (b) (e) (g) (h) (f) GT Predicted " (full res)</formula><p>(2) Uncertainty-guided sampling: During training, a subset of pixels is selected base on the uncertainty. The sampling strategy is explained below in more detail. (3) Pixel-wise refinement: An MLP with three hidden layers, each with 128 nodes and a ReLU <ref type="bibr" target="#b26">[27]</ref> activation, estimates a refined output for the sampled pixels. The input to the MLP is a concatenated vector of the pixel-wise feature and prediction. Same as in the coarse prediction layer, L 2 normalization and the modified ELU activation are applied to ? and ?. During training, the loss is calculated only for the sampled pixels. At test time, the trained MLPs are applied to all pixels. Uncertainty-guided sampling. Suppose that there are h?w pixels in the bilinearly upsampled prediction. In total, we sample N s = r s ? h ? w pixels, where r s is set to 0.4 in all experiments. Firstly, we sample ? UG ? N s pixels with the highest uncertainty (i.e. importance sampling). Then, (1 ? ? UG ) ? N s pixels are sampled uniformly from the remaining pixels (i.e. coverage). ? UG , which can have values from 0 to 1, determines how biased the sampling is towards the highuncertainty pixels. <ref type="figure" target="#fig_0">Fig. 5</ref> illustrates the sampling process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Setup</head><p>Datasets. We evaluate our method on two datasets: Scan-Net <ref type="bibr" target="#b3">[4]</ref> and NYUv2 <ref type="bibr" target="#b32">[33]</ref>. ScanNet contains RGB-D frames from 1613 scans acquired in 807 different scenes. We use the ground truth surface normal and data split provided by FrameNet <ref type="bibr" target="#b17">[18]</ref>. NYUv2 consists of RGB-D video sequences capturing 464 indoor scenes. We evaluate on the official test set using the ground truth generated by Ladicky et al. <ref type="bibr" target="#b21">[22]</ref>. As the official training set only contains 795 images, state-of-the-art methods sample additional images from the training sequences <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b31">32]</ref> or supplement with other datasets <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b13">14]</ref>. To ensure a fair comparison, we use the same training set as GeoNet++ <ref type="bibr" target="#b31">[32]</ref>. Surface normal accuracy metrics. Angular error is measured for the pixels with valid ground truth. Following <ref type="bibr" target="#b8">[9]</ref>, we report the mean, median and root-mean-squared error (lower the better), and the percentage of pixels with error below thresholds t ? [11.25 ? , 22.5 ? , 30 ? ] (higher the better). Uncertainty metrics. The significance of the estimated uncertainty can be evaluated using sparsification curves <ref type="bibr" target="#b29">[30]</ref>. The pixels are sorted based on the uncertainty and an error metric ? is evaluated on the top x% of pixels with low uncertainty. Following <ref type="bibr" target="#b29">[30]</ref>, we transform the accuracy metric (% of pixels with error less than t ? ) into an error metric by subtracting it from 100%. We vary x from 1 to 100, incrementing by 1, and report the area under the sparsification curve (AUSC) as in <ref type="bibr" target="#b15">[16]</ref>. AUSC is affected by two factors: how accurate the predictions are, and how similar the uncertainty-based sorting is to the actual error-based sorting. To only evaluate the latter, we also report the area under the sparsification error (AUSE) <ref type="bibr" target="#b18">[19]</ref>, by subtracting the oracle sparsification (obtained via error-based sorting) from the estimated sparsification. Implementation details. The proposed network is implemented with PyTorch <ref type="bibr" target="#b27">[28]</ref>. For training, we use the AdamW optimizer <ref type="bibr" target="#b24">[25]</ref> and schedule the learning rate using 1cycle policy <ref type="bibr" target="#b33">[34]</ref> with lr max = 3.5?10 ?4 (other hyper-parameters are set as their default values). The batch size is 4 and the number of epochs is 5 unless specified otherwise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>Firstly, we perform a set of ablation studies to demonstrate the effectiveness of the proposed approach. Then, the accuracy is compared against the state-of-the-art methods. Lastly, we evaluate the quality of the estimated uncertainty and compare it against alternative methods of uncertainty estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Ablation Study</head><p>The ablation study experiments are performed on a subset of ScanNet <ref type="bibr" target="#b3">[4]</ref>, obtained by sampling 20% of the images in the training set (which contains 190K images). Training loss. NLL-vonMF (Eq. 3) is the L 2 loss with learned attenuation, and the proposed NLL-AngMF (Eq. 5) is the angular loss (AL) with learned attenuation. We compare the four loss functions in Tab. 1 (top). As L 2 and AL cannot be used for uncertainty estimation, the decoder modules are removed, and the surface normal is directly estimated from the convolutional encoder-decoder, by adding a 3 ? 3 convolutional layer to the final feature-map. Following are the key insights we can obtain from this experiment. ? NLL-AngMF vs. NLL-vonMF. While NLL-vonMF minimizes L 2 , the proposed NLL-AngMF minimizes the angular error, which is more consistent with the error metrics. As a result, NLL-AngMF achieves significantly higher accuracy than NLL-vonMF, except for RMSE.</p><p>? NLL-AngMF vs. AL. Our NLL-AngMF is AL with learned attenuation. As the training is biased to lowuncertainty pixels (mostly on large surfaces), the median error decreases and the accuracy for low thresholds (5.0 ? and 7.5 ? ) increases. On the contrary, the mean error and RMSE increase and the accuracy for higher thresholds decreases. This is because the network is not penalized strongly for making inaccurate predictions for the challenging pixels.</p><p>Decoder architecture. Tab. 1 (bottom) demonstrates the effectiveness of the proposed decoder modules. Firstly, we add the pixel-wise MLPs and train them on all pixels. Then, we apply the uncertainty-guided sampling during training (with ? UG = 0.7). Both components lead to improvement in all metrics. As the uncertainty-guided sampling prevents the bias in training towards large planar surfaces, the quality of prediction is improved especially near object boundaries and on small structures, as shown in <ref type="figure" target="#fig_2">Fig. 6</ref>. Sampling strategy. Tab. 2 shows how the accuracy changes for different values of ? UG . ? UG determines the ratio of the importance sampling. If ? UG = 1.0, only the pixels with the highest uncertainty are sampled. If ? UG = 0.0, the pixels are sampled uniformly. Finding the right balance between the two is important for minimizing the bias in training. Best performance is achieved when ? UG = 0.7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Comparison with the State-of-the-Art</head><p>NYUv2. Tab. 3 compares the accuracy of different methods on NYUv2 <ref type="bibr" target="#b32">[33]</ref>. Note that, compared to ScanNet <ref type="bibr" target="#b3">[4]</ref>, the quality of the ground truth is noticeably worse for NYUv2. While the ground truth for ScanNet is calculated from a 3D mesh that is obtained by fusing thousands of RGB-D    <ref type="table">Table 2</ref>. Influence of ?UG on the accuracy (rs is fixed to 0.4). ?UG is the ratio of the importance sampling. Best performance is achieved when ?UG = 0.7. frames, the ground truth for NYUv2 is calculated from a single noisy depth map. Nonetheless, the proposed training loss (angular loss with learned attenuation) and decoder framework (trained with uncertainty-guided sampling) help the network to learn from noisy data. As a result, our network shows a decisive improvement over GeoNet++ <ref type="bibr" target="#b31">[32]</ref>. Qualitative comparison in <ref type="figure">Fig. 7</ref> shows that the predic-  <ref type="bibr" target="#b31">[32]</ref> and TiltedSN <ref type="bibr" target="#b5">[6]</ref>. The predictions made by our method show clearer object boundaries and preserve the fine-details of the scene geometry (see the regions pointed by the red arrows). The estimated uncertainty is high near object boundaries and on small structures. More examples are provided in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Train mean median rmse 11.25 <ref type="bibr">?</ref>  tions made by our method contain a higher level of detail. We also train the network on ScanNet and test on NYUv2 without fine-tuning. In this cross-dataset evaluation, we win against other methods except for the median error and 11.25 ? , suggesting that the network can generalize well to an unseen dataset.</p><p>ScanNet. Tab. 4 compares different methods trained and tested on ScanNet <ref type="bibr" target="#b3">[4]</ref>. The batch size is set to 16 for this experiment. We outperform the state-of-the-art methods across all metrics. Qualitative comparison against TiltedSN <ref type="bibr" target="#b5">[6]</ref> is provided in <ref type="figure">Fig. 7</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Quality of Uncertainty</head><p>Lastly, we evaluate the quality of the estimated uncertainty by plotting the sparsification curves. As no previous work has estimated the surface normal uncertainty, we compare our method against task-independent approaches. (1) Test-time dropout (Drop): 2D dropout (p = 0.2) is added after each 2D convolutional block in decoder. After training, 8 forward passes are performed, with dropout enabled. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>urs (NLL-AngMF)</head><p>Error Image / GT 60?0?F igure 9. We compare the uncertainty estimated by our method against the uncertainty estimated by applying test-time dropout and augmentation (Drop+Aug). The uncertainty estimated by our method shows higher correlation with the prediction error. method outperforms other methods across all metrics. <ref type="figure" target="#fig_3">Fig.  8</ref> compares the sparsification curves. When evaluated on all pixels, all methods perform similarly. However, as the pixels with high uncertainty are removed, our method gets significantly more accurate than the others. This suggests that our uncertainty correlates better with the prediction error (see <ref type="figure">Fig. 9</ref> for qualitative comparison).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Supplementary Material</head><p>In the supplementary material, we provide the derivations for the AngMF distribution, quantitative evaluation with additional metrics, cross-dataset evaluation on KITTI <ref type="bibr" target="#b12">[13]</ref> and DAVIS <ref type="bibr" target="#b28">[29]</ref> and discussion on failure modes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this work, we estimated and evaluated the aleatoric uncertainty in CNN-based surface normal estimation, for the first time in literature. The proposed method estimates the per-pixel surface normal probability distribution, from which the expected angular error can be inferred to quantify the aleatoric uncertainty. We introduced a new parameterization for the surface normal probability distribution, such that its negative log-likelihood is the angular loss with learned attenuation. We also proposed a novel decoder framework where pixel-wise MLPs are trained on a subset of pixels selected based on the uncertainty. Such uncertainty-guided sampling prevents the bias in training towards large planar surfaces, thereby improving the level of detail in the prediction. Experimental results show that the proposed method achieves state-of-the-art performance on ScanNet <ref type="bibr" target="#b3">[4]</ref> and NYUv2 <ref type="bibr" target="#b32">[33]</ref>, and that the estimated uncertainty correlates well with the prediction error.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Derivations for the proposed AngMF distribution</head><p>In the paper, we introduced a variant of the von Mises-Fisher distribution <ref type="bibr" target="#b7">[8]</ref>, such that its negative log-likelihood (NLL) is the angular loss with learned attenuation. We call this the Angular vonMF (AngMF) distribution. In this section, we provide the derivations for Eq. 4, Eq. 5 and Eq. 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1. Probability density function (Eq. 4)</head><p>The NLL of the distribution should have the form of</p><formula xml:id="formula_7">L i = C(? i ) + ? i cos ?1 ? T i n gt i ,<label>(7)</label></formula><p>where i is the pixel index and cos ?1 ? T i n gt i is the angle between the predicted mean direction ? i and the ground truth surface normal n gt i . The angular error is weighted by the concentration parameter ? i , which encodes the network's confidence in the predicted mean direction. The first term C(? i ) should be a monotonically decreasing function of ? i in order to prevent the trivial solution where ? i = 0 ? i. Then, the probability density function (PDF) should look like</p><formula xml:id="formula_8">p(n i |? i , ? i ) = D(? i ) exp(?? i cos ?1 ? T i n i ),<label>(8)</label></formula><p>where C(? i ) = ? log(D(? i )). We can then compute the cumulative probability of the angular error cos ?1 ? T i n i being less than some threshold ? * . See <ref type="figure" target="#fig_5">Fig. 10-(a)</ref> for the axes orientation used for the integration.</p><formula xml:id="formula_9">P [cos ?1 (? T i n i ) ? ? * ] = 2? 0 ? * 0 D(? i ) exp(?? i ?) sin ?d?d? = 2?D(? i ) ? * 0 exp(?? i ?) sin ?d? = 2?D(? i ) ? exp(?? i ?)(cos ? + ? i sin ?) ? 2 i + 1 ? * 0 = 2?D(? i ) 1 ? exp(?? i ? * )(cos ? * + ? i sin ? * ) ? 2 i + 1<label>(9)</label></formula><p>Solving P [cos ?1 (? T i n i ) ? ?] = 1 gives D(? i ) = 1 2?</p><formula xml:id="formula_10">? 2 i + 1 1 + exp(?? i ?) .<label>(10)</label></formula><p>Inserting Eq. 10 into Eq. 8 gives</p><formula xml:id="formula_11">p i (n i |? i , ? i ) = (? 2 i + 1) exp(?? i cos ?1 ? T i n i ) 2?(1 + exp(?? i ?)) ,<label>(11)</label></formula><p>which is Eq. 4. <ref type="figure" target="#fig_5">Fig. 10-(b)</ref> visualizes the distribution for different values of ?. As ? increases, the distribution becomes more concentrated around the mean direction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Negative log-likelihood (Eq. 5)</head><p>The network is trained by minimizing the NLL of the ground truth normal. The training loss can thus be written as</p><formula xml:id="formula_12">L i = ? log(? 2 i + 1) + log(1 + exp(?? i ?)) + ? i cos ?1 ? T i n gt i ,<label>(12)</label></formula><p>where we drop the constant term, log 2?. This is Eq. 5 in the paper. </p><p>From this, we can calculate the probability density function for the angular error ? via differentiation.</p><formula xml:id="formula_14">p(?|? i , ? i ) = d d? 1 ? exp(?? i ?)(cos ? + ? i sin ?) 1 + exp(?? i ?) = ? exp(?? i ?)(? sin ? + ? i cos ?) + ? i exp(?? i ?)(cos ? + ? i sin ?) 1 + exp(?? i ?) = exp(?? i ?) sin(?)(? 2 i + 1) 1 + exp(?? i ?)<label>(14)</label></formula><p>Then, the expected value of ? can be obtained as</p><formula xml:id="formula_15">E[?] = ? 0 ? exp(?? i ?) sin(?)(? 2 i + 1) 1 + exp(?? i ?) d? = ? 2 i + 1 1 + exp(?? i ?) ? 0 ? exp(?? i ?) sin ?d? = ? 2 i + 1 1 + exp(?? i ?) ? exp(?? i ?)((? i ((? 2 i + 1)? + ? i ) ? 1) sin ? + ((? 2 i + 1)? + 2? i ) cos ?) (? 2 i + 1) 2 ? 0 = ? 2 i + 1 1 + exp(?? i ?) 2? i (1 + exp(?? i ?)) + exp(?? i ?)(? 2 i + 1)? (? 2 i + 1) 2 = 2? i ? 2 i + 1 + exp(?? i ?)? 1 + exp(?? i ?) ,<label>(15)</label></formula><p>which is Eq. 6. This quantity is used as a measure of the aleatoric uncertainty. <ref type="figure" target="#fig_5">Fig. 10</ref>-(c) visualizes Eq. 13, Eq. 14 and Eq. 15 for different values of ?. The expected error decreases as ? increases. For ? = 0, the distribution is uniform and the expected error is ?/2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Quantitative evaluation with additional metrics</head><p>In this section, we provide the quantitative evaluation of our method with additional metrics. Tab. 7, Tab. 8 and Tab. 9 are extensions of Tab. 4, Tab. 5 and Tab. 6, respectively. <ref type="figure">Fig. 11</ref> and <ref type="figure" target="#fig_4">Fig. 12</ref> are extensions of <ref type="figure" target="#fig_3">Fig. 8</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1. Comparison against TiltedSN</head><p>Tab. 7 provides comparison against TiltedSN <ref type="bibr" target="#b5">[6]</ref> on ScanNet <ref type="bibr" target="#b3">[4]</ref> with additional metrics. Note that the difference in the accuracy (% of pixels with error less than t ? ) increases for lower thresholds. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2. Quality of the estimated uncertainty</head><p>Tab. 8 and Tab. 9 compare different methods of estimating the surface normal uncertainty. "Drop" (making 8 inferences with dropout enabled), "Aug" (making 2 inferences by flipping the image) and "Drop+Aug" (making 8?2 inferences by applying both) are task-independent approaches which do not require the output to be distributional. The proposed pipeline, trained with the NLL losses, significantly outperforms other approaches across all metrics, suggesting that the estimated uncertainty better correlates with the prediction error.    <ref type="figure" target="#fig_4">Fig. 11 and Fig. 12</ref> provide the sparsification curves for NYUv2 <ref type="bibr" target="#b32">[33]</ref> and ScanNet <ref type="bibr" target="#b3">[4]</ref>, respectively. When evaluated on all pixels, all methods perform similarly. However, as the pixels with high uncertainty are removed, our method gets significantly more accurate than the others, suggesting that our uncertainty correlates better with the prediction error. For "Ours (NLL-AngMF)", we also show the ideal sparsification (oracle) by sorting the pixels by the error.  <ref type="figure">Figure 11</ref>. Sparsification curves for NYUv2 <ref type="bibr" target="#b32">[33]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3. Sparsification curves</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Cross-dataset evaluation on KITTI and DAVIS</head><p>In the paper, we performed a cross-dataset evaluation by training the network on ScanNet <ref type="bibr" target="#b3">[4]</ref> and testing it on NYUv2 <ref type="bibr" target="#b32">[33]</ref> without fine-tuning. However, this is not a challenging task as both datasets contain images of indoor scenes with similar visual features. In this section, we further demonstrate the generalization ability of our method by testing the network (trained only on ScanNet) on two challenging datasets -KITTI <ref type="bibr" target="#b12">[13]</ref> and DAVIS <ref type="bibr" target="#b28">[29]</ref>. The results are provided in <ref type="figure">Fig. 13 and Fig. 14.</ref> For comparison, we also provide the predictions made by TiltedSN <ref type="bibr" target="#b5">[6]</ref>.</p><p>The ground truth surface normal for ScanNet is calculated from a 3D mesh that is obtained by fusing thousands of depthmaps. For this reason, the ground truth generally does not exist for dynamic objects such as humans. As the dataset is collected in indoor scenes, it also does not contain instances of cars and buildings. Nonetheless, the network can generalize well for such unseen objects. We believe that this is because the network utilizes low-level features, such as edges and shades, which are universal in most datasets. <ref type="figure" target="#fig_2">Fig. 16</ref>, which will be discussed in Sec. D, supports such argument. Even when the input image only contains edges or shades, the network can infer the 3D structure.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Image</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ours -Normal TiltedSN</head><p>Ours -Uncertainty <ref type="figure">Figure 13</ref>. Cross-dataset evaluation on KITTI <ref type="bibr" target="#b12">[13]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Image</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ours -Normal TiltedSN</head><p>Ours -Uncertainty <ref type="figure">Figure 14</ref>. Cross-dataset evaluation on DAVIS <ref type="bibr" target="#b28">[29]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Failure modes</head><p>In this section, we discuss the failure modes of the proposed method. <ref type="figure" target="#fig_0">Fig. 15</ref> shows the predictions made for tilted images. The network is robust against mild rotations (?20 ? ), but suffers when the images are tilted severely (30 ? ?). Nevertheless, the expected error (clamped between 0 ? and 60 ? in all images) also increases for such images, demonstrating the usefulness of the estimated uncertainty. Tilted images can be handled by using a spatial rectifier to warp the images such that its surface normal distribution matches to that of the training images, as done in <ref type="bibr" target="#b5">[6]</ref>. This will be investigated in our future work. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1. Tilted images</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Image</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2. Inherent ambiguity of the problem</head><p>To understand the visual cues used by the network, we created artificial images consisting only of edges and shades. <ref type="figure" target="#fig_2">Fig.  16</ref> shows the predictions made by the network. The first three images show "Y"-shaped structures and the other three are their vertically flipped versions. Note that the depth of each pixel can have any arbitrary value, meaning that the surface can have any form. It can be a concave (or convex) corner or even a drawing on a flat wall.</p><p>For the last three images, the network thinks that it is a concave corner. This is because such structure was mostly seen in the lower corners of cuboid-shaped rooms. However, the prediction is not clear for the "Y"-shaped structures. We believe that this is because such structure was seen in both concave corners (upper corners of rooms) and convex corners (external corners of furnitures). To handle such ambiguity, the network should estimate a multi-modal surface normal distribution, which consists of multiple uni-modal distributions with mixing coefficients. This will be investigated in our future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Image</head><p>Normal Uncertainty <ref type="figure" target="#fig_2">Figure 16</ref>. Predictions made for artificial images consisting only of edges and shades.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Additional comparison against the state-of-the-art</head><p>Lastly, we provide additional qualitative comparison against GeoNet++ <ref type="bibr" target="#b31">[32]</ref> (in <ref type="figure">Fig. 17</ref>) and TiltedSN <ref type="bibr" target="#b5">[6]</ref> (in <ref type="figure" target="#fig_3">Fig. 18</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Image</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GeoNet++ GT</head><p>Ours -Normal Ours -Uncertainty <ref type="figure">Figure 17</ref>. Additional qualitative comparison against GeoNet++ <ref type="bibr" target="#b31">[32]</ref> on NYUv2 <ref type="bibr" target="#b32">[33]</ref>. Despite the poor quality of the ground truth, our method can recover the fine details of the scene geometry (see the areas pointed by the red arrows).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Image TiltedSN GT</head><p>Ours -Normal Ours -Uncertainty <ref type="figure" target="#fig_3">Figure 18</ref>. Additional qualitative comparison against TiltedSN <ref type="bibr" target="#b5">[6]</ref> on ScanNet <ref type="bibr" target="#b3">[4]</ref>. The predictions made by our method contain higher level of detail (see the areas pointed by the red arrows).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 5 .</head><label>5</label><figDesc>(a-b) Input image and the ground truth. (c-d) Prediction made in coarse resolution, during the first epoch. In (d) and (h), white means high ?. The network estimates low ? (i.e. high uncertainty)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 6 .</head><label>6</label><figDesc>Qualitative comparison between the networks with different decoder architecture (showing crops of 200 pixels ? 200 pixels). The proposed uncertainty-guided sampling (UG) enforces the network to focus on the challenging pixels (i.e. those with high uncertainty). This improves the level of detail in the prediction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 8 .</head><label>8</label><figDesc>Sparsification curves obtained by different methods of estimating the surface normal uncertainty.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>( 2 )</head><label>2</label><figDesc>Test-time augmentation (Aug): Following<ref type="bibr" target="#b29">[30]</ref>, we perform 2 forward passes by flipping the input image. (3) Combined approach (Drop + Aug): We apply the image flipping to the network with dropout to make 2 ? 8 = 16 forward passes. For all three methods, the uncertainty is measured as the average angular error with respect to the mean direction. As the uncertainty cannot be estimated in a single forward pass, the uncertainty-guided sampling is disabled, and the networks are trained with the angular loss. Quantitative results in Tab. 5 and Tab. 6 show that the proposedNormal Drop + Aug Uncertainty 60?0?O</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 10 .</head><label>10</label><figDesc>(a) The axes orientation used for the integrations in Eq. 9 and Eq. 15. The mean direction ? is aligned with the z-axis, and is thus excluded in the integration. (b) Visualization of Eq. 11 for different values of ?. ? determines how concentrated the distribution is towards the mean direction. (c) Eq. 13, Eq. 14 and Eq. 15 plotted for different values of ?. The expected error decreases as the confidence ? increases. A.3. Measure of uncertainty (Eq. 6) Inserting Eq. 10 to Eq. 9 gives P [cos ?1 (? T i n i ) ? ? * ] = 1 ? exp(?? i ? * )(cos ? * + ? i sin ? * ) 1 + exp(?? i ?) .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 12 .</head><label>12</label><figDesc>Sparsification curves for ScanNet<ref type="bibr" target="#b3">[4]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 15 .</head><label>15</label><figDesc>Predictions made for tilted images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>ArchitectureLoss fn. mean median rmse 5.0 ? 7.5 ? 11.25?  22.5 ? 30 ? baseline L2 13.53 7.22 21.16 35.10 51.44 65.08 82.38 87.83 (convolutional encoder-decoder with skip connections [2]) NLL-vonMF 14.10 7.19 22.14 36.20 51.46 64.09 80.80 86.34 AL 13.45 6.70 21.78 38.65 54.04 66.73 82.46 87.53 NLL-AngMF 13.82 6.60 22.47 39.69 54.30 65.97 81.64 86.71 baseline + pixel-wise MLPs NLL-AngMF 13.59 6.53 22.23 39.92 54.79 67.03 82.18 87.06 baseline + pixel-wise MLPs + uncertainty-guided sampling 13.17 6.48 21.57 40.09 55.19 67.62 83.10 87.97 Table 1. (top) The baseline network is trained with different loss functions. The proposed NLL-AngMF shows higher accuracy than NLL-vonMF, except for RMSE. NLL-AngMF and NLL-vonMF are AL and L2 with learned attenuation, respectively. As the training is biased to low-uncertainty pixels, the median error decreases, while RMSE increases. (bottom) The bias in training is solved by the proposed decoder modules. Both the pixel-wise MLPs and the uncertainty-guided sampling lead to improvement in all metrics.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>?</head><label></label><figDesc>UG mean median rmse 11.25 ? 22.5 ? 30 ? 0.0 13.58 6.52 22.18 66.68 82.09 87.09 0.6 13.34 6.56 21.76 66.99 82.78 87.74 0.7 13.17 6.48 21.57 67.62 83.10 87.97 0.8 13.28 6.56 21.69 67.45 83.00 87.90 1.0 13.26 6.59 21.57 67.16 82.98 87.92</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 .</head><label>4</label><figDesc><ref type="bibr" target="#b21">22</ref>.5 ? 30 ? Surface normal accuracy on ScanNet<ref type="bibr" target="#b3">[4]</ref>. Our method outperforms other methods across all metrics.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Method</cell><cell cols="2">mean median rmse 11.25 ? 22.5 ? 30 ?</cell></row><row><cell>Ladicky et al. [22]</cell><cell></cell><cell>33.5 23.1</cell><cell>-</cell><cell>27.5 49.0 58.7</cell><cell cols="3">FrameNet[18] 14.7 7.7 22.8 62.5 80.1 85.8</cell></row><row><cell>Fouhey et al. [10]</cell><cell></cell><cell>35.2 17.9</cell><cell>-</cell><cell>40.5 54.1 58.9</cell><cell cols="2">VPLNet[38] 13.8 6.7</cell><cell>-</cell><cell>66.3 81.8 87.0</cell></row><row><cell>Deep3D [39] Eigen et al. [7] SkipNet [1]</cell><cell></cell><cell cols="3">26.9 14.8 20.9 13.2 19.8 12.0 28.2 47.9 70.0 77.8 -42.0 61.2 68.2 -44.4 67.2 75.9</cell><cell cols="3">TiltedSN[6] 12.6 6.0 21.1 69.3 83.9 88.6 Ours 11.8 5.7 20.0 71.1 85.4 89.8</cell></row><row><cell>SURGE [37]</cell><cell>N</cell><cell>20.6 12.2</cell><cell>-</cell><cell>47.3 68.9 76.6</cell><cell></cell><cell></cell></row><row><cell>GeoNet [31]</cell><cell></cell><cell cols="3">19.0 11.8 26.9 48.4 71.5 79.5</cell><cell></cell><cell></cell></row><row><cell>PAP [42]</cell><cell></cell><cell cols="3">18.6 11.7 25.5 48.8 72.2 79.8</cell><cell></cell><cell></cell></row><row><cell>GeoNet++ [32]</cell><cell></cell><cell cols="3">18.5 11.2 26.7 50.2 73.2 80.7</cell><cell></cell><cell></cell></row><row><cell>Ours</cell><cell cols="4">N 14.9 7.5 23.5 62.2 79.3 85.2</cell><cell></cell><cell></cell></row><row><cell>FrameNet[18]</cell><cell></cell><cell cols="3">18.6 11.0 26.8 50.7 72.0 79.5</cell><cell></cell><cell></cell></row><row><cell>VPLNet[38]</cell><cell cols="2">S 18.0 9.8</cell><cell>-</cell><cell>54.3 73.8 80.7</cell><cell></cell><cell></cell></row><row><cell>TiltedSN[6]</cell><cell></cell><cell cols="3">16.1 8.1 25.1 59.8 77.4 83.4</cell><cell></cell><cell></cell></row><row><cell>Ours</cell><cell cols="4">S 16.0 8.4 24.7 59.0 77.5 83.7</cell><cell></cell><cell></cell></row><row><cell cols="5">Table 3. Surface normal accuracy on NYUv2 [33]. The proposed</cell><cell></cell><cell></cell></row><row><cell cols="5">method shows state-of-the-art performance. (top) The networks</cell><cell></cell><cell></cell></row><row><cell cols="5">are trained on NYUv2. (bottom) The networks are trained on</cell><cell></cell><cell></cell></row><row><cell cols="5">ScanNet [4] and tested on NYUv2 without fine-tuning.</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Method AUSC ?Table 5 .Table 6 .</head><label>AUSC56</label><figDesc>AUSE ? mean rmse 11.25 ? mean rmse 11.25 ? NLL-vonMF) 7.03 10.96 14.24 2.11 4.80 5.10 Ours (NLL-AngMF) 6.83 10.92 13.47 2.13 4.98 5.01 Quantitative evaluation of uncertainty on NYUv2 [33]. mean rmse 11.25 ? mean rmse 11.25 ? NLL-vonMF) 5.84 9.30 10.31 1.85 4.38 4.69 Ours (NLL-AngMF) 5.64 9.07 9.48 1.88 4.38 4.47 Quantitative evaluation of uncertainty on ScanNet [4].</figDesc><table><row><cell>Drop</cell><cell cols="2">9.01 15.84 19.32 4.02 9.61 10.23</cell></row><row><cell>Aug</cell><cell cols="2">8.64 15.08 18.75 3.93 9.14 10.25</cell></row><row><cell>Drop + Aug</cell><cell cols="2">8.16 14.32 16.73 3.22 8.15 7.75</cell></row><row><cell>Ours (Method</cell><cell>AUSC ?</cell><cell>AUSE ?</cell></row><row><cell>Drop</cell><cell cols="2">7.25 12.51 13.95 3.24 7.55 8.58</cell></row><row><cell>Aug</cell><cell cols="2">7.06 12.58 13.72 3.32 7.92 8.81</cell></row><row><cell>Drop + Aug</cell><cell cols="2">6.87 12.07 12.73 2.93 7.20 7.49</cell></row><row><cell>Ours (</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>Methodmean median rmse 5.0 ? 7.5 ? 11.25 ? 22.5 ? 30 ?</figDesc><table><row><cell cols="2">TiltedSN[6] 12.6</cell><cell>6.0</cell><cell>21.1 42.8 57.5</cell><cell>69.3</cell><cell>83.9 88.6</cell></row><row><cell>Ours</cell><cell>11.8</cell><cell>5.7</cell><cell>20.0 45.1 59.6</cell><cell>71.1</cell><cell>85.4 89.8</cell></row><row><cell>Difference</cell><cell>-0.8</cell><cell>-0.3</cell><cell>-1.1 +2.3 +2.1</cell><cell>+1.8</cell><cell>+1.5 +1.2</cell></row><row><cell cols="6">Table 7. Quantitative comparison against TiltedSN [6] on ScanNet [4].</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>mean median rmse 11.25 ? 22.5 ? 30.0 ? mean median rmse 11.25 ? 22.5 ? 30.0 ? NLL-vonMF) 7.03 4.47 10.96 14.24 5.51 3.53 2.11 0.56 4.80 5.10 2.92 2.24 Ours (NLL-AngMF) 6.83 4.25 10.92 13.47 5.27 3.45 2.13 0.56 4.98 5.01 2.86 2.22</figDesc><table><row><cell></cell><cell>AUSC ?</cell><cell>AUSE ?</cell></row><row><cell>Drop</cell><cell cols="2">9.01 4.91 15.84 19.32 8.66 6.07 4.02 0.91 9.61 10.23 6.10 4.76</cell></row><row><cell>Aug</cell><cell cols="2">8.64 4.68 15.08 18.75 8.26 5.64 3.93 0.97 9.14 10.25 5.84 4.42</cell></row><row><cell>Drop + Aug</cell><cell cols="2">8.16 4.68 14.32 16.73 7.18 4.97 3.22 0.73 8.15 7.75 4.65 3.68</cell></row><row><cell>Ours (</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 8 .</head><label>8</label><figDesc>Quantitative evaluation of uncertainty on NYUv2 [33]. mean median rmse 11.25 ? 22.5 ? 30.0 ? mean median rmse 11.25 ? 22.5 ? 30.0 ? NLL-vonMF) 5.84 3.92 9.30 10.31 3.21 1.94 1.85 0.64 4.38 4.69 1.86 1.30 Ours (NLL-AngMF) 5.64 3.73 9.07 9.48 3.11 1.90 1.88 0.66 4.38 4.47 1.88 1.29</figDesc><table><row><cell>Method</cell><cell>AUSC ?</cell><cell>AUSE ?</cell></row><row><cell>Drop</cell><cell cols="2">7.25 4.35 12.51 13.95 5.49 3.60 3.24 1.02 7.55 8.58 4.14 2.94</cell></row><row><cell>Aug</cell><cell cols="2">7.06 4.08 12.58 13.72 5.36 3.48 3.32 1.03 7.92 8.81 4.13 2.87</cell></row><row><cell>Drop + Aug</cell><cell cols="2">6.87 4.17 12.07 12.73 4.82 3.13 2.93 0.92 7.20 7.49 3.51 2.49</cell></row><row><cell>Ours (</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 9 .</head><label>9</label><figDesc>Quantitative evaluation of uncertainty on ScanNet<ref type="bibr" target="#b3">[4]</ref>.</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary material</head><p>In this supplementary material, we provide (A) the derivations for the proposed AngMF distribution, (B) quantitative evaluation with additional metrics, (C) cross-dataset evaluation on KITTI <ref type="bibr" target="#b12">[13]</ref> and DAVIS <ref type="bibr" target="#b28">[29]</ref>, (D) discussion on failure modes and (E) additional qualitative comparison against the state-of-the-art.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Marr revisited: 2d-3d alignment via surface normal prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aayush</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>of IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Adabins: Depth estimation using adaptive bins</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ibraheem</forename><surname>Shariq Farooq Bhat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Alhashim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wonka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>of IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Fast and accurate deep network learning by exponential linear units (elus)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Djork-Arn?</forename><surname>Clevert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Scannet: Richly-annotated 3d reconstructions of indoor scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Angel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manolis</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maciej</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Halber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nie?ner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>of IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page">21</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Aleatory or epistemic? does it matter? Structural Safety</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armen</forename><surname>Der Kiureghian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ove</forename><surname>Ditlevsen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="105" to="112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Surface normal estimation of tilted images via spatial rectifier</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tien</forename><surname>Do</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khiem</forename><surname>Vuong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Stergios</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyun</forename><forename type="middle">Soo</forename><surname>Roumeliotis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of European Conference on Computer Vision (ECCV)</title>
		<meeting>of European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page">21</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Predicting depth, surface normals and semantic labels with a common multi-scale convolutional architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>of IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Statistical analysis of spherical data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toby</forename><surname>Nicholas I Fisher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian Jj</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Embleton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993" />
			<publisher>Cambridge university press</publisher>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Datadriven 3d primitives for single image understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>David F Fouhey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martial</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>of IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Unfolding an indoor origami world</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Ford Fouhey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martial</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of European Conference on Computer Vision (ECCV)</title>
		<meeting>of European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Dropout as a bayesian approximation: Representing model uncertainty in deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yarin</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zoubin</forename><surname>Ghahramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Lightweight probabilistic deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jochen</forename><surname>Gast</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>of IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Vision meets robotics: The kitti dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Stiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Robotics Research (IJRR)</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">16</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Floors are flat: Leveraging semantics for real-time surface normal prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Hickson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karthik</forename><surname>Raveendran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alireza</forename><surname>Fathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irfan</forename><surname>Essa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE/CVF International Conference on Computer Vision Workshops</title>
		<meeting>of IEEE/CVF International Conference on Computer Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Moments of von mises and fisher distributions and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Hillen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kevin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><forename type="middle">C</forename><surname>Painter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Albert</forename><forename type="middle">D</forename><surname>Swan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Murtha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematical Biosciences &amp; Engineering</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">673</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A quantitative evaluation of confidence measures for stereo vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyan</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philippos</forename><surname>Mordohai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="2121" to="2133" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Snapshot ensembles: Train 1, get m for free</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoff</forename><surname>Pleiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">E</forename><surname>Hopcroft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Framenet: Learning local canonical frames of 3d surfaces from a single rgb image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingwei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>of IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Uncertainty estimates and multi-hypotheses networks for optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eddy</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ozgun</forename><surname>Cicek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Galesso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Osama</forename><surname>Makansi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of European Conference on Computer Vision (ECCV)</title>
		<meeting>of European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">What uncertainties do we need in bayesian deep learning for computer vision?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yarin</forename><surname>Gal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Advances in Neural Information Processing Systems (NeurIPS)</title>
		<meeting>of Advances in Neural Information essing Systems (NeurIPS)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Kaiming He, and Ross Girshick. Pointrend: Image segmentation as rendering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<idno>2020. 3</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>of IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Discriminatively trained dense surface normal estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubor</forename><surname>Ladicky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Zeisl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Pollefeys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of European Conference on Computer Vision (ECCV)</title>
		<meeting>of European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Simple and scalable predictive uncertainty estimation using deep ensembles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Balaji</forename><surname>Lakshminarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Pritzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Blundell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Advances in Neural Information Processing Systems (NeurIPS)</title>
		<meeting>of Advances in Neural Information essing Systems (NeurIPS)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Spherical regression: Learning viewpoints, surface normals and 3d rotations on n-spheres</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Efstratios</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">M</forename><surname>Cees</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Snoek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>of IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05101</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">Decoupled weight decay regularization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Distinctive image features from scaleinvariant keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="91" to="110" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinod</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Advances in Neural Information Processing Systems (NeurIPS)</title>
		<meeting>of Advances in Neural Information essing Systems (NeurIPS)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A benchmark dataset and evaluation methodology for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordi</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Mcwilliams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>of IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">On the uncertainty of self-supervised monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Poggi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Filippo</forename><surname>Aleotti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Tosi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Mattoccia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>of IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Geonet: Geometric neural network for joint depth and surface normal estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renjie</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengzhe</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>of IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Geonet++: Iterative geometric neural network with edge-aware refinement for joint depth and surface normal estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengzhe</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renjie</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">20</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Indoor segmentation and support inference from rgbd images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushmeet</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of European Conference on Computer Vision (ECCV)</title>
		<meeting>of European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">20</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Super-convergence: Very fast training of residual networks using large learning rates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Leslie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholay</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Topin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.07120</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
			<publisher>JMLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Sharing features: efficient boosting procedures for multiclass object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kevin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William T</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>of IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Surge: Surface regularized geometry estimation from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohui</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Advances in Neural Information Processing Systems (NeurIPS)</title>
		<meeting>of Advances in Neural Information essing Systems (NeurIPS)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Vplnet: Deep single view normal estimation with vanishing points and lines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Geraghty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Matzen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Szeliski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan-Michael</forename><surname>Frahm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>of IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Designing deep networks for surface normal estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Fouhey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>of IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Structure-guided ranking loss for single image depth prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Xian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Mai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiguo</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>of IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Surface normal measurement in the end effector of a drilling robot for aviation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peijiang</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qishen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianmiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengkun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Pattern-affinitive propagation across depth, surface normal and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicu</forename><surname>Sebe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>of IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
