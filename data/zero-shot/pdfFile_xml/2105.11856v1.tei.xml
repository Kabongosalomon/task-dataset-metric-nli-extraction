<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Spectrum Correction: Acoustic Scene Classification with Mismatched Recording Devices</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Micha?</forename><surname>Ko?mider</surname></persName>
							<email>m.kosmider@samsung.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Samsung R&amp;D Institute</orgName>
								<address>
									<country key="PL">Poland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Spectrum Correction: Acoustic Scene Classification with Mismatched Recording Devices</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T17:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms: channel normalization</term>
					<term>acoustic scene classifica- tion</term>
					<term>mismatched recording devices</term>
					<term>convolutional neural net- work</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Machine learning algorithms, when trained on audio recordings from a limited set of devices, may not generalize well to samples recorded using other devices with different frequency responses. In this work, a relatively straightforward method is introduced to address this problem. Two variants of the approach are presented. First requires aligned examples from multiple devices, the second approach alleviates this requirement. This method works for both time and frequency domain representations of audio recordings. Further, a relation to standardization and Cepstral Mean Subtraction is analysed. The proposed approach becomes effective even when very few examples are provided. This method was developed during the Detection and Classification of Acoustic Scenes and Events (DCASE) 2019 challenge and won the 1st place in the scenario with mismatched recording devices with the accuracy of 75%. Source code for the experiments can be found online.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Audio machine learning has many sub-fields. Music and speech processing are well established, while other such as event detection, acoustic scene classification (ASC) or behaviour classification are less common. Moreover, the more specific or unusual the task, the harder it is to find publicly available datasets.</p><p>When a dataset of audio recordings for a specific purpose is collected, it is usually done with some selected set of recording devices <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>. This is sufficient to test the performance of various algorithms, but might not be enough for practical applications. Models trained using such dataset, when evaluated on samples from the same devices, appear to work well. However, their performance deteriorates when applied on samples from devices that were not in the training set or were underrepresented. This suggests that machine learning algorithms do not generalize across samples recorded using different devices <ref type="bibr" target="#b3">[3,</ref><ref type="bibr" target="#b4">4,</ref><ref type="bibr" target="#b5">5,</ref><ref type="bibr" target="#b6">6,</ref><ref type="bibr" target="#b7">7]</ref>. In extreme cases, this forces collection of completely new datasets for the targeted devices. Severity of the problem varies between tasks. This effect is clearly visible in acoustic scene classification.</p><p>The problem of device mismatch was popularized in the Detection and Classification of Acoustic Scenes and Events (DCASE) challenge. In 2018, a task targeting this problem became part of the challenge <ref type="bibr" target="#b1">[2]</ref> and has been continued in 2019, which inspired research in this area resulting in various publications <ref type="bibr" target="#b5">[5,</ref><ref type="bibr" target="#b6">6,</ref><ref type="bibr" target="#b8">8]</ref>. Some of the solutions were based on domain adaptation <ref type="bibr" target="#b9">[9]</ref>, aggressive regularization <ref type="bibr" target="#b9">[9,</ref><ref type="bibr" target="#b10">10]</ref> and most were model ensembles <ref type="bibr" target="#b11">[11]</ref>.</p><p>Independently from DCASE challenge, an approach was proposed to use transfer learning <ref type="bibr" target="#b4">[4]</ref>. The idea was to firstly train the model using a relatively large dataset and then fine tune it using a smaller dataset containing recordings from the targeted devices.</p><p>There currently does not appear to be any general approach to address this problem for ASC. Most solutions focus on tailoring specific models to become less sensitive to characteristics of the recording device. On the other hand, in the speech processing community there are multiple well known methods for channel normalization <ref type="bibr" target="#b12">[12,</ref><ref type="bibr" target="#b3">3,</ref><ref type="bibr" target="#b13">13]</ref>. However a lot of those methods, such as RASTA <ref type="bibr" target="#b14">[14]</ref>, are leveraging specific characteristics of speech. Others such as Cepstral Mean Subtraction (CMS) <ref type="bibr" target="#b15">[15]</ref>, Feature Warping <ref type="bibr" target="#b16">[16]</ref> or more recent MVA <ref type="bibr" target="#b17">[17]</ref> are relying on quefrency domain, which is not usually applied for ASC and similar tasks. For frequency domain, there exists a recently introduced Per-Channel Energy Normalization (PCEN) <ref type="bibr" target="#b18">[18,</ref><ref type="bibr" target="#b19">19]</ref>, which additionally reduces noise. However, noise reduction may not be desirable for ASC, where we would like to retain as much information about the environment as possible. Noise and reverberation are likely to contain information that differentiates various environments. Because of these considerations, a more universal method could be beneficial.</p><p>In this work, a method addressing this problem is described. The approach is straightforward and relies on real-world recordings. It does not influence the design of the architecture of the neural network. This method was designed for ASC, but can be applied to other tasks as well. An initial test of this method was performed during the DCASE 2019 challenge. A submission utilizing this approach won the 1 st place in the scenario with mismatched recording devices with the accuracy of 75% <ref type="bibr" target="#b20">[20]</ref>.</p><p>From the acoustic point of view, the presented method appears to bear a similarity to standard microphone calibration techniques <ref type="bibr">[21]</ref>, for example to the secondary calibration of a microphone by the comparison. This method relies on a direct comparison of a microphone with a physically identical microphone having a known calibration and is repeated at each frequency of interest. These methods assume laboratory settings and require expert knowledge, which makes them hard to apply for practitioners without a background in signal processing. Databases of recordings are sometimes provided without explicitly defining the types of devices used in their creation. Also, sometimes the devices are not easily available. The proposed method is simpler, more approachable and could help with the problem of mismatched recording devices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Proposed Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Linear Distortion Model</head><p>Throughout this paper a linear distortion model will be assumed. Also all amplitudes and responses are positive and real. It can be described in the following way.</p><p>Let ws be the undistorted waveform representation of some audio signal s. Let w s,d denote the distorted representation of the same signal produced by some device d. The signal ws can be analysed in terms of its amplitude spectrum. Let A s (t, f ) denote the amplitude of the frequency f at time t in that signal. Similarly, we define A s,d (t, f ) for signal w s,d .</p><p>The assumption is that the distortion introduced by the device can be approximated using the frequency response of that device H d (f ) as:</p><formula xml:id="formula_0">A s,d (t, f ) ? H d (f ) A s (t, f ) (1) = H d (f ) Hs (f ) A s (t, f ) ,<label>(2)</label></formula><p>where Hs (f ) is the frequency response specific to the signal s and independent from the recording device, for example a room impulse response. The amplitude A s (t, f ) is a remainder after decomposing A s (t, f ) into Hs (f ) and everything else. For most tasks, we would want to eliminate both H d (f ) and Hs (f ), but for ASC preserving Hs (f ) could be desirable, because it is likely to be specific to the environment. This model is na?ve, because it does not take into account for example non-linear nor phase distortions. Nonetheless, it works well in practice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Aligned Spectrum Correction</head><p>In this work, the spectrum correction (SC) method is introduced. The goal is to transform the distortion produced by some device d into the distortion produced by some chosen reference device r by matching their spectra. The device d can be any known device. When all recording devices are known, this method can be used to transform all samples, so that they appear as if they were recorded using the reference device.</p><p>As before, let A s,d (t, f ) and As,r (t, f ) denote amplitudes of frequency f in signals w d and wr, recorded using devices d and r, respectively. If device r is the reference device, the SC is derived by applying approximation from Equation (1) twice:</p><formula xml:id="formula_1">As,r (t, f ) ? Hr (f ) A s (t, f ) (3) = H d (f ) H d (f ) Hr (f ) A s (t, f )<label>(4)</label></formula><p>?</p><formula xml:id="formula_2">Hr (f ) H d (f ) A s,d (t, f ) .<label>(5)</label></formula><p>Then correction coefficients can be defined as:</p><formula xml:id="formula_3">C d?r (f ) := Hr (f ) H d (f ) .<label>(6)</label></formula><p>Note that the correction coefficients usually cannot be directly computed and have to be estimated from the data, which is explained in Section 2.3. In order to transform a recording from device d to look like a recording from device r, one can simply perform element-wise multiplication</p><formula xml:id="formula_4">As,r (t, f ) ? C d?r (f ) A s,d (t, f ) .<label>(7)</label></formula><p>The correction coefficients can be used to directly convert spectrograms. They can also be used in order to re-synthesise audio using a converted Short Time Fourier Transform (STFT). Alternatively, as shown is Section 2.8, using a finite impulse response filter, recordings can be converted directly in the time domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Computing Correction Coefficients</head><p>In order to apply spectrum correction, one needs to compute the correction coefficients. To estimate the coefficients, one can divide the amplitude spectrum from the reference device by the amplitude spectrum from the source device, element-wise. This can be inferred by transforming Equations <ref type="formula" target="#formula_2">(5)</ref> and <ref type="formula" target="#formula_3">(6)</ref> C</p><formula xml:id="formula_5">d?r (f ) = Hr (f ) H d (f ) ? mean s,t As,r (t, f ) A s,d (t, f ) .<label>(8)</label></formula><p>Here mean refers to the geometric mean, as it appears to be an appropriate choice for averaging scaling factors. Note that the mean is calculated over both time and examples. The significance will be explained in Section 2.7.</p><p>In Equation <ref type="formula" target="#formula_2">(5)</ref>, it is implicitly assumed that the two devices recorded exactly the same signal. This could be possible only if the devices were in the same point in space at the exactly same moment. In a controlled laboratory environment it is possible to approximately reproduce the same synthetic signal twice. Then by placing the two devices in the exact same location and repeating the experiment, one could obtain such recordings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Unaligned Spectrum Correction</head><p>Procedure described above requires aligned audio recordings of the same signal. For most applications such examples might not be available. However, this requirement can be easily relaxed.</p><p>Here the only information that is required is which device produced the recording. The trick is to use the geometric mean and its property that</p><formula xml:id="formula_6">C d?r (f ) ? mean s,t As,r (t, f ) A s,d (t, f ) = means,t As,r (t, f ) means,t A s,d (t, f ) .<label>(9)</label></formula><p>The averages can be computed independently. Also the recordings do not need to be aligned and can be of different signals.</p><p>The recorded signals only need to be from similar distributions (requirements are described in the following section).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.">Relation to Standardization</head><p>It is possible to omit the reference device, when multiplicative constants are of no concern. A recording converted in the following way has a characteristic that does not match any device, but is the same across all of the devices. It does not possess any practical advantages over the unaligned spectrum correction, but it is useful for theoretical analysis and simplified implementation. Based on the Equation (9) the following proportion holds</p><formula xml:id="formula_7">means,t As,r (t, f ) means,t A s,d (t, f ) ? 1 means,t A s,d (t, f ) ? C d (f ) ,<label>(10)</label></formula><p>where C d (f ) are the correction coefficients for device d. Then the conversion is performed in the following way</p><formula xml:id="formula_8">C d (f ) A s,d (t, f ) ? A s,d (t, f ) means,t A s,d (t, f ) , ?s?D d ,<label>(11)</label></formula><p>where D d is a collection of signals that were recorded using the device d. Using Equation <ref type="formula">(1)</ref>, this can be simplified to</p><formula xml:id="formula_9">A s,d (t, f ) means,t A s,d (t, f ) = H d (f ) H d (f ) A s (t, f ) means,t A s (t, f ) .<label>(12)</label></formula><p>The converted recordings are the same for all devices if means,t A s (t, f ) is independent from the device. This is true when devices recorded the same signals. However, it is also true if the signals are not the same, but are from the same distribution. It turns out that a simple z-score standardization can be implemented in such a way as to perform this simplified version. Usual way to perform standardization is to subtract the mean and then divide by the standard deviation. If all averages are assumed to be geometric, then</p><formula xml:id="formula_10">log C d (f ) A s,d (t, f ) (13) ? log ? ? ? A s,d (t, f ) s?D d A s,d (f ) 1 |D d | ? ? ? (14) = log A s,d (t, f ) ? 1 |D d | s?D d log A s,d (f ) (15) = log A s,d (t, f ) ? 1 |D d | T s?D d t log A s,d (t, f ) ,<label>(16)</label></formula><p>where A s,d (f ) denotes A s,d (t, f ) averaged over time. This result shows that mean subtraction, when working with log scaled features, such as log-spectrograms or log-melspectrograms is really a special case of the presented method. Note that for this to work the standardization has to be performed for each device and frequency separately. For example average over all examples does not create this effect. This should not be confused with spectral subtraction <ref type="bibr" target="#b21">[22]</ref>. A somewhat similar result was used for deconvolution <ref type="bibr" target="#b22">[23]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6.">Relation to Cepstral Mean Subtraction</head><p>An interesting side note is that something resembling Cepstral Mean Subtraction can be derived from Equation <ref type="formula" target="#formula_3">(16)</ref> using linearity of the inverse Fourier transform.</p><formula xml:id="formula_11">F ?1 log C d (f ) As (t, f ) = (17) F ?1 log As (t, f ) ? 1 |D d | T s?D d t F ?1 log As (t, f ) .</formula><p>The usual formulation of CMS computes average only over time for each example separately <ref type="bibr" target="#b23">[24]</ref> (see Section 2.7).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.7.">Environment Response</head><p>A detail worth pointing out is that the mean in all of the four formulations is always computed for a number of recordings, never for the current recording. For example Equation <ref type="formula" target="#formula_3">(16)</ref> has a different effect than</p><formula xml:id="formula_12">log A s,d (t, f ) ? 1 T t log A s,d (t, f ) .<label>(18)</label></formula><p>This can be seen after substituting Equation <ref type="formula" target="#formula_0">(2)</ref> into both equations. For Equation <ref type="formula" target="#formula_3">(16)</ref> only the H d (f ) will cancel out. However, for Equation <ref type="formula" target="#formula_5">(18)</ref> H d (f ) will cancel out, but so will Hs (f ). This will result in loss of information about the impulse response of the environment. Note that the equations have different effect even if D d contains only a single example.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.8.">Implementation in time domain</head><p>The procedure described in Sections 2.2 and 2.4 requires transformation of the signal into the frequency domain. It is completely unnecessary for models taking raw wave-forms as an input. Computing STFT can also be expensive and cumbersome in some situations. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Experiments</head><p>The experiments for SC were performed based on ASC. The goal of this task is to determine the environment the device is situated in, e.g. that the surroundings of a mobile phone sound like a metro station. The source code for the experiments can be found online (https://github.com/ SRPOL-AUI/spectrum-correction).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Dataset</head><p>The TAU Urban Acoustic Scenes 2019 Mobile <ref type="bibr" target="#b1">[2]</ref> dataset was used for all of the reported experiments. This was the dataset used in the DCASE 2019 challenge. It consists of 46 hours of audio recordings divided into equal 10 s samples with a sampling rate of 44.1 kHz. There are three different recording devices in the dataset. Out of all the recordings, 40 hours are from a high quality device (device A) and 3 hours from each of the two mobile devices, Samsung Galaxy S7 (device B) and iPhone SE (device C). The dataset was collected in twelve cities in Europe.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Model and Training</head><p>The classifier was a neural network consisting of five convolutional layers with ReLU activations and batch normalization <ref type="bibr" target="#b24">[25]</ref> followed by a global average pooling and a single dense layer with a softmax activation, resulting in 71k parameters. The details of this architecture <ref type="bibr" target="#b20">[20]</ref> can be seen in <ref type="table" target="#tab_0">Table 1</ref>.</p><p>The parameters of the network were optimized using Adadelta <ref type="bibr" target="#b25">[26]</ref> with a cross-entropy loss. The learning rate was reduced by half every time accuracy did not increase for 16 epochs by at least 0.1 of a percentage point. Batch size was set to 64.</p><p>Audio samples were transformed to log-melspectrograms in the following way. First, the STFT was computed using a window of 2048 samples and a hop length of 512. To avoid unnecessary computations, spectrum correction was applied at this point (except for the implementation from Equation <ref type="formula" target="#formula_3">(16)</ref>). The resulting output was then converted to mel scale with 256 mel-bins. Next, a logarithm was applied to the amplitudes. Frequency bins were separately standardized to zero mean and unit variance each, using statistics for the entire training dataset. For the implementation from Equation <ref type="bibr" target="#b16">(16)</ref> statistics are computed separately for each device. During training, mixup augmentation <ref type="bibr" target="#b26">[27]</ref> was used with alpha parameter set to 0.4. Mixing was performed without the log scaling. For all experiments device A was chosen as the reference, unless otherwise indicated. <ref type="figure" target="#fig_0">Figure 1</ref> illustrates the computed coefficients.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Time Domain and Reference Device</head><p>To validate the claim that SC can be applied directly in the time domain, a FIR filter was constructed and tested for various reference devices. The filter was created using the least squares method using 1025 taps. Results for this experiment are shown in <ref type="table" target="#tab_1">Table 2</ref>. The difference in accuracy between models trained with SC applied in the time domain using a FIR filter versus in frequency domain using STFT is negligible. It appears that a FIR filter can be used without a loss in performance. Also it is clear that the choice of the reference device does not significantly affect the results and that aligned and unaligned variants are indeed equivalent. On a side note, it is not advisable to use the simplified formulation from Section 2.5 to create the filters (due to the range of the gains).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Required Size of the Dataset</head><p>Various numbers of recordings can be used to compute the correction coefficients. It is interesting to see how many are actually needed. Performance of models trained using the correction coefficients computed using a limited number of examples is shown in <ref type="table" target="#tab_2">Table 3</ref>. It is surprising to see that the accuracy does not change with the increasing number of recordings as much as one would expect.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Comparison with Other Methods</head><p>An important question is how models trained with and without SC compare to models trained using other methods, in terms of their accuracy. Results for those experiments are shown in <ref type="table" target="#tab_3">Table 4</ref>. For all methods, best found configurations were reported.  <ref type="bibr" target="#b27">[28]</ref>. RASTA implementation is based on MATLAB implementation <ref type="bibr" target="#b28">[29]</ref>. CMS is performed in frequency domain (see Equation <ref type="formula" target="#formula_5">(18)</ref>). Methods from speech processing do not appear to perform well on ASC. However, CMS provided improvement for the mobile devices, nonetheless it deteriorated the accuracy on the main high-quality device A. This was consistent with the expectations <ref type="bibr" target="#b23">[24]</ref>. In contrast all implementations of SC resulted in significantly better accuracy for mobile devices. Interestingly, accuracy for device A also increased. It is likely caused by more efficient use of recordings from mobile devices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Summary</head><p>In this paper, a new approach to address learning with mismatched recording devices was proposed. It was experimentally shown that using just a few examples, it is possible to significantly improve the accuracy of a model. Particularly, accuracy for two mobile devices increased from 59% to 66% and from 61% to 72%, when SC was applied. It also outperforms classical methods such as RASTA or CMS, when applied on ASC. This method can be applied both in the frequency domain using the STFT, and in the time domain using FIR filters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Acknowledgements</head><p>I thank Zuzanna Kwiatkowska, S?awomir Kapka, Kornel Jankowski, Krzysztof Rykaczewski, Mateusz Matuszewski, Micha? ?opuszy?ski and Piotr Masztalski for review of this paper and Jakub Tkaczuk for help with administrative duties.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Plot of correction coefficients for devices A, B and C calculated based on 256 examples each. Device A was chosen as the reference device.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Architecture of the network.</figDesc><table><row><cell cols="3">layer outputs kernel stride</cell></row><row><cell>Conv2D+ReLU+BN 16</cell><cell>3</cell><cell>1</cell></row><row><cell>Conv2D+ReLU+BN 32</cell><cell>3</cell><cell>2</cell></row><row><cell>Conv2D+ReLU+BN 32</cell><cell>3</cell><cell>1</cell></row><row><cell>Conv2D+ReLU+BN 64</cell><cell>3</cell><cell>2</cell></row><row><cell>Conv2D+ReLU+BN 64</cell><cell>3</cell><cell>1</cell></row><row><cell>AveragePooling 64</cell><cell>-</cell><cell>-</cell></row><row><cell>Dense+Softmax 10</cell><cell>-</cell><cell>-</cell></row><row><cell cols="3">Spectrum correction can be applied directly in the time do-</cell></row><row><cell cols="3">main. Correction coefficients can be viewed as gains for fre-</cell></row><row><cell cols="3">quency bands, then standard techniques from filter design can</cell></row><row><cell cols="3">be used to construct a finite input response (FIR) filter.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Accuracy computed as macro average for devices A, B and C. Columns represent different choices for the reference device. Spectrum correction is applied both in the frequency domain (STFT) and in the time domain (FIR).</figDesc><table><row><cell></cell><cell cols="2">reference device</cell></row><row><cell cols="2">method variant A</cell><cell>B</cell><cell>C</cell></row><row><cell>aligned STFT</cell><cell cols="3">70.1% 69.4% 69.6%</cell></row><row><cell>aligned FIR</cell><cell cols="3">70.6% 70.2% 70.4%</cell></row><row><cell>unaligned STFT</cell><cell cols="3">70.2% 69.7% 69.5%</cell></row><row><cell>unaligned FIR</cell><cell cols="3">70.7% 70.1% 70.5%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Accuracy for devices A, B and C with respect to the number of recordings used to compute the correction coefficients. Models were trained for 100 epochs.</figDesc><table><row><cell cols="3">number of recordings</cell><cell></cell></row><row><cell>device 1</cell><cell>4</cell><cell>16</cell><cell>128</cell></row><row><cell cols="4">A 72.2% 73.1% 72.4% 72.9%</cell></row><row><cell cols="4">B 65.6% 63.7% 64.1% 64.6%</cell></row><row><cell cols="4">C 69.0% 70.0% 69.6% 71.1%</cell></row><row><cell cols="4">RASTA used alternative with pole at 0.98. For PCEN (a)</cell></row><row><cell cols="4">T = 0.01 s, ? = 10 ?6 , ? = 0.95, ? = 2 and r = 0.5. For</cell></row><row><cell cols="4">PCEN (b) T = 0.2 s, ? = 1.0. The two variants for PCEN trade</cell></row><row><cell cols="4">off accuracy between device A and the mobile devices. Both</cell></row><row><cell cols="2">implementations use Librosa</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Accuracy for devices A, B and C for models trained with and without SC and using other methods. Models were trained for 100 epochs. RASTA 60.2% 59.7% 60.9% 60.1% PCEN (a) 69.5% 55.4% 57.0% 60.6% PCEN (b) 64.0% 58.6% 64.4% 62.3% -71.6% 59.2% 61.2% 64.0% CMS 63.9% 61.4% 67.1% 64.2% SC aligned 73.4% 65.4% 71.8% 70.2% SC unaligned 73.1% 65.9% 71.4% 70.4% SC mean subtraction 73.2% 65.8% 72.2% 70.3%</figDesc><table><row><cell cols="2">source device</cell><cell></cell><cell></cell></row><row><cell>method A</cell><cell>B</cell><cell>C</cell><cell>mean</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Transcribing meetings with the amida systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dines</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">N</forename><surname>Garner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Grezl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">E</forename><surname>Hannani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Huijbregts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Karafiat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lincoln</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Wan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="486" to="498" />
			<date type="published" when="2012-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A multi-device dataset for urban acoustic scene classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mesaros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Heittola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Virtanen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Detection and Classification of Acoustic Scenes and</title>
		<meeting>the Detection and Classification of Acoustic Scenes and</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<title level="m">Workshop (DCASE2018)</title>
		<imprint>
			<date type="published" when="2018-11" />
			<biblScope unit="page" from="9" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Signal Processing for Robust Speech Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>Stern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Acero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F.-H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ohshima</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Automatic Speech and Speaker Recognition: Advanced Topics, ser. The Kluwer International Series in Engineering and Computer Science</title>
		<editor>C.-H. Lee, F. K. Soong, and K. K. Paliwal</editor>
		<meeting><address><addrLine>Boston, MA</addrLine></address></meeting>
		<imprint>
			<publisher>Springer US</publisher>
			<date type="published" when="1996" />
			<biblScope unit="page" from="357" to="384" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Domain adaptation of deep neural networks for automatic speech recognition via wireless sensors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gosztolya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gr?sz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Electrical Engineering</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="page">2016</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Acoustic scene classification with mismatched recording devices using mixture of experts layer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pernkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE International Conference on Multimedia and Expo (ICME)</title>
		<imprint>
			<date type="published" when="2019-07" />
			<biblScope unit="page" from="1666" to="1671" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Unsupervised Adversarial Domain Adaptation for Acoustic Scene Classification,&quot; in Detection and Classification of Acoustic Scenes and Events</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gharib</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Drossos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Cakir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Serdyuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Virtanen</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1808.05777" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
		<respStmt>
			<orgName>Tampere University of Technology</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Effects of device mismatch, language mismatch and environmental mismatch on speaker verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">M</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2007 IEEE International Conference on Acoustics, Speech and Signal Processing -ICASSP &apos;07</title>
		<imprint>
			<date type="published" when="2007-04" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="301" to="304" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Acoustic Scene Classification for Mismatched Recording Devices Using Heated-Up Softmax and Spectrum Correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pernkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kosmider</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2020 -2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<date type="published" when="2020-05" />
			<biblScope unit="page" from="2379" to="190" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Acoustic scene classification and audio tagging with receptive-field-regularized CNNs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Eghbal-Zadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Koutini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Widmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">DCASE2019 Challenge, Tech. Rep</title>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Acoustic scene classification using deep residual networks with late fusion of separated high and low frequency paths</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mcdonnell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">DCASE2019 Challenge, Tech. Rep</title>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Acoustic scene classification using a convolutional neural network ensemble and nearest neighbor filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pernkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">DCASE2018 Challenge, Tech. Rep</title>
		<imprint>
			<date type="published" when="2018-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Robustness in Language and Speech Technology, ser. Text, Speech and Language Technology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-C</forename><surname>Junqua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Van Noord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ide</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>V?ronis</surname></persName>
		</author>
		<idno type="DOI">http:/link.springer.com/10.1007/978-94-015-9719-7</idno>
		<ptr target="http://link.springer.com/10.1007/978-94-015-9719-7" />
		<imprint>
			<date type="published" when="2001" />
			<publisher>Springer</publisher>
			<biblScope unit="volume">17</biblScope>
			<pubPlace>Dordrecht; Netherlands</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Analysis of Feature Extraction and Channel Compensation in a GMM Speaker Recognition System</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Matejka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Schwarz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Glembek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Cernocky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1979" to="1986" />
			<date type="published" when="2007-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">RASTA processing of speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hermansky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Morgan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Speech and Audio Processing</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="578" to="589" />
			<date type="published" when="1994-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Effectiveness of linear prediction characteristics of the speech wave for automatic speaker identification and verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">S</forename></persName>
		</author>
		<idno type="DOI">http:/asa.scitation.org/doi/10.1121/1.1914702</idno>
		<ptr target="http://asa.scitation.org/doi/10.1121/1.1914702" />
	</analytic>
	<monogr>
		<title level="j">The Journal of the Acoustical Society of America</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1304" to="1312" />
			<date type="published" when="1974-06" />
			<publisher>Acoustical Society of America</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Feature Warping for Robust Speaker Verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pelecanos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sridharan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">A Speaker Odyssey: The Speaker Recognition Workshop Meeting Name: 2001 A Speaker Odyssey: The Speaker Recognition Workshop</title>
		<meeting><address><addrLine>Crete, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="213" to="218" />
		</imprint>
	</monogr>
	<note>Proceedings of 2001 A Speaker Odyssey: The Speaker Recognition Workshop</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">MVA Processing of Speech Features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-P</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Bilmes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">conference Name: IEEE Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<date type="published" when="2007-01" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="257" to="270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Trainable Frontend For Robust and Far-Field Keyword Spotting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Getreuer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hughes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">F</forename><surname>Lyon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Saurous</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE ICASSP 2017</title>
		<meeting>IEEE ICASSP 2017<address><addrLine>New Orleans, LA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Per-Channel Energy Normalization: Why and How</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lostanlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Salamon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cartwright</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mcfee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farnsworth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kelling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Bello</surname></persName>
		</author>
		<ptr target="https://ieeexplore.ieee.org/document/8514023/" />
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="39" to="43" />
			<date type="published" when="2019-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Electroacoustics -measurement microphones -part 5: Methods for pressure calibration of working standard microphones by comparison</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ko?mider</surname></persName>
		</author>
		<idno>61.pdf [21</idno>
		<ptr target="https://webstore.iec.ch/publication/24988" />
	</analytic>
	<monogr>
		<title level="j">International Electrotechnical Commission</title>
		<imprint>
			<biblScope unit="page" from="61094" to="61099" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">Tech. Rep.</note>
	<note>Calibrating neural networks for secondary recording devices</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Suppression of acoustic noise in speech using spectral subtraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Boll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">conference Name: IEEE Transactions on Acoustics, Speech, and Signal Processing</title>
		<imprint>
			<date type="published" when="1979-04" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="113" to="120" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Blind deconvolution through digital signal processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Stockham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cannon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ingebretsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="678" to="692" />
			<date type="published" when="1975-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Robust speaker recognition: A feature-based approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mammone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ramachandran</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996-10" />
			<publisher>IEEE</publisher>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">58</biblScope>
		</imprint>
	</monogr>
	<note>Signal Processing Magazine</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno>PMLR, 07-09</idno>
		<ptr target="http://proceedings.mlr.press/v37/ioffe15.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Machine Learning, ser. Proceedings of Machine Learning</title>
		<editor>Research, F. Bach and D. Blei</editor>
		<meeting>the 32nd International Conference on Machine Learning, ser. Machine Learning<address><addrLine>Lille, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-07" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Adadelta: an adaptive learning rate method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.5701</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">mixup: Beyond empirical risk minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lopez-Paz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">librosa: Audio and music signal analysis in python</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mcfee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mcvicar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Battenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Nieto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th python in science conference</title>
		<meeting>the 14th python in science conference</meeting>
		<imprint>
			<date type="published" when="2015-01" />
			<biblScope unit="page" from="18" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">PLP and RASTA (and MFCC, and inversion) in Matlab</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P W</forename><surname>Ellis</surname></persName>
		</author>
		<ptr target="http://www.ee.columbia.edu/?dpwe/resources/matlab/rastamat/" />
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
