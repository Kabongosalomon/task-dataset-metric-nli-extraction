<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DMC-Net: Generating Discriminative Motion Cues for Fast Compressed Video Action Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Shou</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Columbia University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xudong</forename><surname>Lin</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Columbia University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannis</forename><surname>Kalantidis</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Sevilla-Lara</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Univesity of Edinburgh</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Fu</forename><surname>Chang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Columbia University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicheng</forename><surname>Yan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">DMC-Net: Generating Discriminative Motion Cues for Fast Compressed Video Action Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T07:55+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Motion has shown to be useful for video understanding, where motion is typically represented by optical flow. However, computing flow from video frames is very timeconsuming. Recent works directly leverage the motion vectors and residuals readily available in the compressed video to represent motion at no cost. While this avoids flow computation, it also hurts accuracy since the motion vector is noisy and has substantially reduced resolution, which makes it a less discriminative motion representation. To remedy these issues, we propose a lightweight generator network, which reduces noises in motion vectors and captures fine motion details, achieving a more Discriminative Motion Cue (DMC) representation. Since optical flow is a more accurate motion representation, we train the DMC generator to approximate flow using a reconstruction loss and an adversarial loss, jointly with the downstream action classification task. Extensive evaluations on three action recognition benchmarks (HMDB-51, UCF-101, and a subset of Kinetics) confirm the effectiveness of our method. Our full system, consisting of the generator and the classifier, is coined as DMC-Net which obtains high accuracy close to that of using flow and runs two orders of magnitude faster than using optical flow at inference time.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Video is a rich source of visual content as it not only contains appearance information in individual frames, but also temporal motion information across consecutive frames. Previous work has shown that modeling motion is important to various video analysis tasks, such as action recognition <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b23">24]</ref>, action localization <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b26">27]</ref> and video summarization <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b29">30]</ref>. Currently, methods achieving state-of-the-art results usually follow the twostream network framework <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b48">49]</ref>, which consists of This work was partially done when Zheng Shou interned at Facebook.  <ref type="bibr" target="#b54">[55]</ref> is very fast. (b) But in order to reach high accuracy, CoViAR has to follow two-stream networks to add the costly optical flow computation, either using TV-L1 <ref type="bibr" target="#b57">[58]</ref> or PWC-Net <ref type="bibr" target="#b44">[45]</ref>. (c) The proposed DMC-Net not only operates exclusively in the compressed domain, but also is able to achieve high accuracy while being two orders of magnitude faster than methods that use optical flow. The blue box denotes the improvement room from CoViAR to CoViAR + TV-L1 Flow; x-axis is in logarithmic scale.</p><p>two Convolutional Neural Networks (CNNs), one for the decoded RGB images and one for optical flow, as shown in <ref type="figure">Figure 2a</ref>. These networks can operate on either single frames (2D inputs) or clips (3D inputs) and may utilize 3D spatiotemporal convolutions <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b48">49]</ref>.</p><p>Extracting optical flow, however, is very slow and often dominates the overall processing time of video analysis tasks. Recent work <ref type="bibr" target="#b54">[55,</ref><ref type="bibr" target="#b60">61,</ref><ref type="bibr" target="#b59">60]</ref> avoids optical flow computation by exploiting the motion information from compressed videos encoded by standards like MPEG-4 <ref type="bibr" target="#b24">[25]</ref>. Such methods utilize the motion vectors and residuals already present in the compressed video to model motion.</p><p>The recently proposed CoViAR <ref type="bibr" target="#b54">[55]</ref> method, for example, contains three independent CNNs operating over three modalities in the compressed video, i.e. RGB image of Iframe (I), low-resolution Motion Vector (MV) and Residual <ref type="figure">Figure 2</ref>: Illustrations of (a) the two-stream network <ref type="bibr" target="#b40">[41]</ref>, (b) the recent CoViAR <ref type="bibr" target="#b54">[55]</ref> method that achieves high accuracy via fusing compressed video data and optical flow, and (c) our proposed DMC-Net. Unlike CoViAR+Flow that requires video decoding of RGB images and flow estimation, our DMC-Net operates exclusively in the compressed domain at inference time while using optical flow to learn to capture discriminative motion cues at training time.</p><p>(R). The predictions from individual CNNs are combined by late fusion. CoViAR runs extremely fast while modeling motion features (see <ref type="figure">Figure 2b</ref>). However, in order to achieve state-of-the-art accuracy, late fusion with optical flow is further needed (see <ref type="figure">Figure 1</ref>). This performance gap is due to the motion vector being less informative and discriminative than flow. First, the spatial resolution of the motion vector is substantially reduced (i.e. 16x) during video encoding, and fine motion details, which are important to discriminate actions, are permanently lost. Second, employing two CNNs to process motion vectors and residuals separately ignores the strong interaction between them. Because the residual is computed as the difference between the raw RGB image and its reference frame warped by the motion vector. The residual is often well-aligned with the boundary of moving object, which is more important than the motion at other locations for action recognition according to <ref type="bibr" target="#b34">[35]</ref>. Jointly modeling motion vectors and residuals, which can be viewed as coarse-scale and fine-scale motion feature respectively, can exploit the encoded motion information more effectively.</p><p>To address those issues, we propose a novel approach to learn to generate a Discriminative Motion Cue (DMC) representation by refining the noisy and coarse motion vectors. We develop a lightweight DMC generator network that operates on stacked motion vectors and residuals. This generator requires training signals from different sources to capture discriminative motion cues and incorporate highlevel recognition knowledge. In particular, since flow contains high resolution and accurate motion information, we encourage the generated DMC to resemble optical flow by a pixel-level reconstruction loss. We also use an adversarial loss <ref type="bibr" target="#b13">[14]</ref> to approximate the distribution of optical flow. Finally, the DMC generator is also supervised by the downstream action recognition classifier in an end-to-end manner, allowing it to learn motion cues that are discriminative for recognition.</p><p>During inference, the DMC generator is extremely efficient with merely 0.23 GFLOPs, and takes only 0.106 ms per frame which is negligible compared with the time cost of using flow. In <ref type="figure">Figure 2c</ref>, we call our full model DMC-Net. Although optical flow is required during training, our method operates exclusively in the compressed domain at inference time and runs two orders of magnitude faster than methods using optical flow, as shown in <ref type="figure">Figure 1</ref>. Our contributions are summarized as follows:</p><p>? We propose DMC-Net, a novel and highly efficient framework that operates exclusively in the compressed video domain and is able to achieve high accuracy without requiring optical flow estimation.</p><p>? We design a lightweight generator network that can learn to predict discriminative motion cues by using optical flow as supervision and being trained jointly with action classifier. During inference, it runs two orders of magnitude faster than estimating flow.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Video Action Recognition. Advances in action recognition are largely driven by the success of 2D ConvNets in image recognition. The original Two-Stream Network <ref type="bibr" target="#b40">[41]</ref> employs separate 2D ConvNets to process RGB frames and optical flow, and merges their predictions by late fusion. Distinct from image, video possesses temporal structure and motion information which are important for video analysis.</p><p>This motivates researchers to model them more effectively, such as 3D ConvNets <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b4">5]</ref>, Temporal Segment Network (TSN) <ref type="bibr" target="#b51">[52]</ref>, dynamic image networks <ref type="bibr" target="#b1">[2]</ref>, and Non-Local Network <ref type="bibr" target="#b52">[53]</ref>. Despite the enormous amount of effort on modeling motion via temporal convolution, 3D ConvNets can still achieve higher accuracy when fused with optical flow <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b48">49]</ref>, which is unfortunately expensive to compute. Compressed Video Action Recognition. Recently, a number of approaches that utilize the information present in the compressed video domain have been proposed. In the pioneering works <ref type="bibr" target="#b59">[60,</ref><ref type="bibr" target="#b60">61]</ref>, Zhang et al. replace the optical flow stream in two-stream methods by a motion vector stream, but it still needed to decode RGB image for P-frame and ignored other motion-encoding modalities in compressed videos such as the residual maps. More recently, the CoViAR method <ref type="bibr" target="#b54">[55]</ref> proposed to exploit all data modalities in compressed videos, i.e. RGB I-frames, motion vectors and residuals to bypass RGB frame decoding. However, CoViAR fails to achieve performance comparable to that of two-stream methods, mainly due to the lowresolution of the motion vectors and the fact that motion vectors and residuals, although highly related, are processed by independent networks. We argue that, when properly exploited, the compressed video modalities have enough signal to allow us to capture more discriminative motion representation. We therefore explicitly learn such representation as opposed to relying on optical flow during inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Motion Representation and Optical Flow Estimation.</head><p>Traditional optical flow estimation methods explicitly model the displacement at each pixel between successive frames <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b2">3]</ref>. In the last years CNNs have successfully been trained to estimate the optical flow, including FlowNet <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b17">18]</ref>, SpyNet <ref type="bibr" target="#b33">[34]</ref> and PWC-Net <ref type="bibr" target="#b44">[45]</ref>, and achieve low End-Point Error (EPE) on challenging benchmarks, such as MPI Sintel <ref type="bibr" target="#b3">[4]</ref> and KITTI 2015 <ref type="bibr" target="#b30">[31]</ref>. Im2Flow work <ref type="bibr" target="#b12">[13]</ref> also shows optical flow can be hallucinated from still images. Recent work however, shows that accuracy of optical flow does not strongly correlate with accuracy of video recognition <ref type="bibr" target="#b35">[36]</ref>. Thus, motion representation learning methods focus more on generating discriminative motion cues. Fan et al. <ref type="bibr" target="#b9">[10]</ref> proposed to transform TV-L1 optical flow algorithm into a trainable sub-network, which can be jointly trained with downstream recognition network. Ng et al. <ref type="bibr" target="#b31">[32]</ref> employs fully convolutional ResNet model to generate pixel-wise prediction of optical flow, and can be jointly trained with recognition network. Unlike optical flow estimation methods, our method does not aim to reduce EPE error. Also different from all above methods of motion representation learning which take decoded RGB frames as input, our method refines motion vectors in the compressed domain, and requires much less model capacity to generate discriminative motion cues.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Approach</head><p>In this section, we present our approach for generating Discriminative Motion Cues (DMC) from compressed video. The overall framework of our proposed DMC-Net is illustrated in <ref type="figure">Figure 3</ref>. In Section 3.1, we introduce the basics of compressed video and the notations we use. Then we design the DMC generator network in Section 3.2. Finally we present the training objectives in Section 3.3 and discuss inference in Section 3.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Basics and Notations of Compressed Video</head><p>We follow CoViAR <ref type="bibr" target="#b54">[55]</ref> and use MPEG-4 Part2 <ref type="bibr" target="#b24">[25]</ref> encoded videos where every I-frame is followed by 11 consecutive P-frames. Three data modalities are readily available in MPEG-4 compressed video: (1) RGB image of I-frame (I); (2) Motion Vector (MV) records the displacement of each macroblock in a P-frame to its reference frame and typically a frame is divided into 16x16 macroblocks during video compression; (3) Residual (R) stores the RGB difference between a P-frame and its reference I-frame after motion compensation based on MV. For a frame of height H and width W , I and R have shape (3, H, W ) and MV has shape (2, H, W ). But note that MV has much lower resolution in effect because its values within the same macroblock are identical.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">The Discriminative Motion Cue Generator</head><p>Input of the generator. Existing compressed video based methods directly feed motion vectors into a classifier to model motion information. This strategy is not effective in modeling motion due to the characteristics of MV: (1) MV is computed based on simple block matching, making MV noisy and (2) MV has substantially lower resolution, making MV lacking fine motion details. In order to specifically handle these characteristics of MV, we aim to design a lightweight generation network to reduce noise in MV and capture more fine motion details, outputting DMC as a more discriminative motion representation.</p><p>To accomplish this goal, MV alone may not be sufficient. According to <ref type="bibr" target="#b34">[35]</ref>, the motion nearby object boundary is more important than the motion at other locations for action recognition. We also notice R is often well-aligned with the boundary of moving objects. Moreover, R is strongly correlated with MV as it is computed as the difference between the original frame and its reference I-frame compensated  <ref type="figure">Figure 3</ref>: The framework of our Discriminative Motion Cue Network (DMC-Net). Given the stacked residual and motion vector as input, the DMC generator reduces noise in the motion vector and captures more fine motion details, outputting a more discriminative motion cue representation which is used by a small classification network to classify actions. In the training stage, we train the DMC generator and the action classifier jointly using three losses. In the test stage, only the modules highlighted in pink are used.</p><p>Network Architecture GFLOPs C3D <ref type="bibr" target="#b46">[47]</ref> 38.5 Res3D-18 <ref type="bibr" target="#b47">[48]</ref> 19.3 ResNet-152 <ref type="bibr" target="#b14">[15]</ref> 11.3 ResNet-18 <ref type="bibr">[</ref>   using MV. Therefore, we propose to stack MV and R as input into the DMC generator, as shown in <ref type="figure">Figure 3</ref>. This allows utilizing the motion information in MV and R as well as the correlation between them, which cannot be modeled by separate CNNs as in the current compressed video works <ref type="bibr" target="#b54">[55,</ref><ref type="bibr" target="#b60">61,</ref><ref type="bibr" target="#b59">60]</ref>.</p><p>Generator network architecture. Quite a few deep generation networks have been proposed for optical flow estimation from RGB images. One of these works is PWC-Net <ref type="bibr" target="#b44">[45]</ref>, which achieves SoTA performance in terms of both End Point Error (EPE) and inference speed. We there-fore choose to base our generator design principles on the ones used by PWC-Net. It is worth noting that PWC-Net takes decoded RGB frames as input unlike our proposed method operating only in the compressed domain.</p><p>Directly adopting the network architecture of the flow estimator network in PWC-Net for our DMC generator leads to high GFLOPs as indicated in <ref type="table" target="#tab_1">Table 1</ref>. To achieve high efficiency, we have conducted detailed architecture search experimentally to reduce the number of filters in each convolutional layer of the flow estimator network in PWC-Net, achieving the balance between accuracy and complexity. Furthermore, since our goal is to refine MV, we propose to add a shortcut connection between the input MV and the output DMC, making the generator to directly predict the refinements which are added on MV to obtain DMC. <ref type="table" target="#tab_2">Table 2</ref> shows the network architecture of our DMC generator: 6 convolutional layers are stacked sequentially with all convolutional layers densely connected <ref type="bibr" target="#b16">[17]</ref>. Every convolutional filter has a 3x3 kernel with stride 1 and padding 1. Each convolutional layer except conv5 is followed by a Leaky ReLU <ref type="bibr" target="#b27">[28]</ref> layer, where the negative slope is 0.1. <ref type="table" target="#tab_1">Table 1</ref>, our DMC generator only requires 0.63% GFLOPs used by the flow estimator in PWC-Net if it were adopted to implement our DMC generator. Also, <ref type="table" target="#tab_1">Table 1</ref> compares our DMC generator with other popular network architectures for video analysis including framelevel models (ResNet-18 and ResNet-152 <ref type="bibr" target="#b14">[15]</ref>) and cliplevel models (C3D <ref type="bibr" target="#b46">[47]</ref> and Res3D <ref type="bibr" target="#b47">[48]</ref>). We observe that the complexity of DMC generator is orders of magnitude smaller compared to that of other architectures, which makes it running much faster. In the supplementary material, we explored a strategy of using two consecutive networks to respectively rectify errors in MV and capture fine motion details while this did not achieve better accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>As shown in</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Flow-guided, Discriminative Motion Cues</head><p>Compared to MV, optical flow exhibits more discriminative motion information because: (1) Unlike MV is computed using simple block matching, nowadays dense flow estimation is computed progressively from coarse scales to fine scales <ref type="bibr" target="#b57">[58]</ref>. (2) Unlike MV is blocky and thus misses fine details, flow keeps the full resolution of the corresponding frame. Therefore we propose to guide the training of our DMC generator using optical flow. To this end, we have explored different ways and identified three effective training losses as shown in <ref type="figure">Figure 3</ref> to be presented in the following: a flow reconstruction loss, an adversarial loss, and a downstream classification loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Optical Flow Reconstruction Loss</head><p>First, we minimize the per-pixel difference between the generated DMC and its corresponding optical flow. Following Im2Flow <ref type="bibr" target="#b12">[13]</ref> which approximates flow from a single RGB image, we use the Mean Square Error (MSE) reconstruction loss L mse defined as:</p><formula xml:id="formula_0">L mse = E x?p G DMC (x) ? G OF (x) 2 2 ,<label>(1)</label></formula><p>where p denotes the set of P-frames in the training videos, E stands for computing expectation, G DMC (x) and G OF (x) respectively denote the DMC and optical flow for the corresponding input frame x sampled from p. Since only some regions of flow contain discriminative motion cues that are important for action recognition, in the supplementary material we have explored weighting the flow reconstruction loss to encourage attending to the salient regions of flow. But this strategy does not achieve better accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Adversarial Loss</head><p>As pointed out by previous works <ref type="bibr" target="#b28">[29]</ref>, the MSE loss implicitly assumes that the target data is drawn from a Gaussian distribution and therefore tends to generate smooth and blurry outputs. This in effect results in less sharp motion representations especially around boundaries, making the generated DMC less discriminative. Generative Adversarial Networks (GAN) <ref type="bibr" target="#b13">[14]</ref> has been proposed to minimize the Jensen?Shannon divergence between the generative model and the true data distribution, making these two similar. Thus in order to help our DMC generator learn to approximate the distribution of optical flow data, we further introduce an adversarial loss. Note that unlike GAN which samples from random noise, adversarial loss samples from the input dataset, which already has large variability <ref type="bibr" target="#b28">[29]</ref>.</p><p>Let our DMC generator G DMC be the Generator in the adversarial learning process. As shown in <ref type="figure">Figure 3</ref>, a Discriminator D is introduced to compete with G DMC . D is instantiated by a binary classification network that takes as input either real optical flow or fake samples generated via our DMC generator. Then D outputs a two-dimensional vector that is passed through a softmax operation to obtain the probability P D of the input being Real, i.e. flow versus Fake, i.e. DMC. G DMC and D are trained in an alternating manner: G DMC is fixed when D is being optimized, and vice versa.</p><p>During training D, G DMC is fixed and is only used for inference. D aims to classify the generated DMC as Fake and classify flow as Real. Thus the adversarial loss for training D is:</p><formula xml:id="formula_1">L D adv =E x?p [? log P D (Fake|G DMC (x)) ? log P D (Real|G OF (x))],<label>(2)</label></formula><p>where p denotes the set of P-frames in the training set and G DMC (x) and G OF (x) respectively represent the DMC and optical flow for each input P-frame x.</p><p>During training G DMC , D is fixed. G DMC is encouraged to generate DMC that is similar and indistinguishable with flow. Thus the adversarial loss for training G DMC is:</p><formula xml:id="formula_2">L G adv = E x?p [? log P D (Real|G DMC (x))],<label>(3)</label></formula><p>which can be trained jointly with the other losses designed for training the DMC generator in an end-to-end fashion, as presented in Section 3.3.3. Through the adversarial training process, G DMC learns to approximate the distribution of flow data, generating DMC with more fine details and thus being more similar to flow. Those fine details usually capture discriminative motion cues and are thus important for action recognition. We present details of the discriminator network architecture in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.3">The Full Training Objective Function</head><p>Semantic classification loss. As our final goal is to create motion representation that is discriminative with respect to the downstream action recognition task, it is important to train the generator jointly with the follow-up action classifier. We employ the softmax loss as our action classification loss, denoted as L cls . The full training objective. Our whole model is trained with the aforementioned losses putting together in an endto-end manner. The training process follows the alternating training procedure stated in Section 3.3.2. During training the discriminator, D is trained while the DMC generator G DMC and the downstream action classifier are fixed. The full training objective is to minimize the adversarial loss operates exclusively in the compressed domain, but also is able to achieve higher accuracy than (a) while being two orders of magnitude faster than methods that use optical flow. The blue area indicates the improvement room from (a) to (b1).</p><formula xml:id="formula_3">L D adv in Equation 2.</formula><p>During training the generator G DMC , D is fixed while the DMC generator G DMC and the downstream action classifier are trained jointly with the following full training objective to be minimized:</p><formula xml:id="formula_4">L cls + ? ? L mse + ? ? L G adv ,<label>(4)</label></formula><p>where L mse is given by Equation 1, L G adv is given by Equation 3, and ?, ? are balancing weights.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Inference</head><p>As shown in <ref type="figure">Figure 3</ref>, despite having three losses jointly trained end-to-end, our DMC-Net is actually quite efficient during inference: basically first the generator outputs DMC and then the generated DMC is fed into the classification network to make action class prediction. We compare our inference speed with other methods in Section 4.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, we first detail our experimental setup, present quantitative analysis of our model, and finally compare with state-of-the-art methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets and Evaluation</head><p>UCF-101 <ref type="bibr" target="#b42">[43]</ref>. This dataset contains 13,320 videos from 101 action categories, along with 3 public train/test splits. HMDB-51 <ref type="bibr" target="#b22">[23]</ref>. This dataset contains 6,766 videos from 51 action categories, along with 3 public train/test splits. Kinetics-n50. From the original Kinetics-400 dataset <ref type="bibr" target="#b4">[5]</ref>, we construct a subset referred as Kinetics-n50 in this paper. We keep all 400 categories. For each class, we randomly sample 30 videos from the original training set as our training videos and randomly sample 20 videos from the original validation set as our testing videos. We evaluate on the full set in the supplementary material. Evaluation protocol. All videos in the above datasets have single action label out of multiple classes. Thus we evaluate top-1 video-level class prediction accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation Details</head><p>Training. For I, MV, and R, we follow the exactly same setting as used in CoViAR <ref type="bibr" target="#b54">[55]</ref>. Note that I employs ResNet-152 classifier; MV and R use ResNet-18 classifier. To ensure efficiency, DMC-Net also uses ResNet-18 to classify DMC in the whole paper unless we explicitly point out.</p><p>To allow apple-to-apple comparisons between DMC and flow, we also choose frame-level ResNet-18 classifier as the flow CNN shown in <ref type="figure">Figure 2b</ref>. TV-L1 <ref type="bibr" target="#b56">[57]</ref> is used for extracting optical flow to guide the training of our DMC-Net. All videos are resized to 340?256. Random cropping of 224?224 and random flipping are used for data augmentation. More details are in the supplementary material. Testing. For I, MV, and R, we follow the exactly same setting as in CoViAR <ref type="bibr" target="#b54">[55]</ref>: 25 frames are uniformly sampled for each video; each sampled frame has 5 crops augmented with flipping; all 250 (25?2?5) score predictions are averaged to obtain one video-level prediction. For DMC, we following the same setting except that we do not use cropping and flipping, which shows comparable accuracy but requires less computations. Finally, we follow CoViAR <ref type="bibr" target="#b54">[55]</ref> to obtain the final prediction via fusing prediction scores from all modalities (i.e. I, MV, R, and DMC).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Model Analysis</head><p>How much gain DMC-Net can improve over CoViAR? <ref type="figure" target="#fig_2">Figure 4</ref> reports accuracy on all three datasets. CoViAR + TV-L1 and CoViAR + PWC-Net follow two-stream methods to include an optical flow stream computed by TV-L1 <ref type="bibr" target="#b57">[58]</ref> and PWC-Net <ref type="bibr" target="#b44">[45]</ref> respectively. CoViAR + TV-L1 can be regard as our upper bound for improving accuracy because TV-L1 flow is used to guide the training of DMC-Net. By only introducing a lightweight DMC generator, our DMC-Net significantly improves the accuracy of CoViAR to approach CoViAR + Flow. <ref type="figure" target="#fig_3">Figure 5</ref> shows that the generated DMC has less noisy signals such as those in the background area and DMC captures fine and sharp details of motion boundary, leading to the accuracy gain over CoViAR. How effectiveness is each proposed loss? On HMDB-51, when only using the classification loss, the accuracy of DMC-Net is 60.5%; when using the classification loss and the flow reconstruction loss, the accuracy is improved to 61.5%; when further including the adversarial training loss, DMC-Net eventually achieves 61.8% accuracy. As in-   <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b14">15]</ref> and the CoViAR method <ref type="bibr" target="#b54">[55]</ref>. We consider two scenarios of forwarding multiple CNNs sequentially and concurrently, denoted by S and C respectively. We measure CoViAR's CNN forwarding time using our own implementation as mentioned in Section 4.4 and numbers are comparable to those reported in <ref type="bibr" target="#b54">[55]</ref>. (b) Comparing our DMC-Net to deep network based optical flow estimation and motion representation learning methods, whose numbers are quoted from <ref type="bibr" target="#b9">[10]</ref>. CNNs in DMC-Net are forwarded concurrently. All networks have batch size set to 1. For the classifier (denoted as Cls.), all methods use ResNet-18. dicated by previous literature <ref type="bibr" target="#b19">[20]</ref>, using an adversarial loss without a reconstruction loss often introduces artifacts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Inference Speed</head><p>Following <ref type="bibr" target="#b54">[55]</ref>, we measure the average per-frame running time, which consists of the time for data pre-processing and the time for CNN forward pass. For the CNN forward pass, both the scenarios of forwarding multiple CNNs sequentially and concurrently are considered. Detailed results can be found in <ref type="table" target="#tab_4">Table 3</ref> (a). Results of two-stream methods are quoted from <ref type="bibr" target="#b54">[55]</ref>. Due to the need of decoding compressed video into RGB frames and then computing optical flow, its pre-process takes much longer time than compressed video based methods. DMC-Net accepts the same inputs as CoViAR and thus CoViAR and DMC-Net have the same pre-processing time. As for the CNN forward-ing time of compressed video based methods, we measure CoViAR and DMC-Net using the exactly same implementation as stated in Section 4.2 and the same experimental setup: we use one NVIDIA GeForce GTX 1080 Ti and set the batch size of each CNN to 1 while in practice the speed can be further improved to utilize larger batch size. Despite adding little computational overhead on CoViAR, DMC-Net is still significantly faster than the conventional two-stream methods.</p><p>Deepflow <ref type="bibr" target="#b53">[54]</ref>, Flownet <ref type="bibr" target="#b17">[18]</ref> and PWC-Net <ref type="bibr" target="#b44">[45]</ref> have been proposed to accelerate optical flow estimation by using deep networks. TVNet <ref type="bibr" target="#b9">[10]</ref> was proposed to generate even better motion representation than flow with fast speed. Those estimated flow or generated motion representation can replace optical flow used in two-stream methods to go through a CNN for classification. We combine these meth-  <ref type="bibr" target="#b11">[12]</ref> 65.4 92.5 I3D <ref type="bibr" target="#b4">[5]</ref> 80.7 98.0 R(2+1)D <ref type="bibr" target="#b48">[49]</ref> 78.7 97.3 <ref type="table">Table 4</ref>: Accuracy averaged over all three splits on HMDB-51 and UCF-101 for both state-of-the-art compressed video based methods and decoded video based methods.</p><p>ods with a ResNet-18 classifier in <ref type="table" target="#tab_4">Table 3</ref> (b). We can see that our DMC generator runs much faster than these stateof-the-art motion representation learning methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Comparisons with Compressed Video Methods</head><p>As shown in the top section of <ref type="table">Table 4</ref>, DMC-Net outperforms all other methods that operate in the compressed video domain, i.e. CoViAR <ref type="bibr" target="#b54">[55]</ref>, EMV-CNN <ref type="bibr" target="#b59">[60]</ref> and DTMV-CNN <ref type="bibr" target="#b60">[61]</ref>. Our method outperforms methods like <ref type="bibr" target="#b59">[60,</ref><ref type="bibr" target="#b60">61]</ref> that the output of the MV classifier is trained to approximate the output of the optical flow classifier. We believe this is because of the fact that approximating the classification output directly is not ideal, as it does not explicitly address the issues that MV is noisy and low-resolutional. By generating a more discriminative motion representation DMC, we are able to get features that are highly discriminative for the downstream recognition task. Furthermore, our DMC-Net can be combined with these classification networks of high capacity and trained in an end-to-end manner. DMC-Net (I3D) replaces the classifier from ResNet-18 to I3D, achieving significantly higher accuracy and outperforming a number of methods that require video decoding.</p><p>Our supplementary material discusses the speed of I3D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Comparisons with Decoded Video Methods</head><p>In this section we compare DMC-Net to approaches that require decoding all RGB images from compressed video. Some only use the RGB images, while others adopt the twostream method <ref type="bibr" target="#b40">[41]</ref> and further require computing flow. RGB only. As shown in <ref type="table">Table 4</ref>, decoded video methods only based on RGB images can be further divided into three categories. (1) Frame-level classification: 2D CNNs like ResNet-50 and ResNet-152 <ref type="bibr" target="#b14">[15]</ref> have been experimented in <ref type="bibr" target="#b10">[11]</ref> to classify each frame individually and then employ simple averaging to obtain the video-level prediction. Due to lacking motion information, frame-level classification underperforms DMC-Net. (2) Motion representation learning: In <ref type="table">Table 4</ref>, we evaluate PWC-Net (ResNet-18) + CoViAR which feeds estimated optical flow into a ResNet-18 classifier and then fuses the prediction with CoViAR. The accuracy of PWC-Net (ResNet-18) + CoViAR is not as good as DMC-Net because our generated DMC contains more discriminative motion cues that are complementary to MV. For TVNet <ref type="bibr" target="#b9">[10]</ref>, the authors used BN-Inception <ref type="bibr" target="#b18">[19]</ref> to classify the generated motion representation and then fuse the prediction with a RGB CNN. The accuracy of TVNet is better DMC-Net (ResNet-18) thanks to using a strong classifier but is worse than our DMC-Net (I3D). (3) Spatiotemporal modeling: There are also a lot of works using CNN to model the spatio-temporal patterns across multiple RGB frames to implicitly capture motion patterns. It turns out that our DMC-Net discovers motion cues that are complementary to such spatio-temporal patterns: I3D RGB + DMC-Net (I3D) improves I3D RGB via incorporating predictions from our DMC-Net (I3D). RGB + Flow. As shown in <ref type="table">Table 4</ref>, the state-of-the-art accuracy is belonging to the two-stream methods <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b48">49]</ref>, which combine predictions made from a RGB CNN and an optical flow CNN. But as discussed in Section 4.4, extracting optical flow is quite time-consuming and thus these two-stream methods are much slower than our DMC-Net.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we introduce DMC-Net, a highly efficient deep model for video action recognition in the compressed video domain. Evaluations on 3 action recognition benchmarks lead to substantial gains in accuracy over prior work, without the assistance of computationally expensive flow. The supplementary materials can be found in the following appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Acknowledgment</head><p>Zheng Shou would like to thank the support from Wei Family Private Foundation when Zheng was at Columbia.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Appendix</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.">Data Modalities in the Compressed Domain</head><p>Prevailing video compression standards employs the Group Of Pictures (GOP) structure to encode the raw video into successive, non-overlapping GOPs. Frames or pictures within one GOP are compressed together. Each GOP begins with an I-frame (intra coded frame) whose RGB pixel values are stored. I-frame can be decoded independently with other frames.</p><p>The rest of frames within a GOP are P-frame (predictive coded frame) and/or B-frame (bi-predictive coded frame), containing motion-compensated difference information relative to the previously decoded frames. Each P-frame can only reference one frame which could be either I-frame or P-frame while each B frame can only reference two frames. In this thesis, we follow <ref type="bibr" target="#b54">[55]</ref> to focus on the low-latency scenario which only involves P-frame without B-frame. Each P-frame stores motion vectors and residual errors: during encoding, the video codec divides a P-frame into macroblocks of size such as 16x16 and find the most similar image patch in the reference frame for each macroblock; the displacement between a macroblock in P-frame and its most similar image patch in the reference frame is regarded as the corresponding motion vector, which will be used in motion compensation during decoding; the pixel differences between a macroblock in P-frame and its most similar image patch in the reference frame are denoted as residual errors. During the decoding of a P-frame, the video codec performs motion compensation which effectively warps the reference frame using the motion vectors and then adds the residual errors to the motion-compensated reference frame to reconstruct the P-frame.</p><p>Consequently, three data modalities in the compression domain are available: (1) RGB values of I-frame; (2) motion vectors and (3) residual errors of P-frame. We refer readers to <ref type="bibr" target="#b24">[25]</ref> for more details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.">More implementation details</head><p>In addition to Section 4.2 in the main paper, here we present more implementation details. Our model is implemented using PyTorch <ref type="bibr" target="#b32">[33]</ref>. We first train our DMC-Net with the adversarial loss and then train it with all losses together. We elaborate these two steps separately in the following. On all three datasets (i.e. HMDB-51, UCF-101 and Kinetics-n50), we found a generic settings can work well. We use Adam optimizer <ref type="bibr" target="#b21">[22]</ref>.</p><p>Configuration of the training with the flow reconstruction loss and the classification loss. We first train the DMC generator for 1 epoch using the flow reconstruction loss only with the classification network fixed. Then we include the classification loss to train both the generator and classifier end-to-end for 49 epochs. In the total loss (i.e. the Equation 4 in the main paper), we set ? to 10 to balance weights. The overall learning rate is set to 0.01 and it is divided by 10 whenever the total training loss plateaus. All layers in the classification network except its last layer have the learning rate set to be 100x smaller.</p><p>Configuration of the training with all losses including the adversarial loss. Then we use the above trained model as the initialization for training our whole model with all three losses including the adversarial loss. Our whole model consists of the generator, the classifier and the discriminator now. In the total loss (i.e. the Equation 4 in the main paper), we set ? to 10 and set ? to 1. The overall learning rate is set to 0.01 and it is divided by 10 whenever the total training loss plateaus. All layers in the classification network except its last layer have the learning rate set to be 100x smaller. Based on the network architectures for the discriminator used in a popular GAN implementation repository , we experimented with various number of filters in each layer and various number of layers. Finally we identified a network architecture for implementing our discriminator which achieves accuracy comparable to more complicated architectures. This discriminator's architecture consists of a stack of 2D convolutional layers with a two-way Fully Connected layer at the end, as shown in the following <ref type="figure">Figure 6</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3.">Other early fusion possibilities</head><p>As shown in <ref type="figure" target="#fig_5">Figure 7</ref>, we explore other early fusion possibilities: we duplicate the first convolution layer (i.e. conv0) of our DMC generator as conv0 mv and conv0 r to respectively process MV and R independently. Their outputs are fused before feeding into conv1 and two fusion methods are studied: element-wise addition (denoted as Add) and channel-wise concatenation (denoted as Concat). On HMDB-51, our method (i.e. directly stacking MV and R) achieves accuracy 61.80%, which is better than Add (61.32%) and Concat (61.36%). We believe this is because MV and R are strongly correlated in the original pixel space before convolution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4.">Attention-weighted flow reconstruction loss</head><p>In this section we describe a way to attend to the discriminative regions of optical flow during generating DMC. However, in our experiments we found that this idea does not offer quantitative benefit beyond the GAN method on the datasets we experimented with. Thus this idea was not included in the main paper.  <ref type="figure">Figure 6</ref>: The network architecture of our discriminator. We denote each 2D convolutional layer in the format of #filters; kernel size, stride, padding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4.1">Approach</head><p>The Mean Square Error (MSE) loss penalizes errors evenly over the whole image. In many cases, parts of optical flow contain noises, e.g. motions corresponding to background or camera motion. When reconstructing the optical flow, our DMC generator would ideally focus only on parts of the flow that contain motion cues discriminative with respect to the downstream action recognition task. Because these would be the regions of optical flow that are important for action recognition, and the regions where we would want a better reconstruction. Conversely, the reconstruction error in other regions of the optical flow, such as background, may not be important or even could be misleading. This motivates us to try to create an adaptive MSE loss, where a weight is assigned for each location of the optical flow, based on the discriminative ability of that location. To get such a set of weights for each optical flow, we utilize recent related works on network interpretation <ref type="bibr" target="#b58">[59]</ref>, including the Class Activation Map <ref type="bibr" target="#b61">[62]</ref> method and the Guided Back-Propagation <ref type="bibr" target="#b43">[44]</ref> method. Such methods were proposed with a view to highlighting discriminative regions of the input data with respect to the classification outputs and are able to calculate attention-like weights for every location of the input data.</p><p>All methods mentioned above require a trained classifier  to inspect. We therefore first train a ResNet-18 classifier network for action recognition using optical flow as input. We then use network interpretation methods to output a set of attention-like weights A ? R H?W for each input optical flow of height H and width W . These attention-like weights can be computed before training our DMC generator and then be utilized during the training of our DMC-Net. Specifically, we can extend the optical flow reconstruction loss of Equation 1 in the main paper to take into account the location-specific weights and derive the adaptively weighted flow reconstruction loss L MSE?? :</p><formula xml:id="formula_5">L MSE?? = E x?p 1 H ? W H h=1 W w=1 A h,w ? ? h,w ? ? h,w 2 2 ,<label>(5)</label></formula><p>where p denotes the set of P-frames in the training videos, ? is set to the generated DMC denoted as G DMC (x), ? is set to the corresponding optical flow denoted as G OF (x), E stands for computing expectation, A h,w denotes the learned weight for location h, w. In order to obtain A, we have explored two widely used network interpretation techniques as presented in the following.</p><p>Class Activation Map (CAM) <ref type="bibr" target="#b61">[62]</ref>. The CAM method practically switches the order of the last two layers of the trained flow classifier, i.e. the fully connected classification layer and the Global Average Pooling layer pool5. This way, the fully connected classifier can be re-purposed as a convolutional layer f cls to slide over every location of the conv5 (i.e. the layer right before the pool5)'s output, effectively producing a classification score at each location.</p><p>As the output of f cls is of low spatial dimension and each location has a wide receptive field with respect to the input flow, the high activations effectively focus on the most discriminative salient regions of the input flow. We choose the activation map corresponding to the ground truth action class as the attention map A. Finally we deal with the negative values in A via passing A through a ReLU operation, which leads to the best accuracy compared to other common normalization methods according to our experimental explorations. Note that in our experiments discussed in the following Section 7.4.2, we resize the input flow from 224x224 to 448x448 before feeding it into the classifier so that we can obtain the attention map A of higher spatial resolution (i.e. 14x14), covering more details. Further, we upsample A back to 224x224 via bilinear interpolation so that A has the same size as the generated DMC. As shown in <ref type="figure" target="#fig_7">Figure 8 (c)</ref>, the attention map generated by the CAM method can indeed highlight the salient regions of flow such as the player's hands and head. The flow values along the x direction and the y direction at the same spatial location share the same attention weight.</p><p>Guided Back-Propagation (GBP) <ref type="bibr" target="#b43">[44]</ref>. Rather than finding the salient regions, some methods <ref type="bibr" target="#b58">[59,</ref><ref type="bibr" target="#b43">44]</ref> have been proposed to determine the contribution from the input's each value to the final classification output. Since the input in our case is optical flow, the higher the contribution of a value, the more discriminative motion information this value contains. Therefore, we can obtain an attention map A of the data shape as the same as the flow (i.e. 2x224x224 in the following Section 7.4.2). Each value in A stands for the contribution of the corresponding flow's value at the same location. Specifically, we utilize the GBP <ref type="bibr" target="#b43">[44]</ref> method, which improves the De-conv method <ref type="bibr" target="#b58">[59]</ref> by combining it with the regular back-propagation pass. Concretely, we set the classification output as a one-hot vector with the ground truth class indicated and then we backpropagate the one-hot vector back to the input flow. Note that following the conventional back-propagation can only generate a generic attention map independent to the input rather than a map that is related with a specific input flow. To address this issue, GBP further integrates the De-conv method into the conventional back-propagation pass: basically whenever back-propagating gradients through a ReLU layer, GBP sets the negative gradients to 0. Finally, we pass the obtained A through a ReLU operation to set its negative values to 0. <ref type="figure" target="#fig_7">Figure 8</ref>   <ref type="table">Table 5</ref>: Accuracy on HMDB-51 averaged over 3 splits for the study of the effectiveness of attending to the discriminative regions of optical flow during training our DMC-Net.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4.2">Experimental results</head><p>Although it is reasonable and intuitive to attend to the discriminative regions of optical flow during generating DMC, this idea does not offer benefit beyond the GAN method proposed in the main paper. In <ref type="table">Table 5</ref>, DMC-Net is only trained with the flow reconstruction loss and the classification loss; DMC-Net w/ Att (CAM) is replacing the flow reconstruction loss in DMC-Net by the above attentionweighted flow reconstruction loss based on the attention map generated by the CAM method; DMC-Net w/ Att (GBP) is replacing the flow reconstruction loss in DMC-Net by the above attention-weighted flow reconstruction loss based on the attention map generated by the GBP method. We can see that DMC-Net achieves accuracy comparable with DMC-Net w/ Att (GBP) and DMC-Net w/ Att (CAM). But if we equip DMC-Net with the generative adversarial loss, denoted as DMC-Net w/ GAN, the highest accuracy can be achieved. Furthermore, we explore whether this strategy of making flow reconstruction loss attending to the discriminative regions of flow is complementary to the proposed adversarial loss. DMC-Net w/ GAN w/ Att (CAM) and DMC-Net w/ GAN w/ Att (GBP) respectively use the attention map generated by the CAM method and the GBP method to weight the flow reconstruction loss in DMC-Net w/ GAN. But this strategy of attention-weighted flow reconstruction loss hurts the accuracy of DMC-Net w/ GAN and thus is not complementary to the idea of using the adversarial loss. We believe this is because the DMC generator trained with the original flow reconstruction loss, the classification loss and the GAN loss can already capture sufficient motion information that can be learned from approximating flow and thus explicitly focusing on the discriminative regions of flow does not offer additional benefits. Consequently, we opt to do not use the attention-weighted flow reconstruction loss in the main paper. 7.5. More discussions about the speed 7.5.1 Speed of DMC-Net (I3D) <ref type="table">Table 4</ref> in the main paper shows that our DMC-Net implemented using a I3D classifier, denoted as DMC-Net (I3D), achieves much better accuracy than using a ResNet-18 classifier, denoted as DMC-Net (ResNet-18). Note that the speed of DMC-Net (I3D) and the speed of DMC-Net (ResNet-18) are not directly comparable. ResNet-18 is a frame-level classifier: given an input frame, DMC-Net (ResNet-18) can classify it with the speed at 0.76ms as reported in the <ref type="table" target="#tab_4">Table 3</ref> in the main paper.</p><p>However, I3D is a clip-level classifier: during testing, we follow <ref type="bibr" target="#b4">[5]</ref> to feed 250 frames concurrently into a I3D classifier to obtain one action class prediction. The perframe inference time of DMC-Net (I3D) is 0.79ms which is slightly slower yet very close to DMC-Net (ResNet-18) (i.e. 0.76ms). But in order to make one action prediction, DMC-Net (I3D) needs to take 0.79x250=197.5ms while DMC-Net (ResNet-18) only takes 0.76ms with the need of only one input frame.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.6.">Generalize DMC-Net to different compressed video standards</head><p>It is worthwhile pointing out that although we follow CoViAR <ref type="bibr" target="#b54">[55]</ref> to specifically use MPEG-4 video <ref type="bibr" target="#b24">[25]</ref>, in real applications it would be interesting to develop methods that can handle different video encoding formats. In the worst case, we can always convert the input video of arbitrary format into MPEG-4 first. On HMDB-51, FFmpeg [1] takes 1.13ms in average to convert one frame when processing each video sequentially, still being much faster than extracting flow for the two-stream method. <ref type="table" target="#tab_4">Table 3</ref> in the main paper shows that the per-frame inference speed of DMC-Net is 0.76ms and that of the two-stream method is more than 75ms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.7.">More ablation studies</head><p>In addition to the model analysis in the main paper's Section 4.3, to further validate our design choices, here we present more ablation studies and some strategies that are alternative to the current settings used in the main paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.7.1">End-to-end learning</head><p>In the main paper, we train the generator and the classifier in an end-to-end manner with the gradients from the classification loss propagated to not only the classifier but also the generator. An alternative training strategy is to separate the training of the generator and the training of the classifier. Concretely, we can first train the generator without the classifier and the classification loss. Then we fix the generator only for doing inference and then feed the generated DMC into the classification network to train the classifier using the classification loss only.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.7.2">Decomposed two-stage DMC generation</head><p>In the main paper, we design a lightweight network to refine Motion Vector (denoted as MV) to generate DMC. Note that MV has 224x224 spatial size but MV is composed of 16x16 macroblocks in which every pixel has the identical value. If we downsample MV by a factor of 16 (denoted as MV d ), the same amounts of motion information are still preserved. Thus the effects of our DMC generator can be considered to be two-fold: (1) correcting errors and reducing noises in MV and (2) generating fine details of discriminative motion cue during the process of upsampling MV d .</p><p>Consequently, an alternative way of designing the DMC generator is to first have an error correction network to rectify noises in MV d and then have another network to conduct upsampling from 14x14 to 224x224. As shown in <ref type="figure" target="#fig_8">Figure 9</ref> (b), given the stacked residual and MV d both of size 14x14, we have an error correction network to generate MV d of size 14x14. Then the generated MV d is resized from 14x14 to 224x224 via bilinear interpolation to obtain the MV . Finally, we feed the stacked residual and MV into an up-sampling network to generate DMC of more fine motion details. Note that in <ref type="figure" target="#fig_8">Figure 9</ref> (b) we not only measure the flow reconstruction loss between the generated DMC and the corresponding flow but also measure the flow reconstruction loss between the MV d and the downsampled flow of size 14x14.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.8.">Smoothing Motion Vector via bilinear interpolation before fed into the DMC generator</head><p>As shown in <ref type="figure" target="#fig_8">Figure 9</ref> (a), the DMC generator used in the main paper accepts the blocky MV of size 224x224 as input. Since the optical flow extract by TV-L1 is smooth rather than blocky, smoothing MV before feeding it into the generator can generate DMC of much less blocky artifacts which do not exhibit useful motion information. Therefore, instead of directly feeding the blocky MV into the DMC generator, we can make the input MV more smooth by first downsampling MV of size 224x224 to MV d of size 14x14 and then resizing MV d back to 224x224 via bilinear interpolation. The rest process follows the main paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.8.1">Experimental results</head><p>To investigate the effectiveness of the above strategies, we explore different scenarios during the training of DMC-Net using the flow reconstruction loss and the classification loss. We denote the scenario used in the main paper as Ours, which trains the generator and the classifier in an end-toend manner, generates the DMC in one single stage, and takes the blocky MV as input for the DMC generator.</p><p>First, we compare Ours to Ours w/o end-to-end which follows the above Section 7.7.1 to separate the training of the DMC generator and the training of the classifier. <ref type="table">Table 6</ref> confirms the effectiveness of the end-to-end learning strategy and therefore we use it in the main paper.</p><p>Second, we compare Ours to Ours w/ two-stage which follows the above Section 7.7.2 to decompose the DMC generation into a two-stage process. <ref type="table">Table 6</ref> shows that decomposing the DMC generation into two-stage does not offer benefit in terms of accuracy. Thus we opt to use the single network in the main paper to generate DMC via jointly correcting errors and generating fine motion details in one single step.</p><p>Third, we compare Ours to Ours w/ bilinear interp which follows the above Section 7.8 to first smooth MV via bilinear interpolation before feeding it into the DMC generator. It turns out that Ours and Ours w/ bilinear interp can generate DMC of comparably good motion cues that lead to similar accuracy. Therefore in the main paper we directly feed the blocky MV into the DMC generator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Accuracy</head><p>Ours w/o end-to-end 59.3 Ours w/ two-stage 60.6 Ours w/ bilinear interp 61.4 Ours 61.5 <ref type="table">Table 6</ref>: Accuracy on HMDB-51 averaged over 3 splits when our DMC-Net is trained with the flow reconstruction loss and the classification loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.9.">Results on the full Kinetics dataset</head><p>Due to the extremely long training time on the full Kinectics dataset using one single GPU, we directly adopt the training hyper-parameters used for the Kinetics-n50 subset. The accuracy of CoViAR is 65.37%; the accuracy of CoViAR + TV-L1 Flow is 65.43%; the accuracy of DMC-Net (ours) is 65.42%. We can observe that DMC-Net (ours) still improves CoViAR to match the performance of CoViAR + TV-L1 Flow but the performances are very close. We conjecture this is because when training on such a large-scale dataset, the models for I-frame and Residual have already seen training data of large variance and thus motion information cannot offer significantly complementary cues for distinguishing different action categories. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>51 Figure 1 :</head><label>511</label><figDesc>Comparing inference time and accuracy for different methods on HMDB-51. (a) Compressed video based method CoViAR</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Accuracy vs. speed on 3 benchmarks. Results on UCF-101 and HMDB-51 are averaged over 3 splits. (b1) and (b2) use ResNet-18 to classify flow and (c) also uses ResNet-18 to classify DMC. The proposed DMC-Net not only</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>A Cartwheel example (top) and a PlayingTabla (bottom) example. All images in one row correspond to the same frame. For the Cartwheel example, these noisy blocks in the background (highlighted by two red circles) are reduced in our DMC. For the PlayingTabla example, our DMC exhibits sharper and more discriminative motion cues around hands (highlighted by the red circle) than our DMC w/o the adversarial loss during training. Better viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :</head><label>7</label><figDesc>Different early fusion possibilities.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>(d) and (e) show the attention maps generated by the GBP method, highlighting the pixels whose values are sensitive for classifying the input optical flow as PlayingTabla. w/ GAN w/ Att (CAM) 61.5 DMC-Net w/ GAN w/ Att (GBP) 61.0</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 :</head><label>8</label><figDesc>Illustrations of the attention maps generated by CAM [62] and GBP [44] for a PlayingTabla example. (a) shows the RGB image. (b) shows the corresponding optical flow. (c) is the attention map generated using the CAM method. (d) and (e) show the attention maps generated using the GBP method for the input flow respectively along the x direction and the y direction. Better viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 9 :</head><label>9</label><figDesc>Illustrations for (a) the strategy of single one-stage DMC generation used in the main paper and (b) the strategy of decomposed two-stage DMC generation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Computational complexity of different networks. Input has height 224 and width 224.</figDesc><table><row><cell>Layer</cell><cell>Input size</cell><cell cols="2">Output size Filter config</cell></row><row><cell>conv0</cell><cell>5, 224, 224</cell><cell>8, 224, 224</cell><cell>8, 3x3, 1, 1</cell></row><row><cell cols="2">conv1 13, 224, 224</cell><cell>8, 224, 224</cell><cell>8, 3x3, 1, 1</cell></row><row><cell cols="2">conv2 21, 224, 224</cell><cell>6, 224, 224</cell><cell>6, 3x3, 1, 1</cell></row><row><cell cols="2">conv3 27, 224, 224</cell><cell>4, 224, 224</cell><cell>4, 3x3, 1, 1</cell></row><row><cell cols="2">conv4 31, 224, 224</cell><cell>2, 224, 224</cell><cell>2, 3x3, 1, 1</cell></row><row><cell cols="2">conv5 33, 224, 224</cell><cell>2, 224, 224</cell><cell>2, 3x3, 1, 1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell>: The architecture of our Discriminative Motion Cue</cell></row><row><cell>(DMC) generator network which takes stacked motion vec-</cell></row><row><cell>tor and residual as input. Input/output size follows the for-</cell></row><row><cell>mat of #channels, height, width. Filter configuration fol-</cell></row><row><cell>lows the format of #filters, kernel size, stride, padding.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table /><note>Comparisons of per-frame inference speed. (a) Comparing our DMC-Net to the two-stream methods</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">We relax the notational rigor and use G OF (x) to refer to the optical flow corresponding to the frame x, although for many optical flow algorithms the input would be a pair of frames.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Ffmpeg: A complete, cross-platform solution to record</title>
		<ptr target="https://www.ffmpeg.org/.12" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Efstratios Gavves, and Andrea Vedaldi. Action recognition with dynamic image networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hakan</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Basura</forename><surname>Fernando</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Lucas/kanade meets horn/schunck: Combining local and global optic flow methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andr?s</forename><surname>Bruhn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joachim</forename><surname>Weickert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Schn?rr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="231" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A naturalistic open source movie for optical flow evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Butler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wulff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">B</forename><surname>Stanley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conf. on Computer Vision (ECCV), Part IV</title>
		<editor>A. Fitzgibbon et al.</editor>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2003" />
			<biblScope unit="volume">7577</biblScope>
			<biblScope unit="page" from="611" to="625" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Rethinking the faster r-cnn architecture for temporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Wei</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudheendra</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Seybold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sukthankar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1130" to="1139" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Multi-fiber networks for video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunpeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannis</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianshu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Flownet: Learning optical flow with convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eddy</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Hausser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caner</forename><surname>Hazirbas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Golkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Van Der</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Smagt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2758" to="2766" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">End-to-end learning of motion representation for video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijie</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><forename type="middle">Ermon</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boqing</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="6016" to="6025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Spatiotemporal multiplier networks for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Axel</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">P</forename><surname>Wildes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Convolutional two-stream network fusion for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Axel</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Im2flow: Motion hallucination from static images for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruohan</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristen</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Determining optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">P</forename><surname>Berthold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><forename type="middle">G</forename><surname>Horn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schunck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial intelligence</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1-3</biblScope>
			<biblScope unit="page" from="185" to="203" />
			<date type="published" when="1981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Flownet 2.0: Evolution of optical flow estimation with deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eddy</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolaus</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tonmoy</forename><surname>Saikia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margret</forename><surname>Keuper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE conference on computer vision and pattern recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Image-to-image translation with conditional adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinghui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chloe</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudheendra</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Back</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Natsev</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.06950</idno>
		<title level="m">The kinetics human action video dataset</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Hmdb: a large video database for human motion recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hildegard</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hueihan</forename><surname>Jhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Est?baliz</forename><surname>Garrote</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomaso</forename><surname>Poggio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Serre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision (ICCV), 2011 IEEE International Conference on</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="2556" to="2563" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning realistic human actions from movies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcin</forename><surname>Marszalek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Rozenfeld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
	<note>CVPR 2008. IEEE Conference on</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Mpeg: A video compression standard for multimedia applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Didier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Single shot temporal action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianwei</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Shou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 ACM on Multimedia Conference</title>
		<meeting>the 2017 ACM on Multimedia Conference</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="988" to="996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianwei</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Shou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.06750</idno>
		<title level="m">Temporal convolution based action proposal: Submission to activitynet 2017</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Rectifier nonlinearities improve neural network acoustic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Maas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Awni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Hannun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. icml</title>
		<meeting>icml</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Deep multi-scale video prediction beyond mean square error. ICLR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Camille</forename><surname>Couprie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Sports video summarization based on motion analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Engin</forename><surname>Mendi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>H?lio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Coskun</forename><surname>Clemente</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bayrak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers &amp; Electrical Engineering</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="790" to="796" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Joint 3d estimation of vehicles and scene flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moritz</forename><surname>Menze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Heipke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISPRS Workshop on Image Sequence Analysis (ISA)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Actionflownet: Learning motion representation for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joe Yue-Hei</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonghyun</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1616" to="1624" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Optical flow estimation using a spatial pyramid network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Sevilla-Lara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiyi</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fatma</forename><surname>Guney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael J</forename><surname>Black</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.08416</idno>
		<title level="m">On the integration of optical flow and action recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">On the integration of optical flow and action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Sevilla-Lara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiyi</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fatma</forename><surname>Guney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">German Conference on Pattern Recognition (GCPR)</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Cdc: Convolutional-deconvolutional networks for precise temporal action localization in untrimmed videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alireza</forename><surname>Zareian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuyuki</forename><surname>Miyazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Fu</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2017 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1417" to="1426" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Autoloc: Weaklysupervised temporal action localization in untrimmed videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuyuki</forename><surname>Miyazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Fu</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Online action detection in untrimmed, streaming videos-modeling and evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junting</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuyuki</forename><surname>Miyazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hassan</forename><surname>Mansour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Vetro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><forename type="middle">Giroi</forename><surname>Nieto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Fu</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Temporal action localization in untrimmed videos via multi-stage cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Fu</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">UCF101: A dataset of 101 human actions classes from videos in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khurram</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Amir Roshan Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shah</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.0402</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">UCF101: A dataset of 101 human actions classes from videos in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<idno>CRCV-TR-12-01</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Striving for simplicity: The all convolutional net</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jost</forename><surname>Tobias Springenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Riedmiller</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6806</idno>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">PWC-Net: CNNs for optical flow using pyramid, warping, and cost volume</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Summarization of user-generated sports video by using deep action recognition features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Tejero-De Pablos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuta</forename><surname>Nakashima</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomokazu</forename><surname>Sato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naokazu</forename><surname>Yokoya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marko</forename><surname>Linna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Esa</forename><surname>Rahtu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2000" to="2011" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3d convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubomir</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="4489" to="4497" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Convnet architecture search for spatiotemporal feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Fu</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.05038</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">A closer look at spatiotemporal convolutions for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="6450" to="6459" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Action recognition with improved trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3551" to="3558" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Appearance-and-relation networks for video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Temporal segment networks: Towards good practices for deep action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Abhinav Gupta, and Kaiming He. Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Deepflow: Large displacement optical flow with deep matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philippe</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jerome</forename><surname>Revaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zaid</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Compressed video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chao-Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manzil</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hexiang</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">J</forename><surname>Manmatha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kr?henb?hl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Rethinking spatiotemporal feature learning for video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">A duality based approach for realtime tv-l 1 optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Zach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Pock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Horst</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint Pattern Recognition Symposium</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">A duality based approach for realtime tv-l1 optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Zach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Pock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Horst</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint Pattern Recognition Symposium</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Visualizing and understanding convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Real-time action recognition with enhanced motion vector cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanli</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Real-time action recognition with deeply transferred motion vector cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanli</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Learning deep features for discriminative localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agata</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aude</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
