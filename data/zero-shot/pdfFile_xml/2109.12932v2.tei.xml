<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Sparse Spatial Transformers for Few-Shot Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoxing</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Nanjing University</orgName>
								<address>
									<settlement>Nanjing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaxiong</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Nanjing University</orgName>
								<address>
									<settlement>Nanjing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaohui</forename><surname>Li</surname></persName>
							<email>yaohuili@smail.nju.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Nanjing University</orgName>
								<address>
									<settlement>Nanjing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunlin</forename><surname>Chen</surname></persName>
							<email>clchen@nju.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Nanjing University</orgName>
								<address>
									<settlement>Nanjing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Sparse Spatial Transformers for Few-Shot Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T20:43+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Learning from limited data is a challenging task since the scarcity of data leads to a poor generalization of the trained model. The classical global pooled representation is likely to lose useful local information. Recently, many few shot learning methods address this challenge by using deep descriptors and learning a pixel-level metric. However, using deep descriptors as feature representations may lose the contextual information of the image. And most of these methods deal with each class in the support set independently, which cannot sufficiently utilize discriminative information and task-specific embeddings. In this paper, we propose a novel Transformer based neural network architecture called Sparse Spatial Transformers (SSFormers), which can find task-relevant features and suppress taskirrelevant features. Specifically, we first divide each input image into several image patches of different sizes to obtain dense local features. These features retain contextual information while expressing local information. Then, a sparse spatial transformer layer is proposed to find spatial correspondence between the query image and the entire support set to select task-relevant image patches and suppress taskirrelevant image patches. Finally, we propose to use an image patch matching module for calculating the distance between dense local representations, thus to determine which category the query image belongs to in the support set. Extensive experiments on popular few-shot learning benchmarks show that our method achieves the state-of-the-art performance.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>With the availability of large-scale labeled data, visual understanding technology has made great progress in many tasks <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b33">34]</ref>. However, collecting and labeling such a large amount of data is time-consuming and laborious. Fewshot learning is committed to solving this problem, which enables deep models to have better generalization ability even on a small number of samples.</p><p>Recently, many few-shot learning methods have been  <ref type="bibr" target="#b26">[27]</ref> learns a global-level representation in an appropriate feature space and uses Euclidean distance to measure similarities. On the contrary, our model first generates dense local representations through image patches, and then uses Sparse Spatial Transformer Layer (SSTL) to select task-relevant patches, generating task-specific prototypes. Finally, similarities are obtained by matching between attentioned image patches.</p><p>proposed, which can be roughly divided into two categories: meta-learning <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b22">23]</ref> and metric-learning <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b30">31]</ref>. The goal of meta-learning is to learn how to deal with new tasks in the process of learning multiple tasks <ref type="bibr" target="#b8">[9]</ref>. Metric learning focuses on learning a good feature representation or relation measure <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b26">27]</ref>. For feature representations, most of the existing metriclearning based methods <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b30">31]</ref> adopt global features for recognition, which may cause useful local information to be lost and overwhelmed. Recently, DN4 <ref type="bibr" target="#b16">[17]</ref>, MATANet <ref type="bibr" target="#b2">[3]</ref> and DeepEMD <ref type="bibr" target="#b32">[33]</ref> adopt dense feature representations (i.e., deep descriptors) for few-shot learning tasks, which have been verified to be more expressive and effective than using global features. Another branch that enhances image representation uses the attention mechanism to align the query image with the support set. For example, Cross Attention Network (CAN) <ref type="bibr" target="#b11">[12]</ref> and SAML <ref type="bibr" target="#b10">[11]</ref> use the semantic correlation between the support set and query image to highlight the target object.</p><p>For the relation measure, existing dense feature based methods usually adopt a pixel-level metric and the query image is taken as a set of deep descriptors. For example, in DN4 <ref type="bibr" target="#b16">[17]</ref>, for each query deep descriptor, they find its nearest neighbor descriptors in each support class. Also, CovaMNet <ref type="bibr" target="#b17">[18]</ref> calculates a local similarity between each query deep descriptor and a support class by a covariance metric.</p><p>However, most existing methods use global features or deep descriptors, and they are not effective for few-shot image classification. Due to global features lose local information, and deep descriptors lose the contextual information of images. Moreover, the above methods all process each support class independently, and cannot use the context information of the entire task to generate task-specific features.</p><p>In this paper, we propose a novel transformer-based architecture for few-shot learning, called sparse spatial transformer (SSFormers), which extracts the spatial correlation between the query image and the current task (the entire support set), aiming to align task-relevant image patches and suppress task-irrelevant image patches. As shown in <ref type="figure" target="#fig_0">Figure 1</ref>, we first divide each input image into several patches and get dense local features. Second, we select taskrelevant query patches by a two-way selection function, i.e., mutual nearest neighbour <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b19">20]</ref>. And use selected query patches to align support classes. Finally, a patch matching module is proposed to measure the similarity between query images and aligned support classes. For each patch from a query image, the patch matching module calculates its similarity scores to the nearest neighbor patch in each aligned class prototype. Then, similarity scores from all query patches are accumulated as a patch-to-class similarity.</p><p>The main contributions of this work are summarized as follows:</p><p>1) We propose a novel sparse spatial transformers for few-shot learning, which can select task-relevant patches and generate a task-specific prototype.</p><p>2) We propose a patch matching module to get similarity between query images and task-specific prototypes. Experiments prove that it is more suitable for image patch-based feature representation than directly using the cosine similarity.</p><p>3) We conduct extensive experiments on popular fewshot learning benchmarks and show that the proposed model achieving competitive results compared to other state-of-the-art methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Global Feature based Methods. The traditional metriclearning based few-shot learning methods use an additional global average pooling (GAP) layer to obtain the global feature representation at the end of the backbone and utilize different metrics for classification. MatchingNet <ref type="bibr" target="#b30">[31]</ref> utilizes the cosine distance to measure the similarity between the query image and each support class. ProtoNet <ref type="bibr" target="#b26">[27]</ref> takes the empirical mean as the prototype representation of each category and uses Euclidean distance as the distance metric. RelationNet <ref type="bibr" target="#b27">[28]</ref> proposed a non-linear learnable distance metric. These methods based on global features will lose a lot of useful local information, which is harmful to classification tasks under few-shot learning settings.</p><p>Dense Feature based Methods. Another branch of metric-learning based methods uses pixel-level deep descriptors as feature representations. DN4 <ref type="bibr" target="#b16">[17]</ref> uses the knearest neighbor algorithm to obtain the pixel-level similarity between images. MATANet <ref type="bibr" target="#b2">[3]</ref> proposes a multi-scale task adaptive network to select task-relevant deep descriptors at multiple scales. DeepEMD <ref type="bibr" target="#b32">[33]</ref> proposes a differentiable earth mover's distance to calculate the similarity between image patches. Our SSFormers also belong to this method based on dense features. A major difference in our method is that we divide input images into several patches of different sizes and extract features. Compared with global features, the features extracted by our method can express local information. And compared with deep descriptors, the extracted features contain context information.</p><p>Attention based Methods. CAN <ref type="bibr" target="#b11">[12]</ref> proposes a crossattention algorithm to highlight the common objects in the image pair. SAML <ref type="bibr" target="#b10">[11]</ref> proposes a collect and select strategy to align the main objects in the image pair. RENet <ref type="bibr" target="#b12">[13]</ref> improves network generalization performance over unseen categories from a relational perspective. SSFormers also be treated among the family of transformered-based methods as they also aligned query images and support sets. Differently, our SSFormers select task-relevant patches in the query image to align support set to query image by a sparse spatial cross attention algorithm.</p><p>Transformers based Methods. FEAT <ref type="bibr" target="#b31">[32]</ref> first introduced Transformer <ref type="bibr" target="#b29">[30]</ref> to few-shot learning. FEAT utilized Transformer to conduct support set sample relationships and generate task-specific support features. CrossTrans-formers <ref type="bibr" target="#b7">[8]</ref> proposes to use a self-supervised learning algorithm to enhance the feature representation ability of the pre-trained backbone, and use a transformer to achieve alignment. Unlike them, SSFormers models the relationship between query images and support classes on a spatial scale, and introduces MNN <ref type="bibr" target="#b9">[10]</ref> to select task-relevant patches. SSFormers are more efficient and explanatory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Preliminary</head><p>We first introduce the problem definition of few-shot learning. Few-shot learning is dedicated to learning transferable knowledge between tasks and using the learned knowledge to solve new tasks. In the few-shot learning scenario, the task is usually set in the form of N-way Mshot, where N is the number of categories and M is the number of labeled samples in each category. Under this setting, the model is trained on a training set D train with a large amount of labeled data. To learn transferable knowledge, we use episodic training mechanisms to train our model. The episodic training mechanism samples batched tasks from D train for training. In each episode, we first construct query set</p><formula xml:id="formula_0">D Q = {(x q i , y q i )} N ?B i=1 and support set D S = {(x s i , y s i )} N ?M i=1 ,</formula><p>where B is a hyperparameter that we need to fix in our experiments. Typically, B is set to 15 <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b31">32]</ref>. Then our model predicts which support set category each sample in the query set belongs to. When the model training is complete, we sample tasks from unlabeled test sets D test to verify the performance of the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Our Method</head><p>In this section, we first introduce our method for generating dense local representations. Then we describe our sparse spatial transformers layer, which spatially aligns query images and support classes. Finally, we describe the patch matching module (PMM), which is used to calculate the final similarities. The overview of our framework is shown in <ref type="figure">Figure 2</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Dense Local Feature Extractor</head><p>The metric-learning based few-shot learning method aims to find an effective feature representation and a good distance metric to calculate the similarity between images. Different from the methods that use global features, local representation based methods have achieved better results because the local representation contains richer and more transferable semantic information. The difference from previous work is that our SSFormers aims to establish hierarchical local representations for spatial comparison.</p><p>As illustrated in <ref type="figure">Figure 2</ref>, dense local representations extractor F ? evenly divides the image into H ?W patches, and each image patch is individually encoded by the backbone network to generate a feature vector. The feature vectors generated by all patches constitute the dense local representations set of each image. To generate hierarchical local representations, we adopt a pyramid structure in the experiments. Thus the feature representation of an input image x can be denoted as F ? (x) ? R K?C , where C is the number of channels, and K is the number of all local patch representations. Specifically, we adopt two image patch division strategies of size 2?2 and 4?4 to obtain 20 dense local representations.</p><p>In each N-way M-shot few-shot image recognition task, for each support class, we have M samples and get M feature representations. Instead of using empirical mean of M feature representations <ref type="bibr" target="#b26">[27]</ref> to obtain the class representation, we utilize all the patches in each support class, i.e., S n ? R M K?C , where S n is the class representation of the n-th support class. The entire support set representation can be denoted as</p><formula xml:id="formula_1">S ? R N ?M K?C . Similarly, for a query image x q i , through F ? , we can get feature representa- tion q = F ? (x q i ) ? R K?C , where i = {1, ..., BN }.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Sparse Spatial Transformers Layer</head><p>Sparse spatial transformers aim to enhance the discriminant ability of local feature representations by modeling the interdependencies between different patches in the query image and the entire support set. In a N-way M-shot task, key k S and value v S are generated for support set feature S using two independent linear projection: the key projection head h k : R C ? R C and the value projection head h v :</p><formula xml:id="formula_2">R C ? R C .</formula><p>Similarly, the query image feature q is embedded using the value projection head h v and another the query projection head h q : R C ? R C to obtain value v q and query q q .</p><p>Inspired by <ref type="bibr" target="#b9">[10]</ref>, which proposed the mutual nearest neighbor (MNN) algorithm to eliminate batch effects in single-cell RNA sequencing data. We argue that if the patch with the closest semantic distance to patch q i is S j , and the patch with the closest semantic distance to patch S j is q i , then they are likely to have similar local feature, where</p><formula xml:id="formula_3">q i ? q q , i ? {1, ..., K} and S j ? k S , j ? {1, ..., N M K}.</formula><p>On the other hand, if the closest patch of patch S j is not q i , then even if the closest descriptor of q i is S j , the actual relationship between them is relatively weak. In other words, the correlation between two patches is a function of mutual perception, not a function of one-way perception. Therefore, we can use this bidirectionality to select task-relevant patches in the current task.</p><p>We first calculated semantic relation matrix between query image and each support class n, and get R n :</p><formula xml:id="formula_4">R n = q q ? k Sn ? C ? R K?M K<label>(1)</label></formula><p>To find task-relevant patches, we concatenate all semantic relation matrixes R n , n = {1, ..., N } to get R ? </p><formula xml:id="formula_5">Sample N -way M -shot task (D Q , D S ) from D train 3: Compute S = F ? (D S ) ? R N ?M K?C 4: for i in {1, ..., N B} do 5: Compute q = F ? (x q i ) ? R K?C 6:</formula><p>Compute sparse attention map by Eq. (1)-(5) 7:</p><p>Obtain task-spacific prototype v Sn|q by Eq. <ref type="formula" target="#formula_9">(6)</ref> 8:</p><p>Compute similarity P i by Eq. <ref type="formula" target="#formula_10">(7)</ref> and <ref type="formula" target="#formula_11">(8)</ref> 9: end for 10:</p><formula xml:id="formula_6">Obtain Cross Entropy loss L = N B i=1 CE(P i , y q i ) 11:</formula><p>Update parametres in SSFormers by SGD 12: end for 13: return Trained SSFormers R K?N M K . Each row in R represents the semantic similarity of each patch in the query image to all patches of all images in the support set.</p><p>Specifically, we propose a novel sparse spatial cross attention algorithm to find task-relevant patches in the query image. For each patch q i ? q q , we find its nearest neighbor n i q in k S , and then find the nearest neighbor n i S of n i q in q q . If i = n i S , then we consider q i to be a task-relevant patch. After collecting all task-relevant patches in q q , we can get the mask m = [m 1 ; ...; m K ], which can be computed as:</p><formula xml:id="formula_7">n i q = arg max j R i,j<label>(2)</label></formula><formula xml:id="formula_8">n i S = arg max k R k,n i q (3) m i = 1(i = n i S )<label>(4)</label></formula><p>where 1 is the indicator function: when i = n i S , 1 is equal to 1, otherwise it is 0. Using mask m and semantic relation matrix R n , we can get sparse attention map a n and use it to align each support class n to query image q and get taskspecific prototype v Sn|q , which can be computed as:</p><formula xml:id="formula_9">a n = m * R n ? R K?M K (5) v Sn|q = a n ? v Sn ? R K?C<label>(6)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Patch Matching Module</head><p>Patching maching module is built as a similarity metric, which does not have any parameter to train. Given query value v q ? R K?C and the aligned prototype of class n v Sn|q ? R K?C , we can get their patch-to-patch similarity matrix by:</p><formula xml:id="formula_10">D n = v q ? v Sn|q ||v q || ? ||v Sn|q || ? R K?K<label>(7)</label></formula><p>Then, for each patch in v q , we select the most similar patch in all patches from prototype v Sn|q . We sum K selected patches as the similarity between the query image and support class n:</p><formula xml:id="formula_11">P n = K i=1 max j?{1,...,K} D n i,j<label>(8)</label></formula><p>Under the N-way M-shot few-shot learning setting, we can get semantic similarity vectors P ? R N . The training procedure of SSFormers is shown in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>To evaluate the effectiveness of our method, we conduct extensive experiments on several common-used benchmarks for few-shot image recognition. In this section, we first present details about datasets and experimental settings in our network design. Then, we compare our method with the state-of-the-art methods on various few-shot learning tasks, i.e., standard few-shot learning, cross-domain fewshot learning. Finally, we conduct comprehensive ablation studies to validate each component in our network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Datasets</head><p>We conduct few-shot image recognition problems on four popular benchmarks, i.e., miniImageNet, tieredImageNet, CIFAR-FS and FC100.</p><p>miniImageNet <ref type="bibr" target="#b30">[31]</ref> is a subset randomly sampled from ImageNet and is an important benchmark in few-shot learning community. miniImageNet consists of 60,000 images in 100 categories. We follow the standard partition settings <ref type="bibr" target="#b26">[27]</ref>, where there are 64/16/20 categories for training, validation and evaluation.</p><p>tieredImageNet <ref type="bibr" target="#b23">[24]</ref> is also a subset random sampled from ImageNet, which consists of 779,165 images in 608 categories. All 608 categories are grouped into 34 broader categories. Following the same partition settings <ref type="bibr" target="#b11">[12]</ref>, we use 20/6/8 broader categories for training, validation, and evaluation respectively.</p><p>CIFAR-FS [1] is divided from CIFAR-100, which consists of 60,000 images in 100 categories. The CIFAR-FS is divided into 64, 16 and 20 for training, validation, and evaluation, respectively.</p><p>FC100 <ref type="bibr" target="#b21">[22]</ref> is also divided from CIFAR-100, which is more difficult because it is more diverse. The FC100 uses a split similar to tieredImageNet, where train, validation, and test splits contain 60, 20, and 20 classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Implementation Details</head><p>Backbone networks. For fair comparison, following <ref type="bibr" target="#b25">[26]</ref>, we employ Conv-64F and ResNet12 as our model backbone. To generate pyramid dense features, we add a global average pooling layer at the end of the backbone, such that the backbone generates a vector for each input image patch. And we slightly expand the area of the local patches in the grid by 2 times to merge the context information, which is helpful to generate the local representations.</p><p>Training details. For Conv-64F, we train it from scratch. For ResNet12, the training process can be divided into two stages: pre-training and meta-training. Following <ref type="bibr" target="#b32">[33]</ref>, we apply a pre-train strategy. The backbone networks are trained on training categories with a softmax layer. In this stage, we apply data argumentation methods to increase the generalization ability of the model, i.e., color jitter, random crop, and random horizontal flip. The backbone network in our model is initialized with pre-trained weights, which are then fine-tuned along with other components in our model. In the meta-training stage, we conduct N-way M-shot tasks on all benchmarks, i.e., 5-way 1-shot and 5-way 5-shot. Conv-64F is optimized by Adam, and the initial learning rate is set to 0.1 and decay 0.1 every 10 epochs. And ResNet12 is optimized by SGD, and the initial learning rate is set to 5e-4 and decay 0.5 every 10 epochs.</p><p>Evaluation. During the test stage, we random sample 10,000 tasks from the meta-testing set, and take averaged top-1 classification accuracy as the performance of methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Standard Few-shot Image Classification</head><p>To verify the effectiveness of our proposed SSFormers for few-shot image classification task, we conduct comprehensive experiments and compare our methods with other state-of-the-art methods. The experimental results are shown in <ref type="table">Table 1</ref> and <ref type="table" target="#tab_0">Table 2</ref>. The results show that our method achieves the best results in almost all settings.</p><p>For ImageNet derivatives, compared with the classic attention-based method CAN <ref type="bibr" target="#b11">[12]</ref>, our model is around 4.4%/4.0% better than CAN on miniImageNet with ResNet12 for 1-shot and 5-shot tasks. Compared with the previous transformers-based method FEAT <ref type="bibr" target="#b31">[32]</ref>, which uses transformers to find task-specific features in the support set, our method achieves 1.4%/2.0% improvements on tieredImageNet with ResNet12.</p><p>For CIFAR derivatives, our model also achieved competitive results. For example, our model is around 3.2%/3.6%, 16.6%/12.2% better than baseline Prototypical Nets <ref type="bibr" target="#b26">[27]</ref> on CIFAR-FS and FC100 for 1-shot/5-shot tasks, respectively.</p><p>The reason why we can achieve this improvement is that SSFormers can find task-relevant patches in the current task and perform sparse spatial cross attention algorithms based on hierarchical dense representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Venue</head><p>Backbone miniImageNet tieredImageNet 5-way 1-shot 5-way 5-shot 5-way 1-shot 5-way 5-shot Prototypical Networks <ref type="bibr" target="#b26">[27]</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Semi-supervised Few-Shot Learning</head><p>We further verify the effectiveness of our model on more challenging semi-supervised few-shot learning tasks. Under semi-supervised few-shot learning settings, we can select image patches from unlabeled samples that meet the mutual perception function (Eq. (2)-(4)) with the current support set and add them to the support set to provide more support features. Specifically, the workflow of SSFormerssemi is as follows. For the support set n, we first search for all patches that satisfy the mutual perception function in unlabeled sets and put them into the set U n .Then we use U n to extend S n : S n = {S 1 n , ..., S M K n } U n . Then, we use the original SSFormers to calculate the similarity. We use the same experiment setting in <ref type="bibr" target="#b24">[25]</ref>. We use Conv-64F as our backbone and train SSFormers-semi on 300,000 tasks on miniImageNet. The results are shown in <ref type="figure">Figure 3</ref>, where SSFormers-semi shows competitive results with classical baseline methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">Ablation Study</head><p>Analysis of Our Method. Our model consists of different components: dense local feature extractor, sparse spatial transformer layer, and patch matching module. As shown in <ref type="table" target="#tab_1">Table 3</ref>  on the miniImageNet dataset. Note that when replacing the patch matching module, we calculate the cosine similarity spatially. The results show that every component in SS-Formers has a significant contribution. For example, without our sparse spatial transformer layer, the performance of the model will drop by 3.73% on 5-shot tasks.</p><p>Influence of the number of patches. During dividing input images into patches, we have to define the grid for patches. We select various grids and their combinations and conduct analysis experiments on miniImageNet. As shown</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Self Cross  <ref type="table">Table 5</ref>. The 5-way, 1-shot and 5-shot classification accuracy (%) with different attention methods on tieredImageNet.</p><p>!"#$#$%&amp;'()*+,-$.</p><p>/001+23456+! 27435+8" 7499:;</p><p>&lt;&lt;=#"&gt;-". in <ref type="table" target="#tab_2">Table 4</ref>, it is better to use a combination of grids of different sizes. A possible explanation is that the size of the main object in different images is different, and the use of a single size may lose context information, and make it difficult to generate high-level semantic representations.</p><p>Comparison with other attention methods. As shown in <ref type="table">Table 5</ref>, our model has achieved state-of-the-art in all attention-based methods. Specifically, compared with RENet <ref type="bibr" target="#b12">[13]</ref>, our SSFormers only utilize cross attention, but still leads them in both 1-shot and 5-shot tasks.</p><p>Qualitative visualizations. We perform a t-SNE visualization of embeddings that generated by SSFormers from novel query images to demonstrate the effectiveness of our method (see <ref type="figure" target="#fig_3">Figure 4</ref>). We observe that our method main-!"#$%&amp; '())*+, #-, <ref type="figure">Figure 5</ref>. Visualization of the mask and sparese attention. Given 2-way 1-shot tasks, we plot the masked query image and attentioned support images (brighter colors mean higher weight). Within each query image, we choose two image patches (red and yellow boxes), and plot the image patch that best matches in each support class. This proves that our sparse spatial transformer layer can automatically highlight task-relevant areas. tains good class discrimination compared to prototype networks, even for unseen test classes. Moreover, the features generated by our method are more discriminative and the boundaries between categories are more obvious.</p><formula xml:id="formula_12">.(-+/ 01"2- !"#$%3 !"#$%4</formula><formula xml:id="formula_13">+ GaussianBlur (? ? [0.1, 2])</formula><p>To further qualitatively evaluate the proposed SSFormers, we provide some visualization cases as shown in <ref type="figure">Figure 5</ref>. For each query image in the task, we plot the result of its mask, and it can be seen that through the mutual perception function (Eq. (2)-(4)), we can get task-relevant query image patches. For support sets, we plot the prototype generated after SSTL. It can be seen that our model can highlight task-relevant image patches and suppress task-irrelevant features.</p><p>Analyze of stability. A good model should have good robustness and be able to adapt to various environments.</p><p>For this reason, we tested the stability of our model under three different attacks. As shown in <ref type="table">Table 6</ref>, the performance of our SSFormers is relatively stable under various attacks.</p><p>Complexity analysis. The computational complexity of our SSFormers is O(N M CK 2 ), where N , M , C and K are the number of way, shot, the feature dimensionality, and the number of image patches, respectively. Compared to the complexity of CAN <ref type="bibr" target="#b11">[12]</ref>, i.e., O(N M CH 2 W 2 ), where W and H are the size of feature maps, Our method has less computational complexity because K 2 &lt; H 2 W 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this work, we argue that both global features and deep descriptors are not effective for few-shot learning since global features lose local information, and deep descriptors lose the contextual information of images. Moreover, a common embedding space fails to generate discriminative visual representations for a target task. We propose a novel sparse spatial transformers (SSFormers) for few-shot learning, which customizes task-specific prototypes via a transformer-based architecture. Experimental results demonstrate SSFormers can achieve competitive results with other state-of-the-art few-shot learning methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Broader Impact</head><p>The proposed algorithm can be applied to few-shot visual recognition fields, such as robot vision systems that must recognize new objects, camera systems that must infer the existence of new objects, and monitoring systems that must recognize new categories of objects. Since the distribution of data in the real world follows the long-tail distribution, the uncommon (lack of data) objects are common in the practical application of visual recognition system, and our algorithm can improve the robustness of visual recognition system. Although the work in this paper only focuses on the classification problem, the generalization of other tasks in visual recognition (detection, segmentation) can also be improved by using the method in this paper.</p><p>While our methods have made progress in identifying and inferring rare objects, they are still far below human level. For application scenarios with low fault tolerance, such as autonomous driving, surgery, etc., relying on the visual system's ability to correctly interpret anomalies is risky for current systems, even with the advances described in this article.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Implementation Details</head><p>Backbone architecture. We consider two backbones, as suggested in the literature for the purpose of fair comparisons. We resize each image patch to 84 ? 84 ? 3 before using the backbones.</p><p>Conv-64F. The Conv-64F <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b31">32]</ref> contains 4 repeated blocks. In each block, there is a convolutional layer with 3kernel, a Batch Normalization layer, a ReLU, and a Max pooling with size 2. We set the number of convolutional channels in each block as 64. A bit different from the literature, we add a global max pooling layer at last to reduce the dimension of the embedding and get patch embeddings.</p><p>ResNet-12. We use the 12-layer residual network in <ref type="bibr" target="#b14">[15]</ref>. The DropBlock is used in this ResNet architecture to avoid overfitting. A bit different from the ResNet-12 in <ref type="bibr" target="#b14">[15]</ref>, we apply a global average pooling after the final layer, which leads to a 640 dimensional patch embeddings.</p><p>Pre-training strategy. As mentioned before, we apply an additional pre-training strategy as suggested in <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b31">32]</ref>. The backbone network, appended with a softmax layer, is trained to classify all classes in the training class split (e.g., 64 classes in the miniImageNet) with the cross-entropy loss. In this stage, we apply color jitter, random crop, and random horizontal flip to increase the generalization ability of the model. After each epoch, we validate the performance of the pre-trained weights based on its few-shot classification performance on the model validation split. Specifically, we randomly sample 100 N-way 1-shot few-shot learning tasks (N equals the number of classes in the validation split, e.g., 16 in the miniImageNet), which contains 1 instance per class in the support set and 15 instances per class for evaluation. Based on the penultimate layer instance embeddings Algorithm 2 Pseudo code of sparse spatial transformers layer in a PyTorch-like style # key projection head h k # query projection head h q # value projection head h v Input: query feature q, support feature S Output: aligned prototype v S|q support key ? R N ?C ?M K k S = h k (S) # query query ? R C ?K q q = h q (q) # query value ? R C ?K , support value ? R N ?C ?M K v q , v S = h v (q), h v (S) # similarity matrix ? R N ?K?M K R = q q .unsqueeze(0).transpose(-1, -2) ? k S # max index q max q ? R K , k max S ? R N M K q max q = R.permute(1, 0, 2).view(K, ?1).max(-1) <ref type="bibr" target="#b0">[1]</ref> k max S = R.permute(1, 0, 2).view(K, ?1).max(-2) <ref type="bibr" target="#b0">[1]</ref> # mask m ? R K m= torch.gather(k max S , 0, q max q ) m = (m q|S == torch.arange(K)) # attention map a ? R N ?K?M K a = R * m.unsqueeze(0).unsqueeze(-1) a = dropout(nn.Softmax(dim=-1)(a)) aligned prototype feature v S|q ? R N ?K?C v S|q = a ? v S .transpose(-1, -2) return v S|q of the pre-trained weights, we utilize the nearest neighbor classifiers over the few-shot tasks and evaluate the quality of the backbone. We select the pre-trained weights with the best few-shot classification accuracy on the validation set. The pre-trained weights are used to initialize the feature extractor, and the weights of the whole model are then optimized together during the meta-training stage.</p><p>Optimization. Following the literature <ref type="bibr" target="#b31">[32]</ref>, different optimizers are used for the backbones during the model training. For the Conv-64F backbone, stochastic gradient descent with Adam optimizer is employed, with the initial learning rate set to be 0.1 and decay 0.1 every 10 epochs. For the ResNet, vanilla stochastic gradient descent with Nesterov acceleration is used with an initial rate of 5e-4 and decay 0.5 every 10 epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Pseudo Code</head><p>A pseudo code of sparse spatial transformers layer in a PyTorch-like style is shown in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Implementation of SSFormers</head><p>We provide a PyTorch implementation of SSFormers for few-shot learning. Our code is available at https: //github.com/chenhaoxing/SSFormers.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Prototypical Nets</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .Algorithm 1</head><label>21</label><figDesc>Illustration of the proposed SSFormers. We propose to generate dense local features and find task-relevant features by a sparse spatial transformers layer. Training strategy of SSFormers Require: Training set D train 1: for all iteration=1, ..., MaxIteration do 2:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>t-SNE visualization of features for 75 randomly sampled images from 5 randomly selected test classes of miniImageNet dataset. In our case, the learned embeddings provide better discrimination for unseen test classes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 2 .</head><label>2</label><figDesc>Experimental results compared with other methods on CIFAR-FS and FC100. (Top two performances are shown in red and blue.)</figDesc><table><row><cell></cell><cell cols="2">NeurIPS'17 Conv-64F</cell><cell>49.42?0.78</cell><cell>68.20?0.66</cell><cell>53.31?0.89</cell><cell>72.69?0.74</cell></row><row><cell>CovaMNet [18]</cell><cell>AAAI'19</cell><cell>Conv-64F</cell><cell>51.19?0.76</cell><cell>67.65?0.63</cell><cell>54.98?0.90</cell><cell>71.51?0.75</cell></row><row><cell>DN4 [17]</cell><cell>CVPR'19</cell><cell>Conv-64F</cell><cell>51.24?0.74</cell><cell>71.02?0.64</cell><cell>53.37?0.86</cell><cell>74.45?0.70</cell></row><row><cell>DSN [26]</cell><cell>CVPR'20</cell><cell>Conv-64F</cell><cell>51.78?0.96</cell><cell>68.99?0.69</cell><cell>53.22?0.66</cell><cell>71.06?0.55</cell></row><row><cell>DeepEMD  ? [33]</cell><cell>CVPR'20</cell><cell>Conv-64F</cell><cell>52.15?0.28</cell><cell>65.52?0.72</cell><cell>50.89?0.30</cell><cell>66.12?0.78</cell></row><row><cell>SSFormers</cell><cell>Ours</cell><cell>Conv-64F</cell><cell>55.00?0.22</cell><cell>70.55?0.17</cell><cell>55.54?0.19</cell><cell>73.72?0.21</cell></row><row><cell cols="3">Prototypical Networks [27] NeurIPS'17 ResNet12</cell><cell>62.59?0.85</cell><cell>78.60?0.16</cell><cell>68.37?0.23</cell><cell>83.43?0.16</cell></row><row><cell>CAN [12]</cell><cell cols="2">NeurIPS'19 ResNet12</cell><cell>63.85?0.48</cell><cell>79.44?0.34</cell><cell>69.89?0.51</cell><cell>84.23?0.37</cell></row><row><cell>DSN [26]</cell><cell>CVPR'20</cell><cell>ResNet12</cell><cell>62.64?0.66</cell><cell>78.83?0.45</cell><cell>67.39?0.82</cell><cell>82.85?0.56</cell></row><row><cell>DeepEMD [33]</cell><cell>CVPR'20</cell><cell>ResNet12</cell><cell>65.91?0.82</cell><cell>82.41?0.56</cell><cell>71.16?0.87</cell><cell>83.95?0.58</cell></row><row><cell>FEAT [32]</cell><cell>CVPR'20</cell><cell>ResNet12</cell><cell>66.78?0.20</cell><cell>82.05?0.14</cell><cell>70.80?0.23</cell><cell>84.79?0.16</cell></row><row><cell>GLoFA [21]</cell><cell>AAAI'21</cell><cell>ResNet12</cell><cell>66.12?0.42</cell><cell>81.37?0.33</cell><cell>69.75?0.33</cell><cell>83.58?0.42</cell></row><row><cell>ArL [35]</cell><cell>CVPR'21</cell><cell>ResNet12</cell><cell>65.21?0.58</cell><cell>80.41?0.49</cell><cell>-</cell><cell>-</cell></row><row><cell>PSST [5]</cell><cell>CVPR'21</cell><cell>ResNet12</cell><cell>64.05?0.49</cell><cell>80.24?0.45</cell><cell>-</cell><cell>-</cell></row><row><cell>RENet [13]</cell><cell>ICCV'21</cell><cell>ResNet12</cell><cell>67.60?0.44</cell><cell>82.58?0.30</cell><cell>71.61?0.51</cell><cell>85.28?0.35</cell></row><row><cell>SSFormers</cell><cell>Ours</cell><cell>ResNet12</cell><cell>67.25?0.24</cell><cell>82.75?0.20</cell><cell>72.52?0.25</cell><cell>86.61?0.18</cell></row><row><cell>Model</cell><cell>Venue</cell><cell>Backbone</cell><cell cols="2">CIFAR-FS</cell><cell cols="2">FC100</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="4">5-way 1-shot 5-way 5-shot 5-way 1-shot 5-way 5-shot</cell></row><row><cell cols="2">Prototypical Networks [27] NeurIPS'17</cell><cell>Conv-64F</cell><cell>55.50?0.70</cell><cell>72.00?0.60</cell><cell>35.30?0.60</cell><cell>48.60?0.60</cell></row><row><cell>Relation Networks [28]</cell><cell>CVPR'18</cell><cell>Conv-256F</cell><cell>55.00?1.00</cell><cell>69.30?0.80</cell><cell>-</cell><cell>-</cell></row><row><cell>R2D2 [1]</cell><cell>ICLR'19</cell><cell>Conv-512F</cell><cell>65.30?0.20</cell><cell>79.40?0.10</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">Prototypical Networks [27] NeurIPS'17</cell><cell>ResNet-12</cell><cell>72.20?0.70</cell><cell>83.50?0.50</cell><cell>37.50?0.60</cell><cell>52.50?0.60</cell></row><row><cell>TADAM [22]</cell><cell>NeurIPS'18</cell><cell>ResNet-12</cell><cell>-</cell><cell>-</cell><cell>40.10?0.40</cell><cell>56.10?0.40</cell></row><row><cell>MetaOptNet [15]</cell><cell>CVPR'19</cell><cell>ResNet-12</cell><cell>72.60?0.70</cell><cell>84.30?0.50</cell><cell>41.10?0.60</cell><cell>55.50?0.60</cell></row><row><cell>MABAS [14]</cell><cell>ECCV'20</cell><cell>ResNet-12</cell><cell>73.51?0.92</cell><cell>85.49?0.68</cell><cell>42.31?0.75</cell><cell>57.56?0.78</cell></row><row><cell>Fine-tuning [7]</cell><cell>ICLR'20</cell><cell>WRN-28-10</cell><cell>76.58?0.68</cell><cell>85.79?0.50</cell><cell>43.16?0.59</cell><cell>57.57?0.55</cell></row><row><cell>RENet [13]</cell><cell>ICCV'21</cell><cell>ResNet12</cell><cell>74.51?0.46</cell><cell>86.60?0.32</cell><cell>-</cell><cell>-</cell></row><row><cell>SSFormers</cell><cell>Ours</cell><cell>ResNet-12</cell><cell>74.50?0.21</cell><cell>86.61?0.23</cell><cell>43.72?0.21</cell><cell>58.92?0.18</cell></row></table><note>Table 1. Average classification accuracy of 5-way 1-shot and 5-way 5-shot tasks with 95% confidence intervals on miniImageNet and tieredImageNet.? denotes that it is our reimplementation under the same setting. (Top two performances are shown in red and blue.)</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3 .</head><label>3</label><figDesc>, we verify the indispensability of each component Dense Local Feature SSTL PMM Cosine Classifier 5-way 1-shot 5-way 5-shot Ablation study on our model, we can find that each part of our model has important contribution. The experiments are conducted with ResNet12 on miniImageNet. (SSTL: Sparse Spatial Transformer Layer and PMM: Patch Matching Module)</figDesc><table><row><cell>63.15?0.20</cell><cell>79.15?0.25</cell></row><row><cell>66.84?0.47</cell><cell>79.72?0.50</cell></row><row><cell>64.35?0.22</cell><cell>80.17?0.17</cell></row><row><cell>67.25?0.24</cell><cell>82.75?0.20</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 .</head><label>4</label><figDesc>The 5-way, 1-shot and 5-shot classification accuracy (%) with different number of image patches on miniImageNet.</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Meta-learning with differentiable closedform solvers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Bertinetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jo?o</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Openpose: Realtime multi-person 2d pose estimation using part affinity fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gines</forename><surname>Hidalgo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-En</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="172" to="186" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Multi-scale adaptive task attention network for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoxing</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaxiong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaohui</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunlin</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.14479</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Multi-level metric learning for few-shot image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoxing</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaxiong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaohui</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunlin</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.11383</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Pareto self-supervised training for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengyu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jixie</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heshen</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siteng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donglin</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Spot and learn: A maximum-entropy patch sampler for few-shot image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Hsuan</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Jhe</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing-Cheng</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Chiang Frank</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6251" to="6260" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Avinash Ravichandran, and Stefano Soatto. A baseline for few-shot image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guneet</forename><surname>Singh Dhillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pratik</forename><surname>Chaudhari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Crosstransformers: spatially-aware few-shot transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankush</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Modelagnostic meta-learning for fast adaptation of deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="1126" to="1135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Batch effects in single-cell rna-sequencing data are corrected by matching mutual nearest neighbors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laleh</forename><surname>Haghverdi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T L</forename><surname>Aaron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John C</forename><surname>Morgan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Marioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Biotechnol</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Collect and select: Semantic alignment metric learning for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fusheng</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fengxiang</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianzhong</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8459" to="8468" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Cross attention network for few-shot classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruibing</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingpeng</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiguang</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xilin</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="4005" to="4016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Relational embedding for few-shot classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahyun</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heeseung</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juhong</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minsu</forename><surname>Cho</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Modelagnostic boundary-adversarial sampling for test-time generalization in few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaekyeom</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyoungseok</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gunhee</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">12346</biblScope>
			<biblScope unit="page" from="599" to="617" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Meta-learning with differentiable convex optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kwonjoon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhransu</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avinash</forename><surname>Ravichandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Few-shot learning with global class representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aoxue</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiange</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiran</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9714" to="9723" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Revisiting local descriptor based image-to-class measure for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinglin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Huo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="7260" to="7268" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Distribution consistency based covariance metric networks for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinglin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Huo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr Doll?r. Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="318" to="327" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">DMN4: few-shot learning via discriminative mutual nearest neighbor neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tu</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofei</forename><surname>He</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Tailoring embedding function to heterogeneous few-shot tasks by global and local feature adaptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Su</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han-Jia</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">De-Chuan</forename><surname>Zhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="8776" to="8783" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">TADAM: task dependent adaptive metric for improved few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boris</forename><forename type="middle">N</forename><surname>Oreshkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Pau Rodr?guez L?pez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lacoste</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Optimization as a model for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sachin</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Meta-learning for semi-supervised few-shot classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengye</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eleni</forename><surname>Triantafillou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sachin</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jake</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Learn. Represent. (ICLR)</title>
		<meeting>Int. Conf. Learn. Represent. (ICLR)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Meta-learning for semi-supervised fewshot classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengye</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eleni</forename><surname>Triantafillou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sachin</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jake</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Adaptive subspaces for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Koniusz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Nock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehrtash</forename><surname>Harandi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Prototypical networks for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jake</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning to compare: Relation network for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Flood</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongxin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">M</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="1199" to="1208" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Rethinking few-shot image classification: A good embedding is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">12359</biblScope>
			<biblScope unit="page" from="266" to="282" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Matching networks for one shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="3630" to="3638" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Fewshot learning via embedding adaptation with set-to-set functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hexiang</forename><surname>Han-Jia Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>De-Chuan Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Deepemd: Few-shot image classification with differentiable earth mover&apos;s distance and structured classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujun</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guosheng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Enhanced group sparse regularized nonconvex regression for face recognition. TPAMI, 2020</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="DOI">10.1109/TPAMI.2020.3033994.1</idno>
		<imprint/>
	</monogr>
	<note>to be published</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Rethinking class relations: Absolute-relative supervised and unsupervised fewshot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongguang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Koniusz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songlei</forename><surname>Jian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongdong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="9432" to="9441" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
