<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">TRAINING STRATEGIES FOR IMPROVED LIP-READING</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pingchuan</forename><surname>Ma</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Imperial College London</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujiang</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Imperial College London</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stavros</forename><surname>Petridis</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Imperial College London</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Meta AI</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Shen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Imperial College London</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Meta AI</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maja</forename><surname>Pantic</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Imperial College London</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Meta AI</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">TRAINING STRATEGIES FOR IMPROVED LIP-READING</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T18:16+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Visual Speech Recognition</term>
					<term>Lip-reading</term>
					<term>Temporal Convolutional Network</term>
					<term>Self-Distillation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Several training strategies and temporal models have been recently proposed for isolated word lip-reading in a series of independent works. However, the potential of combining the best strategies and investigating the impact of each of them has not been explored. In this paper, we systematically investigate the performance of state-of-the-art data augmentation approaches, temporal models and other training strategies, like self-distillation and using word boundary indicators. Our results show that Time Masking (TM) is the most important augmentation followed by mixup and Densely-Connected Temporal Convolutional Networks (DC-TCN) are the best temporal model for lip-reading of isolated words. Using selfdistillation and word boundary indicators is also beneficial but to a lesser extent. A combination of all the above methods results in a classification accuracy of 93.4%, which is an absolute improvement of 4.6% over the current state-of-theart performance on the LRW dataset. The performance can be further improved to 94.1% by pre-training on additional datasets. An error analysis of the various training strategies reveals that the performance improves by increasing the classification accuracy of hard-to-recognise words.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Lip-reading of isolated words has received a lot of attention recently thanks to the availability of large publicly available datasets like LRW <ref type="bibr" target="#b0">[1]</ref>. The majority of works follow the same lip-reading pipeline consisting of a visual encoder, followed by a temporal model and a softmax classification layer. The visual encoder proposed by <ref type="bibr" target="#b1">[2]</ref> has been widely adopted in most works, hence, most recent efforts aim at improving the temporal model or the training strategy. Bidirectional Gated Recurrent Units (BGRUs) and Multi-Scale Temporal Convolutional Networks (MS-TCNs) have been the most popular temporal models in the literature and conflicting conclusions about their performance have been reported. For example, MS-TCNs outperformed BGRUs in <ref type="bibr" target="#b2">[3]</ref> but not in <ref type="bibr" target="#b3">[4]</ref>. Similarly, different data augmentations have been presented like mixup <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref>, variable length augmentation <ref type="bibr" target="#b2">[3]</ref> and cutout <ref type="bibr" target="#b5">[6]</ref>. Other improvements which have been proposed in the literature include the addition of word boundary indicators <ref type="bibr" target="#b6">[7]</ref>, which define the start and end frame of a word in a video, and self distillation <ref type="bibr" target="#b4">[5]</ref> which results in a series of networks with the same architecture trained via distillation. All these improvements have been proposed separately in the literature and a study combining all of them and investigating the impact of each of them is missing.</p><p>In this work, we present a model trained with some of the most promising recent ideas and evaluate the contribution of each of them via an ablation study. This is a useful study since we can quantify the effect of each method when combined with other augmentation methods or temporal models. We also provide an error analysis demonstrating how each method improves the lip-reading accuracy. To the best of our knowledge, the only similar study that exists is <ref type="bibr" target="#b3">[4]</ref> but despite using some of the latest methods it was only able to match the current state-of-the-art performance.</p><p>Our results demonstrate that: 1) We can achieve a new state-of-the-art performance on the LRW dataset by combining all the latest data augmentation methods, using the recently proposed DC-TCN, word boundary indicators and self-distillation. The accuracy achieved is 92.8% for a single model and 93.4% for an ensemble. The performance can be slightly improved to 93.5% and 94.1%, respectively, by pretraining on additional datasets. 2) Time Masking is the most effective augmentation method followed by mixup. The use of DC-TCN significantly outperforms the MS-TCN which in turn outperforms the BGRU model. The use of word boundary indicators and self-distillation is also beneficial with the former resulting in greater improvement. 3) The error analysis suggests that all these methods improve performance by significantly increasing the classification accuracy of difficult words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">TRAINING STRATEGIES</head><p>Architecture The first building block of the model <ref type="figure" target="#fig_0">(Fig. 1c</ref> encoder consisting of a 3D convolutional layer, which takes as input 5 consecutive frames, followed by a 2D ResNet-18 <ref type="bibr" target="#b1">[2]</ref>. The frame-wise features from the output of the encoder are then fed to a temporal model to capture the temporal dependencies. This is followed by a softmax layer which outputs the class probabilities over the words to be classified. In this work, we investigate the impact of three different temporal models for the recognition of isolated words, BGRUs <ref type="bibr" target="#b8">[9]</ref>, MS-TCNs <ref type="bibr" target="#b2">[3]</ref> and DC-TCNs <ref type="bibr" target="#b9">[10]</ref>. TCNs consist of a stack of temporal convolutional (TC) blocks , where each block consists of a few layers of dilated convolutions with kernel size k. A MS-TCN ( <ref type="figure" target="#fig_0">Fig. 1a</ref>) extends the vanilla TCN by adding multiple branches each with different kernel sizes, and the features from the output of each branch are concatenated to mix information at several temporal scales. A DC-TCN ( <ref type="figure" target="#fig_0">Fig. 1b</ref>) extends the vanilla TCN by adding dense connection at each TC block and using a Squeeze-and-Excitation (SE) attention mechanism. Data Augmentation Random Cropping: We randomly crop an 88 ? 88 patch from the mouth ROI during training. At test time, we simply crop the central patch. This is a commonly used augmentation method that has been used successfully in several lip-reading works <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b8">9]</ref>. Flipping:</p><p>We randomly flip all the frames in a video with a probability of 0.5. This augmentation is commonly used in combination with random cropping <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b8">9]</ref>. Mixup: We create new augmented training examples by linearly combining two input video sequences and their corresponding targets. We set the linear combination weight ? to be 0.4 similarly to <ref type="bibr" target="#b4">[5]</ref>. Time Masking: We mask N consecutive frames for each training sequence where N is sampled between 0 and N max using a uniform distribution. Each masked frame is replaced with the mean frame of the sequence it belongs to. This augmentation is based on SpecAugment <ref type="bibr" target="#b10">[11]</ref>, which has been proposed for ASR applications, and aims at making the model more robust to small segments with missing frames. Word Boundary Indicator Following <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b6">7]</ref>, we add word boundary indicators as extra input to the temporal model. The indicators are basically binary vectors with the same length as the number of frames in the input video. All vector entries which correspond to frames where the target word is present are set to 1 and the rest are set to 0. The vector for the word boundary indicators is concatenated with the frame-wise visual features from the encoder and the new vector is fed into the temporal model. Self-Distillation Self-distillation <ref type="bibr" target="#b12">[13]</ref> is based on the idea of training a series of models with the same architecture using distillation and has been recently applied to lip-reading <ref type="bibr" target="#b4">[5]</ref>. Specifically, we first train a network that acts as a teacher for training a student model with the same architecture. The student network becomes the teacher network in the next iteration and we keep training models until no improvement is observed. The insight behind this is that the teacher network provides extra supervisory signal with inter-class similarity information. The overall loss L to be optimized is the weighted combination of Cross-Entropy loss L CE for hard targets and Kullback-Leibler (KL) divergence loss L KD for soft targets.  <ref type="table">Table 1</ref>: Ablation studies of three temporal models on LRW dataset. Starting from the best-performing DC-TCN model, we remove each data augmentation and the word boundaries indicators to examine their effectiveness. Then we replace the DC-TCN with MS-TCN and BGRU. "Scratch" denotes a model trained from scratch without using external data. "LiRA(LRS3)" indicates a self-supervised pre-trained model using LiRA <ref type="bibr" target="#b11">[12]</ref> on the LRS3 dataset, and "LRS2&amp;3+AVS" indicates a fully supervised pre-trained model on LRS2, LRS3 and AVSpeech.</p><formula xml:id="formula_0">L = L CE (y, ?(z s ; ? s )) + ?L KD (?(z s ; ? s ), ?(z t ; ? t )) (1)</formula><p>where z s and z t represent the embedded representations from student and teacher networks, respectively, ? s and ? t denote learnable parameters of student and teacher models, y is the target label, ?(?) stands for the softmax function, and ? is the balancing weight between the two terms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">EXPERIMENTAL SETUP Databases</head><p>In our experiments, we employ LRW <ref type="bibr" target="#b0">[1]</ref>, which is the largest publicly available dataset for lip-reading of isolated words. The dataset is collected in a form of short clips from more than 1 000 speakers in BBC programs and contains 500 isolated words. Each clip has a duration of 29 frames (1.16 seconds). The isolated word is centred within the clip. The dataset is composed of 488 766, 25 000, and 25 000 short clips in the training, validation and test sets. Pre-Processing We used the RetinaFace <ref type="bibr" target="#b13">[14]</ref> tracker to detect the faces and the Face Alignment Network (FAN) <ref type="bibr" target="#b14">[15]</ref> for landmark detection. The size and rotation differences are removed through registering the faces to the mean face in the training set. A bounding box of 96 ? 96 is used to crop the mouth ROIs. Each frame is normalised by subtracting the mean and dividing by the standard deviation of the training set. Training Details The model is trained for 80 epochs with a mini-batch size of 32. We use the AdamW optimizer <ref type="bibr" target="#b15">[16]</ref> with an initial learning rate of 3e-4. The learning rate is decayed using a cosine annealing strategy without a warm-up phase. We also use the variable-length augmentation <ref type="bibr" target="#b2">[3]</ref> for all experiments. The value for N max used in time-masking (see section 2) is set to 15 frames (0.6 seconds) and was optimised in the LRW validation set. Temporal Models MS-TCN: We adopt the same MS-TCN architecture as in <ref type="bibr" target="#b2">[3]</ref>, that is, each block consists of 3 branches  <ref type="table">Table 2</ref>: Performance of self-distillation models (Teacher = ResNet-18 + DC-TCN). The best-performing models from <ref type="table">Table 1</ref> are serving as teachers in first row. For each student model, the model from the line above is used as its teacher, and "Student i" stands for the model after the i-th self-distillation iteration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Self-Distillation Models</head><p>with 3/5/7 kernel sizes, respectively, and we stack 4 such blocks to formulate the MS-TCN network. DC-TCN: The DC-TCN used in this paper generally follows the structures in <ref type="bibr" target="#b9">[10]</ref>. In particular, we opt for the Partially Dense (PD) architecture in each TC block, while each block consists of 9 densely-connected temporal convolutions with kernel sizes of {3, 5, 7} and dilation rates of {1, 2, 5}. BGRU: A four-layer BGRU with a dropout rate of 0.2 is used with 1024 hidden units.</p><p>Initialisation To investigate the impact of initialisation we consider three cases: 1) we train the model from scratch using only the LRW training set, 2) we pre-train the encoder from <ref type="figure" target="#fig_0">Fig. 1</ref> on the LRS3 dataset <ref type="bibr" target="#b16">[17]</ref> using the LiRA <ref type="bibr" target="#b11">[12]</ref> selfsupervised approach and fine-tune it on the LRW training set.</p><p>3) we pre-train the encoder on LRS2 <ref type="bibr" target="#b17">[18]</ref>, LRS3 <ref type="bibr" target="#b16">[17]</ref> and AVspeech <ref type="bibr" target="#b18">[19]</ref> as described in <ref type="bibr" target="#b19">[20]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method Word Boundary</head><p>Top-1 Acc. (%) 3D-CNN <ref type="bibr" target="#b0">[1]</ref> 61.1 ResNet-34 + BLSTM <ref type="bibr" target="#b1">[2]</ref> 83.0 2*3D-CNN + BLSTM <ref type="bibr" target="#b20">[21]</ref> 84.1 ResNet-18 + BLSTM <ref type="bibr" target="#b6">[7]</ref> 84.3 ResNet-18 + BGRU + Cutout <ref type="bibr" target="#b5">[6]</ref> 85.0 ResNet-18 + BGRU <ref type="bibr" target="#b3">[4]</ref> 85.0 ResNet-18 + MS-TCN <ref type="bibr" target="#b2">[3]</ref> 85.3 ResNet-18 + MS-TCN + S.D. <ref type="bibr" target="#b4">[5]</ref> 88.5 ResNet-18 + DC-TCN <ref type="bibr" target="#b9">[10]</ref> 88  <ref type="table">Table 3</ref>: Comparison with state-of-the-art methods on the LRW dataset in terms of classification accuracy. Experiments are divided into two groups, with and without utilising word boundaries indicators, respectively. "S.D.": self-distillation. "Scratch", "LiRA(LRS3)" and "LRS2&amp;3+AVS" correspond to the three pre-training strategies in <ref type="table">Table 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">RESULTS</head><p>Ablation Study Results for the ablation study are shown in <ref type="table">Table 1</ref>. By removing one augmentation at a time we can estimate its contribution to the final model. We see the Time Masking is the most important augmentation, resulting in an absolute drop of 2.4% followed by mixup with a drop of 1.1%. By replacing DC-TCN with MS-TCN, we observe that the performance drops by 2.1 %, which demonstrates the importance of dense connections and the SE attention mechanism in DC-TCN. The performance drops by 2.4% by replacing DC-TCN with BGRU. Additionally, the removal of word boundary indicators drops the performance by 1.7 %, which demonstrates the benefits of including auxiliary boundary indicators. Finally, we pre-train the encoder in a self-supervised/supervised manner on the LRS3 / LRS2, LRS3 and AVspeech datasets and then fine-tune the model on the LRW training set, and this slightly increases the performance to 92.3 % / 92.9 %. It is clear from <ref type="table">Table 3</ref> that the proposed models significantly outperform the current stateof-the-art. Self-Distillation Results for self-distillation experiments are presented in <ref type="table">Table 2</ref>. We use the best two models from  <ref type="figure">Fig. 2</ref>: A comparison of our method and two baseline methods (End-to-End AVR <ref type="bibr" target="#b8">[9]</ref> and Multi-Scale TCN <ref type="bibr" target="#b2">[3]</ref>) on the five difficulty groups of the LRW test set. <ref type="table">Table 1</ref> as teachers in the first round. It is clear that selfdistillation results in a 0.6 % to 0.7 % absolute improvement in all cases. In addition, an ensemble of all models (all students + teacher) leads to a further absolute improvement of 0.6 %. These results suggest that self-distillation is beneficial for lip-reading. However, we should point out that the improvement is smaller compared to <ref type="bibr" target="#b4">[5]</ref>, probably due to the much better teacher model which makes further improvement harder. Error Analysis In order to better understand how the presented models improve the word classification accuracy, we perform some error analysis. We divide the test samples in the LRW dataset into five groups <ref type="bibr" target="#b9">[10]</ref>. Each group contains 100 distinct isolated words and it is created based on the word accuracy of the model in <ref type="bibr" target="#b8">[9]</ref>. The 100 words with the highest classification accuracy are grouped in the "Very Easy" group, the next 100 words in the "Easy" group and so on. The average classification accuracy in each group is shown in <ref type="figure">Fig.  2</ref>. For comparison purposes, we also include the performance of <ref type="bibr" target="#b2">[3]</ref> and <ref type="bibr" target="#b8">[9]</ref>. We can see that our models outperform the two baselines across all groups and the improvement is more pronounced in the "Difficult" and "Very Difficult" groups.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">CONCLUSION</head><p>In this work, we present a detailed study on the LRW dataset in terms of data augmentation and temporal models and demonstrate how state-of-the-art performance can be achieved by combining the best augmentations and training strategies. We show that Time Masking is the most important data augmentation method followed by mixup. We also show that DC-TCNs result in better performance than MS-TCNs or BGRUs. The use of self-distillation and word boundary indicators further improves the classification accuracy whereas the use of pre-training leads to a slight improvement. Finally, an error analysis reveals that the presented models significantly improve the classification accuracy of hard-to-recognise words.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>) is the most commonly used mouth Region-Of-Interest (ROI) arXiv:2209.01383v3 [cs.CV] 29 Sep 2022 (a): MS-TCN architecture. "C" and "T " refer to the channel number and sequence length, respectively. (b): DC-TCN architecture. SE and C denote the the operations of Squeeze-and-Excitation (SE) [8] and channel-wise concatenation, respectively. "T C" represents a Temporal Convolutional block, while the growth rate is denoted as "C o ". (c): Lip-reading model with a modified ResNet-18 as encoder and DC-TCN as a temporal model. The word boundary indicators are concatenated with the output features of the encoder.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>/o S.D, w/o Word Boundary, DC-TCN) Ours (w/o S.D, w/ Word Boundary, DC-TCN) Ours (w/ S.D., w/ Word Boundary, Student 2 DC-TCN) Ours (w/ S.D., w/ Word Boundary, Ensemble DC-TCN)</figDesc><table><row><cell></cell><cell></cell><cell cols="2">End-to-End AVR</cell></row><row><cell></cell><cell></cell><cell cols="2">Multi-Scale TCN</cell></row><row><cell></cell><cell></cell><cell cols="2">Ours (w</cell></row><row><cell>Top-1 Accuracy</cell><cell>0.9 1.0</cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.8</cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.7</cell><cell></cell><cell></cell></row><row><cell></cell><cell>Very Easy</cell><cell>Easy</cell><cell>Medium Difficulty Categories</cell><cell>Difficult Very Difficult</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Lip Reading in the Wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th Asian Conference on Computer Vision (ACCV)</title>
		<meeting>the 13th Asian Conference on Computer Vision (ACCV)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">10112</biblScope>
			<biblScope unit="page" from="87" to="103" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Combining Residual Networks with LSTMs for Lipreading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Stafylakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th Annual Conference of International Speech Communication Association (INTER-SPEECH)</title>
		<meeting>the 18th Annual Conference of International Speech Communication Association (INTER-SPEECH)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="3652" to="3656" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Lipreading Using Temporal Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Petridis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 45th IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<meeting>the 45th IEEE International Conference on Acoustics, Speech and Signal Processing</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6319" to="6323" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learn an Effective Lip Reading Model without Pains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Towards Practical Lipreading with Distilled and Efficient Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Petridis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 46th IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<meeting>the 46th IEEE International Conference on Acoustics, Speech and Signal Processing</meeting>
		<imprint>
			<biblScope unit="page" from="7608" to="7612" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Can We Read Speech Beyond the Lips? Rethinking RoI Selection for Deep Visual Speech Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th IEEE International Conference on Automatic Face &amp; Gesture Recognition (FG)</title>
		<meeting>the 15th IEEE International Conference on Automatic Face &amp; Gesture Recognition (FG)</meeting>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Pushing the Boundaries of Audiovisual Word Recognition using Residual Networks and LSTMs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Stafylakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Image Understanding</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="22" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Squeeze-and-Excitation Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the 31st IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7132" to="7141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">End-to-end audiovisual speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Petridis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Stafylakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<meeting>the 43rd IEEE International Conference on Acoustics, Speech and Signal Processing</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6548" to="6552" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Lip-reading with Densely Connected Temporal Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="2857" to="2866" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">SpecAugment: A Simple Data Augmentation Method for Automatic Speech Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th Annual Conference of International Speech Communication Association (INTERSPEECH)</title>
		<meeting>the 20th Annual Conference of International Speech Communication Association (INTERSPEECH)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2613" to="2617" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">LiRA: Learning Visual Speech Representations from Audio Through Self-Supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Petridis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd Annual Conference of International Speech Communication Association (INTERSPEECH)</title>
		<meeting>the 22nd Annual Conference of International Speech Communication Association (INTERSPEECH)</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="3011" to="3015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Born-again neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Furlanello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">C</forename><surname>Lipton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tschannen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning (ICML)</title>
		<meeting>the 35th International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1602" to="1611" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">RetinaFace: Single-Shot Multi-Level Face Localisation in the Wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ververas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33rd IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the 33rd IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5203" to="5212" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">How far are we from solving the 2D &amp; 3D Face Alignment problem? (and a dataset of 230,000 3D facial landmarks)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bulat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the 16th IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Decoupled Weight Decay Regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th International Conference on Learning Representations (ICLR)</title>
		<meeting>the 7th International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">LRS3-TED: a large-scale dataset for visual speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Afouras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno>abs/1809.00496</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Lip reading sentences in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the 30th IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3444" to="3453" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Looking to listen at the cocktail party: A speaker-independent audiovisual model for speech separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ephrat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Mosseri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Lang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="112" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Visual speech recognition for multiple languages in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Petridis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
		<idno>abs/2202.13084</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning Spatio-Temporal Features with Two-Stream Deep 3D CNNs for Lipreading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kitani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the British Machine Vision Conference (BMVC)</title>
		<meeting>the British Machine Vision Conference (BMVC)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">269</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
