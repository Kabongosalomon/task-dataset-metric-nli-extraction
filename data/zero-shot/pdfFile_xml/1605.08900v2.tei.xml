<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Aspect Level Sentiment Classification with Deep Memory Network</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duyu</forename><surname>Tang</surname></persName>
							<email>dytang@ir.hit.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Harbin Institute of Technology</orgName>
								<address>
									<settlement>Harbin</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Qin</surname></persName>
							<email>qinb@ir.hit.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Harbin Institute of Technology</orgName>
								<address>
									<settlement>Harbin</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
							<email>tliu@ir.hit.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Harbin Institute of Technology</orgName>
								<address>
									<settlement>Harbin</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Aspect Level Sentiment Classification with Deep Memory Network</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T19:08+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We introduce a deep memory network for aspect level sentiment classification. Unlike feature-based SVM and sequential neural models such as LSTM, this approach explicitly captures the importance of each context word when inferring the sentiment polarity of an aspect. Such importance degree and text representation are calculated with multiple computational layers, each of which is a neural attention model over an external memory. Experiments on laptop and restaurant datasets demonstrate that our approach performs comparable to state-of-art feature based SVM system, and substantially better than LSTM and attention-based LSTM architectures. On both datasets we show that multiple computational layers could improve the performance. Moreover, our approach is also fast. The deep memory network with 9 layers is 15 times faster than LSTM with a CPU implementation.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Aspect level sentiment classification is a fundamental task in the field of sentiment analysis <ref type="bibr" target="#b20">(Pang and Lee, 2008;</ref><ref type="bibr" target="#b16">Liu, 2012;</ref><ref type="bibr">Pontiki et al., 2014)</ref>. Given a sentence and an aspect occurring in the sentence, this task aims at inferring the sentiment polarity (e.g. positive, negative, neutral) of the aspect. For example, in sentence "great food but the service was dreadful!", the sentiment polarity of aspect "food" is positive while the polarity of aspect * Corresponding author. "service" is negative. Researchers typically use machine learning algorithms and build sentiment classifier in a supervised manner. Representative approaches in literature include feature based Support Vector Machine <ref type="bibr" target="#b9">(Kiritchenko et al., 2014;</ref><ref type="bibr" target="#b27">Wagner et al., 2014)</ref> and neural network models <ref type="bibr" target="#b3">(Dong et al., 2014;</ref><ref type="bibr">Lakkaraju et al., 2014;</ref><ref type="bibr" target="#b26">Vo and Zhang, 2015;</ref><ref type="bibr">Nguyen and Shirai, 2015;</ref><ref type="bibr">Tang et al., 2015a)</ref>. Neural models are of growing interest for their capacity to learn text representation from data without careful engineering of features, and to capture semantic relations between aspect and context words in a more scalable way than feature based SVM.</p><p>Despite these advantages, conventional neural models like long short-term memory (LSTM) <ref type="bibr">(Tang et al., 2015a)</ref> capture context information in an implicit way, and are incapable of explicitly exhibiting important context clues of an aspect. We believe that only some subset of context words are needed to infer the sentiment towards an aspect. For example, in sentence "great food but the service was dreadful!", "dreadful" is an important clue for the aspect "service" but "great" is not needed. Standard LSTM works in a sequential way and manipulates each context word with the same operation, so that it cannot explicitly reveal the importance of each context word. A desirable solution should be capable of explicitly capturing the importance of context words and using that information to build up features for the sentence after given an aspect word. Furthermore, a human asked to do this task will selectively focus on parts of the contexts, and acquire information where it is needed to build up an internal representation towards an aspect in his/her mind.</p><p>In pursuit of this goal, we develop deep memory network for aspect level sentiment classification, which is inspired by the recent success of computational models with attention mechanism and explicit memory <ref type="bibr" target="#b6">(Graves et al., 2014;</ref><ref type="bibr" target="#b0">Bahdanau et al., 2015;</ref><ref type="bibr" target="#b23">Sukhbaatar et al., 2015)</ref>. Our approach is data-driven, computationally efficient and does not rely on syntactic parser or sentiment lexicon. The approach consists of multiple computational layers with shared parameters. Each layer is a content-and location-based attention model, which first learns the importance/weight of each context word and then utilizes this information to calculate continuous text representation. The text representation in the last layer is regarded as the feature for sentiment classification. As every component is differentiable, the entire model could be efficiently trained end-toend with gradient descent, where the loss function is the cross-entropy error of sentiment classification.</p><p>We apply the proposed approach to laptop and restaurant datasets from SemEval 2014 <ref type="bibr">(Pontiki et al., 2014)</ref>. Experimental results show that our approach performs comparable to a top system using feature-based SVM <ref type="bibr" target="#b9">(Kiritchenko et al., 2014)</ref>. On both datasets, our approach outperforms both LSTM and attention-based LSTM models <ref type="bibr">(Tang et al., 2015a)</ref> in terms of classification accuracy and running speed. Lastly, we show that using multiple computational layers over external memory could achieve improved performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background: Memory Network</head><p>Our approach is inspired by the recent success of memory network in question answering <ref type="bibr">(Weston et al., 2014;</ref><ref type="bibr" target="#b23">Sukhbaatar et al., 2015)</ref>. We describe the background on memory network in this part.</p><p>Memory network is a general machine learning framework introduced by Weston et al. <ref type="bibr">(2014)</ref>. Its central idea is inference with a long-term memory component, which could be read, written to, and jointly learned with the goal of using it for prediction. Formally, a memory network consists of a memory m and four components I, G, O and R, where m is an array of objects such as an array of vectors. Among these four components, I converts input to internal feature representation, G updates old memories with new input, O generates an out-put representation given a new input and the current memory state, R outputs a response based on the output representation.</p><p>Let us take question answering as an example to explain the work flow of memory network. Given a list of sentences and a question, the task aims to find evidences from these sentences and generate an answer, e.g. a word. During inference, I component reads one sentence s i at a time and encodes it into a vector representation. Then G component updates a piece of memory m i based on current sentence representation. After all sentences are processed, we get a memory matrix m which stores the semantics of these sentences, each row representing a sentence. Given a question q, memory network encodes it into vector representation e q , and then O component uses e q to select question related evidences from memory m and generates an output vector o. Finally, R component takes o as the input and outputs the final response. It is worth noting that O component could consist of one or more computational layers (hops). The intuition of utilizing multiple hops is that more abstractive evidences could be found based on previously extracted evidences. <ref type="bibr" target="#b23">Sukhbaatar et al. (2015)</ref> demonstrate that multiple hops could uncover more abstractive evidences than single hop, and could yield improved results on question answering and language modeling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Deep Memory Network for Aspect Level Sentiment Classification</head><p>In this section, we describe the deep memory network approach for aspect level sentiment classification. We first give the task definition. Afterwards, we describe an overview of the approach before presenting the content-and location-based attention models in each computational layer. Lastly, we describe the use of this approach for aspect level sentiment classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Task Definition and Notation</head><p>Given a sentence s = {w 1 , w 2 , ..., w i , ...w n } consisting of n words and an aspect word w i 1 occur-ring in sentence s, aspect level sentiment classification aims at determining the sentiment polarity of sentence s towards the aspect w i . For example, the sentiment polarity of sentence "great food but the service was dreadful!" towards aspect "food" is positive, while the polarity towards aspect "service" is negative. When dealing with a text corpus, we map each word into a low dimensional, continuous and real-valued vector, also known as word embedding <ref type="bibr" target="#b18">(Mikolov et al., 2013;</ref><ref type="bibr" target="#b20">Pennington et al., 2014)</ref>. All the word vectors are stacked in a word embedding matrix L ? R d?|V | , where d is the dimension of word vector and |V | is vocabulary size. The word embedding of w i is notated as e i ? R d?1 , which is a column in the embedding matrix L.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">An Overview of the Approach</head><p>We present an overview of the deep memory network for aspect level sentiment classification. Given a sentence s = {w 1 , w 2 , ..., w i , ...w n } and the aspect word w i , we map each word into its embedding vector. These word vectors are separated into two parts, aspect representation and context representation. If aspect is a single word like "food" or "service", aspect representation is the embedding of aspect word. For the case where aspect is multi word expression like "battery life", aspect representation is an average of its constituting word vectors <ref type="bibr">(Sun et al., 2015)</ref>. To simplify the interpretation, we consider aspect as a single word w i . Context word vectors {e 1 , e 2 ... e i?1 , e i+1 ... e n } are stacked and regarded as the external memory m ? R d?(n?1) , where n is the sentence length.</p><p>An illustration of our approach is given in <ref type="figure" target="#fig_0">Figure  1</ref>, which is inspired by the use of memory network in question answering <ref type="bibr" target="#b23">(Sukhbaatar et al., 2015)</ref>. Our approach consists of multiple computational layers (hops), each of which contains an attention layer and a linear layer. In the first computational layer (hop 1), we regard aspect vector as the input to adaptively select important evidences from memory m through attention layer. The output of attention layer and the linear transformation of aspect vector 2 are summed and the result is considered as the input of next layer (hop 2). In a similar way, we stack multiple hops and run these steps multiple times, so that more abstractive evidences could be selected from the external memory m. The output vector in last hop is considered as the representation of sentence with regard to the aspect, and is further used as the feature for aspect level sentiment classification. It is helpful to note that the parameters of attention and linear layers are shared in different hops. Therefore, the model with one layer and the model with nine layers have the same number of parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Content Attention</head><p>We describe our attention model in this part. The basic idea of attention mechanism is that it assigns a weight/importance to each lower position when computing an upper level representation <ref type="bibr" target="#b0">(Bahdanau et al., 2015)</ref>. In this work, we use attention model to compute the representation of a sentence with regard to an aspect. The intuition is that context words do not contribute equally to the semantic meaning of a sentence. Furthermore, the importance of a word should be different if we focus on different aspect. Let us again take the example of "great food but the service was dreadful!". The context word "great" is more important than "dreadful" for aspect "food". On the contrary, "dreadful" is more important than "great" for aspect "service".</p><p>Taking an external memory m ? R d?k and an aspect vector v aspect ? R d?1 as input, the attention model outputs a continuous vector vec ? R d?1 . The output vector is computed as a weighted sum of each piece of memory in m, namely</p><formula xml:id="formula_0">vec = k i=1 ? i m i (1)</formula><p>where k is the memory size, ? i ? [0, 1] is the weight of m i and i ? i = 1. We implement a neural network based attention model. For each piece of memory m i , we use a feed forward neural network to compute its semantic relatedness with the aspect. The scoring function is calculated as follows, where W att ? R 1?2d and b att ? R 1?1 .</p><formula xml:id="formula_1">g i = tanh(W att [m i ; v aspect ] + b att )<label>(2)</label></formula><p>After obtaining {g 1 , g 2 , ... g k }, we feed them to a sof tmax function to calculate the final importance scores {? 1 , ? 2 , ... ? k }.</p><formula xml:id="formula_2">? i = exp(g i ) k j=1 exp(g j )<label>(3)</label></formula><p>We believe that such an attention model has two advantages. One advantage is that this model could adaptively assign an importance score to each piece of memory m i according to its semantic relatedness with the aspect. Another advantage is that this attention model is differentiable, so that it could be easily trained together with other components in an end-to-end fashion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Location Attention</head><p>We have described our neural attention framework and a content-based model in previous subsection. However, the model mentioned above ignores the location information between context word and aspect. Such location information is helpful for an attention model because intuitively a context word closer to the aspect should be more important than a farther one. In this work, we define the location of a context word as its absolute distance with the aspect in the original sentence sequence 3 . On this basis, we study four strategies to encode the location information in the attention model. The details are described below.</p><p>? Model 1. Following <ref type="bibr" target="#b23">Sukhbaatar et al. (2015)</ref>, we calculate the memory vector m i with</p><formula xml:id="formula_3">m i = e i v i<label>(4)</label></formula><p>where means element-wise multiplication and v i ? R d?1 is a location vector for word w i . Every element in v i is calculated as follows,</p><formula xml:id="formula_4">v k i = (1 ? l i /n) ? (k/d)(1 ? 2 ? l i /n) (5)</formula><p>where n is sentence length, k is the hop number and l i is the location of w i .</p><p>? Model 2. This is a simplified version of Model 1, using the same location vector v i for w i in different hops. Location vector v i is calculated as follows.</p><formula xml:id="formula_5">v i = 1 ? l i /n<label>(6)</label></formula><p>? Model 3. We regard location vector v i as a parameter and compute a piece of memory with vector addition, namely</p><formula xml:id="formula_6">m i = e i + v i<label>(7)</label></formula><p>All the position vectors are stacked in a position embedding matrix, which is jointly learned with gradient descent.</p><p>? Model 4. Location vectors are also regarded as parameters. Different from Model 3, location representations are regarded as neural gates to control how many percent of word semantics is written into the memory. We feed location vector v i to a sigmoid function ?, and calculate m i with element-wise multiplication:</p><formula xml:id="formula_7">m i = e i ?(v i )<label>(8)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">The Need for Multiple Hops</head><p>It is widely accepted that computational models that are composed of multiple processing layers have the ability to learn representations of data with multiple levels of abstraction <ref type="bibr">(LeCun et al., 2015)</ref>. In this work, the attention layer in one layer is essentially a weighted average compositional function, which is not powerful enough to handle the sophisticated computationality like negation, intensification and contrary in language. Multiple computational layers allow the deep memory network to learn representations of text with multiple levels of abstraction. Each layer/hop retrieves important context words, and transforms the representation at previous level into a representation at a higher, slightly more abstract level. With the composition of enough such transformations, very complex functions of sentence representation towards an aspect can be learned.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Aspect Level Sentiment Classification</head><p>We regard the output vector in last hop as the feature, and feed it to a sof tmax layer for aspect level sentiment classification. The model is trained in a supervised manner by minimizing the cross entropy error of sentiment classification, whose loss function is given below, where T means all training instances, C is the collection of sentiment categories, (s, a) means a sentence-aspect pair.</p><formula xml:id="formula_8">loss = ? (s,a)?T c?C P g c (s, a) ? log(P c (s, a)) (9) P c (s, a)</formula><p>is the probability of predicting (s, a) as category c produced by our system. P g c (s, a) is 1 or 0, indicating whether the correct answer is c. We use back propagation to calculate the gradients of all the parameters, and update them with stochastic gradient descent. We clamp the word embeddings with 300-dimensional Glove vectors <ref type="bibr" target="#b20">(Pennington et al., 2014)</ref>, which is trained from web data and the vocabulary size is 1.9M 4 . We randomize other parameters with uniform distribution U (?0.01, 0.01), and set the learning rate as 0.01.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiment</head><p>We describe experimental settings and report empirical results in this section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Setting</head><p>We conduct experiments on two datasets from <ref type="bibr">Se-mEval 2014</ref><ref type="bibr">(Pontiki et al., 2014</ref>, one from laptop domain and another from restaurant domain. Statistics of the datasets are given in <ref type="table" target="#tab_1">Table 1</ref>. It is worth noting that the original dataset contains the fourth category -conflict, which means that a sentence expresses both positive and negative opinion towards an aspect. We remove conflict category as the number of instances is very tiny, incorporating which 4 Available at: http://nlp.stanford.edu/projects/glove/. will make the dataset extremely unbalanced. Evaluation metric is classification accuracy.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Comparison to Other Methods</head><p>We compare with the following baseline methods on both datasets.</p><p>(1) Majority is a basic baseline method, which assigns the majority sentiment label in training set to each instance in the test set.</p><p>(2) Feature-based SVM performs state-of-the-art on aspect level sentiment classification. We compare with a top system using ngram features, parse features and lexicon features <ref type="bibr" target="#b9">(Kiritchenko et al., 2014)</ref>.</p><p>(3) We compare with three LSTM models (Tang et al., 2015a)). In LSTM, a LSTM based recurrent model is applied from the start to the end of a sentence, and the last hidden vector is used as the sentence representation. TDLSTM extends LSTM by taking into account of the aspect, and uses two LSTM networks, a forward one and a backward one, towards the aspect. TDLSTM+ATT extends TDLSTM by incorporating an attention mechanism (Bahdanau et al., 2015) over the hidden vectors. We use the same Glove word vectors for fair comparison.</p><p>(4) We also implement ContextAVG, a simplistic version of our approach. Context word vectors are averaged and the result is added to the aspect vector. The output is fed to a sof tmax function.</p><p>Experimental results are given in <ref type="table" target="#tab_3">Table 2</ref>. Our approach using only content attention is abbreviated to MemNet (k), where k is the number of hops. We can find that feature-based SVM is an extremely strong performer and substantially outperforms other baseline methods, which demonstrates the importance of a powerful feature representation for aspect level sentiment classification. Among three recurrent models, TDLSTM performs better than LSTM, which indicates that taking into account of the aspect information is helpful. This is reason-  able as the sentiment polarity of a sentence towards different aspects (e.g. "food" and "service") might be different. It is somewhat disappointing that incorporating attention model over TDLSTM does not bring any improvement. We consider that each hidden vector of TDLSTM encodes the semantics of word sequence until the current position. Therefore, the model of TDLSTM+ATT actually selects such mixed semantics of word sequence, which is weird and not an intuitive way to selectively focus on parts of contexts. Different from TDLSTM+ATT, the proposed memory network approach removes the recurrent calculator over word sequence and directly apply attention mechanism on context word representations.</p><p>We can also find that the performance of Contex-tAVG is very poor, which means that assigning the same weight/importance to all the context words is not an effective way. Among all our models from single hop to nine hops, we can observe that using more computational layers could generally lead to better performance, especially when the number of hops is less than six. The best performances are achieved when the model contains seven and nine hops, respectively. On both datasets, the proposed approach could obtain comparable accuracy compared to the state-of-art feature-based SVM system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Runtime Analysis</head><p>We study the runtime of recurrent neural models and the proposed deep memory network approach with different hops. We implement all these approaches based on the same neural network infrastructure, use the same 300-dimensional Glove word vectors, and run them on the same CPU server.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Time  The training time of each iteration on the restaurant dataset is given in <ref type="table" target="#tab_5">Table 3</ref>. We can find that LSTM based recurrent models are indeed computationally expensive, which is caused by the complex operations in each LSTM unit along the word sequence. Instead, the memory network approach is simpler and evidently faster because it does not need recurrent calculators of sequence length. Our approach with nine hops is almost 15 times faster than the basic LSTM model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Effects of Location Attention</head><p>As described in Section 3.4, we explore four strategies to integrate location information into the attention model. We incorporate each of them separately into the basic content-based attention model. It is helpful to restate that the difference between four location-based attention models lies in the usage of location vectors for context words. In Model 1 and Model 2, the values of location vectors are fixed and calculated in a heuristic way. In Model 3 and Model 4, location vectors are also regarded as the parameters and jointly learned along with other parameters in the deep memory network.  the results of sentence "great food but the service was dreadful!" with "food" and "service" as the aspects.</p><p>(a) Aspect: service, Answer: -1,  <ref type="table">Table 5</ref>: Examples of attention weights in different hops for aspect level sentiment classification. The model also takes into account of the location information (Model 2). This example is as same as the one we use in <ref type="table" target="#tab_7">Table 4</ref>.  <ref type="figure" target="#fig_1">Figure 2</ref> shows the classification accuracy of each attention model on the restaurant dataset. We can find that using multiple computational layers could consistently improve the classification accuracy in all these models. All these models perform comparably when the number of hops is larger than five. Among these four location-based models, we prefer Model 2 as it is intuitive and has less computation cost without loss of accuracy. We also find that Model 4 is very sensitive to the choice of neural gate. Its classification accuracy decreases by almost 5 percentage when the sigmoid operation over location vector is removed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Visualize Attention Models</head><p>We visualize the attention weight of each context word to get a better understanding of the deep memory network approach. The results of context-based model and location-based model (Model 2) are given in <ref type="table" target="#tab_7">Table 4</ref> and <ref type="table">Table 5</ref>, respectively.</p><p>From <ref type="table" target="#tab_7">Table 4</ref>(a), we can find that in the first hop the context words "great", "but" and "dreadful" contribute equally to the aspect "service". While after the second hop, the weight of "dreadful" increases and finally the model correctly predict the polarity towards "service" as negative. This case shows the effects of multiple hops. However, in Table 4(b), the content-based model also gives a larger weight to "dreadful" when the target we focus on is "food". As a result, the model incorrectly predicts the polarity towards "food" as negative. This phenomenon might be caused by the neglect of location information. From <ref type="table">Table 5</ref>(b), we can find that the weight of "great" is increased when the location of context word is considered. Accordingly, Model 2 predicts the correct sentiment label towards "food". We believe that location-enhanced model captures both content and location information. For instance, in <ref type="table">Table 5</ref>(a) the closest context words of the aspect "service" are "the" and "was", while "dreadful" has the largest weight.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Error Analysis</head><p>We carry out an error analysis of our location enhanced model (Model 2) on the restaurant dataset, and find that most of the errors could be summarized as follows.</p><p>The first factor is noncompositional sentiment expression. This model regards single context word as the basic computational unit and cannot handle this situation. An example is "dessert was also to die for!", where the aspect is underlined. The sentiment expression is "die for", whose meaning could not be composed from its constituents "die" and "for". The second factor is complex aspect expression consisting of many words, such as "ask for the round corner table next to the large window." This model represents an aspect expression by averaging its constituting word vectors, which could not well handle this situation. The third factor is sentimental relation between context words such as negation, comparison and condition. An example is "but dinner here is never disappointing, even if the prices are a bit over the top". We believe that this is caused by the weakness of weighted average compositional function in each hop. There are also cases when comparative opinions are expressed such as "i 've had better japanese food at a mall food court".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>This work is connected to three research areas in natural language processing. We briefly describe related studies in each area.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Aspect Level Sentiment Classification</head><p>Aspect level sentiment classification is a finegrained classification task in sentiment analysis, which aims at identifying the sentiment polarity of a sentence expressed towards an aspect <ref type="bibr">(Pontiki et al., 2014)</ref>. Most existing works use machine learning algorithms, and build sentiment classifier from sentences with manually annotated polarity labels. One of the most successful approaches in literature is feature based SVM. Experts could design effective feature templates and make use of external resources like parser and sentiment lexicons <ref type="bibr" target="#b9">(Kiritchenko et al., 2014;</ref><ref type="bibr" target="#b27">Wagner et al., 2014)</ref>. In recent years, neural network approaches <ref type="bibr" target="#b3">(Dong et al., 2014;</ref><ref type="bibr">Lakkaraju et al., 2014;</ref><ref type="bibr">Nguyen and Shirai, 2015;</ref><ref type="bibr">Tang et al., 2015a)</ref> are of growing attention for their capacity to learn powerful text representation from data. However, these neural models (e.g. LSTM) are computationally expensive, and could not explicitly reveal the importance of context evidences with regard to an aspect. Instead, we develop simple and fast approach that explicitly encodes the context importance towards a given aspect. It is worth noting that the task we focus on differs from finegrained opinion extraction, which assigns each word a tag (e.g. B,I,O) to indicate whether it is an aspect/sentiment word <ref type="bibr" target="#b2">(Choi and Cardie, 2010;</ref><ref type="bibr">Irsoy and Cardie, 2014;</ref><ref type="bibr" target="#b15">Liu et al., 2015)</ref>. The aspect word in this work is given as a part of the input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Compositionality in Vector Space</head><p>In NLP community, compositionality means that the meaning of a composed expression (e.g. a phrase/sentence/document) comes from the meanings of its constituents (Frege, 1892). <ref type="bibr" target="#b19">Mitchell and Lapata (2010)</ref> exploits a variety of addition and multiplication functions to calculate phrase vector. Yessenalina and Cardie (2011) use matrix multiplication as compositional function to compute vectors for longer phrases. To compute sentence representation, researchers develop denoising autoencoder <ref type="bibr" target="#b5">(Glorot et al., 2011)</ref>, convolutional neural network <ref type="bibr" target="#b7">(Kalchbrenner et al., 2014;</ref><ref type="bibr" target="#b8">Kim, 2014;</ref><ref type="bibr">Yin and Sch?tze, 2015)</ref>, sequence based recurrent neural models <ref type="bibr" target="#b24">(Sutskever et al., 2014;</ref><ref type="bibr" target="#b10">Kiros et al., 2015;</ref><ref type="bibr" target="#b14">Li et al., 2015b)</ref> and tree-structured neural networks <ref type="bibr" target="#b22">(Socher et al., 2013;</ref><ref type="bibr" target="#b25">Tai et al., 2015;</ref>. Several recent studies calculate continuous representation for documents with neural networks <ref type="bibr" target="#b12">(Le and Mikolov, 2014;</ref><ref type="bibr" target="#b1">Bhatia et al., 2015;</ref><ref type="bibr" target="#b13">Li et al., 2015a;</ref><ref type="bibr">Tang et al., 2015b;</ref><ref type="bibr" target="#b28">Yang et al., 2016)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Attention and Memory Networks</head><p>Recently, there is a resurgence in computational models with attention mechanism and explicit mem-ory to learn representations of texts <ref type="bibr" target="#b6">(Graves et al., 2014;</ref><ref type="bibr">Weston et al., 2014;</ref><ref type="bibr" target="#b23">Sukhbaatar et al., 2015;</ref><ref type="bibr" target="#b0">Bahdanau et al., 2015)</ref>. In this line of research, memory is encoded as a continuous representation and operations on memory (e.g. reading and writing) are typically implemented with neural networks. Attention mechanism could be viewed as a compositional function, where lower level representations are regarded as the memory, and the function is to choose "where to look" by assigning a weight/importance to each lower position when computing an upper level representation. Such attention based approaches have achieved promising performances on a variety of NLP tasks <ref type="bibr" target="#b17">(Luong et al., 2015;</ref><ref type="bibr" target="#b11">Kumar et al., 2015;</ref><ref type="bibr" target="#b21">Rush et al., 2015)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We develop deep memory networks that capture importances of context words for aspect level sentiment classification. Compared with recurrent neural models like LSTM, this approach is simpler and faster. Empirical results on two datasets verify that the proposed approach performs comparable to state-of-the-art feature based SVM system, and substantively better than LSTM architectures. We implement different attention strategies and show that leveraging both content and location information could learn better context weight and text representation. We also demonstrate that using multiple computational layers in memory network could obtain improved performance. Our potential future plans are incorporating sentence structure like parsing results into the deep memory network.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>An illustration of our deep memory network with three computational layers (hops) for aspect level sentiment classification.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Classification accuracy of different attention models on the restaurant dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Statistics of the datasets.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Classification accuracy of different methods on laptop and restaurant datasets. Best scores in each group are in bold.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Runtime (seconds) of each training epoch on the restaurant dataset.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 :</head><label>4</label><figDesc>Examples of attention weights in different hops for aspect level sentiment classification. The model only uses content attention. The hop columns show the weights of context words in each hop, indicated by values and gray color. This example shows</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">In practice, an aspect might be a multi word expression such as "battery life". For simplicity we still consider aspect as a single word in this definition.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">In preliminary experiments, we tried directly using aspect vector without a linear transformation, and found that adding a linear layer works slightly better.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">The location of a context word could also be measured by its distance to the aspect along a syntactic path. We leave this as a future work as we prefer to developing a purely data-driven approach without using external parsing results.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We would especially want to thank Xiaodan Zhu for running their system on our setup. We greatly thank Yaming Sun for tremendously helpful discussions. We also thank the anonymous reviewers for their valuable comments. This work was supported by the National High Technology Development 863 Program of China (No. 2015AA015407), National Natural Science Foundation of China (No. 61632011  and No.61273321).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>References</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bahdanau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>ICLR</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Better document-level sentiment analysis from rst discourse parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Bhatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2212" to="2218" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Hierarchical sequential learning for extracting opinions and their attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cardie2010] Yejin</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL 2010 Conference Short Papers</title>
		<meeting>the ACL 2010 Conference Short Papers</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="269" to="274" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Adaptive recursive neural network for target-dependent twitter sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="49" to="54" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">On sense and reference</title>
		<idno>Gottlob Frege. 1892</idno>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="page" from="563" to="584" />
			<pubPlace>Ludlow</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Domain adaptation for largescale sentiment classification: A deep learning approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Glorot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Machine Learning (ICML-11)</title>
		<meeting>the 28th International Conference on Machine Learning (ICML-11)</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="513" to="520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Irsoy and Cardie2014] Ozan Irsoy and Claire Cardie</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Graves</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1410.5401</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="720" to="728" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Neural turing machines</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A convolutional neural network for modelling sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Kalchbrenner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="655" to="665" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for sentence classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1746" to="1751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Nrccanada-2014: Detecting aspects and sentiment in customer reviews</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Kiritchenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th International Workshop on Semantic Evaluation</title>
		<meeting>the 8th International Workshop on Semantic Evaluation</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="437" to="442" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Antonio Torralba, and Sanja Fidler</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Kiros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3276" to="3284" />
		</imprint>
	</monogr>
	<note>Skip-thought vectors</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Ask me anything: Dynamic memory networks for natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Kumar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.07285</idno>
	</analytic>
	<monogr>
		<title level="m">NIPS Workshop on Deep Learning and Representation Learning</title>
		<imprint>
			<publisher>Richard Socher, and Chris Manning</publisher>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Aspect specific sentiment analysis using hierarchical deep learning</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Distributed representations of sentences and documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">]</forename><surname>Mikolov2014</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 31nd International Conference on Machine Learning</title>
		<meeting>The 31nd International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="page" from="436" to="444" />
		</imprint>
	</monogr>
	<note>Deep learning</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A hierarchical neural autoencoder for paragraphs and documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1106" to="1115" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">When are tree structures necessary for deep learning of representations?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2304" to="2314" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Fine-grained opinion mining with recurrent neural networks and word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1433" to="1443" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Sentiment analysis and opinion mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Synthesis Lectures on Human Language Technologies</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="167" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Effective approaches to attention-based neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Luong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1412" to="1421" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Phrasernn: Phrase recursive neural network for aspect-based sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<editor>Nguyen and Shirai2015] Thien Hai Nguyen and Kiyoaki Shirai</editor>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="2509" to="2514" />
		</imprint>
	</monogr>
	<note>Composition in distributional models of semantics</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Opinion mining and sentiment analysis. Foundations and trends in information retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lee2008] Bo</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lillian Lee ; Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Maria Pontiki, Dimitris Galanis, John Pavlopoulos, Harris Papageorgiou, Ion Androutsopoulos, and Suresh Manandhar</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="27" to="35" />
		</imprint>
	</monogr>
	<note>Proceedings of the 8th International Workshop on Semantic Evaluation</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A neural attention model for abstractive sentence summarization</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="379" to="389" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Recursive deep models for semantic compositionality over a sentiment treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1631" to="1642" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Modeling mention, context and entity with neural networks for entity disambiguation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Sukhbaatar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Fourth International Joint Conference on Artificial Intelligence (IJCAI 2015)</title>
		<editor>Yaming Sun, Lei Lin, Duyu Tang, Nan Yang, Zhenzhou Ji, and Xiaolong Wang</editor>
		<meeting>the Twenty-Fourth International Joint Conference on Artificial Intelligence (IJCAI 2015)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1333" to="1339" />
		</imprint>
	</monogr>
	<note>Advances in Neural Information Processing Systems</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Improved semantic representations from tree-structured long short-term memory networks</title>
		<idno type="arXiv">arXiv:1512.01100</idno>
	</analytic>
	<monogr>
		<title level="m">Tang et al.2015a] Duyu Tang, Bing Qin, Xiaocheng Feng, and Ting Liu. 2015a. Target-Dependent Sentiment Classification with Long Short Term Memory</title>
		<editor>Bing Qin, and Ting Liu</editor>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1422" to="1432" />
		</imprint>
	</monogr>
	<note type="report_type">ArXiv preprint</note>
	<note>Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Target-dependent twitter sentiment classification with rich automatic features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duy-Tin</forename><surname>Vo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Fourth International Joint Conference on Artificial Intelligence (IJCAI 2015)</title>
		<meeting>the Twenty-Fourth International Joint Conference on Artificial Intelligence (IJCAI 2015)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1347" to="1353" />
		</imprint>
	</monogr>
	<note>and Zhang2015</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Dcu: Aspectbased polarity classification for semeval task 4</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Wagner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1410.3916</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th International Workshop on Semantic Evaluation</title>
		<meeting>the 8th International Workshop on Semantic Evaluation<address><addrLine>Weston</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="223" to="229" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Sumit Chopra, and Antoine Bordes. 2014. Memory networks</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Multichannel variable-size convolution for sentence classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="204" to="214" />
		</imprint>
	</monogr>
	<note>Proceedings of the Nineteenth Conference on Computational Natural Language Learning</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Long short-term memory over tree structures</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 32nd International Conference on Machine Learning</title>
		<meeting>The 32nd International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1604" to="1612" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
