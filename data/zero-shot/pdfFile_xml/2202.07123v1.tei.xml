<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Published as a conference paper at ICLR 2022 RETHINKING NETWORK DESIGN AND LOCAL GEOM- ETRY IN POINT CLOUD: A SIMPLE RESIDUAL MLP FRAMEWORK</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Ma</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Northeastern University</orgName>
								<address>
									<settlement>Boston</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Can</forename><surname>Qin</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Northeastern University</orgName>
								<address>
									<settlement>Boston</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoxuan</forename><surname>You</surname></persName>
							<email>haoxuanyou@gmail.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Columbia University</orgName>
								<address>
									<settlement>New York</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoxi</forename><surname>Ran</surname></persName>
							<email>ran.h@northeastern.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Northeastern University</orgName>
								<address>
									<settlement>Boston</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Fu</surname></persName>
							<email>yunfu@ece.neu.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Northeastern University</orgName>
								<address>
									<settlement>Boston</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Published as a conference paper at ICLR 2022 RETHINKING NETWORK DESIGN AND LOCAL GEOM- ETRY IN POINT CLOUD: A SIMPLE RESIDUAL MLP FRAMEWORK</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T13:13+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Point cloud analysis is challenging due to irregularity and unordered data structure. To capture the 3D geometries, prior works mainly rely on exploring sophisticated local geometric extractors using convolution, graph, or attention mechanisms. These methods, however, incur unfavorable latency during inference, and the performance saturates over the past few years. In this paper, we present a novel perspective on this task. We notice that detailed local geometrical information probably is not the key to point cloud analysis -we introduce a pure residual MLP network, called PointMLP, which integrates no "sophisticated" local geometrical extractors but still performs very competitively. Equipped with a proposed lightweight geometric affine module, PointMLP delivers the new stateof-the-art on multiple datasets. On the real-world ScanObjectNN dataset, our method even surpasses the prior best method by 3.3% accuracy. We emphasize that PointMLP achieves this strong performance without any sophisticated operations, hence leading to a superior inference speed. Compared to most recent CurveNet, PointMLP trains 2? faster, tests 7? faster, and is more accurate on ModelNet40 benchmark. We hope our PointMLP may help the community towards a better understanding of point cloud analysis. The code is available at</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Lately, point cloud analysis has emerged as a popular topic in 3D understanding, attracting attention from academia and industry <ref type="bibr" target="#b29">(Qi et al., 2017a;</ref><ref type="bibr" target="#b34">Shi et al., 2019;</ref><ref type="bibr" target="#b54">Xu et al., 2020)</ref>. Different from 2D images represented by regular dense pixels, point clouds are composed of unordered and irregular sets of points P ? R N ?3 , making it infeasible to apply image processing methods to point cloud analysis directly. Meanwhile, the nature of sparseness and the presence of noises further restrict the performance. In the past few years, endowing with neural networks, point cloud analysis has seen a great improvement in various applications, including 3D shape classification <ref type="bibr" target="#b29">(Qi et al., 2017a)</ref>, semantic segmentation  and object detection <ref type="bibr" target="#b36">(Shi &amp; Rajkumar, 2020)</ref>, etc.</p><p>Recent efforts have shown promising results for point cloud analysis by exploring local geometric information, using convolution <ref type="bibr" target="#b16">(Li et al., 2021a)</ref>, graph <ref type="bibr" target="#b16">(Li et al., 2021a)</ref>, or attention mechanism (Guo et al., 2021) (see Section 2 for details). These methods, despite their gratifying results, have mainly relied on the premise that an elaborate local extractor is essential for point cloud analysis, leading to the competition for careful designs that explore fine local geometric properties. Nevertheless, sophisticated extractors are not without drawbacks. On the one hand, due to prohibitive computations and the overhead of memory access, these sophisticated extractors hamper the efficiency of applications in natural scenes. As an example, until now, most 3D point cloud applications are still based on the simple PointNet <ref type="bibr">( and PointNet++)</ref> or the voxel-based methods <ref type="bibr" target="#b20">Li et al., 2021b;</ref>. However, applications that employ the aforementioned advanced methods are rare in literature. On the other hand, the booming sophisticated extractors saturate the performance since they already describe the local geometric properties well. A more complicated design is no longer to improve the performance further. These phenomena suggest that we may need to stop the race of local feature extraction designing, rethinking the necessity of elaborate local feature extractors and further revisiting the succinct design philosophy in point cloud analysis. Overall accuracy Inference speed (samples/second) <ref type="figure">Figure 1</ref>: Accuracy-speed tradeoff on Model-Net40. Our PointMLP performs best. Please refer to Section 4 for details.</p><p>In this paper, we aim at the ambitious goal of building a deep network for point cloud analysis using only residual feed-forward MLPs, without any delicate local feature explorations. By doing so, we eschew the prohibitive computations and continued memory access caused by the sophisticated local geometric extractors and enjoy the advantage of efficiency from the highly-optimized MLPs. To further improve the performance and generalization ability, We introduce a lightweight local geometric affine module that adaptively transforms the point feature in a local region. We term our new network architecture as PointMLP. In the sense of MLPbased design philosophy, our PointMLP is similar to <ref type="bibr">PointNet and PointNet++ (Qi et al., 2017a;</ref><ref type="bibr">b)</ref>. However, our model is more generic and exhibits promising performance. Different from the models with sophisticated local geometric extractors (e.g., DeepGCNs , RSCNN <ref type="bibr" target="#b23">(Liu et al., 2019b)</ref>, RPNet <ref type="bibr" target="#b33">(Ran et al., 2021)</ref>.), our PointMLP is conceptually simpler and achieves results on par or even better than these state-of-the-art methods (see <ref type="figure">Figure 1</ref>). Keep in mind that we did not challenge the advantages of these local geometric extractors and we acknowledge their contributions; however, a more succinct framework should be studied considering both the efficiency and accuracy. In <ref type="table" target="#tab_1">Table 1</ref>, we systemically compare our PointMLP with some representative methods.</p><p>Even though the design philosophy is simple, PointMLP (as well as the elite version) exhibits superior performance on 3D point cloud analysis. Specifically, we achieve the state-of-the-art classification performance, 94.5%, on the ModelNet40 benchmark, and we outperform related works by 3.3% accuracy on the real-world ScanObjectNN dataset, with a significantly higher inference speed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Point cloud analysis. There are mainly two streams to process point cloud. Since the point cloud data structure is irregular and unordered, some works consider projecting the original point clouds to intermediate voxels <ref type="bibr" target="#b28">(Maturana &amp; Scherer, 2015;</ref> or images <ref type="bibr" target="#b58">(You et al., 2018;</ref>, translating the challenging 3D task into a well-explored 2D image problem. In this regime, point clouds understanding is largely boosted and enjoys the fast processing speed from 2D images or voxels. Albeit efficient, information loss caused by projection degrades the representational quality of details for point clouds <ref type="bibr" target="#b55">(Yang et al., 2019)</ref>. To this end, some methods are proposed to process the original point cloud sets directly. PointNet (Qi et al., 2017a) is a pioneering work that directly consumes unordered point sets as inputs using shared MLPs. Based on PointNet, PointNet++ (Qi et al., 2017b) further introduced a hierarchical feature learning paradigm to capture the local geometric structures recursively. Owing to the local point representation (and multi-scale information), PointNet++ exhibits promising results and has been the cornerstone of modern point cloud methods <ref type="bibr" target="#b5">Fan et al., 2021;</ref><ref type="bibr" target="#b51">Xu et al., 2021a)</ref>. Our PointMLP also follows the design philosophy of PointNet++ but explores a simpler yet much deeper network architecture.</p><p>Local geometry exploration. As PointNet++ built the generic point cloud analysis network framework, the recent research focus is shifted to how to generate better regional points representation. Predominantly, the explorations of local points representation can be divided into three categories: convolution-, graph-, and attention-based methods. One of the most distinguished convolution-based methods is PointConv <ref type="bibr" target="#b45">(Wu et al., 2019)</ref>. By approximating continuous weight and density functions in convolutional filters using an MLP, PointConv is able to extend the dynamic filter to a new convolution operation. Also, PAConv <ref type="bibr" target="#b51">(Xu et al., 2021a)</ref> constructs the convolution kernel by dynamically assembling basic weight matrices stored in a weight bank. Without modifying network configurations, PAConv can be seamlessly integrated into classical MLP-based pipelines. Unlike convolution-based methods, Graph-based methods investigate mutually correlated relationships among points with a graph. In , an EdgeConv is proposed to generate edge features that describe the relationships between a point and its neighbors. By doing so, a local graph is built, and the point relationships are well preserved. In 3D-GCN <ref type="bibr" target="#b21">(Lin et al., 2021)</ref>, authors aim at deriving deformable 3D kernels using a 3D Graph Convolution Network. Closely related to graphbased methods, the attention-based methods exhibit excellent ability on relationship exploration as well, like <ref type="bibr">PCT (Guo et al., 2021)</ref> and Point Transformer <ref type="bibr" target="#b61">(Zhao et al., 2021;</ref><ref type="bibr" target="#b4">Engel et al., 2020)</ref>.</p><p>With the development of local geometry exploration, the performances on various tasks appear to be saturated. Continuing on this track would bring minimal improvements. In this paper, we showcase that even without the carefully designed operations for local geometry exploration, a pure deep hierarchical MLP architecture is able to exhibit gratifying performances and even better results. Atten.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PointMLP MLP</head><p>Deep network architecture for point cloud. Interestingly, the development of point cloud analysis is closely related to the evolution of the image processing network. In the early era, works in the image processing field simply stack several learning layers to probe the performance limitations <ref type="bibr" target="#b14">(Krizhevsky et al., 2012;</ref><ref type="bibr" target="#b37">Simonyan &amp; Zisserman, 2015;</ref><ref type="bibr" target="#b2">Dong et al., 2014)</ref>. Then, the great success of deep learning was significantly promoted by deep neural architectures like ResNet <ref type="bibr" target="#b10">(He et al., 2016)</ref>, which brings a profound impact to various research fields. Recently, attention-based models, including attention blocks <ref type="bibr" target="#b43">(Wang et al., 2018)</ref>   <ref type="bibr" target="#b45">(Wu et al., 2019;</ref><ref type="bibr" target="#b33">Ran et al., 2021)</ref>, to state-of-the-art Transformer-based models <ref type="bibr" target="#b61">Zhao et al., 2021)</ref>. In this paper, we abandon sophisticated details and present a simple yet effective deep residual MLP network for point cloud analysis. Instead of following the tendency in the vision community deliberately, we are in pursuit of an inherently simple and empirically powerful architecture for point cloud analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">DEEP RESIDUAL MLP FOR POINT CLOUD</head><p>We propose to learn the point cloud representation by a simple feed-forward residual MLP network (named PointMLP), which hierarchically aggregates the local features extracted by MLPs, and abandons the use of delicate local geometric extractors. To further improve the robustness and improve the performance, we also introduce a lightweight geometric affine module to transform the local points to a normal distribution. The detailed framework of our method is illustrated in <ref type="figure" target="#fig_2">Figure 2</ref> Given a set of points P = {p i |i = 1, ? ? ? , N } ? R N ?3 , where N indicates the number of points in a (x, y, z) Cartesian space, point-based methods aims to directly learn the underlying representation f of P using neural networks. One of the most pioneering works is PointNet++, which learns hierarchical features by stacking multiple learning stages. In each stage, N s points are re-sampled by In each stage, we first transform the local points using a geometric affine module, then they are extracted before and after the aggregation operation, respectively. PointMLP progressively enlarges the receptive field and models complete point cloud geometric information by repeating multiple stages.</p><p>the farthest point sampling (FPS) algorithm where s indexes the stage and K neighbors are employed for each sampled point and aggregated by max-pooling to capture local structures. Conceptually, the kernel operation of PointNet++ can be formulated as:</p><formula xml:id="formula_0">g i = A (? (f i,j ) |j = 1, ? ? ? , K) ,<label>(1)</label></formula><p>where A (?) means aggregation function (max-pooling in PointNet++), ? (?) denotes the local feature extraction function (MLP in PointNet++), and f i,j is the j-th neighbor point feature of i-th sampled point. By doing so, PointNet++ is able to effectively capture local geometric information and progressively enlarge the receptive fields by repeating the operation.</p><p>In the sense of network architecture design, PointNet++ exhibits a universal pipeline for point cloud analysis. Following this pipeline, some plug-and-play methods have been proposed, mainly focusing on the local feature extractor ? (?) <ref type="bibr" target="#b51">(Xu et al., 2021a;</ref><ref type="bibr" target="#b23">Liu et al., 2019b;</ref><ref type="bibr" target="#b39">Thomas et al., 2019;</ref><ref type="bibr" target="#b61">Zhao et al., 2021)</ref>. Generally, these local feature extractors thoroughly explore the local geometric information using convolution, graph, or self-attention mechanisms. In RSCNN <ref type="bibr" target="#b23">(Liu et al., 2019b)</ref>, the extractor is mainly achieved by exploring point relations as follow:</p><formula xml:id="formula_1">? (f i,j ) = MLP x i,j ? x i 2 , x i,j ? x i , x i,j , x i * f i,j , ?j ? {1, ? ? ? , K} ,<label>(2)</label></formula><p>where [?] is the concatenation operation and MLP is a small network composed of a Fully-connected (FC) layer, Batch Normalization layer, and activation function. Unlike RSCNN, Point Transformer introduces the self-attention mechanism into point cloud analysis and considers the similarities between pair-wise points in a local region. To this end, it re-formulates the extractor as:</p><formula xml:id="formula_2">? (f i ) = k j=1 ? (? (? (f i ) ? ? (f i,j ) + ?)) (? (f i,j + ?)) ,<label>(3)</label></formula><p>where ?, ?, ? and ? are linear mapping function, " " is a Hadamard product, and ? is a softmax normalization. In particular, Point Transformer introduces a relative position encoding, While these methods can easily take the advantage of detailed local geometric information and usually exhibit promising results, two issues limit their development. First, with the introduction of delicate extractors, the computational complexity is largely increased, leading to prohibitive infer-ence latency 1 . For example, the FLOPs of Equation 3 in Point Transformer would be 14Kd 2 , ignoring the summation and subtraction operations. Compared with the conventional FC layer that enjoys 2Kd 2 FLOPs, it increases the computations by times. Notice that the memory access cost is not considered yet. Second, with the development of local feature extractors, the performance gain has started to saturate on popular benchmarks. Moreover, empirical analysis in <ref type="bibr" target="#b24">Liu et al. (2020)</ref> reveals that most sophisticated local extractors make surprisingly similar contributions to the network performance under the same network input. Both limitations encourage us to develop a new method that circumvents the employment of sophisticated local extractors, and provides gratifying results.</p><formula xml:id="formula_3">? = ? (x i ? x i,j ),</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">FRAMEWORK OF POINTMLP</head><p>In order to get rid of the restrictions mentioned above, we present a simple yet effective MLP-based network for point cloud analysis that no sophisticated or heavy operations are introduced. The key operation of our PointMLP can be formulated as:</p><formula xml:id="formula_4">g i = ? pos (A (? pre (f i,j ) , |j = 1, ? ? ? , K)) ,<label>(4)</label></formula><p>where ? pre (?) and ? pos (?) are residual point MLP blocks: the shared ? pre (?) is designed to learn shared weights from a local region while the ? pos (?) is leveraged to extract deep aggregated features. In detail, the mapping function can be written as a series of homogeneous residual MLP blocks, MLP (x) + x, in which MLP is combined by FC, normalization and activation layers (repeated two times). Following Qi et al. <ref type="formula" target="#formula_0">(2017a)</ref>, we consider the aggregation function A (?) as max-pooling operation. Equation 4 describes one stage of of PointMLP. For a hierarchical and deep network, we recursively repeat the operation by s stages. Albeit the framework of PointMLP is succinct, it exhibits some prominent merits. 1) Since PointMLP only leverages MLPs, it is naturally invariant to permutation, which perfectly fits the characteristic of point cloud. 2) By incorporating residual connections, PointMLP can be easily extended to dozens layers, resulting deep feature representations. 3) In addition, since there is no sophisticated extractors included and the main operation is only highly optimized feed-forward MLPs, even we introduce more layers, our PointMLP still performs efficiently. Unless explicitly stated, the networks in our experiments use four stages, and two residual blocks in both ? pre (?) and ? pos (?). We employ k-nearest neighbors algorithm (kNN) to select the neighbors and set the number K to 24.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">GEOMETRIC AFFINE MODULE</head><p>While it may be easy to simply increase the depth by considering more stages or stacking more blocks in ? pre and ? pos , we notice that a simple deep MLP structure will decrease the accuracy and stability, making the model less robust. This is perhaps caused by the sparse and irregular geometric structures in local regions. Diverse geometric structures among different local regions may require different extractors but shared residual MLPs struggle at achieving this. We flesh out this intuition and develop a lightweight geometric affine module to tackle this problem. Let {f i,j } j=1,??? ,k ? R k?d be the grouped local neighbors of f i ? R d containing k points, and each neighbor point f i,j is a d-dimensional vector. We transform the local neighbor points by the following formulation:</p><formula xml:id="formula_5">{f i,j } = ? {f i,j } ? f i ? + + ?, ? = 1 k ? n ? d n i=1 k j=1 (f i,j ? f i ) 2 ,<label>(5)</label></formula><p>where ? ? R d and ? ? R d are learnable parameters, indicates Hadamard production, and = 1e ?5 is a small number for numerical stability <ref type="bibr" target="#b13">(Ioffe &amp; Szegedy, 2015;</ref><ref type="bibr" target="#b46">Wu &amp; He, 2018;</ref><ref type="bibr" target="#b1">Dixon &amp; Massey Jr, 1951)</ref>. Note that ? is a scalar describes the feature deviation across all local groups and channels. By doing so, we transform the local points to a normal distribution while maintaining original geometric properties.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">COMPUTATIONAL COMPLEXITY AND ELITE VERSION</head><p>Although the FC layer is highly optimized by mainstream deep learning framework, the theoretical number of parameters and computational complexity are still high. To further improve the efficiency, Empirically, we do not observe a significant performance drop. This method reduce the parameters of residual MLP blocks from 2d 2 to 2 r d 2 . By default, we set r to 4 in PointMLPelite. Besides, we also slightly adjust the network architecture, reducing both the MLP blocks and embedding dimension number (see appendix for details). Inspired by <ref type="bibr" target="#b49">Xie et al. (2017)</ref>, we also investigated a grouped FC operation in the network that divides one FC layer into g groups of sub-FC layers, like group convolution layer. However, we empirically found that this strategy would largely hamper the performance. As a result, we did not consider it in our implementation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>In this section, we comprehensively evaluate PointMLP on several benchmarks. Detailed ablation studies demonstrate the effectiveness of PointMLP with both quantitative and qualitative analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">SHAPE CLASSIFICATION ON MODELNET40</head><p>We first evaluate PointMLP on the ModelNet40 <ref type="bibr" target="#b47">(Wu et al., 2015)</ref> benchmark, which contains 9,843 training and 2,468 testing meshed CAD models belonging to 40 categories. Following the standard practice in the community, we report the class-average accuracy (mAcc) and overall accuracy (OA) on the testing set. We train all models for 300 epochs using SGD optimizer. Experimental results are presented in <ref type="table" target="#tab_2">Table 2</ref>. Among these methods, our PointMLP clearly outperforms state-of-the-art method CurveNet by 0.3% (94.5% vs. 94.2%) overall accuracy with only 1k points. Note that this improvement could be considered as a promising achievement since the results on ModelNet40 recent methods have been saturated around 94% for a long time. Even without the voting strategy <ref type="bibr" target="#b23">(Liu et al., 2019b)</ref>, our PointMLP still performs on par or even better than other methods that are tested with voting strategy.</p><p>Despite having better accuracy, our method is much faster than the methods with sophisticated local geometric extractors. We compare PointMLP to several open-sourced methods and report the parameters, classification accuracy, training, and testing speed. As we stated previously, a key intuition behind this experiment is that model complexity can not directly reflect efficiency. For example, CurveNet is lightweight and delivers a strong result, whereas the inference cost is prohibitive (15 samples/second). On the contrary, our PointMLP presents a high inference speed (112 samples/second). To further reduce the model size and speed up the inference, we present a lightweight PointMLP-elite, which significantly reduces the number of parameters to 0.68M, while maintaining high-performance 90.9% mAcc and 94.0% OA on ModelNet40. With PointMLP-elite, we further speed up the inference to 176 samples/second.  <ref type="bibr" target="#b53">(Xu et al., 2018)</ref> 69.8 73.7 PointNet++ <ref type="bibr" target="#b30">(Qi et al., 2017b)</ref> 75.4 77.9 DGCNN  73.6 78.1 PointCNN <ref type="bibr" target="#b19">(Li et al., 2018b)</ref> 75.1 78.5 BGA-DGCNN <ref type="bibr" target="#b41">(Uy et al., 2019)</ref> 75.7 79.7 BGA-PN++ <ref type="bibr" target="#b41">(Uy et al., 2019)</ref> 77.5 80.2 DRNet <ref type="bibr" target="#b31">(Qiu et al., 2021a)</ref> 78.0 80.3 GBNet <ref type="bibr" target="#b32">(Qiu et al., 2021b)</ref> 77.8 80.5 SimpleView <ref type="bibr">(Goyal et al., 2021)</ref> -80.5?0.3 PRANet <ref type="bibr" target="#b0">(Cheng et al., 2021)</ref> 79. While ModelNet40 is the de-facto canonical benchmark for point cloud analysis, it may not meet the requirement of modern methods due to its synthetic nature and the fast development of point cloud analysis. To this end, we also conduct experiments on the ScanObjectNN benchmark <ref type="bibr" target="#b41">(Uy et al., 2019)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">SHAPE CLASSIFICATION ON SCANOBJECTNN</head><p>ScanObjectNN is a recently released point cloud benchmark that contains 15,000 objects that are categorized into 15 classes with 2,902 unique object instances in the real world. Due to the existence of background, noise, and occlusions, this benchmark poses significant challenges to existing point cloud analysis methods. We consider the hardest perturbed variant (PB T50 RS) in our experiments. We train our model using an SGD optimizer for 200 epochs with a batch size of 32. For a better illustration, we train and test our method for four runs and report the mean ? standard deviation in <ref type="table" target="#tab_3">Table 3</ref>.  Empirically, our PointMLP surpasses all methods by a significant improvement on both class mean accuracy (mAcc) and the overall accuracy (OA). For example, we outperform PRANet by 4.8% mAcc and 3.3% OA. Even compared with the heavy multi-view projection method MVTN (12 views), our PointMLP still performs much better (85.39% 82.8%). Notice that we achieve this by fewer training epochs and did not consider the voting strategy. Moreover, we notice that our method achieves the smallest gap between class mean accuracy and overall accuracy. This phenomenon indicates that PointMLP did not bias to a particular category, showing decent robustness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">ABLATION STUDIES</head><p>Network Depth. Network depth has been exploited in many tasks but is rare in point cloud analysis. We first investigate the performance of PointMLP with different depths in <ref type="table" target="#tab_5">Table 4</ref>. We vary the network depth by setting the number of homogeneous residual MLP blocks to 1, 2, and 3, respectively, resulting in 24, 40, and 56-layers PointMLP variants. Detailed depth formulation can be found in Appendix D. At first glance, we notice that simply increasing the depth would not always bring better performance; an appropriate depth would be a good solution. Additionally, the model gets stable with more layers introduced, as demonstrated by the decreasing standard deviation. When the depth is set to 40, we achieve the best tradeoff between accuracy and stability (85.4% mean accuracy and 0.3 standard deviations). Remarkably, PointMLP consistently achieves gratifying results that outperform recent methods, regardless of the depth.</p><p>Geometric Affine Module. Other work provides sophisticated local geometric extractors to explore geometric structures. Instead, our PointMLP discards these burdensome modules and introduces a lightweight geometric affine module. <ref type="figure" target="#fig_4">Figure 3</ref> presents the results of PointMLP with/without the geometric affine module. By integrating the module, we systematically improve the performance of PointMLP by about 3% for all variants. The reasons for this large improvement are two-fold. First, the geometric affine module maps local input features to a normal distribution, which eases the training of PointMLP. Second, the geometric affine module implicitly encodes the local geometrical information by the channel-wise distance to local centroid and variance, remedying the deficiency of geometric information. Besides the gratifying improvements, the geometric affine module also largely boosts the stability of PointMLP, suggesting better robustness. Component ablation study. <ref type="table" target="#tab_6">Table 5</ref> reports the results on ScanOb-jectNN of removing each individual component in PointMLP. Consistent with <ref type="figure" target="#fig_4">Figure 3</ref>, geometric affine module plays an important role in PointMLP, improving the base architecture by 3.9%.</p><p>Remarkably, even without this module, which is an unfair setting for PointMLP, our base network stills achieves 81.5 ? 1.4% OA, outperforming most related methods (see <ref type="table" target="#tab_3">Table 3</ref>). Removing ? pre function (MLPs before aggregator A), the performance drops 2.6% overall accuracy. Combining all these components together, we achieve the best result 85.4% OA. See Appendix C for more ablations.</p><p>Loss landscape. We depict the 3D loss landscape <ref type="bibr" target="#b17">(Li et al., 2018a)</ref> in <ref type="figure" target="#fig_5">Figure 4</ref>. Simply increasing the network depth may not achieve a better representation and even hamper the results. When removing the residual connection in PointMLP, the loss landscape turns sharp, and the performance plummets to 88.1% (6% drop) on ModelNet40. With residual connection, we greatly ease the optimization course of PointMLP and make it possible to train a deep network. Our PointMLP can also be generalized to other 3D point cloud tasks. We next test PointMLP for 3D shape part segmentation task on the ShapeNetPart benchmark <ref type="bibr" target="#b56">(Yi et al., 2016)</ref>. The shapeNetPart dataset consists of 16,881 shapes with 16 classes belonging to 50 parts labels in total.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">PART SEGMENTATION</head><p>In each class, the number of parts is between 2 and 6. We follow the settings from Qi et al. (2017b) that randomly select 2048 points as input for a fair comparison. We compare our methods with several recent works, including SyncSpecCNN <ref type="bibr" target="#b57">(Yi et al., 2017)</ref>, SPLATNet <ref type="bibr" target="#b38">(Su et al., 2018)</ref>, etc. We also visualize the segmentation ground truths and predictions in <ref type="figure" target="#fig_6">Figure 5</ref>. Intuitively, the predictions of our PointMLP are close to the ground truth. Best viewed in color.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>In this paper, we propose a simple yet powerful architecture named PointMLP for point cloud analysis. The key insight behind PointMLP is that a sophisticated local geometric extractor may not be crucial for performance. We begin with representing local points with simple residual MLPs as they are permutation-invariant and straightforward. Then we introduce a lightweight geometric affine module to boost the performance. To improve efficiency further, we also introduce a lightweight counterpart, dubbed as PointMLP-elite. Experimental results have shown that PointMLP outperforms related work on different benchmarks beyond simplicity and efficiency. We hope this novel idea will inspire the community to rethink the network design and local geometry in point cloud.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A POINTMLP DETAIL</head><p>We detail the architecture of PointMLP in <ref type="figure">Figure 6</ref> (as well as PointMLP-elite in <ref type="figure" target="#fig_7">Figure 7</ref>) for a better understanding. Compared with PointMLP, the elite version mainly adjusts three configurations: 1) it reduces the number of residual point (Resp) MLP blocks; 2) it reduces the embedding dimension from 64 to 32, hence the overall model overhead is significantly alleviated; 3) by introducing a bottleneck structure, PointMLP further reduces the parameters by four times.</p><p>For part segmentation task, we use the framework presented in PointNet <ref type="bibr" target="#b29">(Qi et al., 2017a)</ref> and replace the backbone to our PointMLP. With the only modification, we improve the performance from 85.1 to 86.1 Instance mIoU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input Embedding</head><p>Aggregation  Our implementations are based on PyTorch. For ModelNet40, we train models for 300 epochs on one Tesla V100 GPU with a batch size of 32. All our models are trained using synchronous SGD with a Nesterov momentum of 0.9 and a weight decay of 0.0002. The learning rate is set to 0.1 initially. We use the cosine annealing scheduler <ref type="bibr" target="#b26">(Loshchilov &amp; Hutter, 2017)</ref> to adjust the learning rate. For each sample, we randomly select 1024 points and consider the same augmentation strategy as <ref type="bibr" target="#b30">Qi et al. (2017b)</ref>. The setting for ScanObjectNN is similar to ModelNet40, except we train all models for only 200 epochs.</p><formula xml:id="formula_6">? !"# Resp MLP ? !$% Resp</formula><p>For the reported speed in <ref type="table" target="#tab_2">Table 2</ref>, we test the open-source code on a Tesla V100-pcie GPU. All the source codes we used are listed 2 in the footnote.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 SHAPENETPART</head><p>Our setting for part segmentation task is following PointNet <ref type="bibr" target="#b29">(Qi et al., 2017a)</ref>. We randomly sample 2048 points for each sample and re-scale the input in a range of [0.67, 1.5]. Note that we did not test the result using a multi-scale testing strategy, which could further improve the performance, but is not realizable in real-world applications. Hence, we only report the single-scale results. Even the comparison is unfair, we still achieve competitive performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C MORE DETAILED ABLATION STUDIES</head><p>Skip connection. <ref type="figure" target="#fig_5">Figure 4</ref> shows the loss landscapes of our PointMLP with and without skip connections. We also consider adding skip connections to PointNet++ to validate the effectiveness of skip connections. Due to the structure of PointNet++, only two skip connections could be added without modifying the original architecture of PointNet++. By adding the skip connections, we achieve a classification accuracy of 92.7% on ModelNet40 in our re-implementation.</p><p>Pre-MLP block vs. Pos-MLP block. we also modified the configuration of our PointMLP and retrained the model to investigate the importance of Pre-MLP and Pos-MLP blocks. In our original implementation, we set the pre-MLP block list to [2, 2, 2, 2] and the pos-MLP blocks list to [2, 2, 2, 2]. Here, we remove the pos-MLP blocks and change the pre-MLP blocks to <ref type="bibr">[4,</ref><ref type="bibr">4,</ref><ref type="bibr">4,</ref><ref type="bibr">4]</ref> to match the block number. The 3-layer classifier can be considered as the MLP at the end of the last stage. We trained the models two times and got an average OA of 84.13% (83.87% and 84.39%), which is lower than vanilla PointMLP 85.4%, and even the result in <ref type="table" target="#tab_6">Table 5</ref> second-row 84.7%. This result indicates that pos-MLP does benefit our PointMLP, and simply adding more pre-MLP blocks does not help. We acknowledge that the effect of pos-MLP is not as strong as other components and believe that a detailed fine-tuning of the configurations would deliver an even better performance-efficiency balance.</p><p>Geometric Affine Module Applications. Geometric affine module plays an essential role in our PointMLP, exhibiting promising performance improvements. While this module can be considered as a plug-and-play method, the overlap with some local geometric extractors in other methods may limit its application. Here we integrate the module to two popular methods, PointNet++ and DGCNN, for illustration and experiment on the ModelNet40 benchmark. By integrating the geometric affine module, we improve the performance of PointNet++ to 93.3%, achieving an improvement of 1.4%. However, when integrating the module to DGCNN, we get a performance of 92.8%, which is slightly lower than the original results (92.9%). Note that both results are tested without voting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D POINTMLP DEPTH</head><p>Here we format the detailed formulation of layer number in our PointMLP. For the sake of clarity, we ignore Batch Normalization layers and activation functions. Let Pre i and Pos i indicate the repeating number of the ? pre block (which includes 3 layers) and ? pos block (which includes 2 layers) in i-th stage, respectively. Note that we have one layer in feature embedding in the beginning, one layer for channel number matching in each stage, and three layers in the classifier. Hence, the total number of learnable layers L would be</p><formula xml:id="formula_7">L = 1 + 4 i=1</formula><p>(1 + 2 ? Pre i + 2 ? Pos i ) + 3.</p><p>As a result, the depth configuration of our network (24, 40, and 56) can be summarized as: </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>and Transformer architectures (Dosovitskiy et al., 2021), further flesh out the community. Most recently, the succinct deep MLP architectures have attracted a lot of attention due to their efficiency and generality. Point cloud analysis follows the same develop history as well, from MLP-based PointNet (Qi et al., 2017a), deep hierarchical PointNet++ (Qi et al., 2017b), convolution-/graph-/relation-based methods</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>point-based methods for point cloud analysis dates back to the PointNet and Point-Net++ papers (Qi et al., 2017a;b), if not earlier. The motivation behind this direction is to directly consume point clouds from the beginning and avoid unnecessary rendering processes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Overview of one stage in PointMLP. Given an input point cloud, PointMLP progressively extracts local features using residual point MLP blocks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>where the relative position is encoded by two FC layers with a ReLU nonlinearity layer, into both attention weights and features. The lightweight positional encoder largely improves the performance of Point Transformer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Four run results (mean ? std) of PointMLP with/without our geometric affine module on ScanObjectNN test set. We zoom in on the details of PointMLP40 to show the stability difference.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Loss landscape along two rand directions. By introducing residual connection, we ease the optimization of PointMLP and achieve a flat landscape like a simple shallow network intuitively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Part segmentation results on ShapeNetPart. Top line is ground truth and bottom line is our prediction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :</head><label>7</label><figDesc>Detail architecture of PointMLP-elite for classification. B DETAIL EXPERIMENTAL SETTING B.1 MODELNET40 AND SCANOBJECTNN</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1</head><label>1</label><figDesc></figDesc><table><row><cell cols="3">: Systematic comparison among some representa-</cell></row><row><cell cols="3">tive methods. "Deep" indicates that a model is expand-</cell></row><row><cell cols="3">able along depth. "Opt." stands for the principal operator.</cell></row><row><cell>Method</cell><cell>hierarchy locality deep</cell><cell>opt.</cell></row><row><cell>PointNet</cell><cell></cell><cell>MLP</cell></row><row><cell>PointNet++</cell><cell></cell><cell>MLP</cell></row><row><cell>DGCNN</cell><cell></cell><cell>GCN</cell></row><row><cell>DeepGCNs</cell><cell></cell><cell>GCN</cell></row><row><cell>PointConv</cell><cell></cell><cell>Conv.</cell></row><row><cell>Point Trans.</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Classification results on ModelNet40 dataset. With only 1k points, our method achieves state-of-the-art results on both class mean accuracy (mAcc) and overall accuracy (OA) metrics. We also report the speed of some open-sourced methods by samples/second tested on one Tesla V100pcie GPU and four cores AMD EPYC 7351@2.60GHz CPU. * For KPConv, we take the results from the original paper. The best is marked in bold and second best is in blue.</figDesc><table><row><cell>Method</cell><cell>Inputs</cell><cell cols="3">mAcc(%) OA(%) Param.</cell><cell>Train speed</cell><cell>Test speed</cell></row><row><cell>PointNet (Qi et al., 2017a)</cell><cell>1k P</cell><cell>86.0</cell><cell>89.2</cell><cell></cell><cell></cell><cell></cell></row><row><cell>PointNet++ (Qi et al., 2017b)</cell><cell>1k P</cell><cell>-</cell><cell>90.7</cell><cell cols="3">1.41M 223.8 308.5</cell></row><row><cell>PointNet++ (Qi et al., 2017b)</cell><cell>5k P+N</cell><cell>-</cell><cell>91.9</cell><cell>1.41M</cell><cell></cell><cell></cell></row><row><cell>PointCNN (Li et al., 2018b)</cell><cell>1k P</cell><cell>88.1</cell><cell>92.5</cell><cell></cell><cell></cell><cell></cell></row><row><cell>PointConv (Wu et al., 2019)</cell><cell>1k P+N</cell><cell>-</cell><cell>92.5</cell><cell>18.6M</cell><cell>17.9</cell><cell>10.2</cell></row><row><cell>KPConv (Thomas et al., 2019)</cell><cell>7k P</cell><cell>-</cell><cell>92.9</cell><cell cols="3">15.2M 31.0* 80.0*</cell></row><row><cell>DGCNN (Wang et al., 2019)</cell><cell>1k P</cell><cell>90.2</cell><cell>92.9</cell><cell></cell><cell></cell><cell></cell></row><row><cell>RS-CNN (Liu et al., 2019b)</cell><cell>1k P</cell><cell>-</cell><cell>92.9</cell><cell></cell><cell></cell><cell></cell></row><row><cell>DensePoint (Liu et al., 2019a)</cell><cell>1k P</cell><cell>-</cell><cell>93.2</cell><cell></cell><cell></cell><cell></cell></row><row><cell>PointASNL (Yan et al., 2020)</cell><cell>1k P</cell><cell>-</cell><cell>92.9</cell><cell></cell><cell></cell><cell></cell></row><row><cell>PosPool (Liu et al., 2020)</cell><cell>5k P</cell><cell>-</cell><cell>93.2</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Point Trans. (Engel et al., 2020)</cell><cell>1k P</cell><cell>-</cell><cell>92.8</cell><cell></cell><cell></cell><cell></cell></row><row><cell>GBNet (Qiu et al., 2021b)</cell><cell>1k P</cell><cell>91.0</cell><cell>93.8</cell><cell>8.39M</cell><cell>16.3</cell><cell>112</cell></row><row><cell>GDANet (Xu et al., 2021b)</cell><cell>1k P</cell><cell>-</cell><cell>93.8</cell><cell>0.93M</cell><cell>26.3</cell><cell>14.0</cell></row><row><cell>PA-DGC (Xu et al., 2021a)</cell><cell>1k P</cell><cell>-</cell><cell>93.9</cell><cell></cell><cell></cell><cell></cell></row><row><cell>MLMSPT (Han et al., 2021)</cell><cell>1k P</cell><cell>-</cell><cell>92.9</cell><cell></cell><cell></cell><cell></cell></row><row><cell>PCT (Guo et al., 2021)</cell><cell>1k P</cell><cell>-</cell><cell>93.2</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Point Trans. (Zhao et al., 2021)</cell><cell>1k P</cell><cell>90.6</cell><cell>93.7</cell><cell></cell><cell></cell><cell></cell></row><row><cell>CurveNet (Xiang et al., 2021)</cell><cell>1k P</cell><cell>-</cell><cell>94.2</cell><cell>2.04M</cell><cell>20.8</cell><cell>15.0</cell></row><row><cell>PointMLP w/o vot.</cell><cell>1k P</cell><cell>91.3</cell><cell>94.1</cell><cell>12.6M</cell><cell>47.1</cell><cell>112</cell></row><row><cell>PointMLP w/ vot.</cell><cell>1k P</cell><cell>91.4</cell><cell>94.5</cell><cell>12.6M</cell><cell>47.1</cell><cell>112</cell></row><row><cell>PointMLP-elite w/o vot.</cell><cell>1k P</cell><cell>90.9</cell><cell>93.6</cell><cell>0.68M</cell><cell>116</cell><cell>176</cell></row><row><cell>PointMLP-elite w/ vot.</cell><cell>1k P</cell><cell>90.7</cell><cell>94.0</cell><cell>0.68M</cell><cell>116</cell><cell>176</cell></row><row><cell cols="7">we introduce a lightweight version of PointMLP named as pointMLP-elite, with less than 0.7M</cell></row><row><cell cols="7">parameters and prominent inference speed (176 samples/second on ModelNet40 benchmark).</cell></row></table><note>Inspired by He et al. (2016); Hu et al. (2018), we present a bottleneck structure for the mapping function ? pre and ? pos . We opt to reduce the channel number of the intermediate FC layer by a factor of r and increase the channel number as the original feature map. This strategy is opposite to the design in Vaswani et al. (2017); Touvron et al. (2021) which increases the intermediate feature dimensions.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Classification results on ScanObjectNN dataset. We examine all methods on the most challenging variant (PB T50 RS). For our pointMLP and PointMLP-elite, we train and test for four runs and report mean ? std results.</figDesc><table><row><cell>Method</cell><cell>mAcc(%)</cell><cell>OA(%)</cell></row><row><cell>3DmFV</cell><cell>58.1</cell><cell>63</cell></row><row><cell>PointNet (Qi et al., 2017a)</cell><cell>63.4</cell><cell>68.2</cell></row><row><cell>SpiderCNN</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Classification accuracy of pointMLP on ScanObjectNN test set using 24, 40, and 56 layers, respectively.</figDesc><table><row><cell>Depth</cell><cell>mAcc(%)</cell><cell>OA(%)</cell></row><row><cell cols="3">24 layers 83.4?0.4 84.8?0.5</cell></row><row><cell cols="3">40 layers 83.9?0.5 85.4?0.3</cell></row><row><cell cols="3">56 layers 83.2?0.2 85.0?0.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Component ablation studies on ScanObjectNN test set.</figDesc><table><row><cell>?pre ?pos Affine mAcc(%)</cell><cell>OA(%)</cell></row><row><cell cols="2">80.8?0.4 82.8?0.0</cell></row><row><cell cols="2">83.3?0.3 84.7?0.2</cell></row><row><cell cols="2">79.1?1.7 81.5?1.4</cell></row><row><cell cols="2">83.9?0.5 85.4?0.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>Part segmentation results on the ShapeNetPart dataset. Empirically, our method is much faster than the best method KPConv, and presents a competitive performance.</figDesc><table><row><cell>Method</cell><cell>Cls. mIoU</cell><cell>Inst. mIoU</cell><cell>aero bag</cell><cell>cap</cell><cell>car</cell><cell>chair</cell><cell>aerp-hone</cell><cell cols="3">guitar knife lamp laptop</cell><cell>motor-bike</cell><cell cols="2">mug pistol rocket</cell><cell>skate-board</cell><cell>table</cell></row><row><cell>PointNet</cell><cell>80.4</cell><cell>83.7</cell><cell cols="3">83.4 78.7 82.5 74.9</cell><cell cols="2">89.6 73.0</cell><cell>91.5</cell><cell>85.9 80.8</cell><cell>95.3</cell><cell>65.2</cell><cell>93.0 81.2</cell><cell>57.9</cell><cell>72.8</cell><cell>80.6</cell></row><row><cell>PointNet++</cell><cell>81.9</cell><cell>85.1</cell><cell cols="3">82.4 79.0 87.7 77.3</cell><cell cols="2">90.8 71.8</cell><cell>91.0</cell><cell>85.9 83.7</cell><cell>95.3</cell><cell>71.6</cell><cell>94.1 81.3</cell><cell>58.7</cell><cell>76.4</cell><cell>82.6</cell></row><row><cell>Kd-Net</cell><cell>-</cell><cell>82.3</cell><cell cols="3">80.1 74.6 74.3 70.3</cell><cell cols="2">88.6 73.5</cell><cell>90.2</cell><cell>87.2 81.0</cell><cell>94.9</cell><cell>57.4</cell><cell>86.7 78.1</cell><cell>51.8</cell><cell>69.9</cell><cell>80.3</cell></row><row><cell>SO-Net</cell><cell>-</cell><cell>84.9</cell><cell cols="3">82.8 77.8 88.0 77.3</cell><cell cols="2">90.6 73.5</cell><cell>90.7</cell><cell>83.9 82.8</cell><cell>94.8</cell><cell>69.1</cell><cell>94.2 80.9</cell><cell>53.1</cell><cell>72.9</cell><cell>83.0</cell></row><row><cell>PCNN</cell><cell>81.8</cell><cell>85.1</cell><cell cols="3">82.4 80.1 85.5 79.5</cell><cell cols="2">90.8 73.2</cell><cell>91.3</cell><cell>86.0 85.0</cell><cell>95.7</cell><cell>73.2</cell><cell>94.8 83.3</cell><cell>51.0</cell><cell>75.0</cell><cell>81.8</cell></row><row><cell>DGCNN</cell><cell>82.3</cell><cell>85.2</cell><cell cols="3">84.0 83.4 86.7 77.8</cell><cell cols="2">90.6 74.7</cell><cell>91.2</cell><cell>87.5 82.8</cell><cell>95.7</cell><cell>66.3</cell><cell>94.9 81.1</cell><cell>63.5</cell><cell>74.5</cell><cell>82.6</cell></row><row><cell>P2Sequence</cell><cell>-</cell><cell>85.2</cell><cell cols="3">82.6 81.8 87.5 77.3</cell><cell cols="2">90.8 77.1</cell><cell>91.1</cell><cell>86.9 83.9</cell><cell>95.7</cell><cell>70.8</cell><cell>94.6 79.3</cell><cell>58.1</cell><cell>75.2</cell><cell>82.8</cell></row><row><cell>PointCNN</cell><cell>84.6</cell><cell>86.1</cell><cell cols="3">84.1 86.5 86.0 80.8</cell><cell cols="2">90.6 79.7</cell><cell>92.3</cell><cell>88.4 85.3</cell><cell>96.1</cell><cell>77.2</cell><cell>95.2 84.2</cell><cell>64.2</cell><cell>80.0</cell><cell>83.0</cell></row><row><cell>PointASNL</cell><cell>-</cell><cell>86.1</cell><cell cols="3">84.1 84.7 87.9 79.7</cell><cell cols="2">92.2 73.7</cell><cell>91.0</cell><cell>87.2 84.2</cell><cell>95.8</cell><cell>74.4</cell><cell>95.2 81.0</cell><cell>63.0</cell><cell>76.3</cell><cell>83.2</cell></row><row><cell>RS-CNN</cell><cell>84.0</cell><cell>86.2</cell><cell cols="3">83.5 84.8 88.8 79.6</cell><cell cols="2">91.2 81.1</cell><cell>91.6</cell><cell>88.4 86.0</cell><cell>96.0</cell><cell>73.7</cell><cell>94.1 83.4</cell><cell>60.5</cell><cell>77.7</cell><cell>83.6</cell></row><row><cell>SynSpec</cell><cell>82.0</cell><cell>84.7</cell><cell cols="3">81.6 81.7 81.9 75.2</cell><cell cols="2">90.2 74.9</cell><cell>93.0</cell><cell>86.1 84.7</cell><cell>95.6</cell><cell>66.7</cell><cell>92.7 81.6</cell><cell>60.6</cell><cell>82.9</cell><cell>82.1</cell></row><row><cell>SPLATNet</cell><cell>83.7</cell><cell>85.4</cell><cell cols="3">83.2 84.3 89.1 80.3</cell><cell cols="2">90.7 75.5</cell><cell>92.1</cell><cell>87.1 83.9</cell><cell>96.3</cell><cell>75.6</cell><cell>95.8 83.8</cell><cell>64.0</cell><cell>75.5</cell><cell>81.8</cell></row><row><cell>SpiderCNN</cell><cell>82.4</cell><cell>85.3</cell><cell cols="3">83.5 81.0 87.2 77.5</cell><cell cols="2">90.7 76.8</cell><cell>91.1</cell><cell>87.3 83.3</cell><cell>95.8</cell><cell>70.2</cell><cell>93.5 82.7</cell><cell>59.7</cell><cell>75.8</cell><cell>82.8</cell></row><row><cell>KPConv</cell><cell>85.1</cell><cell>86.4</cell><cell cols="3">84.6 86.3 87.2 81.1</cell><cell cols="2">91.1 77.8</cell><cell>92.6</cell><cell>88.4 82.7</cell><cell>96.2</cell><cell>78.1</cell><cell>95.8 85.4</cell><cell>69.0</cell><cell>82.0</cell><cell>83.6</cell></row><row><cell>PA-DGC</cell><cell>84.6</cell><cell>86.1</cell><cell cols="3">84.3 85.0 90.4 79.7</cell><cell cols="2">90.6 80.8</cell><cell>92.0</cell><cell>88.7 82.2</cell><cell>95.9</cell><cell>73.9</cell><cell>94.7 84.7</cell><cell>65.9</cell><cell>81.4</cell><cell>84.0</cell></row><row><cell>PointMLP</cell><cell>84.6</cell><cell>86.1</cell><cell cols="5">83.5 83.4 87.5 80.54 90.3 78.2</cell><cell>92.2</cell><cell>88.1 82.6</cell><cell>96.2</cell><cell>77.5</cell><cell>95.8 85.4</cell><cell>64.6</cell><cell>83.3</cell><cell>84.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>Depth [Pre 1 , Pre 2 , Pre 3 , Pre 4 ] [Pos 1 , Pos 2 , Pos 3 , Pos 4 ]</figDesc><table><row><cell>24</cell><cell>[1, 1, 1, 1]</cell><cell>[1, 1, 1, 1]</cell></row><row><cell>40</cell><cell>[2, 2, 2, 2]</cell><cell>[2, 2, 2, 2]</cell></row><row><cell>56</cell><cell>[3, 3, 3, 3]</cell><cell>[3, 3, 3, 3]</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">We emphasize that the model complexity could not be simply revealed by FLOPs or parameters, other metrics like memory access cost (MAC) and the degree of parallelism also significantly affect the speed<ref type="bibr" target="#b27">(Ma et al., 2018;</ref>. However, these important metrics are always ignored in point clouds analysis.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Pra-net: Point relation-aware network for 3d point cloud analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silin</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiwu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinwei</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="4436" to="4448" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Introduction to statistical analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wilfrid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank J Massey</forename><surname>Dixon</surname><genName>Jr</genName></persName>
		</author>
		<imprint>
			<date type="published" when="1951" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning a deep convolutional network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="184" to="199" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">An image is worth 16x16 words: Transformers for image recognition at scale. ICLR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nico</forename><surname>Engel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vasileios</forename><surname>Belagiannis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Dietmayer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.00931</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">Point transformer. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Scf-net: Learning spatial contextual features for large-scale point cloud segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiulei</forename><surname>Siqi Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fenghua</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yisheng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peijun</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei-Yue</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="14504" to="14513" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Revisiting point cloud shape classification with a simple and effective baseline</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankit</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hei</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Xiong</forename><surname>Meng-Hao Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng-Ning</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tai-Jiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shi-Min</forename><surname>Ralph R Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hu</surname></persName>
		</author>
		<title level="m">Pct: Point cloud transformer. Computational Visual Media</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="187" to="199" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Mvtn: Multi-view transformation network for 3d shape recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdullah</forename><surname>Hamdi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Giancola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xian-Feng</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Jia</forename><surname>Kuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guo-Qiang</forename><surname>Xiao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.13636</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Point cloud learning with transformer</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7132" to="7141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Randla-net: Efficient semantic segmentation of large-scale point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingyong</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linhai</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Rosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhihua</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Trigoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Markham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="11108" to="11117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<editor>F. Pereira, C. J. C. Burges, L. Bottou, and K. Q. Weinberger</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2012" />
			<biblScope unit="volume">25</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deepgcns: Can gcns go as deep as cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guohao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Thabet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9267" to="9276" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Deepgcns: Making gcns go as deep as cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guohao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guocheng</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carolina</forename><forename type="middle">Delgadillo</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdulellah</forename><surname>Abualshour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><forename type="middle">Kassem</forename><surname>Thabet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Visualizing the loss landscape of neural nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gavin</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Studer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Goldstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6391" to="6401" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">End-to-end learning local multi-view descriptors for 3d point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyu</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongbo</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chiew-Lan</forename><surname>Tai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1919" to="1928" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangyan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Bu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingchao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinhan</forename><surname>Di</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoquan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pointcnn</surname></persName>
		</author>
		<title level="m">Convolution on x-transformed points. NeurIPS</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="820" to="830" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Lidar r-cnn: An efficient and universal 3d object detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhichao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naiyan</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="7546" to="7555" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Learning of 3d graph convolution networks for point cloud analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi-Hao</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><forename type="middle">Yu</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Chiang Frank</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Densepoint: Learning densely contextual representation for efficient point cloud processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongcheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaofeng</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwen</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5239" to="5248" />
		</imprint>
	</monogr>
	<note>Shiming Xiang, and Chunhong Pan</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Relation-shape convolutional neural network for point cloud analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongcheng</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Bin Fan, Shiming Xiang, and Chunhong Pan</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8895" to="8904" />
		</imprint>
	</monogr>
	<note>CVPR</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A closer look at local aggregation operators in point cloud analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Tong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="326" to="342" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Group-free 3d object detection via transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Tong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.00678</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Sgdr: Stochastic gradient descent with warm restarts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Shufflenet v2: Practical guidelines for efficient cnn architecture design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ningning</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai-Tao</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="116" to="131" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Voxnet: A 3d convolutional neural network for real-time object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Maturana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Scherer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IROS</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="922" to="928" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Pointnet: Deep learning on point sets for 3d classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Charles R Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaichun</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="652" to="660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Pointnet++: Deep hierarchical feature learning on point sets in a metric space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Charles Ruizhongtai Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Dense-resolution network for point cloud classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saeed</forename><surname>Shi Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Anwar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Barnes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WACV</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="3813" to="3822" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Geometric back-projection network for point cloud classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saeed</forename><surname>Shi Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Anwar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Barnes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Multimedia</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning inner-group relations on point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoxi</forename><surname>Ran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="15477" to="15487" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Pointrcnn: 3d object proposal generation and detection from point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoshuai</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="770" to="779" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Pv-rcnn: Point-voxel feature set abstraction for 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoshuai</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaoxu</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10529" to="10538" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Point-gnn: Graph neural network for 3d object detection in a point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijing</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raj</forename><surname>Rajkumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1711" to="1719" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1409.1556" />
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<editor>Yoshua Bengio and Yann LeCun</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Splatnet: Sparse lattice networks for point cloud processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhransu</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evangelos</forename><surname>Kalogerakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2530" to="2539" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Kpconv: Flexible and deformable convolution for point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugues</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Emmanuel</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beatriz</forename><surname>Deschaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fran?ois</forename><surname>Marcotegui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Goulette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6411" to="6420" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alaaeldin</forename><surname>El-Nouby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Verbeek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.03404</idno>
		<title level="m">Resmlp: Feedforward networks for image classification with data-efficient training</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Revisiting point cloud classification: A new benchmark dataset and classification model on real-world data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikaela</forename><forename type="middle">Angelina</forename><surname>Uy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quang-Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Binh-Son</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thanh</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sai-Kit</forename><surname>Yeung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1588" to="1597" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7794" to="7803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Dynamic graph cnn for learning on point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongbin</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Sanjay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sarma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><forename type="middle">M</forename><surname>Michael M Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Solomon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Acm Transactions On Graphics (tog)</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Pointconv: Deep convolutional networks on 3d point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenxuan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongang</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fuxin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9621" to="9630" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Group normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">3d shapenets: A deep representation for volumetric shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhirong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuran</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linguang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxiong</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1912" to="1920" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Walk in the cloud: Learning curves for point clouds shape analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiange</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaoyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianhui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weidong</forename><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1492" to="1500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Squeezesegv3: Spatially-adaptive convolution for efficient point-cloud segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenfeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bichen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zining</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Vajda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Keutzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masayoshi</forename><surname>Tomizuka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Paconv: Position adaptive convolution with dynamic kernel assembling on point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mutian</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Runyu</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="3173" to="3182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Learning geometry-disentangled representation for complementary understanding of 3d object point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mutian</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhipeng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingye</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="3056" to="3064" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Spidercnn: Deep learning on point sets with parameterized convolutional filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingye</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="87" to="102" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Pointasnl: Robust point clouds processing using nonlocal neural networks with adaptive sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaoda</forename><surname>Xu Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuguang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5589" to="5598" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Std: Sparse-to-dense 3d object detector for point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zetong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="1951" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">A scalable active framework for region annotation in 3d shape collections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Vladimir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duygu</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I-Chao</forename><surname>Ceylan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengyan</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixing</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alla</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><surname>Sheffer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (ToG)</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Syncspeccnn: Synchronized spectral cnn for 3d shape segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingwen</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2282" to="2290" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Pvnet: A joint convolutional network of point cloud and multi-view for 3d shape recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoxuan</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rongrong</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1310" to="1318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chongruo</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongyue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibin</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manmatha</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.08955</idno>
		<title level="m">Split-attention networks</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Self-supervised pretraining of 3d features on any point-cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zaiwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohit</forename><surname>Girdhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.02691</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Point transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
		<ptr target="https://github.com/DylanWusee/pointconvKPConv:https://github.com/HuguesTHOMAS/KPConv-PyTorch" />
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note>1024,3] [1024,64. 512,24,64. 512,128. 256,24,128. 256,256. 128,24,256. 128,512. 64,24,512. 64,1024] [1024,3] [1024,32. 512,24,32. 512,64. 256,24,64. 256,128] [128,24,128] [128,256. 64,24,256] [64,256] 2 all tested methods are listed bellow PointNet++</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
