<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Contextual Convolutional Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ionut</forename><forename type="middle">Cosmin</forename><surname>Duta</surname></persName>
							<email>*icduta@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Bucharest</orgName>
								<address>
									<region>SecurifAI</region>
									<country>Romania, Romania</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mariana</forename><forename type="middle">Iuliana</forename><surname>Georgescu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Bucharest</orgName>
								<address>
									<region>SecurifAI</region>
									<country>Romania, Romania</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><forename type="middle">Tudor</forename><surname>Ionescu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Bucharest</orgName>
								<address>
									<region>SecurifAI</region>
									<country>Romania, Romania</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Contextual Convolutional Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T16:14+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose contextual convolution (CoConv) for visual recognition. CoConv is a direct replacement of the standard convolution, which is the core component of convolutional neural networks. CoConv is implicitly equipped with the capability of incorporating contextual information while maintaining a similar number of parameters and computational cost compared to the standard convolution. CoConv is inspired by neuroscience studies indicating that (i) neurons, even from the primary visual cortex (V1 area), are involved in detection of contextual cues and that (ii) the activity of a visual neuron can be influenced by the stimuli placed entirely outside of its theoretical receptive field. On the one hand, we integrate Co-Conv in the widely-used residual networks and show improved recognition performance over baselines on the core tasks and benchmarks for visual recognition, namely image classification on the ImageNet data set and object detection on the MS COCO data set. On the other hand, we introduce CoConv in the generator of a state-of-the-art Generative Adversarial Network, showing improved generative results on CIFAR-10 and CelebA. Our code is available at</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Contextual information is vital for a visual perception system. A point (or a small patch) in a scene (image) is mostly meaningless without the surrounding contextual information. For instance, it is very difficult for a person to provide a semantic label or a description for a small patch (taken from an image) without providing a broader visual context. As shown in the example illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>, it is even hard to label an entire object without context, let alone some part of the respective object. In neuroscience research, the critical role of the contextual influences on visual perception systems is well proven since long time ago <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b38">38]</ref>. For example, Zipser et al. <ref type="bibr" target="#b38">[38]</ref> studied the contextual modulation in the primary visual cortex of awake, behaving macaque monkeys. The work shows that the activity of a visual neuron is influenced by the stimuli placed entirely outside of its default receptive field. Furthermore, the work demonstrates that the influence of the context on the activity of a visual neuron is present even at the early stages of the visual system (V1 area). Albright et al. <ref type="bibr" target="#b0">[1]</ref> stated that for each local region of an image, the extraction of semantic meaning is only possible if information from other regions is taken into account. This clearly highlights the importance of contextual information in the natural visual systems studied in the field of neuroscience. Convolutional neural networks (CNNs) <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b19">19]</ref> represent the backbone of nearly every current computer vision task and application <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b21">21,</ref><ref type="bibr" target="#b20">20,</ref><ref type="bibr" target="#b30">30,</ref><ref type="bibr" target="#b31">31,</ref><ref type="bibr" target="#b34">34,</ref><ref type="bibr" target="#b35">35,</ref><ref type="bibr" target="#b39">39]</ref>. Although the neuroscience studies mentioned above <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b38">38]</ref> clearly demonstrate the presence and the importance of contextual influence in a visual neuron of a biological visual cortex, in the current artificially-built visual systems, the core building block of CNNs, represented by the convolutional layer (with spatial filters that activate on local patterns), is not implicitly equipped with the ability of integrating contextual information. In general, the convolutional filters have a limited receptive field, usually corresponding to a 3?3 spatial kernel size, due to the fact that increasing the kernel size brings additional costs in terms of parameters and computational resources. There are many approaches that address the integration of contextual information, e.g. <ref type="bibr" target="#b32">[32]</ref>, however, most of them follow the direction of integrating additional building blocks in the CNN to incorporate contextual information. However, this line of research results in additional costs for the CNN in terms of both parameters and computation, which is not in line with the neuroscience findings, pointing out that the visual system is extremely efficient and that the integration of the contextual information is an implicit capability of the visual neuron. Inspired by the aforementioned neuroscience studies and addressing the above limitations, this work makes the following contributions: ? We propose contextual convolution (CoConv), a direct replacement of the standard convolution that can be used at any stage in CNN architectures. CoConv is implicitly equipped with the ability of accessing contextual information at multiple levels without increasing the demands in terms of parameters and computational cost, compared to the standard convolution (see Section 3). ? We integrate CoConv in convolutional and generative networks of various depths, presenting novel architectures based on CoConv for visual recognition and generation (see Section 4). ? We show improved detection, recognition and generation performance obtained by CoConv over the standard convolution and competing methods on the core tasks and benchmarks for visual recognition and generation (see <ref type="bibr">Section 5)</ref>. We underline that our approach, CoConv, is both effective and efficient. We believe that its simplicity coupled with its effectiveness generates a great potential to become widely-adopted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>There are numerous works with the goal of integrating contextual information in an artificial visual system. The work of Wang et al. <ref type="bibr" target="#b32">[32]</ref> introduced a non-local block to capture contextual information in a CNN. In <ref type="bibr" target="#b10">[11]</ref>, the authors proposed a squeeze-and-excitation block to capture global information and scale each feature map accordingly. However, these works propose additional building blocks that need to be inserted in the CNN, therefore bringing significant supplementary parameters and computational costs that can negatively impact the efficiency of the visual system. In contrast, we propose to implicitly integrate the process of capturing contextual information in the core component (the convolutional layer), without increasing the number of parameters and computational costs. Furthermore, our approach can be complementary to these works, as they still need to use convolutional layers, in their overall CNN architectures. Dilated convolution <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b36">36,</ref><ref type="bibr" target="#b37">37]</ref> is an approach to enlarge the receptive field of the convolution kernel. Our work makes use of dilated convolution, however, there are significant differences in the approach and usage from previous works. For example, Chen et al. <ref type="bibr" target="#b1">[2]</ref> proposed atrous spatial pyramid pooling (ASPP) to segment objects at multiple scales. There are fundamental differences that distinguish our work from that of Chen et al. <ref type="bibr" target="#b1">[2]</ref>, as explained next. First, ASPP is proposed just as a head module for image segmentation, while our approach is designed as a direct replacement of the convolution along all stages of the CNN architecture, irrespective of the visual recognition task. Second, different from ASPP, our approach is specifically designed to integrate contextual information at different levels while maintaining the same number of parameters and computational costs as the standard convolution. Importantly, as in the neuroscience findings <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b38">38]</ref> showing that contextual influence is present and relevant, even in the primary visual cortex (V1 area), we integrate the contextual information within all network layers, including the early convolutional layers as well. Contrary to the neuroscience findings, Chen et al. <ref type="bibr" target="#b1">[2]</ref> employed contextual modeling only at the end of the CNN, just as an additional head before the final classification layer for semantic segmentation. A more closely related work to our own is <ref type="bibr" target="#b37">[37]</ref>, which proposes dilated residual networks by integrating dilated convolution just towards the end of the network. Thus, this work is also not in line with the neuroscience conclusion regarding the importance of the contextual modeling at the early stages of the visual cortex. Furthermore, our approach is different from that of Yu et al. <ref type="bibr" target="#b37">[37]</ref>, as we employ different levels of contextual information, being able to capture information about local details and various levels of contextual information in the same time. Importantly, integrating the default approach of Yu et al. <ref type="bibr" target="#b37">[37]</ref> into residual networks for image recognition drastically increases the demands in computational resources. As shown in the experiments, our approach delivers improved recognition performance without increasing the computational costs.</p><p>Another contribution that is aimed at capturing context in all stages of neural architectures is represented by capsule networks (CapsNets) <ref type="bibr" target="#b28">[28]</ref>. Although CapsNets are also backed by neurosciene studies and showed promising results on small data sets such as MNIST and CIFAR-10, their low accuracy gains come with a large computational cost. Furthermore, since their introduction by Sabour et al. <ref type="bibr" target="#b28">[28]</ref>, CapsNets have failed to show their effectiveness on very deep neural networks and on large image recognition benchmarks, mostly due to their extremely large computational costs. Different from <ref type="bibr" target="#b28">[28]</ref>, we present empirical evidence showing that CoConv improves the accuracy of very deep models, e.g. ResNet-152, on very large benchmarks, e.g. ImageNet, at no additional cost. Another plus is that our contribution is fairly easy to implement, having the right ingredients (simple, effective, no additional computational cost) to be widely adopted by the community.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Contextual Convolution</head><p>The standard convolution in state-of-the-art CNN architectures uses a single type of kernel with a fixed receptive field, usually corresponding to a kernel size of 3?3, since Input Feature Maps (M in )  <ref type="figure">Figure 2</ref>: (a) Contextual Convolution (CoConv). Instead of using standard or dilated convolution, we propose to integrate multiple levels of kernels with different dilation ratios in the convolutional layer. At each level, we have multiple kernels. We emphasize the fact that, in this illustration, d 1 = 1, d 2 = 2 and d 3 = 3 is a coincidence that facilitates visualization, yet, in general, we do not constrain d i to be equal to i. Best viewed in color. (b) An example of CoConv residual building block.</p><formula xml:id="formula_0">? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? Contextual</formula><p>increasing the kernel size brings additional costs in terms of the number of learnable parameters and computational time, respectively. The number of learnable parameters (weights) and FLOPs (floating point operations) for the standard convolution can be computed as:</p><formula xml:id="formula_1">params = M in ? K w ? K h ? M out , FLOPs = M in ? K h ? K w ? M out ? W out ? H out ,<label>(1)</label></formula><p>where, M in and M out represent the number of input and output feature maps, K w and K h are the width and height of the convolution kernel, and finally, W out and H out are the width and height of the output feature maps. For the sake of simplicity, we ignored the bias terms and hyperparameters such as stride and padding in Equation <ref type="formula" target="#formula_1">(1)</ref>. Contextual convolution (CoConv), illustrated in <ref type="figure">Figure 2a</ref>, receives a number of input feature maps M in , over which we apply different levels L = {1, 2, 3, ..., n} of convolution kernels with varying dilation ratios D = {d 1 , d 2 , d 3 , ..., d n }. In other words, the kernels at level i have the dilation ratio d i , ?i ? L. By gradually increasing the dilation ratio (basically introducing increasingly larger "holes" into the kernels), the filters can have access to increasingly broader contextual information. As we increase the dilation ratio, the kernels become sparser, thus, being applied over the input feature maps in a sparse pattern, skipping elements in the computation. As depicted in <ref type="figure">Figure 2a</ref>, only the colored spatial locations of the kernels are involved in the computation of the output feature maps. Thus, each level of dilated kernels maintains a similar number of parameters and FLOPs, while increasing the spatial receptive field to integrate more contextual information. The kernels with lower dilation ratios are responsible for capturing information about local details from the input feature maps, while the kernels with higher dilation ratios are empowered with the ability of incorporating contextual information for helping the recognition process. At each level i, the kernels provide a number of output feature maps M out i , for all i ? L, each map having the width W out and the height H out . Hence, the total number of learnable parameters and FLOPs of CoConv is computed as follows:</p><formula xml:id="formula_2">params= M in ?(K w ?K h ) (d1) ?M out 1 + ... + M in ?(K w ?K h ) (dn) ?M out n , FLOPs = M in ?(K w ?K h ) (d1) ?M out 1 ?W out ?H out + ...+M in ?(K w ?K h ) (dn) ?M out n ?W out ?H out ,<label>(2)</label></formula><p>where, (K w ? K h ) (di) refers to the spatial size of the kernel of width w and height h (basically, how many spatial locations of the kernel are involved in the computation of an output feature map), d i points to the dilation ratio used for the kernels at level i, and:</p><formula xml:id="formula_3">M out = n i=1 M out i , ?i ? L.<label>(3)</label></formula><p>Although we use multiple levels of kernels with different dilation ratios, the total number of kernels in CoConv is equal to the total number of kernels in the standard convolution, as it results from Equation <ref type="formula" target="#formula_3">(3)</ref>. We emphasize that all levels and kernels in CoConv are independent, allowing parallel execution, just as in a standard convolutional layer. CoConv is a direct replacement of the standard convolution with the capability of integrating contextual information. Moreover, as formally presented in Equations <ref type="formula" target="#formula_2">(2)</ref>, the number of learnable parameters and FLOPs involved in CoConv is equal to those involved in the standard convolution from Equation <ref type="formula" target="#formula_1">(1)</ref>. We emphasize that the number of dilation levels of CoConv can be adjusted, for instance, based on the resolution of the feature maps. We present various practical examples in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Contextual Convolutional Neural Networks</head><p>We describe some neural architectures based on contextual convolution (CoConv). First, we integrate CoConv in the widely-used residual networks (ResNets) <ref type="bibr" target="#b7">[8]</ref>. ResNets can be split into four main stages depending on the proximity of the layers with respect to the input and, implicitly, on the resolution of the feature maps, as shown in <ref type="table" target="#tab_3">Table 1</ref>. <ref type="figure">Figure 2b</ref> shows an example of a CoConv residual building block used in the first stage of a network. The CoConv residual block uses a 1?1 convolution to reduce the number of feature maps to 64, followed by a CoConv with four levels to capture contextual information. Each CoConv level has a different dilation ratio. The number of output feature maps at each level is 16, regardless of the dilation rate. Then, a 1?1 convolution is used to restore the number of feature maps to 256. As in the standard residual blocks, batch normalization (BN) <ref type="bibr" target="#b12">[13]</ref> and Rectified Linear Unit (ReLU) activations <ref type="bibr" target="#b25">[25]</ref> follow each convolutional block.</p><p>Our network for image classification, termed contextual residual network (CoResNet), is formally presented in <ref type="table" target="#tab_3">Table 1</ref>. Although we illustrate our updates on ResNet-50, thus obtaining CoResNet-50, the changes can be analogously operated on models of different depths. Since the size of the feature maps decreases as the layers are farther away from the input, we also adapt the number of levels in our CoConv layers with respect to the resolution of the feature maps. Hence, the first main stage of the network uses four levels in the CoConv layers, with different dilation ratios. Further, the second stage uses three levels in the CoConv layers, while the third stage uses two levels. As the spatial resolution of the feature maps in the last stage is reduced to 7?7, we consider that using multiple dilation ratios is no longer justified. Thus, the last stage employs just one level of CoConv. In the experimental section, we provide an ablation study on the number of levels in the CoConv layers. However, the number of levels can be tuned for each particular task or application, based, for instance, on the resolution of the feature maps along the network. Based on empirical evidence, we consider that our network has improved recognition capabilities compared to the standard ResNet, as the convolution is equipped with the ability of integrating contextual information at multiple levels and, as can be seen in <ref type="table" target="#tab_3">Table 1</ref>   weights nor it enlarges the computational cost compared to the original network.</p><formula xml:id="formula_4">? ? ??3 ? ? ? ? ? ? ? ? ? ? ? ? 1?1, 64 CoConv4, 64: ? ? ? ? ? 3?3, 16, d 1 =1 3?3, 16, d 2 =2 3?3, 16, d 3 =3 3?3, 16, d 4 =4 ? ? ? ? ? 1?1, 256 ? ? ? ? ? ? ? ? ? ? ? ? ?3 2 28?28 ? ? ? 1?1, 128 3?3, 128 1?1, 512 ? ? ??4 ? ? ? ? ? ? ? ? ? ? 1?1, 128 CoConv3, 128: ? ? ? 3?3, 64, d 1 =1 3?3, 32, d 2 =2 3?3, 32, d 3 =3 ? ? ? 1?1, 512 ? ? ? ? ? ? ? ? ? ? ?4 3 14?14 ? ? ? 1?1,</formula><p>We also show the benefits of integrating CoConv in a generative adversarial network (GAN) <ref type="bibr" target="#b4">[5]</ref>. We specifically consider progressive GAN (ProGAN) <ref type="bibr" target="#b13">[14]</ref>, a model that generates high-resolution images starting with a lowresolution output (4?4 pixels) and gradually adding layers to the network to produce a high-resolution output (up to 1024 ? 1024 pixels). We used the exact same architecture described in <ref type="bibr" target="#b13">[14]</ref>, only replacing the convolutional layers with CoConv layers.  ator for the CelebA data set <ref type="bibr" target="#b24">[24]</ref> is illustrated in <ref type="table" target="#tab_5">Table 2</ref>. The CoProGAN generator designed for the final output of 128?128 pixels starts with a feature map size of 4?4 pixels. Hence, at the first layer, we only use a dilation rate of 1.</p><p>When the output size increases to 8?8 pixels, we add two dilation rates of 1 and 2. Similarly, we add three dilation rates <ref type="figure" target="#fig_0">(1, 2 and 3)</ref>, when the output size is 16?16 pixels. When the output size is 32?32, 64?64 or 128?128 pixels, we use four dilation rates of 1, 2, 3 and 4, regardless of the size of the output. The number of filters in each CoConv layer matches the number of filters in the corresponding conv layer from ProGAN. Thus, the number of parameters and FLOPs in ProGAN and CoProGAN are identical.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Experimental setup</head><p>We perform object recognition experiments on the ImageNet Large Scale Visual Recognition Challenge (ILSVRC) <ref type="bibr" target="#b27">[27]</ref>, which is one of the most popular benchmarks in visual recognition. The ImageNet data set consists of 1000 classes of objects, 1.28 million training images and 50K validation images. As common, we report both the top-1 and top-5 error rates. We follow the standard settings in <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9]</ref> and employ the Stochastic Gradient Descent (SGD) optimizer with a standard momentum rate of 0.9 and a weight decay of 0.0001. We perform the training for 90 epochs, starting with a learning rate of 0.1, reducing it by 1/10 at the 30-th, 60-th and 80-th epochs, similarly to <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b7">8]</ref>. Each model is trained using 8 GPUs. We use the standard mini-batch size of 256 for training and data augmentation as in <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b31">31]</ref>, training and testing on 224?224 image crops. For the object detection task, we consider the MS COCO data set <ref type="bibr" target="#b22">[22]</ref>, which contains 80 object categories. We use COCO 2017 train (118K images) for training and COCO 2017 val (5K images) for testing. We train each model for 130 epochs on 8 GPUs using mini-batches of 32 examples, resulting in 60K training iterations. The training is performed using the SGD optimizer with momentum 0.9, weight decay 0.0005, with the learning rate 0.02 (reduced by 1/10 before the 86-th and 108-th epochs). We also use a linear warm-up in the first epoch, following <ref type="bibr" target="#b5">[6]</ref>. For data augmentation, we perform random crop as in <ref type="bibr" target="#b23">[23]</ref>, color jittering and horizontal flip. We consider an input image size of 300 ? 300 pixels. As evaluation metrics, we report the average precision (AP) and the AP@IoU=0.5.</p><p>We conduct image generation experiments on CIFAR-10 <ref type="bibr" target="#b15">[16]</ref> and CelebA <ref type="bibr" target="#b24">[24]</ref>, considering only fully unsupervised (not class conditional) GAN models. The CIFAR-10 training set is composed of 50K images of 32 ? 32 pixels, while the CelebA training set is formed of 202K images of 128 ? 128 pixels. Following Karras et al. <ref type="bibr" target="#b13">[14]</ref>, the optimization is performed using the Adam <ref type="bibr" target="#b14">[15]</ref> optimizer with ? 1 = 0, ? 2 = 0.99, = 10 ?8 and the learning rate set to 10 ?3 . We train each model until the discriminator sees 12 million real images in total. Each model is trained on a single GeForce GTX 3090 GPU. As evaluation measures, we report the Inception Score (IS) <ref type="bibr" target="#b29">[29]</ref> and the Fr?chet Inception Distance (FID) <ref type="bibr" target="#b9">[10]</ref> for CIFAR-10, and the multiscale structural similarity index measure (MS-SSIM) <ref type="bibr" target="#b33">[33]</ref> and the Sliced Wasserstein Distance (SWD) <ref type="bibr" target="#b26">[26]</ref> for CelebA. To reproduce the results of ProGAN, we used the official Tensor-Flow implementation available at https://github.com/ tkarras/progressive_growing_of_gans.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Ablation experiments on CoConv dilation levels</head><p>In <ref type="table" target="#tab_7">Table 3</ref>, we present ablation experiments with different configurations generated by varying the number of dilation levels in the ConConv residual blocks corresponding to each stage of the network. The first column indicates the number of levels in the CoConv residual block used in each   of the four main stages of the network. The configuration <ref type="figure" target="#fig_0">(1, 1, 1, 1)</ref> refers to the case where a single CoConv level with dilation d 1 = 1 is used in all four stages, thus being completely equivalent to the original ResNet <ref type="bibr" target="#b7">[8]</ref>, which is the baseline in our object recognition experiments.</p><p>Integrating our CoConv with two levels for the first three stages of the network, resulting in the configuration (2, 2, 2, 1), significantly improves the top-1 error rate from 23.88% to 23.24%, while maintaining the same number of parameters and FLOPs. In general, adding more levels to our CoConv residual blocks further improves the results.</p><p>We obtain the best results with the configuration (4, 3, 2, 1) for the number of levels in the CoConv blocks involved in the four main stages of the network. We hereby note that we performed experiments with even more levels of CoConv, but we did not notice further significant improvements in terms recognition performance. Hence, we find the configuration (4, 3, 2, 1) optimal for an input size of 224?224 pixels. If the input size would be higher, then perhaps another configuration considering more dilation levels can provide even further improvements. All in all, the flexibility of adapting the CoConv levels for each network stage with respect to the input resolution is an important strong point of our approach.</p><p>To show the importance of having different levels of dilation in CoConv, we include the configuration top(4, 3, 2, 1) in <ref type="table" target="#tab_7">Table 3</ref>, which considers only the highest level of dilation in each stage of the network. For instance, in the first stage, only the convolution with dilation ratio 4 is used, the second stage uses only the convolution with dilation 3, and so on. For a fair comparison, we stress out that the number of filters in each CoConv layer is always equal to the number of filters in the original ResNet model. Regarding the configuration top(4, 3, 2, 1), we can notice a significant drop in recognition performance. This is basically the opposite case of the baseline <ref type="figure" target="#fig_0">(1, 1, 1, 1)</ref>. We can observe from the results that both (extreme) cases have significantly lower recognition performance than CoConv with multiple levels. This is due to the fact that the baseline <ref type="figure" target="#fig_0">(1, 1, 1, 1)</ref> is only able to capture information about local details (as it uses the lowest dilation), without being equipped with the ability to capture contextual information. On the other side, the case top(4, 3, 2, 1) is only able to capture contextual information, lacking the ability of capturing information about local details. This set of experiments proves an important and strong point of our approach: CoConv captures information regarding both local details and global context, providing a more complete visual representation which improves the recognition performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Comparison of CoResNet to closely related works</head><p>In <ref type="table" target="#tab_8">Table 4</ref>, we present the results of closely related methods <ref type="bibr" target="#b32">[32,</ref><ref type="bibr" target="#b37">37]</ref> by training all neural networks with the same standard settings for providing a direct and fair comparison. All methods are applied on top of the 50-layer deep residual network. First, we observe that our CoResNet-50 outperforms more complex architectures, such as non-local (NL) networks <ref type="bibr" target="#b32">[32]</ref>. It is important to highlight the increase in the number of learnable parameters and computational costs brought by the introduction of the non-local block, while our CoResNet-50 maintains the same number of parameters and computational cost as the baseline ResNet-50 <ref type="bibr" target="#b7">[8]</ref>.</p><p>The dilated residual network (DRN) proposed by Yu et al. <ref type="bibr" target="#b37">[37]</ref> requires to decrease the overall stride of the network from the default 32 to 8. Basically, DRN does not use downsampling of the feature maps for the last two stages of the network. In <ref type="table" target="#tab_8">Table 4</ref>, we can observe that DRN has a significant impact in increasing the requirements in terms of computational resources, increasing the GFLOPs from 4.14 to 19.20. This significant increase in computational cost makes DRN not feasible for standard image classification as, for instance, increasing the depth of the baseline ResNet <ref type="bibr" target="#b7">[8]</ref> from 50 to 101 layers provides a top-1 error improvement from 23.88% to 22.00%, while the requirements in GFLOPs increase from 4.14 to only 7.88. In the same time, DRN-50 improves the baseline top-1 error from 23.88% to 22.44% at a higher computational cost. In comparison, our CoResNet-50 can improve the recognition performance of the baseline without impacting the demands in terms of computational resources.   <ref type="table">Table 5</ref>: ImageNet results of CoResNet in comparison with other state-of-the-art methods <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b32">32]</ref>, considering architectures on different depths, ranging from 50 layers to 152 layers.</p><p>To make a direct comparison between our approach and DRN <ref type="bibr" target="#b37">[37]</ref> under the same number of parameters and FLOPs, we also perform an experiment by setting the stride of CoResNet-50 to 8 instead of 32. As shown in <ref type="table" target="#tab_8">Table 4</ref>, our approach provides improved recognition performance in comparison to DRN. As another evidence that our approach is superior to DRN, we can link the DRN results from <ref type="table" target="#tab_8">Table 4</ref> with our configuration top(4, 3, 2, 1) from <ref type="table" target="#tab_7">Table 3</ref>. More precisely, DRN and top(4, 3, 2, 1) are from the same category of methods, as both use only the top dilation for convolution. We have already shown that this is not the optimal case for attaining good recognition performance, as it is necessary to have kernels that can capture detailed (local) information, as well as kernels that capture contextual information. We conjecture that the range of kernels from local to contextual is important for visual perception, as different levels of kernels bring complementary information into the visual system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Comparison of CoResNet on other architectures of different depths</head><p>In <ref type="figure">Figure 3</ref>  In <ref type="table">Table 5</ref>, we provide the comparative results between CoResNet and several other works <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b32">32]</ref>, considering neural networks of 50, 101 and 152 layers deep, respectively. CoResNet outperforms the baseline ResNet [8] on all network depths. We also outperform the pre-activation ResNet <ref type="bibr" target="#b8">[9]</ref> by a consistent margin, while maintaining the same number of parameters and computational cost. In terms of the top-1 error rate, we outperform the non-local block of Wang et al. <ref type="bibr" target="#b32">[32]</ref> on all three tested network depths, namely 50, 101 and 152. We qualify our results as even more impressive, considering that the work of Wang et al. <ref type="bibr" target="#b32">[32]</ref> significantly increases the number of parameters and FLOPs of the model. Interestingly, our CoResNet provides competitive results even when we compare it to the work of Hu et al. <ref type="bibr" target="#b10">[11]</ref>, although their work proposes an additional attention block (squeeze-and-excitation block) that needs to be inserted into the CNN, thus increasing the number of   learnable parameters of the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">Object detection on MS COCO</head><p>In order to show the generality and the transfer learning capability of our approach, we integrate CoResNet in an object detection pipeline, namely the Single Shot Multi-Box Detector (SSD) <ref type="bibr" target="#b23">[23]</ref>. As in <ref type="bibr" target="#b23">[23]</ref>, we remove all the layers of the ResNet backbones after the third stage to maintain the efficiency of the SSD. We also set the stride to 1 for the third stage to obtain 38 ? 38 output feature maps from the backbones. The corresponding results, which are presented in <ref type="table" target="#tab_11">Table 6</ref>, show that our approach provides significant improvements in detection performance, without affecting the number of parameters and computational cost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6.">Image generation on CIFAR-10 and CelebA</head><p>In <ref type="table" target="#tab_13">Table 7</ref>, we compare our CoProGAN with ProGAN <ref type="bibr" target="#b13">[14]</ref> on CIFAR-10 and CelebA, respectively. While the IS on CIFAR-10 indicates that our model is slightly better, the FID points to a lager difference in favor of CoProGAN. Analogously, on CelebA, the MS-SSIM indicates slight performance gains brought by CoConv, but the improvements measured by SWD are much higher, regardless of the resolution of the output (from 16 ? 16 to 128 ? 128 pixels). Overall, the empirical evidence indicates that CoProGAN produces superior results, regardless of the metric.</p><p>In <ref type="figure" target="#fig_3">Figure 4</ref>, we show the best and worst looking examples generated by ProGAN and CoProGAN selected by an independent human annotator from a set of 50 images generated from each data set. On CIFAR-10, we observe that our successful results seem more realistic, while our failure cases seems to contain objects with a global structure.</p><p>On CelebA, our successful faces seem more symmetrical, while the faces seen in the CoProGAN failure cases are still distinguishable as faces.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>We proposed contextual convolution (CoConv) as a direct replacement of the standard convolution, aiming to integrate contextual information at different levels of neural architectures. CoConv is efficient, maintaining the same requirements in the number of parameters and computational costs as the standard convolution, while providing improved visual recognition capabilities. Our contextual convolutional neural network (CoCNN) architectures are motivated by a series of neuroscience studies which clearly indicate the presence and importance of contextual modulation, even at the early stages of the biological visual systems, specifically in the V1 area. In this work, we showed that the findings in neuroscience can be applied to the artificial visual systems for object detection, recognition and generation, where we obtain significant improvements over several state-of-the-art baselines.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>An example in which the context is crucial in labeling the object (kitchen glove), which can easily be mistaken for something else if the rest of the image is not seen. (a) A picture of a kitchen glove. (b) A picture of the same glove with context.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Examples generated by ProGAN and CoProGAN, selected by a human annotator from a set of 50 images generated from CIFAR-10 and CelebA, respectively. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table /><note>A side-by-side comparison of ResNet-50 and CoResNet- 50. Although we illustrate our updates on ResNet-50, the changes can be analogously operated on models of different depths, e.g. ResNet-152.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table /><note>A side-by-side comparison of ProGAN and CoProGAN.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 3 :</head><label>3</label><figDesc>Ablation experiments on ImageNet with various CoResNet-50 configurations, considering different numbers of dilation levels in the CoConv residual blocks.</figDesc><table><row><cell cols="3">The configuration (1, 1, 1, 1) corresponds to ResNet-50 [8].</cell></row><row><cell cols="2">Lower values are better.</cell><cell></cell></row><row><cell>Network</cell><cell cols="2">stride top-1 top-5 params GFLOPs</cell></row><row><cell>ResNet-50 [8]</cell><cell>32 23.88 7.06 25.56</cell><cell>4.14</cell></row><row><cell cols="2">NL ResNet-50 [32] 32 22.91 6.42 36.72</cell><cell>6.18</cell></row><row><cell cols="2">CoResNet-50 [ours] 32 22.73 6.49 25.56</cell><cell>4.14</cell></row><row><cell>DRN-50 [37]</cell><cell cols="2">8 22.44 6.47 25.56 19.20</cell></row><row><cell cols="3">CoResNet-50 [ours] 8 21.93 6.17 25.56 19.20</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 4 :</head><label>4</label><figDesc>Comparison of CoResNet with closely related works<ref type="bibr" target="#b32">[32,</ref><ref type="bibr" target="#b37">37]</ref> on ImageNet, considering neural models of 50 layers in all cases. Lower values are better.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>, we present the training and validation learning curves on ImageNet, considering ResNet and CoRes-Net architectures of 50, 101 and 152 layers, respectively. Comparing our CoResNet with the baseline ResNet<ref type="bibr" target="#b7">[8]</ref>, we can see an improved convergence during training, this being due to the fact that CoConv provides a more complete visual representation of the input.</figDesc><table><row><cell>Backbone</cell><cell cols="4">AP AP@IoU=0.5 params GFLOPs</cell></row><row><cell>ResNet-50</cell><cell>26.20</cell><cell>43.97</cell><cell>22.89</cell><cell>20.92</cell></row><row><cell cols="2">CoResNet-50 28.63</cell><cell>46.71</cell><cell>22.89</cell><cell>20.92</cell></row><row><cell>ResNet-101</cell><cell>29.58</cell><cell>47.69</cell><cell>41.89</cell><cell>48.45</cell></row><row><cell cols="2">CoResNet-101 31.19</cell><cell>49.89</cell><cell>41.89</cell><cell>48.45</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 6 :</head><label>6</label><figDesc>Results of SSD with various ResNet and CoRes-Net backbones of 50 or 101 layers on the MS COCO data set, for input images of 300 ? 300 pixels. Higher AP and AP@IoU=0.5 values are better.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 7 :</head><label>7</label><figDesc>ProGAN versus CoProGAN results on CIFAR-10 and CelebA. Higher IS values are better. Lower FID, MS-SSIM and SWD are better.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was supported by a grant of the Romanian Ministry of Education and Research, CNCS -UEFIS-CDI, project number PN-III-P1-1.1-TE-2019-0235, within PNCDI III. This article has also benefited from the support of the Romanian Young Academy, which is funded by Stiftung Mercator and the Alexander von Humboldt Foundation for the period 2020-2022.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Contextual influences on visual processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gene</forename><forename type="middle">R</forename><surname>Albright</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Stoner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annual Review of Neuroscience</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="339" to="379" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="834" to="848" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Xception: Deep learning with depthwise separable convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fran?ois</forename><surname>Chollet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1251" to="1258" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The influence of contextual stimuli on the orientation selectivity of cells in primary visual cortex of the cat</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gilbert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Torsten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wiesel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Vision Research</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1689" to="1701" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Noordhuis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Wesolowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aapo</forename><surname>Kyrola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Tulloch</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02677</idno>
		<title level="m">Yangqing Jia, and Kaiming He. Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">Girshick</forename><surname>Mask R-Cnn</surname></persName>
		</author>
		<title level="m">Proceedings of ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2961" to="2969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep Residual Learning for Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Identity Mappings in Deep Residual Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="630" to="645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hubert</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Nessler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6626" to="6637" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Squeeze-and-Excitation Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Albanie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enhua</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Densely Connected Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4700" to="4708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Progressive Growing of GANs for Improved Quality, Stability, and Variation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic gradient descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><forename type="middle">Lei</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
		<respStmt>
			<orgName>University of Toronto</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">ImageNet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Boser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donnie</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">E</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wayne</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hubbard</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Backpropagation applied to handwritten zip code recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jackel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="541" to="551" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L?on</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Kaiming He, Bharath Hariharan, and Serge Belongie. Feature Pyramid Networks for Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2117" to="2125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr Doll?r. Focal Loss for Dense Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Microsoft COCO: Common Objects in Context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">SSD: Single Shot Multibox Detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Yang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="21" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deep Learning Face Attributes in the Wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3730" to="3738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Rectified Linear Units Improve Restricted Boltzmann Machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinod</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Wasserstein Barycenter and Its Application to Texture Mixing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Rabin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Peyr?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julie</forename><surname>Delon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Bernot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SSVM</title>
		<meeting>SSVM</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="435" to="446" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">ImageNet Large Scale Visual Recognition Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Dynamic routing between capsules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sara</forename><surname>Sabour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Frosst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3856" to="3866" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Improved techniques for training GANs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vicki</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2234" to="2242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very Deep Convolutional Networks for Large-Scale Image Recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Non-local Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="7794" to="7803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Multiscale structural similarity for image quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACSSC</title>
		<meeting>ACSSC</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1398" to="1402" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Group normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Aggregated Residual Transformations for Deep Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1492" to="1500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Multi-Scale Context Aggregation by Dilated Convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Dilated Residual Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Funkhouser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Contextual modulation in primary visual cortex</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Zipser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">F</forename><surname>Victor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter H</forename><surname>Lamme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schiller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Neuroscience</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">22</biblScope>
			<biblScope unit="page" from="7376" to="7389" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Learning Transferable Architectures for Scalable Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8697" to="8710" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
