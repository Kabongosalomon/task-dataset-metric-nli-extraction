<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Progressive Seed Generation Auto-encoder for Unsupervised Point Cloud Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juyoung</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Electrical Engineering</orgName>
								<orgName type="institution">KAIST</orgName>
								<address>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pyunghwan</forename><surname>Ahn</surname></persName>
							<email>p.ahn@kaist.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="department">School of Electrical Engineering</orgName>
								<orgName type="institution">KAIST</orgName>
								<address>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Doyeon</forename><surname>Kim</surname></persName>
							<email>doyeonkim@kaist.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="department">School of Electrical Engineering</orgName>
								<orgName type="institution">KAIST</orgName>
								<address>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haeil</forename><surname>Lee</surname></persName>
							<email>haeil.lee@kaist.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="department">School of Electrical Engineering</orgName>
								<orgName type="institution">KAIST</orgName>
								<address>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junmo</forename><surname>Kim</surname></persName>
							<email>junmo.kim@kaist.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="department">School of Electrical Engineering</orgName>
								<orgName type="institution">KAIST</orgName>
								<address>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Progressive Seed Generation Auto-encoder for Unsupervised Point Cloud Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T02:20+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>With the development of 3D scanning technologies, 3D vision tasks have become a popular research area. Owing to the large amount of data acquired by sensors, unsupervised learning is essential for understanding and utilizing point clouds without an expensive annotation process. In this paper, we propose a novel framework and an effective auto-encoder architecture named "PSG-Net" for reconstruction-based learning of point clouds. Unlike existing studies that used fixed or random 2D points, our framework generates input-dependent point-wise features for the latent point set. PSG-Net uses the encoded input to produce point-wise features through the seed generation module and extracts richer features in multiple stages with gradually increasing resolution by applying the seed feature propagation module progressively. We prove the effectiveness of PSG-Net experimentally; PSG-Net shows stateof-the-art performances in point cloud reconstruction and unsupervised classification, and achieves comparable performance to counterpart methods in supervised completion.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Deep neural networks, especially convolutional neural networks (CNNs), have achieved success in the performance of various computer vision tasks <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b12">13]</ref>. Recently, research centered on 2D space has been expanded to 3D space following the development of 3D scanning techniques. 3D sensors such as LiDAR and RGB-D cameras acquire data in form of point clouds, so it is essential to effectively recognize this type of data in robotics and autonomous driving applications. Point clouds lie in 3D space, so they incur significantly higher labeling costs for a specific vision task when compared to 2D data. Therefore, the need for effective unsupervised learning techniques for point clouds is highly emphasized.</p><p>For unsupervised learning, auto-encoder structures based on CNNs have been widely used to deal with 2D image data. Although CNNs show superior ability in learn- ing general features from 2D images, it is difficult to apply CNNs to point clouds because of the irregularity of the data format. Thus, network architectures that are specifically designed for point cloud recognition must be used as the encoder of the auto-encoder. In previous studies <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b34">34,</ref><ref type="bibr" target="#b23">24]</ref>, point cloud classification networks such as PointNet <ref type="bibr" target="#b15">[16]</ref> and PointNet++ <ref type="bibr" target="#b16">[17]</ref> have been used as encoders, and some other works have utilized graph layers <ref type="bibr" target="#b30">[30]</ref>. Encoders extract a global feature representation called a codeword vector, which becomes the input to the decoder.</p><p>For the decoder architecture, a popular approach is the concept of "folding" a 2D plane into a 3D object surface because the number of the output point clouds may not be determined. During the folding operation, the latent point set sampled from the 2D plane is transformed into 3D points, thus achieving point cloud generation. This approach was first proposed in FoldingNet <ref type="bibr" target="#b30">[30]</ref> and has been mathematically established in several studies <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b34">34]</ref>. In FoldingNet, a pre-defined set of fixed grid points in 2D space was used as the latent point set to the decoder along with the output of the encoder. Then, the folding operation was applied to transform this 2D point set into 3D points. In AtlasNet <ref type="bibr" target="#b4">[5]</ref> and 3D Point Capsule Network (3D-PointCapsNet) <ref type="bibr" target="#b34">[34]</ref>, multi-patch approaches were used for point cloud reconstruction. These works used randomly sampled points from the uniform distribution in a fixed area of the 2D plane. Throughout this paper, the latent point set will be referred to as seed.</p><p>One limitation of these methods is that the output point cloud is generated from an arbitrary 2D plane. Fixed grid points <ref type="bibr" target="#b30">[30]</ref> or randomly sampled points <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b34">34]</ref> were used as inputs to the decoder. Considering that an arbitrary 2D plane might not have enough capacity to model a complex 3D surface, AtlasNet and 3D-PointCapsNet utilize multiple patches to improve performance. However, the number of decoders increases with the number of patches, which leads to significant computational costs. Therefore, it is necessary to fundamentally change the sampling process in the 2D plane, rather than simply use more patches.</p><p>In this paper, we propose a novel framework for reconstruction-based learning. The main idea is to generate the seed from the function of the input point cloud. Previous methods used fixed or randomly sampled 2D points as seeds, which can be a substantial constraint to the decoder. We revisit the problem to explain why the generated seed helps our decoder to generate various 3D shapes. We implement our framework by "PSG-Net", which is far more effective and superior to existing methods. The seed is generated in multiple stages through the seed generation module. Then, the seed feature propagation module processes the generated seed and codeword vector to produce the output shape. In addition, we introduce a progressive approach to enrich the information of the feature for the output point cloud. This is achieved by generating a gradually increasing resolution of the seed and interpolated feature maps. The result from the last seed feature propagation module is transformed into 3D point cloud coordinates through the point generation layers.</p><p>Our network achieves performances comparable to the state-of-the-art methods in various unsupervised tasks such as point cloud reconstruction and unsupervised classification, and achieves the best performance among counterpart methods in supervised point cloud completion. Furthermore, our method can be used in combination with other existing methods such as the multi-patch approach, which is expected to result in enhanced performance. The main contributions of this study are as follows.</p><p>1. We propose a novel framework for reconstructionbased learning of point clouds that uses the generated input-dependent point-wise features as seed, instead of using simple 2D points. To implement this framework, we incorporate seed generation module (SGM) and seed feature propagation module (SFPM) in an efficient auto-encoder architecture called PSG-Net.</p><p>2. We analyze two proposed modules and demonstrate the superiority of our model by experimentally proving the analysis.</p><p>3. We show that the performance of PSG-Net is comparable to state-of-the-art methods in various 3D tasks such as point cloud reconstruction, unsupervised classification and supervised point cloud completion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>There has been much research on 3D computer vision using point clouds in recent years <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b11">12]</ref>. We present previous studies related to our method in this section. Because point cloud completion can be seen as a task expanded from point cloud reconstruction, a method for one of these tasks can often be used for another, as in this paper. Given that both tasks have been performed in this study as well, we introduce research on these two tasks.</p><p>Point cloud reconstruction. The first auto-encoder was a simple network based on PointNet introduced in L-GAN <ref type="bibr" target="#b0">[1]</ref>. L-GAN aims to generate the point cloud with generative adversarial network (GAN) and acts as a baseline network for later studies. Subsequently, FoldingNet <ref type="bibr" target="#b30">[30]</ref> performs point cloud reconstruction by learning 2D-to-3D mapping under the intuition that a 2D plane can be transformed into a 3D surface through certain operations. The authors call this a "folding" operation and define it as the concatenation of replicated codeword vectors to lowdimensional grid points, followed by a point-wise multilayer perceptron (MLP). FoldingNet consists of a graphbased encoder and a folding-based decoder, and uses a fixed 2D grid as the sampled points of a 2D plane. AtlasNet <ref type="bibr" target="#b4">[5]</ref> proposed to use multiple 2D patches instead of a single 2D plane as in FoldingNet. In this architecture, 2D points were randomly sampled from a uniform distribution inside the unit square, so that the network can better learn the shape of the objects. 3D-PointCapsNet <ref type="bibr" target="#b34">[34]</ref> extracts latent capsules by applying a dynamic routing system and creates a point cloud from them. There is also a study that performs reconstruction to learn local descriptors. PPF-FoldNet <ref type="bibr" target="#b2">[3]</ref> converts a point cloud into a point pair feature (PPF) representation and performs feature reconstruction using folding operations to learn local descriptors on a point cloud.</p><p>Point cloud completion. Point cloud completion is a rapidly growing research area derived from point cloud reconstruction. PCN <ref type="bibr" target="#b31">[31]</ref> was the first method to perform point cloud completion with a leading-based approach to a point cloud instead of a voxel. PCN uses an extended version of PointNet as an encoder and a decoder that combines the fully connected decoder and the folding-based decoder to generate a dense output point cloud. In <ref type="bibr" target="#b20">[21]</ref>, a tree-structured decoder called TopNet was proposed to create an arbitrary grouping of points. Because the global feature extracted from the incomplete point cloud may re- sult in loss of information about structural details, SA-Net <ref type="bibr" target="#b23">[24]</ref> uses a skip-attention mechanism for utilizing hierarchical information obtained from the PointNet++ encoder in the folding-based decoder. RL-GAN-Net <ref type="bibr" target="#b18">[19]</ref> and Ren-der4Completion <ref type="bibr" target="#b5">[6]</ref> complete the point cloud more realistically by utilizing reinforcement learning and GAN. Recently, GRNet <ref type="bibr" target="#b28">[28]</ref> achieved high performance by combining a voxel-based approach and a point cloud-based approach. There have also been studies focusing on missing parts by separating the known and missing parts <ref type="bibr" target="#b33">[33,</ref><ref type="bibr" target="#b6">7]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>In this section, we introduce our framework and PSG-Net architecture. First, we formulate the problem and explain the model architecture. Then, we describe the analysis, including a comparison with existing methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Formulation and Notation</head><p>We follow the same approximation as those presented in previous studies <ref type="bibr" target="#b30">[30,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b34">34]</ref>. We consider that in the point cloud P = {p i ? R 3 } , every point p comes from the surface S of a 3D shape. Because S is 2-manifold, there exists an open set U ? R 2 and an open set W , that satisfies p ? W ? R 3 , such that S ? W is homeomorphic to U. The mapping C : S ? W ? U is called a chart, and its inverse ? ? C ?1 : U ? S ?W is called a parameterization. Based on these notations, the problem is defined as follows:</p><formula xml:id="formula_0">Definition 1 (Problem)</formula><p>The point cloud reconstruction problem is defined as learning to generate 3D surface S by finding the function ? that satisfies ?(U, ?) = P ? ? P , where ? is a codeword vector.</p><p>In practice, we use a discrete set U ? = {u j ? R 2 } instead of an open set U because the input point cloud P is a discrete sampled subset of S. Because previous studies have already formulated the problem in detail and proved the theorems needed to support this type of method, we refer the reader to <ref type="bibr" target="#b30">[30,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b34">34]</ref> for further information. Additionally, we use the term "seed" for u in this study. Thus, it can be said that p ? j from P ? = {p ? j ? R 3 } is generated from the corresponding seed u j .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Definition 2 (Seed)</head><p>We call the element u of the discrete set U ? a "seed", which generates the corresponding output point.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Model Architecture</head><p>The input to our model is an N ? d point cloud, where d is basically 3 as each point cloud is composed of 3D coordinates (x, y, z). Considering that our main focus is on the decoder, we utilize the existing feature extractor, which is commonly used in 3D tasks as the encoder. For the reconstruction task, we adopted PointNet++ <ref type="bibr" target="#b16">[17]</ref> as our encoder. PointNet++ consists of three set abstraction modules, each of which consists of sampling, grouping, and mini-PointNet layers. This encoder structure helps capture the hierarchical features and finally extracts a global feature vector ? ? as the codeword vector. The encoder can be replaced by any point cloud processing architecture according to its relevance to the target task.</p><p>The proposed decoder contains three different parts: seed generation module (SGM), seed feature propagation module (SFPM), and point generation layers. We explain each module in detail in the following paragraphs. The overall architecture is shown in <ref type="figure" target="#fig_1">Figure 2</ref>.</p><p>Seed generation module takes ? ? as input and generates 2D feature maps from a transposed 2D convolution (Conv) -batch normalization (BN) <ref type="bibr" target="#b7">[8]</ref> -ReLU layer sequence. We use the feature map f l from the l = {1, 2, ? ? ? , L} th layer sequence as seed set U ? l = {u ? il } for the SGM. Given that the feature map f l has a spatial size of</p><formula xml:id="formula_1">[h l , w l ], the number of u ? in U ? l is N ? l = (h l ? w l )</formula><p>. U ? L from the final layer of the SGM has N L seeds, which is equal to or close to the number of input points N .</p><p>Seed feature propagation module takes three inputs: seed generated from the SGM, codeword vector, and intermediate feature map produced by the previous SFPM. As input to the k = {1, 2, ? ? ? , K} th SFPM, the codeword vector (? ? ) is replicated N ? L?(K?k+1) times and then concatenated with U ? L?(K?k+1) and the output of the (k ? 1) th SFPM in the channel dimension. For the 1 st SFPM, we use ? ? and U ? 1 to create the input of the 1 st SFPM. The k th SFPM is made of a 1 ? 1 Conv -BN -ReLU layer sequence and an interpolation layer. After the input passes through the layer sequence, the k th seed-wise feature with the channel dimension |? ? |/2 is created. We interpolate the feature for the (k + 1) th SFPM. This interpolation layer ensures that the output of the k th SFPM V k = {v ik } has the same number of seeds as those of U ? L?K+k . For the interpolation layer, bilinear interpolation is adopted for efficiency.</p><p>Point generation layers take ? ? , U ? L , and V K to generate the output point cloud. The point generation layers are similar to the SFPM but have slightly different components to output a point cloud. Because the size of the generated point-wise feature may be different from the desired output size, we add the interpolation layer to match this. Then, the feature map is flattened and passed through a fully connected (FC) layer to produce an M ? 3 point cloud as the output.</p><p>As in other studies <ref type="bibr" target="#b30">[30,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b34">34]</ref>, we use the discrete chamfer distance as the loss function for training. The loss function is as follows:</p><formula xml:id="formula_2">L CD (P, P ? ) = 1 |P | p?P min p ? ?P ? ?p ? p ? ? 2 + 1 |P ? | p ? ?P ? min p?P ?p ? ? p? 2 . (1)</formula><p>For specific implementation details about our decoder, please refer to Supplementary Material Section 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Analysis</head><p>We introduced the formulated form of the problem in Section 3.1. Now, we analyze our method and its superiority by revisiting the problem. If we represent the encoder as a function E, then ? can be written as E(P ). Therefore, the problem of point cloud reconstruction is to find ? that satisfies ?(U ? , E(P )) = P ? ? P . Now, we can rewrite Definition 1 as follows:</p><formula xml:id="formula_3">Definition 3</formula><p>The problem of point cloud reconstruction is defined as finding the function ?, where ?(U ? , E(P )) forms an identity function I P .</p><p>The fundamental difference between our framework and the previous methods lies in generating U ? . So far, U ? has been set to a fixed grid or randomly generated in the domain ] 0, 1[ 2 : U(0, 1). That is, u is an arbitrary 2D coordinate (x, y) independent of p. However, the presence of another independent variable U ? may make it difficult to optimize ?(U ? , E(P )) to form I P . We ease this optimization issue by setting U ? as a function of P . Because our decoder generates U ? from ? through the function G that represents the SGM, ?(U ? , E(P )) can be expressed as ?(G(?), E(P )) = ?(G(E(P )), E(P )). Given that ?(G(E(P )), E(P )) can be rearranged into ? ? (P ), the problem becomes easier to solve than it had been previously:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Definition 4</head><p>The problem is redefined as finding the function ? ? , where ? ? (P ) forms an identity function I P .</p><p>Furthermore, we expand 2-dimensional point set U ? to an n-dimensional features that contains more semantic information than simple 2D coordinates. This guarantees more informative u to generate p ? .</p><p>Another noticeable difference between our method and previous methods is that our method produces the output in a progressive manner. Because progressive seeds with multiple resolutions are generated through the SGMs, we can combine coarse semantic information and fine detailed information by fusing the feature hierarchy from each seed. Similar to FCN <ref type="bibr" target="#b13">[14]</ref>, we implement feature fusion by a 1?1 Conv -BN -ReLU layer sequence and a bilinear interpolation operation.</p><p>In summary, there are three factors that have led to significant differences between previous methods <ref type="bibr" target="#b30">[30,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b34">34</ref>].</p><formula xml:id="formula_4">1. A discrete set U ? ? R 2 2. Parameterization function ?</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Combination of U ? and a codeword vector ?</head><p>Our framework uses Input-dependent high-dimensional seeds that facilitate the reconstruction, while existing methods used arbitrary 2D coordinates. Since we adopted transposed 2D Conv in the process of generating inputdependent seeds for simplicity and efficiency, our ? is defined as the transposed 2D Conv, MLP (equivalent to 1 ? 1  Conv), and bilinear interpolation. Naturally, the concatenation of ?, U ? , and the corresponding coarse feature becomes the input of ?.</p><p>In addition, we can control the number of output point clouds by adjusting the parameters that make up the decoder, as in existing methods. We show an example of this in the point cloud upsampling task, please refer to Supplementary Material Section 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We conducted extensive experiments to demonstrate the effectiveness of our method. First, we compared the performance of our method against existing methods for various 3D tasks such as point cloud reconstruction, unsupervised classification and point cloud completion. Then, we experimentally proved the analysis provided in Section 3.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Implementation</head><p>Our method is evaluated on four datasets: ShapeNet-Core13 <ref type="bibr" target="#b1">[2]</ref>, ShapeNetCorev2 <ref type="bibr" target="#b1">[2]</ref>, Completion3D <ref type="bibr" target="#b20">[21]</ref>, and ModelNet40 <ref type="bibr" target="#b26">[27]</ref> datasets. We implemented our network using the PyTorch <ref type="bibr" target="#b14">[15]</ref> framework. The network was trained using an ADAM optimizer with betas (0.9, 0.999) and weight decay 1e-6. We used an initial learning rate of  <ref type="bibr" target="#b34">[34]</ref> No 88.9 PointGrow <ref type="bibr" target="#b19">[20]</ref> No 85.8 PointFlow <ref type="bibr" target="#b29">[29]</ref> No 86.8 MRTNet-VAE <ref type="bibr" target="#b3">[4]</ref> No 86.4 PCGAN <ref type="bibr" target="#b10">[11]</ref> No 87.8 SA-Net-cls <ref type="bibr" target="#b23">[24]</ref> No 90.6</p><p>Ours No 90.9 5e-5 for ModelNet40, and 1e-4 for others. The batch size was set to 32. The chamfer distance (CD) was used as the evaluation metric of point cloud reconstruction and completion tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Comparative Study</head><p>Point cloud reconstruction. We performed point cloud reconstruction on ShapeNetCore13 and ShapeNetCorev2. We followed the dataset settings of AtlasNet <ref type="bibr" target="#b4">[5]</ref> for both datasets. The results in <ref type="table">Table 1</ref> clearly show that our model achieves performance that is comparable to the state-of-theart methods. Oracle randomly samples a point cloud from the ground truth shape, and therefore constitutes an upper bound of the performance. Points baseline is a network of MLPs presented with detailed structures in AtlasNet.</p><p>It should be noted that AtlasNet and 3D-PointCapsNet utilize 125 and 32 patches for finer reconstruction, respectively. Our method uses only a single patch but outperforms the two methods on ShapeNetCore13, and achieves comparable performance to AtlasNet on ShapeNetCorev2. We</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Overall Airplane Cabinet Car Chair Lamp Sofa  further present performance that outperforms even AtlasNet in Section 4.3. <ref type="figure" target="#fig_2">Figure 3</ref> visually shows the reconstruction results of ShapeNetCore13.</p><p>Unsupervised classification. To evaluate the efficiency of the feature representation of our method, we performed unsupervised classification on ModelNet40. In accordance with the settings of the previous studies <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b30">30,</ref><ref type="bibr" target="#b34">34]</ref>, we first performed the reconstruction on ModelNet40 and then extracted the codeword vector. Thereafter, we normalized the codeword vector and used it to train the linear support vector machine (SVM) classifier. <ref type="table" target="#tab_2">Table 2</ref> indicates that our method achieves reasonable accuracy among unsupervised learning methods. We note that our method not only outperforms the unsupervised methods, but also surpasses the supervised version of the same encoder.</p><p>Point cloud completion. To demonstrate the effectiveness of our decoder architecture, we train PSG-Net for supervised learning of point cloud completion. Because the experimental settings of existing studies on point cloud completion have not been unified, we participated in the Completion3D benchmark 1 to evaluate our model for the point cloud completion task. We utilized the PCN <ref type="bibr" target="#b31">[31]</ref> encoder, which is mainly used in the completion task for comparison focused on the decoder architecture.</p><p>We added our result to the current leaderboard and present it in <ref type="table" target="#tab_3">Table 3</ref>. We excluded the result of no method being presented. The result shows that the performance of our method is comparable to that of others for the point cloud completion task, even though the structure is aimed at reconstruction. The methods which show better performance than ours, are not suitable for direct comparison with ours because they use the codeword vector and other techniques with much powerful encoders for supervised point cloud completion. However, our method is aimed at unsupervised learning and uses only global representation with PCN encoder. Thus, practically, our method can be considered to have achieved the best performance in counterpart methods, and improvements can be expected depending on the type of encoder. Because no ground truth for the test set</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method ShapeNetCorev2</head><p>Ours (512-dim) <ref type="bibr" target="#b4">5</ref>.78 Ours (1024-dim) 5.15 <ref type="table">Table 4</ref>. Results of point cloud reconstruction on ShapeNetCorev2. The chamfer distance is multiplied by 10 4 for ShapeNetCorev2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method # Parameters (M)</head><p>AtlasNet-25 <ref type="bibr" target="#b4">[5]</ref> 44.8 AtlasNet-125 <ref type="bibr" target="#b4">[5]</ref> 219.3 3D-PointCapsNet <ref type="bibr" target="#b34">[34]</ref> 69.5</p><p>Ours 7.6 is publicly available, we visualize the completion results of the validation set in <ref type="figure" target="#fig_3">Figure 4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Analytical Study</head><p>We explained in Section 3.3 that our method is superior to existing methods. In this section, we demonstrate this experimentally in various ways.</p><p>Effect of different codeword lengths. Because the existing methods used 512-dimensional or 1024-dimensional codeword vectors, we conducted experiments based on 512dimensional codeword vectors. However, to show the effect of using different codeword lengths, we performed point cloud reconstruction on ShapeNetCorev2 with 1024dimensional codeword vector. The results in <ref type="table">Table 4</ref> clearly demonstrate that our method performs well with 1024dimensional codeword vector.</p><p>Computational efficiency. To demonstrate the efficiency of our method, we compare the number of parameters of our method with those of the existing methods. Table 5 indicates that our network requires far fewer parameters than existing methods. As mentioned in Section 1, the number of parameters increases linearly with the number of patches, which is shown by the examples of AtlasNet-25 and AtlasNet-125 <ref type="bibr" target="#b4">[5]</ref>. This observation emphasizes the computational efficiency of our method.</p><p>Combination with other methods. We conducted experiments that produced higher performance, even stateof-the-art, through combinations with existing methods to demonstrate the applicability of our method. For the reconstruction on ShapeNetCorev2, we applied our method to AtlasNet by adding the SGM to the structure of AtlasNet with 32 patches.</p><p>For the unsupervised classification, since the results in <ref type="table" target="#tab_2">Table 2</ref> show the results of training each proposed network by normal reconstruction training with a chamfer distance, studies with higher performance may exist as new unsupervised learning methods are applied. For example, <ref type="bibr" target="#b17">[18]</ref>   <ref type="table">Table 6</ref>. The results of combination with other methods. Rec and cls indicate reconstruction task and unsupervised classification task, respectively. In result, reconstruction task uses the chamfer distance multiplied by 10 4 as an evaluation metric, and classification task uses the accuracy (%) as an evaluation metric.</p><p>cused on new loss functions which help unsupervised learning, rather than the network architecture itself, and achieved state-of-the-art performance on the unsupervised classification task. Although <ref type="bibr" target="#b17">[18]</ref> presented higher performance, we did not compare our method with <ref type="bibr" target="#b17">[18]</ref> in <ref type="table" target="#tab_2">Table 2</ref> because we considered that their improvement is orthogonal to that of our work. Thus, we further conducted an experiment that combines our decoder with <ref type="bibr" target="#b17">[18]</ref>. The results in <ref type="table">Table 6</ref> show that our method not only achieves the state-of-the-art, but is capable of combining with other studies.</p><p>Decoder ablation. We constructed various models suitable for analysis and conducted point cloud reconstruction on the ShapeNetCorev2. Our analysis was focused on the contribution of different U ? s and the progressive seed generation to the performance improvement.</p><p>We first compared the performance of various U ? settings. For fair comparison, we used the same encoder based on PointNet <ref type="bibr" target="#b15">[16]</ref> and the same decoder based on Fold-ingNet <ref type="bibr" target="#b30">[30]</ref>, which consists of two MLP -BN -ReLU sequences. We built the following four decoders which differ only in U ? : Decoder A that sets U ? as fixed 2D grid points; Decoder B that sets U ? as 2D points sampled from the uniform distribution U(0, 1); Decoder C 2 that generates U ? from ?; Decoder C 32 that generates U ? as 32-dimensional features generated from ? (Please refer to Supplementary Material Section 3).</p><p>The results in <ref type="table" target="#tab_6">Table 7</ref> and <ref type="figure" target="#fig_4">Figure 5</ref> demonstrate that our method performs point cloud reconstruction more effectively than existing methods. Interestingly, we observe that Decoder A with U ? set to a fixed grid has a higher performance than Decoder B with U ? set to a uniform distribution. It can also be seen that Decoder C 2 performs better than both Decoders A and B, and that the performance is even better when high-dimensional feature vectors are used instead of 2D points (Decoder C 32 ). These results are consistent with the analysis in Section 3.3. To form ? as an identity function in Def 3, it is best to fix the indepen-  Decoder A Decoder B Decoder C 2 <ref type="figure">Figure 6</ref>. Coordinates of the sampled points used in Decoders A, B, and C2. Decoder A used grid points, Decoder B used random points, and Decoder C2 used points constructed from the codeword vector. dent variable U ? to a constant value. Therefore, Decoder A with U ? set to a fixed 2D grid can learn a function better than Decoder B with U ? set to a random variable. In addition, because U ? is a function of P , Decoder C 2 achieves higher performance than Decoders A and B. Decoder C 32 achieves the highest performance because U ? contains the most information about P . <ref type="figure">Figure 6</ref> shows the coordinates of the 2D points used in Decoders A and B and the coordinates of 2D points learned in Decoder C 2 . We observe that the Decoders A and B sample grid points and completely random points on a rectangular plane, whereas the Decoder C 2 samples points on a uniquely constructed plane.</p><p>We also prove the effectiveness of the progressive approach by adding more SFPMs to Decoder C 32 and observe the performance change in point cloud reconstruction. As shown in <ref type="table" target="#tab_6">Table 7</ref>, the performance is gradually improved as 1, 2, and 3 SFPMs are added.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we propose a novel framework that generates the input-dependent point-wise features as seed for reconstruction-based learning of point cloud. To this end, we constructed efficient yet powerful PSG-Net with two modules, namely SGM and SFPM, focusing on the concept of the seed. PSG-Net achieves reasonable performance in point cloud reconstruction, unsupervised classification, and point cloud completion. Given that we focus on the fundamental concept rather than the technical concept of point cloud reconstruction and the construction of PSG-Net, this study can be used with other methods. Considering the application of PSG-Net, we believe that our study will contribute to the understanding and application of point clouds.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Examples of the 3D point cloud reconstruction, unsupervised classification, point cloud completion results with PSG-Net. Our network successfully performs each task from the input point cloud.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Overview of our framework. PSG-Net consists of three parts : seed generation module (SGM), seed feature propagation module (SFPM), and point generation layers. Several parameters such as the number of layers that make up the SGM or the number of SFPMs can be changed depending on the input conditions. Each convolution and fully connected layer is followed by batch normalization and ReLU layers (omitted from the figure).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Examples of point cloud reconstruction results on ShapeNetCore13. (a) is the ground truth and (b) is our result.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Examples of point cloud completion results on Completion3D. (a) is the input, (b) is our result, and (c) is the ground truth.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Examples of point cloud reconstruction on ShapeNetCorev2 with various decoders for analysis.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>The results of unsupervised classification on ModelNet40.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Results of point cloud completion on Completion3D. The chamfer distance is multiplied by 10 3 .</figDesc><table><row><cell>Table Watercraft</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 .</head><label>5</label><figDesc>The number of parameters on various networks.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 .</head><label>7</label><figDesc>Method Decoder A Decoder B Decoder C 2 Decoder C 32 + 1 SFPM + 2 SFPMs + 3 SFPMs Results of point cloud reconstruction on ShapeNetCorev2 with various decoders for analysis. The chamfer distance is multiplied by 10 4 .</figDesc><table><row><cell>CD</cell><cell>7.43</cell><cell>7.96</cell><cell>7.34</cell><cell>6.84</cell><cell>6.63</cell><cell>6.40</cell><cell>6.38</cell></row><row><cell>Input</cell><cell></cell><cell></cell><cell>Decoder A</cell><cell></cell><cell>Decoder B</cell><cell></cell><cell>Decoder C 32</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://completion3d.stanford.edu</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This research was supported by Naver Labs Corporation.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning representations and generative models for 3d point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Panos</forename><surname>Achlioptas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Diamanti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Mitliagkas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><surname>Guibas</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Angel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pat</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixing</forename><surname>Hanrahan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zimo</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manolis</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuran</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Su</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.03012</idno>
		<title level="m">An information-rich 3d model repository</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Ppf-foldnet: Unsupervised learning of rotation invariant 3d local descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haowen</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tolga</forename><surname>Birdal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slobodan</forename><surname>Ilic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="602" to="618" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Multiresolution tree networks for 3d point cloud processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matheus</forename><surname>Gadelha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhransu</forename><surname>Maji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="103" to="118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A papier-m?ch? approach to learning 3d surface generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thibault</forename><surname>Groueix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Vladimir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bryan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Aubry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Render4completion: Synthesizing multi-view depth maps for 3d shape completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhizhong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Zwicker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision Workshops</title>
		<meeting>the IEEE International Conference on Computer Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Pf-net: Point fractal network for 3d point cloud completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zitian</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yikuan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawen</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyi</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="7662" to="7670" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<idno>PMLR, 2015. 4</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Discrete point flow networks for efficient point cloud generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roman</forename><surname>Klokov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edmond</forename><surname>Boyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Verbeek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Barnabas Poczos, and Ruslan Salakhutdinov</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun-Liang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manzil</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.05795</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">Point cloud gan. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Pointcnn: Convolution on x-transformed points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangyan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Bu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingchao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinhan</forename><surname>Di</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoquan</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Ssd: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Yang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="21" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8026" to="8037" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Pointnet: Deep learning on point sets for 3d classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Charles R Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaichun</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="652" to="660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Pointnet++: Deep hierarchical feature learning on point sets in a metric space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Charles Ruizhongtai Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Global-local bidirectional reasoning for unsupervised representation learning of 3d point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongming</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR, 2020</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Rl-gan-net: A reinforcement learning agent controlled gan network for real-time point cloud shape completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammad</forename><surname>Sarmad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenny</forename><surname>Hyunjoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Young</forename><forename type="middle">Min</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5898" to="5907" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Pointgrow: Autoregressively learned point cloud generation with self-attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongbin</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Siegel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjay</forename><surname>Sarma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Winter Conference on Applications of Computer Vision</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="61" to="70" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Topnet: Structural point cloud decoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lyne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vineet</forename><surname>Tchapmi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamid</forename><surname>Kosaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Rezatofighi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Dynamic graph cnn for learning on point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongbin</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Sanjay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sarma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><forename type="middle">M</forename><surname>Michael M Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Solomon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Softpoolnet: Shape descriptor for point cloud completion and classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yida</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">Joseph</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nassir</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Tombari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Point cloud completion by skip-attention network with hierarchical folding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhizhong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Shen</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="1939" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Pmp-net: Point cloud completion by learning multi-step point moving paths</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhizhong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan-Pei</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Shen</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Pointconv: Deep convolutional networks on 3d point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenxuan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongang</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fuxin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9621" to="9630" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhirong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuran</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linguang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxiong</forename><surname>Xiao</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A deep representation for volumetric shapes</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1912" to="1920" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Grnet: Gridding residual network for dense point cloud completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haozhe</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongxun</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shangchen</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiageng</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengping</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenxiu</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.03761</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Pointflow: 3d point cloud generation with continuous normalizing flows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guandao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zekun</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4541" to="4550" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Foldingnet: Point cloud auto-encoder via deep grid deformation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaoqing</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiru</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Pcn: Point completion network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wentao</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tejas</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Held</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Mertz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martial</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 International Conference on 3D Vision (3DV)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Point set voting for partial point cloud analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijia</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuping</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ram</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Johnson-Roberson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.04537</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Detail preserved point cloud completion via separated feature aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenxiao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingan</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunxia</forename><surname>Xiao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.02374,2020.3</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">3d point capsule networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongheng</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tolga</forename><surname>Birdal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haowen</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Tombari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
