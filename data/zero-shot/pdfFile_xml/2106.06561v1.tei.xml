<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">GANs N&apos; Roses: Stable, Controllable, Diverse Image to Image Translation (works for videos too!)</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><forename type="middle">Jin</forename><surname>Chong</surname></persName>
							<email>mchong6@illinois.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Forsyth</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">GANs N&apos; Roses: Stable, Controllable, Diverse Image to Image Translation (works for videos too!)</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T07:44+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We show how to learn a map that takes a content code, derived from a face image, and a randomly chosen style code to an anime image. We derive an adversarial loss from our simple and effective definitions of style and content. This adversarial loss guarantees the map is diversea very wide range of anime can be produced from a single content code. Under plausible assumptions, the map is not just diverse, but also correctly represents the probability of an anime, conditioned on an input face. In contrast, current multimodal generation procedures cannot capture the complex styles that appear in anime. Extensive quantitative experiments support the idea the map is correct. Extensive qualitative results show that the method can generate a much more diverse range of styles than SOTA comparisons. Finally, we show that our formalization of content and style allows us to perform video to video translation without ever training on videos. Code can be found here</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Imagine building a mapping that takes face images and produces them to anime drawings of faces. Some partsthe content -of the image may be preserved, but othersthe style -must change, because the same face could be represented in many different ways in anime. This means we have a one-to-many mapping, which can be represented as a function that takes a content code (recovered from the face image) and a style code (which is a latent variable), and produces an anime face. But there are important constraints that must be observed. We want control: the content of the anime face can be changed by changing the input face (for example, if the person turns their head, so should the anime). We want consistency: different real faces rendered into anime using the same set of latent variables should clearly match in style (for example, if the person turns their head, the anime doesn't change style unless the latent vari-ables change). Finally, we want coverage: every anime image should be obtainable using some combination of content and style, so that we can exploit the full range of possible anime images.</p><p>Our method -GANs N' Roses or GNR -is a multimodal I2I framework that uses a straightforward formalization of the maps using style and content (section 3.1). Achieving our goals requires carefully structured losses (section 3.3). The most important step is to be exact about what is intended by content and what is intended by style. We adopt a specific definition: content is what changes when face images are subject to a family of data augmentation transformations, and style is what does not change. This definition is very powerful. Our data augmentations involve scaling, rotating, cropping, etc. Thus, the definition means that content is (in essence) where parts of the face are in the image and style is how the parts of the face are rendered.</p><p>This definition allows us to learn a mapping from face images to content codes. We then pass the content codes to a decoder, which must produce anime from them and a style latent variable. It is also very important that the anime produced from a given content code "know" what that code is, and we use a decoder to recover the code from the anime. But one face should yield many anime faces, and one anime face should have the same content as many faces. This means we cannot require 1-1 loop closure of images (in contrast to CycleGAN), but must close the loop on content codes instead. This creates a difficulty, as the method might try to ignore the style code to obtain better cycle consistency on content. We show how to ensure the method produces the correct distribution of anime for a given content, resulting in a method that can produce very diverse anime. This diversity is important in applications; for example, a user might wish to exploit the control that our style code offers ( <ref type="figure" target="#fig_3">Fig. 7)</ref> to get an avatar with just the right eye shape or color. Our contributions are:</p><p>? Our definition of content and style is easily operationalized; we show how to use it to ensure that the anime produced from a single content code are prop- <ref type="figure">Figure 1</ref>. Our loss forces GNR to generate diverse outputs from randomly sampled styles, by requiring that anime generated from augmented versions of a single image generate a batch that is indistinguishable from a sample of the same size of P (Y ). As this figure shows, GNR can produce highly diverse outputs. Each row shows anime generated from the content code of the first image, using sampled style codes; the style codes are the same across each column. The six different style codes shown here produce strongly different anime figures. Note that eye shape, size, style and color change with change of style code (row); Similarly, chin shape, nose rendering, hair color, haircut, and so on vary. For a given style code (column), there is strong consistency in these details. The tilt of the head, overall shape of the face, and positioning of facial features, and fringe are all controlled by the content (first image in each row), as is the collar style (open neck is preserved (row 1), as is hoodie (r2); formal collar (r3); and scoop neck (r4)). Note the spatial coherence of the anime images -we do not have (say) images with eyes different size, etc. (compare <ref type="figure">figure 4</ref>). erly diverse.</p><p>? The resulting method is very effective at producing controllable, diverse synthesis, by both quantitative and qualitative measures.</p><p>? Our definition of content means our method can synthesize anime videos from face videos without ever being trained on multiple frames.</p><p>? While we describe our method in the context of face to anime translation (because anime have a particular rich variation in style that is difficult to capture, section 2), the method applies to any translation problem.</p><p>All figures are best viewed in color at high resolution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head><p>Image to Image Translation Image to image translation (I2I) involves learning a mapping between two different image domains. In general, we want the translated image to maintain certain image semantics from the original domain while obtaining visual similarities to the new domain. Early works on I2I involves learning a deterministic mapping between paired data <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b5">6]</ref>. This was later extended to a multimodal mapping in BicycleGAN <ref type="bibr" target="#b22">[23]</ref>. However, due to the limited availability of paired data, this approach is cannot scale up to bigger unpaired datasets. The pioneering work of CycleGAN <ref type="bibr" target="#b21">[22]</ref> solves this problem by employing the use of cycle consistency to learn image to image translation for unpaired data. Following works <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b10">11]</ref> have used similar approach. A significant limitation of these works is the lack of diversity of the output images due to their unimodal mapping. This is inherently limiting as image-toimage translation is generally a multimodal problem. Recent works on multimodal translation have then expanded on this. MUNIT <ref type="bibr" target="#b4">[5]</ref> and DRIT <ref type="bibr" target="#b12">[13]</ref> decomposes an image into a domain invariant content code and a domain-specific style code and employ random style sampling to produce diverse outputs. StarGANv2 <ref type="bibr" target="#b0">[1]</ref> employs a single generator to produce diverse images for multiple domains. Mode Seeking GAN <ref type="bibr" target="#b14">[15]</ref> builds on top of DRIT and encourages output diversity by penalizing output images that are similar to each other when their input style codes are different.</p><p>In general, current multi-modal frameworks lack a proper definition of style and content; it is unclear what exactly they each constitute. Also, visual inspection of their outputs reveals mode collapse. For a given image, the multiple outputs look very similar, often with just color and slight stylistic changes. One recent work in CouncilGAN <ref type="bibr" target="#b16">[17]</ref> enables diverse outputs by collaborating between multiple GANs. However in the difficult setting of selfie2anime, CouncilGAN cannot capture the complex artistic style of animes, collapsing to few modes and not expressing the stylistic diversity we expect. Very recently, AniGAN <ref type="bibr" target="#b13">[14]</ref> proposes new normalizations to allow selfie2anime by transferring color and textual styles while maintaining global structure. AniGAN generates multimodal outputs based on reference images. Like previous methods, AniGAN does not have explicit style diversity and lack output diversities.</p><p>In contrast to previous work, GNR focuses on achieving controllable style diversity by using our simple but effective definition and content. Our translation is also robust, allowing it to be applied to video to video translation at no additional cost.  <ref type="figure">Figure 2</ref>. GANs N Roses Framework: Unlike previous work which samples different images in a batch, GNR use the same image with different augmentations to form a batch. This allows us to constrain the spatial invariance in style codes e.g. all style codes are the same across the batch. Our Diversity Discriminator looks at batch-wise statistics by explicitly computing the minibatch standard deviation across the batch. This ensures diversity within the batch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">GANs N' Roses</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Framework</head><p>Given two domains X and Y, for a given x ? X , our goal is to generate a diverse set of? in the Y domain that contains similar semantic contents with x. We expound translation from domain X to Y in detail (but exclude the other direction, which is mirrored, for brevity). As <ref type="figure">Figure 2</ref> shows, GANs N' Roses is made up of an Encoder and a Decoder for each direction of X ? Y and Y ? X . The encoder E disentangles an image x into a content code c(x) and a style code s(x). The decoder F takes in a content code and a style code and produces the appropriate image from Y. Together, the encoder and decoder form a Generator. At run time, we will use this generator by passing an image to the encoder, keeping the resulting content code c(x), obtaining some other relevant style code s z , then passing this pair of codes to the decoder. We want the content of the resulting anime to be controlled by the content code, and the style by the style code.</p><p>But what is content, and what style? The key idea of GANs N' Roses to define content as where things are and style as what they look like. This can be made crisp using the idea of data augmentations. Choose a collection of relevant data augmentations: style is all that is invariant under any of these, content is all that is not. Note that this definition We show that our style code is temporally coherent in videos. We can also smoothly interpolate between styles as the face moves across time. In comparison, CouncilGAN introduces artifacts temporally with varying hair/background shades, uneven eyes and jaw shapes. The style outputs are also not diverse, and interpolation offers little changes except for color. Note that neither GNR nor CouncilGAN are trained on video dataset, but GNR perform significantly better due to our sensible definition of style and content. The full video is provided in the supplementary.</p><p>is conditioned on the augmentations -different sets of data augmentations will result in different definitions of style.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Ensuring Style Diversity</head><p>The framework of <ref type="figure">Fig. 2</ref> is well established; the difficulty is ensuring that one actually gets very different anime. As section 2 sketches, there are three kinds of strategy in the current literature. First, one could simply generate from randomly chosen style codes s z ; but there is no guarantee of diversity -the decoder might cheat, by simply ignoring the style code, and produce only one style per face. Second, one could require that the decoder have the property that s z can be recovered from the decoder; but there is no guarantee of diversity -the decoder might cheat, by (say) hiding s z in a few pixel values, or by only changing the overall color of the image to signal s z . Third, one could write an explicit penalty that forces decodes with different style codes to be different; but there is no guarantee the diversity is rightthe decoder might cheat by just changing the overall color of the decodes to be different. None of these strategies is satisfactory.</p><p>Our definition of style and content offers a cure. We must learn a map F (c, s; ?) that takes content codes c and style codes s to generate anime faces. Write x i ? X for a single image, chosen uniformly at random from the data, T (?) for a function that applies a randomly chosen augmentation to that image, P (C) for the distribution of content codes, P (Y ) for the true distribution of real anime (etc), and? for generated anime. We must have that c(x i ) ? P (C). Because we define style to be what does not change under augmentation, and content to be what does, reasonably selected augmentations should mean that c(T (x i )) ? P (C) -ie applying a random augmentation to an image results in a content code that is a sample from the prior on content codes. This assumption is reasonable -if it was strongly violated, then image augmentation for training classifiers would not work.</p><p>At run time, the decoder is presented with content codes obtained by encoding true face images, and style codes that are samples from some known distribution. Now consider a generated batch obtained by: (a) choose a single real face image; (b) apply a random selection of augmentations to that image; (c) compute the content codes for those images, to obtain c i ; (d) sample style codes z i from P (S), and gen-erate? i = F (c i , z i ; ?). Because C and S are disentangled, they should be independent. In turn, (c i , z i ) ? P (C)P (S) and so ? is correct when generated batches? i are indistinguishable from fair samples of P (Y ) that are the same size.</p><p>Now assume that the generator has been trained such that, under the conditions above, generated batches? i are indistinguishable from fair samples of P (Y ) that are the same size. Because P (Y |C = c i ) ? P (Y, C = c i ), our model of P (? |C = C i ) must be correct. More formally, write T n (?) for a function that randomly chooses n augmentations from the relevant family, and applies them to the image. We obtain a batch of content codes as {x 1 , x ??? , x n } = T n (x), then form (c i , s i ) = E(x i ). We then draw z i from P (S), and form a batch of style content pairs B = {(c 1 , z 1 ), . . . , (c n , z n )}. We then use an adversary to ensure that the batch of? i = F (c i , z i ; ?) is indistinguishable from a batch of Y .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Losses</head><p>Style Consistency Loss The style code of an image must be invariant under T (?), so in a batch B as above we expect all s i to be the same. We use:</p><formula xml:id="formula_0">L scon = Var(s)<label>(1)</label></formula><p>Cycle Consistency Loss We want to ensure that the anime preserve the content code. Generate an anime? i from an image x i using (c(x i ), z i ), then map that anime back to an image using (c(? i ), s i (x i )) (where the encoder for the content code is the Y ? X encoder). We should get the original image back, because we are using the original image's style, and a content code that should be preserved. We find it helpful to shuffle the style codes in a batch (all images in the batch B should have the same style code, above), because this encourages style consistency. Writing</p><formula xml:id="formula_1">c i , s i = E Y?X (? i ) x i = F Y?X (c i , s j )<label>(2)</label></formula><p>where i = j andx i is the reconstructed image in the original domain, the cycle consistency loss is:</p><formula xml:id="formula_2">L cyc = E x x i ? x i 2<label>(3)</label></formula><p>In addition to L2 loss, we also apply LPIPs perceptual loss <ref type="bibr" target="#b20">[21]</ref>.</p><p>Diversity Discriminator and Adversarial Loss Our criterion (a generated batch? ij be indistinguishable from a fair sample of P (Y ) of the same size) applies to batches, not just individual samples. This means that we expect that across-sample properties computed within batches should match (rather than just individual sample properties). At run time, conventional classifiers almost always are presented with individual images, rather than batches. A discriminator in an adversarial method is different, because it is trained with and applied to fixed batch sizes. Our Diversity Discriminator exploits the minibatch standard deviation trick used in PGGAN <ref type="bibr" target="#b6">[7]</ref> to exploit this difference. In the penultimate layer of the discriminator, we compute the standard deviation for each feature and pass that into another FC layer. Our Diversity Discriminator outputs the real/fake logits and the standard deviation logits. This means the discriminator can identify reliable differences in within batch variation. We apply the non-saturating adversarial loss L adv from <ref type="bibr" target="#b2">[3]</ref> with R1 regularization <ref type="bibr" target="#b15">[16]</ref> for both discriminator branches. Ablations show this diversity discriminator is important ( <ref type="figure">Fig. 6</ref>), likely because it increases discriminator efficiency: it is easier for a discriminator to spot subtle differences between two sources if it sees whole batches. We use Total Loss Our total loss is</p><formula xml:id="formula_3">L = ? adv L adv + ? scon L scon + ? cyc L cyc (4)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In all experiments, we use batch size of 7, ? scon = 10, ? cyc = 20, ? adv = 1. Our architecture is based on Style-GAN2 <ref type="bibr" target="#b8">[9]</ref> with style code having a dimension of 8. We use Adam optimizer <ref type="bibr" target="#b11">[12]</ref> with a learning rate of 0.002 for 300k batch iterations for all networks. The random augmentations we use on the input images consist of random horizontal flip, rotation between (?20, 20), scaling (0.9, 1.1), translation (0.1, 0.1), shearing (0.15). The images are then upscaled to 286 ? 286 and randomly cropped for resolution of 256 ? 256. For datasets, we primarily focus on  the selfie2anime dataset <ref type="bibr" target="#b9">[10]</ref> with additional experiments of AFHQ <ref type="bibr" target="#b0">[1]</ref>. We compare GANs N' Roses with several open source state-of-the-art multimodal I2I frameworks which we if available, use their pretrained models, otherwise, train using their default hyperparameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Source GNR AniGAN</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Qualitative Comparisons</head><p>Generally GNR produces strongly diverse images when given the same source image and different random style codes. The style codes drive appearance in hair, eyes, nose, mouth, colors, etc. while content drives pose, face sizes, where facial parts are, etc. <ref type="figure">Figure 4</ref> shows that GNR outperforms other SOTA multimodal I2I frameworks in terms of both quality and diversity. GNR produces images with varied colors, hairstyles, eye shapes, facial structures, etc. while other frameworks differs mostly in colors. Note that the style vectors are chosen at random and the images are not cherry-picked. We also compared to the very recent Source GANs N' Roses + Mode Seeking Loss -Div. Disc <ref type="figure">Figure 6</ref>. Ablation: We compare GNR with 1) GNR without Diversity Discriminator 2) GNR without Diversity Discriminator and with Mode Seeking Loss <ref type="bibr" target="#b14">[15]</ref>. Style codes are randomly sampled for each column and not cherry picked. Diversity Discriminator clearly pushes GNR to produce more diverse and realistic outputs. Mode seeking loss introduces less diversity and encourages artifacts. work of AniGAN <ref type="bibr" target="#b13">[14]</ref> in <ref type="figure" target="#fig_2">Fig. 5</ref>. However, we are only comparing with test cases presented in the paper as the source code is not released. Note that even though AniGAN is trained on a bigger and more diverse dataset compared to ours, we are able to generate higher quality images with significantly better diversity. Also, AniGAN generates at 128 ? 128 resolution while we generate at 256 ? 256.</p><p>Ablation Ablation shows the Diversity Discriminator plays a big part in ensuring diverse outputs <ref type="figure">(Fig. 6</ref>). Furthermore, replacing the diversity discriminator with a mode seeking loss produces some diversity, but results are much weaker than GNR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Quantitative Comparisons</head><p>We evaluate GNR quantitatively in <ref type="table" target="#tab_1">Table 1</ref>, using diversity FID, FID , and LPIPS. Generally, we show that GNR is significantly better than all other SoTA frameworks in all metrics over all experiments in <ref type="table" target="#tab_1">Table 1</ref>. Both DFID and LPIPS focus on diversity of images, and scores in those metrics quantitatively confirms our diversity is better than other frameworks. For selfie2anime, our FID is only very slightly worse than CouncilGAN. Note that FID does not represent output diversity as it is a population matching metric computed over all images.</p><p>FID The Fr?chet Inception Distance (FID) <ref type="bibr" target="#b3">[4]</ref> is a widely used metric to measure the performance of generative models. It measures the realism and diversity of generated images compared to real images. All FID calculations are done using FID ? <ref type="bibr" target="#b1">[2]</ref>, as the original FID has a generator dependent bias, and FID ? removes this using simple extrapolation procedures.</p><p>Diversity FID While previous I2I frameworks <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b0">1]</ref> also use FID for evaluation, they generate one image per input image -this does not measure style diversity; for example, a method might only be capable of generating a single unique image per input image and still get a good FID. Our Diversity FID (DFID) is a simple modification to the original FID algorithm that aims to measure style diversity for multi-modal I2I framework. Recall the assumption and notation of section 3.2 -applying a random augmentation to an image results in a content code that is a sample from the prior on content code. This assumption means that if we generate codes for M augmentations of any given test image, we have M IID samples from P (C). In turn, if our model of P (? |C = c i ) is correct, then generating anime from these augmentations using randomly selected style codes should result in a set of anime that are IID samples from P (Y ). This should be true for any selected image. In turn, if the method works, </p><formula xml:id="formula_4">DF ID = 1 N N i=1 F ID({G(T M (x i ), z)|z ? N (0, I)}, Y )<label>(5)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Video to Video Translation</head><p>Our definition of style and content means that, when a face moves in a frame, the style should not change but the content will. In particular, the content encodes where features are while style encodes what they look like. In turn, content codes should capture all frame-to-frame movement, and we should be able to synthesize anime video without ever training on temporal sequences. We apply GNR to face videos frame by frame, then assemble the resulting frames into a video. Results in <ref type="figure" target="#fig_1">Fig. 3 row 2</ref> shows that GNR produces images that moves according to the source while maintaining consistent appearances temporally. Note that very pleasing visual effects can be obtained by manipulating what style code is used at what point on the timeline; for example, a synthesized anime could change style at each beat. The full video in the Supplementary shows a smooth video to video translation with minimal temporal artifacts.</p><p>We also tested the SOTA in selfie2anime generation CouncilGAN in the same task of V2V as shown in <ref type="figure" target="#fig_1">Fig. 3</ref>. While CouncilGAN adequately captures the pose of the image, it is not temporally consistent. Artifacts are present and the shades of the hair changes across time. The interpolation across 2 styles also showed the lack of image diversity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Discussion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Disentanglement</head><p>From our results in <ref type="figure">Fig. 1</ref>, we can clearly see that content dictates the pose, face shape, and to some extent, the hairstyle while style controls everything else. The disentanglement between content and style arise from our style con-sistency loss, which enforces the style codes of randomly augmentations of the same image be consistent. Our Diversity discriminator then forces the distribution of images across styles be diverse while cycle consistency loss ensures information is not loss in the translation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Unsupervised Latent Space Editing</head><p>GANs N' Roses uses the StyleGANv2 <ref type="bibr" target="#b7">[8]</ref> architecture which allows latent space editing techniques to also work on GNR. SeFa <ref type="bibr" target="#b17">[18]</ref> finds latent direction that corresponds to large semantic changes in a style-based GAN by finding eigenvectors of the style modulation weight parameters. We show that our style space is highly expressive and editable using these eigenvectors, <ref type="figure" target="#fig_3">Fig. 7</ref>. In each row, we have different source image with different intial style code. When we apply the same eigenvector direction to the different style code, the output images exhibit similar style changes. This allows a degree of controllability over the I2I translation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this work, we define content as where things are and style as what they look like in the setting of multimodal I2I translation. Using this simple defintion, we propose GANs N' Roses, a multimodal I2I framework that produces truly diverse images that captures the complex artistic styles given a single input image. We then show that our defintion of content and style allows GNR to be applied to the difficult problem of video to video translation with no additional training.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>t e x i t s h a 1 _ b a s e 6 4 = " D s H p V o i j k K i r E J w 5 9 C K M b v v Z A q g = " &gt; A A A B + n i c b V D L S g M x F L 1 T X 7 W + p r p 0 E y x C B S k z o u i y 6 E Y 3 U s E + o C 0 l k 2 b a 0 E x m S D J K G f s p b l w o 4 t Y v c e f f m G l n o a 0 H A o d z 7 u W e H C / i T G n H + b Z y S 8 s r q 2 v 5 9 c L G 5 t b 2 j l 3 c b a g w l o T W S c h D 2 f K w o p w J W t d M c 9 q K J M W B x 2 n T G 1 2 l f v O B S s V C c a / H E e 0 G e C C Y z w j W R u r Z x U 6 A 9 Z B g n t x O y s 4 x u j n q 2 S W n 4 k y B F o m b k R J k q P X s r 0 4 / J H F A h S Y c K 9 V 2 n U h 3 E y w 1 I 5 x O C p 1 Y 0 Q i T E R 7 Q t q E C B 1 R 1 k 2 n 0 C T o 0 S h / 5 o T R P a D R V f 2 8 k O F B q H H h m M g 2 q 5 r 1 U / M 9 r x 9 q / 6 C Z M R L G m g s w O + T F H O k R p D 6 j P J C W a j w 3 B R D K T F Z E h l p h o 0 1 b B l O D O f 3 m R N E 4 q 7 l n F u T s t V S + z O v K w D w d Q B h f O o Q r X U I M 6 E H i E Z 3 i F N + v J e r H e r Y / Z a M 7 K d v b g D 6 z P H 5 5 E k u s = &lt; / l a t e x i t &gt; N (0, I)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Video Temporal Coherence and Interpolation:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 .</head><label>5</label><figDesc>Comparisons: We compare GNR with SoTA AniGAN. GNR produces much greater style diversity (eye shapes, hairstyles, line styles, etc.) while AniGAN produces very similar images that only changes hair color across style. Note that AniGAN was trained on a larger and less diverse dataset compared to GNR. AniGAN figures are taken fromFig.1of their paper since their code is not yet released.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 7 .</head><label>7</label><figDesc>Latent Exploration: GNR encodes rich semantic information in the style space. We can find meaningful latent direction from SeFa which enables us to perform style editing. Each column has differently sampled style but adds the same latent direction; each eigenvector give us semantically meaningful results. col 1: Small eyes and sharp features; col 2: abstract art lines; col 3: Big eyes and dark hair.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>should be small. In all experiments, we use M = 1000 and N = 100. All test images used are the same across experiments.selfie2anime cat2dog DFID ? FID ? LPIPS ? DFID ? FID ? LPIPS ? Quantitative Comparisons:We compare GNR with other SoTA frameworks. DFID compares the distribution of a single image translated with different styles with the distribution of real images, thus focus on output diversity. FID compares general image quality while LPIPS also focuses on output diversity. Our method shows significant improvements across all metrics, especially on the Diversity FID and LPIPS metric which measures diversity.</figDesc><table><row><cell>GNR</cell><cell>35.6</cell><cell>34.4</cell><cell>0.505</cell><cell>26.1</cell><cell>26.9</cell><cell>0.569</cell></row><row><cell>DRIT++</cell><cell>94.6</cell><cell>63.8</cell><cell>0.201</cell><cell>160.1</cell><cell>91.5</cell><cell>0.231</cell></row><row><cell>CouncilGAN</cell><cell>56.2</cell><cell>38.1</cell><cell>0.430</cell><cell>172.5</cell><cell>90.8</cell><cell>0.298</cell></row><row><cell>StarGANv2</cell><cell>83.0</cell><cell>59.8</cell><cell>0.427</cell><cell>53.6</cell><cell>44.2</cell><cell>0.530</cell></row><row><cell cols="3">LPIPS LPIPS [21] is a metric that computes the percep-</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">tual similarity between two images using deep network ac-</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">tiavtions. Following the LPIPS metric used in StarGANv2,</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">we use LPIPS to measure similarity of the different images</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">generated a single source image with varied style vectors.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">A small LPIPS score indicates mode collapse and a lack of</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">diversity when varying style codes. For each test image,</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">we randomly generate 10 outputs which we then compute</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">their pairwise LPIPS distance. We then average the distance</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>across all test images, N = 100.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunjey</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjung</forename><surname>Uh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaejun</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jung-Woo</forename><surname>Ha</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.01865</idno>
		<title level="m">Stargan v2: Diverse image synthesis for multiple domains</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Effectively unbiased fid and inception score and where to find them</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><forename type="middle">Jin</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Forsyth</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.07023</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Gans trained by a two time-scale update rule converge to a local nash equilibrium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hubert</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Nessler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.08500</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Multimodal unsupervised image-to-image translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="172" to="189" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Image-to-image translation with conditional adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinghui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1125" to="1134" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Progressive growing of gans for improved quality, stability, and variation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10196</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Analyzing and improving the image quality of StyleGAN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miika</forename><surname>Aittala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janne</forename><surname>Hellsten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
		<idno>abs/1912.04958</idno>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Analyzing and improving the image quality of stylegan</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miika</forename><surname>Aittala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janne</forename><surname>Hellsten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="8110" to="8119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">U-gat-it: unsupervised generative attentional networks with adaptive layer-instance normalization for image-toimage translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junho</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minjae</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyeonwoo</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kwanghee</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.10830</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning to discover cross-domain relations with generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taeksoo</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moonsu</forename><surname>Cha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyunsoo</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jung</forename><forename type="middle">Kwon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwon</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="1857" to="1865" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Diverse image-to-image translation via disentangled representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hsin-Ying</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hung-Yu</forename><surname>Tseng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maneesh</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Anigan: Style-guided generative adversarial networks for unsupervised anime face generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanlue</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yitong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chia-Wen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linlin</forename><surname>Shen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Mode seeking generative adversarial networks for diverse image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hsin-Ying</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hung-Yu</forename><surname>Tseng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siwei</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Which training methods for gans do actually converge?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><surname>Mescheder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Nowozin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.04406</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Breaking the cycle -colleagues are all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ori</forename><surname>Nizan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ayellet</forename><surname>Tal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE conference on computer vision and pattern recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Closed-form factorization of latent semantics in gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujun</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.06600</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">High-resolution image synthesis and semantic manipulation with conditional gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting-Chun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8798" to="8807" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Dualgan: Unsupervised dual learning for image-to-image translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zili</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minglun</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2849" to="2857" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">The unreasonable effectiveness of deep features as a perceptual metric</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Unpaired image-to-image translation using cycleconsistent adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2223" to="2232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Toward multimodal image-to-image translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepak</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Shechtman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="465" to="476" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
