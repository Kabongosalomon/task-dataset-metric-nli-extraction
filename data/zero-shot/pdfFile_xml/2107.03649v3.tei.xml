<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Detection and Classification of Acoustic Scenes and Events 2021 Heavily Augmented Sound Event Detection utilizing Weak Predictions</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyeonuk</forename><surname>Nam</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Mechanical Engineering</orgName>
								<orgName type="institution">Korea Advanced Institute of Science and Technology</orgName>
								<address>
									<addrLine>291 Daehak-ro, Yuseong-gu</addrLine>
									<postCode>34141</postCode>
									<settlement>Daejeon</settlement>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byeong-Yun</forename><surname>Ko</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Mechanical Engineering</orgName>
								<orgName type="institution">Korea Advanced Institute of Science and Technology</orgName>
								<address>
									<addrLine>291 Daehak-ro, Yuseong-gu</addrLine>
									<postCode>34141</postCode>
									<settlement>Daejeon</settlement>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gyeong-Tae</forename><surname>Lee</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Mechanical Engineering</orgName>
								<orgName type="institution">Korea Advanced Institute of Science and Technology</orgName>
								<address>
									<addrLine>291 Daehak-ro, Yuseong-gu</addrLine>
									<postCode>34141</postCode>
									<settlement>Daejeon</settlement>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seong-Hu</forename><surname>Kim</surname></persName>
							<email>seonghu.kim@kaist.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Mechanical Engineering</orgName>
								<orgName type="institution">Korea Advanced Institute of Science and Technology</orgName>
								<address>
									<addrLine>291 Daehak-ro, Yuseong-gu</addrLine>
									<postCode>34141</postCode>
									<settlement>Daejeon</settlement>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Won-Ho</forename><surname>Jung</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Mechanical Engineering</orgName>
								<orgName type="institution">Korea Advanced Institute of Science and Technology</orgName>
								<address>
									<addrLine>291 Daehak-ro, Yuseong-gu</addrLine>
									<postCode>34141</postCode>
									<settlement>Daejeon</settlement>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sang-Min</forename><surname>Choi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Mechanical Engineering</orgName>
								<orgName type="institution">Korea Advanced Institute of Science and Technology</orgName>
								<address>
									<addrLine>291 Daehak-ro, Yuseong-gu</addrLine>
									<postCode>34141</postCode>
									<settlement>Daejeon</settlement>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong-Hwa</forename><surname>Park</surname></persName>
							<email>yhpark@kaist.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Mechanical Engineering</orgName>
								<orgName type="institution">Korea Advanced Institute of Science and Technology</orgName>
								<address>
									<addrLine>291 Daehak-ro, Yuseong-gu</addrLine>
									<postCode>34141</postCode>
									<settlement>Daejeon</settlement>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Detection and Classification of Acoustic Scenes and Events 2021 Heavily Augmented Sound Event Detection utilizing Weak Predictions</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>Challenge</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T03:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Sound Event Detection</term>
					<term>Data augmen- tation</term>
					<term>weak prediction</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The performances of Sound Event Detection (SED) systems are greatly limited by the difficulty in generating large strongly labeled dataset. In this work, we used two main approaches to overcome the lack of strongly labeled data. First, we applied heavy data augmentation on input features. Data augmentation methods used include not only conventional methods used in speech/audio domains but also our proposed method named FilterAugment. Second, we propose two methods to utilize weak predictions to enhance weakly supervised SED performance. As a result, we obtained the best PSDS1 of 0.4336 and best PSDS2 of 0.8161 on the DESED real validation dataset. This work is submitted to DCASE 2021 Task4 and is ranked on the 3 rd place. Code available: https://github.com/frednam93/FilterAugSED. ?</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Sound Event Detection (SED) aims to classify sound event labels in a sound clip with their corresponding time of onset and offset (timestamp). To train a neural network to perform such task, a large dataset with strong labels (providing class label, onset, offset) is needed. However, it is hard to obtain because manually annotating timestamps is very costly and the annotations largely vary by annotators. Although this problem can be remedied by synthesizing strongly labeled dataset from foreground and background datasets, it is still difficult to obtain large foreground dataset which precisely includes (both event-wise and timestampwise) desired sound event only. As training SED model with strongly labeled dataset synthesized from small foreground dataset could the model to overfit on few sound examples, it is very important to utilize weakly labeled dataset (provides class infor- mation without timestamp) and unlabeled dataset (provides no information at all) which are much simpler to obtain.</p><p>We tackled these problems with two main approaches: applying heavy data augmentation and utilizing trained model's weak prediction. Data augmentations help to overcome the limitation of strongly labeled dataset synthesized using small amount of foreground events (~1.6 hr). We applied frameshift, mixup <ref type="bibr" target="#b0">[1]</ref>, time masking from SpecAugment <ref type="bibr" target="#b2">[2]</ref>, and our original method named FilterAugment. We applied different augmentations on the inputs for student model and teacher model to maximize the advantage of Mean Teacher <ref type="bibr" target="#b3">[3]</ref>.</p><p>It is very important to utilize weakly labeled dataset because it is easier to obtain (weak dataset's size: ~4.3 hr) compared to the strongly labeled dataset and provides more information compared to unlabeled dataset. The DCASE2021 Task4 baseline model [4] produces weak predictions so that it can be trained with additional weakly labeled dataset. This feature has enhanced the model's performance on strong predictions, although weak predictions are not directly used in the evaluation step. We propose two methods to utilize weak predictions of model furthermore. One is weak prediction masking, a simple method to enhance model's performance on strong predictions. The other is weak SED that uses only weak predictions and sets timestamps as the entire duration of audio clip.</p><p>Other than methods mentioned above, we applied normalization to waveform before applying mel-spectrogram transform and context gating <ref type="bibr" target="#b4">[5]</ref> at CNN activation. We applied Asymmetric Focal Loss (AFL) <ref type="bibr" target="#b5">[6]</ref> as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">PROPOSED METHODS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Base feature and model</head><p>We used CRNN model from the baseline model [4], with its width doubled (increased number of CNN channels from {16, 32, 64, 128, 128, 128, 128} to {32, 64, 128, 256, 256, 256, 256} and number of RNN cells from 128 to 256), and applying context gating <ref type="bibr" target="#b4">[5]</ref> as CNN layers' activation function. Input feature used is log mel-spectrogram with 128 mel frequency bins, 2048 window length and 256 hop length of 2048 obtained from input audio clips with sampling rate of 16 kHz, which is the same with the input feature of the baseline[4]. Waveform normalization is applied before converting to mel-spectrogram, so that the waveform's maximum absolute value is set to 1. This ensures that the mixup algorithm <ref type="bibr" target="#b0">[1]</ref> mixes the features with intended mixing ratio (otherwise, one feature might overpower the other so the mixture does not reflect the intended mixing ratio) and helps model to converge better in similar manner that batch normalization does <ref type="bibr" target="#b6">[7]</ref>. In addition, we applied the Asymmetric Focal Loss (AFL) <ref type="bibr" target="#b5">[6]</ref> with ?=0.125 and ?=4 on model 3 only to improve PSDS2 <ref type="bibr" target="#b7">[8]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Heavy data augmentation</head><p>Before discussing data augmentation procedure implemented in this work, we first introduce an original method named Fil-terAugment. FilterAugment is proposed to consider various acoustic conditions: different microphones/speakers, relative positions between mics &amp; speakers, and coloration by early reverberation. As human can classify sound events in various acoustic conditions robustly, we mimicked those acoustic conditions by applying the FilterAugment. FilterAugment algorithm, which is applied on mel-spectrogram, operates in the procedures described as follows. First, the algorithm randomly chooses n, the number of frequency boundaries. Then it randomly chooses frequencies of the boundaries so that the boundaries split the whole frequency range into n + 1 frequency bands. Lastly, for each frequency band determined, it randomly chooses factors multiplied to the amplitudes of mel-spectrogram corresponding to the frequency band. The range of number for frequency bands and the dB range for random factors are predetermined as hyperparameter settings. An example of the FilterAugment application on log mel-spectrogram is shown in <ref type="figure" target="#fig_1">Figure 1</ref>, where amplitudes in frequency bands below 2 kHz and above 4.5 kHz are amplified while the amplitudes in frequency band from 2 kHz to 4.5 kHz are reduced. FilterAugment can be understood as application of certain types of filters such as low pass filter, high pass filter, band pass/reject filter, bookshelf filter, etc. to the melspectrogram to describe randomly distributed acoustic conditions. We applied heavy data augmentation to enhance model's generalization capacity. As strongly labeled dataset is synthesized from the foreground dataset with limited size, we applied various data augmentations to avoid overfitting. In this augmentation process, we applied different augmentations on input features for student model and teacher model to maximize the performance of Mean Teacher method <ref type="bibr" target="#b3">[3]</ref>, unlike the baseline [4] which applied the same transformation to the input features for student and teacher model. Data augmentation methods we applied are frameshift, mixup <ref type="bibr" target="#b0">[1]</ref>, time masking <ref type="bibr" target="#b2">[2]</ref>, and Fil-terAugment. Among them, we applied frameshift, mixup and time masking the same on the input features for student and teacher models because these methods transform label information. The consistency cost has to compare the predictions of features with the same label, so data augmentation that affects label cannot be applied differently on the inputs for student and teacher model. Frameshift shifts the label in time axis, mixup mixes the labels with the same mix ratio applied on the features, and time masking masks the label in the time frames masked in the input feature. We also experimented on the mixup rate and increasing it from 0.5 (from baseline) to 0.8 result in a slight increase in PSDS1 and a slight decrease in PSDS2. After setting timeshift, mixup and time masking the same, we compared the performances of Gaussian noise, frequency masking <ref type="bibr" target="#b2">[2]</ref> and FilterAugment. They can be applied with different augmentations to inputs for student and teacher models as they do not alter the label information. Applying Gaussian noise degraded the model performance while applying frequency masking and FilterAugment enhanced the performance compared to the model trained without these augmentations shown in <ref type="table" target="#tab_0">Table 1</ref>. The PSDSs in this table are obtained from ensemble averages over 6 models for each method. It is also shown in <ref type="table" target="#tab_0">Table 1</ref> that simultaneously applying frequency masking and FilterAugment does not enhance performance furthermore. It is supposed that simultaneous application of two methods causes too much distortion on the feature. To remedy this problem, we tried weaker transforms (narrower dB range for FilterAugment and smaller maximum frequency bins to mask in frequency masking) but it still did not enhance the performance. It seems that both methods work on frequency domain, so their role overlaps. For the final model, FilterAugment was chosen to be applied as it showed better performance. Such heavy data augmentation could enhance model's generalization performance by providing the model with different versions of training data each epoch. To further enhance the model's generalization capacity upon heavy data augmentations, width (number of filters in each layer) of CRNN was doubled.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Utilizing weak prediction</head><p>Although it is not easy to directly compare the performances of strong and weak predictions, we can roughly regard that it is relatively easier to train weak prediction than to train strong prediction. It is due to the unbalance between numbers of active and inactive time frames in each strongly labeled data <ref type="bibr" target="#b5">[6]</ref>. In many cases, there are many more inactive frames than active frames in sound clips, causing the model's tendency to predict inactive frames easily. In weak prediction, such problem is not as severe as in strong prediction, as we consider the label is present as long as the event exists in the clip regardless of its time location. Therefore, we propose two methods to effectively utilize weak predictions of the model.</p><p>Firstly, we propose weak prediction masking, a simple method to enhance strong prediction. Before we apply thresholds on strong prediction, we apply the thresholds on weak prediction to determine whether the events exist or not in the entire sound clip. Then we apply the thresholds on the strong predictions only for the labels that are verified to exist by the weak prediction, to find out at which time frames that class is active. This is equivalent to applying a mask to the strong prediction before applying the thresholds, where the mask is obtained by applying the same threshold on the weak prediction. We tested this method on various settings while exploring through various hyper parameters, and it enhanced the performance for the most of cases.</p><p>Secondly, we propose weak SED, which uses only weak predictions and sets timestamp equal to the entire duration of the audio clip. That is to say, we predict the label to be present from the start until the end of the audio clip if the weak prediction corresponding to the label is above the threshold. This method results in much greater PSDS2, while degrading PSDS1 largely. For example, in one case of 6 ensemble averaged models resulted in PSDS1 decreased from 0.421 to 0.057 while PSDS2 is increased from 0.655 to 0.807. This seems to be working well on PSDS2 due to its low tolerance parameters for Detection Tolerance Criterion and Ground Truth intersection Criterion <ref type="bibr" target="#b7">[8]</ref>. As the audio clips are maximum 10 seconds and the tolerance parameters are 0.1, the prediction for whole 10 seconds (152 frames) are regarded as True Positive when there is at least one ground truth longer than 1 second (15 frames). The prediction that is regarded as True Positive will not be tested if it is crosstrigger or not, so high PSDS2 can be obtained by this method. This method is equivalent to solely applying weak SED (audio tagging). From this result, we can infer that applying weak SED on short audio clips works well for the situations where the exact estimate on timestamps is relatively less important, such as scenario 2 for PSDS2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">RESULTS</head><p>The results for the submitted models on the DESED Real Validation dataset is shown in <ref type="table" target="#tab_1">Table 2</ref>. Model 1 is ensemble average of the 16 models trained with all the technics mentioned above including the methods originally proposed in this work: waveform normalization, context gating, doubling model width, time masking between 7~30 frames, FilterAugment with dB between -7.5 ~ 6dB &amp; number of frequency band between 2 and 4, with weak prediction masking. Model 2~4 are the models that are modified from model 1 with the following details. Model 2 is an ensemble average on the 9 models with mixup rate increased from 0.5 to 0.8, number of frequency bands for FilterAugment between 2 and 3, and median filter of 5 (instead of 7 in baseline model <ref type="bibr">[4]</ref>). Model 3 is an ensemble average on 11 models applied by AFL with ?=0.125 and ?=4. Model 4 is an ensemble average on 9 models with weak SED. The total ranking score on the DESED Real Validation dataset is 1.2494. This work has won 3 rd rank in DCASE 2021 Task4. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">ACKNOWLEDGMENT</head><p>H. N. thanks to Junhyeok Lee from MINDs Lab Inc. for willingly sharing his experiences and insights.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>This work was supported by the Institute of Information &amp; communications Technology Planning &amp; Evaluation (IITP) grant funded by the Korea government (MSIT) (No. 2017-0-00162, Development of Human-care Robot Technology for Aging Society) and Human Resources Program in Energy Technology of the Korea Institute of Energy Technology Evaluation and Planning (KETEP) funded by the Ministry of Trade, Industry &amp; Energy, Republic of Korea (Grant No. 20204030200050).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>An illustration of FilterAugment transformation on a log mel-spectrogram. Original feature is on the top and the transformed feature is at the bottom.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Experimental results with various data augmentation methods which do not alter the label information.</figDesc><table><row><cell>Methods</cell><cell cols="2">PSDS1 PSDS2</cell></row><row><cell>Without Gaussian noise, frequency masking, FilterAugment</cell><cell>0.407</cell><cell>0.618</cell></row><row><cell>Gaussian noise: snr=30~50dB</cell><cell>0.404</cell><cell>0.617</cell></row><row><cell>Gaussian noise: snr=30~45dB</cell><cell>0.405</cell><cell>0.617</cell></row><row><cell>Gaussian noise: snr=35~45dB</cell><cell>0.406</cell><cell>0.614</cell></row><row><cell>Gaussian noise: snr=35~40dB</cell><cell>0.401</cell><cell>0.610</cell></row><row><cell>Frequency masking: max bins=8</cell><cell>0.405</cell><cell>0.623</cell></row><row><cell>Frequency masking: max bins=12</cell><cell>0.403</cell><cell>0.624</cell></row><row><cell>Frequency masking: max bins=16</cell><cell>0.412</cell><cell>0.640</cell></row><row><cell>Frequency masking: max bins=32</cell><cell>0.404</cell><cell>0.621</cell></row><row><cell>FilterAug: dB= -6 ~ 4.5, #band=2~4</cell><cell>0.416</cell><cell>0.640</cell></row><row><cell>FilterAug: dB= -6 ~ 6, #band=2~4</cell><cell>0.415</cell><cell>0.646</cell></row><row><cell>FilterAug: dB= -7.5 ~ 6, #band=2~4</cell><cell>0.4208</cell><cell>0.655</cell></row><row><cell>FilterAug: dB= -7.5 ~ 6, #band=2~3</cell><cell>0.4212</cell><cell>0.645</cell></row><row><cell>Frequency masking: max bins=16 + FilterAug: dB= -7.5 ~ 6, #band=2~4</cell><cell>0.415</cell><cell>0.647</cell></row><row><cell>Frequency masking: max bins=16 + FilterAug: dB= -6 ~ 4.5, #band=2~4</cell><cell>0.417</cell><cell>0.649</cell></row><row><cell>Frequency masking: max bins=4 + FilterAug: dB= -7.5 ~ 6, #band=2~4</cell><cell>0.416</cell><cell>0.651</cell></row><row><cell>Frequency masking: max bins=4 + FilterAug: dB= -6 ~ 4.5, #band=2~4</cell><cell>0.416</cell><cell>0.639</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Final results of the models submitted.</figDesc><table><row><cell>Model No.</cell><cell cols="3">Details PSDS1 PSDS2</cell><cell>Event-based F1</cell></row><row><cell>baseline[4]</cell><cell>SED SED + SS</cell><cell>0.342 0.373</cell><cell>0.527 0.549</cell><cell>40.1% 44.3%</cell></row><row><cell>1</cell><cell>Default</cell><cell>0.4220</cell><cell>0.6586</cell><cell>48.2%</cell></row><row><cell></cell><cell>Mixup rate=0.8</cell><cell></cell><cell></cell><cell></cell></row><row><cell>2</cell><cell>+ FiltAug #band=2~3</cell><cell>0.4336</cell><cell>0.6392</cell><cell>49.6%</cell></row><row><cell></cell><cell>+median filter=5</cell><cell></cell><cell></cell><cell></cell></row><row><cell>3</cell><cell>AFL</cell><cell>0.3790</cell><cell>0.6921</cell><cell>33.3%</cell></row><row><cell>4</cell><cell>Weak SED</cell><cell>0.0641</cell><cell>0.8161</cell><cell>20.3%</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">MixUp: Beyond empirical risk minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>6th Int</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<title level="m">Conf. Learn. Represent. ICLR 2018 -Conf. Track Proc</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Specaugment: A simple data augmentation method for automatic speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. Annu. Conf. Int. Speech Commun. Assoc. INTERSPEECH</title>
		<imprint>
			<biblScope unit="page" from="2613" to="2617" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>2019-Septe</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tarvainen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Valpola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="page" from="1196" to="1205" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Learnable pooling with Context Gating for video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Miech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.06905v2</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>cs.CV</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Impact of Sound Duration and Inactive Frames on Sound Event Detection Performance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Imoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mishima</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Arai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kondo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. Conf. Acoust. Speech, Signal Process</title>
		<imprint>
			<biblScope unit="page" from="860" to="864" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">32nd Int. Conf. Mach. Learn. ICML 2015</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A Framework for the robust evaluation of Sound Event Detction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ferroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tuveri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Azcarreta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sacha</forename><surname>Krstulovi?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. Conf. Acoust. Speech, Signal Process</title>
		<imprint>
			<biblScope unit="page" from="61" to="65" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
