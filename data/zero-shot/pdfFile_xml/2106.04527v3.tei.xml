<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">LaplaceNet: A Hybrid Graph-Energy Neural Network for Deep Semi-Supervised Classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Sellars</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angelica</forename><forename type="middle">I</forename><surname>Aviles-Rivero</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carola-Bibiane</forename><surname>Sch?nlieb</surname></persName>
						</author>
						<title level="a" type="main">LaplaceNet: A Hybrid Graph-Energy Neural Network for Deep Semi-Supervised Classification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T10:16+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Semi-supervised learning</term>
					<term>deep learning</term>
					<term>image classification</term>
					<term>graph-based methods</term>
					<term>pseudo-labelling</term>
					<term>data aug- mentation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Semi-supervised learning has received a lot of recent attention as it alleviates the need for large amounts of labelled data which can often be expensive, requires expert knowledge and be time consuming to collect. Recent developments in deep semi-supervised classification have reached unprecedented performance and the gap between supervised and semi-supervised learning is ever-decreasing. This improvement in performance has been based on the inclusion of numerous technical tricks, strong augmentation techniques and costly optimisation schemes with multi-term loss functions. We propose a new framework, LaplaceNet, for deep semi-supervised classification that has a greatly reduced model complexity. We utilise a hybrid approach where pseudo-labels are produced by minimising the Laplacian energy on a graph. These pseudo-labels are then used to iteratively train a neural-network backbone. Our model outperforms state-of-the-art methods for deep semi-supervised classification, over several benchmark datasets. Furthermore, we consider the application of strong-augmentations to neural networks theoretically and justify the use of a multi-sampling approach for semi-supervised learning. We demonstrate, through rigorous experimentation, that a multi-sampling augmentation approach improves generalisation and reduces the sensitivity of the network to augmentation.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>The advent of deep learning has been key in achieving outstanding performance in several computer vision tasks including image classification <ref type="bibr" target="#b0">[1]</ref>- <ref type="bibr" target="#b4">[5]</ref>, object detection e.g. <ref type="bibr" target="#b5">[6]</ref>- <ref type="bibr" target="#b7">[8]</ref> and image segmentation <ref type="bibr" target="#b8">[9]</ref>- <ref type="bibr" target="#b10">[11]</ref>. Training deep learning models often relies upon access to large amounts of labelled training data. In real-world scenarios we often find that labels are scarce, expensive to collect, prone to errors (high uncertainty) and might require expert knowledge. Therefore, relying on a well-representative dataset to achieve good performance is a major limitation for the practical deployment of machine learnt methods. These issues have motivated the development of techniques which are less reliant on labelled data.</p><p>Semi-supervised learning aims to extract information from unlabelled data, in combination with a small amount of label data, and produce results comparable to fully supervised approaches. In recent years, the developments in deep learning have motivated new directions in semi-supervised learning (SSL) for image classification. The major benefit of these new deep approaches being the ability to learn feature representations rather than rely upon hand-crafted features. In the P. Sellars, Angelica I. Aviles-Rivero and Carola-Bibiane Sch?nlieb are with the Department of Theoretical Physics and Applied Mathematics, Univeristy of Cambridge, Cambridge, UK. ps644,ai323,cbs31@cam.ac.uk . last few years, deep SSL papers have reached unprecedented performance e.g. <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>, and the gap between supervised and semi-supervised models is much smaller now that it was even five years ago, with semi-supervised methods surpassing certain supervised techniques.</p><p>What techniques have been crucial to the improved performance of deep semi-supervised methods? Although, there is no universal answer, there are several shared commonalities between the current SOTA. The works of <ref type="bibr" target="#b11">[12]</ref>- <ref type="bibr" target="#b13">[14]</ref> demonstrated that a key factor for improving performance is the use of strong augmentations strategies such as AutoAugment <ref type="bibr" target="#b14">[15]</ref>, RandAugment <ref type="bibr" target="#b15">[16]</ref>, Cutout <ref type="bibr" target="#b16">[17]</ref> and CTAugment <ref type="bibr" target="#b13">[14]</ref>. Additionally, the use of confidence thresholding <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b17">[18]</ref> and temperature sharpening <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b18">[19]</ref> are thought to be vital in improving performance for pseudo-labeling methods. Other papers <ref type="bibr" target="#b18">[19]</ref>- <ref type="bibr" target="#b20">[21]</ref> have shown great improvement from using interpolating techniques such as MixUp <ref type="bibr" target="#b21">[22]</ref>. Several SOTA have also promoted large batch sizes <ref type="bibr" target="#b11">[12]</ref> with a large ratio of unlabelled to labelled data per batch.</p><p>Recent approaches in SSL have proposed costly optimisation schemes involving multi-term loss functions to improve the generalisation of their models <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b19">[20]</ref>. Some approaches <ref type="bibr" target="#b11">[12]</ref> use separate loss terms for unlabelled and labelled data, whilst consistency regularisation approaches such as <ref type="bibr" target="#b12">[13]</ref> use a standard supervised loss in combination with a specialised consistency loss. Other approaches go even further <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b19">[20]</ref> and use three or more loss terms which promote entropy minimisation, class balancing or simultaneously minimise several consistency losses.</p><p>Over-costly computational approaches and unnecessary complexity, make it hard to directly say what tools or approaches are important for improved generalisation and make it difficult to use SSL methods in realistic settings. Furthermore, despite the significant improvements found in using augmentations, there has been little effort in the field of SSL to investigate how best to include strong augmentations techniques in the learning framework. With these points in mind, in this work, we introduce a new deep SSL framework for image classification which offers state-of-the-art performance with massively reduced model complexity. Our main contributions are:</p><p>-We propose a graph based pseudo-label approach for semi-supervised image classification whi ch we name LaplaceNet. We demonstrate through extensive testing, that our approach produces state-of-the-art results on benchmark datasets CIFAR-10, CIFAR-100 and Mini-ImageNet. We do so with vastly reduced model complexity compared to the current state-of-the-art. We show that <ref type="figure">Fig. 1</ref>. An overview of the approach used in LaplaceNet. We start with labelled data (blue) Z l = {x i , y i } n i=1 and unlabelled data (orange) Zu = {x i } nu i=1 . The data is embedded into a feature space using a neural network back-bone, which is fixed for pseudo-label generations. The features are used to construct a graphical representation of the data. We then propagate the graph Laplacian energy over the graph to obtain an estimated label for each unlabelled point, the output of the process being the pseudo-labelled data set Zu = {x i ,? i } nu i=n l +1 . The pseudo-labels? are not guaranteed to be correct, as show by the misclassification of the cat image. Each image is then augmented na times to create multi-augment groups for each image. The full model is then trained using a simple averaged multi-augmentation cross entropy loss. The updated model is then used to create new feature embeddings and as a result more accurate pseudo-labels. This cycle of pseudo-label generation and model training continues for the duration of the algorithm. a single loss, the classic supervised loss, is all that is required for fantastic performance in the SSL domain. -We show that using an energy-based graphical model for pseudo-label generation produces more accurate pseudolabels, with a small computational overhead, than using the network's predictions directly. Furthermore, we demonstrate that energy-based pseudo-label approaches can produce state-of-the-art results without the techniques (temperature sharpening, confidence thresholding, soft labels) that are currently thought to be essential for pseudolabel methods. -Instead, we offer further evidence that strong augmentation is by far and away the most important tool for improving the performance of semi-supervised models in the natural image domain. With this in mind, we propose, theoretically justify and experimentally demonstrate that a multisample averaging approach to strong augmenation not only improves generalisation but reduces the sensitivity of the model's output to data augmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>The problem of improving image classification performance using SSL has been extensively investigated from the classic perspective e.g. <ref type="bibr" target="#b22">[23]</ref>- <ref type="bibr" target="#b27">[28]</ref>, in which one seeks to minimise a given energy functional that exploits the assumed relationship between labelled and unlabelled data <ref type="bibr" target="#b28">[29]</ref>. However, classical approaches tended to rely on hand-crafted features that limited their performance and generalisation capabilities. With the popularisation of deep learning and its ability to learn generalisable feature representations, many techniques have incorporated neural networks to mitigate problems of generalisation. These modern state-of-the-art methods are dominated by two approaches, consistency regularisation and pseudolabelling, which differ in how they incorporate unlabelled data into the loss function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Consistency Regularisation Techniques</head><p>One of the fundamental assumptions that allows semisupervised learning to help performance is the cluster assumption, which states that points in the same cluster are likely to be in the same class. This can be seen to be equivalent to the low-density assumption which states that the decision boundaries of the model should lie in low-density regions of the data distribution. Following from the above assumptions, if we have access to some labelled data Z l = {x i , y i } n l i=1 and a large amount of unlabelled data Z u = {x i } n l +nu i=n l +1 , we should seek to move our decisions boundaries to be in low density regions of the joint labelled and unlabelled data distributions.</p><p>Consistency regularisation seeks to implement the lowdensity assumption by encouraging the model f to be invariant to perturbations ? to the data x. As a result the decision boundaries are pushed to low-density regions. Mathematically, given some data perturbing function u : X ? X , such that u(x) = x + ?, consistency based approaches seek to minimise some consistency loss L con in the general form of</p><formula xml:id="formula_0">L con = ||f (u(x)) ? f (x)|| 2 2 .<label>(1)</label></formula><p>A large number of papers have applied this idea to SSL including the ?Model and temporal ensembling <ref type="bibr" target="#b29">[30]</ref>, Virtual Adversarial Training (VAT) <ref type="bibr" target="#b30">[31]</ref>, Mean Teacher <ref type="bibr" target="#b31">[32]</ref>, the Interpolation Consistency Training (ICT) <ref type="bibr" target="#b20">[21]</ref> RemixMatch <ref type="bibr" target="#b13">[14]</ref> and MixMatch <ref type="bibr" target="#b18">[19]</ref>. The downside of consistency regularisation techniques is the vagueness in choosing an appropriate ?. This vagueness is reflected in the wide range of perturbations which have been used in the field. Virtual Adversarial Training uses adversarial training to learn an effective ? for each point. Mean Teacher <ref type="bibr" target="#b31">[32]</ref> decided to apply a perturbation to the model itself, and replaces f (u(x)) with an exponential moving average of the model f EMA (x). Interpolation Consistency Training <ref type="bibr" target="#b20">[21]</ref> seeks to train the model to provide consistent predictions at interpolations of unlabelled points. The authors of <ref type="bibr" target="#b12">[13]</ref> demonstrated that by replacing simple noising perturbations with stronger augmentation perturbations (eg,  In (a) we show the fully labelled dataset. In (b) we show the result of supervised learning on a 80/20 train-test data split on a shallow neural network with one hidden layer. In (c) and (d) we show the output of differing semi-supervised approaches where for each we take the same shallow network but only keep 10 labeled samples per class so that 90% of the data is now unlabelled. In (c) we use a pseudo-labelling approach where the pseudo-labels are generated using the common network based approach. In (d) we generate the pseudo-labels via graphical propagation.</p><p>RandAugment <ref type="bibr" target="#b15">[16]</ref> or CTAugment <ref type="bibr" target="#b13">[14]</ref>) leads to a substantial performance improvements. Furthering on from this work, the authors of <ref type="bibr" target="#b32">[33]</ref> proposed optimisation improvements for consistency methods using alpha-divergenes and an expectation minimisation like algorithm. Although these techniques have demonstrated great performance, it is unclear how best to set the perturbations ? and how best to incorporated them in learning frameworks. In our work, we avoid using model based perturbations and instead focus on the the application of strong data augmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Pseudo-Labelling Techniques</head><p>Another family of methods, termed pseudo-label approaches, focus on estimating labels for the unlabelled data points and then using them in a modified loss function. Forcing the network to make predictions on unlabelled points minimises the entropy of the unlabelled predictions <ref type="bibr" target="#b28">[29]</ref> and moves the decision boundaries to low-density regions. Additional, dependent on the accuracy of the pseudo-labels, we increase the amount of labelled data the model has access to and reduce overfitting to the initally small label set. There are many ways to incorporate unlabelled data / pseudo-label pairs into the loss function but the most common ways are to either create a specific loss term for the unlabelled data pseudo-label pairs <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b34">[35]</ref> or by using composite batches containing both labelled and unlabelled data and keeping the standard supervised classification loss <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b35">[36]</ref>.</p><p>The first application of this idea to the deep learning setting was presented by Lee <ref type="bibr" target="#b36">[37]</ref>. Viewing the output of the neural network f (x) as a discrete probability distribution, Lee assigned a hard pseudo-label? for each unlabelled data point according to its most likely prediction? i = arg max f (x i ). These pseudolabels were then used in a two termed loss function of the form labelled loss unlabelled los?</p><formula xml:id="formula_1">L ssl = 1 n l n l i=1 l s (f (x), y) + ? 1 n u nu i=1 l s (f (x),?),<label>(2)</label></formula><p>where l s is some loss function and ? is a weighting parameter. The pseudo-labels are recalculated every-time the unlabeled data is passed through the network. As an alternative to hard labels, <ref type="bibr" target="#b18">[19]</ref> used the full output probability distribution of the network as a soft label for each point. However, it was found that sharpening this distribution helped ensured the model's prediction entropy was minimised. As pointed out by Arazo et al <ref type="bibr" target="#b19">[20]</ref> there is a potential pitfall in this style of approach. Networks are often wrong and the neural network can overfit to its own incorrectly guessed pseudo-labels in a process termed confirmation bias. Arazo et al proposed using MixUp <ref type="bibr" target="#b21">[22]</ref>, soft labels and a minimum ratio of labeled to unlabeled data to reduce confirmation bias. An alternative method to reduce confirmation bias is to use uncertainty quantification for the produced pseudolabels. These methods calculate a confidence score r i for each pseudolabel? i . The works of <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b37">[38]</ref> used the entropy of the probability distribution to give r i whilst <ref type="bibr" target="#b38">[39]</ref> used the distance between unlabelled points and labelled points in feature space. One can then either weight the loss terms by r i or exclude pseudo-labels whose r i is below some threshold ? in an attempt to prevent the network learning from low confidence predictions.The work of <ref type="bibr" target="#b39">[40]</ref> sought to improve upon a fixed threshold value by having a separate threshold for each class. The threshold was lower for classes with a greater learning difficulty, according to the model's entropy scores.</p><p>This style of approach is based upon the idea that the neural network is well calibrated, i.e that the model's softmax score is a good indicator of the actually likelihood of a correct prediction. However, recent research has suggested that modern neural networks are not as well calibrated as may be intuitively thought <ref type="bibr" target="#b40">[41]</ref>. In our work we demonstrate that, whilst a intuitive solution, uncertainty quantification is not needed for our pseudolabel approach.</p><p>In a completely different direction to network predictions, it has been shown from a classical perspective <ref type="bibr" target="#b24">[25]</ref> that energy based models such as graphs are well suited to the task of label propagation. Therefore, several works <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b41">[42]</ref>, <ref type="bibr" target="#b42">[43]</ref> have shown good performance by iteratively feeding the feature representation of a neural network to a graph, performing pseudo-label generation on the graph and then using those labels to train the network. However, graphical approaches have yet to show that they can produce state-of-the-art results compared to model based approaches such as <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>. In our work, we present a graphical approach which surpasses the performance of model based approaches, demonstrating that graphical approaches have a lot of promise for practical applications.In <ref type="figure" target="#fig_1">Fig. 2</ref>, we display the advantage of graphical pseudo-labels using as a case study, the two moon dataset. From that figure, we observe that the graphical approach offers a clear advantage over the network pseudo-label approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. PROPOSED TECHNIQUE</head><p>This section details our proposed semi-supervised method. We cover the generation of pseudo-labels, the optimisation of the model alongside a full algorithm and we explore our multi-sample augmentation approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Problem Statement:</head><p>From a joint distribution Z = (X , Y) we have a dataset Z of size n = n l + n u comprised of a labelled part of joint samples</p><formula xml:id="formula_2">Z l = {x i , y i } n l i=1</formula><p>and a unlabelled part Z u = {x i } n i=n l +1 of single samples on X . The labels come from a discrete set of size C y ? {1, 2, .., C}. Our task is to train a classifier f , modelled by a neural network with parameter vector ?, which can accurately predict the labels of unseen data samples from the same distribution X . The classifier f can be viewed as the composition of two functions z and g such that f (x) = g(z(x)). z : X ? R dp is the embedding function mapping our data input to some d p dimensional feature space and g : R dp ? R C projects from the feature space to the classification space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Pseudo-label Generation</head><p>As a pseudo-label based approach, we iteratively assign a pseudo-label? to all data points in Z u once per epoch. In this work, we generate hard pseudo-labels using a graph based approach first proposed by Zhou et al <ref type="bibr" target="#b25">[26]</ref> and first adapted to deep networks by Iscen et al <ref type="bibr" target="#b35">[36]</ref> which has been thoroughly studied in the classical machine learning literature. We give a visual overview of our approach in <ref type="figure">Fig 1.</ref> We first extract the feature representation of the dataset V by using the embedding function of the neural network z so that V = {z(x 1 ), .., z(x n )}. Unlike other works we do not apply augmentation to the data whilst producing the pseudo-labels. Using V and a similarity metric d, we use d(v i , v j ) = v i , v j , we construct a symmetric weighted adjacency matrix W ? R n?n . The elements w ij ? W are given by W ij = d(v i , v j ) and represent the pairwise similarities between data points. We then sparsify W using the following nearest neighbour approach, which reads:</p><formula xml:id="formula_3">W ij = ? ? ? ? ? d(v i , v j ), if i is one of the k nearest neighbor of j, or vice versa. 0 otherwise.<label>(3)</label></formula><p>We then construct the degree matrix D := diag(W 1 n ) and use this to normalise the affinity matrix W = D ?1/2 W D ?1/2 , which prevent nodes with high degree having a large global impact. Finally, we use the initial label information to create the labelled matrix Y ? R n?C</p><formula xml:id="formula_4">Y ij = 1, if y i = j, 0 otherwise.<label>(4)</label></formula><p>We can then propagate the information contained in Y across the graph structure W by minimising the graphical Laplacian of the prediction matrix F ? R n?C plus a fidelity term to the supplied labelled data:</p><formula xml:id="formula_5">Q(F ) = 1 2 n i,j=1 Wij Fi ? Dii ? Fj Djj 2 + ? 2 n i=1 ||Fi ?Yi|| 2 , (5)</formula><p>where ? is a scalar weight. The first term enforces points which are close according to the metric d to share a similar label whilst the second term encourages initially labelled points to keep their label. To side-step the computationally infeasible closed form solution, we use the conjugate gradient approach to solve the linear system</p><formula xml:id="formula_6">(I ? ?W) F = Y , where ?(1 + ?) = 1.</formula><p>Using F the pseudo-labels? i are given b?</p><formula xml:id="formula_7">y i = arg max j F ij .<label>(6)</label></formula><p>A common problem in label propagation is that the psuedolabels produced by the graph may be unbalanced over the classes and Iscen et al <ref type="bibr" target="#b35">[36]</ref> attempted to weight the optimisation problem to avoid this possibility. We found that the weighting approach of Iscen et al actually made the performance of the model worse than leaving the predictions as is. An alternate approach to counter class in-balances is distribution alignment <ref type="bibr" target="#b13">[14]</ref>, which enforces the distribution of the pseudolabel predictions to match some given prior distribution. The implementation of this idea by ReMixMatch focused on applying this idea to the network predictions and wasn't optimal for a graph based framework.</p><p>Instead we propose a novel smoother version of distribution alignment which can be applied during or just after the conjugate gradient approach. We give a full algorithm for this in Algorithm 1. The algorithm is an iterative approach which smoothly deforms the pseudo-label predictions F by the ratio R between the prior distribution D and the pseudo-label distribution of the unlabelled points D U . Thereby promoting the prediction of underrepresented classes and vice versa. To ensure the deformation is smooth we clip the range of R values to be close to one. We show in the experimental section that this approach improves the performance of the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Semi-Supervised Loss</head><p>In the deep semi-supervising setting, particularly in the current SOTA <ref type="bibr" target="#b11">[12]</ref>  <ref type="bibr" target="#b18">[19]</ref>, several works seek to minimise a semisupervised lossL ssl composed of two or more terms, one each for the labelled and unlabelled data points and potentially others covering entropy minimisation etc., which has the following form:</p><p>labelled loss unlabelled loss other term?</p><formula xml:id="formula_8">L ssl = 1 n l n l i=1 l s (f (x), y) + ? 1 n u nu i=1 l s (f (x),?) + .....,<label>(7)</label></formula><p>where ? is a balancing parameter. For our approach we wanted to strip away as much complexity from the loss function as possible in an effort to see what elements are required for good performance. We move away from using a composite loss and instead only use the standard supervised loss which has worked so well in supervised image classification. To </p><formula xml:id="formula_9">Algorithm 1 Smooth Distribution Alignment 1: Input: Pseudo-label Prediction F ? R n?C , Prior Distribu- tion D ? R C , labelled and unlabelled indexes L = {l i } n l i=1 and U = {u i } nu i=1 and max iteration T 2: Output: Adjusted Pseudo-label Prediction F ? R n?C 3: for t i = 1, t i ++, while t i &lt; T do 4: D U ? R C ? Initialise with zeros Get the pseudo-label distribution: 5: for u i ? U do 6: D U [arg max j F [u i ]] +=</formula><formula xml:id="formula_10">for c i = 1, c i ++, while c i &lt; C do 11: F [U, c i ] * = R[c i ] 12:</formula><p>end for <ref type="bibr">13:</ref> Row normalise F to give valid distributions. <ref type="bibr">14:</ref> end for include our unlabelled data we use composite batches of size b which are made up of b l labelled samples and b u unlabelled samples to which we have assigned a pseudo-label?. Our semi-supervised loss, L ssl , is given by:</p><formula xml:id="formula_11">L ssl = 1 b b i=1 l s (f (x i ), y i ).<label>(8)</label></formula><p>Note that in <ref type="formula" target="#formula_11">(8)</ref> we use y i to refer to both ground truth labels and pseudo-labels for brevity. What is remarkable about this loss is its simplicity. There is no confidence thresholding of the pseudo-labels, additional weighting parameters, no consistency based terms or other regularisations. Instead we rely upon the strength of the combination of an a energy based graphical approach to pseudo-labels estimation and the clever use of strong augmentation to increase generalisation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Training the model</head><p>For initialisation purposes, we quickly extract some baseline knowledge from the dataset by minimising a supervised loss L sup , for one hundred passes through Z l . This supervised loss reads:</p><formula xml:id="formula_12">L sup = 1 b b i=1 l s (f (x i ), y i ),<label>(9)</label></formula><p>where b is the batch size and l s is the cross entropy loss. We emphasis that (9) uses only the tiny labelled set Z l , and is performed once before the main semi-supervised optimisation.</p><p>We then begin our main learning loop which alternates between updating the pseudo-label predictions and minimising the semisupervised loss L ssl for one epoch, where we define one epoch to be one pass through the unlabelled data Z u . This cycle then runs for a total of S optimisation steps and the fully trained model is then tested on the relevant testing set. Note that we do use Mixup <ref type="bibr" target="#b21">[22]</ref> on both L sup and L ssl with a beta distribution Algorithm 2 Training Scheme for LaplaceNet Distributed Alignment on F 12:? i = arg max F i ? n l + 1 ? i ? n <ref type="bibr">13:</ref> for i = 1 to nu bu do 14:</p><formula xml:id="formula_13">1: Input: labelled data Z l = {x i , y i } n l i=1 , unlabelled data Z u = {x i } n i=n l +1 ,</formula><formula xml:id="formula_14">B L = {x i , y i } b l i=1 ? Z l , B U = {x i ,? i } bu i=1 ? Z u 15: Composite Batch B = B L ? B U</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>16:</head><p>Optimise L ssl , s i = s i + 1 <ref type="bibr">17:</ref> end for 18: end while parameters ?. In Algorithm 2, we give an overview of training our model for S optimisation steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Multi-Sampling Augmentation</head><p>Since the work of <ref type="bibr" target="#b12">[13]</ref>, several approaches have implemented the use of strong augmentations <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b17">[18]</ref> to the problem of semi-supervised learning, with each work having a different way of including augmentation to their framework. Very recent works <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b17">[18]</ref> have begun using multiple augmented versions of the same unlabelled image. As yet there is no motivation for why this multiple sampling idea is preferable to alternatives such as larger batch sizes or running the code for more steps. In this section we offer a theoretical motivation for why multi-sampling improves generalization along with a mathematically bound on its performance gain. With this knowledge in mind we provide a simple method for including augmentation averaging into our SSL framework and demonstrate this approach increases accuracy and reduces the sensitivity of the model to data augmentation.</p><p>We view an augmentation strategy A as a set T of transformations t : X ? X and denote it as T = {t 1 , t 2 , .., t ? }. The current standard approach, that the majority of existing techniques follow, is to simply sample t ? T once for each data point and compute some augmented loss L Aug :</p><formula xml:id="formula_15">L Aug = 1 n n i=1</formula><p>l s (f (t(x i )), y i ).</p><p>However, we argue that such a simple implementation, might not extract the full information present in the augmentation. If we want to encourage our model output to be more resistant to data augmentations from T , and as a result produce a more generalisable model, we need to perform a multi-sample approach. To justify this, we consider the following loss L T :</p><formula xml:id="formula_17">L T = 1 n n i=1 E t?T [l(f (t(x i )), y)],<label>(11)</label></formula><p>which measures the risk of the model over the entire augmentation set. If we want to minimise (11) then we must minimise the expected augmentation error over the entire transformation set for each data point E t?T [l(f (t(x i )), y)]. To see how a multi-sample approach helps us do just that we use Hoeffding's inequality which provides us with a probability bound that the sum of bounded independent random variables deviates from its expected value by more than a certain amount. Let Z 1 , .., , Z na be a sequence of i.i.d random variables. Assume that E[Z] = ? and P[a ? Z i ? b] = 1 for every i. Then, by Hoeffding's inequality, for any &gt; 0, one has:</p><formula xml:id="formula_18">P 1 n a na i=1 Z i ? ? &gt; ? 2exp(?2n a 2 /(b ? a) 2 ). (12)</formula><p>We can rewrite <ref type="bibr" target="#b11">(12)</ref> in context of the previously defined augmentation loss where we replace Z 1 , .., , Z na with n a samples from T :</p><formula xml:id="formula_19">f (t 1 (x)), f (t 2 (x)), .., f (t na (x)) P ? ? 1 n a na j=1 l(f (t(x i )), y i ) ? E t?T [l(f (t(x i )), y i )] &gt; ? ? ? 2exp(?2n a 2 /b 2 ).<label>(13)</label></formula><p>As we increase n a , we converge in probability to the desired loss E t?T [l(f (t(x i )), y)] for each data point. Subsequently, we should optimise to a lower of L T meaning that the model output will fluctuate less over the augmentation set T for the used training data, and in the process make our model more generalisable. Furthermore, we can see that the probability is bounded by an exponent whose power is ? ?n a . Therefore, as we increase n a the rate of decrease for the bound also decreases, maxing the first few samples far more important than later ones. This result explains prior behaviours reported but not reasoned in past papers such as <ref type="bibr" target="#b13">[14]</ref>. When using a n a sample the computational complexity increases as O(n a ) but as there should be diminishing returns for increasing n a it should only be necessary to use n a values slightly above one. As we have shown that a multi-sample approach should offer generic performance increases for suitable T we change <ref type="bibr" target="#b7">(8)</ref> and <ref type="bibr" target="#b8">(9)</ref> to a multi-sample version. For (8) this becomes</p><formula xml:id="formula_20">L ssl = 1 b 1 n a b i=1 na j=1 l s (f (t j (x i )), y i ).<label>(14)</label></formula><p>where the index j represents repeated samples from T and again y i refers to both ground truth and pseudo-labels. In the ablation section, we perform a thorough experimental evaluation to test the theoretical predictions we have made in this section. Augmentation Implementation Similarly to other approaches we use two different augmentation strategies: one for labelled data and another for unlabelled data. However, we apply strong augmentations to both labelled and unlabelled data, unlike past approaches <ref type="bibr" target="#b11">[12]</ref> which reported divergences using this approach. For strong augmentations we make use of RandAugment <ref type="bibr" target="#b15">[16]</ref>, and CutOut augmentation <ref type="bibr" target="#b16">[17]</ref>. For completeness we list the full data transformations for labelled and unlabelled data in <ref type="table" target="#tab_0">Table I</ref> and the implementation of RandAugment and CutOut in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. IMPLEMENTATION AND EVALUATION</head><p>In this section we detail the implementation of LaplaceNet, including hyper parameter values and training schemes, and the evaluation protocol we used to measure our model's performance and compare against the current state-of-the-art.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Dataset Description</head><p>We use three image classification datasets: CIFAR-10 and CIFAR-100 <ref type="bibr" target="#b43">[44]</ref> and Mini-ImageNet <ref type="bibr" target="#b44">[45]</ref>. Following standard protocol, we evaluate our method's performance on differing amounts of labelled data for each datasets.</p><p>(a) CIFAR-10,CIFAR-100 Containing 50k training images and 10k test images, these datasets contain 10 and 100 classes respectively. The image size is small at 32 by 32 pixels. We perform experiments using 500,1k,2k and 4k labels for CIFAR-10 and 4k and 10k labels for CIFAR-100.</p><p>(b) Mini-ImageNet A subset of the popular ImageNet dataset, containing 100 classes each with 500 training and 100 test images. The resolution of the images is 84 ? 84 pixels and represents a much harder challenge than the CIFAR-10/CIFAR-100 datasets. We use 4k and 10k labels in our experiment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Implementation Details</head><p>Architectures For a fair comparison to older works we use the "13-CNN" architecture <ref type="bibr" target="#b31">[32]</ref> and for comparison to recent state-of-the-art works we use a WideResNet (WRN) 28-2 and a WRN-28-8 <ref type="bibr" target="#b45">[46]</ref> architecture. We additional use a ResNet-18 <ref type="bibr" target="#b4">[5]</ref> for Mini-Imagenet. For all models we set the drop-out rate to 0. For the "13-CNN" we add a l 2 -normalisation layer to the embedding function. Infrastructure For all experiments, we use 1-2 Nvidia P100 GPUs. Training Details: We train with stochastic gradient descent (SGD) using Nesterov momentum n m with value 0.9 and weight decay ? with value 0.0005. We use an initial learning rate of l r = 0.3 and use S = 250000 optimisation steps in total. We utilise a cosine learning rate decay such that the learning rate decays to zero after 255000 steps. We do not make use of any EM A model averaging. Parameters We list the parameter values used in <ref type="table" target="#tab_0">Table II</ref>. Most parameter values are common parameter settings from the deep learning field and are not fine-tuned to our application. Being able to work with reasonably generic parameters is well suitable to the task of SSL where using fine-tuning over validation sets is often impossible in practical applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Evaluation Protocol</head><p>We evaluate the performance of LaplaceNet on the CIFAR-10/CIFAR-100 and Mini-Imagenet datasets and compare against the current SOTA models for semi-supervised learning. For ease of comparison, we split the current SOTA into two groups.</p><p>1) Methods which used the 13-CNN architecture <ref type="bibr" target="#b31">[32]</ref>: ?-Model <ref type="bibr" target="#b29">[30]</ref>, Mean Teacher(MT) <ref type="bibr" target="#b31">[32]</ref>, Virtual Adversarial Training (VAT) <ref type="bibr" target="#b30">[31]</ref>, Label Propagation for Deep Semi-Supervised Learning (LP) <ref type="bibr" target="#b35">[36]</ref>, Smooth Neighbors on Teacher Graphs (SNTG) <ref type="bibr" target="#b46">[47]</ref>, Stochastic Weight Averaging(SWA) <ref type="bibr" target="#b47">[48]</ref>, Interpolation Consistency Training (ICT) <ref type="bibr" target="#b20">[21]</ref>, Dual Student <ref type="bibr" target="#b48">[49]</ref>, Transductive Semi-Supervised Deep Learning(TSSDL) <ref type="bibr" target="#b38">[39]</ref>, Density-Aware Graphs (DAG) <ref type="bibr" target="#b42">[43]</ref> and Pseudo-Label Mixup <ref type="bibr" target="#b19">[20]</ref>. Unfortunately, due to the natural progress in the field, each paper has different implementation choices which are not standardised. Despite this, comparisons to this group are still useful as a barometer for model performance. 2) Recent methods which used the WRN <ref type="bibr" target="#b45">[46]</ref> (Mix-Match <ref type="bibr" target="#b18">[19]</ref>, FixMatch (RandAugment variant) <ref type="bibr" target="#b11">[12]</ref> and UDA <ref type="bibr" target="#b12">[13]</ref>). To guarantee a fair comparison to these techniques, and as suggested by <ref type="bibr" target="#b49">[50]</ref>, we used a shared code-base for UDA and FixMatch which reimplemented the original baselines. Additionally we then ensured UDA and FixMatch used the same model code, the same optimiser with the same parameters, the same number of optimisation steps and the same RandAugment implementation as our approach. Evaluation Protocol For each dataset we use the official train/test partition and use the Top-1 error rate as the evaluation metric. For each result we give the mean and standard deviation over five label splits.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. RESULTS AND DISCUSSION</head><p>In this section, we discuss the experiments we performed to evaluate and compare our model against the state-of-the-art (SOTA). Additionally, we detail several ablation experiments which explore the benefits of graph-based pseudo-labels, the effect of augmentation averaging and evaluating the importance of individual components.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Comparison to SOTA</head><p>Firstly, we test our model on the less complex CIFAR-10 and CIFAR-100 datasets. In <ref type="table" target="#tab_0">Table III</ref>, we compare LaplaceNet against the first group of methods using the 13-CNN network. Our approach, by some margin, produces the best results on CIFAR-10 and CIFAR-100 and represents a new SOTA for pseudo-labels methods. We obtain a lower error rate using 500 labels than the recent work of Arazo et al <ref type="bibr" target="#b19">[20]</ref> obtain using 4000 labels. For CIFAR-100 LaplaceNet is a full 6% more accurate than any other approach and the first method to achieve an error rate below 30% on CIFAR-100 using 10k labels. In <ref type="table" target="#tab_0">Table IV</ref> we compare against the second group of methods using the WRN-28-2 network. LaplaceNet is again the best performing method, outperforming the works of UDA <ref type="bibr" target="#b12">[13]</ref> and FixMatch <ref type="bibr" target="#b11">[12]</ref>. In particular we find a significant increase in performance on the more complex CIFAR-100 dataset and beat the other considered methods by more than 3% with 10k labels.</p><p>To test the performance of LaplaceNet on a more complex dataset, we evaluate our model on the Mini-ImageNet dataset, which is a subset of the well known ImageNet dataset and in <ref type="table" target="#tab_4">Table V</ref> we compare our results against all others methods which have used this dataset. Once again, we find our method performs very well, producing an error rate a 10% and 7% better than any other method on 4k and 10k labelled images respectively. Demonstrating our approach can be applied to complex problems in the field. Additionally, we are more than 20% more accurate that the nearest graphical approach (LP).</p><p>To test the effect of increasing network size on our performance we also ran our model on CIFAR-10/100 using an WRN-28-8(26 million parameters) architecture and and compared that to the WRN-28-2(1.6 million parameters) architecture in <ref type="table" target="#tab_0">Table VI</ref>. Unsurprisingly, we achieved a large performance improvement using a WRN-28-8 on both CIFAR-10 and CIFAR-100, with an 2.87 error rate on CIFAR-10 using 4k labels and an 22.11% error rate on CIFAR-100 using 10k labels.</p><p>From these results, we underline the strength of our technique. Firstly, we show that our proposed method has a performance advantage over a range of consistency and pseudolabel based methods, including other related graphical models, on both simpler and more complex tasks. Secondly, we also showed this performance behaviour to be true over a range of model architectures. Thus demonstrating that improving the pseudo-labels that are fed-back to the neural network by exploiting classical energy models can greatly improve the final test accuracy of the network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Graph Based Pseudo-Labels</head><p>Many pseudo-label based techniques <ref type="bibr" target="#b11">[12]</ref>  <ref type="bibr" target="#b19">[20]</ref> have produced state-of-the-art results using pseudo-labels generated directly by the network rather than using an energy based approach such as label propagation on a constructed graph, which is   computationally more complex. Therefore, in this section we examine the advantage of using a graph based approach. To test the importance of graph based pseudo-labels, we created two variants of LaplaceNet, both without distribution alignment and with n a = 1.</p><p>1) The pseudo-labels are generated directly from the network predictions:</p><formula xml:id="formula_21">? i = argmax f (x i ) ? i &gt; l 2)</formula><p>The pseudo-labels are generated from the graph, as in <ref type="bibr" target="#b5">(6)</ref>,? i = argmax j F ij ? i &gt; l We then compared the Top-1 error rate of these two variants on the CIFAR-100 dataset, see <ref type="figure">Fig 3.</ref> The graph-variant greatly outperformed the direct prediction variant, emphasising the clear advantage that graphically produced pseudo-labels have. What is contributing to this advantage? As an energy-based approach, propagation on the graph incorporates information on the global structure of the data, whilst the network is making a purely local decision at each point. Arazo et al <ref type="bibr" target="#b19">[20]</ref> showed that naive network based pseudo-label approach could not generate an accurate solution for the "two moons" toy dataset, despite the fact that this problem has been solved by graphical methods for some time <ref type="bibr" target="#b24">[25]</ref>. Thus demonstrating that purely local decisions are detrimental to accuracy when the global structure of data isn't taken into account.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Augmentation Averaging</head><p>In this paper we justify a multiple augmentation approach to further improve semi-supervised models. In this section, we present the experimental verification of our theoretical predictions about augmentation averaging as well as comparing its effect to potential alternative techniques. To test the effect of augmentation averaging we ran our approach on the CIFAR-100 dataset using the 13-CNN network for a range of values n a = <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b4">5]</ref>. Additionally we compared the changed caused by augmentation averaging to the more common approaches of scaling the batch size b and labelled batch size b l by <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b4">5]</ref> and scaling the number of optimisation steps S by <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b4">5]</ref> (a) 4k labels (b) 10k labels <ref type="figure">Fig. 3</ref>. Experimental comparison of the effect of using pseudo-labels produced in a graphical framework versus pseudo-labels generated by the neural network on the Top-1 error rate on the CIFAR-100 dataset ((a) 4k and (b) 10k labelled images) with the 13-CNN network. Using graphically produced pseudo-labels we achieve a much higher accuracy than using the network predictions.</p><p>To quantify the effect of a given change we use two measures: the augmentation invariance of the classifier, which we define in this paper, and Top-1 error. Augmentation invariance measures the extent to which the classifier's performance changes under data augmentation. Given an augmentation function u : X ? X and a classifier f ? the augmentation invariance V with respect to a dataset Z made up of n point-label pairs Z = {x i , y i } n i=1 is given by</p><formula xml:id="formula_22">V Z = 1 n n i=1 1 arg max f ? (u(xi))=yi 1 n n i=1 1 arg max f ? (xi)=yi ,<label>(15)</label></formula><p>which can be viewed as the performance ratio with and without data augmentation. We consider both the augmentation invariance of our model with respect to the fully labelled training and test data in order to give a full picture of the model's invariance, but we still only use a subset of the labelled data for training. In <ref type="figure">Fig 4</ref> we present our findings. We found that naively scaling the number of optimisation steps without changing the hyperparameters led to the model diverging as we spent too many epochs at a high learning rate. Therefore, we provide results for the other two considered techniques which can be directly compared. As theorised in Section III we find that increasing the number of augmentation samples decreased the sensitivity of the model's predictions to augmentation on both the training and test data.An almost identical effect was found by scaling the batch size. However, the major difference between the two is their effect on Top-1 error rate. We found scaling the batch size offered no improvement to Top-1 error, in-fact the largest batch size offered the worst outcome, whilst increasing the number of augmentation samples noticeably improved the model's accuracy. Additionally as theorised in Section III, we see that the gain in performance from n a = 1 ? 3 is much greater than n a = 3 ? 5, supporting our statements regarding the exponential bound in probability.</p><p>To further test the limits of augmentation averaging we ran additional experiments with n a = <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b9">10]</ref> with 4k labels on the CIFAR-10 dataset and present these results in <ref type="table" target="#tab_0">Table VII</ref>. We see that increasing n a above five led to small improvements Together these results suggests that scaling the number of augmentation samples could be a great option for semisupervised models using suitable strong augmentations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Component Evaluation</head><p>As LaplaceNet combines several different techniques, we tested the importance of strong augmentation, distribution alignment and MixUp to the overall accuracy of the model. We created a baseline model (n a = 1) and then remove each component one at a time and tested the performance on the CIFAR-100 dataset, see <ref type="table" target="#tab_0">Table VIII</ref>. Whilst the removal of each component decreased the performance of the model, it is clear the most crucial component to model performance is strong augmentation and removing it drastically reduces model accuracy. However, unlike other works <ref type="bibr" target="#b19">[20]</ref> we find that <ref type="figure">Fig. 4</ref>. A comparison on the effect of increasing batch size versus increasing the number of augmentation samples on Top-1 error rate, test data augmentation invariance and training data augmentation invariance for the CIFAR-100 dataset. Increasing the amount of augmentation averaging decreased the error rate whilst also decreasing the sensitivity of the model's output predictions to augmented data. Increasing the batch size had a similar effect on the model's sensitivity, but it offered no improvement to model accuracy.</p><p>whilst MixUp <ref type="bibr" target="#b21">[22]</ref> offers a small advantage is it not critical for composite batch pseudo-label approaches. This may be due to the advantages of graph-based approaches overcoming the flaws of naive neural network predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSION</head><p>We propose a new graph based pseudo-label approach for semi-supervised image classification, LaplaceNet, that outperforms the current state-of-the-art on several datasets whilst having a much lower model complexity. Our model utilises a simple single term loss function without the need for additionally complexity whilst additionally avoiding the need for confidence thresholding or temperature sharpening which was thought to be essential for state-of-the-art performance. We instead generate accurate pseudo-labels through a graph based technique with distribution alignment. We also explore the role that augmentation plays in semi-supervised learning and justify a multi-sampling approach to augmentation which we demonstrate through rigorous experimentation improves both the generalisation of the network as well as the model's sensitivity to augmented data. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX A AUGMENTATION POOL</head><p>In this work we use RandAugment <ref type="bibr" target="#b15">[16]</ref> rather than a learnt augmentation strategy such as AutoAugment <ref type="bibr" target="#b14">[15]</ref> which has a large computational cost. In <ref type="table" target="#tab_7">Table X</ref> we detail the augmentation pool used. Additionally, we apply CutOut <ref type="bibr" target="#b16">[17]</ref> augmentation after RandAugment sampling.</p><p>We use two different augmentation strategies in our work: one for labelled data and one for unlabelled data. We use "strong" augmentations, RandAugment and CutOut, on both labelled and unlabelled data with the only difference being that we sample once from RandAugment for labelled data and twice for unlabelled data. Given a pre-selected list of transformations, RandAugment randomly samples from the list with each transformation having a magnitude parameter. Rather than optimising this parameter on a validation set, which may not exist in semi-supervised applications, we sample a random magnitude from a pre-set range. This is same as is done in FixMatch <ref type="bibr" target="#b11">[12]</ref> and UDA <ref type="bibr" target="#b12">[13]</ref>. We list the transformations for RandAugment and the implementation of CutOut in <ref type="table" target="#tab_7">Table X</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX B COMPUTATIONAL TIME</head><p>To give clarity on the how long our code takes to run we provide the computational run times of LaplaceNet on the CIFAR-100 dataset using the 13-CNN model for a variety of settings, see <ref type="table" target="#tab_0">Table IX</ref>. Each experiment was run on one P100 NVIDIA GPU. From <ref type="table" target="#tab_0">Table IX</ref>, we see that the time increased caused by increasing the batch size or increasing the number of samples is very similar. Component-wise, removing strong augmentation gives the largest decrease in computational time whilst removing the graphical propogation saved just over an hour on CIFAR-100. This represent a very small time trade off given the advantages present in using graphical pseudo-labels. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>(a) Fully labelled data (b) Supervised learning (c) Network pseudo-labels (d) Graphical pseudo-labels</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>The advantage of graphical pseudo-labels on the toy Two Moon dataset (400 samples).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>R</head><label></label><figDesc>= D/D u Clip values for smooth deformation: 9: R[R &gt; 1.01] = 1.01 and R[R &lt; 0.99] = 0.99 # Deform the current predictions: 10:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>7 :</head><label>7</label><figDesc>untrained model f with trainable parameters ? and embedding function z. Hyper-parameters: Number of optimisation steps S # Initialisation: 2: for i = 1 to 100 do 3: optimise L sup over Z l 4: end for 5: Set current step to zero s i = 0 # Main Optimisation Process: 6: while s i &lt; S do Extract features: V = {z(x i )}</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I THE</head><label>I</label><figDesc>AUGMENTATION TRANSFORMATIONS USED FOR LABELLED AND UNLABELLED DATA. FOR NORMALISATION WE USE THE OFFICIAL CHANNEL</figDesc><table><row><cell>LABELLED TRANSFORM</cell><cell>UNLABELLED TRANSFORM</cell></row><row><cell cols="2">Random Horizontal Flip</cell></row><row><cell cols="2">Random Crop and Pad</cell></row><row><cell>RandAugment Sample</cell><cell>RandAugment Sample</cell></row><row><cell>-</cell><cell>RandAugment Sample</cell></row><row><cell cols="2">CutOut</cell></row><row><cell cols="2">Normalisation</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II LIST</head><label>II</label><figDesc>OF HYPERPARAMETERS USED IN THE PAPER ACROSS THE CIFAR-10/100 AND MINI-IMAGENET DATASETS.</figDesc><table><row><cell>PARAMETER</cell><cell>CIFAR-10</cell><cell>CIFAR-100</cell><cell>MINI-IMAGENET</cell></row><row><cell>?</cell><cell>1.0</cell><cell>0.5</cell><cell>0.5</cell></row><row><cell>?</cell><cell>0.01</cell><cell>0.01</cell><cell>0.01</cell></row><row><cell>k</cell><cell>50</cell><cell>50</cell><cell>50</cell></row><row><cell>S</cell><cell>2.5 ? 10 5</cell><cell>2.5 ? 10 5</cell><cell>2.5 ? 10 5</cell></row><row><cell>b</cell><cell>300</cell><cell>100</cell><cell>100</cell></row><row><cell>b l</cell><cell>48</cell><cell>50</cell><cell>50</cell></row><row><cell>lr</cell><cell>0.03</cell><cell>0.03</cell><cell>0.1</cell></row><row><cell>nm</cell><cell>0.9</cell><cell>0.9</cell><cell>0.9</cell></row><row><cell>?</cell><cell>5 ? 10 ?4</cell><cell>5 ? 10 ?4</cell><cell>5 ? 10 ?4</cell></row><row><cell>na</cell><cell>3</cell><cell>3</cell><cell>3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE III TOP</head><label>III</label><figDesc>-1 ERROR RATE ON THE CIFAR-10/100 DATASETS FOR OUR METHOD AND OTHER METHODS USING THE 13-CNN ARCHITECTURE. WE DENOTE WITH ? ? 0.89 26.60 ? 0.22 19.53 ? 0.12 14.02 ? 0.10 53.10 ? 0.34 36.59 ? 0.47</figDesc><table><row><cell></cell><cell></cell><cell cols="3">EXPERIMENTS WE HAVE RAN.</cell><cell></cell><cell></cell></row><row><cell>DATASET</cell><cell></cell><cell cols="2">CIFAR-10</cell><cell></cell><cell cols="2">CIFAR-100</cell></row><row><cell>METHOD</cell><cell>500</cell><cell>1000</cell><cell>2000</cell><cell>4000</cell><cell>4000</cell><cell>10000</cell></row><row><cell>Supervised Baseline</cell><cell cols="4">37.12 CONSISTENCY BASED APPROACHES</cell><cell></cell><cell></cell></row><row><cell>?-Model</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>12.36 ? 0.31</cell><cell>-</cell><cell>39.19 ? 0.36</cell></row><row><cell>MT ?</cell><cell cols="4">27.45 ? 2.64 21.55 ? 1.48 15.73 ? 0.31 12.31 ? 0.20</cell><cell cols="2">45.36 ? 0.49 36.08 ? 0.51</cell></row><row><cell>VAT</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>11.36 ? 0.34</cell><cell>-</cell><cell>-</cell></row><row><cell>MT-LP</cell><cell cols="4">24.02 ? 2.44 16.93 ? 0.70 13.22 ? 0.29 10.61 ? 0.28</cell><cell cols="2">43.73 ? 0.20 35.92 ? 0.47</cell></row><row><cell>SNTG</cell><cell>-</cell><cell>18.41?0.52</cell><cell>13.64?0.32</cell><cell>9.89?0.34</cell><cell>-</cell><cell>37.97?0.29</cell></row><row><cell>MT-fast-SWA</cell><cell>-</cell><cell cols="2">15.58 ? 0.12 11.02 ? 0.12</cell><cell>9.05 ? 0.21</cell><cell>-</cell><cell>34.10 ? 0.31</cell></row><row><cell>MT-ICT</cell><cell>-</cell><cell>15.48 ? 0.78</cell><cell>9.26 ? 0.09</cell><cell>7.29 ? 0.02</cell><cell>-</cell><cell>-</cell></row><row><cell>Dual Student</cell><cell>-</cell><cell>14.17?0.38</cell><cell>10.72?0.19</cell><cell>8.89?0.09</cell><cell>-</cell><cell>32.77?0.24</cell></row><row><cell></cell><cell></cell><cell cols="3">PSEUDO-LABELLING APPROACHES</cell><cell></cell><cell></cell></row><row><cell>TSSDL ?</cell><cell>-</cell><cell cols="3">21.13 ? 1.17 14.65 ? 0.33 10.90 ? 0.23</cell><cell>-</cell><cell>-</cell></row><row><cell>LP ?</cell><cell cols="4">32.40 ? 1.80 22.02 ? 0.88 15.66 ? 0.35 12.69 ? 0.29</cell><cell cols="2">46.20 ? 0.76 38.43 ? 1.88</cell></row><row><cell>DAG</cell><cell>9.30 ? 0.73</cell><cell>7.42 ? 0.41</cell><cell>7.16 ? 0.38</cell><cell>6.13 ? 0.15</cell><cell cols="2">37.38 ? 0.64 32.50 ? 0.21</cell></row><row><cell>Pseudo-Label Mixup</cell><cell>8.80 ? 0.45</cell><cell>6.85 ? 0.15</cell><cell>-</cell><cell>5.97 ? 0.15</cell><cell cols="2">37.55 ? 1.09 32.15 ? 0.50</cell></row><row><cell>LaplaceNet  ?</cell><cell>5.68 ? 0.08</cell><cell>5.33 ? 0.02</cell><cell>4.99 ? 0.12</cell><cell>4.64 ? 0.07</cell><cell cols="2">31.64 ? 0.02 26.60 ? 0.23</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE IV TOP</head><label>IV</label><figDesc>-1 ERROR RATE FOR CIFAR-10/100. ALL METHODS, EXCEPT MIXMATCH, ARE TESTED USING THE SAME CODE-BASE AND USE THE SAME MODEL CODE, THE SAME OPTIMISER (SGD) WITH THE SAME OPTIMISATION PARAMETERS, THE SAME NUMBER OF OPTIMISATION STEPS AND THE SAME RANDAUGMENT IMPLEMENTATION. WE DENOTE WITH ? EXPERIMENTS WE HAVE RAN. ? 0.74 5.61 ? 0.16 5.40 ? 0.19 36.19 ? 0.39 31.49 ? 0.19 FixMatch(RA) ? 5.92 ? 0.11 5.42 ? 0.11 5.30 ? 0.08 34.87 ? 0.17 30.89 ? 0.18 LaplaceNet ? 5.57 ? 0.60 4.71 ? 0.05 4.35 ? 0.10 33.16 ? 0.22 27.49 ? 0.22</figDesc><table><row><cell>DATASET</cell><cell></cell><cell>CIFAR-10</cell><cell></cell><cell cols="2">CIFAR-100</cell></row><row><cell>METHOD</cell><cell>500</cell><cell>2000</cell><cell>4000</cell><cell>4000</cell><cell>10000</cell></row><row><cell>MixMatch</cell><cell cols="3">9.65 ? 0.94 7.03 ? 0.15 6.34 ? 0.06</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell cols="2">SAME CODEBASE</cell><cell></cell><cell></cell></row><row><cell>UDA  ?</cell><cell>6.88</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE V TOP</head><label>V</label><figDesc>-1 ERROR RATE FOR MINI-IMAGENET. WE COMPARE AGAINST METHODS WHICH HAVE USED AN IDENTICAL RESNET-18 ARCHITECTURE.</figDesc><table><row><cell></cell><cell>METHOD</cell><cell></cell><cell>4000</cell><cell cols="2">10000</cell></row><row><cell cols="2">Supervised Baseline</cell><cell></cell><cell cols="3">66.04 ? 0.32 52.89 ? 0.33</cell></row><row><cell></cell><cell cols="4">Consistency Regularisation Methods</cell></row><row><cell>MT</cell><cell></cell><cell></cell><cell cols="3">72.51 ? 0.22 57.55 ? 1.11</cell></row><row><cell cols="2">MT-LP</cell><cell></cell><cell cols="3">72.78 ? 0.15 57.35 ? 1.66</cell></row><row><cell></cell><cell cols="4">Pseudo-Label Methods</cell></row><row><cell>LP</cell><cell></cell><cell></cell><cell cols="3">70.29 ? 0.81 57.58 ? 1.47</cell></row><row><cell cols="3">Pseudo-Label Mixup</cell><cell cols="3">56.49 ? 0.51 46.08 ? 0.11</cell></row><row><cell cols="2">LaplaceNet</cell><cell></cell><cell cols="3">46.32 ? 0.27 39.43 ? 0.09</cell></row><row><cell></cell><cell></cell><cell cols="2">TABLE VI</cell><cell></cell></row><row><cell cols="6">THE EFFECT ON TOP-1 ERROR RATE BY SCALING UP THE NEURAL</cell></row><row><cell cols="6">NETWORK IN SIZE FROM A WRN-28-2 TO A WRN-28-8 ON THE</cell></row><row><cell></cell><cell cols="4">CIFAR-10/100 DATASETS.</cell></row><row><cell>DATASET</cell><cell cols="2">CIFAR-10</cell><cell></cell><cell cols="2">CIFAR-100</cell></row><row><cell>MODEL</cell><cell>500</cell><cell cols="2">4000</cell><cell>4000</cell><cell>10000</cell></row><row><cell>WRN-28-2</cell><cell>5.57 ? 0.60</cell><cell cols="2">4.35 ? 0.10</cell><cell>33.16 ? 0.22</cell><cell>27.49 ? 0.22</cell></row><row><cell>WRN-28-8</cell><cell cols="3">3.81 ? 0.37 2.87 ? 0.18</cell><cell cols="2">26.61 ? 0.10 22.11 ? 0.23</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE VII THE</head><label>VII</label><figDesc>EFFECT OF FURTHER INCREASING THE NUMBER OF AUGMENTATION SAMPLES FOR EACH IMAGE ON THE CIFAR-100 DATASET USING THE 13-CNN NETWORK AND 4K LABELLED IMAGES. Error and minimal changes in test data augmentation invariance, agreeing with the prior theoretical predictions.</figDesc><table><row><cell>DATASET</cell><cell></cell><cell>CIFAR-100</cell></row><row><cell>na</cell><cell>Top-1 Error</cell><cell cols="2">Test-Data Aug-Invariance</cell></row><row><cell>na = 1</cell><cell>32.44 ? 0.26</cell><cell cols="2">93.97 ? 0.01</cell></row><row><cell>na = 3</cell><cell>31.45 ? 0.18</cell><cell cols="2">94.84 ? 0.01</cell></row><row><cell>na = 5</cell><cell>31.15 ? 0.38</cell><cell cols="2">95.11 ? 0.04</cell></row><row><cell>na = 7</cell><cell>31.02 ? 0.28</cell><cell cols="2">95.09 ? 0.03</cell></row><row><cell>na = 10</cell><cell>30.95 ? 0.25</cell><cell cols="2">95.18 ? 0.06</cell></row><row><cell></cell><cell cols="2">TABLE VIII</cell></row><row><cell cols="4">THE EFFECT OF REMOVING INDIVIDUAL COMPONENTS FROM THE</cell></row><row><cell cols="4">BASELINE MODEL ON TOP-1 ERROR RATE FOR CIFAR-100 ON THE</cell></row><row><cell></cell><cell cols="2">13-CNN NETWORK.</cell></row><row><cell>DATASET</cell><cell></cell><cell>CIFAR-100</cell></row><row><cell>MODEL</cell><cell></cell><cell>4k</cell><cell>10k</cell></row><row><cell>Baseline</cell><cell></cell><cell cols="2">32.41 ? 0.25 27.37 ? 0.20</cell></row><row><cell cols="2">COMPONENT REMOVED</cell><cell>4k</cell><cell>10k</cell></row><row><cell>RandAugment</cell><cell></cell><cell cols="2">44.43 ? 0.66 34.75 ? 0.23</cell></row><row><cell cols="2">Distribution Alignment</cell><cell cols="2">33.26 ? 0.24 29.07 ? 0.07</cell></row><row><cell>MixUp</cell><cell></cell><cell cols="2">33.74 ? 0.26 28.02 ? 0.20</cell></row><row><cell>in Top-1</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE IX COMPUTATIONAL</head><label>IX</label><figDesc>TIME TAKEN FOR OUR APPROACH USING 4K LABELLED IMAGES ON THE CIFAR-100 DATASET USING THE 13-CNN ARCHITECTURE. WE PROVIDE THE TIME TAKEN FOR A NUMBER OF DIFFERENTSETTINGS USED IN THE RESULTS SECTION. ALL EXPERIMENTS WERE PERFORMED USING ONE NVIDIA P100 GPU. NPL) for supporting this work. AIAR gratefully acknowledges the financial support of the CMIH and CCIMI University of Cambridge. CBS acknowledges support from the Philip Leverhulme Prize, the Royal Society Wolfson Fellowship, the EPSRC grants EP/S026045/1 and EP/T003553/1, EP/N014588/1, EP/T017961/1, the Wellcome Innovator Award RG98755, the Leverhulme Trust project Unveiling the invisible, the European Union Horizon 2020 research and innovation programme under the Marie Skodowska-Curie grant agreement No. 777826 NoMADS, the Cantab Capital Institute for the Mathematics of Information and the Alan Turing Institute.</figDesc><table><row><cell>MODEL</cell><cell>COMPUTATIONAL TIME (HOURS)</cell></row><row><cell>Baseline</cell><cell>7.52 ? 0.04</cell></row><row><cell cols="2">COMPONENT REMOVAL</cell></row><row><cell>No Distribution Alignment</cell><cell>6.18 ? 0.01</cell></row><row><cell>No Strong Augmentation</cell><cell>5.84 ? 0.03</cell></row><row><cell>No Graphical Propogation</cell><cell>6.32 ? 0.01</cell></row><row><cell cols="2">MODEL SCALING</cell></row><row><cell>3?-Batch-size</cell><cell>12.28 ? 0.03</cell></row><row><cell>5?-Batch-size</cell><cell>17.23 ? 0.06</cell></row><row><cell>3?-Samples</cell><cell>12.88 ? 0.01</cell></row><row><cell>5?-Samples</cell><cell>18.14 ? 0.11</cell></row><row><cell>oratory (</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE X LIST</head><label>X</label><figDesc>OF TRANSFORMATIONS USED IN OUR APPLICATION OF RANDAUGMENT AS WELL THEIR DESCRIPTION AND MAGNITUDE RANGE. ADDITIONALLY, WE LIST THE CUTOUT TRANSFORMATION USED AT THE END OF RANDAUGMENT SAMPLING. TRANSFORMATION DESCRIPTION RANGE RANDAUGMENT TRANSFORMATIONS Autocontrast Maximises the image contrast by setting the darkest (lightest) pixel to black (white) --Brightness Adjusts the brightness of the image. where B = 0 returns a black image. image B ? [0.05, 0.95] Color Adjusts the colour balance of the image. C l = 0 returns a black and white image. C l ? [0.05, 0.95] Contrast Controls the contrast of the image. C o = 0 returns a gray image. C o ? [0.05, 0.95]</figDesc><table><row><cell>Equalise</cell><cell>Equalises the image histogram.</cell><cell>--</cell></row><row><cell>Identity</cell><cell>Returns the original image.</cell><cell>--</cell></row><row><cell>Posterise</cell><cell>Reduces each pixel to B bits.</cell><cell>B ? [4, 8]</cell></row><row><cell>Rotate</cell><cell>Rotates the image by ? degrees.</cell><cell>? ? [?30, 30]</cell></row><row><cell>Sharpness</cell><cell>Adjusts the sharpness of the image, where S = 0 returns a blurred image</cell><cell>S ? [0.05, 0.95]</cell></row><row><cell>Shear X</cell><cell>Shears the image along the horizontal axis with rate R.</cell><cell>R ? [?0.3, 0.3]</cell></row><row><cell>Shear Y</cell><cell>Shears the image along the vertical axis with rate R</cell><cell>R ? [?0.3, 0.3]</cell></row><row><cell>Solarize</cell><cell>Inverts all pixels above a threshold value of T</cell><cell>T ? [0, 1]</cell></row><row><cell>Translate X</cell><cell>Translates the image horizontally by (??image width) pixels.</cell><cell>? ? [?0.3, 0.3]</cell></row><row><cell>Translate Y</cell><cell>Translates the image vertically by (??image height) pixels</cell><cell>? ? [?0.3, 0.3]</cell></row><row><cell></cell><cell>CUTOUT AUGMENTATION</cell><cell></cell></row><row><cell>CutOut</cell><cell>Sets a random square patch of side-length (L?image width) pixels to grey</cell><cell>L ? [0, 0.5]</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>ACKNOWLEDGMENT PS thanks the UK Engineering and Physical Sciences Research Council (EPSRC) and the National Physical Lab-</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7132" to="7141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Residual attention network for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3156" to="3164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1440" to="1448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention (MICCAI)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="834" to="848" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Fixmatch: Simplifying semisupervised learning with consistency and confidence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Raffel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Unsupervised data augmentation for consistency training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-T</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Remixmatch: Semi-supervised learning with distribution matching and augmentation anchoring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Raffel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eighth International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Autoaugment: Learning augmentation policies from data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.09501</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Randaugment: Practical automated data augmentation with a reduced search space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="702" to="703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Improved regularization of convolutional neural networks with cutout</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Devries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.04552</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Simple: Similar pseudo label exploitation for semi-supervised classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Mixmatch: A holistic approach to semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Pseudo-labeling and confirmation bias in deep semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Arazo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ortego</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Albert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">E</forename><surname>O&amp;apos;connor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mcguinness</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.02983</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Interpolation consistency training for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lamb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kannala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lopez-Paz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Artificial Intelligence (IJCAI)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.09412</idno>
		<title level="m">mixup: Beyond empirical risk minimization</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Manifold regularization: A geometric framework for learning from labeled and unlabeled examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Belkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Niyogi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sindhwani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Regularization and semisupervised learning on large graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Belkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Matveeva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Niyogi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computational Learning Theory</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page" from="624" to="638" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Semi-supervised learning using gaussian fields and harmonic functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">P International conference on Machine learning (ICML)</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="912" to="919" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning with local and global consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Bousquet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Lal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sch?lkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="321" to="328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Semi-supervised learning by entropy minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Grandvalet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="529" to="536" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Semi-supervised regression using hessian energy with an application to semi-supervised dimensionality reduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">I</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Steinke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="979" to="987" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Semisupervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Chapelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sch?lkopf</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Temporal ensembling for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Virtual adversarial training: a regularization method for supervised and semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maeda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ishii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="1979" to="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tarvainen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Valpola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1195" to="1204" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Alphamatch: Improving consistency for semi-supervised learning with alpha-divergence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="13" to="683" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Muscle: strengthening semi-supervised learning via concurrent unsupervised learning using mutual information maximization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Hussein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Galstyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Abd-Almageed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="2586" to="2595" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Semi-supervised learning of visual features by nonparametrically predicting view assignments with support samples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Assran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rabbat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="8443" to="8452" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Label propagation for deep semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Iscen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tolias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Avrithis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Chum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Pseudo-label: The simple and efficient semi-supervised learning method for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Challenges in Representation Learning, ICML</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Simple: Similar pseudo label exploitation for semi-supervised classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="15" to="099" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Transductive semi-supervised deep learning using min-max features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">Maxiaoyu</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="299" to="315" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Flexmatch: Boosting semi-supervised learning with curriculum pseudo labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Okumura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Shinozaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">On mixup training: Improved calibration and predictive uncertainty for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thulasidasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chennupati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bilmes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Bhattacharya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Michalak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">33rd Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Beyond supervised classification: Extreme minimal supervision with the graph 1-laplacian</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">I</forename><surname>Aviles-Rivero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Papadakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Alsaleh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">T</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-B</forename><surname>Schonlieb</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.08635</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Density-aware graph for deep semi-supervised visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="400" to="413" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Matching networks for one shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">E</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Wide residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the British Machine Vision Conference</title>
		<meeting>the British Machine Vision Conference</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Smooth neighbors on teacher graphs for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8896" to="8905" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">There are many consistent explanations of unlabeled data: Why you should average</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Athiwaratkun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Finzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Izmailov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Wilson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Dual student: Breaking the limits of the teacher in semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">W</forename><surname>Lau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Realistic evaluation of deep semi-supervised learning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Odena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
