<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Hi-Fi: Hierarchical Feature Integration for Skeleton Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2018">2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Zhao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">College of Computer and Control Engineering</orgName>
								<orgName type="institution">Nankai University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Shen</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Key Laboratory of Specialty Fiber Optics and Optical Access Networks</orgName>
								<orgName type="institution">Shanghai University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanghua</forename><surname>Gao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">College of Computer and Control Engineering</orgName>
								<orgName type="institution">Nankai University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dandan</forename><surname>Li</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Key Laboratory of Specialty Fiber Optics and Optical Access Networks</orgName>
								<orgName type="institution">Shanghai University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Cheng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">College of Computer and Control Engineering</orgName>
								<orgName type="institution">Nankai University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Hi-Fi: Hierarchical Feature Integration for Skeleton Detection</title>
					</analytic>
					<monogr>
						<title level="m">International Joint Conference on Artificial Intelligence</title>
						<imprint>
							<biblScope unit="volume">IJCAI</biblScope>
							<date type="published" when="2018">2018</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T03:12+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In natural images, the scales (thickness) of object skeletons may dramatically vary among objects and object parts, making object skeleton detection a challenging problem. We present a new convolutional neural network (CNN) architecture by introducing a novel hierarchical feature integration mechanism, named Hi-Fi, to address the skeleton detection problem. The proposed CNNbased approach has a powerful multi-scale feature integration ability that intrinsically captures highlevel semantics from deeper layers as well as lowlevel details from shallower layers. By hierarchically integrating different CNN feature levels with bidirectional guidance, our approach (1) enables mutual refinement across features of different levels, and (2) possesses the strong ability to capture both rich object context and high-resolution details. Experimental results show that our method significantly outperforms the state-of-the-art methods in terms of effectively fusing features from very different scales, as evidenced by a considerable performance improvement on several benchmarks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Object skeletons are defined as the medial axis of foreground objects surrounded by closed boundaries <ref type="bibr" target="#b0">[Blum, 1967]</ref>. Complementary to object boundaries, skeletons are shape-based descriptors which provide a compact representation of both object geometry and topology. Due to its wide applications in other vision tasks such as shape-based image retrieval <ref type="bibr" target="#b1">[Demirci et al., 2006;</ref><ref type="bibr" target="#b8">Sebastian et al., 2004]</ref>, and human pose estimation <ref type="bibr" target="#b2">[Girshick et al., 2011;</ref><ref type="bibr">Shotton et al., 2013;</ref><ref type="bibr" target="#b9">Sun et al., 2012]</ref>. Skeleton detection is extensively studied very recently <ref type="bibr" target="#b8">[Shen et al., 2017;</ref><ref type="bibr" target="#b6">Ke et al., 2017;</ref><ref type="bibr" target="#b9">Tsogkas and Dickinson, 2017]</ref>.</p><p>Because the skeleton scales (thickness) are unknown and may vary among objects and object parts, skeleton detection has to deal with a more challenging scale space problem <ref type="bibr" target="#b8">[Shen et al., 2016b]</ref>  as shown in <ref type="figure" target="#fig_0">Fig. 1</ref>. Consequently, it requires the detector to capture broader context for detecting potential large-scale (thick) skeletons, and also possess the ability to focus on local details in case of small-scale (thin) skeletons.</p><p>Performing multi-level feature fusion has been a primary trend in pixel-wise dense prediction such as skeleton detection <ref type="bibr" target="#b6">[Ke et al., 2017]</ref> and saliency detection <ref type="bibr" target="#b3">Hou et al., 2018]</ref>. These methods fuse CNN features of different levels in order to obtain more powerful representations. The disadvantage of existing feature fusion methods is that they perform only deep-to-shallow refinement, which provides shallow layers the ability of perceiving high-level concepts such as object and image background. Deeper CNN features in these methods still suffer from low-resolution, which is a bottleneck to the final detection results.</p><p>In this paper we introduce hierarchical feature integration (Hi-Fi) mechanism with bidirectional guidance. Different from existing feature fusing solutions, we explicitly enable both deep-to-shallow and shallow-to-deep refinement to enhance shallower features with richer semantics, and enrich deeper features with higher resolution information. Our architecture has two major advantages compared with existing alternatives:</p><p>Bidirectional Mutual Refinement. Different from existing solutions illustrated in <ref type="figure">Fig. 2</ref> (a) and <ref type="figure">Fig. 2</ref>  <ref type="figure">Figure 2</ref>: Different multi-scale CNN feature fusing methods: (a) side-outputs as independent detectors at different scales <ref type="bibr" target="#b9">[Xie and Tu, 2015;</ref><ref type="bibr" target="#b8">Shen et al., 2016b]</ref>; (b) deep-to-shallow refinement <ref type="bibr" target="#b6">[Ke et al., 2017]</ref> that brings high-level semantics to lower layers; (c) directly fuse all feature levels at once; (d) our hierarchical integration architecture, which enables bidirectional mutual refinement across low/high level features by recursive feature integration. ferent feature levels work independently or allow only deepto-shallow guidance, our method <ref type="figure">Fig. 2 (d)</ref> enables not only deep-to-shallow but also shallow-to-deep refinement, which allows the high-level semantics and low-level details to mutually help each other in a bidirectional fashion.</p><formula xml:id="formula_0">(b) where dif- (a) (b) (c) (d)</formula><p>Hierarchical Integration. There are two alternatives for mutual integration: directly fusing all levels of features as shown in <ref type="figure">Fig. 2</ref> (c), or hierarchically integrating them as shown in <ref type="figure">Fig. 2 (d)</ref>. Due to the significant difference between faraway feature levels, directly fusing all of them might be very difficult. We take the second approach as inspired by the philosophy of ResNet <ref type="bibr">[He et al., 2016]</ref>, which decomposes the difficult problem into much easier sub-problems: identical mapping and residual learning. We decompose the feature integration into easier sub-problems: gradually combining nearby feature levels. Because optimizing combinations of close features is more practical and easier to converge due to their high similarity. The advantage of hierarchical integration over directly fusing all feature levels is verified in the experiments ( <ref type="figure">Fig. 9</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Skeleton Detection. Numerous models have been proposed for skeleton detection in the past decades. In general, they can be divided into three categories: (a) early image processing based methods, these methods localize object skeleton based on the geometric relationship between object skeletons and boundaries; (b) learning based methods, by designing handcrafted image features, these methods train a machine learning model to distinguish skeleton pixels from non-skeleton pixels; (c) recent CNN-based methods which design CNN architectures for skeleton detection. Most of the early image processing based methods <ref type="bibr" target="#b8">[Morse et al., 1993;</ref><ref type="bibr" target="#b4">Jang and Hong, 2001</ref>] rely on the hypothesis that skeletons lie in the middle of two parallel boundaries. A boundary response map is first calculated (mostly based on image gradient), then skeleton pixels can be localized with the geometric relationship between skeletons and boundaries. Some researchers then investigate learning-based models for skeleton detection. They train a classifier <ref type="bibr">[Tsogkas and Kokkinos, 2012]</ref> or regressor <ref type="bibr">[Sironi et al., 2014]</ref> with hand-crafted features to determine whether a pixel touches the skeleton. Boundary response is very sensitive to texture and illumination changes, therefore image processing based methods can only deal with images with simple back-grounds. Limited by the ability of traditional learning models and representation capacity of hand-crafted features, they cannot handle objects with complex shapes and various skeleton scales.</p><p>More recently many researchers have been exploiting the powerful convolutional neural networks (CNNs) for skeleton detection and significant improvements have been achieved on several benchmarks. HED <ref type="bibr" target="#b9">[Xie and Tu, 2015]</ref> introduces side-output that is branched from intermediate CNN layers for multi-scale edge detection. FSDS <ref type="bibr" target="#b8">[Shen et al., 2016b]</ref> then extends side-output to be scale-associated side-output, in order to tackle the scale-unknown problem in skeleton detection. The side-output residual network (SRN) <ref type="bibr" target="#b6">[Ke et al., 2017]</ref> exploits deep-to-shallow residual connections to bring high-level, rich semantic features to shallower side-outputs with the purpose of making the shallower side-outputs more powerful to distinguish real object skeletons from local reflective structures.</p><p>Multi-Scale Feature Fusing in CNNs. CNNs naturally learn low/mid/high level features in a shallow to deep layer fashion. Low-level features focus more on local detailed structures, while high-level features are rich in conceptual semantics <ref type="bibr" target="#b9">[Zeiler and Fergus, 2014]</ref>. Pixel-wise dense prediction tasks such as skeleton detection, boundary detection and saliency detection require not only high-level semantics but also high-resolution predictions. As pooling layers with strides down-sample the feature maps, deeper CNN features with richer semantics are always with lower resolution.</p><p>Many researchers <ref type="bibr" target="#b6">[Ke et al., 2017;</ref><ref type="bibr" target="#b3">Hou et al., 2018]</ref> try to fuse deeper rich semantic CNN features with shallower high-resolution features to overcome this semantic vs resolution conflict. In SRN, <ref type="bibr" target="#b6">Ke et. al. [2017]</ref> connected shallower side-outputs with deeper ones to refine the shallower side-outputs. As a result, the shallower sideoutputs become much cleaner because they are capable of suppressing non-object textures and disturbances. Shallower side-outputs of methods without deep-to-shallow refinement such as HED <ref type="bibr" target="#b9">[Xie and Tu, 2015]</ref> and FSDS <ref type="bibr" target="#b8">[Shen et al., 2016b]</ref> are filled with noises.</p><p>A similar strategy has been exploited in DSS <ref type="bibr" target="#b3">[Hou et al., 2018]</ref> and Amulet  for saliency detection, and a schema of these methods with deep-to-shallow refinement can be summarized as <ref type="figure">Fig. 2 (b)</ref>. The problem of these feature fusion methods is that they lack the shallowto-deep refinement, the deeper side-outputs still suffer from low-resolution. For example, DSS has to empirically drop the last side-output for its low-resolution. 3 Hi-Fi: Hierarchical Feature Integration</p><formula xml:id="formula_1">SO2 SO3 SO4 SO5 Fused CONV1-1 CONV2-1 CONV1-2 CONV2-2 CONV3-1 CONV3-2 CONV3-3 CONV4-1 CONV4-2 CONV4-3 CONV5-1 CONV5-2 CONV5-3 SO2-3 SO3-4 SO4-5 Fused SO1 SO1-2 SO1-2-3 SO2-3-4 SO3-4</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Overall Architecture</head><p>We implement the proposed Hi-Fi architecture based on the VGG16 [Simonyan and Zisserman, 2015] network, which has 13 convolutional layers and 2 fully connected layers. The conv-layers in VGG network are divided into 5 groups: conv1-x, ..., conv5-x, and there are 2?3 conv-layers in a group. There are pooling layers with stride = 2 between neighbouring convolution groups.</p><p>In HED, the side-outputs connect only with the last convlayer of each group. RCF (Richer Convolution Features) <ref type="bibr" target="#b7">[Liu et al., 2017]</ref> connects a side-output to all layers of a convolutional group. We follow this idea to get more powerful convolutional features. The overall architecture of Hi-Fi is illustrated in <ref type="figure" target="#fig_1">Fig. 3</ref>, convolutional groups are distinguished by colors, and pooling layers are omitted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Hierarchical Feature Integration</head><p>A detailed illustration of the proposed feature integration procedure is shown in <ref type="figure" target="#fig_2">Fig. 4</ref>. Feature maps to be integrated are branched from the primary network stream through a (1 ? 1) convolutional layer (dotted boxes marked with (a)) on top of an interior convolutional layer. These feature maps are further integrated with element-wise sum (box marked with (c)). The final scale-associated side-output (box marked with (d)) is produced by a (1 ? 1) convolution. Note that due to the ex-istence of pooling layers, deeper convolutional feature maps are spatially smaller than shallower ones. Upsampling (box marked with (b)) is required to guarantee all feature maps to be integrated are of the same size.</p><p>Ideally, the feature integration can be recursively performed until the last integrated feature map contains information from all convolution layers (conv1-1 ? conv5-3). However, limited by the memory of our GPUs and the training time, we end up with two level integration ( <ref type="figure" target="#fig_1">Fig. 3</ref> 'Hi-Fi-2'). In our architecture SOs are built on top of an integration of nearby feature levels, and the "nearby feature integration" is recursively performed. In testing phase, SOs will receive information from both deeper and shallower sides; and in training phase, gradient from SOs will back-prop to both as well. In other words, our approach explicitly enables not only deep-to-shallow but also shallow-to-deep refinement. It is obviously shown in <ref type="figure" target="#fig_3">Fig. 5</ref> that Hi-Fi obtains cleaner shallower SOs than FSDS, and at the same time has much more highresolution deeper SOs than SRN. Consequently, we gain a strong quality improvement in the final fused result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Formulation</head><p>Here we formulate our approach for skeleton detection. Skeleton detection can be formulated as a pixel-wise binary classification problem. Given an input image X = {x j , j = 1, ..., |X|}, the goal of skeleton detection is to predict the corresponding skeleton map? = {? j , j = 1, ..., |X|}, where y j ? {0, 1} is the predicted label of pixel x j .? j = 1 means pixel x j is predicted as a skeleton point, otherwise, pixel x j is the background.</p><formula xml:id="formula_2">CONV1-1 CONV2-1 CONV1-2 CONV2-2 CONV3-1 CONV3-2 CONV3-3 CONV4-1 CONV4-2 CONV4-3 CONV5-1 CONV5-2 CONV5-3</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>HI-FI</head><p>Ground-truth Quantization. Following FSDS [Shen et al., 2016b], we supervise the side-outputs with 'scaleassociated' skeleton ground-truths. To differentiate and supervise skeletons of different scales, skeletons are quantized into several classes according to their scales. The skeleton scale is defined as the distance between a skeleton point and its nearest boundary point. Assume S = {s j , j = 1, ..., |X|}(s j ? R) is the skeleton scale map, where s j represents the skeleton scale of pixel x j . When x j is the background, s j = 0. Let Q = {q j , j = 1, ..., |X|} be the quantized scale map, where q j is the quantized scale of pixel x j . The quantized scale q j can be obtained by:</p><formula xml:id="formula_3">q j = m if r m?1 &lt; s j r m 0 if s j = 0 or s j &gt; r M ,<label>(1)</label></formula><p>where r m (m = 1, ..., M ) is the receptive field of the m-th side-output (SO-m), with r 0 = 0, and M is the number of side-outputs. For instance, pixel x j with scale s j = 39 is quantized as q j = 3, because 14 = r 2 &lt; s j r 3 = 40 (receptive fields of side-outputs are shown in <ref type="figure" target="#fig_1">Fig. 3</ref> with numbers-in-boxes). All background pixels (s j = 0) and skeleton points out of scope of the network (s j &gt; r M ) are quantized as 0.</p><p>Supervise the Side-outputs. Scale-associated side-output is only supervised by the skeleton with scale smaller than its receptive field. We denote ground-truth as G m = {g m j , j = 1, ..., |X|}, which is used to supervise SO-m. G m is modified from Q with all quantized values larger than m set to zero. g m j can be obtained from Q by:</p><formula xml:id="formula_4">g m j = q j if q j &lt; m 0 otherwise.<label>(2)</label></formula><p>Supervising side-outputs with the quantized skeleton map turns the original binary classification problem into a multiclass problem. Fuse the Side-outputs. Suppose P m = {p m j,k , j = 1, ..., |X|, k = 0, ..., K m } is the predicted probability map of SO-m, in which K m is the number of quantized classes SO-m can recognise. p m j,k means the probability of pixel x j belonging to a quantized class #k, and index j is over the spatial dimensions of the input image X. Obviously we have K m k=0 p m j,k = 1. We fuse the probability maps from different side-outputs P m (m = 1, ..., M ) with a weighted summation to obtain the final fused prediction P = {p j,k }:</p><formula xml:id="formula_5">p j,k = m w mk ? p m j,k ,<label>(3)</label></formula><p>where p j,k is the fused probability that pixel x j belongs to quantized class #k, w mk is the credibility of side-output m on quantized class #k. Eq. (3) can be implemented with a simple (1 ? 1) convolution. Loss Function and Optimization. We simultaneously optimize all the side-outputs P m (m = 1, ..., M ) and the fused prediction P in an end-to-end way. In HED <ref type="bibr">[2015]</ref>, Xie and Tu introduce a class-balancing weight to address the problem of positive/negative unbalancing in boundary detection. This problem still exists in skeleton detection because most of the pixels are background. We use the class-balancing weight to offset the imbalance between skeletons and the background. Specifically, we define the balanced softmax loss l(P, G) as:</p><formula xml:id="formula_6">l(P, G) = j ? ? m k =0 log(p m j,k )1(g m j == k)? (1 ? ? m ) log(p m j,0 )1(g m j == 0) ,<label>(4)</label></formula><p>where P = h(X|?) is the prediction from CNN, G is the ground-truth, and m is the index of side-outputs. h(X|?) is the model hypothesis taking image X as input, parameterized by ?. ? m = |X| j=1 1(g m j = 0) /|X| is a balancing factor, where 1(?) is an indicator. The overall loss function can be expressed as follow:</p><formula xml:id="formula_7">L h(X|?), G = L side + L fuse = M m=1 l(P m , G m ) + l(P, Q),<label>(5)</label></formula><p>where P m and G m are prediction/ground-truth of SO-m respectively, G is the ground-truth of final fused output P .</p><p>All the parameters including the fusing weight w mk in Eq. (3) are part of ?. We can obtain the optimal parameters by a standard stochastic gradient descent (SGD):</p><formula xml:id="formula_8">(?) * = argmin L.<label>(6)</label></formula><p>Detect Skeleton with Pretrained Model. Given trained parameters ?, the skeleton response map? = {? j , j = 1, ..., |X|} is obtained via:</p><formula xml:id="formula_9">y j = 1 ? p j,0 ,<label>(7)</label></formula><p>where? j ? [0, 1] indicates the probability that pixel x j belongs to the skeleton.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments and Analysis</head><p>In this section, we discuss the implementation details and report the performance of the proposed method on several open benchmarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>The experiments are conducted on four popular skele-  <ref type="bibr" target="#b6">[Ke et al., 2017]</ref>. Images of these datasets are selected from other semantic segmentation datasets with human annotated object segmentation masks, and the skeleton ground-truths are extracted from segmentations. Objects of SK-SMALL and SK-LARGE are cropped from MSCOCO dataset with 'well defined' skeletons , and there is only one object in each image. SYM-PSCAL selects images from the PASCAL-VOC2012 dataset without cropping and here may be multiple objects in an image. Some representative example images and corresponding skeleton ground-truths of these datasets are shown in <ref type="figure" target="#fig_4">Fig. 6</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implementation Details</head><p>We implement the proposed architecture based on the openly available caffe <ref type="bibr" target="#b5">[Jia et al., 2014]</ref> framework. The hyperparameters and corresponding values are: base learning rate 1 http://kaiz.xyz/sk-large (10 ?6 ), mini-batch size (1), momentum (0.9) and maximal iteration (40000). We decrease the learning rate every 10,000 iterations with factor 0.1. We perform the same data augmentation operations with FSDS for fair comparison. The augmentation operations are: (1) random resize images (and gt maps) to 3 scales (0.8, 1, 1.2), (2) random left-right flip images (and gt maps); and (3) random rotate images (and gt maps) to 4 angles <ref type="bibr">(0,</ref><ref type="bibr">90,</ref><ref type="bibr">180,</ref><ref type="bibr">270)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Evaluation Protocol</head><p>The skeleton response map? is obtained through Eq. <ref type="formula" target="#formula_9">(7)</ref>, to which a standard non-maximal suppression (NMS) is then applied to obtain the thinned skeleton map for evaluation. We evaluate the performance of the thinned skeleton map? in terms of F-measure= 2 P recision?Recall P recision+Recall as well as the precision recall curve (PR-curve) w.r.t ground-truth G. By applying different thresholds to? , a series of precision/recall pairs are obtained to draw the PR-curve. The F-measure is obtained under the optimal threshold over the whole dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Skeleton Detection</head><p>We test our method on four aforementioned datasets. Example images and ground-truths of these datasets are shown in <ref type="figure" target="#fig_4">Fig. 6</ref>, some detection results by different approaches are shown in <ref type="figure">Fig. 8</ref>. Similar to RCF <ref type="bibr" target="#b7">[Liu et al., 2017]</ref> we perform multi-scale detection by resizing input images to different scales (0.5, 1, 1.5) and average their results. We compare the proposed approach with other competitors including one learning based method MIL <ref type="bibr">[Tsogkas and Kokkinos, 2012]</ref>, and several recent CNN-based methods: FSDS <ref type="bibr" target="#b8">[Shen et al., 2016b]</ref>, SRN <ref type="bibr" target="#b6">[Ke et al., 2017]</ref>, <ref type="bibr">HED [Xie and Tu, 2015]</ref>, RCF <ref type="bibr" target="#b7">[Liu et al., 2017]</ref>). FSDS and SRN are specialized skeleton detectors, HED and RCF are developed for edge detection. Quantitative results are shown in <ref type="figure">Fig. 7</ref> and Tab.1, our proposed method outperforms the competitors in both terms of F-measure and PR-curve. Some representative detection results are shown in <ref type="figure">Fig. 8</ref>.</p><p>Comparison results in Tab. 1 reveal that with 1st-level hierarchical integration (Hi-Fi-1), our method (Hi-Fi-1) already outperforms others with a significant margin. Moreover, by integrating 1st-level integrated features we obtain the 2ndlevel integration (Hi-Fi-2) and the performance witnesses a further improvement (Limited by the GPU memory, we didn't implement Hi-Fi-3). Architecture details of Hi-Fi-1 and Hi-Fi-2 are illustrated in <ref type="figure" target="#fig_1">Fig. 3</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>WH-SYMMAX</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SK-SMALL</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Ablation Study</head><p>We do ablation study on SK-LARGE dataset to further probe the proposed method. Different Feature Integrating Mechanisms. We compare different feature integrating mechanisms including: (1) FSDS [Shen et al., 2016b], (2) SRN <ref type="bibr" target="#b6">[Ke et al., 2017]</ref> with deep-to-shallow refinement, (3) Hi-Fi-1 with 1 level hierarchical integration <ref type="figure" target="#fig_0">( Fig. 3 (Hi-Fi-1)</ref>), and (4) Hi-Fi-2 with 2 level hierarchical integration ( <ref type="figure" target="#fig_1">Fig. 3 (Hi-Fi-2)</ref>). Results are shown in Tab. 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>FSDS SRN</head><p>Hi-Fi-1 Hi-Fi-2 0.633 0.640 0.703 0.724 Hierarchical Integration versus Direct Fusing. To further justify the proposed hierarchical feature integration mechanism (Hi-Fi), we compare the proposed Hi-Fi network with another architecture described in <ref type="figure">Fig. 2 (c)</ref>, which fuses features from all levels together at once. The comparison results shown in <ref type="figure">Fig. 9</ref> support our claim that "learning an integration of nearby feature levels is easier than learning combination of features of all levels", as evidenced by a faster convergence and better converged performance.</p><p>Integrating K Feature Levels at Each Step. We also test models that fuse K (K = 1, 2, ..., 5) consecutive feature levels at each step, and the results are summarized in Tab. 3. Convergence Analysis Direct <ref type="figure">(Fig.2 (c)</ref>) Hi-Fi-1 Hi-Fi-2 <ref type="figure">Figure 9</ref>: Hierarchical Feature Integration (Hi-Fi) versus Direct Fusing <ref type="figure">(Fig. 2 (c)</ref>).</p><p>When K = 1 the model reduces to FSDS where side-outputs are working independently. K = 2 represents our proposed Hi-Fi (Hi-Fi-1 and Hi-Fi-2) which combines every two nearby feature levels, and K = 5 is identical to <ref type="figure">Fig. 2 (c)</ref> that combines all feature levels at once. K 1 2 3 4 5 F-measure 0.633 0.703 0.689 0.690 0.679 <ref type="table">Table 3</ref>: Comparison of models that fuse K feature levels at each step.</p><p>Failure Case Exploration. Since our method achieves the least performance gain on SYM-PASCAL <ref type="bibr" target="#b6">[Ke et al., 2017]</ref>, we analysis the failure cases on this dataset. We select top-5 worst detections (ranked by F-measure w.r.t ground-truths) shown in <ref type="figure" target="#fig_0">Fig. 10</ref>. Failure cases on this dataset are mainly GT Hi-Fi SRN <ref type="figure" target="#fig_0">Figure 10</ref>: The top-5 worst detections on SYM-PASCAL dataset. The results are ranked according to F-measure w.r.t ground-truths. caused by the ambiguous annotations and not-well selected objects. Our method (and also others) cannot deal with 'square-shaped' objects like monitors and doors whose skeletons are hard to define and recognise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We propose a new CNN architecture named Hi-Fi for skeleton detection. Our proposed method has two main advantages over existing systems: (a) it enables mutual refinement with both deep-to-shallow and shallow-to-deep guidance; (b) it recursively integrates nearby feature levels and supervises all intermediate integrations, which leads to a faster convergence and better performance. Experimental results on several benchmarks demonstrate that our method significantly outperforms the state-of-the-arts with a clear margin.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>compared with boundary detection, * M.M. Cheng is the corresponding author. Skeleton detection is facing a more challenging scale space problem: (a) object boundaries can be detected with filters of similar size (green boxes); (b) only filters a bit larger than the skeleton scale (green boxes) can capture proper context for skeleton detection; both improper big or small (red boxes) cannot perceive skeletons well; (c) compared with boundary detection and semantic segmentation, skeleton detection requires inhomogeneous feature levels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Architecture of our proposed Hi-Fi network. All sideoutputs (SOs, marked with rounded square box) are supervised by skeletons within their receptive fields (numbers on top of SOs indicate their receptive fields). Features of neighbouring feature levels are integrated to enable mutual refinement, and a lateral feature hierarchy is obtained with recursively integrating neighbouring features.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Illustration of feature integration: (a) feature maps to be integrated are firstly produced by a (1?1) convolution; (b) deeper features are upsampled before integration; (c) the integration is implemented by an element-wise sum; (d) side-outputs are built on top of the integrated features with a (1?1) convolution. Bidirectional Refinement. We explain the proposed bidirectional mutual refinement by comparing it with existing architectures: FSDS [Shen et al., 2016b] and SRN [Ke et al., 2017]. As shown in Fig. 5, side-outputs (SOs) of FSDS are working independently, there is no cross talk between features of different levels. As a result, FSDS has noisy shallow SOs and low-resolution deeper SOs. SRN then introduces deep-to-shallow refinement by bringing deep features to shallow SOs. As shown in Fig. 5, shallower SOs of SRN are much cleaner than that of FSDS. Despite the improvement, deeper SOs in SRN are still suffering from low-resolution, which limits the quality of the final fused result.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Illustration of Hi-Fi, FSDS [Shen et al., 2016b] and SRN [Ke et al., 2017].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Example images and corresponding skeleton groundtruths (red curves) of several skeleton datesets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :Figure 8 :</head><label>78</label><figDesc>Precision Recall curves of recent CNN-based methods HED<ref type="bibr" target="#b9">[Xie and Tu, 2015]</ref>, RCF<ref type="bibr" target="#b7">[Liu et al., 2017]</ref>,FSDS [Shen et al., 2016b], SRN<ref type="bibr" target="#b6">[Ke et al., 2017]</ref> and one learning based method MIL[Tsogkas and Kokkinos, 2012]. Representative detection results. Our detected skeletons are more continuous and finer (thinner) than others.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Performance of different feature integration mechanisms.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This research was supported by NSFC (NO. 61620106008, 61572264, 61672336), Huawei Innovation Research Program, and Fundamental Research Funds for the Central Universities.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Models for the perception of speech and visual forms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><forename type="middle">H</forename><surname>Blum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Blum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">A Transformation for extracting new descriptors of shape</title>
		<meeting><address><addrLine>Boston, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1967" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="363" to="380" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Object recognition as many-to-many feature matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Demirci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="203" to="222" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
	<note>CVPR</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Deeply supervised salient object detection with short connections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Hou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>PAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A pseudo-distance map for the segmentation-free skeletonization of gray-scale images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong ; Jeong-Hun</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ki-Sang</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2001" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="18" to="23" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Caffe: Convolutional architecture for fast feature embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACMMM</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="675" to="678" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Srn: Side-output residual network for object symmetry detection in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Ke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Richer convolutional features for edge detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Multiple instance subspace learning via partial random projection tree for local reflection symmetry in natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Morse</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IPMI</title>
		<editor>Amos Sironi, Vincent Lepetit, and Pascal Fua</editor>
		<meeting><address><addrLine>Shotton, Toby Sharp, Alex Kipman, Andrew Fitzgibbon, Mark Finocchio, Andrew Blake, Mat Cook, and Richard Moore</addrLine></address></meeting>
		<imprint>
			<publisher>PAMI</publisher>
			<date type="published" when="1993" />
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="2697" to="2704" />
		</imprint>
	</monogr>
	<note>CVPR</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Amulet: Aggregating multi-level convolutional features for salient object detection</title>
	</analytic>
	<monogr>
		<title level="m">Tsogkas and Kokkinos, 2012] Stavros Tsogkas and Iasonas Kokkinos</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="818" to="833" />
		</imprint>
	</monogr>
	<note>ICCV</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
