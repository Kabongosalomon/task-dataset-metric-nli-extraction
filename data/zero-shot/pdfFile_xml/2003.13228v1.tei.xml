<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Memory-guided Normality for Anomaly Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyunjong</forename><surname>Park</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Electrical and Electronic Engineering</orgName>
								<orgName type="institution">Yonsei University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongyoun</forename><surname>Noh</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Electrical and Electronic Engineering</orgName>
								<orgName type="institution">Yonsei University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bumsub</forename><surname>Ham</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Electrical and Electronic Engineering</orgName>
								<orgName type="institution">Yonsei University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Learning Memory-guided Normality for Anomaly Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T14:18+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We address the problem of anomaly detection, that is, detecting anomalous events in a video sequence. Anomaly detection methods based on convolutional neural networks (CNNs) typically leverage proxy tasks, such as reconstructing input video frames, to learn models describing normality without seeing anomalous samples at training time, and quantify the extent of abnormalities using the reconstruction error at test time. The main drawbacks of these approaches are that they do not consider the diversity of normal patterns explicitly, and the powerful representation capacity of CNNs allows to reconstruct abnormal video frames. To address this problem, we present an unsupervised learning approach to anomaly detection that considers the diversity of normal patterns explicitly, while lessening the representation capacity of CNNs. To this end, we propose to use a memory module with a new update scheme where items in the memory record prototypical patterns of normal data. We also present novel feature compactness and separateness losses to train the memory, boosting the discriminative power of both memory items and deeply learned features from normal data. Experimental results on standard benchmarks demonstrate the effectiveness and efficiency of our approach, which outperforms the state of the art.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The problem of detecting abnormal events in video sequences, e.g., vehicles on sidewalks, has attracted significant attention over the last decade, which is particularly important for surveillance and fault detection systems. It is extremely challenging for a number of reasons: First, anomalous events are determined differently according to circumstances. Namely, the same activity could be normal or abnormal (e.g., holding a knife in the kitchen or in the park). Manually annotating anomalous events is in this context labor intensive. Second, collecting anomalous datasets requires a lot of effort, as anomalous events rarely happen in real-life situations. Anomaly detection is thus typically deemed to be an unsupervised learning problem, aiming at * Equal contribution. ? Corresponding author.  <ref type="bibr" target="#b23">[24]</ref>. The features and items are shown in points and stars, respectively. The points with the same color are mapped to the same item. The items in the memory capture diverse and prototypical patterns of normal data. The features are highly discriminative and similar image patches are clustered well. (Best viewed in color.) learning a model describing normality without anomalous samples. At test time, events and activities not described by the model are then considered as anomalies.</p><p>There are many attempts to model normality in video sequences using unsupervised learning approaches. At training time, given normal video frames as inputs, they typically extract feature representations and try to reconstruct the inputs again. The video frames of large reconstruction errors are then treated as anomalies at test time. This assumes that abnormal samples are not reconstructed well, as the models have never seen them during training. Recent methods based on convolutional neural networks (CNNs) exploit an autoencoder (AE) <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b16">17]</ref>. The powerful representation capacity of CNNs allows to extract better feature representations. The CNN features from abnormal frames, on the other hand, are likely to be reconstructed by combining those of normal ones <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b7">8]</ref>. In this case, abnormal frames have low reconstruction errors, often oc-curring when a majority of the abnormal frames are normal (e.g., pedestrians in a park). In order to lessen the capacity of CNNs, a video prediction framework <ref type="bibr" target="#b21">[22]</ref> is introduced that minimizes the difference between a predicted future frame and its ground truth. The drawback of these methods <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b21">22]</ref> is that they do not detect anomalies directly <ref type="bibr" target="#b34">[35]</ref>. They instead leverage proxy tasks for anomaly detection, e.g., reconstructing input frames <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b16">17]</ref> or predicting future frames <ref type="bibr" target="#b21">[22]</ref>, to extract general feature representations rather than normal patterns. To overcome this problem, Deep SVDD <ref type="bibr" target="#b34">[35]</ref> exploits the one-class classification objective to map normal data into a hypersphere. Specifically, it minimizes the volume of the hypersphere such that normal samples are mapped closely to the center of the sphere. Although a single center of the sphere represents a universal characteristic of normal data, this does not consider various patterns of normal samples.</p><p>We present in this paper an unsupervised learning approach to anomaly detection in video sequences considering the diversity of normal patterns. We assume that a single prototypical feature is not enough to represent various patterns of normal data. That is, multiple prototypes (i.e., modes or centroids of features) exist in the feature space of normal video frames ( <ref type="figure" target="#fig_0">Fig. 1</ref>). To implement this idea, we propose a memory module for anomaly detection, where individual items in the memory correspond to prototypical features of normal patterns. We represent video frames using the prototypical features in the memory items, lessening the capacity of CNNs. To reduce the intra-class variations of CNN features, we propose a feature compactness loss, mapping the features of a normal video frame to the nearest item in the memory and encouraging them to be close. Simply updating memory items and extracting CNN features alternatively in turn give a degenerate solution, where all items are similar and thus all features are mapped closely in the embedding space. To address this problem, we propose a feature separateness loss. It minimizes the distance between each feature and its nearest item, while maximizing the discrepancy between the feature and the second nearest one, separating individual items in the memory and enhancing the discriminative power of the features and memory items. We also introduce an update strategy to prevent the memory from recording features of anomalous samples at test time. To this end, we propose a weighted regular score measuring how many anomalies exist within a video frame, such that the items are updated only when the frame is determined as a normal one. Experimental results on standard benchmarks, including UCSD Ped2 <ref type="bibr" target="#b20">[21]</ref>, CUHK Avenue <ref type="bibr" target="#b23">[24]</ref> and Shang-haiTech <ref type="bibr" target="#b25">[26]</ref>, demonstrate the effectiveness and efficiency of our approach, outperforming the state of the art.</p><p>The main contributions of this paper can be summarized as follows:</p><p>? We propose to use multiple prototypes to represent the diverse patterns of normal video frames for unsupervised anomaly detection. To this end, we introduce a memory module recording prototypical patterns of normal data on the items in the memory. ? We propose feature compactness and separateness losses to train the memory, ensuring the diversity and discriminative power of the memory items. We also present a new update scheme of the memory, when both normal and abnormal samples exist at test time. ? We achieve a new state of the art on standard benchmarks for unsupervised anomaly detection in video sequences. We also provide an extensive experimental analysis with ablation studies. Our code and models are available online: https:// cvlab.yonsei.ac.kr/projects/MNAD.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>Anomaly detection. Many works formulate anomaly detection as an unsupervised learning problem, where anomalous data are not available at training time. They typically adopt reconstructive or discriminative approaches to learn models describing normality. Reconstructive models encode normal patterns using representation learning methods such as an AE <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b35">36]</ref>, a sparse dictionary learning <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b23">24]</ref>, and a generative model <ref type="bibr" target="#b42">[43]</ref>. Discriminative models characterize the statistical distributions of normal samples and obtain decision boundaries around the normal instances e.g., using Markov random field (MRF) <ref type="bibr" target="#b14">[15]</ref>, a mixture of dynamic textures (MDT) <ref type="bibr" target="#b27">[28]</ref>, Gaussian regression <ref type="bibr" target="#b3">[4]</ref>, and one-class classification <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b13">14]</ref>. These approaches, however, often fail to capture the complex distributions of high-dimensional data such as images and videos <ref type="bibr" target="#b2">[3]</ref>.</p><p>CNNs have allowed remarkable advances in anomaly detection over the last decade. Many anomaly detection methods leverage reconstructive models <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b32">33]</ref> exploiting feature representations from e.g., a convolutional AE (Conv-AE) <ref type="bibr" target="#b8">[9]</ref>, a 3D Conv-AE <ref type="bibr" target="#b49">[50]</ref>, a recurrent neural network (RNN) <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b24">25]</ref>, and a generative adversarial network (GAN) <ref type="bibr" target="#b32">[33]</ref>. Although CNN-based methods outperform classical approaches by large margins, they even reconstruct anomalous samples with a combination of normal ones, mainly due to the representation capacity of CNNs. This problem can be alleviated by using predictive or discriminative models <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b34">35]</ref>. The work of <ref type="bibr" target="#b21">[22]</ref> assumes that anomalous frames in video sequences are unpredictable, and trains a network for predicting future frames rather than the input itself <ref type="bibr" target="#b21">[22]</ref>. It achieves a remarkable performance gain over reconstructive models, but at the cost of runtime for estimating optical flow between video frames. It also requires ground-truth optical flow to train a sub-network for computing flow fields. Deep SVDD <ref type="bibr" target="#b34">[35]</ref>  The memory module performs reading and updating items pm of size 1?1?C using queries q k t of size 1?1?C, where the numbers of items and queries are M and K, respectively, and K = H ?W . The query map qt is concatenated with the aggregated (i.e., read) itemspt. The decoder then inputs them to reconstruct the video frame?t. For the prediction task, we input four successive video frames to predict the fifth one. (Best viewed in color.) leverages CNNs as mapping functions that transform normal data into the center of the hypersphere, whereas forcing anomalous samples to fall outside the sphere, using the one-class classification objective. Our method also lessens the representation capacity of CNNs but using a different way. We reconstruct or predict a video frame with a combination of items in the memory, rather than using CNN features directly from an encoder, while considering various patterns of normal data. In case of future frame prediction, our model does not require computing optical flow, and thus it is much faster than the current method <ref type="bibr" target="#b21">[22]</ref>. Deep-Cascade <ref type="bibr" target="#b36">[37]</ref> detects various normal patches using cascaded deep networks. In contrast, our method leverages memory items to record the normal pattern explicitly even in test sequences. Concurrent to our method, Gong et al. introduce a memory-augmented autoencoder (MemAE) for anomaly detection <ref type="bibr" target="#b7">[8]</ref>. It also uses CNN features but using a 3D Conv-AE to retrieve relevant memory items that record normal patterns, where the items are updated during training only. Unlike this approach, our model better records diverse and discriminative normal patterns by separating memory items explicitly using feature compactness and separateness losses, enabling using a small number of items compared to MemAE (10 vs 2,000 for MemAE). We also update the memory at test time, while discriminating anomalies simultaneously, suggesting that our model also memorizes normal patterns of test data.</p><p>Memory networks. There are a number of attempts to capture long-term dependencies in sequential data. Long short-term memory (LSTM) <ref type="bibr" target="#b10">[11]</ref> addresses this problem using local memory cells, where hidden states of the network record information in the past partially. The memorization performance is, however, limited, as the size of the cell is typically small and the knowledge in the hidden state is compressed. To overcome the limitation, memory networks <ref type="bibr" target="#b44">[45]</ref> have recently been introduced. It uses a global memory that can be read and written to, and performs a memorization task better than classical approaches. The memory networks, however, require layer-wise supervision to learn models, making it hard to train them using standard backpropagation. More recent works use continuous memory representations <ref type="bibr" target="#b39">[40]</ref> or key-value pairs <ref type="bibr" target="#b29">[30]</ref> to read/write memories, allowing to train the memory networks end-to-end. Several works adopt the memory networks for computer vision tasks including visual question answering <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b6">7]</ref>, one-shot learning <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b1">2]</ref>, image generation <ref type="bibr" target="#b50">[51]</ref>, and video summarization <ref type="bibr" target="#b19">[20]</ref>. Our work also exploits a memory module but for anomaly detection with a different memory updating strategy. We record various patterns of normal data to individual items in the memory, and consider each item as a prototypical feature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Approach</head><p>We show in <ref type="figure">Fig. 2</ref> an overview of our framework. We reconstruct input frames or predict future ones for unsupervised anomaly detection. Following <ref type="bibr" target="#b21">[22]</ref>, we input four successive video frames to predict the fifth one for the prediction task. As the prediction can be considered as a reconstruction of the future frame using previous ones, we use almost the same network architecture with the same losses for both tasks. We describe hereafter our approach for the reconstruction task in detail.</p><p>Our model mainly consists of three components: an encoder, a memory module, and a decoder. The encoder inputs a normal video frame and extracts query features. The features are then used to retrieve prototypical normal patterns in the memory items and to update the memory. We feed the query features and memory items aggregated (i.e., read) to the decoder for reconstructing the input video frame. We train our model using reconstruction, feature compactness, and feature separateness losses end-to-end. At test time, we use a weighted regular score in order to prevent the memory from being updated by abnormal video frames. We com- <ref type="figure">Figure 3</ref>: Illustration of reading and updating the memory. To read items in the memory, we compute matching probabilities w k,m t in (1) between the query q k t and items (p1, . . . pM ), and apply a weighted average of the items with the probabilities to obtain the featurep k t . To update the items, we compute another matching probabilities v k,m t in (4) between the item pm and the queries (q 1 t , . . . q K t ). We then compute a weighted average of the queries in the set U m t with the corresponding probabilities, and add it to the initial item pm in (3). c: cosine similarities; s: a softmax function; w: a weighted average; n: max normalization; U m t : a set of indices for the m-th memory item. See text for details. (Best viewed in color.) pute the discrepancies between the input frame and its reconstruction and the distances between the query feature and the nearest item in the memory to quantify the extent of abnormalities in a video frame.</p><formula xml:id="formula_0">? ! " % ! " &amp; ! " ) c ? $ * s + " %,* ? + " &amp;,* + " ),* -" * +? " %,* ? +? " #,* w n Update ! " # $ % $ &amp; $ ' ( $ " # ? c s 0 " #,% ? 0 " #,&amp; 0 " #,1 w Read -" *</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Network architecture 3.1.1 Encoder and decoder</head><p>We exploit the U-Net architecture <ref type="bibr" target="#b33">[34]</ref>, widely used for the tasks of reconstruction and future frame prediction <ref type="bibr" target="#b21">[22]</ref>, to extract feature representations from input video frames and to reconstruct the frames from the features. Differently, we remove the last batch normalization <ref type="bibr" target="#b11">[12]</ref> and ReLU layers <ref type="bibr" target="#b17">[18]</ref> in the encoder, as the ReLU cuts off negative values, restricting diverse feature representations. We instead add an L2 normalization layer to make the features have a com-mon scale. Skip connections in the U-Net architecture may not be able to extract useful features from the video frames especially for the reconstruction task, and our model may learn to copy the inputs for the reconstruction. We thus remove the skip connections for the reconstruction task, while retaining them for predicting future frames. We denote by I t and q t a video frame and a corresponding feature (i.e., a query) from the encoder at time t, respectively. The encoder inputs the video frame I t and gives the query map q t of size H ? W ? C, where H, W , C are height, width, and the number of channels, respectively. We denote by q k t ? R C (k = 1, . . . K), where K = H ? W , individual queries of size 1 ? 1 ? C in the query map q t . The queries are then inputted to the memory module to read the items in the memory or to update the items, such that they record prototypical normal patterns. The detailed descriptions of the memory module are presented in the following section. The decoder inputs the queries and retrieved memory items and reconstructs the video frame? t .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Memory</head><p>The memory module contains M items recording various prototypical patterns of normal data. We denote by p m ? R C (m = 1, . . . , M ) the item in the memory. The memory performs reading and updating the items <ref type="figure">(Fig. 3)</ref>. Read. To read the items, we compute the cosine similarity between each query q k t and all memory items p m , resulting in a 2-dimensional correlation map of size M ? K. We then apply a softmax function along a vertical direction, and obtain matching probabilities w k,m t as follows:</p><formula xml:id="formula_1">w k,m t = exp((p m ) T q k t ) M m =1 exp((p m ) T q k t )</formula><p>.</p><p>(1)</p><p>For each query q k t , we read the memory by a weighted average of the items p m with the corresponding weights w k,m t , and obtain the featurep k t ? R C as follows:</p><formula xml:id="formula_2">p k t = M m =1 w k,m t p m .<label>(2)</label></formula><p>Using all items instead of the closest item allows our model to understand diverse normal patterns, taking into account the overall normal characteristics. That is, we represent the query q k t with a combination of the items p m in the memory. We apply the reading operator to individual queries, and obtain a transformed feature mapp t ? R H?W ?C (i.e., aggregated items). We concatenate it with the query map q t along the channel dimension, and input them to the decoder. This enables the decoder to reconstruct the input frame using normal patterns in the items, lessening the representation capacity of CNNs, while understanding the normality. Update. For each memory item, we select all queries declared that the item is the nearest one, using the matching probabilities in <ref type="bibr" target="#b0">(1)</ref>. Note that multiple queries can be assigned to a single item in the memory. See, for example, <ref type="figure" target="#fig_2">Fig. 5</ref> in Sec. 4.3. We denote by U m t the set of indices for the corresponding queries for the m-th item in the memory. We update the item using the queries indexed by the set U m t only as follows:</p><formula xml:id="formula_3">p m ? f (p m + k?U m t v k,m t q k t ),<label>(3)</label></formula><p>where f (?) is the L2 norm. By using a weighted average of the queries, rather than summing them up, we can concentrate more on the queries near the item. To this end, we compute matching probabilities v k,m t similar to (1) but by applying the softmax function to the correlation map of size</p><formula xml:id="formula_4">M ? K along a horizontal direction as v k,m t = exp((p m ) T q k t ) K k =1 exp((p m ) T q k t ) ,<label>(4)</label></formula><p>and renormalize it to consider the queries indexed by the set U m t as follows:</p><formula xml:id="formula_5">v k,m t = v k,m t max k ?U m t v k ,m t .<label>(5)</label></formula><p>We update memory items recording prototypical features at both training and test time, since normal patterns in training and test sets may be different and they could vary with various factors, e.g., illumination and occlusion. As both normal and abnormal frames are available at test time, we propose to use a weighted regular score to prevent the memory items from recording patterns in the abnormal frames. Given a video frame I t , we use the weighted reconstruction error between I t and? t as the regular score E t :</p><formula xml:id="formula_6">E t = i,j W ij (? t , I t ) ? ij t ? I ij t 2 ,<label>(6)</label></formula><p>where the weight function W ij (?) is</p><formula xml:id="formula_7">W ij (? t , I t ) = 1 ? exp(?||? ij t ? I ij t || 2 ) i,j 1 ? exp(?||? ij t ? I ij t || 2 ) ,<label>(7)</label></formula><p>and i and j are spatial indices. When the score E t is higher than a threshold ?, we regard the frame I t as an abnormal sample, and do not use it for updating memory items. Note that we use this score only when updating the memory. The weight function allows to focus more on the regions of large reconstruction errors, as abnormal activities typically appear within small parts of the video frame.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Training loss</head><p>We exploit the video frames as a supervisory signal to discriminate normal and abnormal samples. To train our model, we use reconstruction, feature compactness, and feature separateness losses (L rec , L compact and L separate , respectively), balanced by the parameters ? c and ? s as follows:</p><formula xml:id="formula_8">L = L rec + ? c L compact + ? s L separate .<label>(8)</label></formula><p>Reconstruction loss. The reconstruction loss makes the video frame reconstructed from the decoder similar to its ground truth by penalizing the intensity differences. Specifically, we minimize the L2 distance between the decoder output? t and the ground truth I t :</p><formula xml:id="formula_9">L rec = T t ? t ? I t 2 ,<label>(9)</label></formula><p>where we denote T by the total length of a video sequence. We set the first time step to 1 and 5 for reconstruction and prediction tasks, respectively.</p><p>Feature compactness loss. The feature compactness loss encourages the queries to be close to the nearest item in the memory, reducing intra-class variations. It penalizes the discrepancies between them in terms of the L2 norm as:</p><formula xml:id="formula_10">L compact = T t K k q k t ? p p 2 ,<label>(10)</label></formula><p>where p is an index of the nearest item for the query q k t defined as, p = argmax m?M w k,m t .</p><p>Note that the feature compactness loss and the center loss <ref type="bibr" target="#b43">[44]</ref> are similar, as the memory item p p corresponds the center of deep features in the center loss. They are different in that the item in (10) is retrieved from the memory, and it is updated without any supervisory signals, while the cluster center in the center loss is computed directly using the features learned from ground-truth class labels. Note also that our method can be considered as an unsupervised learning of joint clustering and feature representations. In this task, degenerate solutions are likely to be obtained <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b46">47]</ref>. As will be seen in our experiments, training our model using the feature compactness loss only makes all items similar, and thus all queries are mapped closely in the embedding space, losing the capability of recording diverse normal patterns.</p><p>Feature separateness loss. Similar queries should be allocated to the same item in order to reduce the number of items and the memory size. The feature compactness loss in (10) makes all queries and memory items close to each other, as we extract the features (i.e., queries) and update the items alternatively, resulting that all items are similar. The items in the memory, however, should be far enough apart from each other to consider various patterns of normal data. To prevent this problem while obtaining compact feature representations, we propose a feature separateness loss, defined with a margin of ? as follows:</p><formula xml:id="formula_12">L separate = T t K k [ q k t ?p p 2 ? q k t ?p n 2 +?] + ,<label>(12)</label></formula><p>where we set the query q k t , its nearest item p p and the second nearest item p n as an anchor, and positive and hard negative samples, respectively. We denote by n an index of the second nearest item for the query q k t :</p><formula xml:id="formula_13">n = argmax m?M,m =p w k,m t .<label>(13)</label></formula><p>Note that this is different from the typical use of the triplet loss that requires ground-truth positive and negative samples for the anchor. Our loss encourages the query and the second nearest item to be distant, while the query and the nearest one to be nearby. This has the effect of placing the items far away. As a result, the feature separateness loss allows to update the item nearest to the query, whereas discarding the influence of the second nearest item, separating all items in the memory and enhancing the discriminative power.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Abnormality score</head><p>We quantify the extent of normalities or abnormalities in a video frame at test time. We assume that the queries obtained from a normal video frame are similar to the memory items, as they record prototypical patterns of normal data. We compute the L2 distance between each query and the nearest item as follows:</p><formula xml:id="formula_14">D(q t , p) = 1 K K k q k t ? p p 2 .<label>(14)</label></formula><p>We also exploit the memory items implicitly to compute the abnormality score. We measure how well the video frame is reconstructed using the memory items. This assumes that anomalous patterns in the video frame are not reconstructed by the memory items. Following <ref type="bibr" target="#b21">[22]</ref>, we compute the PSNR between the input video frame and its reonstruction:</p><formula xml:id="formula_15">P (? t , I t ) = 10 log 10 max(? t ) ? t ? I t 2 2 /N.<label>(15)</label></formula><p>where N is the number of pixels in the video frame. When the frame I t is abnormal, we obtain a low value of PSNR and vice versa. Following <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b25">26]</ref>, we normalize each error in <ref type="formula" target="#formula_4">(14)</ref> and <ref type="bibr" target="#b14">(15)</ref> in the range of [0, 1] by a min-max normalization <ref type="bibr" target="#b21">[22]</ref>. We define the final abnormality score S t for each video frame as the sum of two metrics, balanced by the parameter ?, as follows:</p><formula xml:id="formula_16">S t = ?(1 ? g(P (? t , I t ))) + (1 ? ?)g(D(q t , p)),<label>(16)</label></formula><p>where we denote by g(?) the min-max normalization <ref type="bibr" target="#b21">[22]</ref> over whole video frames, e.g., g(D(q t , p)) = D(q t , p) ? min t (D(q t , p) max t (D(q t , p)) ? min t <ref type="figure">(D(q t , p)</ref>) . Training. We resize each video frame to the size of 256 ? 256 and normalize it to the range of [-1, 1]. We set the height H and the width W of the query feature map, and the numbers of feature channels C and memory items M to 32, 32, 512 and 10, respectively. We use the Adam optimizer <ref type="bibr" target="#b15">[16]</ref> with ? 1 = 0.9 and ? 2 = 0.999, with a batch size of 4 for 60, 60, and 10 epochs on UCSD Ped2 <ref type="bibr" target="#b20">[21]</ref>, CUHK Avenue <ref type="bibr" target="#b23">[24]</ref>, and ShanghaiTech <ref type="bibr" target="#b25">[26]</ref>, respectively. We set initial learning rates to 2e-5 and 2e-4, respectively, for reconstruction and prediction tasks, and decay them using a cosine annealing method <ref type="bibr" target="#b22">[23]</ref>. For the reconstruction task, we use a grid search to set hyper-parameters on the test split of UCSD Ped1 <ref type="bibr" target="#b20">[21]</ref>: ? c = 0.01, ? s = 0.01, ? = 0.7, ? = 1 and ? = 0.015. We use different parameters for the prediction task similarly chosen using a grid search: ? c = 0.1, ? s = 0.1, ? = 0.6, ? = 1 and ? = 0.01. All models are trained end-to-end using PyTorch <ref type="bibr" target="#b31">[32]</ref>, taking about 1, 15 and 36 hours for UCSD Ped2, CUHK Avenue, and Shang-haiTech, respectively, with an Nvidia GTX TITAN Xp.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Results</head><p>Comparison with the state of the art. We compare in <ref type="table">Table 1</ref> our models with the state of the art for anomaly detection on UCSD Ped2 <ref type="bibr" target="#b20">[21]</ref>, CUHK Avenue <ref type="bibr" target="#b23">[24]</ref>, and ShanghaiTech <ref type="bibr" target="#b25">[26]</ref>. Following the experimental protocol in <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b25">26]</ref>, we measure the average area under curve (AUC) by computing the area under the receiver operation characteristics (ROC) with varying threshold values for abnormality scores. We report the AUC performance of our models using memory modules for the tasks of frame reconstruction and future frame prediction. For comparison, we also provide the performance without the memory module. The suffices '-R' and '-P' indicate the reconstruction and prediction tasks, respectively. From the table, we observe three things: (1) Our model with the prediction task (Ours-P w/ Mem.) gives the best results on UCSD Ped2 and CUHK Avenue, achieving the average AUC of 97.0% and 88.5%, respectively. This demonstrates the effectiveness of our approach to exploiting a memory module for anomaly detection. Although our method is outperformed by Frame-Pred <ref type="bibr" target="#b21">[22]</ref> on Shang- <ref type="table">Table 1</ref>: Quantitative comparison with the state of the art for anomaly detection. We measure the average AUC (%) on UCSD Ped2 <ref type="bibr" target="#b20">[21]</ref>, CUHK Avenue <ref type="bibr" target="#b23">[24]</ref>, and ShanghaiTech <ref type="bibr" target="#b25">[26]</ref>. Numbers in bold indicate the best performance and underscored ones are the second best.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>Ped2 <ref type="bibr" target="#b20">[21]</ref> Avenue <ref type="bibr" target="#b23">[24]</ref> Shanghai <ref type="bibr" target="#b25">[26]</ref> -MPPCA <ref type="bibr" target="#b14">[15]</ref> 69.3 --MPPC+SFA <ref type="bibr" target="#b14">[15]</ref> 61.3 --MDT <ref type="bibr" target="#b27">[28]</ref> 82.9 --AMDN <ref type="bibr" target="#b45">[46]</ref> 90.8 --Unmasking <ref type="bibr" target="#b40">[41]</ref> 82.2 80.6 -MT-FRCN <ref type="bibr" target="#b9">[10]</ref> 92. haiTech, it uses additional modules for estimating optical flow, which requires more network parameters and groundtruth flow fields. Moreover, Frame-Pred leverages an adversarial learning framework, taking lots of effort to train the network. On the contrary, our model uses a simple AE for extracting features and predicting the future frame, and thus it is much faster than Frame-Pred (67 fps vs. 25 fps). This suggests that our model offers a good compromise in terms of AUC and runtime;</p><p>(2) Our model with the reconstruction task (Ours-R w/ Mem.) shows the competitive performance compared to other reconstructive methods on UCSD Ped2, and outperforms them on other datasets, except MemAE <ref type="bibr" target="#b7">[8]</ref>. Note that MemAE exploits 3D convolutions with 2,000 memory items of size 256. On the contrary, our model uses 2D convolutions and it requires 10 items of size 512. It is thus computationally much cheaper than MemAE: 67 fps for our model vs. 45 fps for MemAE;</p><p>(3) Our memory module boosts the AUC performance significantly regardless of the tasks on all datasets. For example, the AUC gains are 2.7%, 4.0%, and 3.7% on UCSD Ped2, CUHK Avenue, and ShanghaiTech, respectively, for the prediction task. This indicates that the memory module is generic and it can be added to other anomaly detection methods.</p><p>Runtime. With an Nvidia GTX TITAN Xp, our current implementation takes on average 0.015 seconds to determine abnormality for an image of size 256 ? 256 on UCSD Ped2 <ref type="bibr" target="#b20">[21]</ref>. Namely, we achieve 67 fps for anomaly detection, which is much faster than other state-of-the-art meth- <ref type="figure">Figure 4</ref>: Qualitative results for future frame prediction on (top to bottom) UCSD Ped2 <ref type="bibr" target="#b20">[21]</ref>, CUHK Avenue <ref type="bibr" target="#b23">[24]</ref>, and Shang-haiTech <ref type="bibr" target="#b25">[26]</ref>: input frames (left); prediction error (middle); abnormal regions (right). We can see that our model localizes the regions of abnormal events. Best viewed in color.</p><p>ods based on CNNs, e.g., 20 fps for Unmasking <ref type="bibr" target="#b40">[41]</ref>, 50 fps for StackRNN <ref type="bibr" target="#b25">[26]</ref>, 25 fps for Frame-Pred <ref type="bibr" target="#b21">[22]</ref>, and 45 fps for MemAE <ref type="bibr" target="#b7">[8]</ref> with the same setting as ours.</p><p>Qualitative results. We show in <ref type="figure">Fig. 4</ref> qualitative results of our model for future frame prediction on UCSD Ped2 <ref type="bibr" target="#b20">[21]</ref>, CUHK Avenue <ref type="bibr" target="#b23">[24]</ref>, and ShanghaiTech <ref type="bibr" target="#b25">[26]</ref>. It shows input frames, prediction error, and abnormal regions overlaid to the frame. For visualizing the anomalies, we compute pixel-wise abnormality scores similar to <ref type="bibr" target="#b15">(16)</ref>. We then mark the regions whose abnormality scores are larger than the average value within the frame. We can see that 1) normal regions are predicted well, while abnormal regions are not, and 2) abnormal events, such as the appearance of vehicle, jumping and fight on UCSD Ped2, CUHK Avenue, and ShanghaiTech, respectively, are highlighted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Discussions</head><p>Ablation study. We show an ablation analysis on different components of our models in <ref type="table" target="#tab_2">Table 2</ref>. We report the AUC performance for the variants of our models for reconstruction and prediction tasks on UCSD Ped2 <ref type="bibr" target="#b20">[21]</ref>. As the AUC performance of both tasks shows a similar trend, we describe the results for the frame reconstruction in detail.</p><p>We train the baseline model in the first row with the reconstruction loss, and use PSNR only to compute abnormality scores. From the second row, we can see that our model with the memory module gives better results. The third row shows that the AUC performance even drops when the feature compactness loss is additionally used, as the memory  items are not discriminative. The last row demonstrates that the feature separateness loss boosts the performance drastically. It provides the AUC gain of 3.8%, which is quite significant. The last four rows indicate that 1) feature compactness and separateness losses are complementary, 2) updating the memory item using E t with normal frames only at test time largely boosts the AUC performance, and 3) our abnormality score S t , using both PSNR and memory items, quantifies the extent of anomalies better than the one based on PSNR only. Memory items. We visualize in <ref type="figure" target="#fig_2">Fig. 5</ref> matching probabilities in (1) from the model trained with/without the feature separateness loss for the reconstruction task on UCSD Ped2 <ref type="bibr" target="#b20">[21]</ref>. We observe that each query is highly activated on a few items with the separateness loss, demonstrating that the items and queries are highly discriminative, allowing the sparse access of the memory. This also indicates that abnormal samples are not likely to be reconstructed with a combination of memory items. Feature distribution. We visualize in <ref type="figure">Fig. 6</ref> the distribution of query features for the reconstruction task, randomly chosen from UCSD Ped2 <ref type="bibr" target="#b20">[21]</ref>, learned with and without the feature separateness loss. We can see that our model trained <ref type="figure">Figure 6</ref>: t-SNE <ref type="bibr" target="#b41">[42]</ref> visualization for query features and memory items. We randomly sample 10K query features, learned with (left) and without (right) the feature separateness loss, from UCSD Ped2 <ref type="bibr" target="#b20">[21]</ref>. The features and memory items are shown in points and stars, respectively. The points with the same color are mapped to the same item. The feature separateness loss enables separating the items, recording the diverse prototypes of normal data. Best viewed in color.</p><p>without the separateness loss loses the discriminability of memory items, and thus all features are mapped closely in the embedding space. The separateness loss allows to separate individual items in the memory, suggesting that it enhances the discriminative power of query features and memory items significantly. We can also see that our model gives compact feature representations.</p><p>Reconstruction with motion cues. Following <ref type="bibr" target="#b7">[8]</ref>, we use multiple frames for the reconstruction task. Specifically, we input sixteen successive video frames to reconstruct the ninth one. This achieves AUC of 91.0% for UCSD Ped2, providing the AUC gain of 0.8% but requiring more network parameters (?4MB).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We have introduced an unsupervised learning approach to anomaly detection in video sequences that exploits multiple prototypes to consider the various patterns of normal data. To this end, we have suggested to use a memory module to record the prototypical patterns to the items in the memory. We have shown that training the memory using feature compactness and separateness losses separates the items, enabling the sparse access of the memory. We have also presented a new memory update scheme when both normal and abnormal samples exist, which boosts the performance of anomaly detection significantly. Extensive experimental evaluations on standard benchmarks demonstrate the our model outperforms the state of the art.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Distributions of features and memory items of our model on CUHK Avenue</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>1 DecoderFigure 2 :</head><label>12</label><figDesc>Overview of our framework for reconstructing a video frame. Our model mainly consists of three parts: an encoder, a memory module, and a decoder. The encoder extracts a query map qt of size H ? W ? C from an input video frame It at time t.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :</head><label>5</label><figDesc>Visualization of matching probabilities in (1) learned with (left) and without (right) the feature separateness loss (blue: low, yellow: high). We randomly select 10 query features for the purpose of visualization. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Quantitative comparison for variants of our model. We measure the average AUC (%) on UCSD Ped2<ref type="bibr" target="#b20">[21]</ref>.</figDesc><table><row><cell>Task</cell><cell cols="3">Memory Lcompact Lseparate Et module</cell><cell cols="2">St Ped2 [21]</cell></row><row><cell></cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>86.4</cell></row><row><cell>Recon.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>86.9 86.4 89.3</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>87.1</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>89.0</cell></row><row><cell></cell><cell>Query features</cell><cell></cell><cell cols="2">Query features</cell><cell>90.2</cell><cell>Query features</cell></row><row><cell></cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>94.3</cell></row><row><cell>Pred.</cell><cell>Memory items</cell><cell></cell><cell>Memory items</cell><cell></cell><cell>95.0 94.8 96.5 96.0 95.7</cell><cell>Memory items</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>97.0</cell></row><row><cell></cell><cell>Query features</cell><cell></cell><cell cols="2">Query features Query features</cell><cell></cell><cell>Query features Query features</cell><cell>Query features</cell></row><row><cell></cell><cell>Memory items</cell><cell>Memory items</cell><cell>Memory items</cell><cell></cell><cell>Memory items</cell><cell>Memory items</cell><cell>Memory items</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Query features</cell><cell></cell><cell>Query features</cell><cell>Query features</cell></row><row><cell></cell><cell></cell><cell>Memory items</cell><cell></cell><cell></cell><cell>Memory items</cell><cell>Memory items</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments. This research was partly supported by Samsung Electronics Company, Ltd., Device Solutions under Grant, Deep Learning based Anomaly Detection, 20182020, and R&amp;D program for Advanced Integratedintelligence for Identification (AIID) through the National Research Foundation of KOREA(NRF) funded by Ministry of Science and ICT (NRF-2018M3E3A1057289).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Greedy layer-wise training of deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Lamblin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Popovici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Memory matching networks for one-shot image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingwei</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenggang</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Deep learning for anomaly detection: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raghavendra</forename><surname>Chalapathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjay</forename><surname>Chawla</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.03407</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Video anomaly detection and localization using hierarchical feature representation and gaussian process regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai-Wen</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yie-Tarng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Hsien</forename><surname>Fang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Abnormal event detection in videos using spatiotemporal autoencoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Shean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong Haur</forename><surname>Tay</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Sparse reconstruction cost for abnormal event detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Cong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsong</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Heterogeneous memory enhanced multimodal attention model for video question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenyou</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wensheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Moussa Reda Mansour, Svetha Venkatesh, and Anton van den Hengel. Memorizing normality to detect anomaly: Memory-augmented deep autoencoder for unsupervised anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingqiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vuong</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Budhaditya</forename><surname>Saha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning temporal regularity in video sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahmudul</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonghyun</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Amit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry S</forename><surname>Roy-Chowdhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Joint detection and recounting of abnormal events by learning deep generic knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryota</forename><surname>Hinami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shin&amp;apos;ichi</forename><surname>Satoh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning to remember rare events</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ofir</forename><surname>Nachum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aurko</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Ioannis Kompatsiaris, Leontios J Hadjileontiadis, and Michael Gerasimos Strintzis. Swarm intelligence for detecting interesting events in crowded environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vagia</forename><surname>Kaltsa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexia</forename><surname>Briassouli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Observe locally, infer globally: a space-time MRF for detecting abnormal activities with incremental updates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaechul</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristen</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Auto-encoding variational bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6114</idno>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">ImageNet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Ask me anything: Dynamic memory networks for natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankit</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ozan</forename><surname>Irsoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Ondruska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishaan</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Romain</forename><surname>Paulus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A memory network approach for story-based temporal summarization of 360 videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangho</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinyoung</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjae</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gunhee</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Anomaly detection and localization in crowded scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weixin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Mahadevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nuno</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Future frame prediction for anomaly detection-a new baseline</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weixin</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongze</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenghua</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">SGDR: Stochastic gradient descent with warm restarts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.03983</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Abnormal event detection at 150 FPS in MATLAB</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Remembering history with convolutional lstm for anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weixin</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenghua</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICME</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A revisit of sparse coding based anomaly detection in stacked RNN framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weixin</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenghua</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Time-series novelty detection using one-class support vector machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junshui</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Perkins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCNN</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Viral Bhalodia, and Nuno Vasconcelos. Anomaly detection in crowded scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Mahadevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weixin</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Anomaly detection in video using predictive convolutional long short-term memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jefferson</forename><surname>Ryan Medel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Savakis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.00390</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Key-value memory networks for directly reading documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Fisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesse</forename><surname>Dodge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Amir-Hossein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Karimi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Anomaly detection in video sequence with appearance-motion correspondence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Trong-Nguyen Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Meunier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Automatic differentiation in PyTorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Abnormal event detection in videos using generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahdyar</forename><surname>Ravanbakhsh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moin</forename><surname>Nabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enver</forename><surname>Sangineto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucio</forename><surname>Marcenaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlo</forename><surname>Regazzoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicu</forename><surname>Sebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICIP</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">U-Net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Deep one-class classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Ruff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Vandermeulen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nico</forename><surname>Goernitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Deecke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICML</title>
		<editor>Shoaib Ahmed Siddiqui, Alexander Binder, Emmanuel M?ller, and Marius Kloft</editor>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Real-time anomaly detection and localization in crowded scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Sabokrou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahmood</forename><surname>Fathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mojtaba</forename><surname>Hoseini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reinhard</forename><surname>Klette</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPRW</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Deep-Cascade: Cascading 3d deep neural networks for fast anomaly detection and localization in crowded scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Sabokrou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohsen</forename><surname>Fayyaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahmood</forename><surname>Fathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reinhard</forename><surname>Klette</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Meta-learning with memory-augmented neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Bartunov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Botvinick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Lillicrap</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICML</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Estimating the support of a high-dimensional distribution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Sch?lkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Platt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">J</forename><surname>Shawe-Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert C</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Williamson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Endto-end memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sainbayar</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Unmasking the abnormal events in video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sorina</forename><surname>Radu Tudor Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bogdan</forename><surname>Smeureanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Alexe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Popescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Accelerating t-SNE using treebased algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Shape Activity&quot;: A continuous-state hmm for moving/deforming shapes with application to abnormal activity detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Namrata</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Amit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rama</forename><surname>Roy-Chowdhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">A discriminative feature learning approach for deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yandong</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaipeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Detecting anomalous events in videos by learning deep representations of appearance and motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elisa</forename><surname>Ricci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicu</forename><surname>Sebe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
		<respStmt>
			<orgName>CVIU</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Joint unsupervised learning of deep representations and image clusters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Deep structured energy based models for anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuangfei</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weining</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongfei</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.07717</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Online detection of unusual events in videos via dynamic sparse coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Spatio-temporal autoencoder for video anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiru</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongtao</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xian-Sheng</forename><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">DM-GAN: Dynamic memory generative adversarial networks for text-to-image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minfeng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pingbo</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
