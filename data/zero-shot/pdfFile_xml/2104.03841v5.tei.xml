<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ORBIT: A Real-World Few-Shot Dataset for Teachable Object Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniela</forename><surname>Massiceti</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luisa</forename><surname>Zintgraf</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Oxford</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Bronskill</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">University of Cambridge</orgName>
								<address>
									<addrLine>4 City</addrLine>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">University of London</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lida</forename><surname>Theodorou</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">Tobias</forename><surname>Harris</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Cutrell</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cecily</forename><surname>Morrison</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katja</forename><surname>Hofmann</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simone</forename><surname>Stumpf</surname></persName>
						</author>
						<title level="a" type="main">ORBIT: A Real-World Few-Shot Dataset for Teachable Object Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.25383/city.14294597</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T07:41+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Object recognition has made great advances in the last decade, but predominately still relies on many high-quality training examples per object category. In contrast, learning new objects from only a few examples could enable many impactful applications from robotics to user personalization. Most few-shot learning research, however, has been driven by benchmark datasets that lack the high variation that these applications will face when deployed in the real-world. To close this gap, we present the ORBIT dataset and benchmark, grounded in the real-world application of teachable object recognizers for people who are blind/low-vision. The dataset contains 3,822 videos of 486 objects recorded by people who are blind/low-vision on their mobile phones. The benchmark reflects a realistic, highly challenging recognition problem, providing a rich playground to drive research in robustness to few-shot, high-variation conditions. We set the benchmark's first state-of-the-art and show there is massive scope for further innovation, holding the potential to impact a broad range of real-world vision applications including tools for the blind/low-vision community. We release the dataset at https://doi.org/10.25383/city.14294597 and benchmark code at https://github.com/microsoft/ORBIT-Dataset.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Object recognition systems have made spectacular advances in recent years <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b35">36]</ref> however, most systems still rely on training datasets with 100s to 1,000s of high-quality, labeled examples per object category. These demands make training datasets expensive to collect, and limit their use to all but a few application areas.</p><p>Few-shot learning aims to reduce these demands by training models to recognize completely novel objects from only a few examples <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b45">46]</ref>. This will enable recognition systems that can adapt in real-world, dynamic scenarios, from self-driving cars to applications where users provide the training examples themselves. Meta-learning algorithms which "learn to learn" <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b10">11]</ref> hold partic-  Most few-shot learning research, however, has been driven by datasets that lack the high variation -in number of examples per object and quality of those examples (framing, blur, etc.; see <ref type="table" target="#tab_1">Table 1</ref>) -that recognition systems will likely face when deployed in the real-world. Key datasets such as Omniglot <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b48">49]</ref> and miniImageNet <ref type="bibr" target="#b48">[49]</ref>, for example, present highly structured benchmark tasks which assume a fixed number of objects and training examples per object. Meta-Dataset <ref type="bibr" target="#b47">[48]</ref>, another key dataset, poses a more challenging benchmark task of adapting to novel datasets given a small (random) number of training examples. Its constituent datasets <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b5">6]</ref>, however, mirror the high-quality images of Omniglot and miniImageNet, leaving robustness to the noisy frames that would be streamed from a real-world system unaddressed. While these datasets have catalyzed research in few-shot learning, state-of-the-art performance is now relatively saturated and leaves reduced scope for algorithmic innovation <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b32">33]</ref>.</p><p>To drive further innovation in few-shot learning for realworld impact, there is a strong need for datasets that capture the high variation inherent in real-world applications. We motivate that both the dataset and benchmark task should be grounded in a potential real-world application to bring real-world recognition challenges to life in their entirety. An application area that neatly encapsulates a few-shot, highvariation scenario are teachable object recognisers (TORs) for people who are blind/low-vision <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b17">18]</ref>. Here, a user can customize an object recognizer by capturing a small number of (high-variation) training examples of essential objects on their mobile phone. The recognizer is then trained (in deployment) on these examples such that it can recognize the user's objects in novel scenarios. As a result, TORs capture a microcosm of highly challenging and realistic conditions that can be used to drive research in real-world recognition tasks, with the potential to impact a broad range of applications beyond just tools for the blind/low-vision community.</p><p>We introduce the ORBIT dataset <ref type="bibr" target="#b30">[31]</ref>, a collection of videos recorded by people who are blind/low-vision on their mobile phones, and an associated few-shot benchmark grounded in TORs. Both were designed in collaboration with a team of machine learning (ML), human-computer interaction, and accessibility researchers, and will enable the ML community to 1) accelerate research in few-shot, highvariation object recognition, and 2) explore new research directions in few-shot video recognition. We intend both as a rich playground to drive research in robustness to challenging, real-world conditions, a step beyond what curated few-shot datasets and structured benchmark tasks can offer, and to ultimately impact a broad range of real-world vision applications. In summary, our contributions are: 1. ORBIT benchmark dataset. The ORBIT benchmark dataset <ref type="bibr" target="#b30">[31]</ref> (Section 3) is a collection of 3822 videos of 486 objects recorded by 77 blind/low-vision people on their mobile phones and can be downloaded at https://doi.org/10.25383/city.14294597. Examples are shown in Figures 1 and A.5. Unlike existing datasets <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b47">48]</ref>, ORBIT show objects in a wide range of realistic conditions, including when objects are poorly framed, occluded by hands and other objects, blurred, and in a wide variation of backgrounds, lighting, and object orientations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">ORBIT teachable object recognition benchmark.</head><p>We formulate a few-shot benchmark on the ORBIT dataset (Section 4) that is grounded in TORs for people who are blind/low-vision. Contrasting existing few-shot (and other) works, the benchmark proposes a novel user-centric formulation which measures personalization to individual users. It also incorporates metrics that reflect the potential computational cost of real-world deployment on a mobile device. These and the benchmark's other metrics are specifically designed to drive innovation for realistic settings.</p><p>3. State-of-the-art (SOTA) on the ORBIT benchmark. We implement 4 few-shot learning models that cover the main classes of approach in the field, extend them to videos, and establish the first SOTA on the ORBIT benchmark (Sec-tion 5). We also perform empirical studies showing that training on existing few-shot learning datasets is not sufficient for good performance on the ORBIT benchmark (Table 4) leaving significant scope for algorithmic innovation in few-shot techniques that can handle high-variation data.</p><p>Code for loading the dataset, computing benchmark metrics, and running the baselines is available at https://github.com/microsoft/ORBIT-Dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Few-shot learning datasets.</p><p>Omniglot <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b48">49]</ref>, miniImageNet <ref type="bibr" target="#b48">[49]</ref>, and Meta-Dataset <ref type="bibr" target="#b47">[48]</ref> have driven recent progress in few-shot learning. Impressive gains have been achieved on Omniglot and miniImageNet <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b32">33]</ref>, however results are now largely saturated and highly depend on the selected feature embedding. Meta-Dataset, a dataset of 10 datasets, formulates a more challenging task where whole datasets are held-out, but these datasets contain simple and clean images, such as clipart drawings of characters/symbols <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b16">17]</ref>, and ImageNet-like images <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b5">6]</ref> showing objects in uniform lighting, orientations, and camera viewpoints. The ORBIT dataset and benchmark presents a more challenging few-shot task with high-variation examples captured in real-world scenarios. High-variation datasets. Datasets captured by users in real-world settings are naturally high-variation <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b12">13]</ref>, but none collected thus far explicitly target few-shot object recognition. ObjectNet <ref type="bibr" target="#b0">[1]</ref> is a test-only dataset of challenging images (e.g. unusual orientations/backgrounds) for "many-shot" classification. Something-Something <ref type="bibr" target="#b11">[12]</ref> and EPIC-Kitchens <ref type="bibr" target="#b6">[7]</ref> are video datasets collected by users with mobile and head-mounted cameras, respectively, but are focused on action recognition based on many examples and "action captions". Core50 <ref type="bibr" target="#b26">[27]</ref> is a video dataset captured on mobile phones for a continual learning recognition task. In contrast to ORBIT, the videos are high quality (captured by sighted people, with well-lit centered objects). Other high-variation datasets include those collected by people who are blind/lowvision <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b12">13]</ref> (see IncluSet for a repository of accessibility datasets <ref type="bibr" target="#b18">[19]</ref>) however, most are not appropriate for few-shot learning. TeGO <ref type="bibr" target="#b17">[18]</ref> contains mobile phone images of 19 objects taken by only 2 users (1 sighted, 1 blind) in 2 environments (1 uniform background, 1 cluttered scene). It validates the TOR use-case, but is too small to deliver a robust, deployable system. VizWiz <ref type="bibr" target="#b12">[13]</ref>, although larger scale <ref type="bibr" target="#b30">(31,</ref><ref type="bibr">173</ref> mobile phone images contributed by 11,045 blind/low-vision users) targets image captioning and question-answering tasks, and is not annotated with object labels. The ORBIT dataset and benchmark is motivated by the lack of datasets that have the scale and structure required for few-shot, high-variation real-world applications, and adds to the growing repository of datasets for accessibility.</p><p>Omniglot <ref type="bibr" target="#b22">[23]</ref> miniImageNet <ref type="bibr" target="#b48">[49]</ref> Meta-Dataset <ref type="bibr" target="#b47">[48]</ref> TEgO <ref type="bibr" target="#b23">[24]</ref> ORBIT Benchmark  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">ORBIT Benchmark Dataset</head><p>Our goal is to drive research in recognition tasks under few-shot, high-variation conditions so that deployed fewshot systems are robust to such conditions. Toward this goal, we focus on a real-world application that serves as a microcosm of a few-shot, high-variation setting -TORs for people who are blind/low-vision -and engage the blind/lowvision community in collecting a large-scale dataset.</p><p>The collection took place in two phases, and collectors recorded and submitted all videos (completely anonymously) via an accessible iOS app (see Appendix A.2). The collection protocol was designed and validated through extensive user studies <ref type="bibr" target="#b43">[44]</ref> and led to the key decision to capture videos rather than images of objects. This was based on the hypothesis that a video increases a blind collector's chances of capturing frames that contained the object while reducing the time/effort cost to the collector, compared to multiple attempts at a single image. The study was approved by the City, University of London Research Ethics Committee. The full data collection protocol is described in Appendix A.1 and a datasheet <ref type="bibr" target="#b9">[10]</ref> for the dataset is included in Appendix E.</p><p>We summarize the benchmark dataset in <ref type="table" target="#tab_4">Table 2</ref> and describe it in detail below (see Appendix B for dataset preparation, and Appendix C for example clips). The benchmark dataset is used to run the benchmark described in Section 4. Number of collectors. Globally, 77 collectors contributed to the ORBIT benchmark dataset. Collectors who contributed only 1 object were merged to enforce a minimum of 3 objects per user such that the per-user classification task was a minimum of 3-way, resulting in an effective 67 users. Numbers of videos and objects. Collectors contributed a total of 486 objects and 3,822 videos (2,687,934 frames, 83GB). 2,996 videos showed the object in isolation, referred to as clean videos, while 826 showed the object in a realistic, multi-object scene, referred to as clutter videos. We collected both types to match what a TOR will encounter in the real-world (see Section 4.2.2). Each collector contributed on average 7.3 (?2.8) objects, with 5.8 (?3.9) clean videos and 1.8 (?1.1) clutter videos per object. <ref type="figure" target="#fig_5">Figure 2</ref> shows the number of objects (2a) and number of videos per collector (2b). We discuss the impact of the 2 collectors who contributed more videos than the average collector in Appendix B.3.       </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Teachable Object Recognition Benchmark</head><p>The ORBIT dataset can be used to explore a wide set of real-world recognition tasks from continual learning <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28]</ref> to video segmentation <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b28">29]</ref>. In this paper, we focus on few-shot object recognition from high-variation examples and present a realistic and challenging few-shot benchmark grounded in TORs for people who are blind/low-vision.</p><p>In Section 4.1, we describe how a TOR works, mapping it to a few-shot learning problem, before presenting the benchmark's evaluation protocol and metrics in Section 4.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Teachable Object Recognition</head><p>We define a TOR as a generic recognizer that can be customized to a user's personal objects using a small number of training examples -in our case, videos -which the user has captured themselves. The 3 steps to realizing a TOR are: (1) Train. A recognition model is trained on a large dataset of objects where each object has only a few examples. The model can be optimized to either i) directly recognize a set of objects <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b4">5]</ref> or ii) learn how to recognize a set of objects (i.e. meta-learn) <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b37">38]</ref>. This happens before deploying the model in the real world. recognizer to identify their personal objects in novel (test) scenarios. As the user points their recognizer at a scene, it delivers frame-by-frame predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">TORs as a few-shot learning problem</head><p>The (1) train step of a TOR can be mapped to the 'metatraining' phase typically used in few-shot learning set-ups. The (2) personalize and (3) recognize steps can be mapped to 'meta-testing' (see <ref type="figure">Figure 3</ref>). With this view, we now formalize the teachable object recognition task, drawing on nomenclature from the few-shot literature <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b10">11]</ref>. We construct a set of train users K train and test users K test (K train ? K test = ?) akin to the train and test object classes used in few-shot learning. A user ? has a set of personal objects P ? that they want a recognizer to identify, setting up a |P ? |-way classification problem. To this end, the user captures a few videos of each object, together called the user's "context" set</p><formula xml:id="formula_0">C ? = {(v, p) i } N i=1 ,</formula><p>wherev is a context video, p ? P ? is its object label, and N is the total number of the user's context videos. The goal is to use C ? to learn a recognition model f ? ? that can identify the user's objects, where ? ? are the model parameters specific to user ?.</p><p>Once personalized, the user can point their recognizer at novel "target" scenarios to receive per-frame predictions:</p><formula xml:id="formula_1">y * f = arg max y f ?P ? f ? ? (v f ) v f ? v (v, p) ? T ? (1)</formula><p>where v f is a target frame, v is a target video, T ? is all the user's target videos, and y f ? P ? is the frame-level label. <ref type="bibr" target="#b0">1</ref> Following the typical paradigm, during meta-training (i.e. the train step), multiple tasks are sampled per user ? ? K train where a task is a random sub-sample of the user's C ? and T ? (see Appendix G.2). The recognition model can be trained on these tasks using an episodic <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b37">38]</ref> or non-episodic approach <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b21">22]</ref>. We formalize both in the context of TORs in Appendix F. Then, at meta-testing, one task is sampled per test user ? ? K test containing all the user's context and target videos. For each test user, the recognizer is personalized using all their context videos C ? (i.e. the personalize step), and then evaluated on each of the user's target videos in T ? (i.e. the recognize step). In the following section, we discuss this evaluation protocol. <ref type="bibr">*</ref>   <ref type="figure">Figure 3</ref>: Teachable object recognizers cast as a few-shot learning problem. P is the personalization method, for example, several gradient steps using a optimization-based approach, or parameter generation using a model-based approach (see Section 5.1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Evaluation protocol</head><p>ORBIT's evaluation protocol is designed to reflect how well a TOR will work in the hands of a real-world userboth in terms of performance and computational cost to personalize. To achieve this, we test (and train) in a user-centric way where tasks are sampled per-user (that is, only from a given user's objects and its associated context/target videos). This contrasts existing few-shot (and other) benchmarks, and offers powerful insights into how well a meta-trained TOR can personalize to a single user.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Train/validation/test users</head><p>The user-centric formulation in Section 4.1.1 calls for a disjoint set of train users K train and test users K test . We therefore separate the 67 ORBIT collectors into 44 train users and 17 test users, with the remaining 6 marked as validation users K val . To ensure the test case is sufficiently challenging, we enforce that test (and validation) users have a minimum of 5 objects (see further details in Appendix B.3). The total number of objects in the splits are 278/50/158, respectively. We report statistics for each set of train/validation/test users in Appendix C, mirroring those over all users in Section 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Evaluation modes</head><p>We establish 2 evaluation modes: Clean video evaluation (CLE-VE). We construct a test user's context set C ? from their clean videos, and target set T ? from a held-out set of their clean videos. This mode serves as a simple check that the user's clean videos can be used to recognize the user's objects in novel 'simple' scenarios when the object is in isolation. Clutter video evaluation (CLU-VE). We construct a test user's context set C ? from their clean videos, and target set T ? from their clutter videos. This mode matches the realworld usage of a TOR where a user captures clean videos to register objects, and needs to identify those objects in complex, cluttered environments. We consider CLU-VE to be ORBIT's primary evaluation mode since it most closely matches how a TOR will be used in the real-world.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">Evaluation metrics</head><p>For a test user ? ? K test , we evaluate their personalized recognizer f ? ? on each of their target videos. We denote a target video of object p ? P ? as v = [v 1 , . . . , v F ], and its frame predictions as y * = [y * 1 , . . . , y * F ], where F is the number of frames and y * f ? P ? . We further denote y * mode as the video's most frequent frame prediction. For a given target video, we compute its:</p><p>Frame accuracy: the number of correct frame predictions, by the total number of frames in the video. Frames-to-recognition (FTR): the number of frames (w.r.t.</p><p>the first frame v 1 ) before a correct prediction is made, by the total number of frames in the video. Video accuracy: 1, if the video-level prediction equals the video-level object label, y * mode = p, otherwise 0. We compute these metrics for each target video in all tasks for all users in K test . We report the average and 95% confidence interval of each metric over this flattened set of videos, denoted T all (see equations in <ref type="table" target="#tab_6">Table 3</ref>). We also compute a further 2 computational cost metrics: MACS to personalize: number of Multiply-Accumulate operations (MACS) to compute a test user's personalized parameters ? ? using their context videos C ? , reported as the average over all tasks pooled across test users. Number of parameters: total parameters in recognizer.</p><p>We flag frame accuracy as ORBIT's primary metric because it most closely matches how a TOR will ultimately be used. The remaining metrics are complementary: FTR captures how long a user would have to point their recognizer at a FRAME ACCURACY (?) scene before it identified the target object (with fewer frames being better) while video accuracy summarizes the predictions over a whole video. MACS to personalize provides an indication whether personalization could happen directly on a user's device or a cloud-based service is required, each impacting how quickly a recognizer could be personalized. The number of parameters indicates the storage and memory requirements of the model on a device, and if cloud-based, the bandwidth required to download the personalized model. It is also useful to normalize performance by model capacity.</p><formula xml:id="formula_2">FRAMES-TO-RECOGNITION (?) VIDEO ACCURACY (?) 1 |T all | (v,p)?T all |v| f =1 1[y * f =p] |v| 1 |T all | (v,p)?T all arg min v f ?v y * f =p |v| 1 |T all | (v,p)?T all 1 y * mode = p y * mode = arg max p?P ? |v| f =1 1[y * f = p]</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experimental analyses and results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Baselines &amp; training set-up</head><p>Baselines. There are 3 main classes of few-shot learning approaches. In metric-based approaches, a per-class embedding is computed using the (labeled) examples in the context set, and a target example is classified based on its distance to each <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b48">49]</ref>. In optimization-based approaches, the model takes many <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b4">5]</ref> or few <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b1">2]</ref> gradient steps on the context examples, and the updated model then classifies the target examples. Finally, in amortization-based approaches, the model uses the context examples to directly generate the parameters of the classifier which is then used to classify a target example <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b10">11]</ref>.</p><p>We establish baselines on the ORBIT dataset across these 3 classes. Within the episodic approaches, we choose Prototypical Nets <ref type="bibr" target="#b39">[40]</ref> for the metric family, MAML <ref type="bibr" target="#b8">[9]</ref> for the optimization family, and CNAPs <ref type="bibr" target="#b37">[38]</ref> for the amortization family. We also implement a non-episodic fine-tuning baseline following <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b4">5]</ref> who show that it can rival more complex methods. This selection of models offers good coverage over those that are competitive on current few-shot learning image classification benchmarks. For all implementation details of these baselines see Appendix G.1.</p><p>Video representation. In Section 4.1.1, tasks are constructed from the context and target videos of a given user's objects. We sample clips from each video and represent each clip as an average over its (learned) frame-level features. For memory reasons, we do not sample all clips from a video. Instead, during meta-training, we randomly sample S train non-overlapping clips, each of L contiguous frames, from both context and target videos. Each clip is averaged and treated as an 'element' in the context/target set, akin to an image in typical few-shot image classification. During meta-testing, however, following Section 4.2 and Eq. (1), we must evaluate a test user's personalized recognizer on every frame in all of their target videos. We, therefore, sample all overlapping clips in a target video, where a clip is an L-sized buffer of each frame plus its short history. Ideally, this should also be done for context videos, however, due to memory reasons, we sample S test non-overlapping L-sized clips from each context video, similar to meta-training. In our baseline implementations, S train = 4, S test = 8, and L = 8 (for further details see Appendices G.2 and G.3).</p><p>How frames are sampled during training/testing, and how videos are represented is flexible. The evaluation protocol's only strict requirement is that a model outputs a prediction for every frame from every target video for every test user.</p><p>Number of tasks per test user. Because context videos are sub-sampled during meta-testing, a test user's task contains a random set, rather than all, context clips. To account for potential variation, therefore, we sample 5 tasks per test user, and pool all their target videos into T all for evaluation. If memory was not a constraint, following Section 4.1.1, we would sample one task per test user which contained all context and all target clips.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Analyses</head><p>Baseline comparison. Performance is largely consistent across the baseline models in both CLE-VE and CLU-VE modes (see <ref type="table" target="#tab_8">Table 4</ref>). In CLE-VE, all methods are equivalent in frame accuracy, FTR and video accuracy, except for ProtoNets and CNAPs which trail slightly in frame accuracy. Comparing this to CLU-VE, we see overall performance drops of 10-15 percentage points. Here, models are overall equivalent on frame and video accuracy, however ProtoNets and FineTuner lead in FTR. Further, absolute CLU-VE scores are in the low 50s. Looking at the best possible bounds (computed using the bounding box annotations, see <ref type="figure">Figure A</ref>.6c) suggests that there is ample scope for improvement and motivates the need for approaches that can handle distribution shifts from clean (context) to real-world, cluttered scenes (target), and are robust to high-variation data more generally.</p><p>In computational cost, ProtoNets has the lowest cost to personalize requiring only a single forward pass of a user's context videos, while FineTuner has the highest, requiring 50 gradient steps. This, along with the total number of parameters (which are similar across models), suggests that ProtoNets and CNAPs would be better suited to deployment on a mobile device.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Meta-training on other few-shot learning datasets.</head><p>A meta-trained model should, in principle, have the ability to learn any new object (from any dataset) with only a few examples. We investigate this by meta-training the baseline models on Meta-Dataset <ref type="bibr" target="#b47">[48]</ref> using its standard task sampling protocol and then testing them on the ORBIT dataset (i.e. personalizing to test users with no training). We adapt the meta-trained models to videos by taking the average over frame features in clips sampled from context and target videos (see Section 5.1). In <ref type="table" target="#tab_9">Table 5</ref>, we see that even on the easier, clean videos (CLE-VE), performance is notably lower than the corresponding baselines in <ref type="table" target="#tab_8">Table 4</ref> (for CLU-VE see <ref type="table" target="#tab_14">Table A</ref>.3). MAML and CNAPs perform particularly poorly while ProtoNets and FineTuner fare slightly better, however, are still 6-8 percentage points below their above counterparts in frame accuracy. This suggests that even though much progress has been made on existing few-shot benchmarks, they are not representative of real-world conditions and models trained on them may struggle to learn new objects when only high-variation examples are available. Per-user performance. In addition to averaging over T all , the benchmark's user-centric paradigm allows us to average per-user (i.e. over just their target videos). This is useful because it provides a measure of how well a meta-trained TOR would personalize to an individual real-world user. In <ref type="figure" target="#fig_8">Figure 4</ref> however, we show that ProtoNets' personalization is not consistent across users, for some going as low as 25% in frame accuracy (for other metrics/models see <ref type="figure">Figure A</ref>.10). A TOR should be able adapt to any real-world user, thus future work should not only aim to boost performance on the metrics but also reduce variance across test users.  Train task composition. Finally, we investigate the impact of the number of context videos per object ( <ref type="figure" target="#fig_9">Figure 5</ref>), and the number of objects per user ( <ref type="figure" target="#fig_11">Figure 6</ref>) sampled in train tasks on CLU-VE frame accuracy. In the first case, we expect that with more context videos per object, the more diversity the model will see during meta-training, and hence generalize better at meta-testing to novel (target) videos. To test this hypothesis, we fix a quota of 96 frames per object in each train task and sample these frames from increasing numbers of context videos. Frame accuracy increases with more context videos, but overall plateaus between 4-6 context videos per object. Looking at the number of objects sampled per user next, we cap all train user's objects at {2, 4, 6, 8}, respectively, when meta-training. We then meta-test in two ways: 1) we keep the caps in place on the test users, and 2) we remove the caps. For 1), we see reducing accuracy for increasing numbers of objects, as is expected -classifying between 8 objects is harder than classifying between 2. For 2), we see a significant drop in accuracy relative to 1) suggesting that meta-training with fewer objects than would be encountered at meta-testing is detrimental. This is an important real-world consideration since it is likely that over months/years, a user will accumulate many more objects than is currently present per user in the ORBIT dataset. Overall, however, training with a cap of 6 or more objects yields    <ref type="table" target="#tab_8">Table 4</ref>, suggesting that models may be able to adapt to more objects in the real-world.</p><formula xml:id="formula_3">P642 P753 P999 P901 P421 P953 P609 P452 P198 P485 P204 P455 P900 P177 P271 P233 P554 0 0.1 0.2</formula><p>roughly equivalent performance to that reported in <ref type="table" target="#tab_8">Table 4</ref> where no caps are imposed during training. Since ORBIT test users have up to 12 objects (see <ref type="figure">Figure A</ref>.3c), our results suggest that a minimum of half the number of ultimate objects for a test user may be sufficient for meta-training. We repeat these analyses for the other metrics in Figures A.8 and A.9, and include the corresponding tables in <ref type="table" target="#tab_9">Tables A.5</ref> and A.6. We also investigate the impact of the number of tasks sampled per train user, included in Appendix H.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Discussion</head><p>We present the ORBIT dataset and benchmark, both grounded in the few-shot application of TORs for people who are blind/low-vision. Our baseline performance and further analyses demonstrate, however, that current few-shot approaches struggle on realistic, high-variation data. This gap offers opportunities for new and exciting research, from making models robust to high-variation video data to quantifying the uncertainty in model predictions. More than just pushing the state-of-the-art in existing lines of thought, the ORBIT dataset opens up new types of challenges that derive from systems that will support human-AI partnership. We close by discussing three of these unique characteristics.</p><p>ORBIT's user-centric formulation provides an opportunity to measure how well the ultimate system will work in the hands of real-world users. This contrasts most few-shot (and other) benchmarks which retain no notion of the end-user. Our results show that the baselines do not perform consistently across users. In the real-world, the heterogeneity of users, their objects, videoing techniques and devices will make this even more challenging. It will therefore be important for models to quantify, explain and ultimately minimize variation across users, particularly as models are deployed in a wider variety of scenarios outside the high-income countries in which the dataset was collected.</p><p>Directly involving users in collecting a dataset intended to drive ML research comes with challenges: user-based datasets are harder to scale than web-scraped datasets <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b47">48]</ref> and users need an understanding of the potential system in order to contribute useful data. Building the system first would address these challenges, but it cannot be done without algorithmic innovation (which itself requires the dataset). The ORBIT dataset is a starting point and can be used to build the first generation of TORs, which can be deployed and themselves be used to collect more real-world data to drive a cycle of innovation between dataset and application.</p><p>Finally, grounding in a real-world application encourages innovation in new directions to meet the real-world conditions of deployment. This could range from new models that are lightweight enough to be personalized directly on a user's phone to new research problems like handling the scenario when none of a user's objects are in the frame.</p><p>In conclusion, the ORBIT dataset and benchmark aims to shape the next generation of recognition tools for the blind/low-vision community starting with TORs, and to improve the robustness of vision systems across a broad range of other applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Unfiltered ORBIT dataset</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1. Dataset collection protocol</head><p>The unfiltered ORBIT dataset was collected in two phases, the first with blind/low-vision collectors based only in the UK, and the second with blind/low-vision collectors globally <ref type="bibr" target="#b43">[44]</ref>. Collectors were recruited via blind charities and networks, and were screened for blindness/low-vision before beginning the collection task. Following this, they were sent instructions for collecting the videos using an accessible iOS app (see Appendix A.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1.1 Phase 1 protocol</head><p>Collectors were asked to record videos for at least 10 objects, including at least 2 large or immovable objects. For each of these objects, they were asked to record two types of videos, train and test which corresponded to clean and clutter, respectively. Collectors were asked to adopt specific video recording techniques to help capture their objects in-frame.</p><p>For clean videos, the collector was asked to place the object on a clear surface with no other objects present and record a video of the object, repeating this on at least 6 different surfaces. Collectors were given step-by-step instructions to record these using a zoom-out technique:</p><p>1. Keep one hand on the surface next to the object as an anchor point to help aim the camera at the object 2. Hold the phone in your other hand and bring it as close as possible to the object 3. Start recording, then slowly draw the phone away from the object until it reaches your body at shoulder height 4. Rotate the object so that a different side is facing you, while returning the phone close to your anchor hand 5. Repeat this at least 3 times for different sides of the object, then stop recording We helped collectors time each rotation by playing a tick sound every 5 seconds, and asking them to record a new side of the object between ticks. To prevent inadvertently long videos, we automatically ended recordings after 2 minutes.</p><p>For large or immovable objects, collectors were asked to position themselves relative to the object so that they were recording different aspects or angles of the object. Again, they were asked to place an anchor hand on three different parts or positions of the object, and then draw their phone slowly towards their body. We asked them to record the object from 3 different angles, repeated this twice, to provide a total of 6 clean videos.</p><p>For clutter videos, collectors were asked to construct a cluttered scene based on a real-life situation in which the recognizer might be used -for example, a surface on which someone else may have placed an object. The scene needed to include at least 5 other distractor objects that did not include any of the collector's selected objects. For immov-able objects like a front door, we asked them to position some likely objects around or in front of it, such as packages, umbrellas, or shoes. Collectors were asked to record their clutter videos using 2 different techniques: a zoom-out and a panning technique, recording at least one video with each technique for each of their objects. For the zoom-out technique, similar to the clean videos, they were asked to place an anchor hand in the scene, and then draw their phone slowly towards their body, before ending the recording. For the panning technique, collectors were asked to place an anchor hand in the scene, making sure the selected object was not directly in front of them. They were then asked to remove their anchor hand, and pan over the scene at shoulder height from right to left by turning their upper body.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1.2 Phase 2 protocol</head><p>In Phase 2, we reduced the number of objects to at least 5 per collector, with no specification for large or immovable objects. We also reduced the number of videos to 5 clean (taken on 5 different surfaces) and 2 clutter videos. For clean videos, collectors were asked to use the zoom-out technique, showing at least four sides of the object. For clutter videos, instead of creating a real-world scene as in Phase 1, collectors were asked to record the object, including its surroundings, in a scene where it would normally be found. For simplicity, the panning technique was dropped, and collectors were asked to record both clutter videos using only the zoom-out technique. For the second clutter video, they were simply asked to record it from a different position. Finally, we reduced the cut-off video time to 30 and 20 seconds for clean and clutter videos, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. ORBIT Camera iOS app</head><p>The dataset was collected using a custom-built iOS app (see <ref type="figure">Figure A</ref>.1) that followed Apple's accessibility guidelines and was tested with blind/low-vision users. On first use, the collector was asked to read an accessible participant information and consent form, and then provide their explicit consent. Once provided, collectors could access the app's home screen and start to add objects to their library. This was done by entering an object's name into a text box at the top of the home screen. This name then became the label for all that object's videos. The collector could then navigate to a second screen to i) record new videos, and ii) review videos already recorded for that object. Here, the collector could also delete and re-record videos already stored. Status indicators were available for each recorded video, including whether it was uploaded to the data server, validated, and visible in the public dataset.</p><p>All contributed videos were uploaded to a data server which provided administrative functionality. Administrators were able to set-up data collection periods, review videos uploaded by collectors, and mark them as reviewed and safe for export. All videos were manually checked to ensure i) the labeled object was present in the video, ii) the video did not contain any personally identifiable information (PII), and iii) the object as well as object name were appropriate. If a video did not meet all of these criteria, it was removed from the server and the collector received a notification via the app to re-record the video. All PII obtained via the consent screen was encrypted, and decryption keys were held by administrators outside the server.</p><p>Code for the app and back-end data server is open-sourced at https://github.com/orbit-a11y. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3. Unfiltered dataset summary</head><p>The unfiltered ORBIT dataset contains 4733 videos (3,161,718 frames, 97GB) of 588 objects, collected by 97 people who are blind/low-vision <ref type="bibr" target="#b30">[31]</ref>. We summarize it in <ref type="table" target="#tab_14">Table A</ref>.1 and describe it in more detail below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Number of collectors 97 blind/low-vision collectors contributed to the unfiltered ORBIT dataset.</head><p>Numbers of videos and objects Collectors contributed a total of 588 objects and 4,733 videos. 3,356 of these videos showed the object in isolation, referred to as clean videos, while 1,377 showed the object in realistic, multiobject scenes, referred to as clutter videos. Of the clutter videos, 873 were recorded with the zoom-out technique, and 504 with the panning technique (see video techniques in Appendix A.1). Each collector contributed on average 6.1 (?3.8) objects, with 34.6 (?41.5) clean videos, 9.0 (?10.5) clutter-zoom-out videos, and 5.2 (?10.5) clutter-pan videos per object. <ref type="figure">Figure A</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. ORBIT benchmark dataset preparation</head><p>The ORBIT benchmark dataset is a subset of the unfiltered ORBIT dataset and was selected to meet the requirements of the ORBIT benchmark (Section 4). The following sections detail the steps to prepare the benchmark dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1. File structure</head><p>The following summarizes the file hierarchy of the OR-BIT benchmark dataset:</p><formula xml:id="formula_4">&gt; mode (train/validation/test) &gt; user ? &gt; object p ? P ? &gt; clean &gt; videov i ? V ? p ? frame v 1 ? ... ? frame v Fi &gt; clutter &gt; video v i ? V ? p B.2</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>. Video filtering</head><p>From the 97 collectors who submitted 4,733 videos (3,356 clean, 873 clutter, 504 clutter-pan) in the unfiltered dataset, the videos of 77 collectors were selected for the benchmark dataset -a total of 3,822 videos (2,996 clean, 826 clutter, 0 clutter-pan) of 486 objects (see <ref type="table" target="#tab_4">Table 2</ref> and <ref type="figure" target="#fig_5">Figure 2</ref>). The following steps detail the video selection procedure:</p><p>1. All clutter-pan videos were removed (504 videos, 4 objects, 0 collectors). This was done to maintain consistency because the panning technique was explored in the first, but not the second, phase of the dataset collection. Clutter-zoom-out videos are, therefore, referred to as simply clutter videos in the benchmark dataset. 2. Videos shorter than 1 second were removed (20 videos, 0 objects, 0 collectors). Extremely short videos were assumed to be a mistaken recording. 3. Objects with &lt; 2 clean videos and &lt; 1 clutter video were remove (387 videos, 101 objects, 20 collectors). These limits were the minimum numbers required for the benchmark evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3. Train/validation/test sets</head><p>Of the 77 collectors remaining, 12 contributed only 1 object. To avoid discarding these videos, we merged these collectors together (or with others who had only contributed 2 objects) to enforce a minimum of 3 objects per collector 2 . This yielded an effective total of 67 benchmark users. These 67 users were then split into train/validation/test users (44/6/17, respectively 3 ) according to the following criteria:    Validation/test user criteria We enforced that validation and test users should have at least 5 objects to ensure the test case was sufficiently challenging. In total, 53 out of 67 collectors met this criteria, from which we randomly sampled 17 test users and 6 validation users. Note, one test user (P204) contributed ?1 order of magnitude more clutter videos per object than the average test user. Since the CLU-VE evaluation mode averages performance over all clutter videos pooled from all tasks across all test users (i.e. T all ), results are likely slightly skewed toward P204. This can be addressed by also reporting the per-user scores as in <ref type="figure" target="#fig_8">Figure 4</ref>. Note, P204's clean videos were inline with the average, thus CLE-VE performance remains unaffected.</p><formula xml:id="formula_5">P642 P753 P452 P901 P573 P407 P953 P193 P999 P177 P235 P194 P900 P744 P638 P463 P806 P231 P485 P867 P421 P980 P204 P653 P778 P400 P455 P613 P609 P198 P100 P587 P544 P697 P800 P277 P241 P996 P271 P447 P233 P470 P418 P303 P883 P554 P579 P767 P189 P174 P833 P543 P282 P685 P141 P106 P398 P284 P954 P538 P865 P665 P332 P423 P164 P295 P159 P417 P304 P699 P641 P259 P610 P715 P473 P456 P199 P903 P379 P403 P652 P468 P936 P845 P834 P238 P574 P202 P704 P160 P298 P120 P766 P180 P448 P631 P260 0</formula><formula xml:id="formula_6">P642 P753 P452 P901 P573 P407 P953 P193 P999 P177 P235 P194 P900 P744 P638 P463 P806 P231 P485 P867 P421 P980 P204 P653 P778 P400 P455 P613 P609 P198 P100 P587 P544 P697 P800 P277 P241 P996 P271 P447 P233 P470 P418 P303 P883 P554 P579 P767 P189 P174 P833 P543 P282 P685 P141 P106 P398 P284 P954 P538 P865 P665 P332 P423 P164 P295 P159 P417 P304 P699 P641 P259 P610 P715 P473 P456 P199 P903 P379 P403 P652 P468 P936 P845 P834 P238 P574 P202 P704 P160 P298 P120 P766 P180 P448 P631 P260</formula><p>Train user criteria The remaining 44 collectors were marked as train users (14 of which did not meet the above criteria). Note, one train user (P587) captured ?1 order of magnitude more clean (though not more clutter) videos than the average train user. Since models are trained on a fixed number of tasks per train user and each task is capped in the number of clean videos sampled per object (see Appendix G.2), the results are not skewed by P587.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.4. Video pre-processing</head><p>Videos were split into frames using ffmpeg at a rate of 30 FPS. Frames were also re-sized from 1080 ? 1080 pixels to 84 ? 84. This was done for GPU memory purposes and future work will look toward scaling to larger images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Extended benchmark dataset summary</head><p>In Section 3, we summarized the benchmark dataset over all 67 users. Here, we report the same summaries but now broken down by train/validation/test users. <ref type="table" target="#tab_4">Table 2</ref>, we report the statistics for train/validation/test users in <ref type="table" target="#tab_14">Table A</ref> Bounding box annotations. We provide bounding box annotations around the ground-truth object (as per the video label) in all clutter videos. We compute the proportion of each clutter video for which the target object is in-frame (identified by if the frame contained a bounding box), and report these for train/validation/test users in Figure A.6. We use the summarization of test users ( <ref type="figure" target="#fig_11">Figure A.6c)</ref> as an upper bound on model performance in <ref type="table" target="#tab_8">Table 4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Number of videos and objects. Mirroring</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Object clustering</head><p>The objects submitted by collectors varied widely in highlevel object categories. For summarization purposes, we grouped objects into clusters based on object similarity.</p><p>Clustering algorithm Each object label (as entered by the user in the app) was converted to lowercase, tokenized and parts-of-speech tagged (using NLTK). A weighted average of all nouns (NN, NNS) in the label was computed (each represented as a FastText <ref type="bibr" target="#b2">[3]</ref> embedding) <ref type="bibr" target="#b3">4</ref> where each weight was the (normalized) frequency of the noun across all nouns in the dataset. Intuitively, this encouraged more <ref type="bibr" target="#b3">4</ref> If the label had no nouns or only 1 word, the whole label was kept. common nouns to dominate the object label's embedding (and down-weighted typos and collectors' idiosyncrasies). We then clustered these embeddings using an agglomerative clustering algorithm based on pair-wise cosine distances (With a distance threshold of 0.4). This produced cluster proposals which we then manually tweaked to obtain the final clusterings shown in <ref type="figure">Figure A</ref> Clusters for <ref type="figure" target="#fig_30">Figure A.7a [97</ref>      <ref type="table" target="#tab_1">P573  P638  P463  P806  P407  P193  P653  P400  P613  P587  P544  P980  P697  P100  P744  P277  P241  P447  P470  P303  P996  P767  P174  P833  P282  P284  P954  P538  P865  P332  P118  P478  P543  P685  P141  P106  P398  P665  P940  P295  P159  P537  P499  P681</ref>               </p><formula xml:id="formula_7">P573 P638 P463 P806 P407 P193 P653 P400 P613 P587 P544 P980 P697 P100 P744 P277 P241 P447 P470 P303 P996 P767 P174 P833 P282 P284 P954 P538 P865 P332 P118 P478 P543 P685 P141 P106 P398 P665 P940 P295 P159 P537 P499 P681</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Datasheet for the ORBIT Dataset</head><p>Here we include a datasheet <ref type="bibr" target="#b9">[10]</ref> for the ORBIT dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.1. Motivation for dataset creation</head><p>Why was the dataset created? (e.g., was there a specific task in mind?; was there a specific gap that needed to be filled?) The ORBIT dataset was created with two aims in mind: 1) to drive research in few-shot object recognition from high-variation videos -compared to existing fewshot learning datasets which contain largely curated images and pose structured benchmark tasks, the ORBIT dataset contains realistic, high-variation videos captured by people who are blind/low-vision on their mobile phones in realworld settings; and 2) to enable the real-world application of teachable object recognisers (TORs) for people who are blind/low-vision.</p><p>Has the dataset been used already? If so, where are the results so others can compare (e.g., links to published papers)? No, this is a new dataset. This paper presents the first results on a few-shot learning benchmark task (see Section 4) that was developed on this dataset.</p><p>What (other) tasks could the dataset be used for? The dataset could be used to explore a wide range of frame-and video-level recognition tasks including classification (this paper), object detection, object tracking, semantic segmentation, and instance segmentation. Within each of these tasks, the dataset lends itself to exploring the robustness of models (e.g. performance, quantification of uncertainty) in the face of few high-variation examples. Beyond a few-shot setting, it could also be used to explore other learning paradigms, for example, continual learning where new objects are incrementally added over time.</p><p>In addition, this dataset supports research from a human and accessibility perspective. It provides evidence of how people who are blind/low-vision take videos, what objects are of interest to them, and how they could be further supported in taking videos. This work could be directly applied in assistive technology to support object recognition, and be extended to other application areas, such as navigation, question-answering, etc.</p><p>Who funded the creation of the dataset? The dataset was funded by the Microsoft AI for Accessibility program.</p><p>Any other comment? None.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.2. Dataset composition</head><p>What are the instances? (that is, examples; e.g. documents, images, people, countries) Are there multiple types of instances? (e.g. movies, users, ratings; people, interactions between them; nodes, edges) The ORBIT dataset is a collection of videos of objects recorded by people who are blind/low-vision on their mobile phones. Each collector recorded 2 types of videos for each object: clean videos which show the object in isolation on a clear surface, and clutter videos which show the object in a realistic, multi-object scene. The clutter videos were recorded with two different techniques: a zoom-out and a panning motion (see Appendix A.1). The dataset is arranged into two parts: the unfiltered ORBIT dataset which contains all videos contributed by collectors, and the ORBIT benchmark dataset which is a subset of the unfiltered dataset and is used to run the benchmark evaluation (see Section 4.2).</p><p>How many instances are there in total (of each type, if appropriate)? The unfiltered dataset has 4,733 videos (3,161,718 frames, 97GB) of 588 objects. 3,356 of these videos are clean videos and 1,377 are clutter videos. Of the clutter videos, 873 were recorded with a zoom-out technique, and 504 with a panning technique. The benchmark dataset has 3,822 videos (2,687,934 frames, 83GB) of 486 objects. 2996 of these videos are clean videos and 826 videos are clutter videos. Here, the clutter videos were all recorded with the zoom-out technique. For detailed statistics of the unfiltered and benchmark datasets, see <ref type="table" target="#tab_1">Table A.1 and Table 2</ref>, respectively.</p><p>What data does each instance consist of? "Raw" data (e.g. unprocessed text or images)? Features/attributes? If the instances related to people, are sub-populations identified (e.g. by age, gender, etc.) and what is their distribution? Each instance is an unprocessed video frame (i.e. a JPEG). Frames are grouped into folders by video. The video folders are then grouped by video type (clean/clutter), then by object, and finally by the (anonymized) collector who contributed them (see file hierarchy in Appendix B.1). Sub-populations of collectors were not identified.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Is there a label or target associated with each instance?</head><p>If so, please provide a description. All frames from a video are labeled with the (video-level) object name entered by the collector prior to recording the video. All clutter videos were further annotated (offline) with per-frame bounding boxes around the target object.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Is any information missing from individual instances?</head><p>If so, please provide a description, explaining why this information is missing (e.g., because it was unavailable). This does not include intentionally removed information, but might include, e.g., redacted text. No information is missing from individual instances (i.e. frames), however, in the collection process, whole videos were removed by a human annotator if: i) the object was not present in the video, ii) the video contained personally identifiable information (PII), or iii) the object or object name were inappropriate. Note, if PII was present in a small number of frames in a video, only these frames were removed.</p><p>Are relationships between individual instances made explicit (e.g. users' movie ratings, social network links)? If so, please describe how these relationships are made explicit. Yes, frames are organized by video, video type, object and finally the (anonymized) collector who contributed them. These relationships are made explicit in the dataset's file structure (see Appendix B.1).</p><p>Does the dataset contain all possible instances or is it a sample (not necessarily random) of instances from a larger set? If the dataset is a sample, then what is the larger set? Is the sample representative of the larger set (e.g. geographic coverage)? If so, please describe how this representativeness was validated/verified. If it is not representative of the larger set, please describe why not (e.g. to cover a more diverse range of instances, because instances were withheld or unavailable). The dataset was collected by people who are blind/low-vision.</p><p>Although participation was open to the public, most collectors were recruited via blind/low-vision charities in Level 4 5 countries (primarily UK, Canada, USA, Australia). In addition, the data collection task required an iPhone with iOS ? 13.2, with sufficient space to store the videos before upload. To upload the videos to the server, collectors required a stable internet connection, and power supply.</p><p>Are there recommended data splits (e.g., training, development/validation, testing)? If so, please provide a description of these splits, explaining the rationale behind them. Yes, the recommended splits for the benchmark dataset are described in Appendix B.3. The unfiltered dataset has no recommended splits as it is not explicitly used for the benchmark evaluation.</p><p>Are there any errors, sources of noise, or redundancies in the dataset? If so, please provide a description. By design, the dataset contains high-variation videos of objects. Variation comes from objects being fully/partially out-offrame, blurred (due to camera or object motion), too close or far from the camera, obstructed by hands or other objects, or under/overexposed. All videos are unique, but there is always at least one video per unique object. Some objects occur multiple times across different collectors, but each collector never has duplicate objects.</p><p>Is the dataset self-contained, or does it link to or otherwise rely on external resources (e.g., websites, tweets, other datasets)? If it links to or relies on external resources, a) are there guarantees that they will exist, and remain constant, over time; b) are there official archival versions of the complete dataset (i.e., including the external resources as they existed at the time the dataset was created); c) are there any restrictions (e.g., licenses, fees) associated with any of the external resources that might apply to a future user? Please provide descriptions of all external resources and any restrictions associated with them, as well as links or other access points, as appropriate. The dataset and its additional annotations are self-contained and do not rely on any external resources.</p><p>Any other comments? None.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.3. Collection process</head><p>What mechanisms or procedures were used to collect the data (e.g. hardware apparatus or sensor, manual human curation, software program, software API)? How were these mechanisms or procedures validated? The procedures used to collect the ORBIT data through manual curation are described in Appendix A.1. We validated these procedures through user testing and user feedback. The app used to collect the data is described in Appendix A.2. It followed Apple's accessibility guidelines and was tested with blind/low-vision users.</p><p>How was the data associated with each instance acquired? Was the data directly observable (e.g. raw text, movie ratings), reported by subjects (e.g. survey responses), or indirectly inferred/derived from other data (e.g. part-of-speech tags, model-based guesses for age or language)? If data was reported by subjects or indirectly inferred/derived from other data, was the data validated/verified? If so, please describe how. Videos were recorded and contributed manually by blind/low-vision collectors via a custom-built iOS app (see Appendix A.2).</p><p>If the dataset is a sample from a larger set, what was the sampling strategy (e.g. deterministic, probabilistic with specific sampling probabilities)? The dataset was not sampled from a larger set.</p><p>Over what time-frame was the data collected? Does this time-frame match the creation time-frame of the data associated with the instances (e.g. recent crawl of old news articles)? If not, please describe the time-frame in which the data associated with the instances was created. The dataset was collected in two phases. The first phase ran from June to August 2020 and involved 49 collectors who were blind/low-vision. The second phase ran from November 2020 to January 2021 and involved 49 collectors who were blind/low-vision. The time-frames for the dataset creation and the data itself overlap.</p><p>Who was involved in the data collection process (e.g. students, crowdworkers, contractors) and how were they compensated (e.g. how much were crowdworkers paid)? The dataset was collected by people who are blind/lowvision. In the first phase, collectors were based in the UK and were incentivized with a ?50 voucher for their participation. In the second phase, collectors were based globally and a donation of ?25 (or the local currency equivalent) was made to one of five charities chosen by the collector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.4. Data pre-processing</head><p>Was any pre-processing/cleaning/labeling of the data done (e.g., discretization or bucketing, tokenization, part-of-speech tagging, SIFT feature extraction, removal of instances, processing of missing values)? If so, please provide a description. If not, you may skip the remainder of the questions in this section. During the data collection process, videos were expunged from the server if i) the object was not present in the video, ii) the video contained PII, or iii) the object or object name were inappropriate. All remaining videos formed the unfiltered ORBIT dataset, and were not pre-processed in any other way.</p><p>The ORBIT benchmark dataset was constructed from the unfiltered dataset by removing i) clutter videos recorded with the panning technique, ii) videos shorter than 30 frames, and iii) objects without at least 2 clean videos and 1 clutter video (see further details in Appendix B). Finally, to run the baseline models on the benchmark task, video frames were resized from 1080 ? 1080 to 84 ? 84 pixels.</p><p>Regarding labeling, in addition to the object labels provided by collectors for each video, the clutter videos were further annotated with bounding boxes around the target object. These were provided by a private company with whom we worked closely to ensure consistent and high-quality annotations across the dataset.</p><p>Was the "raw" data saved in addition to the pre-processed/cleaned/labeled data (e.g., to support unanticipated future uses)?</p><p>If so, please provide a link or other access point to the "raw" data. Both the unfiltered and benchmark datasets are available at https://doi.org/10.25383/city.14294597. For download details see the code repository at https://github.com/microsoft/ORBIT-Dataset. Note, videos expunged from the server during the data collection process were not saved.</p><p>Is the software used to pre-process/clean/label the instances available? If so, please provide a link or other access point. Pre-processing/cleaning involved removing videos that did not meet specific criteria from the unfiltered dataset to obtain the benchmark dataset (see Appendix B.2). While scripts to do this are not explicitly provided, scripts to download (and resize frames) in the unfiltered and benchmark datasets are provided in the code repository. Annotations beyond the video labels (e.g. bounding boxes) were collected by a private annotation company using proprietary software.</p><p>Does this dataset collection/processing procedure achieve the motivation for creating the dataset stated in the first section of this datasheet? If not, what are the limitations? Yes, by collecting videos from blind/low-vision collectors "in situ" and doing minimal pre-processing/cleaning of these videos has yielded a dataset that is highly representative of the high-variation real-world scenarios they encounter. As a result, the unfiltered and benchmark datasets can be used to 1) drive research in few-shot object recognition from high-variation videos, and 2) realize TORs for people who are blind/low-vision.</p><p>Any other comments None.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.5. Dataset distribution</head><p>How will the dataset be distributed? (e.g., tarball on website, API, GitHub; does the data have a DOI and is it archived redundantly?) The ORBIT dataset is available as 4 zip files (train, validation, test, other) at DOI: https://doi.org/10.25383/city.14294597. We provide versions of each zip file with frames resized to 224 ? 224 pixels.</p><p>When will the dataset be released/first distributed? What license (if any) is it distributed under? The dataset was released on 7 April 2021 under an CC4.0 license. The code repository is released under an MIT license.</p><p>Are there any copyrights on the data? Under the CC4.0 license, the data can be shared and adapted, even commercially. The data must be correctly attributed.</p><p>Are there any fees or access/export restrictions? No, the dataset and code are open-source.</p><p>Any other comments? None.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.6. Dataset maintenance</head><p>Who is supporting/hosting/maintaining the dataset?</p><p>The ORBIT dataset (unfiltered and benchmark) are hosted, and maintained by City, University of London.</p><p>Will the dataset be updated? If so, how often and by whom? The dataset may be updated in the future with more videos collected by people who are blind/low-vision.</p><p>How will updates be communicated? (e.g. mailing list, GitHub) All updates will be communicated via the dataset's DOI (https://doi.org/10.25383/city.14294597), code repository (https://github.com/microsoft/ORBIT-Dataset), and website (https://orbit.city.ac.uk), including if the dataset becomes obsolete.</p><p>Is there a repository to link to any/all papers/systems that use this dataset? No explicit repository has been built for this purpose, however the usage of this dataset can be tracked via the 'cited by' feature available in most referencing tools.</p><p>If others want to extend/augment/build on this dataset, is there a mechanism for them to do so? If so, is there a process for tracking/assessing the quality of those contributions. What is the process for communicating/distributing these contributions to users? Individuals are free to extend/augment/build on the dataset. This may include collecting more data/annotations, developing new models, or contributing to code in the repository. For any questions, individuals can reach out to info@orbit.city.ac.uk or use the issue tracker in the code repository. Any future updates will be communicated/distributed via the DOI, code repository, and website.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.7. Legal and ethical considerations</head><p>Were any ethical review processes conducted (e.g., by an institutional review board)? If so, please provide a description of these review processes, including the outcomes, as well as a link or other access point to any supporting documentation. The pilot study for the ORBIT data collection was approved by the City, University of London, UK Computer Science and Library &amp; Information Science Research Ethics Committee (ETH1920-0331). The first and second phases of the data collection were approved by the City, University of London, UK Computer Science and Library &amp; Information Science Research Ethics Committee (ETH1920-1126 and ETH2021-0032, respectively).</p><p>Does the dataset contain data that might be considered confidential (e.g. data that is protected by legal privilege or by doctor patient confidentiality, data that includes the content of individuals non-public communications)? If so, please provide a description. No, all videos were checked by a human annotator during the data collection process and were expunged from the server if they contained any PII or confidential content.</p><p>Does the dataset contain data that, if viewed directly, might be offensive, insulting, threatening, or might otherwise cause anxiety? If so, please describe why No, all videos were checked by a human annotator during the data collection process and were expunged from the server if they contained any offensive or inappropriate content.</p><p>Does the dataset relate to people? If not, you may skip the remaining questions in this section. Yes, the dataset was collected by people who are blind/low-vision with the goal of realizing a real-world application, namely TORs.</p><p>Does the dataset identify any sub-populations (e.g., by age, gender)? If so, please describe how these subpopulations are identified and provide a description of their respective distributions within the dataset. No, no sub-populations are identified in the dataset.</p><p>Is it possible to identify individuals (i.e., one or more natural persons), either directly or indirectly (i.e., in combination with other data) from the dataset? If so, please describe how. No, collection was completely anonymized. No meta-data or PII was collected that would allow individuals to be identified indirectly.</p><p>Does the dataset contain data that might be considered sensitive in any way (e.g. data that reveals racial or ethnic origins, sexual orientations, religious beliefs, political opinions or union memberships, or locations; financial or health data; biometric or genetic data; forms of government identification, such as social security numbers; criminal history)? If so, please provide a description. No, the dataset contains no sensitive data.</p><p>Did you collect the data from the individuals in question directly, or obtain it via third parties or other sources (e.g. websites)? All videos were collected directly from the individuals in question.</p><p>Were the individuals in question notified about the data collection? If so, please describe (or show with screenshots or other information) how notice was provided, and provide a link or other access point to, or otherwise reproduce, the exact language of the notification itself.</p><p>Yes, collectors were invited to take part in the data collection through advertisements on social media, mailing lists, blogs, and podcasts.</p><p>Did the individuals in question consent to the collection and use of their data? If so, please describe (or show with screenshots or other information) how consent was requested and provided, and provide a link or other access point to, or otherwise reproduce, the exact language to which the individuals consented. Collectors were required to provide informed consent as part of the sign-up procedure and app download. Informed consent covered the following points:</p><p>? I confirm that I have read and understood the participant information dated [INSERT DATE AND VERSION NUMBER] for the above study. I have had the opportunity to consider the information and ask questions which have been answered satisfactorily. ? I understand that, if approved, the videos I take with the iPhone app will become part of an open dataset. ? I understand that the researcher might choose not to publish one or more of my videos in the open dataset. ? I understand that my participation is voluntary and that I am free to withdraw without giving a reason, and without being penalized or disadvantaged. ? I agree that, unless I decide to withdraw, the data collected up to this point will be used in the study. ? I agree to City, University of London recording and processing this information about me. I understand that this information will be used only for the purposes explained in the participant information, and my consent is conditional on City, University of London complying with its duties and obligations under the General Data Protection Regulation (GDPR).</p><p>If consent was obtained, were the consenting individuals provided with a mechanism to revoke their consent in the future or for certain uses? If so, please provide a description, as well as a link or other access point to the mechanism (if appropriate). Collectors were able to delete data through the iOS app before the data collection period ended. Collectors were free to withdraw their consent at any time, and email the researchers to delete their data from the server.</p><p>Has an analysis of the potential impact of the dataset and its use on data subjects (e.g. a data protection impact analysis) been conducted? If so, please provide a description of this analysis, including the outcomes, as well as a link or other access point to any supporting documentation. A Data Protection Impact Analysis was carried out as part of the Ethics approval process, and the public release of the dataset. It was determined low risk.</p><p>Any other comments? None.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Episodic &amp; non-episodic learning for TORs</head><p>In the (2) personalize step, a few-shot recognition model, or a TOR, aims to learn to recognize completely new objects from only a few examples (in our case videos) captured by the user themselves. The recognition model can be trained to do this in a episodic or non-episodic manner (i.e. the (1) train step). Here, we describe both classes of approach.</p><p>For a user ?, we denote that each of their personal objects p ? P ? has N ? p context videos and M ? p target videos:</p><formula xml:id="formula_8">V ? p = {(v, p) i } N ? p i=1 V ? p = {(v, p) i } M ? p i=1<label>(1)</label></formula><p>wherev, v are context and target videos, respectively, and p is the object label. We group the user's context and target videos over all their |P ? | objects as:</p><formula xml:id="formula_9">V ? = V ? 1 ? ? ? ? ? V ? |P ? | V ? = V ? 1 ? ? ? ? ? V ? |P ? | . (2)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.1. Episodic few-shot learning</head><p>An episodic approach <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b37">38]</ref> is characterized by training the model in "episodes" or tasks. For a train user ? ? K train , a task is a random sample of their context videos C ? and target videos T ? :</p><formula xml:id="formula_10">C ? = {(v, p) i } N i=1 T ? = {(v, p) i } M i=1<label>(3)</label></formula><p>wherev, v are context and target videos, respectively, and p ? P ? is the object label. Note,</p><formula xml:id="formula_11">C ? ? V ? and T ? ? V ? ,</formula><p>where V ? and V ? are the total set of the user's context and target videos, respectively (see Appendix G.2 for further task sampling details). A few-shot recognition model f is trained over many randomly sampled tasks per train user ? ? K train , typically by maximizing the likelihood of predicting the user's correct personal object in each frame in a target video v f ? v after seeing only that user's context videos C ? :</p><formula xml:id="formula_12">? * = arg max ? E ??K train E C ? ?V ? T ? ?V ? E (v,p)?T ? v f ?v log p ? ? (y f | v f , C ? )<label>(4)</label></formula><p>where p ? ? (y f | v f , C ? ) is the probability the user's personalized recognition model f ? ? assigns to the ground-truth object label for frame v f . The parameters are typically updated with stochastic gradient descent (SGD) and a cross entropy loss function.</p><p>The resulting model f ? * can be thought of as knowing how to personalize to a user. That is, for a completely new test user ? ? K test in the real-world, ? * can rapidly be updated to the user's personalized parameters ? ? with only the context videos C ? of that user's personal objects. The user can then scan f ? ? around any new target scenario to identify their objects using Eq. (1). model on all target videos per test user in K test (repeated for T test tasks per user), and report the metrics over T all .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.2. Sampling tasks</head><p>The only strict requirement of the ORBIT evaluation protocol is that a model outputs a prediction for every frame in every target video for every test user. How tasks are sampled from context/target videos during training, and context videos during testing is otherwise flexible. In this section, we detail how we sample tasks, but acknowledge that other strategies could be chosen. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.3. Task sampling hyper-parameters</head><p>The number of tasks sampled per train user (per epoch) was selected over a range of 5 to 500 (see <ref type="table" target="#tab_14">Table A</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.4. Optimization hyper-parameters</head><p>The learning rates were selected with a grid search over the range 10 -5 to 10 -1 . Unless otherwise specified, for all baselines we use the Adam optimizer <ref type="bibr" target="#b19">[20]</ref> with default parameters ?=(0.9, 0.999), ? = 10 -8 , and no weight decay.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H. Extended analyses</head><p>Here we extend analyses from Section 5.2 in the main paper.</p><p>Meta-training on other few-shot learning datasets. Table A.3 shows the results when meta-training on Meta-Dataset and meta-testing on the ORBIT test users, for each of the baseline models (extends <ref type="table" target="#tab_9">Table 5</ref> in the main paper).</p><p>User-centric versus user-agnostic training. The baselines are trained in a user-centric manner whereby each task is sampled from only one train user's objects. We compare this to a user-agnostic training regime in <ref type="table" target="#tab_14">Table A</ref>.4 where we pool objects across all train users in K train and construct train tasks by randomly sampling objects from this pool (keeping all other sampling procedures the same). This is akin to typical few-shot learning pipelines. The performance of both regimes is overall equivalent, suggesting that imposing a user-centric hierarchy on task sampling does not negatively impact performance, relative to imposing no hierarchy.</p><p>Per-user performance. Extending <ref type="figure" target="#fig_8">Figure 4</ref> in the main paper, we plot CLU-VE performance across the full suite of metrics (frame accuracy, frames-to-recognition and video accuracy) per user across all baseline models in <ref type="figure" target="#fig_1">Figure A.10</ref>. Here, we see the same trend as the main paper -personalization performance varies widely across users.</p><p>Train task composition. In <ref type="figure" target="#fig_9">Figures 5 and 6</ref> in the main paper, we investigate the impact of the number of context videos per object, and the number of objects per user, sampled in train tasks on CLU-VE frame accuracy. We extend these plots to the frames-to-recognition and video accuracy metrics in <ref type="figure">Figure</ref>  Number of tasks per train user. In <ref type="table" target="#tab_14">Table A</ref>.7), we investigate the impact of the number of tasks sampled per train user (per epoch), T train during meta-training, and observe that CLU-VE performance increases with more tasks, but levels off at around 50 tasks per user. We expect that bigger gains could be achieved by accounting for the informativeness of frames, instead of uniformly sampling frames from videos. This is likely because clean and clutter videos are noisy and contain many redundant frames.   frames-to-recognition -center; video accuracy -right). Frames are sampled from an increasing number of clean videos per object using the number of clips per video (S train ) to keep the total number of context frames fixed per train task. Extends <ref type="figure" target="#fig_9">Figure 5</ref> in the main paper.  <ref type="table" target="#tab_8">Table 4</ref>, suggesting that models may be able to adapt to more objects in the real-world (frame accuracy -left; frames-to-recognition -center; video accuracy -right). Extends <ref type="figure" target="#fig_11">Figure 6</ref> in the main paper.    <ref type="table" target="#tab_14">Table A</ref>.5: Meta-training with more context videos per object leads to better CLU-VE performance. Frames are sampled from an increasing number of clean videos per object (N ? p ) using the number of clips per video (S train C ) to keep the total number of context frames fixed per train task. Corresponds to <ref type="figure" target="#fig_9">Figure 5</ref> in the main paper.   <ref type="table" target="#tab_8">Table 4</ref>, suggesting that models may be able to adapt to more objects in the real-world. Corresponds to <ref type="figure" target="#fig_11">Figure 6</ref> in the main paper.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>(a) Frames from clean videos (b) Frames from clutter videos</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>High-variation examples in the ORBIT dataset -a facemask, hairbrush, keys, and watering can. Full videos in the supplementary material. Further examples in Figure A.5. ular promise toward this goal with recent advances opening exciting possibilities for light-weight, adaptable recognition.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Number of objects per collector.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>0</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Collector</head><label></label><figDesc>Number of videos by object (b) Number of videos (stacked by object) per collector.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 2 :</head><label>2</label><figDesc>Number of objects and videos across 67 collectors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>( 2 )</head><label>2</label><figDesc>Personalize. A real-world user captures a few examples of a set of their personal objects. The deployed model is trained on this user's objects using just these examples. (3) Recognize. The user employs their now-personalized</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 4 :</head><label>4</label><figDesc>CLU-VE frame accuracy varies widely across test users (error bars are 95% confidence intervals) with Pro-toNets<ref type="bibr" target="#b39">[40]</ref>. For other metrics and models seeFigure A.10.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 5 :</head><label>5</label><figDesc>Meta-training with more context videos per object leads to better CLU-VE performance. Frames are sampled from an increasing number of clean videos per object using the number of clips per video (S train ) to keep the total number of context frames fixed per train task.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 6 :</head><label>6</label><figDesc>Meta-training and -testing with more objects per user poses a harder recognition problem (solid line), however, meta-training with fewer objects than encountered at meta-testing (dashed line) shows only a small CLU-VE performance drop compared to</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure A. 1 :</head><label>1</label><figDesc>ORBIT Camera iOS app: Informed consent screen (left). 'Things' screen showing a collector's personal objects (center-left). 'Thing' screen for a specific object (center-right). 'Thing' screen after recording a video (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head></head><label></label><figDesc>.2 shows the number of objects (A.2a) and number of videos per collector (A.2b).Types of objects For summarization purposes, we clustered objects based on object similarity and show their longtailed distribution inFigure A.7a. The clustering algorithm and full contents of each cluster are included in Appendix D.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head></head><label></label><figDesc>(a) Number of objects per collector.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure A. 2 :</head><label>2</label><figDesc>Number of objects and videos across 97 collectors in the unfiltered ORBIT dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head></head><label></label><figDesc>.2. Extending Figures 2a and 2b, we report the number of objects and videos per user in Figures A.3 and A.4, respectively. Types of objects. Following Appendix D, we include the object cluster histogram for all benchmark users in Figure A.7b, and for train (A.7c), validation (A.7d) and test (A.7e) users. Extending Figure 1, we include more examples of clean and clutter frames from the ORBIT benchmark dataset in Figure A.5.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head></head><label></label><figDesc>.7.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head></head><label></label><figDesc>by object (b) Validation users.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head></head><label></label><figDesc>by object (c) Test users.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_21"><head>Figure A. 4 :</head><label>4</label><figDesc>Number of clean and clutter videos (stacked by object) per collector in the ORBIT benchmark dataset, grouped by train, validation, and test users.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_22"><head>Figure A. 5 :</head><label>5</label><figDesc>Video clips from the ORBIT benchmark dataset. Each row comes from a different video (alternating clean and clutter). Rows 1,2: "dog toy". Rows 3,4: "dog lead". Rows 5,6: "white cane". Rows 7,8: "victor stream book reader". Rows 9,10: "toddler cup". Rows 11,12: "hearing aid".</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_24"><head>Figure A. 6 :</head><label>6</label><figDesc>The proportion of the video the target object spends in-versus out-of-frame, averaged over all clutter videos per collector and grouped by train, validation, and test users.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_25"><head></head><label></label><figDesc>cluster (a) All users (unfiltered dataset).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_26"><head></head><label></label><figDesc>All users (benchmark dataset).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_27"><head></head><label></label><figDesc>Train users (benchmark dataset).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_28"><head></head><label></label><figDesc>Validation users (benchmark dataset).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_29"><head></head><label></label><figDesc>Test users (benchmark dataset).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_30"><head>Figure A. 7 :</head><label>7</label><figDesc>Histogram of object clusters (grouped by object similarity) in the ORBIT dataset (benchmark and unfiltered). Objects in each cluster are listed in Appendix D.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_31"><head></head><label></label><figDesc>A.8 and Figure A.9 and complement them with their corresponding tables in Tables A.5 and A.6.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_32"><head>Figure A. 8 :</head><label>8</label><figDesc>Meta-training with more context videos per object leads to better CLU-VE performance (frame accuracy -left;</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_33"><head>Figure A. 9 :</head><label>9</label><figDesc>Meta-training and -testing with more objects per user poses a harder recognition problem (solid line), however, meta-training with fewer objects than encountered at meta-testing (dashed line) shows only a small CLU-VE performance drop compared to</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_34"><head>Figure A. 10 :</head><label>10</label><figDesc>CLU-VE performance varies widely across test users, for all metrics, across all baseline models. Error bars are 95% confidence intervals.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Comparison of few-shot learning datasets. Note, the ORBIT benchmark dataset is a subset of all videos contributed by collectors (see Appendix B). *Collected in 2 controlled environments -1 uniform background, 1 cluttered space.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>ORBIT benchmark dataset.</figDesc><table><row><cell>categories (e.g. Apple TV remote, Virgin remote, Samsung</cell></row><row><cell>TV remote control). For summarization purposes, we clus-</cell></row><row><cell>tered the objects based on object similarity and observe a</cell></row><row><cell>long-tailed distribution (see Figure A.7b). The largest clus-</cell></row><row><cell>ters contained different types of remotes/controls, keys, wal-</cell></row><row><cell>lets/purses, guidecanes, doors, airpods, headphones, mobile</cell></row></table><note>phones, watches, sunglasses and Braille readers. More than half of the clusters contained just 1 object. The clustering algorithm and cluster contents are included in Appendix D. Bounding box annotations. Since the clutter videos could contain multiple objects, we provide bounding box annota- tions around the target object in all clutter videos (available in the code repository). We use these to compute the propor- tion of time the target object spends in-versus out-of-frame per video, and show this in Figure A.6 averaged over all clutter videos per collector. On average, the target object is in-frame for ?95% of any given clutter video. Video lengths. Video lengths depended on the recording technique required for each video type (see Appendix A.1). On average, clean videos were 25.7s (?771 frames at 30 FPS), and clutter videos were 15.2s (?457 frames at 30 FPS). Unfiltered ORBIT dataset. Some collectors did not meet the minimum requirements to be included in the benchmark dataset (e.g. an object did not have both clean and clutter videos). The benchmark dataset was therefore extracted from a larger set of 4733 videos (3,161,718 frames, 97GB) of 588 objects contributed by 97 collectors. We summarize the unfiltered dataset in Appendix A.3.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 :</head><label>3</label><figDesc>ORBIT evaluation metrics. Symbols ? / ? indicate up / down is better, respectively. T all is the set of all target videos pooled across all tasks for all test users in K test .</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>282.09 ? 10 12 53.73 (1.80) 14.44 (1.50) 63.07 (2.44) 353.30 ? 10 12 50 gradient steps 11.17M</figDesc><table><row><cell></cell><cell cols="4">Clean Video Evaluation (CLE-VE)</cell><cell cols="4">Clutter Video Evaluation (CLU-VE)</cell><cell></cell><cell></cell></row><row><cell>MODEL</cell><cell>FRAME ACC</cell><cell>FTR</cell><cell>VIDEO ACC</cell><cell>MACS TO PERSONALIZE</cell><cell>FRAME ACC</cell><cell>FTR</cell><cell>VIDEO ACC</cell><cell>MACS TO PERSONALIZE</cell><cell>METHOD TO PERSONALIZE</cell><cell># PARAMS</cell></row><row><cell>Best possible</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="3">95.31 (1.37) 0.00 (0.00) 100.00 (0.00)</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="4">ProtoNets [40] 65.16 (1.96) 7.55 (1.35) 81.88 (2.51)</cell><cell>2.82 ? 10 12</cell><cell cols="3">50.34 (1.74) 14.93 (1.52) 59.93 (2.48)</cell><cell>3.53 ? 10 12</cell><cell>1 forward pass</cell><cell>11.17M</cell></row><row><cell>CNAPs [38]</cell><cell cols="3">66.15 (2.08) 8.40 (1.40) 79.56 (2.63)</cell><cell>3.09 ? 10 12</cell><cell cols="3">51.47 (1.81) 17.87 (1.69) 59.53 (2.48)</cell><cell>3.87 ? 10 12</cell><cell>1 forward pass</cell><cell>12.75M</cell></row><row><cell>MAML [9]</cell><cell cols="4">70.58 (2.10) 8.62 (1.56) 80.88 (2.56) 84.63 ? 10 12</cell><cell cols="6">51.67 (1.88) 20.95 (1.84) 57.87 (2.50) 105.99 ? 10 12 15 gradient steps 11.17M</cell></row><row><cell cols="4">FineTuner [46] 69.47 (2.16) 7.82 (1.54) 79.67 (2.62)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 4 :</head><label>4</label><figDesc>Baselines on the ORBIT Dataset. Results are reported as the average (95% confidence interval) over all target videos pooled from 85 test tasks (5 tasks per test user, 17 test users). Best possible scores are computed using bounding box annotations which are available for the clutter videos (see Appendix C and Figure A.6).</figDesc><table><row><cell>MODEL</cell><cell>FRAME ACC</cell><cell>FTR</cell><cell>VIDEO ACC</cell></row><row><cell>ProtoNets [40]</cell><cell cols="3">58.98 (2.23) 11.55 (1.79) 69.17 (3.01)</cell></row><row><cell>CNAPs [38]</cell><cell cols="3">51.86 (2.49) 20.81 (2.33) 60.77 (3.18)</cell></row><row><cell>MAML [9]</cell><cell cols="3">42.55 (2.67) 37.28 (2.99) 46.96 (3.25)</cell></row><row><cell cols="4">FineTuner [46] 61.01 (2.24) 11.53 (1.82) 72.60 (2.91)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table><row><cell>CLE-VE performance when meta-training on Meta-</cell></row><row><cell>Dataset and meta-testing on ORBIT (for CLU-VE see Ta-</cell></row><row><cell>ble A.3). Even on clean videos, models perform poorly</cell></row><row><cell>compared to when meta-training on ORBIT (Table 4) sug-</cell></row><row><cell>gesting that existing few-shot datasets may be insufficient</cell></row><row><cell>for real-world adaptation.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head></head><label></label><figDesc>users, 588 objects, 132 clusters] 1: keys, house keys, radar key, my keys, front door keys, keychain, door keys, key, set of keys, 2: remote, tv remote, apple tv remote, amazon remote control, television remote control, remote control, virgin remote control, tv remote control, samsung tv remote control, presentation remote, sky q remote, clickr, fire stick remote, control, television control, remote tv, remote comtrol, apple tv ii remote control, t. v. remote control, 3: black small wallet, my purse, my wallet, ladies purse, money pouch, coin purse, wallet for bus pass cards and money, i d wallet, ipod in wallet, walletv, wallet, purse, 4: symbol cane, black mobility cane, my cane, white mobility came, folded white cane, long cane, cane, folded cane, folded long guide cane, p939411 white cane, white came, visibility stick, mobility, mobility cane, white cane, 5: earphones, apple headphones, apple earpods, airpods, my airpod pros case, iphone air pods, airpods pro charging case, airpod case, airpods, my airpods, airpods case, earpods, apple airpods case, 6: mug, personal mug, cup again, coffee mug, toddler cup, styrofoam cup, my mug, my cup, 7: front door, back door, house door, front door to house, door, my front door, shed door, front door from outside, 8: blue headphones, my headphones, headphone, bose wireless headphones, headset, my sennheiser pxc 350-2, headphones, 9: phone, mobile phone, iphone 6, apple mobile phone, i phone 11 pro, iphone in case, iphone, cell phone, work phone, phone case, 10: watch, wrist watch, apple watch, apple wath, risk watch, my apple watch, 11: backpack, my work backpack, work bag, bag, shoulder bag, bum bag, back pack, work backpack, sports bag, sport bago, numb bag, 12: sunglasses, my wraparound sunglasses, dark glasses, 13: slippers, nike trainers, my shoes, boot, trainers, trainer shoe, slipper, my trainers, shoes, running shoes, 14: medication, blister pack of painkillers, tablets, money, aspirin vs tylenol, aspirin, pill bottle, my pill dosette, buckleys, box of tablets, 15: victor stream book reader, orbit braille reader and notetaker, victor reader stream, orbit reader 20 braille display, braillepen slim braille keyboard, braille orbit reader, solo audiobook player, braille note, my braille displat, rnib talking book envelope, 16: guide dog play cola, dogs lead, guide dog harness, retractable dog lead, leash, guide dog harness, dog lead, dogs, running tether, 17: blue tooth keyboard, bluetooth keyboard, my keyboard, apple wireless keyboard, mini bluetooth keyboard, portable keyboard, keyboard, 18: my water bottle, bottle, pop bottle, green water bottle, water bottle, bottle of water, clean canteen stainless</figDesc><table><row><cell></cell><cell cols="2">Collectors Objects</cell><cell>Videos</cell><cell></cell><cell>Videos per object</cell><cell></cell><cell></cell><cell>Frames per video</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">mean/std 25/75 th perc. min/max</cell><cell>mean/std</cell><cell>25/75 th perc.</cell><cell>min/max</cell></row><row><cell>Total</cell><cell>44</cell><cell>278</cell><cell>2277</cell><cell>8.2/6.0</cell><cell>7.0/7.0</cell><cell cols="3">3.0/46.0 716.8/448.6 378.0/898.0</cell><cell>34.0/3600.0</cell></row><row><cell>Clean</cell><cell></cell><cell></cell><cell>1814</cell><cell>6.5/6.0</cell><cell>5.0/6.0</cell><cell cols="3">2.0/44.0 774.0/471.6 394.5/899.0</cell><cell>34.0/3600.0</cell></row><row><cell>Clutter</cell><cell></cell><cell></cell><cell>463</cell><cell>1.7/0.8</cell><cell>1.0/2.0</cell><cell>1.0/7.0</cell><cell cols="2">492.7/235.5 324.5/599.0</cell><cell>47.0/2412.0</cell></row><row><cell>Per-collector</cell><cell>1</cell><cell cols="2">6.3/2.6 51.8/55.2</cell><cell>7.6/4.8</cell><cell>6.5/7.4</cell><cell cols="3">3.4/38.4 735.5/232.0 629.7/805.7 213.1/1614.3</cell></row><row><cell>Clean</cell><cell></cell><cell></cell><cell>41.2/52.7</cell><cell>5.9/4.8</cell><cell>4.6/6.0</cell><cell cols="3">2.4/36.5 823.0/279.0 710.2/898.4 219.3/1872.6</cell></row><row><cell>Clutter</cell><cell></cell><cell></cell><cell>10.5/5.2</cell><cell>1.7/0.5</cell><cell>1.2/2.0</cell><cell>1.0/3.0</cell><cell cols="2">467.3/163.2 362.4/597.0</cell><cell>177.7/853.0</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">(a) Train users.</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">Collectors Objects</cell><cell>Videos</cell><cell></cell><cell cols="2">Videos per object</cell><cell></cell><cell>Frames per video</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">mean/std 25/75 th perc. min/max</cell><cell>mean/std</cell><cell>25/75 th perc.</cell><cell>min/max</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table A</head><label>A</label><figDesc>glass, glass, 53: guide dog, my flee, 54: dog streetball, dog toy, adaptive tennis ball, 55: stylus, apple pencil, 56: my laptop, laptop, 57: ipad, ipad pro, 58: mouse, 59: local post box, post box, 60: black strappy vest, t-shirts, 61: gloves, winter gloves, 62: hand gel, lotion bottle, 63: perfume, deodorant, 64: hair clip, headband, 65: Number of objects per collector in the ORBIT benchmark dataset, grouped by train, validation, and test users. miter saw, 73: valve oil, slide grease, 74: cooker, 75: shelf unit with things, wardrobe, 76: knitting basket, washing basket, basket, 77: home phone, cordless phone, 78: skipping rope, spinner, 79: stairgate, back patio gate, 80: large sewing needle, knitting needle, 81: mayonnaise jar, mustard, 82: rice, chicken instant noodles, 83: tea, cranberry cream tea, 84: ottawa bus stop, bus stop sign, 85: journal, book, 86: security fob, 87: money clip, 88: headphone case, 89: ps4 controller, 90: compact disc, 91: garden wall, 92: garden shed, 93: litter and dog waste bin, 94: my clock, 95: sleep mask, 96: glasses cleaning wipe, 97: tissue box, 98: condom box, 99: make up, 100: clear nail varnish, 101: eye drops, 102: battery drill, 103: bed, 104: sofa, 105: cushion, 106: garden bench, 107: mirror, 108: table fan, 109: tred mill, Clusters for Figure A.7b [67 users, 486 objects, 122 clusters] 1: keys, house keys, radar key, my keys, front door keys, keychain, door keys, key, set of keys, 2: remote, tv remote, apple tv remote, amazon remote control, television remote control, remote control, virgin remote control, tv remote control, samsung tv remote control, presentation remote, sky q remote, clickr, fire stick remote, control, television control, remote tv, 3: black small wallet, my purse, my wallet, ladies purse, money pouch, coin purse, wallet for bus pass cards and money, i d wallet, ipod in wallet, walletv, wallet, purse, 4: symbol cane, black mobility cane, my cane, white mobility came, folded white cane, long cane, cane, folded cane, folded long guide cane, p939411 white cane, white came, visibility stick, mobility, mobility cane, white cane, 5: earphones, apple headphones, apple earpods, airpods, my airpod pros case, iphone air pods, airpods pro charging case, airpod case, airpods, my airpods, airpods case, earpods, 6: blue headphones, my headphones, headphone, bose wireless headphones, headset, my sennheiser pxc 350-2, headphones, 7: phone, mobile phone, iphone 6, apple mobile phone, i phone 11 pro, iphone in case, iphone, cell phone, work phone, phone case, 8: mug, personal mug, cup again, coffee mug, toddler cup, styrofoam</figDesc><table><row><cell></cell><cell>10</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Number of objects</cell><cell>0 2 4 6 8 Number of videos by object</cell><cell>P573 100 P638 P463 2 4 6 8 10 Number of objects 0 20 40 60 80 120 140</cell><cell>P806 P642</cell><cell cols="5">P407 obj 13 clutter P193 P653 P400 P613 obj 10 clutter obj 7 clutter obj 4 clutter obj 1 clutter P753 P999</cell><cell>P587</cell><cell>P544 obj 13 clean P980 P697 P100 P744 obj 10 clean obj 7 clean obj 4 clean obj 1 clean P901 P421</cell><cell>P277 P953</cell><cell cols="3">P241 obj 12 clutter P447 P470 P303 P996 Collector P767 P174 P833 obj 12 clean P282 P284 P954 obj 9 clutter obj 9 clean obj 6 clutter obj 6 clean obj 3 clutter obj 3 clean (a) Train users. 2 4 6 8 10 12 P609 P452 P198 P485 Number of objects</cell><cell>P538 P204</cell><cell cols="5">P865 obj 11 clutter P332 P118 P478 P543 obj 8 clutter obj 5 clutter obj 2 clutter P455 P900</cell><cell>P685</cell><cell>P141 obj 11 clean P106 P398 P665 P940 obj 8 clean obj 5 clean obj 2 clean P177 P271</cell><cell>P295 P233</cell><cell>P159</cell><cell>P537</cell><cell>P499 P554</cell><cell>P681</cell></row><row><cell></cell><cell></cell><cell>0</cell><cell>P235</cell><cell>P867</cell><cell>P778</cell><cell>P194</cell><cell>P579</cell><cell>P189</cell><cell></cell><cell></cell><cell></cell><cell>0</cell><cell>P642 Collector P753 P999 P901 P421</cell><cell>P953</cell><cell>P609</cell><cell>P452</cell><cell>P198</cell><cell>P485</cell><cell>P204</cell><cell>P455</cell><cell>P900</cell><cell>P177</cell><cell>P271</cell><cell>P233</cell><cell>P554</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Collector</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Collector</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="8">(b) Validation users.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">(c) Test users.</cell><cell></cell></row><row><cell cols="14">Figure A.3: 110: exercise bench, 111: watering can, watering, 112: vase</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="14">with flowers, 113: veg peeler, 114: sharp knife, 115: wall</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="14">plug, 116: embroidery thread cone, 117: hole punch, 118:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="14">pencil case, 119: paperclips, 120: uno cards, 121: baked</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="14">bean tin, 122: banana, 123: cappuccino pods, 124: mountain</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="14">dew can, 125: pinesol cleaner, 126: fish food, 127: cat , 128:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="14">dog poo, 129: my slate, 130: av tambourine, 131: grinder,</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">132: fluffy blanket</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>.2: The ORBIT benchmark dataset, grouped by train, validation, and test users.steel water bottle, 19: reading glasses, glasses, eye glasses, spectacles, 20: adaptive washing machine, washing machine, tumble dryer, adaptive dryer, tumble dry setting washing machine, washing machine setting, 21: baseball cap, cap, or- ange skullcap, my tilly hat, hat, my tilly hat upside down, 22: microwave, toaster, kettle, one cup kettle, 23: digital dab ra- dio, dab radio, speaker, radio, my bose bluetooth speaker, 24: wheely bin, wheelie bin, black bin, recycling bin, bin, brown wheelie bin, 25: hairbrush, comb, hair brush, 26: necklace, brown leather bracelet, ladies silver bracelet, favourite ear- ings, silver male wedding band, clear quartz, 27: screwdriver, small space screwdriver, small screwdriver, spanner, wood phillips head, pex plumbers pliers, wood screw phillips head, 28: 12 measuring cup, 13 measuring cup, 14 measuring cup, 1 cup measuring cup, measuring spoon, measuring spoons, 29: lime green marker, yellow marker, pink marker, mi- genta marker, reptile green marker, pen, 30: digital recorder, dictaphone, pen friend, handheld police scanner, 31: face mask, covid mask, my purple mask, blue facemask, 32: airpod pro, single airpod, apple airpods, pro, 33: wireless earphones, bone conducting headset, bose earpods, my muse s headband, 34: phone stand, nobile phone stand, ipod stand, iphone stand, 35: memory stick, usb c dongle, usb stick, usb, 36: apple phone charger, phone charger, 37: scissors, secateurs, 38: car, my sisters car, 39: socks, sock, 40: fridge, fridge freezer indicator, 41: tv unit, flat screen television, tv, smarttv, television, 42: prosecco, jd whisky bottle, bottle of alcoholic drink, vagabond ale bottle, 43: lighter, vape pen, cannabis vape battery, 44: pepper shaker, pink himalayan salt, mediterranean sea salt, salt grinder, 45: sunglasses case, glasses case, eyewear case, 46: lipstick, chap stick, lip balm, 47: tweezers, finger nail clipper, hook , 48: inhaler, insulin pen, spacer, 49: dining table setup, desk, garden table, 50: liquid level indicator, water level sensor, 51: bottle opener, corkscrew, 52: pint glass, winetoothbrush, electric toothbrush, 66: alcohol wipe, skip prep, 67: my hearing aid, hearing aid, 68: magnifier, pocket mag- nifying glass, 69: blue artificial eye, green artificial eye, 70: hand saw, 71: tape measure, ruler, 72: electric sanding disc,</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head></head><label></label><figDesc>cup, my mug, 9: front door, back door, house door, front door to house, door, my front door, shed door, 10: sunglasses, my wraparound sunglasses, dark glasses, 11: watch, wrist watch, apple watch, apple wath, risk watch, my apple watch, 12: victor stream book reader, orbit braille reader and notetaker, victor reader stream, orbit reader 20 braille display, braillepen slim braille keyboard, braille orbit reader, solo audiobook player, braille note, my braille displat, 13: blue tooth keyboard, bluetooth keyboard, my keyboard, apple wireless keyboard, mini bluetooth keyboard, portable keyboard, keyboard, 14: medication, blister pack of painkillers, tablets, money, aspirin vs tylenol, aspirin, pill bottle, my pill dosette, buckleys, 15: backpack, my work backpack, work bag, bag, shoulder bag, bum bag, back pack, work backpack, 16: slippers, nike trainers, my shoes, boot, trainers, trainer shoe, slipper, 17: reading glasses, glasses, eye glasses, 18: baseball cap, cap, orange skullcap, my tilly hat, hat, my tilly hat upside down, 19: guide dog play cola, dogs lead, guide dog harness, retractable dog lead, leash, guide dog harness, dog lead, dogs, 20: my water bottle, bottle, pop bottle, green water bottle, water bottle, 21: hairbrush, comb, hair brush, 22: adaptive washing machine, washing machine, tumble dryer, adaptive dryer, 23: digital recorder, dictaphone, pen friend, handheld police scanner, 24: wheely bin, wheelie bin, black bin, recycling bin, bin, 25: face mask, covid mask, my purple mask, blue facemask, 26: screwdriver, small space screwdriver, small screwdriver, spanner, wood phillips head, pex plumbers pliers, 27: 12 measuring cup, 13 measuring cup, 14 measuring cup, 1 cup measuring cup, measuring spoon, 28: lime green marker, yellow marker, pink marker, migenta marker, reptile green marker, 29: airpod pro, single airpod, apple airpods, pro, 30: wireless earphones, bone conducting headset, bose earpods, my muse s headband, 31: phone stand, nobile phone stand, ipod stand, iphone stand, 32: memory stick, usb c dongle, usb stick, usb, 33: scissors, secateurs, 34: socks, sock, 35: necklace, brown leather bracelet, ladies silver bracelet, favourite earings, 36: microwave, toaster, kettle, one cup kettle, 37: fridge, fridge</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head></head><label></label><figDesc>freezer indicator, 38: prosecco, jd whisky bottle, bottle of alcoholic drink, vagabond ale bottle, 39: digital dab radio, dab radio, speaker, 40: apple phone charger, phone charger, 41: sunglasses case, glasses case, eyewear case, 42: lipstick, chap stick, lip balm, 43: tv unit, flat screen television, tv, smarttv, 44: dining table setup, desk, garden table, 45: liquid level indicator, water level sensor, 46: bottle opener, corkscrew, 47: pint glass, wine glass, glass, 48: lighter, Clusters for Figure A.7c [44 train users, 278 objects, 100 clusters] 1: keys, house keys, radar key, my keys, front door keys, keychain, door keys, key, set of keys, 2: remote, tv remote, apple tv remote, amazon remote control, television remote control, remote control, virgin remote control, tv remote control, samsung tv remote control, presentation remote, sky q remote, clickr, fire stick remote, control, television control, remote tv, 3: symbol cane, black mobility cane, my cane, white mobility came, folded white cane, long cane, cane, folded cane, folded long guide cane, p939411 white cane, white came, visibility stick, mobility, mobility cane, white cane, 4: earphones, apple headphones, apple earpods, airpods, my airpod pros case, iphone air pods, airpods pro charging case, airpod case, airpods, my airpods, airpods case, earpods, 5: black small wallet, my purse, my wallet, ladies purse, money pouch, coin purse, wallet for bus pass cards and money, i d wallet, ipod in wallet, walletv, wallet, purse, 6: blue headphones, my headphones, headphone, bose wireless headphones, headset, my sennheiser pxc 350-2, headphones, 7: phone, mobile phone, iphone 6, apple mobile phone, i phone 11 pro, iphone in case, iphone, cell phone, work phone, phone case, 8: sunglasses, my wraparound sunglasses, dark glasses, 9: medication, blister pack of painkillers, tablets, money, aspirin vs tylenol, aspirin, pill bottle, my pill dosette, buckleys, 10: mug, personal mug, cup again, coffee mug, toddler cup, styrofoam cup, my mug, 11: reading glasses, glasses, eye glasses, 12: slippers, nike trainers, my shoes, boot, trainers, trainer shoe, slipper, 13: watch, wrist watch, apple watch, apple wath, risk watch, my apple watch, 14: backpack, my work backpack, work bag, bag, shoulder bag, bum bag, back pack, work backpack, 15: guide dog play cola, dogs lead, guide dog harness, retractable dog lead, leash, guide dog harness, dog lead, dogs, 16: front door, back door, house door, front door to house, door, my front door, shed door, 17: blue tooth keyboard, bluetooth keyboard, my keyboard, apple wireless keyboard, mini bluetooth keyboard, portable keyboard, keyboard, 18: face mask, covid mask, my purple mask, blue facemask, 19: baseball cap, cap, orange skullcap, my tilly hat, hat, my tilly hat upside down, 20: hairbrush, comb, hair brush, 21: victor stream book reader, orbit braille reader and notetaker, victor reader stream, orbit reader 20 braille display, braillepen slim braille keyboard, braille orbit reader, solo audiobook player, braille note, my braille displat, 22: my water bottle, bottle, pop bottle, green water bottle, water bottle, 23: digital recorder, dictaphone, pen friend, handheld police scanner, 24: memory stick, usb c dongle, usb stick, usb, 25: sunglasses case, glasses case, eyewear case, 26: fridge, fridge freezer indicator, 27: adaptive washing machine, washing machine, tumble dryer, adaptive dryer, 28: pint glass, wine glass, glass, 29: airpod pro, single airpod, apple airpods, pro, 30: wireless earphones, bone conducting headset, bose earpods, my muse s headband, 31: mouse, 32: apple phone charger, phone charger, 33: scissors, secateurs, 34: local post box, post box, 35: wheely bin, wheelie bin, black bin, recycling bin, bin, 36: black strappy vest, t-shirts, 37: lipstick, chap stick, lip balm, 38: alcohol wipe, skip prep, 39: my hearing aid, hearing aid, 40: cooker, 41: microwave, toaster, kettle, one cup kettle, 42: bottle opener, corkscrew, 43: large sewing needle, knitting needle, 44: prosecco, jd whisky bottle, bottle of alcoholic drink, vagabond ale bottle, 45: lighter, vape pen, cannabis vape battery, 46: mayonnaise jar, mustard, 47: rice, chicken instant noodles, 48: tea, cranberry cream tea, 49: ottawa bus stop, bus stop sign, 50: money clip, 51: headphone case, 52: stylus, apple pencil, 53: my laptop, 54: compact disc, 55: digital dab radio, dab radio, speaker, 56: phone stand, nobile phone stand, ipod stand, iphone stand, 57: litter and dog waste bin, 58: socks, sock, 59: gloves, winter gloves, 60: sleep mask, 61: glasses cleaning wipe, 62: tissue box, 63: hand gel, lotion bottle, 64: make up, 65: clear nail varnish, 66: hair clip, headband, 67: toothbrush, 68: inhaler, insulin pen, 69: eye drops, 70: necklace, brown leather bracelet, ladies silver bracelet, favourite earings, 71: magnifier, pocket magnifying glass, 72: screwdriver, small space screwdriver, small screwdriver, spanner, wood phillips head, pex plumbers pliers, 73: hand saw, 74: battery drill, 75: tape measure, ruler, 76: electric sanding disc, miter saw, 77: tv unit, flat screen television, tv, smarttv, 78: bed, 79: sofa, 80: dining table setup, desk, garden table, Clusters for Figure A.7d [6 validation users, 50 objects, 36 clusters] 1: keys, house keys, radar key, my keys, front door keys, keychain, door keys, key, set of keys, 2: black small wallet, my purse, my wallet, ladies purse, money pouch, coin purse, wallet for bus pass cards and money, i d wallet, ipod in wallet, walletv, wallet, purse, 3: blue headphones, my headphones, headphone, bose wireless headphones, headset, my sennheiser pxc 350-2, headphones, 4: front door, back door, house door, front door to house, door, my front door, shed door, 5: remote, tv remote, apple tv remote, amazon remote control, television remote control, remote control, virgin remote control, tv remote control, samsung tv remote control, presentation remote, sky q remote, clickr, fire stick remote, control, television control, remote tv, 6: blue tooth keyboard, bluetooth keyboard, my keyboard, apple wireless keyboard, mini bluetooth keyboard, portable keyboard, keyboard, 7: phone, mobile phone, iphone 6, apple mobile phone, i phone 11 pro, iphone in case, iphone, cell phone, work phone, phone case, 8: watch, wrist watch, apple watch, apple wath, risk watch, my apple watch, 9: sunglasses, my wraparound sunglasses, dark glasses, 10: necklace, brown leather bracelet, ladies silver bracelet, favourite earings, 11: tv unit, flat screen television, tv, smarttv, 12: apple phone charger, phone charger, 13: car, 14: slippers, nike trainers, my shoes, boot, trainers, trainer shoe, slipper, 15: socks, sock, 16: my clock, 17: condom box, 18: baseball cap, cap, orange skullcap, my tilly hat, hat, my tilly hat upside down, 19: hairbrush, comb, hair brush, 20: perfume, deodorant, 21: tweezers, finger nail clipper , 22: hair clip, headband, 23: backpack, my work backpack, work bag, bag, shoulder bag, bum bag, back pack, work backpack, 24: symbol cane, black mobility cane, my cane, white mobility came, folded white cane, long cane, cane, folded cane, folded long guide cane, p939411 white cane, white came, visibility stick, mobility, mobility cane white cane, 25: victor stream book reader, orbit braille reader and notetaker, victor reader stream, orbit reader 20 braille display, braillepen slim braille keyboard, braille orbit reader, solo audiobook player, braille note, my braille displat, 26: magnifier, pocket magnifying glass, 27: guide dog play cola, dogs lead, guide dog harness, retractable dog lead, leash, guide dog harness, dog lead, dogs, 28: shelf unit with things, wardrobe, 29: dining table setup, desk, garden table, 30: vase with flowers, 31: mug, personal mug, cup again, coffee mug, toddler cup, styrofoam cup, my mug, 32: wall plug, 33: paperclips, 34: prosecco, jd whisky bottle, bottle of alcoholic drink, vagabond ale bottle, 35: pinesol cleaner, 36: my slate Clusters for Figure A.7e [17 test users, 158 objects, 67 clusters] 1: black small wallet, my purse, my wallet, ladies purse, money pouch, coin purse, wallet for bus pass cards and money, i d wallet, ipod in wallet, walletv, wallet, purse, 2: keys, house keys, radar key, my keys, front door keys, keychain, door keys, key, set of keys, 3: remote, tv remote, apple tv remote, amazon remote control, television remote control, remote control, virgin remote control, tv remote control, samsung tv remote control, presentation remote, sky q remote, clickr, fire stick remote, control, television control, remote tv, 4: front door, back door, house door, front door to house, door, my front door, shed door, 5: earphones, apple headphones, apple earpods, airpods, my airpod pros case, iphone air pods, airpods pro charging case, airpod case, airpods, my airpods, airpods case, earpods, 6: victor stream book reader, orbit braille reader and notetaker, victor reader stream, orbit reader 20 braille display, braillepen slim braille keyboard, braille orbit reader, solo audiobook player, braille note, my braille displat, 7: 12 measuring cup, 13 measuring cup, 14 measuring cup, 1 cup measuring cup, measuring spoon, 8: lime green marker, yellow marker, pink marker, migenta marker, reptile green marker, 9: watch, wrist watch, apple watch, apple wath, risk watch, my apple watch, 10: symbol cane, black mobility cane, my cane, white mobility came, folded white cane, long cane, cane, folded cane, folded long guide cane, p939411 white cane, white came, visibility stick, mobility, mobility cane, white cane, 11: screwdriver, small space screwdriver, small screwdriver, spanner, wood phillips head, pex plumbers pliers, 12: mug, personal mug, cup again, coffee mug, toddler cup, styrofoam cup, my mug, 13: blue headphones, my headphones, headphone, bose wireless headphones, headset, my sennheiser pxc 350-2, headphones, 14: blue tooth keyboard, bluetooth keyboard, my keyboard, apple wireless keyboard, mini bluetooth keyboard, portable keyboard, keyboard, 15: phone, mobile phone, iphone 6, apple mobile phone, i phone 11 pro, iphone in case, iphone, cell phone, work phone, phone case, 16: phone stand, nobile phone stand, ipod stand, iphone stand, 17: wheely bin, wheelie bin, black bin, recycling bin, bin, 18: backpack, my work backpack, work bag, bag, shoulder bag, bum bag, back pack, work backpack, 19: adaptive washing machine, washing machine, tumble dryer, adaptive dryer, 20: my water bottle, bottle, pop bottle, green water bottle, water bottle, 21: airpod pro, single airpod, apple airpods, pro, 22: wireless earphones, bone conducting headset, bose earpods, my muse s headband, 23: digital recorder, dictaphone, pen friend, handheld police scanner, 24: digital dab radio, dab radio, speaker, 25: scissors, secateurs, 26: slippers, nike trainers, my shoes, boot, trainers, trainer shoe, slipper, 27: socks, sock, 28: sunglasses, my wraparound sunglasses, dark glasses, 29: baseball cap, cap, orange skullcap, my tilly hat, hat, my tilly hat upside down, 30: microwave, toaster, kettle, one cup kettle, 31: knitting basket, washing basket, basket, 32: liquid level indicator, water level sensor, 33: pepper shaker, pink himalayan salt, mediterranean sea salt, 34: dog streetball, dog toy, adaptive tennis ball, 35: stylus, apple pencil, 36: ps4 controller, 37: ipad, 38: memory stick, usb c dongle, usb stick, usb, 39: car, 40: garden wall, 41: garden shed, 42: gloves, winter gloves, 43: face mask, covid mask, my purple mask, blue facemask, 44: reading glasses, glasses, eye glasses, 45: hand gel, lotion bottle, 46: hairbrush, comb, hair brush, 47: perfume, deodorant, 48: lipstick, chap stick, lip balm, 49: tweezers, finger nail clipper , 50: medication, blister pack of painkillers, tablets, money, aspirin vs tylenol, aspirin, pill bottle, my pill dosette, buckleys, 51: inhaler, insulin pen, 52: necklace, brown leather bracelet, ladies silver bracelet, favourite earings, 53: guide dog play cola, dogs lead, guide dog harness, retractable dog lead, leash, guide dog harness, dog lead, dogs, 54: hand saw, 55: tape measure, ruler, 56: electric sanding disc, miter saw, 57: fridge, fridge freezer indicator, 58: cushion, 59: shelf unit with things, wardrobe, 60: dining table setup, desk, garden table, 61: table fan, 62: stairgate, back patio gate, 63: watering can, watering, 64: veg peeler, 65: bottle opener, corkscrew, 66: prosecco, jd whisky bottle, bottle of alcoholic drink, vagabond ale bottle, 67: lighter, vape pen, cannabis vape battery</figDesc><table><row><cell>vape pen, cannabis vape battery, 49: pepper shaker, pink</cell></row><row><cell>himalayan salt, mediterranean sea salt, 50: dog streetball,</cell></row><row><cell>dog toy, adaptive tennis ball, 51: stylus, apple pencil, 52: 81: mirror, 82: tred mill, 83: exercise bench, 84: skipping</cell></row><row><cell>mouse, 53: car, 54: local post box, post box, 55: black rope, 85: stairgate, back patio gate, 86: liquid level indicator,</cell></row><row><cell>strappy vest, t-shirts, 56: gloves, winter gloves, 57: hand gel, water level sensor, 87: sharp knife, 88: embroidery thread</cell></row><row><cell>lotion bottle, 58: perfume, deodorant, 59: tweezers, finger cone, 89: hole punch, 90: pencil case, 91: baked bean tin,</cell></row><row><cell>nail clipper , 60: hair clip, headband, 61: inhaler, insulin pen, 92: banana, 93: pepper shaker, pink himalayan salt, mediter-</cell></row><row><cell>62: alcohol wipe, skip prep, 63: my hearing aid, hearing aid, ranean sea salt, 94: mountain dew can, 95: fish food, 96:</cell></row><row><cell>64: magnifier, pocket magnifying glass, 65: hand saw, 66: dog poo, 97: dog streetball, dog toy, adaptive tennis ball, 98:</cell></row><row><cell>tape measure, ruler, 67: electric sanding disc, miter saw, 68: av tambourine, 99: grinder, 100: journal</cell></row><row><cell>cooker, 69: shelf unit with things, wardrobe, 70: knitting</cell></row><row><cell>basket, washing basket, basket, 71: stairgate, back patio gate,</cell></row><row><cell>72: large sewing needle, knitting needle, 73: mayonnaise jar,</cell></row><row><cell>mustard, 74: rice, chicken instant noodles, 75: tea, cranberry</cell></row><row><cell>cream tea, 76: ottawa bus stop, bus stop sign, 77: money</cell></row><row><cell>clip, 78: headphone case, 79: ps4 controller, 80: my laptop,</cell></row><row><cell>81: ipad, 82: compact disc, 83: garden wall, 84: garden</cell></row><row><cell>shed, 85: litter and dog waste bin, 86: my clock, 87: sleep</cell></row><row><cell>mask, 88: glasses cleaning wipe, 89: tissue box, 90: condom</cell></row><row><cell>box, 91: make up, 92: clear nail varnish, 93: toothbrush, 94:</cell></row><row><cell>eye drops, 95: battery drill, 96: bed, 97: sofa, 98: cushion,</cell></row><row><cell>99: mirror, 100: table fan, 101: tred mill, 102: exercise</cell></row><row><cell>bench, 103: skipping rope, 104: watering can, watering, 105:</cell></row><row><cell>vase with flowers, 106: veg peeler, 107: sharp knife, 108:</cell></row><row><cell>wall plug, 109: embroidery thread cone, 110: hole punch,</cell></row><row><cell>111: pencil case, 112: paperclips, 113: baked bean tin, 114:</cell></row><row><cell>banana, 115: mountain dew can, 116: pinesol cleaner, 117:</cell></row><row><cell>fish food, 118: dog poo, 119: my slate, 120: av tambourine,</cell></row><row><cell>121: grinder, 122: journal</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head></head><label></label><figDesc>Train user tasks We sample T train tasks (per epoch) for each train user ? ? K train as follows:1. Way: sample a set of the user's objectsP ? ? P ? 2. Shot: for each object p ?P ? , sample n p ? [1, . . . , N ? p ] and m p ? [1, . . . , M ? p ] 7 3. Construct context set C ? = {(v, p)For memory reasons, during training we impose a video cap per object of n p = 5 and m p = 4 if |P ? | ? 6 (otherwise the caps are doubled) and an object cap of |P ? | = 10.Test user tasks We sample T test tasks (per epoch) for each test user ? ? K test as follows:1. Way: select all the user's objects,P ? = P ? (i.e. test personalization for all objects) 2. Shot: for each object p ?P ? , n p = N ? p and m p = M ? p (i.e. use all context and target videos) 7 3. Construct context set C ? = {(v, p) For testing, no caps are imposed on the number of videos per object, or number of objects.</figDesc><table><row><cell></cell><cell>np ? V</cell><cell>? p | ?p ?P ? }</cell></row><row><cell>4. Construct target set T ? = {(v, p)</cell><cell cols="2">mp ? V ? p | ?p ?P ? }</cell></row><row><cell></cell><cell>np ? V</cell><cell>? p | ?p ?P ? }</cell></row><row><cell>4. Construct target set T ? = {(v, p)</cell><cell cols="2">mp ? V ? p | ?p ?P ? }</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head></head><label></label><figDesc>.7). The number of tasks sampled per test (and validation) user was selected to reduce variance in the reported metrics. The number of clips per video and the clip length were selected based on available GPU memory during training/testing (2? Dell NVIDIA Tesla V100 32GB GPUs).</figDesc><table><row><cell>? Tasks per train user, T train = 50</cell></row><row><cell>? Tasks per test (and validation) user, T test = 5</cell></row><row><cell>? Clip length, L = 8 frames</cell></row><row><cell>? Clips per context video in a train task, S train C ? Clips per target video in a train task, S train = 4 = 4 T ? Clips per context video in a test task, S test C = 8</cell></row></table><note>7 Note, N and M in Eq. (3) and Eq. (5) are N = p?P ? np and M = p?P ? mp, respectively.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19"><head></head><label></label><figDesc>ProtoNets<ref type="bibr" target="#b39">[40]</ref> 58.98 (2.23) 11.55 (1.79) 69.17 (3.01) 46.97 (1.84) 20.42 (1.71) 52.80 (2.53) CNAPs [38] 51.86 (2.49) 20.81 (2.33) 60.77 (3.18) 41.59 (1.94) 30.72 (2.13) 43.00 (2.51) MAML [9] 42.55 (2.67) 37.28 (2.99) 46.96 (3.25) 24.35 (1.83) 62.30 (2.34) 25.73 (2.21) FineTuner [46] 61.01 (2.24) 11.53 (1.82) 72.60 (2.91) 48.45 (1.86) 19.13 (1.69) 54.07 (2.52) Table A.3: Models perform poorly when meta-trained on Meta-Dataset and meta-tested on ORBIT test users, suggesting that existing few-shot datasets may be insufficient for real-world adaptation. Results are reported as the average (95% confidence interval) over all target videos pooled from 85 test tasks (5 tasks per test user, 17 test users).</figDesc><table><row><cell></cell><cell cols="3">Clean Video Evaluation (CLE-VE)</cell><cell cols="3">Clutter Video Evaluation (CLU-VE)</cell></row><row><cell>MODEL</cell><cell>FRAME ACC</cell><cell>FTR</cell><cell>VIDEO ACC</cell><cell>FRAME ACC</cell><cell>FTR</cell><cell>VIDEO ACC</cell></row><row><cell></cell><cell cols="3">Clean Video Evaluation (CLE-VE)</cell><cell cols="3">Clutter Video Evaluation (CLU-VE)</cell></row><row><cell>MODEL</cell><cell>FRAME ACC</cell><cell>FTR</cell><cell>VIDEO ACC</cell><cell>FRAME ACC</cell><cell>FTR</cell><cell>VIDEO ACC</cell></row><row><cell cols="2">ProtoNets [40] 65.31 (2.10)</cell><cell>7.44 (1.42)</cell><cell cols="4">78.67 (2.67) 49.95 (1.76) 14.87 (1.50) 59.07 (2.49)</cell></row><row><cell>CNAPs [38]</cell><cell>67.21 (2.14)</cell><cell>9.30 (1.59)</cell><cell cols="4">79.56 (2.63) 51.26 (1.76) 15.36 (1.54) 60.13 (2.48)</cell></row><row><cell>MAML [9]</cell><cell>68.98 (2.16)</cell><cell>9.54 (1.68)</cell><cell cols="4">79.45 (2.63) 50.95 (1.92) 21.67 (1.89) 55.00 (2.52)</cell></row><row><cell cols="2">FineTuner [46] 69.01 (2.19)</cell><cell>7.56 (1.48)</cell><cell cols="4">78.45 (2.68) 53.17 (1.85) 15.32 (1.57) 62.73 (2.45)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_20"><head></head><label></label><figDesc>Table A.4: User-agnostic training on the ORBIT dataset (i.e. a train task is sampled from objects pooled across all users in K train ) performs as well as user-centric training (i.e. a train task is sampled from only one user at a time). Results are reported as the average (95% confidence interval) over all target videos pooled from 85 test tasks (5 tasks per test user, 17 test users).</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_21"><head></head><label></label><figDesc>Results with ProtoNets<ref type="bibr" target="#b39">[40]</ref>.</figDesc><table><row><cell></cell><cell>0.9</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Frame accuracy</cell><cell>0.2 0.3 0.4 0.5 0.6 0.7 0.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Frames to recognition</cell><cell>0.1 0.2 0.3 0.4 0.5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Video accuracy</cell><cell>0.2 0.4 0.6 0.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0</cell><cell>P642</cell><cell>P753</cell><cell>P999</cell><cell>P901</cell><cell>P421</cell><cell>P953</cell><cell>P609</cell><cell>P452</cell><cell>P198</cell><cell>P485</cell><cell>P204</cell><cell>P455</cell><cell>P900</cell><cell>P177</cell><cell>P271</cell><cell>P233</cell><cell>P554</cell><cell>0</cell><cell>P642</cell><cell></cell><cell>P753</cell><cell cols="2">P999</cell><cell>P901</cell><cell>P421</cell><cell>P953</cell><cell>P609</cell><cell>P452</cell><cell>P198</cell><cell>P485</cell><cell>P204</cell><cell>P455</cell><cell>P900</cell><cell>P177</cell><cell>P271</cell><cell>P233</cell><cell>P554</cell><cell></cell><cell>0</cell><cell>P642</cell><cell>P753</cell><cell>P999</cell><cell>P901</cell><cell>P421</cell><cell>P953</cell><cell>P609</cell><cell>P452</cell><cell>P198</cell><cell>P485</cell><cell>P204</cell><cell>P455</cell><cell>P900</cell><cell>P177</cell><cell>P271</cell><cell>P233</cell><cell>P554</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Collector</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Collector</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Collector</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="15">(a) Results with CNAPs [38].</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Frame accuracy</cell><cell>0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Frames to recognition</cell><cell cols="2">0.1 0.15 0.2 0.25 0.3 0.35</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Video accuracy</cell><cell>1 0.2 0.4 0.6 0.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">0.05</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0</cell><cell>P642</cell><cell>P753</cell><cell>P999</cell><cell>P901</cell><cell>P421</cell><cell>P953</cell><cell>P609</cell><cell>P452</cell><cell>P198</cell><cell>P485</cell><cell>P204</cell><cell>P455</cell><cell>P900</cell><cell>P177</cell><cell>P271</cell><cell>P233</cell><cell>P554</cell><cell></cell><cell>0</cell><cell>P642</cell><cell>P753</cell><cell></cell><cell>P999</cell><cell>P901</cell><cell>P421</cell><cell>P953</cell><cell>P609</cell><cell>P452</cell><cell>P198</cell><cell>P485</cell><cell>P204</cell><cell>P455</cell><cell>P900</cell><cell>P177</cell><cell>P271</cell><cell>P233</cell><cell>P554</cell><cell></cell><cell>0</cell><cell>P642</cell><cell>P753</cell><cell>P999</cell><cell>P901</cell><cell>P421</cell><cell>P953</cell><cell>P609</cell><cell>P452</cell><cell>P198</cell><cell>P485</cell><cell>P204</cell><cell>P455</cell><cell>P900</cell><cell>P177</cell><cell>P271</cell><cell>P233</cell><cell>P554</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Collector</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Collector</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Collector</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Frame accuracy</cell><cell>0 1 0.2 0.4 0.6 0.8</cell><cell cols="22">P753 (b) P642 P999 P901 P421 P953 P609 P452 P198 P485 P204 P455 P900 P177 P271 P233 P554 P642 P753 0 0.4 0.05 0.1 0.15 0.2 0.25 0.3 0.35 Frames to recognition</cell><cell>P999</cell><cell>P901</cell><cell>P421</cell><cell>P953</cell><cell>P609</cell><cell>P452</cell><cell>P198</cell><cell>P485</cell><cell>P204</cell><cell>P455</cell><cell>P900</cell><cell>P177</cell><cell>P271</cell><cell>P233</cell><cell>P554</cell><cell>Video accuracy</cell><cell>0 1 0.2 0.4 0.6 0.8</cell><cell>P642</cell><cell>P753</cell><cell>P999</cell><cell>P901</cell><cell>P421</cell><cell>P953</cell><cell>P609</cell><cell>P452</cell><cell>P198</cell><cell>P485</cell><cell>P204</cell><cell>P455</cell><cell>P900</cell><cell>P177</cell><cell>P271</cell><cell>P233</cell><cell>P554</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Collector</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Collector</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Collector</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="14">(c) Results with MAML [9].</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Frame accuracy</cell><cell>0.2 0.4 0.6 0.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Frames to recognition</cell><cell cols="2">0.05 0.1 0.15 0.2 0.25 0.3 0.35</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Video accuracy</cell><cell>0.2 0.4 0.6 0.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0</cell><cell>P642</cell><cell>P753</cell><cell>P999</cell><cell>P901</cell><cell>P421</cell><cell>P953</cell><cell>P609</cell><cell>P452</cell><cell>P198</cell><cell>P485</cell><cell>P204</cell><cell>P455</cell><cell>P900</cell><cell>P177</cell><cell>P271</cell><cell>P233</cell><cell>P554</cell><cell></cell><cell></cell><cell>P642</cell><cell>P753</cell><cell></cell><cell>P999</cell><cell>P901</cell><cell>P421</cell><cell>P953</cell><cell>P609</cell><cell>P452</cell><cell>P198</cell><cell>P485</cell><cell>P204</cell><cell>P455</cell><cell>P900</cell><cell>P177</cell><cell>P271</cell><cell>P233</cell><cell>P554</cell><cell></cell><cell>0</cell><cell>P642</cell><cell>P753</cell><cell>P999</cell><cell>P901</cell><cell>P421</cell><cell>P953</cell><cell>P609</cell><cell>P452</cell><cell>P198</cell><cell>P485</cell><cell>P204</cell><cell>P455</cell><cell>P900</cell><cell>P177</cell><cell>P271</cell><cell>P233</cell><cell>P554</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Collector</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Collector</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Collector</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="16">(d) Results with FineTuner [46].</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_24"><head>Table A .</head><label>A</label><figDesc>6: Meta-training and -testing with more objects per user poses a harder recognition problem, however, metatraining with fewer objects than encountered at meta-testing shows only a small CLU-VE performance drop compared to</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_26"><head>Table A .</head><label>A</label><figDesc>7: Sampling more tasks per train user has limited benefits on CLU-VE performance beyond T train = 50 suggesting that sampling techniques that account for frame informativeness may be required.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Note, y f = p where p ? P ? is the video-level object label</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">See merged collectors in data/orbit_benchmark_users _to_split.json in the code repository<ref type="bibr" target="#b2">3</ref> See data/orbit_benchmark_mode_splits.json in the code repository</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">https://www.gapminder.org/fw/income-levels</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">Because there are similar objects across the train users, we use the objects' cluster labels (i.e. 100-way classification task), as inFigure A.7c</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The ORBIT Dataset is funded by Microsoft AI for Accessibility. LZ is supported by the 2017 MSR PhD Scholarship Program and 2020 MSR EMEA PhD Award. JB is supported by the EPSRC Prosperity Partnership EP/T005386/1. We thank VICTA, RNC, RNIB, CNIB, Humanware, Tekvision School for the Blind, BlindSA, NFB, and AbilityNet. Finally, we thank Emily Madsen for help with the video validation, and all the ORBIT collectors for their time and contributions.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.2. Non-episodic few-shot learning</head><p>A non-episodic approach <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b4">5]</ref> is characterized by training the model in a standard batch-wise manner. For a train user ? ? K train , a batch B ? is the union of the user's context and target videos from Eq. (3): <ref type="bibr" target="#b4">(5)</ref> where v is a context or target video, p ? P all is the object label, and P all is the set of personal objects pooled across all users in K train .</p><p>A recognition model f is trained over many randomly sampled batches per train user ? ? K train , typically by maximizing the likelihood of predicting the user's correct personal object in each frame in a target video v f ? v:</p><p>where p ? (y f | v f ) is the probability the (generic) model f ? assigns to the ground-truth label y f ? P all for frame v f . As above, typically SGD and a cross entropy loss are used. Unlike an episodic approach, the resulting f ? * is a generic recognizer for all the objects in P all and does not explicitly know how to personalize. Instead, the generic model is personalized to a new test user ? ? K test by fine-tuning f ? * at test-time with the user's context videos C ? of their personal objects. This is equivalent to transfer learning where the (generic) feature extractor is typically frozen and only a new linear classification layer is learned per user for their objects -together giving the user's personalized parameters ? ? . As above, the user can then scan f ? ? around to identify their objects in target scenarios using Eq. (1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Implementation Details</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.1. Baselines</head><p>ProtoNets <ref type="bibr" target="#b39">[40]</ref>. ProtoNets is a metric-based few-shot learner which classifies a target frame in a task based on the shortest distance (typically squared Euclidean) between its embedding and the mean embedding of each object class computed using the task's context set. We use a ResNet18 <ref type="bibr" target="#b14">[15]</ref> feature extractor pre-trained on ILSVRC <ref type="bibr" target="#b38">[39]</ref>. We then train the model episodically on T train tasks per train user (drawn from K train ) for 20 epochs with a learning rate of 10 -4 (reduced by 0.1 for the feature extractor). Note, each epoch samples T train new tasks per train user. We evaluate the trained model on all target videos per test user in K test (repeated for T test tasks per user), and report the metrics over T all , the flattened set of all target videos across all tasks for all test users.</p><p>CNAPs <ref type="bibr" target="#b37">[38]</ref>. CNAPs is an amortization-based few-shot learner which classifies a target frame in a task with a model that has been generated by forward passing the task's context set through a hypernet. In practice, the hypernet generates only a subset of the model's parameters following <ref type="bibr" target="#b37">[38]</ref> -the task's classifier, and FiLM adapters <ref type="bibr" target="#b34">[35]</ref> which are placed after convolution blocks in the feature extractor. We use a ResNet18 <ref type="bibr" target="#b14">[15]</ref> feature extractor that is pre-trained on ILSVRC <ref type="bibr" target="#b38">[39]</ref> and kept frozen. We train the hypernet episodically on T train tasks per train user (drawn from K train ) for 10 epochs with a learning rate of 10 -4 . Note, each epoch samples T train new tasks per train user. We evaluate the trained model on all target videos per test user in K test (repeated for T test tasks per user), and report the metrics over T all . MAML <ref type="bibr" target="#b8">[9]</ref>. MAML is an optimization-based few-shot learner which classifies a target frame in a task with a model after it has taken G gradient steps on the task's context set. We use a ResNet18 <ref type="bibr" target="#b14">[15]</ref> feature extractor that is pre-trained on ILSVRC <ref type="bibr" target="#b38">[39]</ref>. We then train the model episodically on T train tasks per train user (drawn from K train ) for 10 epochs with the Adam optimizer <ref type="bibr" target="#b19">[20]</ref> and a learning rate of 10 -5 for the outer loop, and the SGD optimizer and a learning rate of 10 -3 for the inner loop (rates reduced by 0.1 for the feature extractor in both loops). Note, each epoch samples T train new tasks per train user. We evaluate the trained model on all target videos per test user in K test (repeated for T test tasks per user), and report the metrics over T all . Note that since each (train and test) task can have a different number of objects, we append a new classifier per task, initialized with zeroes following <ref type="bibr" target="#b47">[48]</ref>. The classifier is therefore not learned but instead the result after G gradient steps. At both train and test time, G = 15. FineTuner <ref type="bibr" target="#b45">[46]</ref>. FineTuner is a non-episodic few-shot learner which classifies a target frame in a task with a model that has been finetuned using the task's context set, typically with its feature extractor frozen. This is equivalent to a transfer learning approach. We use a ResNet18 <ref type="bibr" target="#b14">[15]</ref> feature extractor pre-trained on ILSVRC <ref type="bibr" target="#b38">[39]</ref>. Following <ref type="bibr" target="#b45">[46]</ref>, we first train the extractor using K train : we pool the personal objects across all train users in K train and train a standard classification model 6 for 10 epochs using T train tasks per train user. Note, each epoch samples T train new tasks per train user. Because the model is non-episodic, the context and target videos of each task are pooled and treated as a standard batch, following Eq. (5). We use the Adam optimizer <ref type="bibr" target="#b19">[20]</ref> and a learning rate of 10 -4 (reduced by 0.1 for the feature extractor). At test time, this feature extractor is frozen, and for each test user drawn from K test , a new classifier is appended, initialized with zeroes following <ref type="bibr" target="#b47">[48]</ref> and tuned for G = 50 gradient steps on the user's context videos, using SGD and a learning rate of 0.1. We evaluate the trained</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">ObjectNet: A large-scale bias-controlled dataset for pushing the limits of object recognition models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><surname>Barbu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mayo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Alverio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Gutfreund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josh</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boris</forename><surname>Katz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33rd Annual Conference on Neural Information Processing Systems (NeurIPS)</title>
		<meeting>the 33rd Annual Conference on Neural Information Processing Systems (NeurIPS)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Meta-learning with differentiable closed-form solvers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Bertinetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jo?o</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th International Conference on Learning Representations (ICLR)</title>
		<meeting>the 7th International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Enriching word vectors with subword information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics (TACL)</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">15</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Self-supervised learning for few-shot image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Da</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuefeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<meeting>the IEEE International Conference on Acoustics, Speech and Signal Processing</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">A new meta-baseline for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinbo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huijuan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.04390</idno>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">29</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Describing textures in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mircea</forename><surname>Cimpoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhransu</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sammy</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Scaling egocentric vision: The EPIC-KITCHENS dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dima</forename><surname>Damen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hazel</forename><surname>Doughty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giovanni</forename><forename type="middle">Maria</forename><surname>Farinella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonino</forename><surname>Furnari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evangelos</forename><surname>Kazakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davide</forename><surname>Moltisanti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Munro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toby</forename><surname>Perrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Wray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">ImageNet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Modelagnostic meta-learning for fast adaptation of deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning (ICML)</title>
		<meeting>the 34th International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page">34</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timnit</forename><surname>Gebru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Morgenstern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Briana</forename><surname>Vecchione</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jennifer</forename><forename type="middle">Wortman</forename><surname>Vaughan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanna</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hal</forename><surname>Daum?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Crawford</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.09010</idno>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">24</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">Datasheets for datasets. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Meta-learning probabilistic inference for prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Bronskill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Turner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th International Conference on Learning Representations (ICLR)</title>
		<meeting>the 7th International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The &quot;something something&quot; video database for learning and evaluating visual common sense</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raghav</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samira</forename><forename type="middle">Ebrahimi</forename><surname>Kahou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Michalski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joanna</forename><surname>Materzy?ska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Susanne</forename><surname>Westphal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heuna</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valentin</forename><surname>Haenel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ingo</forename><surname>Fruend</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Yianilos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moritz</forename><surname>Mueller-Freitag</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">VizWiz grand challenge: Answering visual questions from blind people</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danna</forename><surname>Gurari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abigale</forename><forename type="middle">J</forename><surname>Stangl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anhong</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristen</forename><surname>Grauman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><forename type="middle">P</forename><surname>Bigham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">Girshick</forename><surname>Mask R-Cnn</surname></persName>
		</author>
		<idno>2017. 1</idno>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">29</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Leveraging the feature distribution in transfer-based few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuqing</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Gripon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">St?phane</forename><surname>Pateux</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.03806</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">The Quick, Draw! -A.I. experiment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Jongejan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henry</forename><surname>Rowley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takashi</forename><surname>Kawashima</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongmin</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Fox-Gieg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Teachable machines for accessibility</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hernisa</forename><surname>Kacorri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International ACM SIGACCESS Conference on Computers and Accessibility (ASSETS)</title>
		<meeting>the International ACM SIGACCESS Conference on Computers and Accessibility (ASSETS)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">IncluSet: A data surfacing repository for accessibility datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hernisa</forename><surname>Kacorri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Utkarsh</forename><surname>Dwivedi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sravya</forename><surname>Amancherla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mayanka</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Riya</forename><surname>Chanduka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International ACM SIGACCESS Conference on Computers and Accessibility (ASSETS)</title>
		<meeting>the International ACM SIGACCESS Conference on Computers and Accessibility (ASSETS)</meeting>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd International Conference on Learning Representations (ICLR)</title>
		<meeting>the 3rd International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">WILDS: A Benchmark of in-the-Wild Distribution Shifts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pang</forename><surname>Wei Koh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiori</forename><surname>Sagawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henrik</forename><surname>Marklund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sang</forename><forename type="middle">Michael</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marvin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akshay</forename><surname>Balsubramani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michihiro</forename><surname>Yasunaga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">Lanas</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sara</forename><surname>Beery</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th International Conference on Machine Learning (ICML)</title>
		<meeting>the 38th International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">On episodes, prototypical networks, and few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steinar</forename><surname>Laenen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Bertinetto</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.09831</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">One shot learning of simple visual concepts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brenden</forename><surname>Lake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Meeting of the Cognitive Science Society (CogSci)</title>
		<meeting>the Annual Meeting of the Cognitive Science Society (CogSci)</meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Hands holding clues for object recognition in teachable machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyungjun</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hernisa</forename><surname>Kacorri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM Conference on Human Factors in Computing Systems (CHI)</title>
		<meeting>the ACM Conference on Human Factors in Computing Systems (CHI)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Keysegments for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong Jae</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaechul</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristen</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Microsoft COCO: Common Objects in Context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Core50: A new dataset and benchmark for continuous object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincenzo</forename><surname>Lomonaco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davide</forename><surname>Maltoni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Robot Learning (CoRL)</title>
		<meeting>the Conference on Robot Learning (CoRL)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Rehearsal-free continual learning over small non-IID batches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincenzo</forename><surname>Lomonaco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davide</forename><surname>Maltoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Pellegrini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshop, 2020</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshop, 2020</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning video object segmentation from unlabeled videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiankai</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenguan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbing</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Wing</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">J</forename><surname>Crandall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hoi</surname></persName>
		</author>
		<idno>2020. 4</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Exploring the limits of weakly supervised pretraining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Mahajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vignesh</forename><surname>Ramanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Ashwin Bharambe, and Laurens van der Maaten</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">ORBIT: A realworld few-shot dataset for teachable object recognition collected from people who are blind or low vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniela</forename><surname>Massiceti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lida</forename><surname>Theodorou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luisa</forename><surname>Zintgraf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">Tobias</forename><surname>Harris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simone</forename><surname>Stumpf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cecily</forename><surname>Morrison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Cutrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katja</forename><surname>Hofmann</surname></persName>
		</author>
		<idno type="DOI">10.25383/city.14294597</idno>
		<ptr target="https://doi.org/10.25383/city.14294597.City" />
		<imprint>
			<date type="published" when="2021-04" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
		<respStmt>
			<orgName>University of London</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Automated flower classification over a large number of classes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria-Elena</forename><surname>Nilsback</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Indian Conference on Computer Vision, Graphics &amp; Image Processing (ICVGIP)</title>
		<meeting>the Indian Conference on Computer Vision, Graphics &amp; Image Processing (ICVGIP)</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Meta-curvature</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eunbyung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Junier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Oliva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33rd Annual Conference on Neural Information Processing Systems (NeurIPS)</title>
		<meeting>the 33rd Annual Conference on Neural Information Processing Systems (NeurIPS)</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning video object segmentation from static images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Khoreva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
		<idno>2017. 4</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">FiLM: Visual reasoning with a general conditioning layer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ethan</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Harm De</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd AAAI Conference on Artificial Intelligence (AAAI)</title>
		<meeting>the 32nd AAAI Conference on Artificial Intelligence (AAAI)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">29</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">YOLO9000: Better, faster, stronger</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<idno>2017. 1</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th Annual Conference on Neural Information Processing Systems (NeurIPS)</title>
		<meeting>the 29th Annual Conference on Neural Information Processing Systems (NeurIPS)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Fast and flexible multitask classification using conditional neural adaptive processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Requeima</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Bronskill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">E</forename><surname>Turner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33rd Annual Conference on Neural Information Processing Systems (NeurIPS)</title>
		<meeting>the 33rd Annual Conference on Neural Information Processing Systems (NeurIPS)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page">34</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">ImageNet Large Scale Visual Recognition Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">29</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Prototypical networks for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jake</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st Annual Conference on Neural Information Processing Systems</title>
		<meeting>the 31st Annual Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page">34</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Hands on&quot; visual recognition for visually impaired users</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Sosa-Garc?a</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesca</forename><surname>Odone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Accessible Computing (TACCESS)</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="30" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Inception-V4, Inception-ResNet and the impact of residual connections on learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">A</forename><surname>Alemi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st AAAI Conference on Artificial Intelligence (AAAI)</title>
		<meeting>the 31st AAAI Conference on Artificial Intelligence (AAAI)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">EfficientNet: Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th International Conference on Learning Representations (ICLR)</title>
		<meeting>the 7th International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Disability-first Dataset Creation: Lessons from Constructing a Dataset for Teachable Object Recognition with Blind and Low Vision Data Collectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lida</forename><surname>Theodorou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniela</forename><surname>Massiceti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luisa</forename><surname>Zintgraf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simone</forename><surname>Stumpf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cecily</forename><surname>Morrison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Cutrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">Tobias</forename><surname>Harris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katja</forename><surname>Hofmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Proceedings of the International ACM SIGACCESS Conference on Computers and Accessibility (ASSETS)</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Learning to learn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Thrun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorien</forename><surname>Pratt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>Springer Science &amp; Business Media</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Rethinking few-shot image classification: A good embedding is all you need?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page">34</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.08237</idno>
		<title level="m">Matthijs Douze, and Herv? J?gou. Fixing the train-test resolution discrepancy: FixEffi-cientNet</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Meta-Dataset: A Dataset of Datasets for Learning to Learn from Few Examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eleni</forename><surname>Triantafillou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tyler</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Lamblin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Utku</forename><surname>Evci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelvin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Goroshin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carles</forename><surname>Gelada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre-Antoine</forename><surname>Manzagol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th International Conference on Learning Representations (ICLR)</title>
		<meeting>the 8th International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page">29</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Matching networks for one shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th Annual Conference on Neural Information Processing Systems (NeurIPS)</title>
		<meeting>the 30th Annual Conference on Neural Information Processing Systems (NeurIPS)</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">28</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">The Caltech-UCSD Birds-200-2011 dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catherine</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<idno>CNS-TR-2011-001</idno>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Yoshua Bengio, and Hod Lipson. How transferable are features in deep neural networks?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Clune</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th Annual Conference on Neural Information Processing Systems (NeurIPS)</title>
		<meeting>the 28th Annual Conference on Neural Information Processing Systems (NeurIPS)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">29</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Fast context adaptation via meta-learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luisa</forename><forename type="middle">M</forename><surname>Zintgraf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyriacos</forename><surname>Shiarlis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vitaly</forename><surname>Kurin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katja</forename><surname>Hofmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shimon</forename><surname>Whiteson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning (ICML)</title>
		<meeting>the 36th International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">28</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
