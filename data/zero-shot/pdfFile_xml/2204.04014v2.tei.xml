<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">MULTIMODAL QUASI-AUTOREGRESSION: FORECASTING THE VISUAL POPULARITY OF NEW FASHION PRODUCTS</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanos</forename><forename type="middle">I</forename><surname>Papadopoulos</surname></persName>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christos</forename><surname>Koutlis</surname></persName>
							<email>ckoutlis@iti.gr</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Symeon</forename><surname>Papadopoulos</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Kompatsiaris</surname></persName>
						</author>
						<title level="a" type="main">MULTIMODAL QUASI-AUTOREGRESSION: FORECASTING THE VISUAL POPULARITY OF NEW FASHION PRODUCTS</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T10:57+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Popularity Forecasting</term>
					<term>Trend Detection</term>
					<term>Quasi Autoregression</term>
					<term>Multimodal learning</term>
					<term>Computer Vision</term>
					<term>Deep Learning</term>
					<term>Fashion</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Estimating the preferences of consumers is of utmost importance for the fashion industry as appropriately leveraging this information can be beneficial in terms of profit. Trend detection in fashion is a challenging task due to the fast pace of change in the fashion industry. Moreover, forecasting the visual popularity of new garment designs is even more demanding due to lack of historical data. To this end, we propose MuQAR, a Multimodal Quasi-AutoRegressive deep learning architecture that combines two modules: (1) a multi-modal multi-layer perceptron processing categorical, visual and textual features of the product and (2) a quasi-autoregressive neural network modelling the "target" time series of the product's attributes along with the "exogenous" time series of all other attributes. We utilize computer vision, image classification and image captioning, for automatically extracting visual features and textual descriptions from the images of new products. Product design in fashion is initially expressed visually and these features represent the products' unique characteristics without interfering with the creative process of its designers by requiring additional inputs (e.g manually written texts). We employ the product's target attributes time series as a proxy of temporal popularity patterns, mitigating the lack of historical data, while exogenous time series help capture trends among interrelated attributes. We perform an extensive ablation analysis on two large scale image fashion datasets, Mallzee and SHIFT15m to assess the adequacy of MuQAR and also use the Amazon Reviews: Home and Kitchen dataset to assess generalisability to other domains. A comparative study on the VISUELLE dataset, shows that MuQAR is capable of competing and surpassing the domain's current state of the art by 4.65% and 4.8% in terms of WAPE and MAE respectively.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Fashion is a dynamic domain, and fashion trends and styles are highly time-dependent. Systematic analysis of fashion trends is not only useful for consumers who want to be up-to-date with current trends, but is also vital for fashion designers, retailers and brands in order to optimize production cycles and design products that customers will find appealing when they hit the shelves. Moreover, it could potentially help mitigate the problem of unsold inventory in fashion which is caused by a mismatch between supply and demand <ref type="bibr" target="#b0">[1]</ref> and has a significant environmental impact; with million tonnes of garments ending up in landfills or being burned every year <ref type="bibr" target="#b1">[2]</ref>.</p><p>At the same time, fashion is a primarily visually-driven domain. As a result, computer vision has successfully been utilized to assist fashion recommendations and trend forecasting <ref type="bibr" target="#b2">[3]</ref>. Recent studies have utilized visual featuresextracted by computer vision models -in order to identify fashion styles <ref type="bibr" target="#b3">[4]</ref> or attributes <ref type="bibr" target="#b4">[5]</ref> and then detect and analyse trends in fashion. However, such approaches are limited to detecting coarse-level trends and can not work for specific garment designs. They can forecast whether "chunky trainers" will be trending this season but all "chunky trainers" will receive the same popularity score. Specific visual differences in individual garments are not taken into consideration. Autoregressive (AR) neural networks have been used for forecasting the popularity of specific garments based on their past popularity <ref type="bibr" target="#b5">[6]</ref>. However, new products by definition lack historical data which renders the use of conventional AR networks impracticable. Few recent research works have addressed sales forecasting of new garments, by utilizing arXiv:2204.04014v2 [cs.CV] 5 May 2022 KNN-based (nearest neighbors) <ref type="bibr" target="#b6">[7]</ref>, auto-regressive networks with auxiliary features (images, fashion attributes and events) <ref type="bibr" target="#b0">[1]</ref> or a non-AR Transformer modelling images and fashion attributes along with the "target" time series of those attributes collected from Google Trends 1 <ref type="bibr" target="#b7">[8]</ref>. However, fashion attributes are not always independent of each other. Trends in certain attributes may affect other interdependent attributes. If for example "warm minimalism" was trending in fashion, a series of light, neutral and pastel colors would show an increase in popularity while bold graphics and patterns would decrease. The objective of this study is to accurately forecast the visual popularity of new garment designs that lack historical data. To this end, we propose MuQAR, a Multi-Modal Quasi-AutoRegressive neural network architecture and in <ref type="figure" target="#fig_0">Figure 1</ref> we illustrate its high level workflow. MuQAR combines two modules: (1) a multi-modal multilayer perceptron (FusionMLP) representing the visual, textual, categorical and temporal aspects of a garment and (2) a quasi-autoregressive (QAR) neural network modelling the time series of the garment's fashion attributes (target time series) along with the time series of all other fashion attributes (exogenous time series). We expand the QAR framework by employing the logic of nonlinear autoregressive network with exogenous inputs (NARX). Our rationale is that modelling the target time series of the garment's attributes will work as an informative proxy of temporal patterns mitigating the lack of historical data while exogenous time series will help the model identify relations among fashion attributes.</p><p>The aim of this study is to provide fashion designers with real-time feedback for their new designs without interfering in their creative process. In fashion, design usually begins with sketching and visual prototyping -in 2D or 3D programs -expressing the silhouette, fitment, colors and fabrics of the new garment. Previous works have relied on computer vision to extract relevant visual features from the garment's images along with its fashion attributes in order to forecast its popularity without interference in the creative process (e.g by requiring textual descriptions of the garment). We expand upon this idea by utilizing image captioning (IC) for automatically extracting textual descriptions of the new garment which could provide richer descriptions and useful contextual information about the attributes of the garment. For example, while an attribute detection model may recognise that a "varsity college jacket" has a graphic design, it is black and green and is made out of leather and jersey, an IC model could also describe the position of the graphic design (e.g across the chest) colors and fabrics (e.g black leather sleeves, green jersey body).</p><p>The main contributions of our work are:</p><p>? Propose a novel deep learning architecture that employs the logic of NARX models in QAR for forecasting the popularity of new products that lack historical data. We compare various QAR models, including: CNN, LSTM, ConvLSTM, Feedback-LSTM, Transformers and DA-RNN.</p><p>? Integrate image captioning in the multi-modal module for capturing contextual and positional relations of the products' attributes.</p><p>? Collect a new large-scale fashion dataset that includes popularity scores in relation to demographic groups allowing specialised forecasts for different market segments.</p><p>? Perform an extensive ablation study on three datasets (two fashion and one home decoration) to assess the validity and generalisability of the proposed methodology. A comparative study on a fourth fashion-related dataset shows that our model surpasses the domain's state of the art by 4.65% and 4.8% improvements in terms of WAPE and MAE respectively. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Deep learning has been used for time series forecasting in numerous domains including climate modelling, biological sciences, medicine <ref type="bibr" target="#b8">[9]</ref>, music, meteorology, solar activity, finance <ref type="bibr" target="#b9">[10]</ref> and among other industries, in fashion <ref type="bibr" target="#b10">[11]</ref>. Additionally, researchers have been experimenting in recent years with the inclusion of visual information for detecting fashion trends and forecasting the popularity of garments or complete outfits <ref type="bibr" target="#b2">[3]</ref>.</p><p>Visual features extracted by computer vision models have been used to discover fashion styles <ref type="bibr" target="#b3">[4]</ref> or fashion attributes <ref type="bibr" target="#b4">[5]</ref> and neural networks to detect fashion trends. More recent works have examined how fashion trends from one city may influence other cities <ref type="bibr" target="#b11">[12]</ref> or how knowledge enhanced neural networks can improve fashion trend detection <ref type="bibr" target="#b12">[13]</ref>. The aforementioned studies extracted fashion styles or attributes from fashion imagery in order to forecast fashion trends. A significant limitation of these approaches however is that they can only work on a coarse level but not a finer level. Trends may show that "floral dresses will be trending next spring" but all new "floral dress" designs will receive the same score and can not produce informed and specialized predictions for individual garments or outfits. Autoregressive (AR) networks have also been used to forecast the visual popularity for specific garments or outfits <ref type="bibr" target="#b5">[6]</ref>. Nevertheless, new garment design, by definition, lack historical data and therefore conventional AR networks -that rely on past sequences to predict future outcomes -can not be utilized.  <ref type="bibr" target="#b7">[8]</ref> criticised the reliance on purely AR networks for new product sale forecasting because of the compounding effect caused by first-step errors. Instead, they propose GTM-Transformer, a multi-modal, non-AR Transformer that utilizes images, text and time series of the garment's attributes collected from Google Trends. Their work can be considered the first to utilize a version of quasi-autoregression using the "target" attributes time series and it was capable of outperforming both KNN-based approaches <ref type="bibr" target="#b6">[7]</ref> and multi-modal AR <ref type="bibr" target="#b0">[1]</ref>.</p><p>However, fashion attributes are not always independent of each other. Trends in some attributes may positively or negatively affect others, e.g. complementary colors or matching categories. To this end, we employ the logic of nonlinear autoregressive network with exogenous inputs (NARX) <ref type="bibr" target="#b15">[16]</ref> within QAR by integrating the "exogenous" fashion attributes along with the "target" attributes of a new garment in order to forecast its popularity.</p><p>Apart from the visual features of the garment, both <ref type="bibr" target="#b0">[1]</ref> and <ref type="bibr" target="#b7">[8]</ref> used fashion attributes as textual information. Fashion attributes can be extracted from images by specialised classification models without requiring manual annotation from the fashion designers and can offer valuable information to the overall neural network. The advantage of such approaches is that, in real world scenario, a forecasting model utilizing information about attributes would not interfere with the creative process of the designers. We expand upon this idea by utilizing an image captioning model (IC) for automatically extracting full textual descriptions from the images of new garment designs. Our hypothesis is that IC could create richer descriptions that capture useful contextual and positional information about the garment's attributes that will help improve the performance of multi-modal forecasting models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head><p>In this study we attempt to forecast the visual popularity of new garment designs. Conventional autoregressive (AR) forecasting models can not be utilized since new products lack historical data. On the other hand, conventional regression models are not as well equipped to detect temporal trends which play an important role in the domain of fashion. To this end, inspired by previous works, we propose MuQAR, a multimodal quasi-autoregressive neural architecture that consists of two modules: FusionMLP and QAR as well as a final prediction component that combines them. In <ref type="figure" target="#fig_1">Figure 2</ref>, the details of its architecture are illustrated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">FusionMLP</head><p>The first module consists of a multimodal multilayer perceptron that processes the visual, textual, categorical and temporal features of a product. The visual features vector F v ? R V , is extracted by the last convolutional layer of CNN-based networks. As will be discussed in Section 4.2, we use features extracted from pre-trained networks on ImageNet for SHIFT15m, Amazon Reviews and VISUELLE to ensure comparability. However, we utilize a hierarchical deep learning network, fine tuned on fashion imagery for the Mallzee image dataset <ref type="bibr" target="#b16">[17]</ref>. After the extraction, global average pooling and L2 normalisation is applied.</p><p>We utilize OFA [18] -a state-of-the-art IC model on COCO Captions 2 -for automatically extracting textual descriptions from fashion imagery. OFA does not specialise on fashion imagery but it has been trained on a large-scale e-commerce dataset that also included numerous fashion products. We manually examined hundreds of inference texts and deemed its predictions to be very accurate. We pre-process the extracted captions by lower-casing, removing punctuation and stop-words and then tokenizing them. We use a word embedding layer that produces the F w ? R W vector based on one integer index per token in relation to the whole vocabulary W. We apply IC on the Mallzee and VISUELLE datasets since they provide the garment's images but not on SHIFT15m and Amazon Reviews that only provide pre-computed visual features.</p><p>The catecorical features vector F c ? R cp?dc , is the concatenation of c p learnable embeddings of size d c each corresponding to a fashion label assigned to product p, defined by:</p><formula xml:id="formula_0">F c = [f 1 E c ; f 2 E c ; . . . ; f cp E c ]<label>(1)</label></formula><p>where [; ] denotes concatenation, E c ? R C?dc is the embedding matrix for fashion labels, C is the total number of fashion labels and f i ? R C are one-hot encoding vectors with 1 at the index of the corresponding fashion label and zero elsewhere. The temporal feature vector F t ? R 4?dt , is the concatenation of 4 learnable embeddings of size d t corresponding to the day, week, month and season of the target date, defined by:</p><formula xml:id="formula_1">F t = [dE d ; wE w ; mE m ; sE s ]<label>(2)</label></formula><p>where</p><formula xml:id="formula_2">E d ? R 366?dt , E w ? R 52?dt , E m ? R 12?dt , E s ? R 4?dt</formula><p>are embedding matrices for day, week, month and season of the year respectively and d ? R 366 (leap year provision), w ? R 52 , m ? R 12 , s ? R 4 are the corresponding one-hot encoding vectors. For the demographic group input, that is optional and considered only in one dataset here, we also consider a learnable embedding F g = gE g ? R dg , accordingly. Finally, a standard MLP network with n mlp dense layers of u mlp relu activated units processes the concatenation of all features resulting in F F ? R f :</p><formula xml:id="formula_3">F F = MLP([F v ; F w ; F c ; F t ; F g ])<label>(3)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">QAR</head><p>The second module utilizes the product's attributes' time series ("target") {A t } along with all other attributes time series ("exogenous") {X t } as input in order to predict the product's popularity time series {P t }. More precisely, we feed QAR module with two matrices A = {A 1 , . . . , A n } ? R n?cp that contains n time steps prior to the forecast date for c p target fashion labels assigned to product p and X = {X 1 , . . . , X n } ? R n?cx for all "exogenous" fashion labels c x . X includes the time series for all available fashion attributes within the time period, but we set the target attributes of c p to zero in c x to avoid information leakage.</p><p>The proposed methodology, MuQAR, is a modular architecture meaning that it can integrate any AR network to the QAR module. This allows for identifying the optimal AR network for a given task. In this study we experimented with multiple AR architectures that have been used for time series forecasting, namely: Long Short Term Memory network (LSTM) <ref type="bibr" target="#b18">[19]</ref>, (2) Feedback LSTM (F-LSTM) <ref type="bibr" target="#b19">[20]</ref>, (3) Convolutional Neural Network (CNN) <ref type="bibr" target="#b20">[21]</ref>, (4) Convolutional LSTM (ConvLSTM) <ref type="bibr" target="#b21">[22]</ref>, (5) Transformer <ref type="bibr" target="#b22">[23]</ref> for experiments that only utilize {A t } and <ref type="formula">(6)</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Prediction component</head><p>The 4 Experimental Setup</p><formula xml:id="formula_4">concatenated vector F = [F F ; F Q ] ? R f</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Evaluation Protocol</head><p>In order to correctly assess the task of forecasting the popularity of new products, we propose an evaluation protocol where the models are trained on established products and evaluated on new products. We consider as established those products that have multiple records in the dataset in different times while new are considered products that make only a single appearance in the dataset and the model has not previously encountered. We therefore split the data into established products which are used as the training set and new products which are split in half for the validation and testing sets. We apply this protocol on Mallzee, SHIFT15m and Amazon Reviews datasets but for VISUELLE we follow the experimental protocol described in <ref type="bibr" target="#b7">[8]</ref>.</p><p>For the evaluation we used multiple evaluation metrics. For regression tasks we used the: Mean Absolute Error (MAE), Pearson Correlation Coefficient (PCC) and Binary Accuracy (BA) while for classification tasks we used: Accuracy and the Area under the ROC Curve (AUC). We selected the best performances of each model with the use of TOPSIS, a multi-criteria decision analysis method <ref type="bibr" target="#b25">[26]</ref>.</p><p>We perform an extensive grid-search for tuning the hyper parameters of FusionMLP and the QAR networks. We integrate the best performing QAR network with FusionMLP's best performing hyper-parameter combinations on each dataset separately to create MuQAR. QAR models are trained with weekly aggregated time series using 12 weeks as input and 1 week as output.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Datasets</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">VISUELLE</head><p>VISUELLE 3 is a public fashion image dataset that contains 12 week long sales sequences for 5577 garments spanning from October 2016 to December 2019 <ref type="bibr" target="#b7">[8]</ref>. For each garment, it provides an image, textual information (categorical labels related to fashion categories, fabrics, colors) and time series related to the categorical labels collected from Google Trends. The dataset is sorted by date and split into 5080 garments for training and 497 for evaluation. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">SHIFT15m</head><p>Due to the fact that VISUELLE is a relatively small scale dataset we also experiment with larger-scale image fashion datasets. SHIFT15m 4 is a public, large scale and multi-objective fashion dataset that consists of 15,218,721 outfits posted by users in a social network platform among 193,574 users and 2,335,598 garments between 2010 and 2020 <ref type="bibr" target="#b26">[27]</ref>. The dataset provides the user ID, the number of likes that the outfit has received, the date it was published and the items that constitute the outfit, including the item IDs and two types of fashion categories (comprising 7 and 43 unique categories respectively). The outfits' images are not available but SHIFT15m provides the visual features extracted by a VGG network pre-trained on the ImageNet dataset.</p><p>We re-purpose SHIFT15m for new garment popularity forecasting. We remap the initial outfit-level onto the garmentlevel by splitting each outfit into the individual garments that make it up and by defining the number of likes as the target variable. We assume that each garment has an equal contribution to the overall popularity of the outfit. The number of likes follows a skewed distribution. We therefore normalise it with the logarithmic transform, namely log(likes + 1) and scale it within the range of (0, 1) with the use of min-max scaling. To create the time series used in QAR, we simply compute the weekly mean for each fashion category. The time series exhibit 10.72% sparsity, so we apply linear interpolation to fill the missing values. The dataset is split into 14,342,771 samples for established products (training set) and 875,950 samples for new products (validation and testing sets).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">Mallzee</head><p>One limitation of SHIFT15m is that it provides no user-side information. Forecasting networks trained on SHIFT15m would learn to forecast the general popularity of a new garment design but the predictions would be identical regardless of age group or gender. For example, a newly designed "floral dress" would have the same popularity score for "women, age: 18-25" and "men, age: 40-50". In order to alleviate this issue and perform more targeted and useful forecasts for different segments of the market we collect the Mallzee dataset.</p><p>Project partner Mallzee collected 5,412,193 records from their databases between 14 demographic groups and 571,333 unique products. The selected demographics consist of two gender groups (men, women) and 7 age-groups including 0-18, 18-25, 25-30, 30-35, 35-45, 45-55, &gt;55. The data span 1081 days between 2017-10-16 and 2020-09-30. We extract the visual features from a three-stage convolutional neural network consisting of an object detector, a garment category classifier and a fine-grained attribute detector, proposed in <ref type="bibr" target="#b16">[17]</ref>. We use the object-detector to identify individual garments in an image and the other two modules to extract the predicted fashion labels and the visual features from their last convolutional layers. The Mallzee dataset is classified into 22 garment categories such as blouses, dresses, jackets etc. and 109 fine-grained attributes including patterns (e.g. checked, quilted), prints (e.g. floral, graphic) and styles (e.g. bomber jacket, puffer jacket). The dataset is split into 5,320,076 samples for established products (training set) and 92,117 samples for new products (validation and testing sets).</p><p>As the target variable, for each product p we consider the popularity metric P (Eq.4a) that expresses both likability L (Eq.4b) and reachability R (Eq.4c), given the set A of the product's attributes, the target demographic group G and the target time t:</p><formula xml:id="formula_5">P (p | A, G, t) = L(p | G, t) ? R(A, G, s) (4a) L(p | G, t) = I(?, G, p, t) I(?, G, p, t) + I( , G, p, t) (4b) R(A, G, s) = a?A | {u ? G | (u ? a) ? s} | | {u ? G | s} | (4c)</formula><p>where s is the year's season that day t belongs to, I(?, G, p, t) and I( , G, p, t) denote the number of positive and negative interactions between G and p at day t respectively, u denotes user, | ? | denotes set cardinality and u ? a denotes positive interaction of user u with a product having attribute a. Hence, likability is the probability that demographic group G likes product p at time t, while reachability is the probability that demographic group G interacts with the set of attributes A at season s. The reason for incorporating reachability to calculate popularity is that positive interaction of small fractions of demographic groups with unusual attributes results in unexpected high likability. For instance, we found dresses, jumpsuits and heels in the top categories for men 18-25 based only on likability. The incorporation of reachability not only mitigated this issue but gave reasonable seasonal patterns to all garment categories as well. To create the time series used in QAR, we compute the weekly mean popularity for each of the 22 categories and 109 attributes which exhibit 1.13% and 8.49% sparsity respectively. We apply linear interpolation to fill the missing values. Time series of certain fashion classes can be seen in <ref type="figure" target="#fig_3">Figure 3</ref> where we can observe clear seasonal patterns.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.4">Amazon Reviews: Home and Kitchen</head><p>Fashion is the central focus of our work but we also deem important to examine the generalisability of MuQAR to other domains. To this end, we utilize the Amazon Reviews dataset 5 and specifically the "Home and Kitchen" subset <ref type="bibr" target="#b27">[28]</ref>. Our selection criteria (cr.) for the dataset are: (1) relate to a primarily visually-driven domain, (2) provide a popularity metric, (3) provide images or extracted visual features, (4) provide categories that produce dense time series.</p><p>We select the "Home and Kitchen" subset since it mostly contains products relating to furniture, decorative items, artwork posters and kitchen appliances. We consider that visual appearances play a very important role in driving customer choices in this domain similarly to fashion (cr. 1). We define the 'star ratings' as the target variable (cr.</p><p>2). The dataset provides the visual features extracted from the products' images by a convolutional neural network pre-trained on ImageNet (cr. 3).</p><p>The dataset spans from 1999 to 2014 and comes with 965 unique categories. However categories contain duplicates (e.g "sheet" and "sheets") and rare items (e.g "charcoal drawings" and "crayon drawings" only make a single appearance in the dataset). Aggregating the weekly time series for the 965 categories results in 64.34% sparsity. Even after filtering out the products before 2009 (a period with higher sparsity rates), results in 33.34% sparsity. In order to mitigate this issue, we use the K-Means algorithm to cluster categories based on their TF-IDF textual representation into K=300 clusters. The weekly time series sparsity is reduced to 36.91% for the whole dataset and to only 2.13% for the filtered subset. (cr. 4). Filtering out the products before 2009 reduced the total size of the dataset from 3,261,846 to 3,002,786 which we consider a sensible trade-off between data loss and reduced time series sparsity. The dataset is split into </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Implementation Details</head><p>We perform a grid search for hyper-parameter optimization for FusionMLP and QAR modules. For FusionMLP we define n mlp= 3,4,5 or 6 fully connected layers with progressively narrower units u mlp = (2048, 1024, 512, 256, 128) (2048 units layer used twice in the case of 6 layers only) and an embedding space of d c = d t = d g =8, 16 or 32 dimensions. For the QAR networks we define a hyper-parameter grid search as shown in <ref type="table" target="#tab_2">Table 1</ref>. For each QAR network we define the number of its component layers and their units. Values in parenthesis indicate experiments with progressively narrower units. After each LSTM, CNN or MLP layer we add a dropout layer of 10% probability in order to reduce overfitting with the exception of Transformers which we define at 20% probability.</p><p>For training we consider the Adam optimiser, cyclical learning rate <ref type="bibr" target="#b28">[29]</ref> (initial learning rate: 1e-4, max learning rate: 1e-2, step size: 2, gamma: 0.1), the mean squared error (MSE) loss function for regression tasks (VISUELLE, SHIFT15m, Mallzee) and the categorical cross entropy for classification tasks (Amazon Reviews: Home and Kitchen). We use a batch size of 1024 for Mallzee and Amazon datasets, 8192 for SHIFT15m since it is a larger dataset and we wanted to exploit parallelization and 16 for VISUELLE since it is a very small dataset. <ref type="table" target="#tab_3">Table 2</ref> presents the ablation analysis of MuQAR on Mallzee, SHIFT15m and Amazon Reviews datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Ablation Analysis</head><p>QAR: By performing the hyper parameter optimisation grid search on the Mallzee dataset we identify the following best performing combinations, LSTM: u lstm = (512) units, CNN: u cnn = (512, 256, 128), DA-RNN: u lstm = (128), ConvLSTM: u cnn = (512, 256, 128), u lstm = (512, 256) and u mlp = (128), F-LSTM: u lstm = (256) and u mlp = (512), Transformer: n block = 1, n head = 2 with 256 units, ConvLSTM+X: we use the same hyper parameters as ConvLSTM. We apply the same hyper-parameters on the rest of the datasets. The CNN, ConvLSTM and Transformer QAR networks yielded the best performance for Mallzee, SHIFT15m and Amazon Reviews datasets respectively when only using [A], the target attribute time series with 12 previous weeks as input and 1 as output. We integrate these specific QAR models for each of the three datasets in the MuQAR experiments. We can observe that there is not a single QAR network that consistently performs better. Experimentation is required in order to identify the most appropriate QAR network for specific tasks and datasets.</p><p>When QAR utilizes both [A + X] its performance decreases on Mallzee and SHIFT15m; compared to the best QAR with [A]. However, we observe an impressive improvement on the Amazon dataset with +18% and +10% increases in terms of Accuracy and AUC respectively. This may be attributed to the fact that Amazon initially had 965 categories which we clustered into 300 with the use of K-Means -a lot more than in Mallzee and SHIFT15m -some of which may be quite noisy. Thus, feeding the exogenous time series [X] may help QAR discern the informative from the noisy In <ref type="figure" target="#fig_4">Figure 4</ref> we present an inference sample predicted by MuQAR trained on the Mallzee dataset. We use the same image, depicting a jumpsuit and a pair of chelsea boots and MuQAR performs predictions for different demographic groups. We can observe that the popularity of all garments increases in June for women over 55 compared with January ( <ref type="figure" target="#fig_4">Fig. 4a and 4b)</ref>. Moreover, the outfit is less popular with younger women <ref type="figure" target="#fig_4">(Fig. 4c</ref>) and very unpopular with men ( <ref type="figure" target="#fig_4">Fig. 4d</ref>). In <ref type="figure" target="#fig_4">Figures 4e and 4f</ref> we observe that a sweater with a "playful" graphic design is more popular with younger (mostly teenagers) than older women. Finally, a minimal white "boxy t-shirt" shows an increase in popularity from January <ref type="figure" target="#fig_4">(Fig. 4g)</ref> to June <ref type="figure" target="#fig_4">(Fig. 4g)</ref> presumably due to the seasonal change and by extension the warmer weather -while still remaining relatively unpopular for young teenage boys. MuQAR seems to have learned both seasonal trends and the average preferences of different demographic groups.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>In this study we propose MuQAR, a Multimodal Quasi Auto-Regressive deep learning architecture, for forecasting the popularity of new products that lack historical data. The proposed architecture consists of two modules: (1) a multi-modal multilayer perceptron (FusionMLP) representing visual, textual, categorical and temporal aspects of a product and (2) a quasi-autoregressive (QAR) neural network modelling the time series of the product's attributes along with all other attributes time series. For FusionMLP, we extract the visual and categorical features (fashion attributes) from a computer vision model representing the unique characteristics of new products along with rich textual descriptions extracted from an image captioning model. In QAR, the time series of its attributes provide a proxy of temporal patterns for the lack of historical data while the exogenous time series are used to identify relations among the target and all other attributes.</p><p>Our focus is centered around the fashion industry and new garment designs. We experiment with three fashion image datasets: Mallzee, SHIFT15m and VISUELLE. We also experiment with the Amazon Reviews: Home and Kitchen dataset to examine the generalisability of the proposed architecture. Performing an extensive internal ablation analysis as well as a comparative analysis, we show that MuQAR is capable of competing and surpassing the domain's current state of the art by 4.65% in terms of WAPE and 4.8% in terms of MAE on the VISUELLE dataset.</p><p>In this study, we experiment with two QAR models that utilize the exogenous time series, namely DA-RNN and ConvLSTM+X. It would be interesting for future research to examine how other models perform within the QAR module such as Lavarnet <ref type="bibr" target="#b9">[10]</ref>, MTNet <ref type="bibr" target="#b24">[25]</ref>, LSTNet <ref type="bibr" target="#b29">[30]</ref> or the Informer <ref type="bibr" target="#b30">[31]</ref>. Moreover, we have only experimented with image datasets but the proposed architecture MuQAR could be adapted and applied to other domains with different types of data. For example audio feature combined with time series of musical genres could be used to forecast the popularity of new tracks or albums. In future work we plan on examining how visual-temporal features extracted from MuQAR can facilitate recommendation systems and especially in mitigating the cold start problem of new items.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>High level workflow of MuQAR. The new garment's image and the trends of fashion attributes are analysed by the modules of MuQAR which predicts the garment's popularity for a given demographic segment and target date.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Architecture of MuQAR. Intermittent arrows are optional and applicable only to the Mallzee dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>+q is further processed by another dense layer on top of which a linear layer forecasts the product's next k popularity time steps {P n+1 , . . . , P n+k }, as can be seen inFigure 2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Monthly aggregated time series for fashion categories from the Mallzee dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Inference samples by the MuQAR on Mallzee data for different demographic groups and dates.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>While no works have directly addressed popularity forecasting of new garments few recent research works have addressed sales forecasting of new garments, which are closely related. Loureiro et. al.<ref type="bibr" target="#b13">[14]</ref> and Singh et. al.<ref type="bibr" target="#b14">[15]</ref> were two of the first works to utilize regressive deep learning for sales forecasting of new garments but did not utilize visual features. Craparotta et. al.<ref type="bibr" target="#b6">[7]</ref> proposed a KNN-based approach relying on an image similarity network connected to a siamese network for forecasting sales of new garments. Ekambaram et. al.<ref type="bibr" target="#b0">[1]</ref> utilized an AR multi-modal RNN that combines visual and textual features with exogenous factors (holidays, discount season, events etc). Since new products do not have historical data, the authors prepend two starting delimiters and utilize teacher enforcing for the proceeding steps during the training process. Skenderi et. al.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>ConvLSTM + X) inspired by the encoder proposed in<ref type="bibr" target="#b24">[25]</ref> for experiments that utilize both {A t } and {X t }. DA-RNN is a dual-stage architecture that first processes {X t } and then {A t } while ConvLSTM+X processes {A t } and {X t } in parallel -with two separate ConvLSTM neural networks -and then concatenates the resulting representation vectors. After processing the input time series, QAR produces a vector representation F Q ? R</figDesc><table><row><cell>Dual-Stage Attention-</cell></row><row><cell>Based Recurrent Neural Network (DA -RNN) [24] adapted for multivariate time series, (7) Convolutional LSTM with</cell></row><row><cell>exogenous time series (</cell></row></table><note>q pertinent to the forecast.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Grid search for hyper-parameter optimization of the QAR networks in terms of the number of layers n {Layer} and their units u {Layer} .</figDesc><table><row><cell>Model</cell><cell cols="2">Layer n {Layer}</cell><cell>u {Layer}</cell></row><row><cell>LSTM</cell><cell>lstm</cell><cell>1, 2, 3</cell><cell>(512, 256, 128)</cell></row><row><cell>CNN</cell><cell>cnn</cell><cell>1, 2, 3</cell><cell>(512, 256, 128)</cell></row><row><cell>DA-RNN</cell><cell>lstm</cell><cell>2</cell><cell>64 or 128</cell></row><row><cell>ConvLSTM</cell><cell>cnn lstm</cell><cell>1, 2, 3 1, 2, 3</cell><cell>(512, 256, 128) (512, 256, 128)</cell></row><row><cell>F-LSTM</cell><cell>lstm mlp</cell><cell>1, 2, 3 0,1</cell><cell>(512, 256, 128) 256 or 512</cell></row><row><cell>Transformer</cell><cell>block head</cell><cell>1, 2, 3 2 or 4</cell><cell>128, 256 or 512</cell></row><row><cell cols="4">2,840,178 samples for established products (training set) and 149,414 samples for new products (validation and testing</cell></row><row><cell>sets).</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Popularity forecasting models trained on 'established' and evaluated on 'new' garments for three datasets: Mallzee (MLZ), SHIFT15m and Amazon Reviews: Home and Kitchen. Features used: Images [I], target attributes time series [A], exogenous attributes time series [X] and image captions [C]. [A] and [X] have 12 weeks-long time series as input. The models forecast the next week. Bold denotes the best overall performance per metric and dataset. Underline denotes the best performing QAR network per dataset; which are used in the final MuQAR models. *[C] are only available on MLZ; they are ignored on Amazon and SHIFT15m, [I+A+X] are used instead.FusionMLP: an embedding space of d c = d t = d g = 32 dimensions, n mlp = 4 with u mlp = (2048, 1024, 512, 256) and a dropout rate of 10% performed best on the three datasets. FusionMLP performed significantly better than the QAR networks on the Mallzee dataset but not as good on SHIFT15m and Amazon Reviews. The visual features[I]   in the Mallzee data are extracted by a fined-tuned network on fashion imagery, while the other two datasets utilized networks pre-trained on ImageNet and are therefore less specialised. This fact may have affected the results since, presumably, the quality and specialisation of the extracted visual features plays a crucial role in the task. When image captions [C] are added in FusionMLP there is only a negligible improvement in terms of MAE but a small decrease in terms of PCC and Accuracy. This issue may be due to the hyper-parameter tuning being done on the [I] features and not fine-tuned for [I+C].MuQAR: we can observe that MuQAR using [I+A] is capable of consistently surpassing all QAR networks and FusionMLP on the three datasets. By combining the visual features [I] with the target time series [A], MuQAR is able to improve upon the task of forecasting the popularity of new products while being robust to less specialised visual features. Finally, utilizing exogenous time series [X] and captions [C] further improves the MuQAR architecture for Mallzee and Amazon datasets but not on SHIFT15m. While [A+X] did not consistently improve the performance of QAR, neither did [C] improve FusionMLP, when integrated within the MuQAR architecture they can show significant improvements.</figDesc><table><row><cell>Input</cell><cell>Model</cell><cell cols="2">MAE(?)</cell><cell cols="2">PCC(?)</cell><cell></cell><cell>Accuracy(?)</cell><cell></cell><cell>AUC(?)</cell></row><row><cell></cell><cell></cell><cell>MLZ</cell><cell>SHIFT15m</cell><cell>MLZ</cell><cell cols="5">SHIFT15m MLZ SHIFT15m Amazon Amazon</cell></row><row><cell></cell><cell>LR</cell><cell>0.1878</cell><cell>0.1162</cell><cell>0.2439</cell><cell>0.3177</cell><cell>63.10</cell><cell>59.58</cell><cell>48.52</cell><cell>65.41</cell></row><row><cell></cell><cell>CNN</cell><cell>0.1611</cell><cell>0.1148</cell><cell>0.5379</cell><cell>0.3406</cell><cell>70.99</cell><cell>61.51</cell><cell>47.18</cell><cell>69.34</cell></row><row><cell>[A]</cell><cell>LSTM F-LSTM</cell><cell>0.1656 0.1809</cell><cell>0.1150 0.1149</cell><cell>0.5109 0.3395</cell><cell>0.3371 0.3376</cell><cell>69.67 64.62</cell><cell>61.42 61.43</cell><cell>45.58 44.95</cell><cell>67.54 67.89</cell></row><row><cell></cell><cell>Transformer</cell><cell>0.1842</cell><cell>0.1149</cell><cell>0.3071</cell><cell>0.3398</cell><cell>63.67</cell><cell>61.28</cell><cell>51.10</cell><cell>71.29</cell></row><row><cell></cell><cell>ConvLSTM</cell><cell>0.1641</cell><cell>0.1147</cell><cell>0.5225</cell><cell>0.3411</cell><cell>69.98</cell><cell>61.58</cell><cell>46.58</cell><cell>68.59</cell></row><row><cell>[A+X]</cell><cell cols="2">ConvLSTM+X 0.1686 DA-RNN 0.1863</cell><cell>0.1185 0.1187</cell><cell>0.4913 0.2652</cell><cell>0.2191 0.2050</cell><cell>68.86 64.39</cell><cell>59.50 59.55</cell><cell>60.61 60.61</cell><cell>78.95 78.90</cell></row><row><cell>[I]</cell><cell>LR FusionMLP</cell><cell>0.1599 0.1074</cell><cell>0.1186 0.1148</cell><cell>0.5314 0.7893</cell><cell>0.1940 0.2811</cell><cell>71.86 81.52</cell><cell>57.93 60.89</cell><cell>41.46 46.96</cell><cell>68.18 71.69</cell></row><row><cell>[I+C]</cell><cell>FusionMLP</cell><cell>0.1073</cell><cell>-</cell><cell>0.7879</cell><cell>-</cell><cell>81.30</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>[I+A]</cell><cell>MuQAR</cell><cell>0.0949</cell><cell>0.1100</cell><cell>0.8362</cell><cell>0.3934</cell><cell>83.41</cell><cell>63.57</cell><cell>51.51</cell><cell>74.24</cell></row><row><cell>[I+A+X+C]</cell><cell>MuQAR</cell><cell>0.0911</cell><cell>0.1118*</cell><cell>0.8484</cell><cell>0.3448*</cell><cell>84.26</cell><cell>62.30*</cell><cell>60.63*</cell><cell>80.40*</cell></row><row><cell cols="10">time series. Overall, we observe that ConvLSTM+X tends to outperform DA-RNN in 6 out of 8 cases. We therefore</cell></row><row><cell cols="5">integrate ConvLSTM+X in the final MuQAR experiments.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Comparative analysis between MuQAR and its modules against state of the art networks on the VISUELLE dataset using 52 week-long time series as input from Google Trends and forecasting the next 6. Features used: [T]ext, [I]mage, target [A]ttribute time series from Google trends, e[X]ogenous time series from google trends and image [C]aptions. Underline denotes the best performing network per input type. Bold denotes the best overall performance.</figDesc><table><row><cell>Method</cell><cell>Input</cell><cell cols="2">IN:52, OUT:6</cell></row><row><cell></cell><cell></cell><cell cols="2">WAPE(?) MAE(?)</cell></row><row><cell>GTM-Transformer [8]</cell><cell></cell><cell>62.6</cell><cell>34.2</cell></row><row><cell>Attribute KNN [1]</cell><cell>[T]</cell><cell>59.8</cell><cell>32.7</cell></row><row><cell>FusionMLP</cell><cell></cell><cell>55.15</cell><cell>30.12</cell></row><row><cell>Image KNN [1]</cell><cell></cell><cell>62.2</cell><cell>34</cell></row><row><cell>GTM-Transformer [8]</cell><cell>[I]</cell><cell>56.4</cell><cell>30.8</cell></row><row><cell>FusionMLP</cell><cell></cell><cell>54.59</cell><cell>29.82</cell></row><row><cell>Transformer</cell><cell></cell><cell>62.5</cell><cell>34.1</cell></row><row><cell>LSTM</cell><cell></cell><cell>58.7</cell><cell>32.0</cell></row><row><cell>ConvLSTM GTM-Transformer [8]</cell><cell>[A]</cell><cell>58.6 58.2</cell><cell>32.0 31.8</cell></row><row><cell>F-LSTM</cell><cell></cell><cell>58.0</cell><cell>31.7</cell></row><row><cell>CNN</cell><cell></cell><cell>57.4</cell><cell>31.4</cell></row><row><cell>ConvLSTM+X DA-RNN</cell><cell>[A + X]</cell><cell>55.73 58.05</cell><cell>30.44 31.71</cell></row><row><cell>Attribute + Image KNN [1]</cell><cell></cell><cell>61.3</cell><cell>33.5</cell></row><row><cell>Cross-Attention RNN [1] GTM-Transformer [8]</cell><cell>[T + I]</cell><cell>59.5 56.7</cell><cell>32.3 30.9</cell></row><row><cell>FusionMLP</cell><cell></cell><cell>54.11</cell><cell>29.56</cell></row><row><cell>FusionMLP</cell><cell>[T+I+C]</cell><cell>53.50</cell><cell>29.22</cell></row><row><cell>GTM-Transformer AR [8]</cell><cell></cell><cell>59.6</cell><cell>32.5</cell></row><row><cell>Cross-Attention RNN+A [1]</cell><cell></cell><cell>59.0</cell><cell>32.1</cell></row><row><cell>GTM-Transformer [8]</cell><cell></cell><cell>55.2</cell><cell>30.2</cell></row><row><cell>MuQAR w/ Transformer MuQAR w/ F-LSTM</cell><cell>[T + I + A]</cell><cell>54.87 54.37</cell><cell>29.97 29.7</cell></row><row><cell>MuQAR w/LSTM</cell><cell></cell><cell>54.3</cell><cell>29.66</cell></row><row><cell>MuQAR w/CNN</cell><cell></cell><cell>53.9</cell><cell>29.44</cell></row><row><cell>MuQAR w/ ConvLSTM</cell><cell></cell><cell>53.61</cell><cell>29.28</cell></row><row><cell>MuQAR w/ DA-RNN MuQAR w/ ConvLSTM+X</cell><cell>[T+I+A+X+C]</cell><cell>54.43 52.63</cell><cell>29.73 28.75</cell></row><row><cell>5.2 Comparative Analysis</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3</head><label>3</label><figDesc>presents the comparison between MuQAR and its modules against state of the art networks proposed by<ref type="bibr" target="#b0">[1]</ref> and<ref type="bibr" target="#b7">[8]</ref> on the VISUELLE dataset. We do not fine tune the hyperparameters of MuQAR and its modules on the VISUELLE dataset. Rather, we use the same values as described in 5.1 with the exception of increasing the dropout rate to 0.3 to avoid overfitting since VISUELLE is a significantly smaller dataset.Regarding QAR using [A], CNN and F-LSTM were able to surpass both GTM-Transformer and our Transformer. This indicates that it is advisable to also experiment with "simpler" neural network architectures and not immediately leaping to more complicated networks. Moreover, ConvLSTM+X utilizing both target and exogenous time series[A+X] surpasses all QAR models using [A]. FusionMLP utilizing [T+I], or solely [T] or [I] can not only outperform its similar-input competitors but also the GTM-Transformer using [T+I+A]. On top of that, adding image captions [C] improves the performance of FusionMLP. We also observe that MuQAR with any of the QAR networks is capable of surpassing GTM-Transformer (non-AR), Cross Attention RNN+A and GTM-Transformer AR (an autoregressive variant) when using [T+I+A]. Finally, the proposed MuQAR w/ ConvLSTM+X employing all features [T+I+A+X+C] is capable of surpassing all other models, setting a new state-of-the-art on VISUELLE with 4.65% and 4.8% improvements in terms of WAPE and MAE respectively. These results further prove the validity of our two main proposals, the use of image captioning and the inclusion of exogenous attributes time series within the MuQAR architecture when forecasting the visual popularity of new garment designs.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://trends.google.com</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://paperswithcode.com/sota/image-captioning-on-coco-captions</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://github.com/HumaticsLAB/GTM-Transformer</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">https://github.com/st-tech/zozo-shift15m/tree/main/benchmarks</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">https://jmcauley.ucsd.edu/data/amazon/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Acknowledgments</head><p>This work is partially funded by the project "eTryOn -virtual try-ons of garments enabling novel human fashion interactions" under grant agreement no. 951908. The authors would also like to thank Jamie Sutherland, Manjunath Sudheer and the company Mallzee/This Is Unfolded for the data acquisition as well as all the useful insights and feedback.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Attention based multi-modal new product sales time-series forecasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Ekambaram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kushagra</forename><surname>Manglik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumanta</forename><surname>Mukherjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Surya Shravan Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satyam</forename><surname>Sajja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vikas</forename><surname>Dwivedi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Raykar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3110" to="3118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The environmental price of fast fashion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kirsi</forename><surname>Niinim?ki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helena</forename><surname>Dahlbo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patsy</forename><surname>Perry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Rissanen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alison</forename><surname>Gwilt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Reviews Earth &amp; Environment</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="189" to="200" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Fashion meets computer vision: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Huang</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sijie</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chieh-Yun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shintami</forename><surname>Chusnul Hidayati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaying</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys (CSUR)</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="41" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Fashion forward: Forecasting visual style in fashion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziad</forename><surname>Al-Halah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rainer</forename><surname>Stiefelhagen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristen</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="388" to="397" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Geostyle: Discovering fashion trends and events</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Utkarsh</forename><surname>Mall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Matzen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Snavely</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kavita</forename><surname>Bala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="411" to="420" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Dressing for attention: Outfit based fashion popularity prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chia-Lin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rong-An</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong-Han</forename><surname>Shuai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Huang</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE International Conference on Image Processing (ICIP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3222" to="3226" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A siamese neural network application for sales forecasting of new fashion products using heterogeneous data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giuseppe</forename><surname>Craparotta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S?bastien</forename><surname>Thomassey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amedeo</forename><surname>Biolatti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computational Intelligence Systems</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1537" to="1546" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Well googled is half done: Multimodal forecasting of new fashion product sales with image-based google trends</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geri</forename><surname>Skenderi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Joppi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Denitto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Cristani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.09824</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Time-series forecasting with deep learning: a survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Zohren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Philosophical Transactions of the Royal Society A</title>
		<imprint>
			<biblScope unit="volume">379</biblScope>
			<biblScope unit="page">20200209</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Lavarnet: Neural network modeling of causal variable relationships for multivariate time series forecasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christos</forename><surname>Koutlis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Symeon Papadopoulos, Manos Schinas, and Ioannis Kompatsiaris</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">96</biblScope>
			<biblScope unit="page">106685</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Fashion trend forecasting using machine learning techniques: A review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Audrey</forename><forename type="middle">Aurelia</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jihad</forename><surname>Fahri Ramadhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zainina</forename><surname>Kyla Shafira Adnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bayu</forename><surname>Kanigoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edy</forename><surname>Irwansyah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Computational Methods in Systems and Software</title>
		<meeting>the Computational Methods in Systems and Software</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="34" to="44" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">From paris to berlin: Discovering fashion style influences around the world</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziad</forename><surname>Al-Halah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristen</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10136" to="10145" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Knowledge enhanced neural fashion trend forecasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunshan</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujuan</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xun</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lizi</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wai</forename><forename type="middle">Keung</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 International Conference on Multimedia Retrieval</title>
		<meeting>the 2020 International Conference on Multimedia Retrieval</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="82" to="90" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Exploring the use of deep neural networks for sales forecasting in fashion retail</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">D</forename><surname>Ana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Loureiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Vera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas Fm Da</forename><surname>Migu?is</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Silva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Decision Support Systems</title>
		<imprint>
			<biblScope unit="volume">114</biblScope>
			<biblScope unit="page" from="81" to="93" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Fashion retail: Forecasting demand for new items</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yadunath</forename><surname>Pawan Kumar Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nilpa</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aruna</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rajan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.01960</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Nonlinear system identification: NARMAX methods in the time, frequency, and spatio-temporal domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Stephen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Billings</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>John Wiley &amp; Sons</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Attentive hierarchical label sharing for enhanced garment and attribute classification of fashion imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christos</forename><surname>Stefanos-Iordanis Papadopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manjunath</forename><surname>Koutlis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martina</forename><surname>Sudheer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Delphine</forename><surname>Pugliese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rabiller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Recommender Systems in Fashion and Retail</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022" />
			<biblScope unit="page" from="95" to="115" />
		</imprint>
	</monogr>
	<note>Symeon Papadopoulos, and Ioannis Kompatsiaris</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Unifying architectures, tasks, and modalities through a simple sequence-to-sequence learning framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">An</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Men</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhikang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingren</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongxia</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.03052</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Generating sequences with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1308.0850</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for time series classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bendong</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huanzhang</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shangfeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junliang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongya</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Systems Engineering and Electronics</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="162" to="169" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Convolutional lstm network: A machine learning approach for precipitation nowcasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhourong</forename><surname>Shi Xingjian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dit-Yan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wai-Kin</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang-Chun</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Woo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="802" to="810" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">A dual-stage attentionbased recurrent neural network for time series prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongjin</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guofei</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Garrison</forename><surname>Cottrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.02971</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">A memory-network based solution for multivariate time-series forecasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Yu</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan-Yun</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yueh-Hua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shou-De</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.02105</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Methods for multiple attribute decision making</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ching-Lai</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kwangsun</forename><surname>Yoon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Multiple attribute decision making</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1981" />
			<biblScope unit="page" from="58" to="191" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masanari</forename><surname>Kimura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takuma</forename><surname>Nakamura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuki</forename><surname>Saito</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.12992</idno>
		<title level="m">Shift15m: Multiobjective large-scale fashion dataset with distributional shifts</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Image-based recommendations on styles and substitutes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Mcauley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Targett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qinfeng</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th international ACM SIGIR conference on research and development in information retrieval</title>
		<meeting>the 38th international ACM SIGIR conference on research and development in information retrieval</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="43" to="52" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Cyclical learning rates for training neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Leslie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE winter conference on applications of computer vision (WACV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="464" to="472" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Modeling long-and short-term temporal patterns with deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guokun</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Cheng</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 41st International ACM SIGIR Conference on Research &amp; Development in Information Retrieval</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="95" to="104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Informer: Beyond efficient transformer for long sequence time-series forecasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanghang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jieqi</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wancai</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
