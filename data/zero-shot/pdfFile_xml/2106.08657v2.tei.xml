<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">EIDER: Empowering Document-level Relation Extraction with Efficient Evidence Extraction and Inference-stage Fusion</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiqing</forename><surname>Xie</surname></persName>
							<email>xyiqing2@illinois.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
								<address>
									<region>IL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaming</forename><surname>Shen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
								<address>
									<region>IL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sha</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
								<address>
									<region>IL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuning</forename><surname>Mao</surname></persName>
							<email>yuningm2@illinois.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
								<address>
									<region>IL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Han</surname></persName>
							<email>hanj@illinois.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
								<address>
									<region>IL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">EIDER: Empowering Document-level Relation Extraction with Efficient Evidence Extraction and Inference-stage Fusion</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T10:17+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Document-level relation extraction (DocRE)</head><p>aims to extract semantic relations among entity pairs in a document. Typical DocRE methods blindly take the full document as input, while a subset of the sentences in the document, noted as the evidence, are often sufficient for humans to predict the relation of an entity pair. In this paper, we propose an evidenceenhanced framework, EIDER, that empowers DocRE by efficiently extracting evidence and effectively fusing the extracted evidence in inference. <ref type="bibr" target="#b39">1</ref> We first jointly train an RE model with a lightweight evidence extraction model, which is efficient in both memory and runtime. Empirically, even training the evidence model on silver labels constructed by our heuristic rules can lead to better RE performance. We further design a simple yet effective inference process that makes RE predictions on both extracted evidence and the full document, then fuses the predictions through a blending layer. This allows EIDER to focus on important sentences while still having access to the complete information in the document. Extensive experiments show that EIDER outperforms state-ofthe-art methods on three benchmark datasets (e.g., by 1.37/1.26 Ign F1/F1 on DocRED). Head:Hero of the Day Tail:the United States Rel:[country of origin]</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Relation extraction (RE) is the task of extracting semantic relations among entities within a given text, which has abundant applications such as knowledge graph construction, question answering, and biomedical text analysis <ref type="bibr" target="#b23">(Yu et al., 2017;</ref><ref type="bibr" target="#b10">Shi et al., 2019;</ref><ref type="bibr" target="#b12">Trisedya et al., 2019)</ref>. Prior studies mostly focus on predicting the relation between two entity mentions in a single sentence. However, in reality, an entity may have multiple mentions throughout a document. It is also common that a relation can only be inferred given multiple sentences as the <ref type="figure">Figure 1</ref>: A test sample in the DocRED dataset <ref type="bibr" target="#b21">(Yao et al., 2019)</ref>, where the i th sentence in the document is marked with [i] at the start. Our model correctly predicts <ref type="bibr" target="#b39">[1,</ref><ref type="bibr">10]</ref> as evidence, and if we only use the extracted evidence as input, the model can predict the relation "country of origin" correctly.</p><p>context. As a result, recent studies have been moving towards the more realistic setting of documentlevel relation extraction (DocRE) <ref type="bibr" target="#b7">(Peng et al., 2017;</ref><ref type="bibr" target="#b21">Yao et al., 2019;</ref><ref type="bibr" target="#b25">Zeng et al., 2020)</ref>.</p><p>Unlike typical DocRE models that blindly take the whole document as input, a human may only need a few sentences to infer the relation of an entity pair. For each entity pair, we define the minimal set of sentences required by human annotators to infer their relation as their evidence sentences. As shown in <ref type="figure">Figure 1</ref>, to predict the relation between "Hero of the Day" and "the United States", it is sufficient to know that Load (the album) was released in the United States from the 1 st sentence, and "Hero of the Day" is a single of Load from the 10 th sentence. In other words, the 1 st and 10 th sentences serve as the evidence to infer this relation. Although the 9 th sentence also mentions "the United States", it is irrelevant to this specific relation. Including such irrelevant sentences in input might sometimes introduce noise to the model and be more detrimental than beneficial.</p><p>Despite the usefulness of evidence, few prior studies leverage it in a proper way <ref type="bibr">(Huang et al., 2021a,b)</ref>. In particular, <ref type="bibr" target="#b2">Huang et al. (2021a)</ref> extracts the evidence sentences together with RE but does not utilize them after extraction. Besides, it requires human-annotated evidence for training, and also suffers from massive memory usage and training time. Another work <ref type="bibr" target="#b3">(Huang et al., 2021b)</ref> trains an RE model solely on evidence sentences, which misses important information in the original document and fails to show improvements when paired up with pre-trained language models.</p><p>In this paper, we propose an evidence-enhanced DocRE framework EIDER, which efficiently extracts evidence and effectively leverages the extracted evidence to improve DocRE. During training, we enhance DocRE by jointly extracting relations and evidence using multi-task learning, which allows the two tasks to benefit from providing additional training signals for each other. There are two major challenges regarding evidence extraction. The first challenge is the memory and runtime overhead due to training an additional task. For example, a prior multi-task method <ref type="bibr" target="#b2">(Huang et al., 2021a)</ref> needs over 14h and three consumer GPUs to train, while the individual RE model only takes around 90min on one GPU. In comparison, EIDER uses a simpler evidence extraction model, which can fit into a single GPU and only requires 95min runtime. The second challenge is that human-annotated evidence sentences are costly and heavily relying on them limits model applicability. Therefore, we design several heuristic rules to construct silver labels in case the evidence annotation is unavailable. We observe that EIDER still improves RE performance when trained with our silver labels, and sometimes even performs on par with using gold labels.</p><p>With the evidence extracted, either by our rules or evidence extraction model, we propose to further enhance DocRE by utilizing the evidence in inference. In the extreme case, if there is only one sentence related to the relation, one can make predictions solely based on this sentence and reduce the problem to sentence-level relation extraction. One naive approach is thus to directly replace the original document with the extracted evidence <ref type="bibr" target="#b3">(Huang et al., 2021b)</ref>. However, since no systems can extract evidence perfectly, solely relying on extracted sentences may miss important information and harm model performance in certain cases (see <ref type="table" target="#tab_10">Table 5</ref>). To avoid information loss, we fuse the prediction results of the original document and extracted evidence through a blending layer <ref type="bibr" target="#b17">(Wolpert, 1992)</ref>. In this way, EIDER pays more attention to the extracted important sentences, while still having access to all the information in the document. Empirical analysis demonstrates that removing either source would lead to degenerate performance.</p><p>We conduct extensive experiments on three widely-adopted DocRE benchmarks: DocRED <ref type="bibr" target="#b21">(Yao et al., 2019)</ref>, CDR <ref type="bibr" target="#b5">(Li et al., 2016)</ref> and GDA <ref type="bibr" target="#b18">(Wu et al., 2019)</ref>. Experiment results show that EIDER achieves state-of-the-art performance on all the datasets. Performance analysis further shows that the improvement of EIDER is most significant on inter-sentence entity pairs, suggesting that leveraging evidence is especially effective in reasoning over multiple sentences. In particular, EIDER significantly improves the performance on entity pairs that require co-reference/multi-hop reasoning by 1.98/2.08 F1 on DocRED, respectively. Contributions. (1) We propose an efficient joint relation and evidence extraction model that allows the two tasks to mutually enhance each other without heavily relying on evidence annotation. <ref type="formula" target="#formula_2">(2)</ref> We design a simple and effective DocRE inference process enhanced by the extracted evidence, enabling more focus on the important sentences with no information loss. (3) We demonstrate that our evidence-enhanced framework outperforms stateof-the-art methods on three DocRE datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Problem Formulation</head><p>Given a document d comprised of N sentences</p><formula xml:id="formula_0">{s n } N n=1 , L tokens {h l } L l=1 , E named entities {e i } E i=1</formula><p>and all the proper-noun mentions of each entity, {m i j }, the task of document-level relation extraction (DocRE) is to predict the set of all possible relations between all entity pairs (e h , e t ) from a pre-defined relation set R {NA}. We refer to e h and e t as the head entity and tail entity, respectively. A relation r belongs to the positive class P T h,t if it exists between (e h , e t ) and otherwise the negative class N T h,t . For each entity pair (e h , e t ) that possesses a non-NA relation, we define its evidence 2 V h,t = {s v k } K k=1 as the subset of sentences in the document that are sufficient for human annotators to infer the relation. Human annotation of evidence may or may not be given in training, depending on the datasets, but is not available in inference.  <ref type="figure">Figure 2</ref>: The overall architecture of EIDER. The left part illustrates the training stage and the right shows the inference stages of EIDER. We highlight head entities, tail entities and extracted evidences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head><p>An illustration of the framework of EIDER is shown in <ref type="figure">Figure 2</ref>. In training, we jointly extract relation and evidence using multi-task learning, where the two tasks have their own classifier and share the base encoder (Sec. 3.1). In inference, we fuse the predictions on the original document and the extracted evidence using a blending layer (Sec. 3.2). In case the evidence annotation is not available, we also provide several heuristic rules to construct silver evidence labels as an alternative (Sec. 3.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Joint Relation and Evidence Extraction</head><p>In our framework, we jointly train the relation extraction model with an evidence extraction model using multi-task learning. As shown in <ref type="figure">Figure 2</ref>, the two tasks have their own classifier but share the base encoder. Intuitively, tokens relevant to predicting the relation are essential in both models. By sharing the base encoder, the two tasks can provide additional training signals for each other and hence mutually enhance each other <ref type="bibr" target="#b9">(Ruder, 2017)</ref>. Base Encoder. We leverage pre-trained language models <ref type="bibr" target="#b1">(Devlin et al., 2019)</ref> to encode the semantic meanings of each token in the document. Specifically, given a document d = [h l ] L l=1 , we insert a special token "*" before and after each entity mention {m i j } and leverage the encoder to obtain the sdim token embeddings H = [h 1 , ..., h L ], h l ? R s and the cross token attention A ? R L?L :</p><formula xml:id="formula_1">H, A = Encoder([h 1 , ..., h L ]),<label>(1)</label></formula><p>where A is the average of the attention heads in the last transformer layer <ref type="bibr" target="#b13">(Vaswani et al., 2017)</ref>. For each mention of an entity e i , we use the embedding of the start symbol "*" as its mention embedding m i j . Then, we obtain the embedding of entity e i by adopting LogSumExp pooling <ref type="bibr" target="#b4">(Jia et al., 2019;</ref><ref type="bibr" target="#b31">Zhou et al., 2021)</ref> over the embeddings of all its mentions: e i = log j exp(m i j ). To predict the relation of different entity pairs, a model may need to focus on different parts of the context. To capture the context relevant to each entity pair (e h , e t ), we compute its context embedding c h,t ? R s based on the attention matrix A from the pre-trained encoder <ref type="bibr" target="#b31">(Zhou et al., 2021)</ref>:</p><formula xml:id="formula_2">c h,t = H T A h ? A t A T h A t ,<label>(2)</label></formula><p>where ? is the Hadamard product and A h ? R L is e h 's attention to all the tokens in the document, obtained by averaging e h 's mention-level attention.</p><p>Similarly for A t . The intuition is that tokens with high attention towards both e h and e t are important to both entities. Hence, these tokens are likely to be essential to the relation and should contribute more to the context embedding.</p><p>Relation Classifier. To predict the relation between an entity pair (e h , e t ), we first compute their context-aware representations (z h , z t ) by combining their entity embeddings (e h , e t ) with their context embedding c h,t and then utilize a bilinear function to calculate the logit of how likely a relation r ? R exists between e h and e t :</p><formula xml:id="formula_3">z h = tanh (W h e h + W c h c h,t ) , z t = tanh (W t e t + W ct c h,t ) , y r = z h W r z t + b r ,<label>(3)</label></formula><p>where W h , W t , W c h , W ct , W r and b r are learnable parameters. As the model may have different confidence for different entity pairs, we apply the adaptive-thresholding loss <ref type="bibr" target="#b31">(Zhou et al., 2021)</ref>, which learns a dummy relation class TH that serves as the dynamic threshold for each entity pair:</p><formula xml:id="formula_4">y TH = z h W TH z t + b r .<label>(4)</label></formula><p>During inference, for each tuple (e h , e t , r), r ? R, we obtain the prediction score: S</p><formula xml:id="formula_5">(O)</formula><p>h,t,r = y r ? y T H . Finally, we define our training objective for relation extraction as follows:</p><formula xml:id="formula_6">LRE = ? h =t r?P T h,t log exp (yr) r ?P T h,t ?{TH} exp (y r ) ? log exp (yTH) r ?N T h,t ?{TH} exp (y r ) .<label>(5)</label></formula><p>Evidence Classifier. In addition to the relation, we also predict whether each sentence s n is an evidence sentence of entity pair (e h , e t ). Similar to entity embeddings, to obtain sentence embedding s n , we apply a LogSumExp pooling over all the tokens in s n : s n = log h l ?sn exp (h l ). Intuitively, if s n is an evidence sentence of (e h , e t ), the tokens in s n would be relevant to the relation prediction, and should contribute more to c h,t . Hence, we use a bilinear function between context embedding c h,t and sentence embedding s n to measure the importance of sentence s n to entity pair (e h , e t ):</p><formula xml:id="formula_7">P (s n |e h , e t ) = ? (s n W v c h,t + b v ) ,<label>(6)</label></formula><p>where W v and b v are learnable parameters.</p><p>As an entity pair may have more than one evidence sentence, we use the binary cross entropy as the objective to train the evidence extraction model.</p><formula xml:id="formula_8">L Evi = ? h =t,NA / ?P T h,t sn?D y n ? P (s n |e h , e t ) + (1 ? y n ) ? log(1 ? P (s n |e h , e t )),<label>(7)</label></formula><p>where the evidence label y n is 1 when s n ? V h,t and otherwise 0. If golden labels are not provided, we use several heuristic rules to construct silver labels instead. Details are introduced in Sec 3.3. Finally, we optimize our model by the combination of the relation extraction loss L RE and evidence extraction loss L Evi :</p><formula xml:id="formula_9">L = L RE + L Evi .<label>(8)</label></formula><p>Efficiency Considerations. Compared to a previous method E2GRE <ref type="bibr" target="#b2">(Huang et al., 2021a)</ref> that also extracts the evidence, EIDER is significantly more efficient in both memory and training time for two reasons. First, E2GRE learns |R| representations for each sentence. Namely, it makes evidence prediction for every (entity, entity, sentence, relation) tuple, which requires expensive computation especially when |R| is large (e.g., |R| = 96 in DocRED).</p><p>In contrast, we observe that most entity pairs only have one set of evidence across relations and thus predict only one set of evidence for each entity pair. Second, E2GRE regards the evidence label of entity pairs with r = NA as an empty set. However, these entity pairs may still involve some relation beyond the pre-defined relation set R, which also have their evidence sentences. Hence, we train the evidence extraction model only on entity pairs with at least one non-NA relation, which accounts for a small subset (e.g., 2.97% in DocRED) of all the entity pairs. Experiments show that EIDER achieves better performances than E2GRE in both RE and evidence extraction while requiring only 30% of its memory usage and 11% of its runtime.</p><p>Furthermore, E2GRE does not utilize the evidence after extraction and relies heavily on the human annotation of evidence, which we will address in the following sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Fusion of Evidence in Inference</head><p>Suppose the extracted evidence sentences already contain all the information relevant to the relation, then there is no need to use the whole document for relation extraction. However, no system can perfectly extract the evidence without missing any sentences. Solely relying on the extracted evidence may miss important information in the document and lead to sub-optimal performance. Therefore, we combine the prediction results on both the original document and the extracted evidence, which can either be learned by our evidence classifier (Sec. 3.1) or constructed by our heuristic rules (Sec. 3.3) if evidence annotation is unavailable. Even without joint training, one may directly improve general (trained) DocRE models by applying our proposed inference process (noted as EIDER (Rule)-Nojoint in <ref type="table" target="#tab_10">Table 5</ref>).</p><p>Specifically, as shown in <ref type="figure">Figure 2</ref>, we first obtain a set of relation prediction scores S (O) h,t,r from the original documents. Then we construct a pseudo document d h,t for each entity pair by concatenating the extracted evidence sentences V h,t in the order they present in the original document. The prediction score of the RE model on the pseudo document is noted as S (E) h,t,r . Finally, we fuse the results by aggregating the two sets of prediction sores through a blending layer <ref type="bibr" target="#b17">(Wolpert, 1992)</ref>:</p><formula xml:id="formula_10">P F use (r|e h , e t ) = ?(S (O) h,t,r + S (E) h,t,r ? ? ). (9)</formula><p>We choose this design because it is simple and only includes one learnable parameter, ? , alleviating over-fitting in the development set. We optimize the parameter ? on the development set as follows:</p><formula xml:id="formula_11">L F use = ? d?D h =t r?R y r ? P F use (r|e h , e t ) + (1 ? y r ) ? log(1 ? P F use (r|e h , e t ))</formula><p>, <ref type="formula" target="#formula_1">(10)</ref> where y r = 1 if relation r holds between (e h , e t ) and y r = 0 otherwise. Empirically, using other loss functions does not affect the performance much.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Heuristic Evidence Label Construction</head><p>In case that human annotation of evidence is not available, we design a set of heuristic rules to automatically construct silver labels for evidence extraction. Then we train our joint model on the silver labels and directly use the silver labels as pseudo documents in inference. The percentage of test samples covered by each rule is shown in <ref type="table" target="#tab_12">Table 6</ref>.</p><p>Co-occur. If the head and tail entities co-occur in the same sentence (e.g., "Load" and "the United States" co-occur in the 1 st sentence in <ref type="figure">Figure 2</ref>), we use all the sentences they co-occur as evidence.</p><p>Coref. If the proper-noun mentions of the head and tail entity do not co-occur, but their coreferential mentions co-occur (e.g., "Hero of the Day" and "the album", the co-reference of "Load" co-occur in the 10 th sentence in <ref type="figure">Figure 2</ref>), we use all the sentences where their coreferential mentions cooccur as evidence. In practice, we directly apply a pre-trained coreference resolution model, HOI <ref type="bibr" target="#b19">(Xu and Choi, 2020)</ref>, without fine-tuning on our dataset.</p><p>Bridge. If the first two conditions are not met, but there exists a third bridge entity whose coreferential mention co-occurs with both head and tail (e.g., "Load" or its coreferential mention "the album" cooccurs with both "the United States" and "Hero of the Day" in <ref type="figure">Figure 2</ref>), we take all the sentences where the bridge co-occurs with head or tail as the evidence. If there are more than one bridge entities, we choose the one with the highest frequency.</p><p>While this rule can be easily extended to multiple bridges, we empirically observe that capturing one bridge already leads to satisfying results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experiment Setup</head><p>Datasets. We evaluate the effectiveness of EI-DER on three datasets: DocRED <ref type="bibr" target="#b21">(Yao et al., 2019)</ref>, CDR <ref type="bibr" target="#b5">(Li et al., 2016)</ref> and GDA <ref type="bibr" target="#b18">(Wu et al., 2019)</ref>, where DocRED is the only dataset that provides evidence labels as part of the annotation. The details of the datasets are listed in Appendix A.1.</p><p>Implementation Details. Our model is implemented based on PyTorch and Huggingface's Transformers <ref type="bibr" target="#b16">(Wolf et al., 2019)</ref>. We use cased-BERT base <ref type="bibr" target="#b1">(Devlin et al., 2019)</ref> and RoBERTa large as the base encoders and optimize our model using AdamW with learning rate 5e-5 for the encoder and 1e ? 4 for other parameters. We adopt a linear warmup for the first 6% steps. The batch size (number of documents per batch) is set to 4 and the ratio between relation extraction and evidence extraction losses is set to 0.1. We perform early stopping based on the F1 score on the development set, with a maximum of 30 epochs. Our BERT base models are trained with one GTX 1080 Ti GPU and RoBERTa large models with one RTX A6000 GPU.</p><p>Evaluation Metrics. Following prior studies <ref type="bibr" target="#b21">(Yao et al., 2019)</ref>, we use F1 and Ign F1 as the main evaluation metrics for relation extraction, where Ign F1 measures the F1 score excluding the relations shared by the training and development/test set. We also report Intra F1 and Inter F1, where the former measures the performance on the co-occurred (intra-sentence) entity pairs and the latter evaluates the inter-sentence entity pairs where none of their proper-noun mentions co-occurs. For evidence extraction, we compute the F1 score (denoted as Evi F1) and further introduce PosEvi F1, which measures the F1 score of evidence only on positive entity pairs (i.e., those with non-NA relations).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Main Results</head><p>We compare our methods with both Graph-based methods and transformer-based methods. Graphbased methods explicitly perform inference on document-level graphs. Transformer-based methods, including EIDER, implicitly capture the longdistance token dependencies via transformers. Noted that EIDER is trained on gold labels and  <ref type="bibr" target="#b20">(Xu et al., 2021)</ref> 58.13 60.18 --57.12 59.45 GAIN-BERT base <ref type="bibr" target="#b25">(Zeng et al., 2020)</ref> 59  <ref type="bibr" target="#b11">(Tang et al., 2020)</ref> 54.29 56.31 --53.70 55.60 E2GRE-BERT base <ref type="bibr" target="#b2">(Huang et al., 2021a)</ref> 55.22 58.72 ----CorefBERT base <ref type="bibr" target="#b22">(Ye et al., 2020)</ref> 55.32 57.51 --54.54 56.96 ATLOP-BERT base <ref type="bibr" target="#b31">(Zhou et al., 2021)</ref> 59   <ref type="bibr" target="#b31">(Zhou et al., 2021)</ref> 65.1 ? 0.6 82.5 ? 0.3 DHG-BERT base <ref type="bibr" target="#b30">(Zhang et al., 2020b)</ref> 65.9 83.1 GLRE-SciBERT base  68.5 -ATLOP-SciBERT base <ref type="bibr" target="#b31">(Zhou et al., 2021)</ref> 69.4 ? 1.1 83.9 ? 0.2 EIDER (Rule)-SciBERT base 70.63 ? 0.49 84.54 ? 0.22 leverages the evidence extracted by our model in inference. EIDER (Rule) is trained on silver evidence labels constructed by rules and also leverages them in inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Relation Extraction Results</head><p>. <ref type="table" target="#tab_4">Tables 1 and 2</ref> show that EIDER outperforms the DocRE baseline methods in all datasets. Our improvement is especially large on Inter F1 (e.g., 1.21/2.01 Intra/Inter F1 compared to ATLOP-BERT base ). We hypothesize that the bottleneck of inter-sentence pairs is to locate the relevant context, which often spreads through the whole document. EIDER learns to capture important sentences in training and focuses more on these important sentences in inference. Among the baselines, the Inter F1 of GAIN is 0.70 higher than ATLOP while the Intra F1 of AT-LOP is 0.16 higher than GAIN, indicating that document-level graphs may be effective in multi-  hop reasoning. Although EIDER does not involve explicit multi-hop reasoning modules, it still notably outperforms graph-based models in Inter F1. Finally, EIDER (Rule) also outperforms all the baselines in both DocRED and the two biomedical datasets which do not have evidence annotation. The improvement on DocRED and CDR is much larger than that on GDA. We hypothesize that it is because more than 85% relations in GDA are intra-sentence ones, making it trivial even for the single RE model to focus on these sentences.</p><p>Evidence Extraction Results. To our knowledge, E2GRE is the only method that has reported their evidence extraction result. The results in <ref type="table" target="#tab_7">Table 3</ref> indicate that EIDER outperforms E2GRE significantly (e.g., by 3.57 Dev Evi F1 under BERT base ). The results show that it may be sufficient to train the evidence classifier only on pairs with r ? R   and over each (entity, entity, sentence) tuple instead of (entity, entity, sentence, relation) as in E2GRE. Our ablation studies in <ref type="table" target="#tab_9">Table 4</ref> show that our three heuristic rules, denoted as Rules (ours), already capture most of the evidence for positive entity pairs. The high quality of silver labels explains why our model can perform well using silver labels only. Furthermore, training the RE model and evidence extraction model separately (denoted as NoJoint) results in a sharp performance drop. As the relation and evidence classifiers share the same base encoder, discarding the relation classifier will result in insufficient training of the base encoder and harm the performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Performance Analysis</head><p>Ablation Study. <ref type="table" target="#tab_10">Table 5</ref> shows the ablation studies that analyzes the utility of each module in EI-DER. We observe that NoJoint leads to sharp performance drop in DocRE. Besides, EIDER (Rule)-Nojoint achieves significant "free gains" (0.90/1.08 Ign F1/F1) by simply fusing the evidence constructed by rules in the inference of AT-LOP. In principle, this inference process can be applied to general DocRE models.</p><p>We also remove the pseudo document (constructed from the extracted evidence) and the original document separately, denoted as NoPseudo and NoOrigDoc, respectively. We observe that removing either source will lead to performance drops. Also, the drop of Inter F1 is much larger than Intra F1 for NoPseudo, indicating that our inference process is effective for inter-sentence pairs where the evidence may not be consecutive.</p><p>As for NoBlending, we remove the blending layer and simply take the union of the two sets of   results. The sharp drop of performance indicates the blending layer can successfully learn a dynamic threshold to combine the prediction results. Finally, we further finetune the RE model on ground truth evidence before feeding it the extracted evidence (denoted as FinetuneOnEvi) but the performance is not improved, probably because the encoded entity representations in evidence and original documents are already highly similar.</p><p>Performance Breakdown. To further analyze the performance of EIDER on different types of entity pairs, we categorize the relations into three categories based on our three heuristic rules in Sec. 3.3: Co-occur, Coref and Bridge. The number and percentage of relations covered by each rule are listed in <ref type="table" target="#tab_12">Table 6</ref>. We can see that the three categories cover over 88% of the relations in the development set. The results on each category are shown in <ref type="figure" target="#fig_0">Figure 3</ref>. We can see that our full model has the best performance in all three categories and our ablations also outperform ATLOP. For all our methods, the improvements over ATLOP is Bridge &gt; Coref Co-occur. This reveals that both modules mainly improve the model's reasoning ability from multiple sentences, either by coreference reasoning or by multi-hop reasoning over a third entity.   <ref type="table">Table 8</ref>: Case studies of our proposed framework EIDER. We use red, blue and green to color the head entity, tail entity and relation, respectively. The indices of extracted evidence sentences are highlighted with yellow.</p><p>Efficiency Comparison. We benchmark the time and memory usage of EIDER on an RTX A6000 GPU. <ref type="table" target="#tab_14">Table 7</ref> shows that our joint model incurs only~5% training time and~14% GPU memory overhead. Experiments also show that EIDER can be trained on a single consumer GPU (e.g., an 11GB GTX 1080 Ti) but E2GRE is not able to. <ref type="table">Table 8</ref> shows a few examples of EIDER. Detailed statistics and error analysis are provided in Appendix A.2. In the first example, the head entity is mentioned in the first sentence and the tail entity appears in the second. We can see that EIDER correctly extracts these sentences as evidence. Since the evidence sentences are consecutive, the predictions on both the original document and the evidence sentences are correct. In the second example, the prediction using only the original document is incorrect, possibly because the "King Louie" in the 1 st and 3 rd sentences are so far away from each other that the model fails to recognize them as coreference. Hence, it fails to distinguish "King Louie" as a bridge entity and wrongly predicts "NA". Instead, these two sentences are consecutive in the extracted evidence, making it easier for the model to find the bridge. In the last example, the 6 th sentence is missing in the extracted evidence, so the extracted evidence does not contain enough information to predict the relation. However, the prediction on the original document is correct, leading to the correct final result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Case Studies</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>Relation Extraction. Previous research efforts on relation extraction mainly concentrate on predicting relations within a sentence <ref type="bibr" target="#b0">(Cai et al., 2016;</ref><ref type="bibr" target="#b28">Zhang et al., 2018</ref><ref type="bibr" target="#b27">Zhang et al., , 2020a</ref>. Despite their effectiveness, in the real world, certain relations can only be inferred from multiple sentences. Consequently, recent studies <ref type="bibr" target="#b7">Peng et al., 2017;</ref><ref type="bibr" target="#b21">Yao et al., 2019)</ref> started to work on document-level relation extraction (DocRE).</p><p>Graph-based DocRE. Graph-based DocRE methods generally construct a graph with mentions, entities, sentences, or documents as the nodes, and infer the relations by reasoning on this graph. <ref type="bibr" target="#b25">Zeng et al. (2020)</ref> performs multi-hop reasoning on both a mention-level graph and an entity-level graph. <ref type="bibr" target="#b20">Xu et al. (2021)</ref> extracts a reasoning path for each relation and encourages the model to reconstruct the path during training. <ref type="bibr" target="#b24">Zeng et al. (2021)</ref> separately deals with intra-and inter-sentential entity pairs and performs multi-hop reasoning on a mentionlevel graph for inter-sentential entity pairs. However, the extracted graph may omit some important information in the text. Complicated operations on the graphs may also hinder the model from capturing the text structure.</p><p>Transformer-based DocRE. Another line of studies model cross-sentence relations by implicitly capturing the long-distance token dependencies via the transformer <ref type="bibr" target="#b13">(Vaswani et al., 2017)</ref>. <ref type="bibr" target="#b31">Zhou et al. (2021)</ref> uses attention in the transformers to extract useful context and adopts an adaptive threshold for each entity pair. <ref type="bibr" target="#b26">Zhang et al. (2021)</ref> views DocRE as a semantic segmentation task over the entity matrix and applies a U-Net to capture the correlations between relations. <ref type="bibr" target="#b2">Huang et al. (2021a)</ref> guides DocRE by extracting evidence but does not leverage them after extraction. It also highly relies on evidence annotations and suffers from massive runtime and memory overhead. <ref type="bibr" target="#b3">Huang et al. (2021b)</ref> predicts on only a few sentences selected by rules, which may miss important information and does not show consistent improvements. In comparison, we design a lightweight evidence extraction model that is significantly more efficient than <ref type="bibr" target="#b2">Huang et al. (2021a)</ref> and can improve DocRE even trained on silver labels. EIDER also fuses the extracted evidence in inference, putting more attention to the important sentences without information loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this work, we propose EIDER, an evidenceenhanced RE framework, which improves DocRE by joint relation and evidence extraction and fusion of extracted evidence in inference. In training, the RE and evidence extraction model provide additional training signals for each other and mutually enhance each other. The joint model is efficient in time and memory and does not rely heavily on the human annotation of evidence. During inference, the prediction results on both the original document and the extracted evidence are combined, which encourages the model to focus on the important sentences while reducing information loss. Experiment results demonstrate that EIDER significantly outperforms existing methods on three public datasets (DocRED, CDR, and GDA), especially on inter-sentence relations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Appendices</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Dataset Statistics</head><p>Our model is evaluated on three benchmark datasets, where the statistics are shown in <ref type="table" target="#tab_16">Table 9</ref>: DocRED <ref type="bibr" target="#b21">(Yao et al., 2019</ref>) is a large humanannotated document-level RE dataset constructed from Wikipedia. In the training set, around 97.03% entity pairs do not hold any explicit relations. In our experiments, the performance on the test set is validated through the Leader board 3 .</p><p>CDR <ref type="bibr" target="#b5">(Li et al., 2016</ref>) is a biomedical relation extraction dataset consisting of 1,500 PubMed abstracts. The only two entity types are chemicals and diseases and the only non-NA relation is the causal relation between chemicals and disease concepts.</p><p>GDA <ref type="bibr" target="#b18">(Wu et al., 2019)</ref> contains 30,192 MED-LINE abstracts. It is also a biomedical dataset with two entity types only: diseases and genes, and one non-NA relation type only: the interactions between disease concepts and genes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Statistics</head><p>DocRED CDR GDA  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Error Analysis of EIDER</head><p>The detailed statistics of the predictions of our model are listed in <ref type="table" target="#tab_4">Table 10</ref>. Among all the errors, the majority is because the model wrongly predicts the non-NA relations (i.e., r ? R) as "NA" or predicts "NA" as some non-NA relations. Only 287 287+4340+3613 = 3.48% of the errors result from wrongly taking some non-NA relation as another.</p><p>To check the exact reason why our model makes these errors, we randomly select 50 cases from DocRED where our model predicts wrongly. We summarize the error types in <ref type="table" target="#tab_4">Table 11</ref> and provide one or two examples for each of the common error types in <ref type="table" target="#tab_4">Table 12</ref>. <ref type="table" target="#tab_4">Table 10</ref>: Statistics of one run of EIDER-RoBERTa large . "r ? R" means non-NA relations. We use " " and "" to denote correct and wrong predictions, respectively. For example, we have 4,340 wrong predictions where the ground truth is some r ? R but the prediction is NA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Reason Count</head><p>Labeling  <ref type="table" target="#tab_4">Table 11</ref>: Error types of EIDER in 50 randomly sampled error cases in DocRED. Where "Labeling Mistakes" means our model predicts correctly but the annotation is wrong.</p><p>Our analysis shows that 18 out of 50 "error cases" are actually correct. It suggests that labeling mistakes are still prevalent in the DocRED dataset. We show an example under "Error Type 1" in <ref type="table" target="#tab_4">Table 12</ref>. The annotator wrongly labels "U.S. Route 20", a highway, as the country of "Capital District".</p><p>Another common error type is "Error Type 2": failing in commonsense reasoning. These error examples normally require commonsense knowledge of the related entities that does not explicitly present in the document. In the first case, the document shows that the airport is located in "Michigan" and is near the "Crooks Road". Then we still require the commonsense knowledge that a road (Crooks Road) is a rather small location compared to a state (Michigan). Finally, we can conclude that "Crooks Road" locates in "Michigan".</p><p>The second case requires the commonsense knowledge about the church. Specifically, if a pope (Benedict XVI) can remove a priest (Maciel) from the ministry, they must be in the same church and hence share the same religion. From sentence <ref type="bibr">[2]</ref> we know the priest, Maciel, is a Catholic, hence the pope, Benedict XVI, must also be a Catholic. Even though our prediction on extracted evidence is correct, the confidence is still not high, leading to the incorrect final prediction. As the logic chain of <ref type="table" target="#tab_4">Table 12</ref>: Examples for the four most common error types. We use red, blue and green to color the head entity, tail entity and relation, respectively. The indices of extracted evidence sentences are highlighted with yellow. commonsense reasoning is always complicated, it is not easy to find a very similar pattern in the training set, or even during pre-training, which makes the problem difficult for a model.</p><p>In most of the cases (5 out of 6) in "Error Type 3: Fail in Coreferential Reasoning", human can still identify the correct relation based on the extracted evidence only. As shown in our example in Table 12, in the first sentence, the model wrongly predicts "Giacomo Casanova" as the father of "Manon Balletti", but her real father should be an "Italian actor performing in France". It shows that even the reasoning within a single sentence can be difficult.</p><p>Similarly, the example in "Error Type 4" also shows that the prediction can still be wrong even if we extract the correct evidence sentences and simplify the problem to sentence-level RE. This suggests that if the performance of sentence-level RE is improved, the performance of DocRE will also improve.</p><p>Finally, as described by "Error Type 5", some examples require direct reasoning from the surface names of the head and tail entities. As shown in the the last case in <ref type="table" target="#tab_4">Table 12</ref>, humans can directly identify that "China" is the country of North China without reading the document, despite that there are no clue in the document indicates this relation. However, most DocRE models, including EIDER, learn to predict the relations only based on the given document and sometimes fail in such cases.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 :</head><label>3</label><figDesc>Performance gains in F1 by relation categories. The gains are relative to the second best baseline (ATLOP-RoBERTa large ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Pred Scores from Orig doc Country of Origin: -2.84 Creator: -7.82 Location: -11.53 ?the United States ? Hero of the Day were released ? for the album the United States Hero of the Day</head><label></label><figDesc></figDesc><table><row><cell cols="2">Predicted Relation: NA (</cell><cell>)</cell><cell cols="2">Extracted Evidence: [1, 10]</cell><cell></cell><cell></cell><cell>Final Predicted Relation: Country of Origin ( ? )</cell></row><row><cell>? Head Emb</cell><cell cols="2">Relation Classifier ?</cell><cell>Evidence Classifier</cell><cell>?</cell><cell>?</cell><cell>[1]</cell><cell>Blending Layer</cell></row><row><cell cols="4">? Tail Emb Weighted Sum Load is ? in ? ? Context Emb ? ?</cell><cell cols="2">? ? Sent Embs</cell><cell>[9] [10]</cell><cell>Pred Scores from Orig doc Country of Origin: -2.84 Creator: -7.82 Location: -11.53 ?</cell><cell>Pred Scores from Pseudo doc Country of Origin: 4.86 Creator: -9.70 Location: -14.47 ?</cell></row><row><cell cols="2">Attention to head &amp; tail</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Relation Extraction</cell></row><row><cell></cell><cell cols="3">Encoder (Pre-trained Language Model)</cell><cell></cell><cell></cell><cell></cell><cell>Pseudo Document: [1] Load is ... released ? in</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>the United States ? [10] Four singles -"Hero</cell></row><row><cell cols="7">Original Document: [1] Load is ... released ... in the United States ? [9] It was</cell><cell>of the Day", ? as part of the ? for the album.</cell></row><row><cell cols="7">certified 5?platinum ? in the United States. [10] Four singles -"Hero of the Day",</cell></row><row><cell cols="6">"Until It Sleeps", ? were released as part of the marketing campaign for the album.</cell><cell></cell><cell>Evidence Extraction (by Classifier OR Rules)</cell></row><row><cell></cell><cell cols="3">Joint Relation and Evidence Extraction in Training</cell><cell></cell><cell></cell><cell></cell><cell>(Extracted) Evidence Empowered Inference</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>.11 ? 0.14 ? 61.01 ? 0.10 ? 67.26 ? 0.15 ? 53.20 ? 0.19 ? 59.31 61.30</figDesc><table><row><cell>EIDER (Rule)-BERT base</cell><cell cols="4">60.36 ? 0.13 62.34 ? 0.08 68.40 ? 0.14 54.79 ? 0.13</cell><cell>60.23 62.21</cell></row><row><cell>EIDER-BERT base</cell><cell cols="4">60.51 ? 0.11 62.48 ? 0.13 68.47 ? 0.08 55.21 ? 0.21</cell><cell>60.42 62.47</cell></row><row><cell>RoBERTa large (Ye et al., 2020)</cell><cell>57.14</cell><cell>59.22</cell><cell>-</cell><cell>-</cell><cell>57.51 59.62</cell></row><row><cell>CorefRoBERTa large (Ye et al., 2020)</cell><cell>57.35</cell><cell>59.43</cell><cell>-</cell><cell>-</cell><cell>57.90 60.25</cell></row><row><cell>E2GRE-RoBERTa large (Huang et al., 2021a)</cell><cell>59.55</cell><cell>62.91</cell><cell>-</cell><cell>-</cell><cell>60.29 62.51</cell></row><row><cell>GAIN-BERT large (Zeng et al., 2020)</cell><cell>60.87</cell><cell>63.09</cell><cell>-</cell><cell>-</cell><cell>60.31 62.76</cell></row><row><cell>ATLOP-RoBERTa large (Zhou et al., 2021)</cell><cell cols="5">61.30 ? 0.22  39 63.40</cell></row><row><cell>EIDER (Rule)-RoBERTa large</cell><cell cols="4">61.73 ? 0.07 63.91 ? 0.07 69.99 ? 0.09 56.27 ? 0.11</cell><cell>61.93 64.12</cell></row><row><cell>EIDER-RoBERTa large</cell><cell cols="4">62.34 ? 0.14 64.27 ? 0.10 70.36 ? 0.07 56.53 ? 0.15</cell><cell>62.85 64.79</cell></row></table><note>? 63.15 ? 0.21 ? 69.61 ? 0.25 ? 55.01 ? 0.18 ? 61.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table><row><cell>Model</cell><cell>CDR</cell><cell>GDA</cell></row><row><cell>LSR-BERT base (Nan et al., 2020)</cell><cell>64.8</cell><cell>82.2</cell></row><row><cell>SciBERT base</cell><cell></cell><cell></cell></row></table><note>Relation extraction results on DocRED. We report the mean and standard deviation on the development set by conducting 5 runs with different random seeds. We report the official test score of the best checkpoint on the development set. Results with ? are based on our implementation. Others are reported in their original papers. We separate graph-based and transformer-based methods into two groups.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2 :</head><label>2</label><figDesc>Relation extraction results on CDR and GDA.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table /><note>Evidence extraction results on DocRED. We compare EIDER with E2GRE (Huang et al., 2021a).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>Rules (ours) EIDER-BERT base NoJoint</figDesc><table><row><cell>PosEvi F1</cell><cell>77.43</cell><cell>80.33</cell><cell>51.13</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 4 :</head><label>4</label><figDesc>Ablation study for evidence extraction.</figDesc><table><row><cell>Ablation</cell><cell>Ign F1</cell><cell>F1</cell><cell cols="2">Intra F1 Inter F1</cell></row><row><cell>EIDER-BERT base</cell><cell cols="2">60.51 62.48</cell><cell>68.47</cell><cell>55.21</cell></row><row><cell>NoJoint</cell><cell cols="2">59.98 62.03</cell><cell>68.51</cell><cell>54.10</cell></row><row><cell>NoPseudo</cell><cell cols="2">59.70 61.53</cell><cell>67.55</cell><cell>54.01</cell></row><row><cell>NoOrigDoc</cell><cell cols="2">58.47 60.44</cell><cell>66.24</cell><cell>53.23</cell></row><row><cell>NoBlending</cell><cell cols="2">58.93 61.46</cell><cell>67.33</cell><cell>54.37</cell></row><row><cell>FinetuneOnEvi</cell><cell cols="2">60.11 62.29</cell><cell>68.13</cell><cell>54.84</cell></row><row><cell cols="3">EIDER (Rule)-BERT base 60.36 62.34</cell><cell>68.40</cell><cell>54.79</cell></row><row><cell>NoJoint</cell><cell cols="2">60.01 62.09</cell><cell>68.21</cell><cell>54.34</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 5 :</head><label>5</label><figDesc>Ablation study of EIDER on DocRED.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 6 :</head><label>6</label><figDesc>Statistics of the 12,323 relations in the Do-cRED development set.</figDesc><table><row><cell></cell><cell>2.0 2.5</cell><cell>Eider-Full Eider-NoPseudo Eider-NoJoint</cell><cell>+1.98</cell><cell></cell><cell>+2.08</cell></row><row><cell></cell><cell></cell><cell>ATLOP</cell><cell></cell><cell></cell><cell></cell></row><row><cell>F1</cell><cell>1.5</cell><cell></cell><cell></cell><cell></cell><cell>+1.30</cell></row><row><cell></cell><cell>0.5 1.0</cell><cell>+0.25 +0.49 +0.75</cell><cell cols="2">+0.57 +1.01</cell><cell cols="2">+0.85</cell></row><row><cell></cell><cell>0.0</cell><cell>Co-occur 69.61</cell><cell>Coref</cell><cell>61.61</cell><cell>Bridge</cell><cell>53.07</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 7 :</head><label>7</label><figDesc>Training time and memory usage on DocRED. Truth Relation: Located in Ground Truth Evidence Sentence(s): [1, 2] Extracted Evidence Sentence(s): [1, 2] Document: [1] The Portland Golf Club is a private golf club in the northwest United States , in suburban Portland, Oregon. [2] It is located in the unincorporated Raleigh Hills area of eastern Washington County, southwest of downtown Portland and east of Beaverton. [3] The club was established in the winter of 1914, when a group of nine businessmen assembled to form a new club after leaving their respective clubs ... King Louie is a fictional character introduced in Walt Disney's 1967 animated musical film, The Jungle Book. [2]Unlike the majority of the adapted characters in the film, Louie was not featured in Rudyard Kipling's original works.[3] King Louie was portrayed as an orangutan who was the leader of the other jungle primates, and who attempted to gain knowledge of fire from Mowgli, ... Russian entrepreneur and cycling sponsor. ...<ref type="bibr" target="#b33">[5]</ref> Tinkoff is the founder and chairman of the Tinkoff Bank board of directors (until 2015 it was called Tinkoff Credit Systems).[6] The bank was founded in 2007 and as of December 1, 2016, it is ranked 45 in terms of assets and 33 for equity among Russian banks. ...</figDesc><table><row><cell>Ground Final Prediction: Located in</cell><cell>Prediction on Orig. Doc: Located in</cell><cell cols="2">Prediction on Extracted Evidences: Located in</cell></row><row><cell cols="4">Ground Truth Relation: Characters Ground Truth Evidence Sentence(s): [1, 3] Extracted Evidence Sentence(s): [1, 3]</cell></row><row><cell>Document: [1] Final Prediction: Characters</cell><cell>Prediction on Orig. Doc: NA</cell><cell cols="2">Prediction on Extracted Evidences: Characters</cell></row><row><cell cols="3">Ground Truth Relation: Inception Ground Truth Evidence Sentence(s): [5, 6]</cell><cell>Extracted Evidence Sentence(s): [5]</cell></row><row><cell cols="2">Document: [1] Oleg Tinkov (born 25 December 1967 ) is a Final Prediction: Inception Prediction on Orig. Doc: Inception</cell><cell></cell><cell>Prediction on Extracted Evidences: NA</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 9 :</head><label>9</label><figDesc>Statistics of the datasets in experiments.The percentage of intra-sentence relations is calculated from the development set of DocRED and calculated from the test set of CDR and GDA.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Our code is available at https://github.com/ Veronicium/Eider</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">We use "evidence sentence" and "evidence" interchangeably throughout the paper.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Results can be found at https://competitions. codalab.org/competitions/20717.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Bidirectional recurrent convolutional neural network for relation classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houfeng</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter</title>
		<meeting>the 2019 Conference of the North American Chapter</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Entity and evidence guided document-level relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangtao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengyu</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th Workshop on Representation Learning for NLP</title>
		<meeting>the 6th Workshop on Representation Learning for NLP</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="4" to="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Three sentences are all you need: Local path enhanced document relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quzhe</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengqi</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yansong</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxuan</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyan</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Short Papers</publisher>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Document-level n-ary relation extraction with multiscale representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robin</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cliff</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hoifung</forename><surname>Poon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter</title>
		<meeting>the 2019 Conference of the North American Chapter</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Biocreative V CDR task corpus: a resource for chemical disease relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yueping</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robin</forename><forename type="middle">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniela</forename><surname>Sciaky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chih-Hsuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Leaman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Allan</forename><forename type="middle">Peter</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carolyn</forename><forename type="middle">J</forename><surname>Mattingly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">C</forename><surname>Wiegers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyong</forename><surname>Lu</surname></persName>
		</author>
		<idno type="DOI">10.1093/database/baw068</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Reasoning with latent structure refinement for document-level relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoshun</forename><surname>Nan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhijiang</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Sekulic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Cross-sentence n-ary relation extraction with graph LSTMs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanyun</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hoifung</forename><surname>Poon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Quirk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Distant supervision for relation extraction beyond the sentence boundary</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Quirk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hoifung</forename><surname>Poon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the 15th Conference of the European Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">An overview of multi-task learning in deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>ArXiv</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Discovering hypernymy in text-rich heterogeneous information network by exploiting context granularity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaming</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchen</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengzhi</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Lou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myung-Hwan</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">HIN: hierarchical inference network for documentlevel relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengzhu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanan</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangxia</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fang</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Yin</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-47426-3_16</idno>
	</analytic>
	<monogr>
		<title level="m">Advances in Knowledge Discovery and Data Mining -24th Pacific-Asia Conference</title>
		<meeting><address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-05-11" />
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
	<note>Part I</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Neural relation extraction for knowledge base enrichment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerhard</forename><surname>Bayu Distiawan Trisedya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianzhong</forename><surname>Weikum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Global-to-local neural networks for document-level relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Difeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ermei</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>EMNLP</publisher>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Fine-tune bert for docred with two-step process</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christfried</forename><surname>Focke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Sylvester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nilesh</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>Computing Research Repository</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Huggingface&apos;s transformers: Stateof-the-art natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clement</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierric</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Rault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R?mi</forename><surname>Louf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Morgan</forename><surname>Funtowicz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ArXiv</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wolpert</surname></persName>
		</author>
		<title level="m">Stacked generalization. Neural Networks</title>
		<imprint>
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">RENET: A deep learning approach for extracting gene-disease associations from literature</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruibang</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">M</forename><surname>Henry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hing-Fung</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tak Wah</forename><surname>Ting</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lam</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-17083-7_17</idno>
	</analytic>
	<monogr>
		<title level="m">Research in Computational Molecular Biology -23rd Annual International Conference</title>
		<meeting><address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-05-05" />
		</imprint>
	</monogr>
	<note>Proceedings</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Revealing the myth of higher-order inference in coreference resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liyan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jinho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Document-level relation extraction with reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kehai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiejun</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">DocRED: A large-scale document-level relation extraction dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deming</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenghao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lixin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Coreferential Reasoning Learning for Language Representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deming</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaju</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenghao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>EMNLP</publisher>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Improved neural relation detection for knowledge base question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mo</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenpeng</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kazi Saidul Hasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Cicero Dos Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Sire: Separate intra-and inter-sentential reasoning for document-level relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuting</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baobao</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Joint Conference of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Double graph based reasoning for documentlevel relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Runxin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baobao</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>EMNLP</publisher>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Document-level relation extraction as semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ningyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shumin</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuanqi</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mosha</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luo</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huajun</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.24963/ijcai.2021/551</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence</title>
		<meeting>the Thirtieth International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">21</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Relation adversarial network for low resource knowledge graph completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ningyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shumin</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanlin</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaoyan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huajun</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The Web Conference 2020</title>
		<meeting>The Web Conference 2020</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Attentionbased capsule networks with dynamic routing for relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ningyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shumin</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanlin</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huajun</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Long-tail relation extraction via knowledge graph embeddings and graph convolution networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ningyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shumin</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanlin</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanying</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huajun</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Document-level relation extraction with dual-tier heterogeneous graph</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaobo</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tingwen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengzhu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Yubin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Computational Linguistics</title>
		<meeting>the 28th International Conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Document-level relation extraction with adaptive thresholding and localized context pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenxuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengyu</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Ground Truth Evidence Sentence(s)</title>
	</analytic>
	<monogr>
		<title level="m">Error Type 1: Labeling Mistakes Ground Truth Relation: Country (</title>
		<meeting><address><addrLine>Albany County, New York</addrLine></address></meeting>
		<imprint/>
	</monogr>
	<note>1, 4, 5, 7] Extracted Evidence Sentence(s): [5, 7] Document: [1] Westmere is a hamlet in the town of Guilderland. It is a suburb of the neighboring city of Albany</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Crossgates Mall, the Capital District&apos;s largest shopping mall, is in Westmere&apos;s northeastern corner. Final Prediction: NA Prediction on Orig. Doc: NA Prediction on Extracted Evidences: NA Error Type 2: Fail in Commonsense Reasoning Ground Truth Relation: Located in Ground Truth Evidence Sentence(s): [1, 5] Extracted Evidence Sentence(s): [1, 5] Document: [1] Oakland / Troy Airport is a county-owned public-use airport located east of the central business district of Troy</title>
	</analytic>
	<monogr>
		<title level="m">Western Avenue) bisects the community and is the major thoroughfare and main street</title>
		<editor>U.S. Route 20</editor>
		<meeting><address><addrLine>Oakland County, Michigan, United States</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>] It is included in the Federal Aviation Administration (FAA) National Plan of Integrated Airport Systems for. in which it is categorized as a regional reliever airport facility. ... [5] It is located between Maple Road and 14 Mile Road and Coolidge Highway and Crooks Road. [6] ..</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<title level="m">Final Prediction: NA () Prediction on Orig. Doc: NA (</title>
		<imprint/>
	</monogr>
	<note>Prediction on Extracted Evidences: NA (</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Pope Benedict XVI removed Maciel from active ministry based on the results of an investigation that he had started while head of the Congregation for the Doctrine of the Faith, before his election as Pope in</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcial</forename><surname>Maciel Degollado</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Final Prediction: NA () Prediction on Orig. Doc: NA (</title>
		<imprint>
			<date type="published" when="1920-03-10" />
		</imprint>
	</monogr>
	<note>Throughout most of his career, he was respected within the church as &quot;the greatest fundraiser of the modern Roman Catholic church&quot; and as a prolific recruiter of new seminarians. Prediction on Extracted Evidences: Religion Error Type 3: Fail in Coreferential Reasoning Ground Truth Relation: NA Ground Truth Evidence Sentence(s): [] Extracted Evidence Sentence(s): [1</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">) was the daughter of Italian actors performing in France and lover of the famous womanizer Giacomo Casanova. [2] She was ten years old when she first met him; she happened to be the daughter of Silvia Balletti, an actress of the Com?die Italienne company and younger sister of Casanova&apos;s closest friend</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Document</surname></persName>
		</author>
		<editor>1] Manon Balletti</editor>
		<imprint>
			<biblScope unit="page" from="1740" to="1776" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Prediction on Orig</title>
		<imprint/>
	</monogr>
	<note>Prediction on Extracted Evidences: Child (</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Error Type 4: Fail in Multi-hop Reasoning Ground Truth Relation</title>
		<imprint/>
	</monogr>
	<note>Educated at Ground Truth Evidence Sentence(s): [4] Extracted Evidence Sentence(s): [4</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">He was a winner of the Walter Naumburg Competition while a student at the Curtis Institute of Music</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">.</forename><forename type="middle">.</forename><surname>Orlando Cole</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>He has had a distinguished career as a soloist, chamber musician, principal cellist and teacher</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<title level="m">Final Prediction: NA () Prediction on Orig. Doc: NA (</title>
		<imprint/>
	</monogr>
	<note>Prediction on Extracted Evidences: NA (</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Document: [1] A Route Army was a type of military organization during the Chinese Republic, and usually exercised command over two or more corps or a large number of divisions or independent brigades. [2] It was a common formation in China prior to the Second Sino-Japanese War but was discarded as a formation type by the National Revolutionary Army after 1938 (other than the 8th Route Army), in favor of the Group Army</title>
	</analytic>
	<monogr>
		<title level="m">Error Type 5: Fail in Surface-name Reasoning Ground Truth Relation: Country Ground Truth Evidence Sentence(s)</title>
		<imprint/>
	</monogr>
	<note>Final Prediction: NA (. Prediction on Orig. Doc: NA (. Prediction on Extracted Evidences: NA (</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
