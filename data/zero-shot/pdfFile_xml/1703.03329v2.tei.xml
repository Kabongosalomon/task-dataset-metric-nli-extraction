<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">UntrimmedNets for Weakly Supervised Action Recognition and Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Computer Vision Laboratory</orgName>
								<orgName type="institution">ETH Zurich</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Information Engineering</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
								<address>
									<settlement>Hong Kong</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Information Engineering</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
								<address>
									<settlement>Hong Kong</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Computer Vision Laboratory</orgName>
								<orgName type="institution">ETH Zurich</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">UntrimmedNets for Weakly Supervised Action Recognition and Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T21:23+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Current action recognition methods heavily rely on trimmed videos for model training. However, it is expensive and time-consuming to acquire a large-scale trimmed video dataset. This paper presents a new weakly supervised architecture, called UntrimmedNet, which is able to directly learn action recognition models from untrimmed videos without the requirement of temporal annotations of action instances. Our UntrimmedNet couples two important components, the classification module and the selection module, to learn the action models and reason about the temporal duration of action instances, respectively. These two components are implemented with feed-forward networks, and UntrimmedNet is therefore an end-to-end trainable architecture. We exploit the learned models for action recognition (WSR) and detection (WSD) on the untrimmed video datasets of THUMOS14 and ActivityNet. Although our UntrimmedNet only employs weak supervision, our method achieves performance superior or comparable to that of those strongly supervised approaches on these two datasets. 1 1 The code and models are available at https://github.com/ wanglimin/UntrimmedNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Training Testing</head><p>UntrimmedNet Action Model</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Action recognition in videos has attracted extensive research attention in the past few years, and much progress has been made in computer vision community, on both aspects of hand-crafted representations <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b47">48]</ref> and deeply-learned representations <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b49">50]</ref>. In general, action recognition is usually cast as a classification problem, where each action instance is manually trimmed from a long video sequence during the training phase, and the learned action model is exploited for action recognition in trimmed clips (e.g., HMDB51 <ref type="bibr" target="#b24">[25]</ref> and UCF101 <ref type="bibr" target="#b40">[41]</ref>) or untrimmed videos (e.g., THUMOS14 <ref type="bibr" target="#b21">[22]</ref> and ActivityNet <ref type="bibr" target="#b15">[16]</ref>). Although these precise temporal annotations could relieve the difficulty of learning action models, it may be difficult to adapt to large-scale action recognition in more realistic and <ref type="bibr">Figure 1</ref>. Weakly supervised action recognition and detection: during training phase, we simply have untrimmed videos without temporal annotation and we train action models from these untrimmed videos directly; during test phase, the learned action models could be applied to action recognition (WSR) and detection (WSD) in untrimmed videos. challenging scenario due to several reasons. First, annotating temporal duration for each action instance is expensive and time-consuming. Meanwhile, huge numbers of videos on Youtube website are temporally untrimmed by nature, and trimming videos in such scale would be impractical. More importantly, unlike object boundary, there might even be no sensible definition about the exact temporal extent of actions <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b37">38]</ref>. Thus, these temporal annotations may be subjective and not consistent across different persons.</p><p>To overcome the above limitations of using trimmed videos for training, we introduce a more efficient setting of directly learning action recognition models from untrimmed videos, as shown in <ref type="figure">Figure 1</ref>. In this new setting, only the video-level action label is available during training, and the goal is to learn the models from untrimmed videos, which could be applied to new videos to perform action recognition or detection. As we do not have precise temporal annotations of action instances in training, we call this new problem as weakly supervised action recognition (WSR) and detection (WSD). Without the requirement of exact temporal annotations of action instances, the setup of WSR and WSD would greatly reduce the human efforts in building largescale datasets. However, this weakly supervised setting also poses new challenges in that our learning algorithm needs to not only learn the visual patterns for each action class, but also automatically reason the temporal locations of possible action instances. Therefore, to deal with the problems of WSR and WSD, the designed method should consider these two aspects at the same time.</p><p>In this work, we address the challenges of the WSR and WSD problems by proposing a new end-to-end architecture, called UntrimmedNet. Without temporal annotations of action instances, our UntrimmedNet directly takes an untrimmed video as input and simply exploits its videolevel label to learn the network weights. Considering the requirements mentioned above, in a nutshell, our Untrimmed-Net is mainly composed of two components, namely a classification module and a selection module, which handle the problems of learning action models and detecting action instances, respectively. The outputs of the classification and selection modules are fused to yield the prediction results of untrimmed videos, which can be exploited to tune the UntrimmedNet parameters in an end-to-end manner.</p><p>Specifically, our UntrimmedNet starts with generating clip proposals, which may contain action instances, by using uniform or shot based sampling. Then, these clip proposals are fed into UntrimmedNet for feature extraction. Based on these clip-level representations, the classification module aims to predict the classification scores for each clip proposal, while the selection module tries to select or rank those clip proposals. In practice, the design of classification module is based on a standard Softmax classifier and the selection module is implemented with two alternative mechanisms: hard selection and soft selection. For hard selection, a top-k pooling method is utilized to determine the most k discriminative clips, and for soft selection, an attention weight is learned to rank the importance of different clips. Finally, the results of classification and selection modules are fused with an weighted summation multiplication to produce the untrimmed video-level prediction. With this video-level prediction and the global video label, we are able to jointly optimize the components of classification modules, selection modules, and feature extraction networks using the standard back propagation algorithm.</p><p>We perform experiments on two challenging untrimmed video datasets, namely THUMOS14 <ref type="bibr" target="#b21">[22]</ref> and Acitivi-tyNet <ref type="bibr" target="#b15">[16]</ref>, to examine the UntrimmedNet on the tasks of weakly supervised action recognition (WSR) and detection (WSD). Although our UntrimmedNet does not employ the temporal annotations of action instances, it obtains superior performance for action recognition and comparable performance for action detection, when compared with the stateof-the-art methods that use strong supervision for training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Deep learning for action recognition. Since the breakthrough <ref type="bibr" target="#b23">[24]</ref> in image classification with Convolutional Neural Networks (CNNs) <ref type="bibr" target="#b28">[29]</ref> at ILSVRC 2012 <ref type="bibr" target="#b35">[36]</ref>, several works have been trying to design effective deep network architectures for action recognition in videos <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b46">47]</ref>. Karpathy et al. <ref type="bibr" target="#b22">[23]</ref> first tested deep networks on a large-scale dataset (Sports-1M) and achieved lower performance than traditional features <ref type="bibr" target="#b44">[45]</ref>. Simonyan et al. <ref type="bibr" target="#b39">[40]</ref> designed two stream CNNs containing spatial and temporal nets by explicitly exploiting pre-trained models and optical flow calculation. Tran et al. <ref type="bibr" target="#b41">[42]</ref> investigated 3D CNNs <ref type="bibr" target="#b19">[20]</ref> on the realistic and large-scale video datasets. Meanwhile, several works <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b49">50]</ref> tried to model long-term temporal information for action understanding. Ng et al. <ref type="bibr" target="#b31">[32]</ref> and Donahue et al. <ref type="bibr" target="#b6">[7]</ref> utilized recurrent neural networks (LSTM) to capture the long range dynamics for action recognition. Wang et al. <ref type="bibr" target="#b49">[50]</ref> designed a sparse sampling strategy to model the entire video information with average aggregation. In addition, several deep learning methods have been proposed for action proposal generation and detection <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b38">39]</ref>. Our UntrimmedNets differ to those deep networks in that the UntrimmedNets take the untrimmed videos as inputs and only require weak supervision to guide model training, while those previous architectures all uses the trimmed clips for training.</p><p>Weakly supervised learning in videos. Weakly supervised learning was extensively studied in object recognition and detection <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b33">34]</ref>, and there were several works adapting this method to learn action models from videos <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13]</ref>. The first type of weak supervision is movie script, which provides uncertain temporal annotations of action instances. For example, Laptev et al. <ref type="bibr" target="#b27">[28]</ref> proposed to learn action models from movie scripts for action recognition, and Duchennel et al. <ref type="bibr" target="#b7">[8]</ref> tried to localize action instances in movies with the help of scripts. Compared with our work, movie script supervision show two differences: (1) movie scripts are usually aligned with frames and so they can provide approximate temporal annotations of instance, while our weak supervision does not provide any temporal information about action instances, (2) movie script supervision only applies to movie videos while our method applies to all kinds of videos. The second type of weak supervision is a ordered list of action classes occurring in the videos. For instance, Bojanowski et al. <ref type="bibr" target="#b2">[3]</ref> proposed a discriminative clustering method for weakly supervised action labeling, and Huang et al. <ref type="bibr" target="#b16">[17]</ref> adapted the framework of Connectionist Temporal Classification <ref type="bibr" target="#b14">[15]</ref> from speech recognition to weakly supervised action labeling. Our UntrimmedNet differs from them in that our weak supervision contains no any order information on the containing action instances. For each proposal <ref type="figure">Figure 2</ref>. Pipeline of learning from untrimmed videos: our UntrimmedNets start with clip proposal generation, where we sample a set of short clips from the continuous untrimmed videos. Then, these clip proposals are separately fed into pre-trained networks for feature extraction. After this, a classification module is designed to perform action recognition for each clip proposal independently, and a selection module is proposed to detect or rank important clip proposals. Finally, the outputs of classification module and selection module are combined to yield the video-level prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Learning from Untrimmed Videos</head><p>In this section we introduce the pipeline of learning from untrimmed videos. First, we describe the methods of generating clip proposals for UntrimmedNets. Second, we give a detailed description on the architecture design of UntrimmedNet. Finally, we present the learning algorithm to tune the parameters of UntrimmedNet in an end-to-end manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Clip sampling</head><p>An action instance usually describes the continuous and coherent motion pattern with a specific intention, which may last for a few seconds and contain no shot changes. However, an untrimmed video often exhibits extremely complex motion dynamics, action instances may only occupy small portions of it. Therefore, our UntrimmedNet starts with generating short clips from the untrimmed videos, which could serve as action proposals for UntrimmedNet training.</p><p>Formally, given an untrimmed video V with the duration of T frames, our method generates a set of clip pro-</p><formula xml:id="formula_0">posals C = {c i } N i=1 ,</formula><p>where N is the number of proposals and c i = (b i , e i ) denote the beginning and ending location of the i th proposal c i . We design two simple yet effective methods to generate proposals: uniform sampling and shotbased sampling.</p><p>Uniform sampling. Under the assumption that an action instance may have a relatively short duration, we propose to divide the long video into N clips with equal duration, i.e., b i = i?1 N T + 1 and e i = i N T . This sampling method ignores the continuous and consistent properties of action instances and is prone to generating imprecise proposals.</p><p>Shot-based sampling. It is expected each action in-stance focuses on the consistent motion within a single shot. We present a sampling method based on shot change detection. Specifically, we extract the HOG features for each frame and calculate the HOG feature difference between adjacent frames. Then, we use the absolute value of this difference to measure the change of visual content and if it is larger than a threshold, a shot change would be detected. After this, in each shot, we propose to sample shot clips of fixed duration of K frames in a sequential manner (K set to 300 in practice), which helps to break down shots with very long durations. Suppose we have a shot denoted by</p><formula xml:id="formula_1">s i = (s b i , s e i ), where (s b i , s e i )</formula><p>represents the beginning and ending locations of this shot, we produce proposals from this shot as C(</p><formula xml:id="formula_2">s i ) = {(s b i +(i?1)?K, s b i +i?K)} i:s b i +i?K&lt;s e i .</formula><p>Finally, we merge all these clip proposals from different shots for UntrimmedNet training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">UntrimmedNets</head><p>As shown in <ref type="figure">Figure 2</ref>, the architecture of UntrimmedNet is composed of a feature extraction module, a classification module, and a selection module. These different components are all designed to be differentiable and render the UntrimmedNet to be trainable in an end-to-end manner.</p><p>Feature extraction module. After proposal generation, these shot clips are fed into deep networks for feature extraction. These feature representations are utilized to describe the clip visual content and passed to the next layers for action recognition. Formally, given a video V with a set of clip proposals C = {c i } N i=1 , we extract the representation as ?(V ; c) ? R D for each clip proposal c. Our UntrimmedNet is a general framework for weakly supervised action recognition and detection, and does not depend on the choice of feature extraction network. In the experi-ments, we try out two architectures: Two-Stream CNN <ref type="bibr" target="#b39">[40]</ref> with deeper architecture <ref type="bibr" target="#b17">[18]</ref> and Temporal Segment Network <ref type="bibr" target="#b49">[50]</ref> with the same architecture. More details will be described in Section 5.</p><p>Classification module. In the classification module, we aim to classify each clip proposal c into the predefined action categories based on the extracted features ?(c). Suppose we have C action classes, we learn a linear map-</p><formula xml:id="formula_3">ping W c ? R C?D to transform the feature representa- tion ?(c) into a C-dimensional score vector x c (c), i.e., x c (c) = W c ?(c),</formula><p>where C is the number of action categories and W c are the model parameters. This score vector can be also passed through a softmax layer as follows:</p><formula xml:id="formula_4">x c i (c) = exp(x c i (c)) C k=1 exp(x c k (c)) ,<label>(1)</label></formula><p>where x c i (c) denotes the i th dimension of x c (c). For clarity, we use the notation x c (c) to denote the original classification score of clip proposal c andx c (c) to represent the softmax classification score. There is a slight difference between those two types of classification scores. The original classification scorex c (c) encodes the raw class activation and its response is able to reflect the degree of containing a specific action class. In the case of containing no action instance, its value could be very small for all classes. However, the softmax classification scorex c (c) undergoes the normalization operation, turning its sum into 1. If there is an action instance in this clip, this softmax score could encode information of action class distribution. But for the case of background clips, this normalization operation may amplify noisy activations and its response may not encode the visual information correctly.</p><p>Selection module. The selection module aims to select those clip proposals of most probably containing action instances. Here we design two kinds of selection mechanisms for this goal: hard selection based on the principle of multiple instance learning (MIL) <ref type="bibr" target="#b5">[6]</ref> and soft selection based on the attention-based modeling <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b52">53]</ref>. As we shall see in experiments, those two selection method can both well handle the problem of weakly supervised learning.</p><p>In the hard selection method, we try to identify a subset of k clip proposals (instances) for each action class. Inspired by the idea of multiple instance learning, we choose top k instances with the highest classification scores and then average among these selected instances. It should be noted that here we use the original classification score as its value is able to correctly reflect the likelihood of containing certain action instances. Formally, let us use x s i (c j ) = ?(j ? S k i ) to encode the selection choice for class i and instance c j , where S k i is the set of indices of clip proposals with the highest k classification scores for class i.</p><p>In the soft selection method, we want to combine the classification scores of all clip proposals and learn an im-portance weight to rank different clip proposals. Intuitively, these clip proposals are not all relevant to the action class and we could learn an attention weight to highlight the discriminative clip proposals and suppress the background clip proposals. Formally, for each clip proposal, we learn this attention weight based on the feature representation ?(c) with a linear transformation, i.e., x s (c) = w sT ?(c), where w s ? R D is the model parameter. Then the attention weights of different clip proposals are passed through a softmax layer and compared with each other as follows:</p><formula xml:id="formula_5">x s (c i ) = exp(x s (c i )) N n=1 exp(x s (c n )) ,<label>(2)</label></formula><p>where x s (c) denotes the original selection score of clip proposal c andx s (c) is the softmax selection score. It should be noted that, in the classification module, the softmax operation (Eq. <ref type="formula" target="#formula_4">(1)</ref>) is applied to the classification scores of different action classes, for each clip proposal separately, while in the selection module, this operation (Eq. <ref type="formula" target="#formula_5">(2)</ref>) is performed across different clip proposals. In spite of sharing a similar mathematical formulation, these two softmax layers are designed for the purpose of classification and selection, respectively. Video prediction. Finally, we are able to produce the prediction scorex p (V ) for the untrimmed video V by combining the classification and selection scores. Specifically, for hard selection, we simply average the selected top-k instances as follows:</p><formula xml:id="formula_6">x p i (V ) = N n=1 x s i (c n )x c i (c n ), x p i (V ) = exp(x r i (V )) C k=1 exp(x r k (V )) ,<label>(3)</label></formula><p>where x s (c n ) and x c (c n ) are the hard selection indicator and classification score for clip proposal c n , respectively. As our hard selection module is based on the original classification score, we need to perform a softmax operation to normalize the aggregated video-level score.</p><p>In the case of soft selection, as we have learned an attention weight to rank those clip proposals, we simply employ a weighted summation to combine the scores of the classification and selection modules, as follows:</p><formula xml:id="formula_7">x p (V ) = N n=1x s (c n )x c (c n ).<label>(4)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Training</head><p>After the introduction of UntrimmedNet architecture in the previous subsection, we turn to discuss how to optimize the model parameters. The components of feature extraction, classification module, and selection module are implemented with feed-forward neural networks that are all differentiable with model parameters. Therefore, following training methods of strongly supervised architecture (e.g., Two-Stream CNNs), we employ the standard back propagation method with cross-entropy loss:</p><formula xml:id="formula_8">(w) = M i=1 C k=1 y ik logx p k (V i ),<label>(5)</label></formula><p>where y ik is set to 1 if video V i contains action instances of k th category, and to 0 otherwise, M is the number of training videos. A weight decay rate of 0.0005 is enforced during the training. In the case of video containing action instances from multiple classes, we first normalize the label vector y with its 1 -norm <ref type="bibr" target="#b50">[51]</ref>, i.e.? = y/ y 1 , and then use this normalized label vector? to calculate cross-entropy loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Action Recognition and Detection</head><p>Having introduced UntrimmedNet for directly learning from untrimmed videos, we now turn to describing how to exploit these learned models for action recognition and detection in untrimmed videos.</p><p>Action recognition. As our UntrimmedNets are built on the two stream CNNs <ref type="bibr" target="#b39">[40]</ref> or temporal segment networks <ref type="bibr" target="#b49">[50]</ref>, the learned models can be viewed as snippetlevel classifiers. Following the recognition pipeline of previous methods <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b51">52]</ref>, we perform snippet-wise evaluation for action recognition in untrimmed videos. In practice, we sample a single frame (or 5 frame stacking of optical flow) every 30 frames. The recognition scores of sampled frames are aggregated with top-k pooling (k set to <ref type="bibr" target="#b19">20)</ref> or weighted sum to yield the final video-level prediction.</p><p>Action detection. Our UntrimmedNet with soft selection module not only delivers a recognition score, but also outputs an attention weight for each snippet. Naturally, this attention weight could be exploited for action detection (temporal localization) in untrimmed videos. For more precise localization, we perform test every 15 frames and keep the prediction score and attention weight for each frame. Based on the attention weight, we remove background by thresholding (set to 0.0001) on it. Finally, after removing background, we produce the final detection results by thresholding (set to 0.5) on the classification score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>In this section we describe the experimental results of our method. First, we introduce the evaluation datasets and the implementation details of our UntrimmedNets. Then, we perform exploration studies to determine important configurations of our approach. Finally, we examine our method on weakly supervised action recognition (WSR) and action detection (WSD), and compare with the stateof-the-art methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Datasets</head><p>We evaluate our UntrimmedNet on two large datasets, namely THUMOS14 <ref type="bibr" target="#b21">[22]</ref> and ActivityNet <ref type="bibr" target="#b15">[16]</ref>. These two datasets are suitable to evaluate our method as they provide the original untrimmed videos. It should be noted that these two datasets also have temporal annotations of action instances for training data, but we do not use these temporal annotations when training our UntrimmedNets.</p><p>The THUMOS14 dataset has 101 classes for action recognition and 20 classes for action detection. It is composed of four parts: training data, validation data, testing data, and background data. To verify the effectiveness of our UntrimmedNet on learning from untrimmed videos, we mainly use the validation data (1,010 videos) to train our models and the test data (1,574 videos) to evaluate their performance. The ActivityNet dataset is a recently introduced benchmark for action recognition and detection in untrimmed videos. We use the ActivityNet release 1.2 for our experiments. In this release, the ActivityNet consists of 4,819 videos for training, 2,383 videos for validation, and 2,480 videos for testing, of 100 activity classes. We perform two kinds of experiments: 1) learning UntrimmedNets on the training data and testing it on the validation data, 2) learning UntrimmedNets on the combination of training and validation data and submitting testing results to the evaluation server. The evaluation metric is based on mean average precision (mAP) for action recognition on these two datasets. For action detection, we follow the standard evaluation metric by reporting mAP values for different intersection over union (IoU) values on the dataset of THUMOS14.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Implementation details</head><p>We use the video extension version <ref type="bibr" target="#b49">[50]</ref> of the Caffe toolbox <ref type="bibr" target="#b20">[21]</ref> to implement the UntrimmedNet. We choose two successful deep architectures for feature extraction in our UntrimmedNet, namely Two Stream CNNs <ref type="bibr" target="#b39">[40]</ref> and Temporal Segment Network <ref type="bibr" target="#b49">[50]</ref>. The two networks are both based on two stream inputs (RGB and Optical Flow) and Temporal Segment Network is equipped with segmental modeling (3 segments) to capture long-range temporal information. Following the Temporal Segment Network, the input to the spatial stream is 1 RGB frame and the temporal stream takes 5-frame stacks of TVL1 optical flow.  We choose the Inception architecture <ref type="bibr" target="#b17">[18]</ref> with Batch Normalization for the UntrimmedNet design and we initialize UntrimmedNet parameters of both streams with pre-trained models from ImageNet <ref type="bibr" target="#b4">[5]</ref> with the method introduced in <ref type="bibr" target="#b49">[50]</ref>. The UntrimmedNet parameters are optimized with the mini-batch stochastic gradient algorithm, where the batch size is set to 256 and the momentum to 0.9. The initial learning rate is set to 0.001 for the spatial stream and decreases every 4,000 iterations by a factor of 10, and the whole training stops at 10, 000 iterations. For the temporal stream, we set the initial learning rate to 0.005, which is decreased every 6,000 iterations by a factor of 10, and it stops training at 18, 000 iterations. As the training set size of THUMOS14 and ActivityNet is relatively small, we use high dropout ratios (0.8 for the spatial stream and 0.7 for the temporal stream) and common data augmentation techniques including cropping augmentation and scale jittering.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Exploration studies</head><p>In this subsection, we focus on the exploration studies to determine the important setups of UntrimmedNet. Specifically, we perform investigation on the THUMOS14 dataset, where we train the UntrimmedNet on the validation data and conduct evaluation on the testing data. In all these experiments, we report performance of both hard selection and soft selection Clip sampling. We design two simple sampling method in Section 3.1. We start our experiments by comparing these two proposal sampling methods. In this study, we use the two stream CNNs for feature extraction in the Untrimmed-Net and seven clips are randomly sampled from each video. The numerical results are summarized in <ref type="figure" target="#fig_1">Figure 3</ref>. From the results, we see that both sampling methods can give good performance for UntrimmedNet training and the shot based sampling is able to yield better performance (71.6% vs. 70.2% for the soft selection module). We ascribe the better performance of shot based sampling to the fact that shot detection is able to automatically detect the action boundary and is more natural for video temporal segmentation than uniform segmentation. In the remaining experiments, we choose the shot based proposal sampling by default.   Feature extraction. An important component in our UntrimmedNet is feature extraction as the classification and selection modules both depend on feature representations. In this experiment, we choose two networks, namely two stream CNNs <ref type="bibr" target="#b39">[40]</ref> and temporal segment networks <ref type="bibr" target="#b49">[50]</ref>, and sample seven clip proposals per video during the training phase. The experimental results are reported in <ref type="figure" target="#fig_3">Figure 4</ref>, and we observe that the temporal segment networks consistently outperform the original two stream CNNs for both hard and soft selection modules, due to their long-term modeling over the entire clip (74.2% vs. 71.6% for the soft selection module). Therefore, we choose the temporal segment networks for feature extraction in the remaining experiments.</p><p>Number of proposals. Another important parameter in the design of UntrimmedNet is the number of clip proposals sampled from each video. As the GPU memory is limited, we need to strike a balance between the number of sampled clip proposals per video and the number of videos per batch. According to our experiment, on average, we generate 40 clip proposals for each video on the THUMOS14 dataset and 20 clip proposals for each video on the Activ-ityNet dataset. In our experiment, we set the number of sampled clip proposals per video to <ref type="bibr">5, 7, 9.</ref> In the hard selection module, we set the parameter k in top-k pooling as <ref type="bibr">N 2</ref> , where N is the number of sampled clip proposals. The experimental results are summarized in <ref type="figure" target="#fig_4">Figure 5</ref> and we see that for separate streams, the performance slightly varies when the number of sampled proposals changes, but the performance of two stream networks is quite stable for the hard selection module. For the soft selection module, the values 7 and 9 show a small advantage over 5 and there-Method THUMOS14 ActivityNet (a) ActivityNet (b) TSN (3 seg) <ref type="bibr" target="#b49">[50]</ref> 67 THUMOS14 ActivityNet iDT+FV <ref type="bibr" target="#b44">[45]</ref> 63.1% iDT+FV <ref type="bibr" target="#b44">[45]</ref> 66.5% * Two Stream <ref type="bibr" target="#b39">[40]</ref> 66.1% Two Stream <ref type="bibr" target="#b39">[40]</ref> 71.9% * EMV+RGB <ref type="bibr" target="#b55">[56]</ref> 61.5% C3D <ref type="bibr" target="#b41">[42]</ref> 74.1% * Objects+Motion <ref type="bibr" target="#b18">[19]</ref> 71.6% Depth2Action <ref type="bibr" target="#b56">[57]</ref> 78.1% * TSN (3 seg) <ref type="bibr" target="#b49">[50]</ref> 78.5% TSN (3 seg) <ref type="bibr" target="#b49">[50]</ref> 88.8% * UntrimmedNet (hard) 81.2% UntrimmedNet (hard) 91.3% UntrimmedNet (soft) 82.2% UntrimmedNet (soft) 90.9% fore, to keep a balance between accuracy and efficiency, we fix the number of sampled proposal to 7 in the remaining experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Evaluation on WSR</head><p>After the exploration study on different configurations, we turn to the investigation of UntrimmedNet on the problem of weakly supervised action recognition (WSR) on the datasets of THUMOS14 and ActivityNet in this subsection.</p><p>Effectiveness of selection module. We first examine the effectiveness of leveraging selection modules in UntrimmedNets for learning from untrimmed videos. In order to study the setting of learning from untrimmed videos, we use the validation data for training on the THUMOS14 dataset, and use the untrimmed videos without temporal annotations for training on the ActivityNet dataset.</p><p>We choose two baseline methods to compare: the standard temporal segment network with the average aggregation function (TSN), which is the state-of-the-art action recognition method, and TSN with more segments, which uses more segments during training. The numerical results are summarized in <ref type="table">Table 1</ref>. From these results, we first see that our UntrimmedNet equipped with a hard or soft selection module outperforms the original TSN frameworks on both datasets. Furthermore, for the sake of a fair comparison with our UntrimmedNet, we increase the segment number of TSN to 21, which is equal to the number of seg-  ments in our UntrimmedNet (3?7), and we see that increasing the segment numbers indeed contributes to improving the recognition performance. But the performance of TSN with 21 segments is still below that of our UntrimmedNet, which indicates that explicitly designing selection modules for learning from untrimmed videos is effective.</p><p>Comparison with the state of of the art. After a separate study on the effectiveness of selection modules on WSR, we now compare the UntrimmedNet with other stateof-the-art methods on those two challenging datasets. To get a fair comparison with other methods, we use the training and validation videos to learn UntrimmedNets on the THUMOS14 dataset. As its training data (UCF101) is already trimmed, we simply use the whole video clips as proposals to train our UntrimmedNet. On the dataset of Activ-ityNet, we combine the training and validation videos to train our models and report the performance on the testing videos. It is worth noting that other methods all use strong supervision (i.e. temporal annotation and video labels), while our UntrimmedNet only uses weak supervision (i.e. only video labels)</p><p>We compare with several previous successful action recognition methods, which previously achieved the stateof-the-art performance on these two datasets, including improved trajectories (iDT+FV) <ref type="bibr" target="#b44">[45]</ref>, two stream CNNs <ref type="bibr" target="#b39">[40]</ref>, 3D convolutional networks (C3D) <ref type="bibr" target="#b41">[42]</ref>, temporal segment networks (TSN) <ref type="bibr" target="#b49">[50]</ref>, Object+Motion <ref type="bibr" target="#b18">[19]</ref>, and Depth2Action <ref type="bibr" target="#b56">[57]</ref>. The numerical results are summarized in <ref type="table" target="#tab_2">Table 2</ref>. We see that our UntrimmedNets outperform all these previous methods. Our best performance is 3.7% above that of other methods on the THUMOS14 dataset and 2.5% on the ActivityNet dataset. This superior performance of UntrimmedNet justifies the importance of jointly learning classification and selection modules. Furthermore, we are only using weak supervision and have obtained better performance than those methods relying on strong supervision, which could be explained by the fact that our UntrimmedNet could well utilize useful context information in the whole untrimmed videos rather than only learning from trimmed activity clips. <ref type="figure">Figure 6</ref>. Visualization of attention weights on the test data of THUMOS14 and AcitivtyNet. The left four frames are with the highest attention weights and the right four frames are with the lowest attention weights. The above three videos are from THUMOS14 test data with action categories of Rafting, FrontCrawl, BandMarching, and the below three videos are from ActivityNet test data with action classes of TripleJump, ShovelingSnow and PlayingHarmonica.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">Evaluation on WSD</head><p>After evaluation on the problem of weakly supervised action recognition (WSR), we turn to the problem of weakly supervised action detection (WSD) in this subsection. Specifically, we explore the performance of our UntrimmedNet with soft selection module on this problem.</p><p>Qualitative results. We first visualize the some examples of learned attention weights on the test data of THU-MOS14 and ActivityNet. These examples are presented in <ref type="figure">Figure 6</ref>. In this illustration, each row describes one video, where the first 4 images show frames with highest attention weights while the last 4 images are frames with lowest weights. We see that our selection module is able to automatically highlight important frames and to avoid irrelevant frames corresponding to static background or nonaction poses.</p><p>Quantitative results. We also report the performance of action detection on the THUMOS14 dataset, based on the standard intersection over union (IoU) criteria <ref type="bibr" target="#b21">[22]</ref>. We simply try a simple detection strategy by thresholding on the attention weights and detection scores as described in Section 4, and aim to illustrate that the learned models with UntrimmedNets could also be applied to action detection. In the future, we may try more advanced detection methods and post-processing techniques. We compare our detection results with other state-of-the-art methods in <ref type="table" target="#tab_4">Table 3</ref>. We notice although our UntrimmedNets simply employ the weak supervision of video-level labels, we can still achieve comparable performance to that of strongly supervised methods, which demonstrates the effectiveness of UntrimmedNets on learning from untrimmed videos.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions</head><p>In this paper we have presented a novel architecture, called UntrimmedNet, for weakly supervised action recognition and detection, by directly learning action models from untrimmed videos. As demonstrated on two challenging datasets of untrimmed videos, our Untrimmed-Net achieves better or comparable performance for action recognition and detection, when compared with those strongly supervised methods. The superior performance of UntrimmedNet may be ascribed to its advantages of the joint design of classification and selection modules, and optimizing these model parameters in an end-to-end manner.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Comparison of different clip proposal sampling methods on the THUMOS14 dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Comparison of different architectures for feature extraction on the THUMOS14 dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Performance of UntrimmedNets with different numbers of clip proposal per video on the THUMOS14 dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Untrimmed Video Clip proposal 1 Clip proposal 2 ?? Clip proposal N Clip Sampling Feature Extraction</head><label></label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="2">Classification</cell><cell>Selection</cell></row><row><cell>Two stream CNN</cell><cell cols="2">For each proposal</cell><cell></cell><cell></cell><cell>Over proposals</cell></row><row><cell cols="2">Spatial Stream CNN Temporal Stream CNN Spatial Stream CNN Temporal Stream CNN Spatial Stream CNN Temporal Stream CNN Spatial Stream CNN Temporal Segment Networks</cell><cell>Segment Consensus Segment Consensus</cell><cell>FC</cell><cell>Softmax</cell><cell>H S</cell><cell>with Softmax Cross Entropy Loss Cross Entropy Loss</cell></row><row><cell cols="2">Temporal Stream CNN</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Comparison of our UntrimmedNet with other state-ofthe-art methods on the datasets of THUMOS14 and AcitivtyNet (v1.2) for action recognition. For ActivityNet, we train the models on train+val videos and evaluate on the test server.</figDesc><table /><note>* indicates using strong supervision for training.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>IoU (?)?= 0.5 ? = 0.4 ? = 0.3 ? = 0.2 ? = 0.1 Oneata et al.<ref type="bibr" target="#b32">[33]</ref> </figDesc><table><row><cell></cell><cell>14.4</cell><cell>20.8</cell><cell>27.0</cell><cell>33.6</cell><cell>36.6</cell></row><row><cell>Richard et al. [35]  *</cell><cell>15.2</cell><cell>23.2</cell><cell>30.0</cell><cell>35.7</cell><cell>39.7</cell></row><row><cell>Shou et al. [39]  *</cell><cell>19.0</cell><cell>28.7</cell><cell>36.3</cell><cell>43.5</cell><cell>47.7</cell></row><row><cell>Yeung et al. [54]  *</cell><cell>17.1</cell><cell>26.4</cell><cell>36.0</cell><cell>44.0</cell><cell>48.9</cell></row><row><cell>Yuan et al. [55]  *</cell><cell>18.8</cell><cell>26.1</cell><cell>33.6</cell><cell>42.6</cell><cell>51.4</cell></row><row><cell>UntrimmedNet (soft)</cell><cell>13.7</cell><cell>21.1</cell><cell>28.2</cell><cell>37.7</cell><cell>44.4</cell></row></table><note>*</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 .</head><label>3</label><figDesc>Comparison of our UntrimmedNet with other state-of-theart methods on the datasets of THUMOS14 for action detection.</figDesc><table /><note>* indicates using strong supervision for training.</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">Here, different from hard selection, we use the softmax classification score for each clip proposal, as this normalized score would make attention weight learning easier and more stable. Note that Eq. (4) forms a convex combination of probability vectors. Hence no further normalization is required.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>This work is partially supported by the ERC Advanced Grant VarCity, the Toyota Research Project TRACE-Zurich, the Big Data Collaboration Research grant from SenseTime Group (CUHK Agreement No. TS1610626), and Early Career Scheme (ECS) grant (No. 24204215).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Weakly supervised deep detection networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2846" to="2854" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Finding actors and actions in movies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">R</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2280" to="2287" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Weakly supervised action labeling in videos under ordering constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Lajugie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">R</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="628" to="643" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Weakly supervised object localization with multi-fold multiple instance learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">G</forename><surname>Cinbis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Verbeek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="189" to="203" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Ima-geNet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Solving the multiple instance problem with axis-parallel rectangles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">G</forename><surname>Dietterich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">H</forename><surname>Lathrop</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lozano-P?rez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artif. Intell</title>
		<imprint>
			<biblScope unit="volume">89</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="31" to="71" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Long-term recurrent convolutional networks for visual recognition and description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2625" to="2634" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Automatic annotation of human actions in video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Duchenne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">R</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1491" to="1498" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">WELDON: weakly supervised learning of deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Durand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4743" to="4752" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Daps: Deep action proposals for action understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">C</forename><surname>Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Niebles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="768" to="784" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Convolutional two-stream network fusion for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1933" to="1941" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Webly-supervised video recognition by mutually voting for relevant web images and web video frames</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="849" to="866" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">You lead, we exceed: Labor-free video concept learning by jointly exploiting web videos and images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="923" to="932" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Finding action tubes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="759" to="768" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fern?ndez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">J</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="369" to="376" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">ActivityNet: A large-scale video benchmark for human activity understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">C</forename><surname>Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="961" to="970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Connectionist temporal modeling for weakly supervised action labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="137" to="153" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">What do 15, 000 object categories tell us about classifying and localizing actions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Van Gemert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">G M</forename><surname>Snoek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="46" to="55" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">3D convolutional neural networks for human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="221" to="231" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Caffe: Convolutional architecture for fast feature embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno>abs/1408.5093</idno>
		<imprint>
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-G</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<title level="m">THUMOS challenge: Action recognition with a large number of classes</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Large-scale video classification with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1725" to="1732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">ImageNet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1106" to="1114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">HMDB: A large video database for human motion recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Garrote</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">A</forename><surname>Poggio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Serre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2556" to="2563" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Weakly supervised learning of actions from transcripts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<idno>abs/1610.02237</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">On space-time interest points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="issue">2-3</biblScope>
			<biblScope unit="page" from="107" to="123" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning realistic human actions from movies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Marszalek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rozenfeld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Gradientbased learning applied to document recognition. Proceedings of the IEEE</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning activity progression in LSTMs for activity detection and early detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sigal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sclaroff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1942" to="1950" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Recurrent models of visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2204" to="2212" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Beyond short snippets: Deep networks for video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hausknecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4694" to="4702" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">The lear submission at thumos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Oneata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Verbeek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">THUMOS Action Recognition challenge</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Is object localization for free? -weakly-supervised learning with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Oquab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="685" to="694" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Temporal action detection using a statistical language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3131" to="3140" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">ImageNet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Modeling the temporal extent of actions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="536" to="548" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Action snippets: How many frames does human action recognition require</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Temporal action localization in untrimmed videos via multi-stage CNNs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1049" to="1058" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="568" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">UCF101: A dataset of 101 human actions classes from videos in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<idno>abs/1212.0402</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3D convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">D</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4489" to="4497" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">APT: action localization proposals from dense trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Van Gemert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">G M</forename><surname>Snoek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="177" to="178" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Long-term temporal convolutions for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Varol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<idno>abs/1604.04494</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Action recognition with improved trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3551" to="3558" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Motionlets: Mid-level 3D parts for human motion recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2674" to="2681" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Action recognition with trajectory-pooled deep-convolutional descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4305" to="4314" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">MoFAP: A multi-level representation for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">119</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="254" to="271" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Actionness estimation using hybrid fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2708" to="2717" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Temporal segment networks: Towards good practices for deep action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="20" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">CNN: single-label to multi-label. CoRR, abs/1406</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">5726</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">CUHK &amp; ETHZ &amp; SIAT submission to ActivityNet challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Ac-tivityNet Large Scale Activity Recognition Challenge</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2048" to="2057" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Endto-end learning of action detection from frame glimpses in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2678" to="2687" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Temporal action localization with pyramid of score distribution features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Kassim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3093" to="3102" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Realtime action recognition with enhanced motion vector CNNs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2718" to="2726" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Depth2action: Exploring embedded depth for large-scale action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">D</forename><surname>Newsam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV Workshops</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="668" to="684" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
