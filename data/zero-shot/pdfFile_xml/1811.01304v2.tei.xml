<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ColNet: Embedding the Semantics of Web Tables for Column Type Prediction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaoyan</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Oxford</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ernesto</forename><surname>Jim?nez-Ruiz</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">The Alan Turing Institute</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Department of Informatics</orgName>
								<orgName type="institution">University of Oslo</orgName>
								<address>
									<country key="NO">Norway</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Horrocks</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Oxford</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">The Alan Turing Institute</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Sutton</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">The Alan Turing Institute</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">School of Informatics</orgName>
								<orgName type="institution">The University of Edinburgh</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">ColNet: Embedding the Semantics of Web Tables for Column Type Prediction</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T17:10+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Automatically annotating column types with knowledge base (KB) concepts is a critical task to gain a basic understanding of web tables. Current methods rely on either table metadata like column name or entity correspondences of cells in the KB, and may fail to deal with growing web tables with incomplete meta information. In this paper we propose a neural network based column type annotation framework named ColNet which is able to integrate KB reasoning and lookup with machine learning and can automatically train Convolutional Neural Networks for prediction. The prediction model not only considers the contextual semantics within a cell using word representation, but also embeds the semantics of a column by learning locality features from multiple cells. The method is evaluated with DBPedia and two different web table datasets, T2Dv2 from the general Web and Limaye from Wikipedia pages, and achieves higher performance than the state-of-the-art approaches.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Tables on the Web, which often contain highly valuable data, are growing at an extremely fast speed. Their power has been explored in various applications including web search <ref type="bibr" target="#b0">(Cafarella et al. 2008)</ref>, question answering <ref type="bibr" target="#b9">(Sun et al. 2016)</ref>, knowledge base (KB) construction ) and so on. For most applications, web table annotation which is to gain a basic understanding of the structure and meaning of the content is critical. This however is often difficult in practice due to metadata (e.g., table and column names) being missing, incomplete or ambiguous.</p><p>An entity column is a table column whose cells are text phrases, i.e., mentions of entities. Type annotation of an entity column 1 means matching the common type of its cells with widely recognized concepts such as semantic classes of a KB. For example, a column composed of "Mute swan", "Yellow-billed duck" and "Wandering albatross" is annotated with dbo:Species and dbo:Bird, two classes of DBpedia <ref type="bibr" target="#b0">(Auer et al. 2007</ref>). Column types not only enable understanding the meaning of cells but also form the base of other table annotation tasks such as property annotation <ref type="bibr" target="#b6">(Pham et al. 2016</ref>) and foreign key discovery <ref type="bibr" target="#b12">(Zhang et al. 2010</ref>).</p><p>Copyright ? 2019, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. <ref type="bibr">1</ref> Note that data type prediction is not considered in this work.</p><p>Table annotation tasks are often transformed into matchings between the table and a KB, such as cell to entity matching, column to class matching and column pair to property matching. Traditional methods jointly solve all the matching tasks with their correlations considered using graphical models <ref type="bibr" target="#b3">(Limaye, Sarawagi, and Chakrabarti 2010;</ref><ref type="bibr" target="#b6">Mulwad, Finin, and Joshi 2013;</ref><ref type="bibr" target="#b0">Bhagavatula, Noraset, and Downey 2015)</ref> or iterative procedures <ref type="bibr" target="#b8">(Ritze, Lehmberg, and Bizer 2015;</ref><ref type="bibr" target="#b13">Zhang 2014;</ref><ref type="bibr">Zhang 2017)</ref>. Considering the inter-matching correlation improves the disambiguation, but their metrics for computing the matchings (i) mostly adopt lexical comparisons which ignore the contextual semantics, and (ii) rely on metadata like column names and sometimes even external information like table description, both of which are often unavailable in real world applications.</p><p>Recent studies <ref type="bibr" target="#b1">(Efthymiou et al. 2017)</ref> and <ref type="bibr" target="#b3">(Luo et al. 2018</ref>) match column cells to KB entities with their contextual semantics considered using machine learning techniques like word embeddings and neural networks. With the matched entities, the column type can be further inferred using strategies like majority voting. However, such a cell to entity matching based column type annotation procedure assumes that the table cells have KB entity correspondences. Its performance will decrease when the column has a small number of cells, or there are missing or not accurate correspondences between the table cells and the KB entities, which we refer to as knowledge gap.</p><p>This study focuses on type annotation of entity columns, assuming table metadata like column names and table structures are unknown. We propose a neural network based framework named ColNet, as shown in <ref type="figure">Figure 1</ref>. It first embeds the overall semantics of columns into vector space and then predicts their types with a set of candidate KB classes, using machine learning techniques like Convolutional Neural Networks (CNNs) and an ensemble of results from lookup. To automatically train robust prediction models, we use the cells to retrieve the candidate KB classes, infer their entities to construct training samples, and deal with the challenge of sample shortage using transfer learning.</p><p>In summary, this study contributes a more accurate column type annotation framework by combining knowledge lookup and machine learning with the knowledge gap considered. As the framework does not assume any table metadata or table structure, it can be applied to not only web ta- <ref type="figure">Figure 1</ref>: Column Type Annotation with ColNet bles but also general tabular data. The study also provides a general approach that embeds the overall semantics of a column where the correlation between cells is incorporated. Our experiments with DBPedia and two different web table sets, T2Dv2 from the general Web  and Limaye from Wikipedia pages <ref type="bibr" target="#b3">(Limaye, Sarawagi, and Chakrabarti 2010)</ref> have shown that our method is effective and can outperform the state-of-the-art approaches.</p><p>Next section reviews the related work. Then we introduce the technical details of our approach. We finally present the evaluation, conclude the paper and discuss our future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Work</head><p>Annotating web tables with semantics from a KB has been studied for several years. It includes tasks of matching (i) table columns to KB classes, (ii) column pairs i.e., intercolumn relationships to KB properties, and (iii) column cells to KB entities. More specifically, task (i) is equivalent to matching tables to KB classes while task (iii) is equivalent to matching rows to KB entities when one of the columns serves as a primary key (PK) which is defined as a column that can uniquely identify (most of) the table rows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Collective Approaches</head><p>Collective approaches encompass multiple matching tasks and solve them together. We divide them into joint inference models and iterative approaches. <ref type="bibr" target="#b3">(Limaye, Sarawagi, and Chakrabarti 2010)</ref> represents different matchings with a probabilistic graphical model and searches for value assignments of the variables that maximize the joint probability. <ref type="bibr" target="#b6">(Mulwad, Finin, and Joshi 2013)</ref> extends this work with a more lightweight graphical model. TabEL <ref type="bibr" target="#b0">(Bhagavatula, Noraset, and Downey 2015)</ref> weakens the assumption that columns and column pairs have KB correspondences, but assigns higher likelihood to entity sets that tend to co-occur in Wikipedia documents. <ref type="bibr" target="#b11">(Venetis et al. 2011</ref>) proposes a maximum likelihood inference model that predicts the class of an column by maximizing the probability of all its cells. while <ref type="bibr" target="#b1">(Chu et al. 2015</ref>) adopts a scoring model. TableMiner+ <ref type="bibr" target="#b13">(Zhang 2014;</ref><ref type="bibr">Zhang 2017)</ref> and T2K Match <ref type="bibr" target="#b8">(Ritze, Lehmberg, and Bizer 2015)</ref> are two state-of-the-art iterative approaches. TableMiner adopts a bootstrapping pattern which first learns an initial interpretation with partial table data and then constrains the interpretation of the remaining data with the initial interpretation. T2K Match iteratively adjusts the weight of matchings until the overall similarity values converge. Early work  and ) adopt a straightforward process which first determines the class of a column by matching its cells to KB entities and then refines the matchings with the column class.</p><p>These methods can achieve good performance on some datasets by jointly or iteratively determining multiple matchings with the inter-matching correlations modeled, but their performance will decrease on tabular data with cells provided alone, as they utilize some table metadata like column names and sometimes even external table information like table caption to calculate some correspondences. Meanwhile, these methods assume that all the table cells have corresponding entities in the KB, without considering the knowledge gap between them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Column Type Annotation</head><p>Different from those collective work, some studies focus only on cell-to-entity matching <ref type="bibr" target="#b16">(Zwicklbauer, Seifert, and Granitzer 2016;</ref><ref type="bibr" target="#b1">Efthymiou et al. 2017;</ref><ref type="bibr" target="#b3">Luo et al. 2018)</ref>. The matched KB entities in turn can determine the column type with strategies like majority voting <ref type="bibr" target="#b15">(Zwicklbauer et al. 2013)</ref>. Such an approach can work with table contents alone, without relying on any metadata, but still ignoring the cases where a large part of cells have no entity correspondences.</p><p>A few studies take such knowledge gap cases into consideration. <ref type="bibr" target="#b6">(Pham et al. 2016</ref>) utilizes machine learning and handcrafted features to predict the similarity between a target column and a seeding column whose type has been annotated. It actually transforms the gap between KB and target columns to the gap between seeding columns and target columns, with additional cost to annotate seeding columns. (Quercini and Reynaud 2013) does not match cells to entities but directly predicts the type of each cell by feeding the web page queried by the cell into a machine learning classifier. The idea is close to ours, but our method (i) uses novel column semantic embedding and CNN-based locality feature learning for high accuracy, and (ii) automatically trains machine learning models with samples inferred from a KB.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Semantic Embedding</head><p>Most table annotation methods calculate the degree of matchings with text comparison metrics like TF-IDF, Jaccard and so on, without considering the contextual semantics. The cell-to-entity matching by (Efthymiou et al. 2017) embeds cells and entities into vectors using word representations to introduce contextual semantics in prediction. It explores intra-cell semantic embedding (i.e., representing cells into vector space), but ignores inter-cell correlations.</p><p>As the locality correlation of tabular data is not as obvious as images and text, there are few studies learning inter-cell semantics. <ref type="bibr" target="#b6">(Nishida et al. 2017)</ref> uses CNNs to learn locality features for table classification. The study presents that inter-cell correlations do exist and learning high level table features is meaningful. However, it differs from ours as it predicts a table structure type instead of semantic types, and uses manually labeled tables instead of automatic sample extraction from KBs to supervise the training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methodology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ColNet Framework</head><p>ColNet is a framework that utilizes a KB, word representations and machine learning to automatically train prediction models for annotating types of entity columns that are assumed to have no metadata. As shown in <ref type="figure">Figure 1</ref>, it mainly includes three steps, each of which will be explained in detail in the following subsections.</p><p>The first step is called lookup. Given an entity column, it retrieves column cells' corresponding entities in the KB and adopts the classes of the matched entities as a set of candidate classes for annotation. Meanwhile, this step generates labeled samples from the KB for model training, including particular samples which have a close data distribution to the column cells, and general samples that deal with the challenge of sample shortage caused by the big knowledge gap, small column size, etc.</p><p>The second step is called prediction, which calculates a score for each candidate class of a given column. For each candidate class, a customized binary CNN classifier which is able to learn both inter-cell and intra-cell locality features is trained and applied to predict whether cells of a column are of this class. In training, ColNet adopts word representations to incorporate contextual semantics and uses transfer learning to integrate particular and general samples.</p><p>The third step is called ensemble. Given a column and a candidate class, ColNet combines the vote from the matched entities of cells with the score predicted by the prediction model so as to keep the advantages of both. Cell to entity matching and voting with majority can contribute to a highly confident prediction, while prediction with CNNs, which considers the contextual semantics of words can deal with type disambiguation and recall cells missed by lookup.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Knowledge Lookup</head><p>In ColNet, we use a KB that is composed of a terminology and assertions. The former include classes (e.g., c 1 and c 2 ) and class relationships (e.g., subClass(c 1 , c 2 )), while the latter includes entities (e.g., e) and classification assertions (e.g., c 1 (e)). The KB can support entity type inference and reasoning with the transitivity of subClass.</p><p>In sampling, we first retrieve a set of entities from the KB, by matching all the column cells with KB entities according to the entity label and entity anchor text using a lexical index. Those matched entities are called particular entities. The classes and super classes of each particular entity are inferred (via KB reasoning) and they are used as candidate classes for annotation, denoted as C. The reason of selecting candidate classes instead of using all the KB classes is to avoid additional noise, thus reducing false positive predictions and computation. For each candidate class, we further infer all of its KB entities that are not matched. They are defined as general entities.</p><p>We also repeat the above lookup step with another round for refinement, inspired by <ref type="bibr" target="#b5">Mulwad et al. 2010)</ref>. The second round uses each column's candidate classes from the first round to constrain the cell to entity matching, thus refining the entity suggestions. This step filters out some particular entities and candidate classes with limited matching confidence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Synthetic Columns</head><p>In training, ColNet automatically extracts labeled samples from the KB. A training sample s . = (e, c) is composed of a synthetic column e and a class c in C, while a synthetic column is constructed by concatenating a specific number of entities. This number is denoted as h.</p><p>ColNet constructs a set of training samples by selecting different sets of entities with size h and concatenating entities in each set in different orders. In prediction, the input is a synthetic column that is constructed by concatenating table cells (cf. details in Prediction and Ensemble). Using the synthetic column as an input enables the neural network to learn the inter-cell correlation for some salient features like word co-occurrence, thus improving the prediction accuracy.</p><p>For example, given a column of IT Company that is composed of "Apple", "MS" and "Google", ColNet outputs a high score if the three cells are input as a synthetic column, but a low score if they are predicted individually and then averaged. This is because "Apple" and "MS" alone can be easily classified as other types like Fruit and Operating System, but will be correctly recognized as IT Company if their co-occurrence with "Google" is considered.</p><p>For another example, the combination of cell "Oxford University Museum of Natural History" and cell "British Museum" is more likely to be predicted as the right type Museum than the first cell alone, because the signal of Museum is augmented in the locality feature learned from the inter-cell word sequence "Museum", "of", "Natural", "History", "British" and "Museum".</p><p>Sampling For each candidate class c in C, both positive and negative training samples are generated, where a sample is defined as positive if each entity in its synthetic column is inferred as an instance of c, and negative otherwise. To fully capture the hyperplane that can distinguish column cells for the class, we develop a table-adapted negative sampling approach. Given the candidate class c, ColNet first finds out its neighboring candidate classes, each of which is defined to be a specific column's candidate class co-occurring with c. Then ColNet uses the entities that are of a neighboring class of c but are not of class c to construct the synthetic column of negative samples.</p><p>Meanwhile, ColNet extracts two sample sets for each candidate class c. They are i) particular samples which are constructed with particular entities, denoted as S p , and ii) general samples which are constructed with general entities, denoted as S g . For example, given the above column of IT Company and its general entities retrieved from DBPedia "dbr:Google", "dbr:Apple", "dbr:Apple Inc." and "dbr:Microsoft Windows", synthetic columns constructed with "dbr:Google" and "dbr:Apple Inc." (resp. "dbr:Apple" and "dbr:Microsoft Windows") are particular positive (resp. negative) samples of IT Company, while synthetic columns constructed by general entities like "dbr:Amazon.com" and "dbr:Alibaba Group" are general positive samples.</p><p>Compared with S g , S p has a closer data distribution to the column cells, and therefore is able to make the models adaptive to the prediction data. However, because of the big knowledge gap, short column size, ambiguous cell to entity matching, etc., S p in many cases is too small to train robust classifiers, especially for those complex models like CNN. Consequently, we use transfer learning to incorporate both S g and S p in training (cf. details in the next subsection).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Training</head><p>Synthetic Column Embedding In model training, we first embed each synthetic column into a real valued matrix using word representation models like word2vec <ref type="bibr" target="#b4">(Mikolov et al. 2013)</ref>, so as to feed it into a machine learning algorithm with the contextual semantics of words incorporated.</p><p>The label of each entity is first cleaned (e.g., removing the punctuation) and split into a word sequence. Then the word sequences of all the entities of a synthetic column are concatenated into one. To align word sequences of different synthetic columns, their lengths are fixed to a specific value n which is set to the length of the longest word sequence. Those abnormally long sequences are not considered in setting n. A word sequence shorter than n is padded with "NULL" whose word representation is a zero vector, while those that are longer than n are cropped.</p><p>Briefly, the word sequence of a synthetic column e is denoted as ws(e) = [word 1 , word 2 , ? ? ? , word n ], and its embedded matrix is calculated as</p><formula xml:id="formula_0">x(e) = v(word 1 ) ? v(word 2 ) ? ? ? ? v(word n ) (1)</formula><p>where v(?) represents d dimension word representation and ? represents stacking two vectors. For example, considering a synthetic column composed of DBpedia entities dbr:Bishopsgate Institute and dbr:Royal Academy of Arts, its matrix is the stack of the word vectors of "Bishopsgate", "Institute", "Royal", "Academy", "of", "Arts" and two zero vectors, where we assume n is fixed to 8.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Neural Network</head><p>We use a CNN to predict the type of a synthetic column, inspired by its successful application in text classification <ref type="bibr">(Kim 2014</ref>  As shown in <ref type="figure" target="#fig_0">Figure 2</ref>, the CNN architecture includes one convolutional (Conv) layer which is composed of multiple filters (i.e., convolution operations over the input matrix) with a fixed width (i.e., word vector dimension d) but different heights. For each Conv filter w, one feature vector, with a specific granularity of locality, is learned:</p><formula xml:id="formula_1">f = g(w ? x(e) + b)<label>(2)</label></formula><p>where b is a bias vector, ? represents the convolution operation and g(?) is an activation function (e.g., ReLU). Considering a convolution filter with size of k ? d, the dimension of its feature vector f is n ? k + 1, and its i th element is calculated as f i = g(w ? x(e) i:i+k?1 + b i ), where ? represents the element-wise matrix multiplication. After the Conv layer, a max pooling layer which selects the maximum value of each feature vector and further concatenates all the maximum values is stacked:</p><formula xml:id="formula_2">f = max(f 1 ), max(f 2 ), ..., max(f m )<label>(3)</label></formula><p>where m is the number of convolution filters and f j is the feature vector of j th filter. Intuitively, f can be regarded as the salient signals of the learned features with regard to the specific classification task. For example, considering the input word sequence composed of "Oxford", "University", "Museum", "of", "Natural" and "History", the max pooling layer highlights the signal of "Museum" in training a binary CNN classifier for the class Museum and highlights the signal of "University" for the class Educational Institute. The max pooling layer also reduces the complexity of the network, playing a role of regularization.</p><p>A fully connected (FC) layer which learns the nonlinear relationship between the input and output is further stacked:</p><formula xml:id="formula_3">y = g(f ? w + b )<label>(4)</label></formula><p>where w and b are the weight matrix and bias vector to learn, ? represents the matrix multiplication operation. For binary classification, the dimensions of w and b are m ? 2 and 1 ? 2 respectively. With y, a Softmax layer is eventually added to calculate the output score p. No regularizations are added to the FC layer, as the max pooling is already able to prevent the network from over fitting.</p><p>Transfer Learning The training of each CNN classifier incorporates both general sample set S g and particular sample set S p . ColNet first pre-trains the CNN with S g , and then fine tunes its parameters with S p . To make the model fully adapted to the table data, the iteration number of fine tuning is set to be inversely proportional to the size of S p . Specially, when there are no matched KB entities, we can reuse candidate classes from other columns in the table set and train the classifiers using S g alone. For example, consider a column with (yet unknown) researcher names like "Ernesto Jimenez-Ruiz" and DBPedia as a KB. The particular sample set S p is empty as there are not DBPedia correspondences for this column. ColNet, however, may still be able to predict a type for such a column, by relying on general entities extracted from other columns such as "dbr:Ernesto Sabato" (of type dbo:Person).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Prediction</head><p>For each column, ColNet uses the trained CNNs of its candidate classes to predict its types. Synthetic columns are first extracted. However, traversing all the cell combinations costs exponential computation time, which is impractical. ColNet samples N testing synthetic columns by (i) sliding a window with size of h over the column, and (ii) randomly selecting ordered cell subsets with size of h from the column. N is often set to a large number for a high coverage and stable predictions. Each sampled synthetic column is embed- </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ensemble</head><p>We integrate the prediction from ColNet with the vote by KB entities that are retrieved by column cells. The latter method, denoted as Lookup-Vote is widely used for column type annotation (e.g., <ref type="bibr" target="#b15">(Zwicklbauer et al. 2013)</ref>). Given a target column, it first matches cells to entities according to a lexical index, and then uses the rate of cells that have entity correspondences of class c as the score of annotating the column with c, denoted as v c . There have been many methods for combing multiple classifiers <ref type="bibr" target="#b7">(Ponti Jr 2011)</ref>. We use a customized rule that can utilize the advantage of both ColNet and Lookup-Vote. For a candidate class c, we combine p c and v c as follows:</p><formula xml:id="formula_4">s c = v c , if v c ? ? 1 or v c &lt; ? 2 p c , otherwise<label>(5)</label></formula><p>where ? 1 and ? 2 are two hyper parameters in [0, 1], and ? 1 ? ? 2 . The rule accepts classes supported by a large part of cells (i.e., v c ? ? 1 ) and rejects classes supported by few cells (i.e., v c &lt; ? 2 ). By setting an intermediate or high value (e.g., 0.5) to ? 1 and a small value (e.g., 0.1) to ? 2 , the rule helps achieve a high precision. For the classes with less confidence from voting (i.e., ? 2 ? v c &lt; ? 1 ), it adopts the predicted score, which helps recall some classes that have no entity correspondences. In final decision making, the column is annotated by class c if s c ? ?, and not otherwise, where ? is a threshold hyper parameter in [0, 1]. The optimized setting of ? 1 , ? 2 and ? can be searched with small steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Evaluation Experiment Settings</head><p>We  In the experiment, we adopt the DBpedia lookup service to retrieve particular entities. The service, which is based on an index of DBPedia Spotlight <ref type="bibr" target="#b4">(Mendes et al. 2011</ref>), returns DBpedia entities that match a given text phrase. The DBPedia SPARQL endpoint is used to infer an entity's class and super classes. A word2vec model trained with the latest dump of Wikipedia articles is used. Each classifier is trained within 2 minutes on our workstation with Xeon CPU E5-2670, with our Tensforflow implementation 3 . Efficiency and scalability will be improved and evaluated in future work. We evaluate two aspects: (i) the overall performance of Col-Net on column type annotation, and (ii) the impact of learning techniques on the prediction models (CNNs).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Overall Performance</head><p>We use precision, recall and F1 score to measure the overall performance of ColNet under both "strict" and "tolerant" models. Given a target column, the "tolerant" model equally counts each of its predictions, while the "strict" model counts its predictions if the "best" class is hit and directly regards all of them as false positives otherwise. On both table datasets, ColNet, with and without ensemble (i.e., s c and p c ), is evaluated and compared with Lookup-Vote (i.e., v c ) and T2K Match 4 (Ritze, Lehmberg, and Bizer 2015) whose authors developed T2Dv2. On Limaye, Col-Net is further compared with a voting method using entities matched by <ref type="bibr" target="#b1">(Efthymiou et al. 2017)</ref>, named Efthymiou17-Vote. On both table datasets, ? 1 and ? 2 are set to 0.5 and 0.08, while ? has been adjusted and set to a value with the highest F1 score. ? is set to 0.45, 0.55, 0.2, 0.2 and 0.1 for ColNet Ensemble , ColNet, Lookup-Vote, T2K Match and Efthymiou17-Vote respectively. The results are shown in <ref type="table" target="#tab_4">Table 2 and Table 3</ref>. 5</p><p>Prediction Impact We first present the impact of prediction models by comparing ColNet Ensemble with Lookup-Vote. On T2Dv2, ColNet Ensemble has 2.3% and 0.8% higher F1 score under "tolerant" and "strict" models, while on Limaye, the corresponding improvements by integrating prediction are 15.0% and 23.8%. The comparison also verifies that the prediction can improves the recall as it can predict the type of columns that lack entity correspondences. The</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Models</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>All   average recall improvement is around 3.9% on T2Dv2 and around 32.0% on Limaye, each of which is much higher than the corresponding F1 score improvement. Meanwhile, we can also find ColNet (pure prediction) has higher F1 score, precision and recall than Lookup-Vote on Limaye which has a small average column size and is hard to be voted with entity correspondences. The F1 score outperforming is 14.0% and 19.2% under "tolerant" and "strict" models.</p><note type="other">Columns PK</note><p>Ensemble Impact ColNet with an ensemble of lookup (ColNet Ensemble ) achieves higher F1 score than ColNet without ensemble on both table sets. For example, the ensemble benefits ColNet with 7.9% and 4.9% F1 score improvements on all columns of T2Dv2 under "strict" and "tolerant" models. Actually, ColNet Ensemble also outperforms ColNet on precision and recall in all the cases except for the recall on Limaye under "tolerant" model. Meanwhile, the results show that the improvement on precision is more significant than on recall. In the two cases of the above example, the precision (recall) improvements by ensemble are 11.5% and 5.4% (4.3% and 1.6%). This phenomena verifies that integrating the score from Lookup-Vote with our ensemble rule improves the precision.</p><p>Comparison with The State-of-the-art First, both Col-Net and ColNet Ensemble outperforms T2K Match on precision, recall and F1 score in all the cases. For example, the F1 score outperforming by ColNet Ensemble is 27.7% (20.6%) on all (PK) columns of T2Dv2 under "tolerant" model. One potential reason is that the matchings in T2K Match, which ignore contextual semantics of words are ambiguous. Second, ColNet Ensemble and ColNet also outperform Efthymiou17-Vote. On Limaye, F1 score of ColNet Ensemble is 48.9% and 36.6% higher than Efthymiou17-Vote under "tolerant" and "strict" models respectively. Efthymiou17-Vote has competitive precision but much lower recall, because a large part of cells have no entity correspondences. In ColNet, for the cells without entity correspondences, the lookup part can get close entities with the same or overlapping classes, while the prediction part which considers the contextual semantics then accurately predict these cells' classes.</p><p>Column Size Impact By comparing <ref type="table" target="#tab_4">Table 2</ref> with <ref type="table" target="#tab_5">Table 3</ref> (results on two different data sets), we find that all the methods are more accurate on T2Dv2 than on Limaye, although the former has more "best" and "okay" ground truth classes. For example, ColNet Ensemble has 14.4% and 36.9% higher F1 score on Limaye under "tolerant" and "strict" models. One potential reason is that the average cell number per column in T2Dv2 is much larger than that in <ref type="bibr">Limaye (i.e.,</ref><ref type="bibr">124 vs 23)</ref>. This provides more evidence for prediction and voting. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The Prediction Models</head><p>We further evaluate the CNNs with the impact of synthetic columns, knowledge gap and transfer learning. To this end, we extract labeled synthetic columns as the testing samples, and divide the candidate classes into truly matched (TM) if they are among the ground truths, and falsely matched (FM) otherwise. For TM classes, we adopt Area Under ROC Curve (AUC) as the metric, while for FM classes, we use the average score (AS) of testing samples as only negative testing samples can be extracted. The higher AUC or the lower AS, the better performance. Results on four kinds of T2Dv2 columns are reported in <ref type="figure" target="#fig_2">Figure 3</ref> and 4. <ref type="figure" target="#fig_2">Figure 3</ref> shows that the performance of CNNs with respect to both TM classes and FM classes mostly increases as the synthetic column size increases, especially from 1 to 4. For example, the average AUC of TM classes of Person increases from around 0.93 to 0.97, while the average AS of its FM classes drops from around 0.35 to 0.22. This phenomenon verifies that classification of synthetic columns is more accurate than classification of cells.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Synthetic Columns</head><p>The synthetic column on one hand provides more evidence, on the other hand enables the CNN to learn additional intercell locality features. <ref type="figure" target="#fig_2">Figure 3</ref> also indicates that 4 is an optimized synthetic column size setting on T2Dv2.</p><p>Knowledge Gap <ref type="figure" target="#fig_3">Figure 4</ref> shows that the performance of CNNs on both TM classes and FM classes significantly drops as the knowledge gap increases, especially when no transfer from general samples is conducted.  increases to higher than 0.5, which means the classifiers predict over half of the negative testing samples as positive. Such a performance drop is mainly caused by underfitting in training, due to particular sample shortage. The performance drop of CNNs on FM classes is more significant, because the FM classes, introduced by incorrect cell to entity matchings, have a smaller number of particular entities. These results support the fact that ColNet and ColNet Ensemble perform worse on Limaye than on T2Dv2, since the former has small column size with fewer entity correspondences.</p><p>Transfer Learning <ref type="figure" target="#fig_3">Figure 4</ref> shows that transfer learning with general samples significantly benefits the CNNs, especially when the knowledge gap is large. When the ratio of particular entities used in training is set to 0.1, 0.25, 0.5, 0.75 and 1.0, the average improvements of CNNs of TM (FM) classes are 0.9%, 0.8%, 1.7%, 2.7% and 6.5% (56.4%, 68.7%, 70.2%, 71.7% and 77.7%) respectively. <ref type="figure" target="#fig_3">Figure 4</ref> also shows that particular entities are essential in training. For example, considering Organization, training with both particular and general samples for FM classes achieves 25.8% lower average AS than training with general samples alone. Fine tuning with particular samples helps bridge the data distribution gap between column cells and KB entities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discussion</head><p>On the one hand, we analyze the impact of the knowledge gap. By comparing the performance on two different web table datasets that have a big gap in average column size, we find the shorter columns, which have less entity correspondences in average, are harder to be annotated. ColNet outperforms the latest collective approach T2K Match and two cell-to-entity matching based approaches Lookup-Vote and Efthymiou17-Vote, especially on shorter columns. This indicates that ColNet can be distinguished from the other methods by dealing with the knowledge gap for higher performance. It is further verified by the analysis on CNNs' performance under different simulated knowledge gaps. We could not compare our approach to TableMiner+ <ref type="bibr">(Zhang 2017)</ref>, T2K Match++ <ref type="bibr">(Ritze 2017)</ref>, and Efthymiou17-Vote on T2Dv2 <ref type="bibr" target="#b1">(Efthymiou et al. 2017)</ref> as no runnable code was available at the time of conducting the evaluation.</p><p>On the other hand, we evaluate the impact of synthetic column size, which indicates that embedding the semantics There are two reasons. First, ColNet targets a different table annotation task and holds a different input assumption, where tabular data are provided column by column without any structure information. Second, ColNet automatically supervises the learning of prediction models by KB lookup and reasoning while those two studies use labeled tables. The former encounters some additional challenges like synthetic column construction, sample shortage and so on.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion and Outlook</head><p>The paper presents a neural network and semantic embedding based column type prediction framework named Col-Net. Different from existing methods, it (i) utilizes column cells alone without assuming any table metadata or table structures, thus being able to be extended to any tabular data, (ii) learns both cell level and column level semantics with CNNs and word representation for high accuracy, (iii) automatically trains prediction models utilizing KB lookup and reasoning as well as machine learning methods like transfer learning, and (iv) takes the knowledge gap into consideration, thus being able to deal with growing web tables or be applied in populating KBs with new tabular data. The evaluation on two different web table sets T2Dv2 and Limaye under both "tolerant" and "strict" models verifies the effectiveness of ColNet and shows that it can outperform the state-of-the-art approaches.</p><p>In the future, we will extend column type annotation to other related tasks like property annotation and further study learning table locality features with semantic reasoning.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>The CNN architecture used in ColNet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>ded into a matrix by the same way used in model training, and then predicted by model M c for each candidate class c of the column: p c k Mc ? ? ? x(e k ), where k = 1, ? ? ? , N and p c k is a score in [0, 1]. ColNet eventually averages all the scores as the prediction score: p c = 1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>The performance of CNNs on TM classes [left] and FM classes [right] under different synthetic column sizes, trained by particular samples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>The performance of CNNs of TM classes [above] and FM classes [below], under different knowledge gaps, with and without transfer learning. Knowledge gap is simulated by randomly selecting a ratio of particular entities for training. The lower ratio, the larger gap. of columns and learning locality features cross cells can improve prediction models' performance. We do not compare ColNet with (Nishida et al. 2017) and (Luo et al. 2018), although they also learn locality features of a table.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>). For each candidate class c in C, one binary CNN classifier M c is trained.</figDesc><table><row><cell>Input: Matrix of</cell><cell>Conv Filters (2? , 3? , ?)</cell><cell>Features Max Pooling (Salient Signals)</cell><cell>FC Layer</cell></row><row><cell>a Synthetic</cell><cell></cell><cell></cell><cell>Output</cell></row><row><cell>Column ( )</cell><cell></cell><cell></cell><cell>Score</cell></row><row><cell>%</cell><cell></cell><cell></cell><cell></cell></row><row><cell>.</cell><cell></cell><cell></cell><cell></cell></row><row><cell>.</cell><cell></cell><cell></cell><cell></cell></row><row><cell>.</cell><cell></cell><cell></cell><cell>.</cell></row><row><cell>&amp;</cell><cell></cell><cell></cell><cell>.</cell></row><row><cell></cell><cell></cell><cell></cell><cell>.</cell></row></table><note>. . .</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>use DBPedia<ref type="bibr" target="#b0">(Auer et al. 2007</ref>) and two web table datasets T2Dv2 and Limaye for our experiments. T2Dv2 includes common tables from the Web, 2 with 237 PK entity columns, each of which is annotated by a fine-grained DBPedia class. We call such fine-grained classes as "best" classes, while their super classes which are right but not classes and (ii) inferring "okay" classes of all the columns. Limaye contains tables from Wikipedia pages. We adopt the version published by<ref type="bibr" target="#b1">(Efthymiou et al. 2017)</ref> with 428 PK entity columns, manually annotate these columns with "best" classes and infer the "okay" classes. Some statistics of T2Dv2 and Limaye are shown inTable 1.</figDesc><table><row><cell>Name</cell><cell>Columns</cell><cell>Avg. Cells</cell><cell>Different "Best" ("Okay") Classes</cell></row><row><cell>T2Dv2</cell><cell>411</cell><cell>124</cell><cell>56 (35)</cell></row><row><cell>Limaye</cell><cell>428</cell><cell>23</cell><cell>21 (24)</cell></row></table><note>perfect as "okay" classes. We further extend T2Dv2 by (i) annotating its 174 non-PK entity columns with "best"2 http://webdatacommons.org/webtables/goldstandardV2.html</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Some statistics of the web table sets.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Results (precision, recall, F1 score) on T2Dv2.</figDesc><table><row><cell>Models</cell><cell>Methods</cell><cell>PK Columns</cell></row><row><cell></cell><cell>ColNetEnsemble</cell><cell>0.796, 0.799, 0.798</cell></row><row><cell></cell><cell>ColNet</cell><cell>0.763, 0.820, 0.791</cell></row><row><cell>Tolerant</cell><cell>Lookup-Vote</cell><cell>0.732, 0.660, 0.694</cell></row><row><cell></cell><cell>T2K Match</cell><cell>0.560, 0.408, 0.472</cell></row><row><cell></cell><cell>Efthymiou17-Vote</cell><cell>0.759, 0.414, 0.536</cell></row><row><cell></cell><cell>ColNetEnsemble</cell><cell>0.602, 0.639, 0.620</cell></row><row><cell></cell><cell>ColNet</cell><cell>0.576, 0.619, 0.597</cell></row><row><cell>Strict</cell><cell>Lookup-Vote</cell><cell>0.571, 0.447, 0.501</cell></row><row><cell></cell><cell>T2K Match</cell><cell>0.453, 0.330, 0.382</cell></row><row><cell></cell><cell>Efthymiou17-Vote</cell><cell>0.626, 0.357, 0.454</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table /><note>Results (precision, recall, F1 score) on Limaye.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>For example, in the case without transfer, the average AUC of TM classes of Place, Person, Species and Organization drops by 7.2%, 4.4%, 4.8% and 8.1% respectively, when the ratio of particular entities drops from 1.0 to 0.1. When only 0.1 of particular entities are used, the average AS of FM classifiers</figDesc><table><row><cell></cell><cell>1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.43</cell><cell></cell><cell cols="2">Place*</cell></row><row><cell></cell><cell>0.99</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Person*</cell></row><row><cell></cell><cell>0.98</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.38</cell><cell></cell><cell cols="2">Species*</cell></row><row><cell>Avg. AUC</cell><cell>0.95 0.96 0.97</cell><cell></cell><cell cols="2">Person* Place*</cell><cell>Avg. AS</cell><cell>0.28 0.33</cell><cell></cell><cell cols="2">Organization*</cell></row><row><cell></cell><cell>0.94</cell><cell></cell><cell cols="2">Species*</cell><cell></cell><cell>0.23</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.93</cell><cell></cell><cell cols="2">Organization*</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.92</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.18</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell></row><row><cell></cell><cell></cell><cell cols="2">Synthetic Column Size</cell><cell></cell><cell></cell><cell></cell><cell cols="2">Synthetic Column Size</cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://github.com/alan-turing-institute/SemAIDA 4 Runnable system from https://goo.gl/AGj3dg 5 Note that the results reported in(Ritze 2017) are different as they use a more tolerant calculation of precision and recall.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The work is supported by the AIDA project (UK Government's Defence &amp; Security Programme in support of the Alan Turing Institute), the SIRIUS Centre for Scalable Data Access (Research Council of Norway, project 237889), the Royal Society, EPSRC projects DBOnto, MaSI 3 and ED 3 .</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Dbpedia: A nucleus for a web of open data. The Semantic Web 722-735</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>References</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Auer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Semantic Web Conference</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="538" to="549" />
		</imprint>
	</monogr>
	<note>Webtables: exploring the power of tables on the web</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Matching web tables with knowledge base entities: from entity lookups to entity embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Chu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Kim; Kim, Y.</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1746" to="1751" />
		</imprint>
	</monogr>
	<note>Proceedings of the 2015 ACM SIGMOD International Conference on Management of Data</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A large public corpus of web tables containing time and context metadata</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lehmberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th International Conference Companion on World Wide Web</title>
		<meeting>the 25th International Conference Companion on World Wide Web</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="75" to="76" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Annotating and searching web tables using entities, types and relationships</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarawagi</forename><surname>Limaye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chakrabarti ; Limaye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sarawagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chakrabarti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="362" to="369" />
		</imprint>
	</monogr>
	<note>Cross-lingual entity linking for web tables</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Mendes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th international conference on semantic systems</title>
		<meeting>the 7th international conference on semantic systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
	<note>Advances in Neural Information Processing Systems</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Using linked data to interpret tables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mulwad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the the First International Workshop on Consuming Linked Data</title>
		<meeting>the the First International Workshop on Consuming Linked Data</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Understanding the semantic structures of tables with a hybrid deep neural network architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Finin</forename><surname>Mulwad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Joshi ; Mulwad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Finin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Nishida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sadamitsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Higashinaka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Matsuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Alse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">A</forename><surname>Knoblock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Szekely</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Semantic Web Conference</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="446" to="462" />
		</imprint>
	</monogr>
	<note>International Semantic Web Conference</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Profiling the potential of web tables for augmenting cross-domain knowledge bases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><surname>Ponti</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">P</forename><surname>Ponti</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Quercini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Reynaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ritze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Lehmberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Oulabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bizer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Graphics, Patterns and Images Tutorials (SIBGRAPI-T), 2011 24th SIBGRAPI Conference on</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="251" to="261" />
		</imprint>
	</monogr>
	<note>Proceedings of the 25th International Conference on World Wide Web</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Matching html tables to dbpedia</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lehmberg</forename><surname>Ritze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bizer ; Ritze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Lehmberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bizer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th International Conference on Web Intelligence, Mining and Semantics</title>
		<meeting>the 5th International Conference on Web Intelligence, Mining and Semantics<address><addrLine>Germany</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">10</biblScope>
		</imprint>
		<respStmt>
			<orgName>University of Mannheim</orgName>
		</respStmt>
	</monogr>
	<note>Ritze 2017] Ritze, D. 2017. Web-Scale Web Table to Knowledge Base Matching. Ph.D. Dissertation</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Table cell search for question answering</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th International Conference on World Wide Web</title>
		<meeting>the 25th International Conference on World Wide Web</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="771" to="782" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Exploiting a web of semantic data for interpreting tables</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second Web Science Conference</title>
		<meeting>the Second Web Science Conference</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Recovering semantics of tables on the web</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Venetis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the VLDB Endowment</title>
		<meeting>the VLDB Endowment</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="528" to="538" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">On multicolumn foreign key discovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the VLDB Endowment</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="805" to="814" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Towards efficient and effective semantic table interpretation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Semantic Web Conference</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="487" to="502" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Effective and efficient semantic table interpretation using tableminer+</title>
	</analytic>
	<monogr>
		<title level="j">Semantic Web</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="921" to="957" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Towards disambiguating web tables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Zwicklbauer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Semantic Web Conference</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="205" to="208" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Doser-a knowledgebase-agnostic framework for entity disambiguation using semantic embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seifert</forename><surname>Zwicklbauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zwicklbauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Seifert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Granitzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Semantic Web Conference</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="182" to="198" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
