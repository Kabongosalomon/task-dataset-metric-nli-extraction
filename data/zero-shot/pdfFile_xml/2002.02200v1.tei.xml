<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">RGB-based Semantic Segmentation Using Self-Supervised Depth Pre-Training</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Lahoud</surname></persName>
							<email>jean.lahoud@kaust.edu.sa</email>
							<affiliation key="aff0">
								<orgName type="institution">King Abdullah University of Science and Technology (KAUST</orgName>
								<address>
									<country key="SA">Saudi Arabia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
							<email>bernard.ghanem@kaust.edu.sa</email>
							<affiliation key="aff0">
								<orgName type="institution">King Abdullah University of Science and Technology (KAUST</orgName>
								<address>
									<country key="SA">Saudi Arabia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">RGB-based Semantic Segmentation Using Self-Supervised Depth Pre-Training</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T10:40+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Although well-known large-scale datasets, such as Ima-geNet <ref type="bibr" target="#b7">[8]</ref>, have driven image understanding forward, most of these datasets require extensive manual annotation and are thus not easily scalable. This limits the advancement of image understanding techniques. The impact of these large-scale datasets can be observed in almost every vision task and technique in the form of pre-training for initialization. In this work, we propose an easily scalable and self-supervised technique that can be used to pre-train any semantic RGB segmentation method. In particular, our pretraining approach makes use of automatically generated labels that can be obtained using depth sensors. These labels, denoted by HN-labels, represent different height and normal patches, which allow mining of local semantic information that is useful in the task of semantic RGB segmentation. We show how our proposed self-supervised pre-training with HN-labels can be used to replace Ima-geNet pre-training, while using 25x less images and without requiring any manual labeling. We pre-train a semantic segmentation network with our HN-labels, which resembles our final task more than pre-training on a less related task, e.g. classification with ImageNet. We evaluate on two datasets (NYUv2 [28] and CamVid [3]), and we show how the similarity in tasks is advantageous not only in speeding up the pre-training process, but also in achieving better final semantic segmentation accuracy than ImageNet pretraining.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>One of the main goals in computer vision is to achieve a human-like understanding of images. This understanding has been recently represented in various forms, including image classification, object detection, semantic segmentation, among many others. All of these tasks are alleviated with large annotated datasets, especially after the emergence of deep learning techniques. The importance of such large-scale datasets e.g. ImageNet <ref type="bibr" target="#b7">[8]</ref>, can be easily ob- <ref type="figure">Figure 1</ref>. Self-supervised learning with depth for pre-training. Typically, a semantic segmentation model is initialized using another model pre-trained using hand labelled images (e.g. Ima-geNet). We propose HN-labels that can be automatically generated from corresponding depth maps. These labels can be used for pre-training while maintaining the final semantic segmentation performance. served in most deep learning methods, as nearly all techniques initialize from models pre-trained on these datasets.</p><p>Since annotation is tedious and cannot be easily scaled, it currently represents a bottleneck to the advancement of deep learning methods. In an attempt to address this limitation, several approaches have explored the potential to minimize the amount of manual work required prior to training. One of the main trends today is the use of synthetic datasets to create photo-realistic environments that are inherently labeled. Although synthetic images have become to some extent similar to images taken in real environments, generating these realistic-looking scenes still requires considerable manual effort in most cases. Another promising research direction is self-supervised learning, as well as reinforcement learning. Such learning does not require annotated data, and hence loosens the tie between learning methods and available datasets. Nevertheless, these techniques require correct modeling and might become computationally demanding.</p><p>This work aims at generating labels that (i) can be auto-  <ref type="table">Table 1</ref>. A comparison between various training requirements for semantic segmentation. We show the advantages (+) of every method requirement as well as the disadvantages (-). (o) implies unclear benefit to the corresponding specification. Our proposed method is easily scalable, and operates directly on RGB images at test time without the need for additional information. Model based methods require a library of known shapes, such as 3D CAD models.</p><p>matically obtained without manual annotation, and (ii) improve the task of semantic segmentation. Specifically, we use the depth channel supplied by RGBD sensors to extract patches that are labeled according to height and normal information only. We denote our labels by HN-labels, which we use to pre-train a semantic segmentation network in a self-supervised fashion. In this way, both our pre-training and training processes are closely related as both aim at semantically segmenting a given scene. Our method makes use of this depth information for pre-training only, as depth information need not be present at inference time. We show how our HN-labels can easily replace largescale annotations required for pre-training. An added advantage of our proposed technique is that it helps introduce new segmentation models not based on popular architectures such as VGG-16 <ref type="bibr" target="#b32">[33]</ref>, AlexNet <ref type="bibr" target="#b22">[23]</ref>, GoogleNet <ref type="bibr" target="#b34">[35]</ref>, and ResNet <ref type="bibr" target="#b17">[18]</ref>. Our goal is to substantially reduce the amount of manual work required prior to training. Therefore, we formulate the pre-training as a self-supervised learning process. The only manual work needed is the collection of aligned RGBD images, which is straightforward, not time consuming, and does not require manual semantic labeling. <ref type="figure">Figure 1</ref> shows the end target of our method: to replace hand-labeled datasets used in initializing semantic segmentation networks with a dataset that can be easily collected and automatically labeled, without significant impact to segmentation performance.</p><p>We formulate the initialization as an HN-labels segmentation task, which is closely related to semantic segmentation. The HN-labels represent different classes that are automatically formed based on grouping normal angles and height relative to the floor plane. Learning to segment the HN-labels classes enables mining of local semantic information. We show how initializing from a closely related task is much more efficient than initializing from less related tasks. Predicting heights and normals from a single RGB image is a by-product of our method and is not the main goal. Similar to depth estimation, height can take continuous values in 3D which makes the estimation task harder. Nevertheless, indoor scenes usually have structured environments in which common objects generally have similar heights. Unlike depth, it is important to note that height are independent of the camera viewpoint. <ref type="table">Table 1</ref> shows a comparison between the various training requirements of semantic segmentation techniques. Our approach aims at providing labels that can be easily scaled, do not require additional complex modeling, and can be easily used to label any given RGB image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Since this work aims at providing better semantic segmentation based on transferring knowledge from a closely related and easily scalable self-supervised task, we provide a literature review on three related topics: (i) semantic segmentation, (ii) multi-task and transfer learning, and (iii) self-supervised and scalable learning.</p><p>Semantic Segmentation: The recent success of deep learning techniques in image classification and object detection have motivated researchers to explore the task of semantic segmentation with deep learning. This could only be achieved with pixel-level labeling provided by large-scale datasets, such as PASCAL VOC <ref type="bibr" target="#b9">[10]</ref>, MS COCO <ref type="bibr" target="#b24">[25]</ref>, Cityscapes <ref type="bibr" target="#b6">[7]</ref>, CamVid <ref type="bibr" target="#b3">[4]</ref>, and KITTI <ref type="bibr" target="#b11">[12]</ref>. Most recent methods benefit from existing classification CNNs (Convolution Neural Networks) by adapting them into ones that can output pixel labeling instead of classification scores. An early method is the Fully Convolution Network (FCN) network <ref type="bibr" target="#b25">[26]</ref>, which replaced the fully connected layers of popular classification networks with what was later referred to as 'deconvolutions' to produce pixel labeling. A similar approach was proposed with the SegNet architecture <ref type="bibr" target="#b1">[2]</ref>, which also utilized convolution layers from a classification network (encoder), but introduced a different upsampling mapping (decoder). The upsampling was done through multi-stage learnable 'deconvolutions' with upsampling indexed similar to max-pooling indices of the encoder part. These techniques, along with numerous others such as <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b36">37]</ref>, require large-scale datasets in training and can only be improved with more groundtruth labeling. <ref type="figure">Figure 2</ref>. General overview of our method. Given an RGB image along with its corresponding depth image, we compute the normals on the generated point cloud. We then estimate the room orientation, and extract the floor plane. Height and normal angles relative to the floor are then used to generate our HN-labels.</p><p>Indoor scenes benefit from the use of off-the-shelf RGBD sensors that measure coupled depth and color information. Multiple datasets were collected using such sensors and were labeled for pixel-level semantic segmentation, including NYU-D v2 <ref type="bibr" target="#b27">[28]</ref> and SUNRGBD <ref type="bibr" target="#b33">[34]</ref>. Encoding the depth cue as an input to neural networks is not straightforward, and multiple approaches have been proposed. Gupta et al. <ref type="bibr" target="#b15">[16]</ref> proposed to encode depth as additional channels in the form of horizontal disparity, height above ground, and the angle between the local surface normal and the gravity direction (HHA). This leads to better segmentation compared to using the color channel alone. However, this method, similar to all RGBD-based techniques, requires the presence of depth information at inference time.</p><p>Transfer Learning and Multi-task Learning: In practice, most CNN approaches do not train their models from scratch. Instead, learned feature extractors are transferred <ref type="bibr" target="#b31">[32]</ref> from models trained on large-scale datasets, such as ImageNet <ref type="bibr" target="#b7">[8]</ref>. Other forms of transfer learning can be done by pre-training on synthetic datasets <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b10">11]</ref>. Transfer learning can be viewed as a special case of multi-task learning, where tasks are sequentially learned instead of being jointly learned. Numerous works train a network to perform multiple tasks simultaneously. The approach proposed by <ref type="bibr" target="#b8">[9]</ref> jointly predicts depth, surface normals, and semantic labels with a multi-scale network. Although these task are closely related, labels for all tasks need to be available for training. The work presented in <ref type="bibr" target="#b26">[27]</ref> introduces a principled way to share activations from multiple networks. The sharing unit, known as cross-stitch, can be trained end-to-end to choose between sharing the layers or separating them at task-specific activations. Other work <ref type="bibr" target="#b20">[21]</ref> proposes a systematic way to combine multiple loss functions of multiple tasks by considering the uncertainty of each task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Self-Supervised Learning:</head><p>The technique presented in <ref type="bibr" target="#b37">[38]</ref> proposes a self-supervised method to generate a large labeled dataset without the need for manual labeling. Labels of known objects were generated by manipulating the camera around a controlled environment, in which object locations are known. The generated dataset was then used to train a robot to better perform in picking tasks. A main drawback of this technique is that the dataset was generated in a controlled environment with known camera locations and also required human involvement in arranging objects. Other self-supervised techniques aim at regressing depth from videos <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b35">36]</ref>. Hence, these techniques are good at estimating depth especially in outdoor scenes in the context of autonomous driving. These applications mainly focus on objects located on the road. This greatly differs from how object are arranged in indoor scenes. Since we model our pre-training as a self-supervised process, the amount of RGBD data that is available for pre-training is large and can be easily scaled up. One can easily append training data with personal data collected with RGBD sensors in any environment of interest. Self-supervision for representation learning was also explored in <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b23">24]</ref>, in which proxy tasks were proposed using the color images. Although our proposed proxy segmentation task requires depth data, this additional data provides information that cannot be learned from the image alone. Techniques, which only require color, are a good alternative, but since they are not provided with inaccessible information, their performance is shy of ImageNet pre-training for the same task.</p><p>Contributions. We propose a self-supervised technique for the task of height and normal estimation from a single RGB image. The network trained on this task can be used to initialize a semantic segmentation network. We show how this self-supervised pre-training strategy can be easily scaled and is more efficient than initializing from networks trained on different fully supervised tasks. Our experiments show that semantic networks initialized using our proposed selfsupervised method outperform ones initialized from networks trained on large-scale datasets annotated for a different task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head><p>Our method aims at overcoming the limitations of semantic segmentation techniques when training on a small dataset or when training from scratch. An overview of our method is depicted in <ref type="figure">Figure 2</ref>. In the following sections, we detail our process in choosing the labels, generating the dataset, and using these labels to pre-train a given network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Alternative Category Representation</head><p>The motivation to this work is to look at the depth channel as a source for labeling RGB images. RGBD sensors are used to easily collect aligned color and depth image pairs. Our aim is to automatically generate labels that are not necessarily designed for the target task (semantic segmentation), but could afford helpful information for it. Successfully transforming depth into labels makes this method capable of generating free labels as compared to the tedious, time consuming, RGB pixel labeling needed for semantic segmentation. The first intuitive use of depth is to train a network to directly regress the depth from a single RGB image. However, depth requires complex knowledge of the environment and varies with camera location/viewpoint.</p><p>With the complexity of indoor scenes, regressing depth necessitates a high capacity network and can be tedious to train. On the other hand, with additional effort, one can transform depth into height above ground, which is viewindependent. Taking the ground plane as reference, one can also transform normals relative to the camera viewpoint, which depend on camera angle, to view-independent normal angles relative to the ground plane.</p><p>Our method uses height and normal information as the main precursor for pre-training. We choose to bin the height into n h bins (or levels) and normals into n n bins and train a model to output the correct normal and height level per pixel. Since it learns to generate features specific to segmentation, this pre-training is closer to our end goal of semantic segmentation than to pre-train with models trained on other tasks, e.g. image classification. In fact, one can think of height bins as semantic segmentation with object part specific labels: chair and table lower legs, lower part of wall and door, table top, and sofa back, etc. This type of grouping consistently clusters objects and their parts into specific bins. To highlight this point, <ref type="figure">Figure 3</ref> shows the height distribution of 40 indoor objects on a subset of NYUv2 dataset <ref type="bibr" target="#b27">[28]</ref>. For every object, we show the percentage of its occurrence in every height bin. As expected, objects like floor (label 2) and floor mat (label 20) only occur within the lowest height bin, whereas ceiling (label 22) occurs within the highest bins. Other objects almost always occur in the same bin, such as bathtub (label 36) and counter (label 12). Furthermore, when considering other object classes, there exists an overall trend in height, with object parts falling in specific height bins. In light of these observations, training a network to distinguish between different height levels will inherently train it to distinguish between some object classes and/or their parts. This informa-tion is expected to be useful for the task of semantic segmentation. <ref type="figure">Figure 3</ref>. Object height distribution. This colormap shows the distribution of height for various object classes in NYUv2 <ref type="bibr" target="#b27">[28]</ref>. Colors represent the fraction of 3D points of the same label that are located at a particular height bin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Automated Label Collection</head><p>Floor Identification: Our method mainly relies on the floor plane as a reference for the proposed labels. To identify a possible floor plane, we first use the camera intrinsic parameters to map the depth image into a 3D point cloud. Next, we compute the normals at each 3D point using the method of <ref type="bibr" target="#b30">[31]</ref>. We then use the normals to estimate the scene layout orientation using the method of <ref type="bibr" target="#b12">[13]</ref>. Once the orientation is obtained, the room is rotated to make the ground normal aligned with the upward direction. Afterwards, we choose the ground location as the first percentile of points along the upward direction. The first percentile is chosen instead of the minimum in order to make the method more robust to sensor errors especially at far distances. Our method takes into account that in most indoor images captured by RGBD sensors, a part of the ground is visible. In order to account for cases where the ground is not visible, we discard scenes in which the majority of the normals from points on the hypothesized floor plane (first percentile) are not aligned with the upward direction. HN-Label Generation: To transform depth into height, we calculate the distance between each 3D point and the floor plane. As for the normals, we compute the angle between every 3D point normal and the floor's upward direction. We only use one angle to keep labels consistent over multiple viewpoints. Normals and heights are then binned, and we define our proposed alternative categories as all possible combinations of the normal and height bins. <ref type="figure" target="#fig_0">Figure 4</ref> shows our proposed labels for the same scene from different viewpoints. Since we choose our labeling scheme to be viewindependent, objects maintain the same labels throughout the scene recording. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Training with HN-Labels</head><p>Labels collected from height and normal information can be used to train the same network architecture used for semantic segmentation. Specifically, we choose the SegNet architecture <ref type="bibr" target="#b1">[2]</ref> for this purpose, with an encoder network identical to the 13 convolutional layers in the VGG16 network <ref type="bibr" target="#b32">[33]</ref>, as well as, the DeepLabv3 architecture <ref type="bibr" target="#b5">[6]</ref> with ResNetv2 layers <ref type="bibr" target="#b18">[19]</ref>. Note that our method can be easily implemented with any other architecture and that the number of labels for semantic segmentation need not be similar to that of pre-training with HN-labels. In what follows, we will reference this trained network as the HN-network.</p><p>Once we train our HN-network with labels that are selfsupervised, we need to transfer the representation to another network that learns to segment RGB images. In both the SegNet and DeepLabv3 architectures, the only difference between the two networks is the number of object classes, i.e. the number of outputs from the last convolution layer. Transfer learning can be done in two ways. The first way is to initialize all the layers of the semantic segmentation network with what was learned in the first network, and then finetune all the layers based on the manually labeled segmentation dataset. Another way of transfer learning is to keep some of the first layers unchanged, especially when the dataset used for pre-training is much larger than the dataset used for finetuning. This is motivated by the idea that generic information is usually shared at early layers.</p><p>Choosing to finetune only part of the network raises an important question: how many layers should be fixed and how many should be finetuned? Instead of manually selecting which layers to share and which layers to be taskspecific, one can model the problem as a multi-task learning problem, in which sharing can be learned between the networks. This is mainly inspired by the work in <ref type="bibr" target="#b26">[27]</ref>. Our joint network architecture, named cross-stitch <ref type="bibr" target="#b26">[27]</ref>, is shown in <ref type="figure">Figure 5</ref>. The shared representation part of the network is extracted from a network that was trained to out- <ref type="figure">Figure 5</ref>. Proposed fusion of HN-network and semantic segmentation network (Cross-Stitch). The HN-network is considered as a shared representation whereas the semantic segmentation network is task specific). At every max-pooling layer, the network gets to choose between the pre-trained layers and the task specific layers.</p><p>put HN-labels, whereas the task-specific part learns to semantically segment. The shared representation part is kept fixed, reasoned by the large data that can be used to train that part. Sharing of layers is allowed at the first 4 maxpooling layers. We denote the output at every max-pooling by L xs and L xh , where x represents the index of the maxpooling layer. L xs refers to the output generated in the "semantic segmentation network", whereas L xh refers to output from the HN-network. At every max-pooling layer, both representations are merged into L = ? xh L xh + ? xs L xs and L is used as an input to the subsequent layer of the semantic segmentation part of the network.</p><p>The final target is to achieve a higher accuracy in semantic segmentation only, especially when semantic labels are scarce. This differs from <ref type="bibr" target="#b26">[27]</ref>, which aims at achieving a better joint accuracy in multiple tasks. Implementation Details: Our method is implemented using the Caffe framework <ref type="bibr" target="#b19">[20]</ref> for the SegNet architecture and Tensorflow <ref type="bibr" target="#b0">[1]</ref> for the DeepLabv3 architecture, both running on Titan X and Titan Xp GPUs. We use the same image input and output size for all of our networks. In the cases where depth sensors output a different size image, we compute the height on the original size image, then crop and reshape the color image with bilinear interpolation, while using nearest-neighbour interpolation for the labels. As for the training loss, we use the cross-entropy loss <ref type="bibr" target="#b25">[26]</ref> in all our networks.  than 400K frames of indoor RGBD video sequences.</p><p>To evaluate the effect of our pre-training on semantic segmentation, we use the 1449 segmented frames of NYUv2 that are split into train and test, with the class label mapping coming from <ref type="bibr" target="#b14">[15]</ref>. We also perform experiments on the Cambridge-driving Labeled Video Database (CamVid) <ref type="bibr" target="#b2">[3]</ref>, which contains scenes from the perspective of a driving automobile.</p><p>We use the following three conventional evaluation metrics: (i) global accuracy, (ii) class average accuracy, and (iii) mean intersection over union (IoU). Global accuracy is computed as the ratio between the number of correctly labeled pixels and the total number of pixels. The class average accuracy represents a scoring measure that weighs all classes similarly. The mean IoU also presents a measure that equally accounts for all classes and does not favor high accuracies in frequently occurring object classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Training the HN-Network</head><p>To train our proposed HN-network using the generated alternative labels, we use 41K frames from NYUv2 sampled uniformly from all scenes. We use a fixed n n = 2 for normal bins, as indoor objects have surfaces that are mostly oriented either parallel or perpendicular to the floor plane. Our final label set contains n n ? n h labels, with n h = 10, and we discard points that do not have depth information. Also, we use the softmax cross-entropy loss during training that discards pixels with unknown depth label. For all experiments, the input image size is 424?560, which enables us to only fit a batch of 4 images when training our HNnetwork. After 200K iterations in about 2 days, the training loss decreases to 4-5 times its initial value. We test our HN-network on NYUv2 test set. An example of the predicted height of the HN-network is shown in <ref type="figure" target="#fig_1">Figure 6</ref> and compared to the groundtruth. Clearly, the network is able to learn and predict the correct height bin, which is an essential prior to using the same representation for semantic segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Training for Semantic Segmentation</head><p>We use the trained HN-network to initialize our second network for semantic segmentation on RGB images, as described in Section 3.3.</p><p>Ablation Studies: A comparison between different versions of our semantic segmentation method is presented in <ref type="table" target="#tab_1">Table 2</ref>. We fix all the learning hyperparameters and run our experiment with SegNet architecture for 40K iterations (equivalent to about 50 epochs on the NYUv2 dataset). We compare our results against training from scratch with weight filler from <ref type="bibr" target="#b16">[17]</ref>. Note that all measures are computed on 40 classes of NYUv2, which only contains 795 training images. Our network pre-trained with height labels achieved good performance, whereas adding the normal information boosted up the accuracy.</p><p>We also study the effect of training size on the final semantic segmentation accuracy in <ref type="figure" target="#fig_2">Figure 7</ref>. In this case, we fix the number of iterations for the HN-network and  <ref type="table">Table 3</ref>. Evaluation of our proposed method. The accuracy on some of the most occuring object labels in NUYv2 is shown, as well as the global and average accuracies on all 40 classes. Our method is compared to three baselines. The first one does not use additional labels to pre-train, whereas the second and third baselines use manually annotated labels for pre-training.  <ref type="table">Table 4</ref>. Evaluation on the CamVid dataset using SegNet architecture. The global accuracy is shown at different number of training epochs semantic segmentation training. Our proposed method achieves good accuracy even with relatively small number of images with HN-labels. Higher accuracy is achieved with more training images, thus, exploiting the diversity among images in the larger set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Semantic Segmentation Evaluation:</head><p>Here, we compare our results to training from scratch, as well as, pretraining from the ImageNet and CIFAR100 <ref type="bibr" target="#b21">[22]</ref> classification datasets. Semantic segmentation results on NYUv2 dataset are shown in <ref type="table">Table 3</ref> for both the SegNet architecture <ref type="bibr" target="#b1">[2]</ref> and the DeepLabv3 architecture <ref type="bibr" target="#b5">[6]</ref>. All finetuning experiments were done for for 40K iterations (? 50 epochs). If we compare the size of ImageNet to that of our alternative label dataset, ImageNet is about 25 times larger and requires more time to train. Nevertheless, initializing from our proposed dataset outperforms the Ima-geNet initialization for both architecture types. This shows that ability of our method to generalize to other architectures and backbones without requiring any expensive manual labelling. CIFAR100 initialization shows a degradation in accuracy over training from scratch (MSRA initialization <ref type="bibr" target="#b16">[17]</ref>). This degradation is not unusual in the deep learning field, where training with atypical data would produce results closer to undesirable local minima. <ref type="table">Table 4</ref> shows our experiments on the CamVid dataset using the SegNet architecture with VGG16 encoder, which contains images collected from outdoor settings. Initializ-ing from the HN-labels dataset is still beneficial in such a setting, mainly due to the similarity in the task. The main intuition is that training on HN labels has learned to describe low-level feature information for the visual content in the image. Nonetheless, there exist differences in the high-level information between indoor and outdoor scenes, and this is noticeable in the relatively lower improvement for the outdoor scenes. Also, our method achieves the final accuracy much faster than other methods. This shows that although most objects in the outdoor setting were not previously seen by our pre-training network, it has still learned to discriminate between different patches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fixed Layers</head><p>ImageNet Ours  <ref type="table">Table 5</ref>. Effect of fixing layers relative to finetuning all layers.(Global acc/Avg acc/mIoU). Although ImageNet pre-training leads to better performance when some layers are fixed, our proposed method would always outperform it in the same setting.</p><p>We also try to initialize the segmentation network from our HN-network while keeping some layers intact. We finetune all other layers and stop at 40K iterations. The result on NYUv2 is shown in <ref type="table">Table 5</ref>, which shows how global accuracy varies with fixing more layers. As can be seen, fixing up to four layers improves the final accuracy. This result can be reasoned because of the significant difference between the dataset used for pre-training and the one used for finetuning. Nevertheless, this observation cannot be used as a general conclusion as best accuracies can be achieved by fixing different number of layers in different scenarios. This motivates our next experiment, which learns what to share among layers between the two networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cross-Stitching:</head><p>We also conduct an experiment to fuse </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Global acc Avg acc mIoU the HN-network with the semantic segmentation network. Instead of manually selecting the layers to share, our crossstitch network learns to weigh the amount of sharing required between the two streams. The results are shown in <ref type="table" target="#tab_5">Table 6</ref>, where we compare the final cross-stitched network to the non-stitched network and to the network pre-trained on ImageNet. We observe in our experiments that the final semantic segmentation accuracy increases without the need to manually select the number of layers to share and that cross-stitching is an essential component of its final performance. This represents a better way of transfer learning than basic initialization.</p><p>Qualitative Results: <ref type="figure" target="#fig_3">Figure 8</ref> presents a set of RGB im-ages from NYUv2 with their corresponding groundtruth height labels, predicted height labels from HN-network, along with the semantic groundtruth labels and the predicted semantic labels after cross-stitching. These images show that the current technique is capable of aptly generating semantic labels for RGB images along with height labels as a by-product. Also, the relation between the two tasks can be observed in areas like the ground, where a single height label corresponds to a single semantic label.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We present a novel method to pre-train any semantic segmentation network without the need for expensive manual labeling. Our pre-training learns from object heights, which are automatically extracted from depth sensors. Extensive experiments show that our proposed pre-training is better than initializing with ImageNet, while using much less data and without requiring any manual labels. We also propose to fuse the pre-training with the semantic segmentation network to better differentiate between task specific and shared representations, leading to higher overall accuracy. Since our pre-training does not require manual annotation, it is easily scalable. Therefore, we suggest exploring such use of information to better improve the pre-training process to further improve performance over ImageNet pre-training.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 4 .</head><label>4</label><figDesc>HN-labels for different images in the same room. Our proposed labels are view-independent and are consistent among different viewpoints.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 6 .</head><label>6</label><figDesc>Example of the output height labels obtained from the HN-network. The labels are inpainted on the corresponding 3D point cloud. Groundtruth height labels are also shown.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 7 .</head><label>7</label><figDesc>Effect of dataset size. Semantic segmentation network accuracy on NYUv2 increases when pre-trained from a larger number of training images with HN-labels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 8 .</head><label>8</label><figDesc>Qualitative evaluation of our proposed HN-network as well as the output of the semantic segmentation network. From left to right: Input RGB image, groundtruth height label, output height label, semantic groundtruth label, and output semantic label.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Method Requirement Scalability Straightforward Modeling Data Available at test Transferable to RGB only</figDesc><table><row><cell>Labeled RGB</cell><cell>-</cell><cell>++</cell><cell>++</cell><cell>++</cell></row><row><cell>Labeled RGBD</cell><cell>-</cell><cell>+</cell><cell>-</cell><cell>-</cell></row><row><cell>Synthetic</cell><cell>o</cell><cell>++</cell><cell>++</cell><cell>o</cell></row><row><cell>Self-supervised</cell><cell>++</cell><cell>-</cell><cell>++</cell><cell>++</cell></row><row><cell>Model Based</cell><cell>-</cell><cell>o</cell><cell>-</cell><cell>o</cell></row><row><cell>Ours</cell><cell>+</cell><cell>++</cell><cell>++</cell><cell>++</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Ablation study. This table shows the semantic segmentation network accuracy on NYUv2 when pre-trained from different versions of our proposed HN-network.</figDesc><table><row><cell>Pre-training Method</cell><cell cols="3">Global acc Avg acc mIoU</cell></row><row><cell>no pre-training</cell><cell>38.74</cell><cell>13.73</cell><cell>8.78</cell></row><row><cell>height only</cell><cell>50.66</cell><cell cols="2">24.27 16.35</cell></row><row><cell>height 5 bins + normals</cell><cell>51.98</cell><cell cols="2">25.28 17.38</cell></row><row><cell>height 10 bins + normals</cell><cell>52.92</cell><cell cols="2">26.16 18.24</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Arch Pre-training Method Wall Floor Cabinet Bed Bookshelf Sofa Dresser Avg acc mIoU Global acc</figDesc><table><row><cell>SegNet</cell><cell>No pre-training CIFAR100 pre-train 66.33 66.96 19.84 0.03 77.57 74.91 42.36 13.25 ImageNet pre-train 81.54 86.1 58.86 37.76 HN pre-training 83.26 90.63 58.81 58.40</cell><cell>6.58 0.01 35.05 38.79</cell><cell>22.42 3.17 0.05 0 38.25 4.35 37.05 14.24</cell><cell>13.73 10.70 25.40 17.34 8.78 6.23 26.16 18.24</cell><cell>38.74 31.90 50.58 52.92</cell></row><row><cell>DeepLab</cell><cell>No pre-training ImageNet pre-train 28.85 74.69 87.98 64.40 18.70 66.72 60.78 22.30 HN pre-training 32.00 77.64 87.45 60.15</cell><cell>20.70 56.22 55.67</cell><cell>15.24 12.49 59.10 50.78 60.51 53.69</cell><cell>24.77 57.84 34.27 6.73 56.04 33.49</cell><cell>31.41 61.70 62.98</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 .</head><label>6</label><figDesc>Evaluation of the cross-stitching result. The cross-stitch result is compared against no stitching and ImageNet pre-training.</figDesc><table><row><cell>ImageNet pre-training</cell><cell>50.58</cell><cell>25.40</cell><cell>17.34</cell></row><row><cell>ImageNet cross-stitch</cell><cell>50.94</cell><cell>25.33</cell><cell>17.35</cell></row><row><cell>Ours -No stitching</cell><cell>52.92</cell><cell>26.16</cell><cell>18.24</cell></row><row><cell>Ours cross-stitch</cell><cell>54.25</cell><cell>27.83</cell><cell>19.46</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">. Experiments4.1. DatasetsWe generate our alternative label dataset from the unlabeled set of NYUv2<ref type="bibr" target="#b27">[28]</ref> dataset, which comprises more</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Tensorflow: A system for large-scale machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mart?n</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjay</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Isard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">12th {USENIX} Symposium on Operating Systems Design and Implementation ({OSDI} 16)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="265" to="283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Segnet: A deep convolutional encoder-decoder architecture for image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Semantic object classes in video: A high-definition ground truth database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gabriel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Brostow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Fauqueur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Semantic object classes in video: A high-definition ground truth database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gabriel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Brostow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Fauqueur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="88" to="97" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="834" to="848" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Rethinking atrous convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.05587</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uwe</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3213" to="3223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note>CVPR 2009. IEEE Conference on</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Predicting depth, surface normals and semantic labels with a common multi-scale convolutional architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2650" to="2658" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">I</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="303" to="338" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Virtual worlds as proxy for multi-object tracking analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrien</forename><surname>Gaidon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yohann</forename><surname>Cabon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eleonora</forename><surname>Vig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4340" to="4349" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Vision meets robotics: The kitti dataset. The International</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Stiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Robotics Research</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1231" to="1237" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Robust manhattan frame estimation from a single rgb-d image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Thabet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">Carlos</forename><surname>Niebles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian Caba</forename><surname>Heilbron</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3772" to="3780" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Unsupervised monocular depth estimation with leftright consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cl?ment</forename><surname>Godard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oisin</forename><surname>Mac Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><forename type="middle">J</forename><surname>Brostow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="270" to="279" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Perceptual organization and recognition of indoor scenes from rgb-d images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Arbelaez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="564" to="571" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning rich features from rgb-d images for object detection and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Arbel?ez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="345" to="360" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Identity mappings in deep residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="630" to="645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Caffe: Convolutional architecture for fast feature embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM international conference on Multimedia</title>
		<meeting>the 22nd ACM international conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="675" to="678" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Multi-task learning using uncertainty to weigh losses for scene geometry and semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yarin</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7482" to="7491" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Citeseer</title>
		<imprint>
			<biblScope unit="issue">7</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>F. Pereira, C. J. C. Burges, L. Bottou, and K. Q. Weinberger</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2012" />
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Colorization as a proxy task for visual understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gustav</forename><surname>Larsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Shakhnarovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6874" to="6883" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Cross-stitch networks for multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martial</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Derek Hoiem and Rob Fergus. Indoor segmentation and support inference from rgbd images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushmeet Kohli Nathan</forename><surname>Silberman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Unsupervised learning of visual representations by solving jigsaw puzzles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Noroozi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Favaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="69" to="84" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">The synthia dataset: A large collection of synthetic images for semantic segmentation of urban scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">German</forename><surname>Ros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Sellart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joanna</forename><surname>Materzynska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Vazquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio M</forename><surname>Lopez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3234" to="3243" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Augmented-lagrangian regularization of matrixvalued maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guy</forename><surname>Rosman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xue-Cheng</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ron</forename><surname>Kimmel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alfred</forename><forename type="middle">M</forename><surname>Bruckstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Methods and Applications of Analysis</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="121" to="138" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Cnn features off-the-shelf: an astounding baseline for recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Sharif Razavian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hossein</forename><surname>Azizpour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josephine</forename><surname>Sullivan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Carlsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition workshops</title>
		<meeting>the IEEE conference on computer vision and pattern recognition workshops</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="806" to="813" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition. CoRR, abs/1409.1556</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Sun rgb-d: A rgb-d scene understanding benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuran</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxiong</forename><surname>Lichtenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="567" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudheendra</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Susanna</forename><surname>Ricco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katerina</forename><surname>Fragkiadaki</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.07804</idno>
		<title level="m">Sfmnet: Learning of structure and motion from video</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Multi-scale context aggregation by dilated convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Multiview self-supervised deep learning for 6d pose estimation in the amazon picking challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuan-Ting</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuran</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Suo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ed</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>Rodriguez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxiong</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1386" to="1383" />
		</imprint>
	</monogr>
	<note>Robotics and Automation (ICRA</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2881" to="2890" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Unsupervised learning of depth and ego-motion from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinghui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Snavely</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1851" to="1858" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
