<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Self-attention Presents Low-dimensional Knowledge Graph Embeddings for Link Prediction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-07-22">22 Jul 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peyman</forename><surname>Baghershahi</surname></persName>
							<email>p.baghershahi@ut.ac.ir</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Tehran Tehran</orgName>
								<address>
									<country key="IR">Iran</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reshad</forename><surname>Hosseini</surname></persName>
							<email>reshad.hosseini@ut.ac.ir</email>
							<affiliation key="aff1">
								<orgName type="institution">University of Tehran Tehran</orgName>
								<address>
									<country key="IR">Iran</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hadi</forename><surname>Moradi</surname></persName>
							<email>moradih@ut.ac.ir</email>
							<affiliation key="aff2">
								<orgName type="institution">University of Tehran Tehran</orgName>
								<address>
									<country key="IR">Iran</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Self-attention Presents Low-dimensional Knowledge Graph Embeddings for Link Prediction</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-07-22">22 Jul 2022</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T03:10+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>A few models have tried to tackle the link prediction problem, also known as knowledge graph completion, by embedding knowledge graphs in comparably lower dimensions. However, the state-of-the-art results are attained at the cost of considerably increasing the dimensionality of embeddings which causes scalability issues in the case of huge knowledge bases. Transformers have been successfully used recently as powerful encoders for knowledge graphs, but available models still have scalability issues. To address this limitation, we introduce a Transformer-based model to gain expressive low-dimensional embeddings. We utilize a large number of self-attention heads as the key to applying querydependent projections to capture mutual information between entities and relations. Empirical results on WN18RR and FB15k-237 as standard link prediction benchmarks demonstrate that our model has favorably comparable performance with the current state-of-the-art models. Notably, we yield our promising results with a significant reduction of 66.9% in the dimensionality of embeddings compared to the five best recent state-of-the-art competitors on average. 1</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Information in lots of real-world problems such as question answering, information retrieval, recommendation systems, and web search can potentially be represented as Knowledge Graphs (KGs) consisting of entities and their relations. However, these KGs are commonly incomplete and might have lots of missing relations due to lack of observation or cost of exploration. Link prediction, also called knowledge graph completion, is a kind of automated reasoning to predict missing links between entities of KGs.</p><p>Existing link prediction methods on KGs lie in two principal directions: embedding-based methods and neural network methods. Embedding-based methods first associate an embedding vector to each relation and entity, formally in Elucidian, complex, or hyperbolic spaces; Then some of them learn linear transformations from source to target entities considering their relations <ref type="bibr" target="#b0">(Bordes et al., 2013;</ref><ref type="bibr" target="#b1">Wang et al., 2014;</ref><ref type="bibr" target="#b2">Sun et al., 2019;</ref><ref type="bibr" target="#b4">Chami et al., 2020)</ref>, while the others match latent semantics of entities with respect to relations <ref type="bibr" target="#b5">(Nickel et al., 2011;</ref><ref type="bibr" target="#b6">Yang et al., 2015;</ref><ref type="bibr" target="#b7">Trouillon et al., 2016;</ref><ref type="bibr" target="#b8">Kazemi and Poole, 2018;</ref><ref type="bibr" target="#b9">Lacroix et al., 2018;</ref><ref type="bibr" target="#b10">ZHANG et al., 2019)</ref>. Embedding-based methods require high dimensional embeddings to effectively encode the whole information of KGs. Consequently, they have scalability issues facing huge KGs in lots of real-world applications. Also, they easily overfit due to complexity overload of high dimensional embeddings.</p><p>Alternatively, neural network methods <ref type="bibr" target="#b11">(Socher et al., 2013;</ref><ref type="bibr" target="#b12">Schlichtkrull et al., 2018;</ref><ref type="bibr" target="#b13">Dettmers et al., 2018;</ref><ref type="bibr" target="#b14">Bansal et al., 2019;</ref><ref type="bibr" target="#b15">Bala?evi? et al., 2019;</ref><ref type="bibr" target="#b18">Vashishth et al., 2020a;</ref><ref type="bibr">Chen et al., 2021)</ref> enrich representations by storing some learned knowledge in nonembedding free parameters of the model to be shared through the whole KG. However, exiting neural network models also achieve their best results at high-dimensions. Therefore, passing through high to low dimensions degrades the performance of both methods significantly which necessitates an expressive model to fill the gap.</p><p>Inspired by recent advances offered by Transformers <ref type="bibr" target="#b20">(Vaswani et al., 2017)</ref>, they have been successfully used as general encoders for knowledge graph completion with promising results, CoKE  and HittER <ref type="bibr">(Chen et al., 2021)</ref>. However these models similarly use highdimensional embeddings to gain their best results and have scalability issues consequently. Moreover, they are both large models and use multiple encoder blocks which imposes too many nonembedding free parameters and increases time and memory complexity considerably.</p><p>We propose a Transformer-based model to tackle the aforementioned limitations. We found the number of self-attention heads significantly important to capture dependencies in different projection spaces. Consequently, we use a single encoder block but large number of heads and notably less parameters than previous similar models. Also, We attain competitive results with embeddings in remarkably low-dimensions by stacking simple decoders to our general powerful encoder.</p><p>Self-attention is proved to be strong enough to effectively capture the mutual information and dependencies within a sequence <ref type="bibr" target="#b21">(Shen, 2019)</ref>. Formally, the self-attention matrix in our model applies query-dependant linear projections on the input query <ref type="bibr" target="#b22">(Likhosherstov et al., 2021)</ref> and is responsible to propagate dependencies and interactions between the input entity and relation. Therefore, our model generates query-specific representations rather than just memorizing the general embeddings.</p><p>Our experimental results on standard link prediction benchmarks, FB15k-237 and WN18RR, shows that our model has competitive performance with state-of-the-art models. Interestingly, our remarkable results are gained by a significant reduction of 66.9% in the dimensionality of embeddings compared to the five best state-of-the-art competitors on average. In consequence, we tackle the scalability issue very effectively and earn great performance without conducting any base assumptions on the input data that makes our results more consistent through different datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Works</head><p>Generally, there are two broad categories in the field of link prediction on KGs: embedding-based methods and neural network methods. Embedding-based methods score triples using computed embeddings for entities and relations; They contain two subcategories: transformational and semantic matching approaches.</p><p>The earliest transformational methods proposed to be in Elucidian domain such as TransE <ref type="bibr" target="#b0">(Bordes et al., 2013)</ref> which considers a simple translation from source to target parameterized by relation, and TransH <ref type="bibr" target="#b1">(Wang et al., 2014)</ref> which extends these translations to hyperplanes. On the other hand, semantic matching methods try to tie latent semantics of entities with respect to relations. In this category, there are bilinear methods such as RESCALE <ref type="bibr" target="#b5">(Nickel et al., 2011)</ref> and DistMult <ref type="bibr" target="#b6">(Yang et al., 2015)</ref>, both with single embedding for each entity, and similarly SimplE (Kazemi and Poole, 2018) based on Canonical Polyadic (CP) decomposition <ref type="bibr" target="#b23">(Hitchcock, 1927)</ref> except that it dependently embeds entities as source or target with respect to relations and their inverses. While these three methods use three-way multiplication HOLE <ref type="bibr" target="#b24">(Nickel et al., 2016)</ref> suggested circular correlation to create compositional representations.</p><p>Apart from embedding models in Elucidian domain, ComplEx <ref type="bibr" target="#b7">(Trouillon et al., 2016)</ref> noted the capability of complex space to capture anti-symmetric relations. Further, ComplEx-N3 <ref type="bibr" target="#b9">(Lacroix et al., 2018)</ref> utilized reciprocal learning and a new regularization method to get much better results. In order to infer relations such as (anti-)symmetry and inversion, RotatE <ref type="bibr" target="#b2">(Sun et al., 2019)</ref> defined relations as rotations from source to target entity on a single complex plane, and QuatE <ref type="bibr" target="#b10">(ZHANG et al., 2019)</ref> generalized these rotations to two planes by quaternions.</p><p>Contrary to embedding methods, neural network methods obtain expressive representations from pure embeddings using different neural network models. More specifically, NTN <ref type="bibr" target="#b11">(Socher et al., 2013)</ref> first proposed a simple feed-forward neural tensor network. Later, ConvE <ref type="bibr" target="#b13">(Dettmers et al., 2018)</ref> got effective results, using a Convolutional Neural Network (CNN) on interactive 2d reshaped embeddings. Recently, inspired by ConvE, InteractE <ref type="bibr" target="#b18">(Vashishth et al., 2020a)</ref> increased the number of interactions between relation and entity resulting in significant improvements. Additionally, HypER <ref type="bibr" target="#b15">(Bala?evi? et al., 2019)</ref>, based on CNNs with relation-specific filter weights, and TuckER  based on Tucker decomposition <ref type="bibr" target="#b26">(Tucker, 1966)</ref> with a trainable core tensor, are tensor decomposition methods that utilize multi-task learning to share information. Furthermore, Graph Neural Network (GNN) models such as R-GCN <ref type="bibr" target="#b12">(Schlichtkrull et al., 2018)</ref>, and COMPGCN <ref type="bibr" target="#b27">(Vashishth et al., 2020b)</ref>, take advantage of graph structure and entities neighborhood to obtain better representations. Similar to GNN models, A2N <ref type="bibr" target="#b14">(Bansal et al., 2019)</ref> also represents entities based on query-conditioned bilinear attention on their graph neighbors.</p><p>Recently, Transformers <ref type="bibr" target="#b20">(Vaswani et al., 2017)</ref> have also been popular for KG completion. KG-BERT  fine-tunes BERT pre-trained language model <ref type="bibr" target="#b28">Devlin et al. (2019)</ref> to predict a triple plausibility. CoKE  also tackles KG completion, as Masked Language Model (MLM) task, using a stack of Transformer blocks as encoder. In order to capture graph context, HittER <ref type="bibr">Chen et al. (2021)</ref> utilizes two levels of Transformer blocks, one to provide relationdependent embeddings for an entity's neighbors, the other to aggregate their information.</p><p>Out of the above main categories, some other methods such as MINERVA <ref type="bibr" target="#b29">(Das et al., 2018)</ref>, a reinforcement learning model, and DRUM <ref type="bibr" target="#b30">(Sadeghian et al., 2019)</ref>, a rule-based model, are proposed and reached interesting results.</p><p>To the best of our knowledge, MuRP  and ATTH <ref type="bibr" target="#b4">(Chami et al., 2020)</ref> are the only embedding-based methods that concerned low-dimensional embeddings. Both of these methods encode KGs in hyperbolic space due to its efficiency for embedding hierarchical data. However, despite their promising results in low-dimensions, their best results are still achieved in highdimensions. In addition, their performances oscillate between different datasets depending on the number of hierarchical relations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head><p>Despite the recent improvements on low dimensional KG embeddings, the considerable gap between the effectiveness of models in low dimensions and high dimensions has currently remained. This gap inspired us to exploit multi-head self-attention mechanism of Transformers as a general poweful method to effectively encode KGs in notably low dimensions and solve the link prediction problem. In the following, we will elaborate on our model, SAttLE. In the meantime, we briefly introduce the formulation of KG completion and the main notation we will use for the rest of this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Background and Problem Formulation</head><p>Let V and R represent the set of all entities and relations respectively. In addition, a triple is represented as (s, r, t), where s, t ? V are the source and target entities and r ? R is the corresponding relation between them. A triple is true if it exists in a world or false either. A knowledge graph KG is a subset of all possible true triples. Formally, each entity and relation in KG is associated with an embedding vector.</p><p>Link prediction In the context of KGs with relational data, the purpose of link prediction is to find true triples which are not observed in one KG and add them to it. Concretely, given a triple (s, r, t) that is not in the KG, the problem is to validate if it is true or not.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Self-attention</head><p>Recently, attention mechanisms have been widely used in deep learning studies especially in the domain of Natural Language Processing (NLP). The whole idea is to attend to different features of data based on their relative importance. Besides, one common and effective method is to permit input data itself to be the basis of attention which is called self-attention, first proposed in the Transformer model <ref type="bibr" target="#b20">(Vaswani et al., 2017)</ref>, and is applied to solve various problems and provides promising improvements <ref type="bibr" target="#b28">(Devlin et al., 2019;</ref><ref type="bibr" target="#b31">Dosovitskiy et al., 2021)</ref>. We desired to apply self-attention to our task since it is proved to be able to effectively capture dependencies and interactions within a sequence and produce highly expressive feature representations <ref type="bibr" target="#b21">(Shen, 2019;</ref><ref type="bibr" target="#b22">Likhosherstov et al., 2021)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">SAttLE</head><p>In summary, SAttLE takes the following steps: (1) assigns embedding vectors to each entity and relation in KG, (2) considers pair (s, r) of each triple (s, r, t) as the input sequence and feeds to a Transformer encoder, (3) generates highly expressive representations for r and s of the input sequence in low-dimensions, (4) scores t as the target of the corresponding triple based on these representations, and (5) converts the score of t to probability by applying a logistic sigmoid to its computed score. Details of the above steps are explained in the following.</p><p>The first step is to map each entity v ? V to an embedding vector e v ? R de and each relation r ? R to an embedding vector e r ? R dr . To preserve the compatibility of tensor calculus in our model, both relations and entities have a same dimensionality of embeddings (DoE); It means that we consider d e = d r = d.</p><p>Following the original method of <ref type="bibr" target="#b20">Vaswani et al. (2017)</ref> for each triple (s, r, t) where s has embedding e s ? R d and r has embedding e r ? R d , the embedding pair (e s ; e r ) is constructed as query Q ? R 2d , key K ? R 2d , and value V ? R 2d . The attention matrix A sr ? R 2x2 is computed as:</p><formula xml:id="formula_0">A sr = softmax( QK T ? d k )<label>(1)</label></formula><p>Finally, the output representation matrix H ? R 2d for the source and relation would be:</p><formula xml:id="formula_1">H = A sr V<label>(2)</label></formula><p>Attention matrix A sr indicates how much should the source entity focus on its own information against the information of the relation and vice versa. As noted by <ref type="bibr" target="#b22">Likhosherstov et al. (2021)</ref>, we can interpret A sr as a query-dependent linear projection that applies to the input and propagates mutual information between input elements through itself. Thus, we found it an efficient way to fuse information of the entity and the relation.</p><p>Multi-head attention Considering the importance of the attention matrix A sr in our model, multihead attention is the key to gaining expressive representations. Intuitively, multiple linear projections apply to query, key, and value by different sets of parameters resulting in multiple queries, keys, and values. Self-attention mechanism is conducted over each set of query, key, and value, then final value vectors are concatenated as: </p><formula xml:id="formula_2">MultiH(Q, K, V ) = [H 1 , . . . , H h ]W O (3) where H i = softmax( Q i K T i ? d k )V i In the above equation Q i = QW Q i , K i = KW K i , and V i = V W V i where W Q i ? R d?d k , W K i ? R d?d k , W V i ? R d?dv ,</formula><formula xml:id="formula_3">H o = F F N (MultiH(Q, K, V )) (4) where F F N (x) = max(0, xW 1 + b 1 )W 2 + b 2 (5) where W 1 ? R d?d h , b 1 ? R d h , W 2 ? R d h ?d , and b 2 ? R d are learnable parameters.</formula><p>Unlike selfattention, the token-wise feed-forward transforms all elements independently without considering their interactions. Noteworthy, <ref type="bibr" target="#b32">Yun et al. (2020)</ref> proves that the value-mapping ability of the tokenwise feed-forward in combination with self-attention makes Transformers universal approximators.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Decoding</head><p>So far, we have just efficiently represented the source and the relation in low-dimensional space. In order to effectively engage the information embedded in the target entity and score the triple (s, r, t), we will propose two kinds of strategies. It is highly important to state that we chose these methods since they guarantee the model effectiveness as well as preserving its simplicity. However, one could stack any other decoding method to our powerful general decoder get better results. Here we select simple decoding methods and leave the others for interested researchers in future works.</p><p>In the following, we present our approaches to score each triple (s, r, t) and investigate its validation. For the following scoring functions, consider that H o = (? s ;? r ) is the encoder output representation and? s ? R d and? r ? R d are the output representations of the s and r respectively.</p><p>TwoMult First, inspired by the method proposed by <ref type="bibr" target="#b6">(Yang et al., 2015)</ref> we make use of a simple decoding method. Intuitively, Given a triple (s, r, t), we take the representation of the relation? r and naively compute its two-way inner product with the embedding of the target entity e t . Then the scoring function would be:</p><formula xml:id="formula_4">?(s, r, t) =? T r e t<label>(6)</label></formula><p>For the rest of this paper, we refer to this method of scoring as TwoMult standing for Two-way Multiplication. Despite its simplicity, we suppose that TwoMult can be an efficient way of decoding the embedded features. The logic behind it is that the model effectively embeds the information of the source entity into the relation representation in an implicit manner. As a result, the relation output representation would be descriptive enough to utilize solely. Our experimental results confirm this intuition.</p><p>Note that we also examined using the output representation of the source entity instead of the relation. Interestingly, the MRR metric, see definition in Appendix A.3, reduced by almost 1.5 percent by this replacement. Reasonably, we found that in our model the output representation of the relation carries more important information than the source entity.</p><p>Tucker Recently, the state-of-the-art tensor decomposition model TuckER  has gained promising results in link prediction on KGs. Considering a KG represented as a binary tensor, TuckER uses Tucker decomposition <ref type="bibr" target="#b26">(Tucker, 1966)</ref> to compute a validation score for each triple as:</p><formula xml:id="formula_5">?(s, r, t) = W c ? 1 e s ? 2 e r ? 3 e t<label>(7)</label></formula><p>where W c ? R de?dr?de is the core tensor of the decomposition which can be learned and ? n indicates n-mode tensor product. In SAttLE we reuse the original TuckER in the following form with some changes in the model configuration (check Appendix A.1 for details):</p><formula xml:id="formula_6">?(s, r, t) = W c ? 1?s ? 2?r ? 3 e t<label>(8)</label></formula><p>and W c ? R d?d?d . It is worthwhile to state that, the number of core tensor free parameters in the original TuckER scales exponentially with the size of embeddings. While, our embedding vectors are low-dimensional so even the core tensor does not impose too many free parameters.</p><p>Eventually, to validate a triple (s, r, t) we compute the probability of t as the target of pair (s, r) by applying a logistic sigmoid function to its score computed by one of the mentioned decoding approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Training</head><p>At inference time given a triple (s, r, t) we expect the model to score true target t high among other entities for query (s, r, ?) and score true source s high among other entities for query <ref type="bibr">(?, r, t)</ref>. In order to satisfy this expectation, we train our model using reciprocal learning <ref type="bibr" target="#b9">(Lacroix et al., 2018)</ref>. Thus for each triple (s, r, t) we add its inverse (t, r ?1 , s) to the dataset. Also, we utilize 1-N scoring method <ref type="bibr" target="#b13">(Dettmers et al., 2018)</ref> and score each pair (s, r), containing the inverse triples as well, with all entities as target t. We optimize with Binary Cross-Entropy (BCE) loss to enforce model score observed (valid) triples higher than others. Accordingly, for one sample triple (s, r, t) we have:</p><formula xml:id="formula_7">L = ? 1 |V| t ? ?V y t ? log p t ? + (1 ? y t ? ) log (1 ? p t ? )<label>(9)</label></formula><p>where p t ? = ?(?(s, r, t ? )),</p><formula xml:id="formula_8">and y t ? = 1 if t ? = t 0 otherwise</formula><p>in our case ?(.) is the logistic sigmoid function, and V is the set of all entities. Also, we think that using an advanced alternative of BCE-loss like the OSM (One-Sided Margin) loss proposed by   <ref type="bibr" target="#b33">Karimi et al. (2022)</ref>, which can guarantee the width of the prediction margin, could improve our performance. This could be pursue in future researches.</p><p>Furthermore, we observed that adding L2 regularization weakens our model performance. However, we anticipate adding other types of regularization approaches such as N3 regularization <ref type="bibr" target="#b9">(Lacroix et al., 2018)</ref> would be a good practice that can make notable improvements. Here we do not add any regularization and let it as an opportunity for future works.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>Here we compare SAttLE with the recent state-of-the-art models on two most commonly used benchmark datasets for link prediction to verify its effectiveness. Moreover, we also study the influence of the main components of our model on its performance. Check Appendix A.1 for model setups and Appendix A.2 for hyper-parameter setups. We make our implementation publicly available.</p><p>Datasets We evaluate SAttLE on FB15k-237 <ref type="bibr" target="#b34">(Toutanova et al., 2015)</ref> and WN18RR <ref type="bibr" target="#b13">(Dettmers et al., 2018)</ref> which are subsets of FB15k and WN18, both by <ref type="bibr" target="#b0">Bordes et al. (2013)</ref>, and are more challenging to infer since they do not have their supersets' problem of train dataset leakage to test and validation. Consequently, we do not experiment on FB15k and WN18 since it is worthless to make comparisons based on them as <ref type="bibr" target="#b13">Dettmers et al. (2018)</ref> noted. Statistics of the datasets are summarized in <ref type="table" target="#tab_1">Table 1</ref>.</p><p>Baseline We compared our method against some strong baselines and the current state-of-the-art models of different categories. From neural network approaches in Elucidian space we selected ConvE, A2N, and two Transformer-based models CoKE and HittER. Also we take TuckER and Hy-pER as tensor factorization models in the same space. RotatE and ComplEx are chosen as complex space methods. Finally from hyperbolic space approaches MuRP, ATTH are considered.</p><p>Evaluation Protocol We evaluate our model for link prediction using two standard metrics which are commonly used: Mean Reciprocal Rank (MRR), and Hits@k ratios both with filtered setting <ref type="bibr" target="#b0">(Bordes et al., 2013)</ref>. More information is provided in Appendix A.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head><p>In order to evaluate the performance of our approach, we challenged SAttLE against some powerful state-of-the-art models. The empirical results on FB15k-237 and WN18RR are summarized in  Obviously, on both datasets HittER has achieved the best performance. Actually, HittER is a powerful model composed of two transformer blocks to produce relation-dependent informaton for neighbors and capture graph structure. However, these two blocks impose two many parameters and increase time and memory complexity considerably. On the other hand, HittER and CoKE both use multiple encoder blocks same as <ref type="bibr" target="#b20">(Vaswani et al., 2017)</ref> which makes their complexity issue severe.</p><p>Clearly, on FB15k-237, TuckER is our closest competitor. Multi-task learning by sharing knowledge across relations makes TuckER effective on datasets with relatively more relations. However, on datasets like WN18RR with more entities and small number of relations has low performance simultaneously. In contrast, self-attention in SAttLE captures dependencies and interactions between both entities and relations; Therefore, it performs well on both datasets with lots of entities or lots of relations. On the other hand, number of free parameters in SAttLE-Twomult grows linearly with respect to the dimension of the embeddings while it grows exponentially for TuckER because of its core tensor.</p><p>QuatE has the closets results to ours on WN18RR since it efficiently models such datasets with numerous symmetry relations. On the other hand, WN18RR has lots of hierarchical relations that explains the good results of ATTH and MURP on this dataset using embeddings in the hyperbolic domain. However, their performance decrease on FB15k-237 with fewer hierarchical relations compared to WN18RR. In contrast, SAttLE makes no assumptions about the data making it efficiently applicable on different datasets.</p><p>Lastly, The main goal of this work is to decrease the dimensionality of embeddings while retaining the model effectiveness. The empirical results in <ref type="table" target="#tab_2">Table 2</ref> clearly demonstrate how notably efficient SAttLE is with embeddings in comparably lower dimensions than other methods.</p><p>Considering both datasets, SAttLE outperforms or performs competitively with HittER, CoKE, QuatE, TuckER, and ATTH as the five best state-of-the-arts which have respectively 3.2x, 2.56x, 4x, 2x, and 5x, or in average 3.35x, greater number of dimensions. Consequently, SAttLE is highly expressive and successfully alleviates the scalability problem for huge KGs with a vast number of entities and relations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Ablation Study</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1">Effect of the number of attention-heads</head><p>Using self-attention to obtain powerful representations by absorbing the information interaction and dependencies between the entities and relations is the core idea of our model. We discussed that multi-head attention is beneficial to produce representations in different subspaces and encode different dependencies and relationships by attending to each representation space.</p><p>Previously HittER and CoKE have used multiple encoder blocks with low number of attention heads same as the original Transfomer. However, we found that a sufficiently large number of heads results in more expressiveness than using multiple encoder blocks. More detailed comparison in Section 5.2.1.</p><p>The outcomes in <ref type="figure" target="#fig_0">Figure 1(a)</ref> clearly confirm our hypothesis that enhancing the number of heads offers better representations and makes advantages without overfitting. Meanwhile, <ref type="figure" target="#fig_0">Figure 1</ref> illustrates how linearly the number of Nonembedding Free Parameters (NFP) grows by increasing the number of attention heads. In consequence, we make a trade-off and choose 64 heads since it leads to desirable performance while avoids enlarging the model. As declared in <ref type="figure" target="#fig_0">Figure 1</ref>, we find that self-attention with small number of heads and negligible number of free parameters can also offer great advantages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2">Effect of dimensionality of embeddings</head><p>We have also challenged SAttLE in very low dimensions to demonstrate its great scalability to very big KGs. <ref type="table" target="#tab_5">Table 3</ref> shows SAttLE performance in different dimensionality of embeddings. Notably, in d = 64 SAttLE surpasses most of our competitors. The model efficiency is also considerable in d = 32. Consequently, SAttLE is so applicable and expressive when numerous entities or relations make memory and computation bottlenecks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.3">Effect of the decoding methods</head><p>SAttLE implicitly encodes the information of the source entity in the relation before decoding. In consequence, our experiments in <ref type="table" target="#tab_2">Table 2</ref> demonstrate that the simple TwoMult method interestingly performs quite well and reduces risk of overfitting. Especially, TwoMult scoring method is powerful and useful when a considerably low memory rules decision making on model architecture since it adds no more parameters.</p><p>As discussed before, SAttLE is easily adaptive with different decoding methods. <ref type="table" target="#tab_5">Table 3</ref> shows that applying a more complex decoding method like Tucker would make considerable improvements in very low dimensions, i.e. d = 32.</p><p>However, SAttLE performance with Tucker decoder degrades in d = 100. We think a suitable regularization method or just a wider hyper-parameter search could refine this behavior. In contrast, TwoMult simplicity brings the advantage of better generalization in d = 100 to attain great results.  As discussed before, compared to previous Transformer-based models SAttLE utilizes single encoder block with large number of attention heads rather than multiple encoders blocks. <ref type="table" target="#tab_6">Table 4</ref> illustrates the parameter efficiency of SAttLE based on the number of Nonembedding Free Parameters (NFP) and Embedding Free Parameters (EFP).</p><p>Obviously, our model is remarkably smaller than CoKE and HittER with 75% and 87% lower #NFP on FB15k-237, also 82% and 89% #NFP on WN18RR respectively. This confirms our claim that instead of multiple encoder blocks and low number of heads, as used in CoKE and HittER, we can decrease model complexity and gain competitive performance by one encoder block and increasing the number of heads.</p><p>On the other hand, as discussed before, we have considerably lower #EFP because of lower dimensionality of embeddings. Especially, these two parameter reductions are more important and significant in case of immense KGS with numerous entities and relations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Convergence Behavior</head><p>KGs are intrinsically so dynamic, as a result in lots of real world applications multiple models should be trained based on the amount of data changes in a period of time. So the convergence of models developed for link prediction is important. <ref type="figure" target="#fig_1">Figure 2</ref> shows the convergence behavior of SAttLE in different DoEs on FB15k-237. Obviously, with small number of epochs, SAttLE achieves sufficiently desirable performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion and Future Works</head><p>We introduce SAttLE, a model powered by self-attention to embed KGs in low dimensions. Our Transformer-based method, efficiently models dependencies and interactions between entities and relations using multi-head self-attention mechanism to produce highly expressive representations. We utilized simple scoring methods to decode the representations. Regarding the empirical results on two standard link prediction datasets, SAttLE requires embeddings in significantly lower dimensions than the recent state-of-the-art models to achieve favorably comparable results on FB15k-237 and WN18RR.</p><p>There would be several extensions of our work that future researchers can look into them. The first is to take advantage of our encoder expressive representations and attach more efficient decoding methods. We did not use any regularization methods, so adding an appropriate regularization method such as the one proposed by <ref type="bibr" target="#b9">Lacroix et al. (2018)</ref> could lead to considerable improvements. Lastly, we believe that one could just inclusively fine-tune the vast number of our model hyper-parameters, to explore a proper set and get better results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Experimental Details</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Model Setups</head><p>Following the settings of <ref type="bibr" target="#b20">Vaswani et al. (2017)</ref> for the original Transformer model, we use dropout before and after the multi-head attention block, after applying softmax in each self-attention block, and also after the position feed-forward block with rates of do enc , do mha , do sdp , and do pf f respectively.</p><p>Moreover, compared to the original setting we remove the first fully-connected layer before the multi-head attention block since we observed that it slightly degrades the model performance. We conjecture that in our case, this projection layer might cause a non-optimal information fusion between the input relation and entity due to the difference between their embedding spaces. By the same intuition, the layer normalization before the multi-head attention block is respectively replaced by two separate batch normalizations, one for relations and one for entities.</p><p>As the last configuration, we remove the layer normalization after the position-wise feed-forward when Tucker is used as the scoring function. Also, compared to the setting of  for TuckER, we preserve the first batch normalization on the source entity and remove all other batch normalizations and dropouts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Hyper-parameter Setups</head><p>We implemented our model in PyTorch <ref type="bibr" target="#b35">(Paszke et al., 2019)</ref> and trained it on a single GPU. To optimize our model we use <ref type="bibr">Adam (Kingma and Ba, 2015)</ref>. It is noteworthy to state that inheriting the characteristics of the original Transformers, our model has lots of hyper-parameters. Consequently, we could not search for all hyper-parameters at once. Instead, we fixed the number of attention heads to 64 as discussed in Section 5.1.1 and searched for other hyper-parameters based on the validation sets performance according to MRR in the following manner.</p><p>We find all the hyper-parameters of the model with random search. First we tuned training parameters and fixed them for the rest of tunings. We chose batch size from {128, 512, 1024, 2048}, learning rate from {0.0001, 0.0005, 0.001}, learning rate decay from 1, 0.995 with fixed step size of 2 epochs. Second, we tuned mode architecture hyper-parameters except number of attention heads where d k and d v varied between {32, 50, 64} and d h between {100, 512, 1024, 2048}.</p><p>We fixed previous hyper-parameters and tuned dropout rates in {0.1, 0.2, 0.3, 0.4} using Bayesian optimization combined with ASHA <ref type="bibr" target="#b37">(Li et al., 2020)</ref>, an asynchronous aggressive early stopping method. <ref type="table" target="#tab_7">Table 5</ref> lists the full set hyper-parameters we chose to get the reported results of this paper. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Evaluation Protocol</head><p>To evaluate link prediction in KGs, we use two standard metrics which are commonly used: Mean Reciprocal Rank (MRR), and Hits@k ratios. Both of these metrics are basically based on the ranking of the ground truth triple score against all of the corrupted triples. Specifically, we use filtered setting <ref type="bibr" target="#b0">(Bordes et al., 2013)</ref>, so to compute left rank, we first perturb target entity of a triple with all entities that don't make a corrupted triple which already exists in train, test, or validation sets except the true triple itself. Then, all corrupted triples are scored and sorted in descending order and the rank of the true triple is saved. We follow the same procedure to compute right rank of each triple by perturbing source entity. Take note that, for our results to be fair for comparison we use the random protocol proposed by <ref type="bibr" target="#b38">Sun et al. (2020)</ref>, as a result, the true triple is randomly placed among corrupted triples before scoring and sorting. MRR is the average of the inverse of the mean ranks (over left and right ranks) of the true triples. Hits@k ratios are the percentage of times a true triple is ranked lower than or equal to k.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>(a) MRR of SAttLE-TwoMult on FB15k-237 and (b) the Number of Nonembedding Free Parameters (NFP) of model for different number of attention heads in fixed DoE of 100 and #heads ? {4, 8, 16, 32, 64, 128}. Sufficiently large #heads makes improvements without adding lots of parameters. Even with low #heads and negligible free parameters SAttLE is so effective.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Convergence behavior of SAttLE-TwoMult on FB15k-237 for different DoEs where d ? {32, 64, 100}.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>and W O ? R hdv?d are projection matrices and h is the number of heads. In addition, inheriting the Transformer model of Vaswani et al. (2017), a token-wise feed-forward is applied to produce the final output representation matrix H o for each triple.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Datasets statistics.</figDesc><table><row><cell>Datasets</cell><cell cols="2">(|V|) (|R|) Train</cell><cell>Valid</cell><cell>Test</cell></row><row><cell>FB15k-237</cell><cell cols="4">14541 237 272115 17535 20466</cell></row><row><cell>WN18RR</cell><cell>40943</cell><cell>11 86835</cell><cell>3034</cell><cell>3134</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Link prediction results on FB15k-237 and WN18RR. Results of ? are taken from<ref type="bibr" target="#b2">(Sun et al., 2019)</ref>. Other results are taken from original papers and their own experimental setups. DoE stands for Dimensionality of Embeddings. The best results are in bold and the second bests are underlined.</figDesc><table><row><cell></cell><cell>DoE</cell><cell></cell><cell cols="2">FB15k-237</cell><cell></cell><cell></cell><cell cols="2">WN18RR</cell><cell></cell></row><row><cell></cell><cell></cell><cell>MRR</cell><cell>Hits@1</cell><cell>Hits@3</cell><cell>Hits@10</cell><cell>MRR</cell><cell>Hits@1</cell><cell>Hits@3</cell><cell>Hits@10</cell></row><row><cell cols="2">ComplEx ? (Trouillon et al., 2016) 400</cell><cell>.247</cell><cell>.158</cell><cell>.275</cell><cell>.428</cell><cell>.44</cell><cell>.41</cell><cell>.46</cell><cell>.51</cell></row><row><cell>ConvE (Dettmers et al., 2018)</cell><cell>200</cell><cell>.325</cell><cell>.237</cell><cell>.356</cell><cell>.501</cell><cell>.43</cell><cell>.40</cell><cell>.44</cell><cell>.52</cell></row><row><cell>A2N (Bansal et al., 2019)</cell><cell>512</cell><cell>.317</cell><cell>.232</cell><cell>.348</cell><cell>.486</cell><cell>.45</cell><cell>.42</cell><cell>.46</cell><cell>.51</cell></row><row><cell>MURP (Balazevic et al., 2019)</cell><cell>200</cell><cell>.335</cell><cell>.243</cell><cell>.367</cell><cell>.518</cell><cell>.481</cell><cell>.44</cell><cell>.495</cell><cell>.566</cell></row><row><cell>RotatE (Sun et al., 2019)</cell><cell cols="2">1000 .338</cell><cell>.241</cell><cell>.375</cell><cell>.533</cell><cell>.476</cell><cell>.428</cell><cell>.492</cell><cell>.571</cell></row><row><cell>HypER (Bala?evi? et al., 2019)</cell><cell>200</cell><cell>.341</cell><cell>.252</cell><cell>.376</cell><cell>.52</cell><cell>.465</cell><cell>.436</cell><cell>.477</cell><cell>.522</cell></row><row><cell>QuatE (ZHANG et al., 2019)</cell><cell>400</cell><cell>.348</cell><cell>.248</cell><cell>.382</cell><cell>.55</cell><cell>.488</cell><cell>.438</cell><cell>.508</cell><cell>.582</cell></row><row><cell>ATTH (Chami et al., 2020)</cell><cell>500</cell><cell>.348</cell><cell>.252</cell><cell>.384</cell><cell>.54</cell><cell>.486</cell><cell>.443</cell><cell>.499</cell><cell>.573</cell></row><row><cell>TuckER (Balazevic et al., 2019)</cell><cell>200</cell><cell>.358</cell><cell>.266</cell><cell>.394</cell><cell>.544</cell><cell>.47</cell><cell>.444</cell><cell>.482</cell><cell>.526</cell></row><row><cell>CoKE (Wang et al., 2019)</cell><cell>256</cell><cell>.364</cell><cell>.272</cell><cell>.400</cell><cell>.549</cell><cell>.484</cell><cell>.450</cell><cell>.496</cell><cell>.553</cell></row><row><cell>HittER Chen et al. (2021)</cell><cell>320</cell><cell>.373</cell><cell>.279</cell><cell>.409</cell><cell>.558</cell><cell>.503</cell><cell>.462</cell><cell>.516</cell><cell>.584</cell></row><row><cell>SAttLE-TwoMult</cell><cell>100</cell><cell>.360</cell><cell>.268</cell><cell>.396</cell><cell>.545</cell><cell>.491</cell><cell>.454</cell><cell>.508</cell><cell>.558</cell></row><row><cell>SAttLE-Tucker</cell><cell>64</cell><cell>.358</cell><cell>.266</cell><cell>.394</cell><cell>.541</cell><cell>.476</cell><cell>.442</cell><cell>.490</cell><cell>.54</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 .</head><label>2</label><figDesc>The Dimensionality of Embeddings (DoE) for each model is also reported alongside its name. For ComplEx, Rotate, and QuatE we reported DoE as the sum of all assigned embedding vectors which for ComplEx and Rotate it includes both real and complex components and for QuatE includes real and all quaternion components. To retain the fairness of comparisons, the QuatE results are reported without N3 regularization (check Appendix 7.2 of ZHANG et al.(2019)).</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2</head><label>2</label><figDesc>clearly brightens the great performance of SAttLE against the competitors with significantly lower dimensionality of embeddings. Overall, SAttLE has the second best performance on WN18RR. In addition, SAttLE beats most of the state-of-the-art methods on FB15k-237 except for CoKE and HittER.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Results on FB15k-237 in different DoEs for our two decoders: TwoMult and Tucker.</figDesc><table><row><cell></cell><cell>DoE (d)</cell><cell>MRR</cell><cell>Hits@1 Hits@3 Hits@10</cell></row><row><cell></cell><cell>100</cell><cell cols="2">.360 .268 .396 .545</cell></row><row><cell>SAttLE-TwoMult</cell><cell>64</cell><cell cols="2">.355 .264 .389 .535</cell></row><row><cell></cell><cell>32</cell><cell cols="2">.335 .247 .367 .507</cell></row><row><cell></cell><cell>100</cell><cell cols="2">.357 .265 .395 .541</cell></row><row><cell>SAttLE-Tucker</cell><cell>64</cell><cell cols="2">.358 .266 .393 .541</cell></row><row><cell></cell><cell>32</cell><cell cols="2">.340 .252 .372 .513</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Parameter efficiency on FB15k-237 and WN18RR.</figDesc><table><row><cell></cell><cell cols="2">FB15k-237</cell><cell cols="2">WN18RR</cell></row><row><cell></cell><cell>#NFP</cell><cell>#EFP</cell><cell>#NFP</cell><cell>#EFP</cell></row><row><cell>CoKE</cell><cell cols="4">6.16M 3.84M 6.51M 10.49M</cell></row><row><cell>HittER</cell><cell cols="4">11.20M 4.80M 10.89M 13.11M</cell></row><row><cell cols="5">SAttLE-TwoMult 1.50M 1.50M 1.14M 4.10M</cell></row><row><cell>5.2 Discussion</cell><cell></cell><cell></cell><cell></cell></row><row><cell>5.2.1 Parameter efficiency of SAttLE</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Hyper-parameters settings for the reported results in this paper.DatasetsDecoder d doenc do mha do pf f do sdp d h bs dr lr ls d k dv #heads</figDesc><table><row><cell></cell><cell></cell><cell>100</cell><cell>0.4</cell><cell>0.3</cell><cell>0.2</cell><cell>0.1</cell><cell></cell><cell></cell></row><row><cell></cell><cell>TwoMult</cell><cell>64</cell><cell>0.2</cell><cell>0.3</cell><cell>0.2</cell><cell>0.1</cell><cell></cell><cell></cell></row><row><cell>FB15k-237</cell><cell>Tucker</cell><cell>32 100 64</cell><cell>0.1 0.4 0.4</cell><cell>0.1 0.2 0.2</cell><cell>0.1 0.3 0.2</cell><cell>0.1 0.1 0.1</cell><cell cols="2">2048 2048 0.995</cell><cell>0.001 0.1 32 50 64</cell></row><row><cell></cell><cell></cell><cell>32</cell><cell>0.1</cell><cell>0.2</cell><cell>0.1</cell><cell>0.1</cell><cell></cell><cell></cell></row><row><cell>WN18RR</cell><cell cols="2">TwoMult 100 Tucker 64</cell><cell>0.3 0.3</cell><cell>0.4 0.4</cell><cell>0.4 0.4</cell><cell cols="2">0.1 100 1024 0.1</cell><cell>1</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Code available at https://github.com/Peyman-Bi/SAttLE</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Translating embeddings for modeling multi-relational data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>Garcia-Duran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oksana</forename><surname>Yakhnenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">26</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Knowledge graph embedding by translating on hyperplanes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlin</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">28</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Rotate: Knowledge graph embedding by relational rotation in complex space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi-Hong</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian-Yun</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Multi-relational poincar? graph embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivana</forename><surname>Balazevic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Lowdimensional hyperbolic knowledge graph embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ines</forename><surname>Chami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adva</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Da-Cheng</forename><surname>Juan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frederic</forename><surname>Sala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujith</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>R?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6901" to="6914" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A three-way model for collective learning on multi-relational data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximilian</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans-Peter</forename><surname>Volker Tresp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kriegel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="809" to="816" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Embedding entities and relations for learning and inference in knowledge bases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bishan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Complex embeddings for simple link prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Th?o</forename><surname>Trouillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Welbl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ric</forename><surname>Gaussier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Bouchard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2071" to="2080" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Simple embedding for link prediction in knowledge graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Seyed Mehran Kazemi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Poole</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Canonical tensor decomposition for knowledge base completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timoth?e</forename><surname>Lacroix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Obozinski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2863" to="2872" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Quaternion knowledge graph embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lina</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Reasoning with neural tensor networks for knowledge base completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="926" to="934" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Modeling relational data with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Schlichtkrull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rianne</forename><surname>Bloem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European semantic web conference</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="593" to="607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Convolutional 2d knowledge graph embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Dettmers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pasquale</forename><surname>Minervini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pontus</forename><surname>Stenetorp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-second AAAI conference on artificial intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A2n: Attending to neighbors for knowledge graph inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trapit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Da-Cheng</forename><surname>Juan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujith</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4387" to="4392" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Hypernetwork knowledge graph embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivana</forename><surname>Bala?evi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Neural Networks</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="553" to="565" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Kg-bert: Bert for knowledge graph completion. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengsheng</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Luo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1909" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pingping</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songtai</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yajuan</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.02168</idno>
		<title level="m">Coke: Contextualized knowledge graph embedding</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Interacte: Improving convolution-based knowledge graph embeddings by increasing feature interactions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shikhar</forename><surname>Vashishth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumya</forename><surname>Sanyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vikram</forename><surname>Nitin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nilesh</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Partha</forename><surname>Talukdar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="3009" to="3016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Hitter: Hierarchical transformers for knowledge graph embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanxing</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruofei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangfeng</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2021 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Mutual information scaling and expressive power of sequence models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huitao</forename><surname>Shen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.04271</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">On the expressive power of self-attention matrices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valerii</forename><surname>Likhosherstov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krzysztof</forename><surname>Choromanski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Weller</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.03764</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">The expression of a tensor or a polyadic as a sum of products</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hitchcock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Mathematics and Physics</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="164" to="189" />
			<date type="published" when="1927" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Holographic embeddings of knowledge graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximilian</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Rosasco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomaso</forename><surname>Poggio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Tensor factorization for knowledge graph completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivana</forename><surname>Balazevic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">Hospedales</forename><surname>Tucker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5185" to="5194" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Some mathematical notes on three-mode factor analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ledyard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tucker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychometrika</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="279" to="311" />
			<date type="published" when="1966" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Composition-based multirelational graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shikhar</forename><surname>Vashishth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumya</forename><surname>Sanyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vikram</forename><surname>Nitin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Partha</forename><surname>Talukdar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Go for a walk and arrive at the answer: Reasoning over paths in knowledge bases using reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajarshi</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shehzaad</forename><surname>Dhuliawala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manzil</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Vilnis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Durugkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akshay</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Drum: End-toend differentiable rule mining on knowledge graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Sadeghian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammadreza</forename><surname>Armandpour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daisy</forename><forename type="middle">Zhe</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Are transformers universal approximators of sequence-to-sequence functions?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chulhee</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srinadh</forename><surname>Bhojanapalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankit</forename><surname>Singh Rawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sashank</forename><surname>Reddi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjiv</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Introducing one sided margin loss for solving classification problems in deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Karimi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reshad</forename><surname>Zahra Mousavi Kouzehkanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hadi</forename><surname>Hosseini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Asheri</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Representing text for joint embedding of text and knowledge bases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Pantel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hoifung</forename><surname>Poon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pallavi</forename><surname>Choudhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Gamon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 conference on empirical methods in natural language processing</title>
		<meeting>the 2015 conference on empirical methods in natural language processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1499" to="1509" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="8026" to="8037" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A system for massively parallel hyperparameter tuning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liam</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Jamieson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Afshin</forename><surname>Rostamizadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ekaterina</forename><surname>Gonina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Ben-Tzur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moritz</forename><surname>Hardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ameet</forename><surname>Talwalkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Third Conference on Systems and Machine Learning</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A reevaluation of knowledge graph completion methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shikhar</forename><surname>Vashishth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumya</forename><surname>Sanyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Partha</forename><surname>Talukdar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5516" to="5522" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
