<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Capturing Humans in Motion: Temporal-Attentive 3D Human Pose and Shape Estimation from Monocular Video</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Li</forename><surname>Wei</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Information Science</orgName>
								<orgName type="institution">Academia Sinica</orgName>
								<address>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jen-Chun</forename><surname>Lin</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Information Science</orgName>
								<orgName type="institution">Academia Sinica</orgName>
								<address>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tyng-Luh</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Information Science</orgName>
								<orgName type="institution">Academia Sinica</orgName>
								<address>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong-Yuan</forename><forename type="middle">Mark</forename><surname>Liao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Information Science</orgName>
								<orgName type="institution">Academia Sinica</orgName>
								<address>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Capturing Humans in Motion: Temporal-Attentive 3D Human Pose and Shape Estimation from Monocular Video</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>* Both authors contributed equally to this work ? Mark Liao is also a Chair Professor of Providence University</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T19:19+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="figure">Figure 1</ref><p>. By coupling motion continuity attention with hierarchical attentive feature integration, the proposed MPS-Net can achieve more accurate pose and shape estimations (bottom row), when dealing with in-the-wild videos. For comparison, the results (top row) obtained by TCMR [6], the state-of-the-art video-based 3D human pose and shape estimation method, are included.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>Learning to capture human motion is essential to 3D human pose and shape estimation from monocular video. However, the existing methods mainly rely on recurrent or convolutional operation to model such temporal information, which limits the ability to capture non-local context relations of human motion. To address this problem, we propose a motion pose and shape network (MPS-Net) to effectively capture humans in motion to estimate accurate and temporally coherent 3D human pose and shape from a video. Specifically, we first propose a motion continuity attention (MoCA) module that leverages visual cues observed from human motion to adaptively recalibrate the range that needs attention in the sequence to better capture the motion continuity dependencies. Then, we develop a hierarchical attentive feature integration (HAFI) module to effectively combine adjacent past and future feature representations to strengthen temporal correlation and refine the feature representation of the current frame. By coupling the MoCA and HAFI modules, the proposed MPS-Net excels in estimating 3D human pose and shape in the video. Though conceptually simple, our MPS-Net not only outperforms the state-of-the-art methods on the 3DPW, MPI-INF-3DHP, and Human3.6M benchmark datasets, but also uses fewer network parameters. The video demos can be found at https://mps-net.github.io/MPS-Net/.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Estimating 3D human pose and shape by taking a simple picture/video without relying on sophisticated 3D scanning devices or multi-view stereo algorithms, has important applications in computer graphics, AR/VR, physical therapy and beyond. Generally speaking, the task is to take a single image or video sequence as input and to estimate the parameters of a 3D human mesh model as output. Take, for example, the SMPL model <ref type="bibr" target="#b23">[24]</ref>. For each image, it needs to estimate 85 (including pose, shape, and camera) parameters, which control the 6890 vertices that form the full 3D mesh of a human body <ref type="bibr" target="#b23">[24]</ref>. Despite recent progress on 3D human pose and shape estimation, it is still a frontier challenge due to depth ambiguity, limited 3D annotations, and complex motion of non-rigid human body <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21]</ref>.</p><p>Different from 3D human pose and shape estimation from a single image <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b30">31]</ref>, estimating it from monocular video is a more complex task <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b33">34]</ref>. It needs to not only estimate the pose, shape and camera parameters of each image, but also correlate the continuity of human motion in the sequence. Although existing single image-based methods can predict a reasonable output from a static image, it is difficult for them to estimate temporally coherent and smooth 3D human pose and shape in the video sequence due to the lack of modeling the continuity of human motion in consecutive frames. To solve this problem, several methods have recently been proposed to extend the single image-based methods to the video cases, <ref type="figure">Figure 2</ref>. Visualization of the attention map generated by the selfattention module <ref type="bibr" target="#b37">[38]</ref> in 3D human pose and shape estimation. The visualization shows that the attention map is easy to focus attention on less correlated temporal positions (i.e., far apart frames with very different action poses) and lead to inaccurate 3D human pose and shape estimation (see frame It). In the attention map, red indicates a higher attention value, and blue indicates a lower one.</p><p>which mainly rely on recurrent neural network (RNN) or convolutional neural network (CNN) to model temporal information (i.e., continuity of human motion) for coherent predictions <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b24">25]</ref>. However, RNNs and CNNs are good at dealing with local neighborhoods <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b37">38]</ref>, and the models alone may not be effective for learning long-range dependencies (i.e., non-local context relations) between feature representations to describe the relevance of human motion. As a result, there is still room for improvement for existing video-based methods to estimate accurate and smooth 3D human pose and shape (see <ref type="figure">Figure 1</ref>).</p><p>To address the aforementioned issue, we propose a motion pose and shape network (MPS-Net) for 3D human pose and shape estimation from monocular video. Our key insights are two-fold. First, although a self-attention mechanism <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b37">38]</ref> has recently been proposed to compensate (i.e., better learn long-range dependencies) for the weaknesses of recurrent and convolutional operations, we empirically find that it is not always good at modeling human motion in the action sequence. Because the attention map computed by the self-attention module is often unstable, which is easy to focus attention on less correlated temporal positions (i.e., far apart frames with very different action poses) and ignore the continuity of human motion in the action sequence (see <ref type="figure">Figure 2</ref>). To this end, we propose a motion continuity attention (MoCA) module to achieve the adaptability to diverse temporal content and relations in the action sequence. Specifically, the MoCA module contributes in two points. First, a normalized self-similarity matrix (NSSM) is developed to capture the structure of tem-poral similarities and dissimilarities of visual representations in the action sequence, thereby revealing the continuity of human motion. Second, NSSM is regarded as the a priori knowledge and applied to guide the learning of the self-attention module, which allows it to adaptively recalibrate the range that needs attention in the sequence to capture the motion continuity dependencies. In the second insight, motivated by the temporal feature integration scheme in 3D human mesh estimation <ref type="bibr" target="#b5">[6]</ref>, we develop a hierarchical attentive feature integration (HAFI) module that utilizes adjacent feature representations observed from past and future frames to strengthen temporal correlation and refine the feature representation of the current frame. By coupling the MoCA and HAFI modules, our MPS-Net can effectively capture humans in motion to estimate accurate and temporally coherent 3D human pose and shape from monocular video (see <ref type="figure">Figure 1</ref>). We characterize the main contributions of our MPS-Net as follows:</p><p>? We propose a MoCA module that leverages visual cues observed from human motion to adaptively recalibrate the range that needs attention in the sequence to better capture the motion continuity dependencies. ? We develop a HAFI module that effectively combines adjacent past and future feature representations in a hierarchical attentive integration manner to strengthen temporal correlation and refine the feature representation of the current frame. ? Extensive experiments on three standard benchmark datasets demonstrate that our MPS-Net achieves the state-of-the-art performance against existing methods and uses fewer network parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>3D human pose and shape estimation from a single image. The existing single image-based 3D human pose and shape estimation methods are mainly based on parametric 3D human mesh models, such as SMPL <ref type="bibr" target="#b23">[24]</ref>, i.e., trains a deep-net model to estimate pose, shape, and camera parameters from the input image, and then decodes them into a 3D mesh of the human body through the SMPL model. For example, Kanazawa et al. <ref type="bibr" target="#b16">[17]</ref> proposed an end-to-end human mesh recovery (HMR) framework to regress SMPL parameters from a single RGB image. They employ 3D to 2D keypoint reprojection loss and adversarial training to alleviate the limited 3D annotation problem and make the output 3D human mesh anatomically reasonable. Pavlakos et al. <ref type="bibr" target="#b30">[31]</ref> used 2D joint heatmaps and silhouette as cues to improve the accuracy of SMPL parameter estimation. Similarly, Omran et al. <ref type="bibr" target="#b28">[29]</ref> used a semantic segmentation scheme to extract body part information as a cue to estimate the SMPL parameters. Kolotouros et al. <ref type="bibr" target="#b20">[21]</ref> proposed a self-improving framework that integrates the SMPL parameter regressor and iterative fitting scheme to better estimate 3D human pose and shape. Zhang et al. <ref type="bibr" target="#b40">[41]</ref> designed a pyramidal mesh alignment feedback (PyMAF) loop in the deep SMPL parameter regressor to exploit multi-scale contexts for better mesh-image alignment of the reconstruction.</p><p>Several non-parametric 3D human mesh reconstruction methods <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b34">35]</ref> have been proposed. For example, Kolotouros et al. <ref type="bibr" target="#b21">[22]</ref> proposed a graph CNN, which takes the 3D human mesh template and image embedding (extracted from ResNet-50 <ref type="bibr" target="#b12">[13]</ref>) as input to directly regress the vertex coordinates of the 3D mesh. Moon and Lee <ref type="bibr" target="#b27">[28]</ref> proposed an I2L-MeshNet, which uses a lixel-based 1D heatmap to directly localize the vertex coordinates of the 3D mesh in a fully convolutional manner.</p><p>Despite the above methods are effective for static images, they are difficult to generate temporally coherent and smooth 3D human pose and shape in the video sequence, i.e., jittery, unstable 3D human motion may occur <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b19">20]</ref>.</p><p>3D human pose and shape estimation from monocular video. Similar to the single image-based methods, the existing video-based 3D human pose and shape estimation methods are mainly based on the SMPL model. For example, Kanazawa et al. <ref type="bibr" target="#b17">[18]</ref> proposed a convolution-based temporal encoder to learn human motion kinematics by further estimating SMPL parameters in adjacent past and future frames. Doersch et al. <ref type="bibr" target="#b7">[8]</ref> trained their model on a sequence of 2D keypoint heatmaps and optical flow by combining CNN and long short-term memory (LSTM) network to demonstrate that considering pre-processed motion information can improve SMPL parameter estimation. Sun et al. <ref type="bibr" target="#b33">[34]</ref> proposed a skeleton-disentangling framework, which divides the task into multi-level spatial and temporal subproblems. They further proposed an unsupervised adversarial training strategy, namely temporal shuffles and order recovery, to encourage temporal feature learning. Kocabas et al. <ref type="bibr" target="#b19">[20]</ref> proposed a temporal encoder composed of bidirectional gated recurrent units (GRU) to encode static features into a series of temporally correlated latent features, and feed them to the regressor to estimate SMPL parameters. They further integrated adversarial training strategy that leverages the AMASS dataset <ref type="bibr" target="#b25">[26]</ref> to distinguish between real human motion and those estimated by its regressor to encourage the generation of reasonable 3D human motion. Luo et al. <ref type="bibr" target="#b24">[25]</ref> proposed a two-stage model that first estimates the coarse 3D human motion through a variational motion estimator, and then uses a motion residual regressor to refine the motion estimates. Recently, Choi et al. <ref type="bibr" target="#b5">[6]</ref> proposed a temporally consistent mesh recovery (TCMR) system that uses GRU-based temporal encoders with three different encoding strategies to encourage the network to better learn temporal features. In addition, they proposed a temporal feature integration scheme that combines the output of three temporal encoders to help the SMPL parameter regressor estimate accurate and smooth 3D human pose and shape.</p><p>Despite the success of RNNs and CNNs, both recurrent and convolutional operations can only deal with local neighborhoods <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b37">38]</ref>, which makes it difficult for them to learn long-range dependencies (i.e., non-local context relations) between feature representations in the action sequence. Therefore, existing methods are still struggling to estimate accurate and smooth 3D human pose and shape.</p><p>Attention mechanism. The attention mechanism has enjoyed widespread adoption as a computational module for natural language processing <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b39">40]</ref> and visionrelated tasks <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b38">39]</ref> because of its ability to capture long-range dependencies and selectively concentrate on the relevant subset of the input. There are various ways to implement the attention mechanism. Here we focus on self-attention <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b37">38]</ref>. For example, Vaswani et al. <ref type="bibr" target="#b35">[36]</ref> proposed a self-attention-based architecture called Transformer, in which the self-attention module is designed to update each sentence's element through the entire sentence's aggregated information to draw global dependencies between input and output. The Transformer entirely replaces the recurrent operation with the self-attention module, and greatly improves the performance of machine translation. Later, Wang et al. <ref type="bibr" target="#b37">[38]</ref> showed that self-attention is an instantiation of non-local mean <ref type="bibr" target="#b2">[3]</ref>, and proposed a non-local block for the CNN to capture long-range dependencies. Like the self-attention module proposed in Transformer, the non-local operation computes the correlation between each position in the input feature representation to generate an attention map, and then performs the attentionguided dense context information aggregation to draw longrange dependencies.</p><p>Despite the self-attention mechanism performs well, we empirically find that the attention map computed by the selfattention module (e.g., non-local block) is often unstable, which means that it is easy to focus attention on less correlated temporal positions (i.e., far apart frames with very different action poses) and ignore the continuity of human motion in the action sequence (see <ref type="figure">Figure 2</ref>). In this work, we propose the MoCA module, which extends the learning of the self-attention module by introducing the a priori knowledge of NSSM to adaptively recalibrate the range that needs attention in the sequence, so as to capture motion continuity dependencies. The HAFI module is further proposed to strengthen the temporal correlation and refine the feature representation of each frame through its neighbors. <ref type="figure">Figure 3</ref> shows the overall pipeline of our MPS-Net. We elaborate each module in MPS-Net as follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Temporal encoder</head><p>Given an input video sequence V = {I t } T t=1 with T frames. We first use ResNet-50 <ref type="bibr" target="#b12">[13]</ref> pre-trained by Kolotouros et al. <ref type="bibr" target="#b20">[21]</ref> to extract the static feature of each frame to <ref type="bibr">Figure 3</ref>. Overview of our motion pose and shape network (MPS-Net). MPS-Net estimates pose, shape, and camera parameters ? in the video sequence based on the static feature extractor, temporal encoder, temporal feature integration, and SMPL parameter regressor to generate 3D human pose and shape. <ref type="figure">Figure 4</ref>. A MoCA module. X is shown as the shape of T ? 2048 for 2048 channels. g, ?, ?, and ? denote convolutional operations, ? denotes matrix multiplication, and ? denotes elementwise sum. The computation of softmax is performed on each row. form a static feature representation sequence X = {x t } T t=1 , where x t ? R 2048 . Then, the extracted X is sent to the proposed MoCA module to calculate the temporal feature</p><formula xml:id="formula_0">representation sequence Z = {z t } T t=1 , where z t ? R 2048 . MoCA Module.</formula><p>We propose a MoCA operation to extend the non-local operation <ref type="bibr" target="#b37">[38]</ref> in two ways. First, we introduce an NSSM to capture the structure of temporal similarities and dissimilarities of visual representations in the action sequence to reveal the continuity of human motion. Second, we regard NSSM as the a priori knowledge and combine it with the attention map generated by the non-local operation to adaptively recalibrate the range that needs attention in the action sequence.</p><p>We formulate the proposed MoCA module as follows (see <ref type="figure">Figure 4</ref>). Given the static feature representation se-quence X ? R T ?2048 , the goal of the MoCA operation is to obtain a non-local context response Y ? R T ? 2048 m , which aims to capture the motion continuity dependencies across the whole representation sequence by weighted sum of the static features at all temporal positions,</p><formula xml:id="formula_1">Y = ?([f (X, X), f (?(X), ?(X))])g(X),<label>(1)</label></formula><p>where m is a reduction ratio used to reduce computational complexity <ref type="bibr" target="#b37">[38]</ref>, and it is set to 2 in our experiments. g(?), ?(?), and ?(?) are learnable transformations, which are implemented by using the convolutional operation <ref type="bibr" target="#b37">[38]</ref>. Thus, the transformations can be written as</p><formula xml:id="formula_2">g(X) = XW g ? R T ? 2048 m ,<label>(2)</label></formula><formula xml:id="formula_3">?(X) = XW ? ? R T ? 2048 m ,<label>(3)</label></formula><p>and</p><formula xml:id="formula_4">?(X) = XW ? ? R T ? 2048 m ,<label>(4)</label></formula><p>parameterized by the weight matrices W g , W ? , and W ? ? R 2048? 2048 m , respectively. f (?, ?) represents a pairwise function, which computes the affinity between all positions. We use dot product <ref type="bibr" target="#b37">[38]</ref> as the operation for f , i.e.,</p><formula xml:id="formula_5">f (?(X), ?(X)) = ?(X)?(X) T ,<label>(5)</label></formula><p>where the size of the resulting pairwise function</p><formula xml:id="formula_6">f (?(X), ?(X)) is denoted as R T ? 2048 m ? R 2048 m ?T ? R T ?T ,</formula><p>which encodes the mutual similarity between temporal positions under the transformed static feature representation sequence. Then, the softmax operation is used to normalize it into an attention map (see <ref type="figure">Figure 4</ref>).</p><p>We empirically find that although calculating the similarity in the transformed feature space provides an opportunity for insight into implicit long-range dependencies, it may sometimes be unstable and lead to attention on less <ref type="figure">Figure 5</ref>. A HAFI module. It utilizes the temporal features observed from the past and future frames to refine the temporal feature of the current frame zt in a hierarchical attentive integration manner. Where ? denotes matrix multiplication. correlated temporal positions (see <ref type="figure">Figure 2</ref>). To this end, we introduce NSSM into the MoCA operation to enable the MoCA module to learn to focus attention on a more appropriate range of action sequence.</p><p>Regarding NSSM construction, unlike the non-local operation <ref type="bibr" target="#b37">[38]</ref>, we directly use the static feature representation sequence X extracted from the input video to reveal the explicit dependencies between the frames through the selfsimilarity matrix <ref type="bibr" target="#b9">[10]</ref> construction f (X, X) = XX T ? R T ?T . In this way, the continuity of human motion in the input video can be more straightforwardly revealed. Similarly, we normalize the resultant self-similarity matrix through the softmax operation to form an NSSM (see <ref type="bibr">Figure 4)</ref> to facilitate subsequent combination with the attention map.</p><p>For the combination of NSSM and attention map, we first regard NSSM as the a priori knowledge to concatenate the attention map through the operation [?, ?], and then use the learnable transformation ?(?), i.e., 1 ? 1 convolution to recalibrate the attention map by referring to NSSM (see <ref type="figure">Figure 4</ref> and Eq. (1)). The resultant ?(?) is then normalized through the softmax operation, which is called the MoCA map. By jointly considering the characteristics of the NSSM and the attention map, the MoCA map can reveal the non-local context relations related to the human motion of the input video in a more appropriate range. To this end, the non-local context response Y ? R T ? 2048 m can be calculated from the linear combination between the matrices resulted from ?(?) and g(?).</p><p>Finally, as in the design of the non-local block <ref type="bibr" target="#b37">[38]</ref>, we use residual connection <ref type="bibr" target="#b12">[13]</ref> to generate the output temporal feature representation sequence Z ? R T ?2048 (see <ref type="figure">Figure 4)</ref> in the MoCA module as follows:</p><formula xml:id="formula_7">Z = YW z + X,<label>(6)</label></formula><p>where W z is a learnable weight matrix implemented by using the convolutional operation <ref type="bibr" target="#b37">[38]</ref>, and the number of channels in W z is scaled up to match the number of channels (i.e., 2048) in X. "+X" denotes a residual connection. The residual connection allows us to insert the MoCA module into any pre-trained network, without breaking its initial behavior (e.g., if W z is initialized as zero). As a result, by further considering the non-local context response Y, Z will contain rich temporal information, so Z can be regarded as enhanced X.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Temporal feature integration</head><p>Given the temporal feature representation sequence Z ? R T ?2048 , the goal of the HAFI module is to refine the temporal feature of the current frame z t by integrating the adjacent temporal features observed from past and future frames to strengthen their temporal correlation and obtain better pose and shape estimation, as shown in <ref type="figure">Figure 3</ref>. HAFI Module. Specifically, we use T /2 adjacent frames (i.e., {z t? T 4 }) to refine the temporal feature of the current frame z t in a hierarchical attentive integration manner, as shown in <ref type="figure">Figure 5</ref>. For each branch in the HAFI module, we consider the temporal features of three adjacent frames as a group (adjacent frames between groups do not overlap), and resize them from 2048 dimensions to 256 dimensions respectively through a shared fully connected (FC) layer to reduce computational complexity. The resized temporal features are concatenated (z concat ? R 768 ) and passed to three FC layers and a softmax activation to calculate the attention values a = {a k } 3 k=1 by exploring the dependencies among them. Then, the attention value is weighted back to each corresponding frame to amplify the contribution of important frames in the temporal feature integration to obtain the aggregated temporal feature (see <ref type="figure">Figure 5</ref>). The aggregated temporal features produced by the bottom branches will be passed to the upper layer and integrated in the same way to produce the final refined z t . By gradually integrating temporal features in adjacent frames to strengthen temporal correlation, it will provide opportunities for the SMPL parameter regressor to learn to estimate accurate and temporally coherent 3D human pose and shape.</p><p>In this work, like Kocabas et al. <ref type="bibr" target="#b19">[20]</ref>, we use the SMPL parameter regressor proposed in <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b20">21]</ref> as our regressor to estimate pose, shape, and camera parameters ? t ? R 85 according to each refined z t (see <ref type="figure">Figure 3</ref>). In the training phase, we initialize the SMPL parameter regressor with pretrained weights from HMR <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b20">21]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Loss functions</head><p>In terms of MPS-Net training, for each estimated ? t , following the method proposed by Kocabas et al. <ref type="bibr" target="#b19">[20]</ref>, we impose L 2 loss between the estimated and ground-truth SMPL parameters and 3D/2D joint coordinates to supervise MPS-Net to generate reasonable real-world poses. The 3D joint coordinates are obtained by forwarding the estimated SMPL parameters to the SMPL model <ref type="bibr" target="#b23">[24]</ref>, and the 2D joint coordinates are obtained through the 2D projection of the 3D joints using the predicted camera parameters <ref type="bibr" target="#b19">[20]</ref>. In addition, like Kocabas et al. <ref type="bibr" target="#b19">[20]</ref>, we also apply adversarial loss L adv , i.e., using the AMASS <ref type="bibr" target="#b25">[26]</ref> dataset to train a discriminator to distinguish between real human motion and those generated by MPS-Net's SMPL parameter regressor to encourage the generation of reasonable 3D human motion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Implementation details</head><p>Following the previous works <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b19">20]</ref>, we set T = 16 as the sequence length. We use ResNet-50 <ref type="bibr" target="#b12">[13]</ref> pre-trained by Kolotouros et al. <ref type="bibr" target="#b20">[21]</ref> to serve as our static feature extractor. The static feature extractor is fixed and outputs a 2048-dimensional feature for each frame, i.e., x t ? R 2048 . The SMPL parameter regressor has two FC layers, each with 1024 neurons, and followed an output layer to output 85 pose, shape, and camera parameters ? t for each frame <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b20">21]</ref>. The discriminator architecture we use is the same as <ref type="bibr" target="#b19">[20]</ref>. The parameters of MPS-Net and discriminator are optimized by the Adam solver <ref type="bibr" target="#b18">[19]</ref> at a learning rate of 5 ? 10 ?5 and 1 ? 10 ?4 , respectively. The mini-batch size is set to 32. During training, if the performance does not improve within 5 epochs, the learning rate of both the MPS-Net and the discriminator will be reduced by a factor of 10. We use an NVIDIA Titan RTX GPU to train the entire network for 30 epochs. PyTorch <ref type="bibr" target="#b29">[30]</ref> is used for code implementation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>We first illustrate the datasets used for training and evaluation and the evaluation metrics. Then, we compare our MPS-Net against other state-of-the-art video-based methods and single image-based methods to demonstrate its advantages in addressing 3D human pose and shape estimation. We also provide an ablation study to confirm the effectiveness of each module in MPS-Net. Finally, we visualize some examples to show the qualitative evaluation results.</p><p>Datasets. Following the previous works <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b19">20]</ref>, we adopt batches of mixed 3D and 2D datasets for training. For 3D datasets, we use 3DPW <ref type="bibr" target="#b36">[37]</ref>, MPI-INF-3DHP <ref type="bibr" target="#b26">[27]</ref>, Hu-man3.6M <ref type="bibr" target="#b15">[16]</ref>, and AMASS <ref type="bibr" target="#b25">[26]</ref> for training, where 3DPW and AMASS provide SMPL parameter annotations, while MPI-INF-3DHP and Human3.6M include 3D joint annotations. For 2D datasets, we use PoseTrack <ref type="bibr" target="#b0">[1]</ref> and InstaVariety <ref type="bibr" target="#b17">[18]</ref> for training, where PoseTrack provides groundtruth 2D joints, while InstaVariety includes pseudo groundtruth 2D joints annotated using a 2D keypoint detector <ref type="bibr" target="#b3">[4]</ref>. In terms of evaluation, the 3DPW, MPI-INF-3DHP, and Hu-man3.6M datasets are used. Among them, Human3.6M is an indoor dataset, while 3DPW and MPI-INF-3DHP contain challenging outdoor videos. More detailed settings are in the supplementary material.</p><p>Evaluation metrics. For the evaluation, four standard metrics are used <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b24">25]</ref>, including the mean per joint position error (MPJPE), the Procrustes-aligned mean per joint position error (PA-MPJPE), the mean per vertex position error (MPVPE), and the acceleration error (ACC-ERR). Among them, MPJPE, PA-MPJPE, and MPVPE are mainly used to express the accuracy of the estimated 3D human pose and shape (measured in millimeter (mm)), and ACC-ERR (mm/s 2 ) is used to express the smoothness and temporal coherence of 3D human motion. A detailed description of each metric is included in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Comparison with state-of-the-art methods</head><p>Video-based methods. <ref type="table" target="#tab_0">Table 1</ref> shows the performance comparison between our MPS-Net and the state-of-the-art video-based methods on the 3DPW, MPI-INF-3DHP, and Human3.6M datasets. Following TCMR <ref type="bibr" target="#b5">[6]</ref>, all methods are trained on the training set including 3DPW, but do not use the Human3.6M SMPL parameters obtained from Mosh <ref type="bibr" target="#b22">[23]</ref> for supervision. Because the SMPL parameters from Mosh have been removed from public access due to legal issues <ref type="bibr" target="#b24">[25]</ref>. The values of the comparison method are from TCMR <ref type="bibr" target="#b5">[6]</ref>, but we validated them independently.</p><p>The results in <ref type="table" target="#tab_0">Table 1</ref> show that our MPS-Net outperforms the existing video-based methods in almost all metrics and datasets. This demonstrates that by capturing the motion continuity dependencies and integrating temporal features from adjacent past and future, performance can indeed be improved. Although TCMR <ref type="bibr" target="#b5">[6]</ref> has also made great progress, it is limited by the ability of recurrent operation (i.e., GRU) to capture non-local context relations in the action sequence <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b37">38]</ref>, thereby reducing the accuracy of the estimated 3D human pose and shape (i.e., PA-MPJPE, MPJPE, and MPVPE are higher than MPS-Net). In addition, the number of network parameters and model size of TCMR are also about 3 times that of MPS-Net (see <ref type="table">Table 2</ref>), which is relatively heavy. Regarding MEVA <ref type="bibr" target="#b24">[25]</ref>, as shown in <ref type="table" target="#tab_0">Table 1</ref>   <ref type="bibr" target="#b37">[38]</ref>, considering only the MoCA module (without using HAFI), and replacing the HAFI module with the temporal feature integration scheme proposed by Choi et al. <ref type="bibr" target="#b5">[6]</ref>. For performance comparison, it is obvious from <ref type="table" target="#tab_1">Table 3</ref> that the proposed MoCA module (i.e., MPS-Netonly MoCA) is superior to non-local block (i.e., MPS-Netonly Non-local) in all metrics. The results confirm that by further introducing the a priori knowledge of NSSM to guide self-attention learning, the MoCA module can indeed improve 3D human pose and shape estimation. On the other hand, the results also show that our HAFI module (i.e., MPS-Net-MoCA+HAFI) outperforms the temporal feature integration scheme (i.e., MPS-Net-MoCA+TFintgr.), which demonstrates that the gradual integration of adjacent features through a hierarchical attentive integration manner can indeed strengthen temporal correlation and make the generated 3D human motion smoother (i.e., lower ACC-ERR). Overall, the ablation analysis confirmed the effectiveness of the proposed MoCA and HAFI modules. Single image-based and video-based methods. We further compare our MPS-Net with the methods including single image-based methods on the challenging in-the-wild 3DPW dataset. Notice that a number of previous works <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b19">[20]</ref><ref type="bibr" target="#b20">[21]</ref><ref type="bibr" target="#b21">[22]</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b40">41]</ref> did not use the 3DPW training set to train their models, so in the comparison in <ref type="table">Table 4</ref>, all methods are not trained on 3DPW. Similar to the results in <ref type="table" target="#tab_0">Table 1</ref>, the results in <ref type="table">Table 4</ref> demonstrate that our MPS-Net performs favorably against existing single image-based and video-based methods on the PA-MPJPE, MPJPE, and MPVPE evaluation metrics. Although TCMR achieves the lowest ACC-ERR, it tends to be overly smooth, thereby sacrificing the accuracy of pose and shape estimation. Specifically, when TCMR reduces ACC-ERR 0.8 mm/s 2 compared to MPS-Net, MPS-Net reduces PA-MPJPE, MPJPE, and MPVPE by 1.8 mm, 3.4 mm, and 1.7 mm, respectively. <ref type="table">Table 4</ref> further confirms the importance of considering temporal information in consec- <ref type="figure">Figure 6</ref>. Qualitative comparison of TCMR <ref type="bibr" target="#b5">[6]</ref> (left) and our MPS-Net (right) on the challenging in-the-wild 3DPW <ref type="bibr" target="#b36">[37]</ref> dataset (the 1st and 2nd clips) and MPI-INF-3DHP <ref type="bibr" target="#b26">[27]</ref> dataset (the 3rd clip). This is an embedded video, please use Adobe Acrobat (standalone version) to view it. Note that the video animation has been slowed down for better viewing. utive frames, i.e., compared with single-image-based methods, video-based methods have lower ACC-ERR. In summary, MPS-Net achieves a better balance in the accuracy and smoothness of 3D human pose and shape estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Qualitative evaluation</head><p>We present 1) visual comparisons with the TCMR <ref type="bibr" target="#b5">[6]</ref>, 2) visual effects of MPS-Net in alternative viewpoints, and 3) visual results of the learned human motion continuity. Visual comparisons with the TCMR. The qualitative comparison between TCMR and our MPS-Net on the 3DPW and MPI-INF-3DHP datasets is shown in <ref type="figure">Figure 6</ref>. From the results, we observe that the 3D human pose and shape estimated by MPS-Net can fit the input images well, especially on the limbs. TCMR seems to be too focused on generating smooth 3D human motion, so the estimated pose has relatively small changes from frame to frame, which limits its ability to fit the input images.</p><p>Visual effects of MPS-Net in alternative viewpoints. We visualize the 3D human body estimated by MPS-Net from different viewpoints in <ref type="figure" target="#fig_0">Figure 7</ref>. The results show that MPS-Net can estimate the correct global body rotation. This is quantitatively demonstrated by the improvements in the PA-MPJPE, MPJPE, and MPVPE (see <ref type="table" target="#tab_0">Table 1</ref>). Visual results of the learned human motion continuity. We use a relatively extreme example to show the continuity of human motion learned by MPS-Net. In this example, we randomly downloaded two pictures with different poses from the Internet, and copied the pictures multiple times to form a sequence. Then, we send the sequence to VIBE <ref type="bibr" target="#b19">[20]</ref> and MPS-Net for 3D human pose and shape estimation. As shown in <ref type="figure" target="#fig_1">Figure 8</ref>, compared with VIBE, it is obvious from the estimation results that our MPS-Net produces a transition effect between pose exchanges, and this transition conforms to the continuity of human kinematics. It demonstrates that MPS-Net has indeed learned the continuity of human motion, and explains why MPS-Net can achieve lower ACC-ERR in the benchmark (action) datasets (see <ref type="table" target="#tab_0">Table 1</ref>). This result is also similar to using a 3D motion predictor to estimate reasonable human motion in-betweening of two key frames <ref type="bibr" target="#b11">[12]</ref>. In contrast, VIBE relies too much on the features of the current frame, making it unable to truly learn the continuity of human motion. Thus, its ACC-ERR is still high (see <ref type="table" target="#tab_0">Table 1</ref>).</p><p>For more results and video demos can be found at https://mps-net.github.io/MPS-Net/.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>We propose the MPS-Net for estimating 3D human pose and shape from monocular video. The main contributions of this work lie in the design of the MoCA and HAFI modules. The former leverages visual cues observed from human motion to adaptively recalibrate the range that needs attention in the sequence to capture the motion continuity dependencies, and the later allows our model to strengthen temporal correlation and refine feature representation for producing temporally coherent estimates. Compared with existing methods, the integration of MoCA and HAFI modules demonstrates the advantages of our MPS-Net in achieving the state-of-the-art 3D human pose and shape estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Material</head><p>7. Datasets 3DPW. 3DPW <ref type="bibr" target="#b36">[37]</ref> is mainly captured from outdoors and in-the-wild. It combines a hand-held camera and a set of inertial measurement unit (IMU) sensors attached at the body limbs to calculate the near ground-truth SMPL parameters. It contains a total of 60 videos of different lengths. We use the official split to train and test the model, where the training, validation, and test sets are composed of 24, 12, and 24 videos, respectively. In addition, we report MPVPE on 3DPW because it is the only dataset that contains groundtruth 3D shape annotations among the datasets we used. <ref type="bibr" target="#b26">[27]</ref> is a dataset consisting of both constrained indoor and complex outdoor scenes. It is captured using a multi-view camera setting with a markerless motion capture system, and the 3D joint annotation is calculated through the multiview method. Following existing methods <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b19">20]</ref>, we use the official split to train and test the model. The training set contains 8 subjects, each of which has 16 videos, and the test set contains 6 subjects, performing 7 actions in both indoor and outdoor environments.</p><formula xml:id="formula_8">MPI-INF-3DHP. MPI-INF-3DHP</formula><p>Human3.6M. Human3.6M <ref type="bibr" target="#b15">[16]</ref> is one of the largest motion capture datasets, which contains 3.6 million video frames and corresponding 3D joint annotations. This dataset is collected in an indoor and controlled environment. Same as existing methods <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b19">20]</ref>, we train the model on 5 subjects (i.e., S1, S5, S6, S7, and S8) and evaluate it on 2 subjects (i.e., S9 and S11). We subsampled the dataset to 25 frames per second (fps) for training and testing. <ref type="bibr" target="#b25">[26]</ref> is a large-scale human motion sequence database that unifies 15 existing motion capture (mocap) datasets by representing them within a common framework and parameterization. These motion sequences are annotated with Mosh++ to generate SMPL parameters. AMASS has a total of 42 hours of mocap, 346 subjects, and 11, 451 human motions. Following the setting of the existing method <ref type="bibr" target="#b19">[20]</ref>, we use this database to train our MPS-Net.  <ref type="table">Table 5</ref>. Effect of the number of frames per group in the HAFI module. The training and evaluation settings are the same as the experiments on the 3DPW dataset <ref type="bibr" target="#b36">[37]</ref> in <ref type="table" target="#tab_0">Table 1.</ref> 2D joint annotations. Same as existing methods <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b19">20]</ref>, we adopt this dataset for training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>AMASS. AMASS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Evaluation metrics</head><p>Four standard evaluation metrics <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b24">25]</ref> are considered, including MPJPE, PA-MPJPE, MPVPE, and ACC-ERR. Specifically, MPJPE is calculated as the mean of the Euclidean distance between the ground-truth and the estimated 3D joint positions after aligning the pelvis joint on the ground truth location. PA-MPJPE is calculated similarly to MPJPE, but after the estimated pose is rigidly aligned with the ground-truth pose. MPVPE is calculated as the mean of the Euclidean distance between the ground truth and the estimated 3D mesh vertices (output by the SMPL model). ACC-ERR is measured as the mean difference between the ground-truth and the estimated 3D acceleration for every joint.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.">Ablation study of HAFI module</head><p>To analyze the effect of the number of frames per group in the HAFI module, we conduct ablation studies on MPS-Net with different HAFI module settings under the 3DPW dataset <ref type="bibr" target="#b36">[37]</ref>. The results in <ref type="table">Table 5</ref> indicate that considering the temporal features of three adjacent frames as a group can enable our MPS-Net to achieve the best 3D human pose and shape estimation. Therefore, for all experiments (i.e., <ref type="table" target="#tab_0">Table 1</ref> to <ref type="table">Table 4</ref>), the HAFI module defaults to three frames as a group.</p><p>On the other hand, we further add the results considering only the HAFI module on MPS-Net (called MPS-Net-only HAFI) to supplement the ablation experiments in <ref type="table" target="#tab_1">Table 3</ref>, with results of 54.0, 87.6, 103.5, and 7.5, respectively, for PA-MPJPE, MPJPE, MPVPE and ACC-ERR. Compared with MPS-Net-only MoCA (see <ref type="table" target="#tab_1">Table 3</ref>), MPS-Net-only HAFI yields better ACC-ERR but worse on PA-MPJPE, MPJPE and MPVPE. However, by coupling MoCA with HAFI, MPS-Net (Ours) achieves the best result. <ref type="figure">Figure 9</ref>. Visual comparison of 3D human pose and shape estimation of MoCA module and non-local block <ref type="bibr" target="#b37">[38]</ref> on the 3DPW dataset <ref type="bibr" target="#b36">[37]</ref>. Where the attention map is generated from the non-local operation, and the MoCA map is generated from the MoCA operation. In the attention and MoCA maps, red indicates a higher attention value, and blue indicates a lower one. The results demonstrate that the MoCA map generated by our MoCA operation can indeed allow the MPS-Net to focus attention on a more appropriate range of action sequence to improve the estimation results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10.">Qualitative comparison between MoCA module and non-local block</head><p>To further verify whether the proposed MoCA operation can improve the 3D human pose and shape estimation by introducing NSSM to recalibrate the attention map generated by the non-local operation <ref type="bibr" target="#b37">[38]</ref>, we conduct the following qualitative experiments. Specifically, we visualize the 3D human pose and shape estimation resulted from the methods of MPS-Net-only Non-local and MPS-Net-only MoCA in <ref type="table" target="#tab_1">Table 3</ref>, respectively. The results can be seen from the two examples in <ref type="figure">Figure 9</ref> that the MoCA map generated by our MoCA operation can indeed allow the MPS-Net to focus attention on a more appropriate range of action se-quence, thereby improving the accuracy of 3D human pose and shape estimation. On the contrary, the attention map generated by non-local operation is often unstable, and it is easy to focus attention on less correlated frames and ignore the continuity of human motion in the action sequence, which reduces the accuracy of estimation. Such a result is quantitatively demonstrated by the improvement of MPS-Net-only MoCA in the MPJPE, PA-MPJPE, and MPVPE errors (see <ref type="table" target="#tab_1">Table 3</ref>).</p><p>On the other hand, we also add the results considering only the setting of NSSM on MPS-Net (called MPS-Netonly NSSM) to supplement the ablation experiments in Table 3, with results of 53.3, 88.0, 104.1, and 26.0, respec-tively, for PA-MPJPE, MPJPE, MPVPE and ACC-ERR. These (cf. <ref type="table" target="#tab_1">Table 3</ref>) indicate that MPS-Net-only NSSM and MPS-Net-only Non-local are complementary, and their fusion, i.e., MPS-Net-only MoCA, can further achieve better results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="11.">Effect of motion speed and input fps</head><p>To demonstrate how motion speed and frame rate influence MPS-Net performance, we conducted experiments with various demo videos (e.g., fast-moving dancing or general walking video) at different frame rates, from 24fps to 60fps, and found that it has little impact on MPS-Net. Frame rate info for each demo video is available on our demo website.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 7 .</head><label>7</label><figDesc>Qualitative results of MPS-Net on the challenging inthe-wild 3DPW<ref type="bibr" target="#b36">[37]</ref> dataset and MPI-INF-3DHP<ref type="bibr" target="#b26">[27]</ref> dataset. For each sequence, the top row shows input images, the middle row shows the estimated body mesh from the camera view, and the bottom row shows the estimated mesh from an alternate viewpoint.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 8 .</head><label>8</label><figDesc>An example of visualization of the VIBE [20] and our MPS-Net on the continuity of human motion.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>, MEVA requires at least 90 input frames, which MPJPE ? MPJPE ? MPVPE ? ACC-ERR ? PA-MPJPE ? MPJPE ? ACC-ERR ? PA-MPJPE ? MPJPE ? ACC-ERR ? Input Frames Evaluation of state-of-the-art video-based methods on 3DPW<ref type="bibr" target="#b36">[37]</ref>, MPI-INF-3DHP<ref type="bibr" target="#b26">[27]</ref>, and Human3.6M<ref type="bibr" target="#b15">[16]</ref> datasets. Following Choi et al.<ref type="bibr" target="#b5">[6]</ref>, all methods are trained on the training set including 3DPW, but do not use the Human3.6M SMPL parameters obtained from Mosh<ref type="bibr" target="#b22">[23]</ref>. The number of input frames follows the original protocol of each method.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">3DPW</cell><cell></cell><cell></cell><cell>MPI-INF-3DHP</cell><cell></cell><cell></cell><cell>Human3.6M</cell><cell></cell><cell>Number of</cell></row><row><cell>Method PA-VIBE [20]</cell><cell>57.6</cell><cell>91.9</cell><cell>-</cell><cell>25.4</cell><cell>68.9</cell><cell>103.9</cell><cell>27.3</cell><cell>53.3</cell><cell>78.0</cell><cell>27.3</cell><cell>16</cell></row><row><cell>MEVA [25]</cell><cell>54.7</cell><cell>86.9</cell><cell>-</cell><cell>11.6</cell><cell>65.4</cell><cell>96.4</cell><cell>11.1</cell><cell>53.2</cell><cell>76.0</cell><cell>15.3</cell><cell>90</cell></row><row><cell>TCMR [6]</cell><cell>52.7</cell><cell>86.5</cell><cell>103.2</cell><cell>6.8</cell><cell>63.5</cell><cell>97.6</cell><cell>8.5</cell><cell>52.0</cell><cell>73.6</cell><cell>3.9</cell><cell>16</cell></row><row><cell>MPS-Net (Ours)</cell><cell>52.1</cell><cell>84.3</cell><cell>99.7</cell><cell>7.4</cell><cell>62.8</cell><cell>96.7</cell><cell>9.6</cell><cell>47.4</cell><cell>69.4</cell><cell>3.6</cell><cell>16</cell></row><row><cell cols="3">#Parameters (M)</cell><cell>FLOPs (G)</cell><cell cols="2">Model Size (MB)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>VIBE [20]</cell><cell>72.43</cell><cell></cell><cell>4.17</cell><cell>776</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>MEVA [25]</cell><cell>85.72</cell><cell></cell><cell>4.46</cell><cell cols="2">858.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>TCMR [6]</cell><cell>108.89</cell><cell></cell><cell>4.99</cell><cell cols="2">1073</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>MPS-Net (Ours)</cell><cell>39.63</cell><cell></cell><cell>4.45</cell><cell>331</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">Table 2. Comparison of the number of network parameters,</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">FLOPs, and model size.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>MPS-Net -only Non-local [38]</cell><cell>54.1</cell><cell></cell><cell>87.6</cell><cell>103.1</cell><cell>24.1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>MPS-Net -only MoCA</cell><cell>53.0</cell><cell></cell><cell>86.7</cell><cell>102.2</cell><cell>23.5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>MPS-Net -MoCA + TF-intgr. [6]</cell><cell>52.4</cell><cell></cell><cell>86.0</cell><cell>101.5</cell><cell>10.5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>MPS-Net (Ours) -MoCA + HAFI</cell><cell>52.1</cell><cell></cell><cell>84.3</cell><cell>99.7</cell><cell>7.4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>3DPW Method PA-MPJPE ? MPJPE ? MPVPE ? ACC-ERR ?</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3 .</head><label>3</label><figDesc>Ablation study for different modules of the MPS-Net on the 3DPW<ref type="bibr" target="#b36">[37]</ref> dataset. The training and evaluation settings are the same as the experiments on the 3DPW dataset inTable 1.</figDesc><table><row><cell>single image</cell><cell>-based</cell><cell>HMR [17] GraphCMR [22] SPIN [21] PyMAF [41]</cell><cell>76.7 70.2 59.2 58.9</cell><cell>130.0 -96.9 92.8</cell><cell>--116.4 110.1</cell><cell>37.4 -29.8 -</cell></row><row><cell></cell><cell></cell><cell>I2L-MeshNet [28]</cell><cell>57.7</cell><cell>93.2</cell><cell>110.1</cell><cell>30.9</cell></row><row><cell></cell><cell></cell><cell>HMMR [18]</cell><cell>72.6</cell><cell>116.5</cell><cell>139.3</cell><cell>15.2</cell></row><row><cell>video-based</cell><cell></cell><cell>Doersch et al. [8] Sun et al. [34] VIBE [20] TCMR [6]</cell><cell>74.7 69.5 56.5 55.8</cell><cell>--93.5 95.0</cell><cell>--113.4 111.3</cell><cell>--27.1 6.7</cell></row><row><cell></cell><cell></cell><cell>MPS-Net (Ours)</cell><cell>54.0</cell><cell>91.6</cell><cell>109.6</cell><cell>7.5</cell></row><row><cell cols="7">Table 4. Evaluation of state-of-the-art single image-based and</cell></row><row><cell cols="7">video-based methods on the 3DPW [37] dataset. All methods do</cell></row><row><cell cols="3">not use 3DPW for training.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="7">means it cannot be trained and tested on short videos. This</cell></row><row><cell cols="7">greatly reduces the value in practical applications. Overall,</cell></row><row><cell cols="7">our MPS-Net can effectively estimate accurate (lower PA-</cell></row><row><cell cols="7">MPJPE, MPJPE, and MPVPE) and smooth (lower ACC-</cell></row><row><cell cols="7">ERR) 3D human pose and shape from a video, and is rel-</cell></row><row><cell cols="7">atively lightweight (fewer network parameters). The com-</cell></row><row><cell cols="7">parisons on the three datasets also show the strong general-</cell></row></table><note>3DPW Method PA-MPJPE ? MPJPE ? MPVPE ? ACC-ERR ?</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>PoseTrack. PoseTrack<ref type="bibr" target="#b0">[1]</ref> is a 2D benchmark dataset for video-based multi-person pose estimation and articulated tracking. It contains a total of 1, 337 videos, divided into 792, 170, and 375 videos for training, validation, and testing. Each person instance in the video is annotated with 15 keypoints. Same as existing methods<ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b19">20]</ref>, we use the training set for model training.InstaVariety. InstaVariety<ref type="bibr" target="#b17">[18]</ref> is a 2D benchmark dataset captured from Instagram using 84 hashtags. There are 28, 272 videos in total, with an average length of 6 seconds, and OpenPose<ref type="bibr" target="#b3">[4]</ref> is used to acquire pseudo ground-truth 3DPWMethodPA-MPJPE ? MPJPE ? MPVPE ? ACC-ERR ?</figDesc><table><row><cell>MPS-Net (HAFI, 2 frames/group)</cell><cell>52.6</cell><cell>85.4</cell><cell>101.0</cell><cell>7.8</cell></row><row><cell>MPS-Net (HAFI, 3 frames/group)</cell><cell>52.1</cell><cell>84.3</cell><cell>99.7</cell><cell>7.4</cell></row><row><cell>MPS-Net (HAFI, 4 frames/group)</cell><cell>52.5</cell><cell>85.9</cell><cell>101.2</cell><cell>7.6</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">PoseTrack: A benchmark for human pose estimation and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhaylo</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Umar</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eldar</forename><surname>Insafutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juergen</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A non-local algorithm for image denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoni</forename><surname>Buades</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bartomeu</forename><surname>Coll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Michel</forename><surname>Morel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Realtime multi-person 2D pose estimation using part affinity fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-En</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Sheikh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Adaptive image transformer for one-shot object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ding-Jie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">He-Yen</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tyng-Luh</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Beyond static features for temporally consistent 3D human pose and shape from a video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsuk</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gyeongsik</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Ju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoung Mu</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Sim2real transfer learning for 3D human pose estimation: Motion to the rescue</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<idno>ICLR, 2021. 3</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Visualizing music and audio using selfsimilarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Foote</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Multimedia</title>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Hierarchical kinematic human mesh recovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Georgakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ren</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srikrishna</forename><surname>Karanam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terrence</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jana</forename><surname>Ko?eck?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyan</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Robust motion in-betweening</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>F?lix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Harvey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Yurick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Nowrouzezahrai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pal</surname></persName>
		</author>
		<idno>60:1-60:12, 2020. 8</idno>
	</analytic>
	<monogr>
		<title level="j">ACM ToG</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">One-shot object detection with co-attention and co-excitation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-I</forename><surname>Ting</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Chen</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwann-Tzong</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tyng-Luh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Albanie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enhua</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2011" to="2023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">6M: Large scale datasets and predictive methods for 3D human sensing in natural environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catalin</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragos</forename><surname>Papava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vlad</forename><surname>Olaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Sminchisescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Human3</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">End-to-end recovery of human shape and pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angjoo</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">W</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning 3D human dynamics from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angjoo</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><forename type="middle">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Panna</forename><surname>Felsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Adam: a method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><forename type="middle">Lei</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">VIBE: Video inference for human body pose and shape estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammed</forename><surname>Kocabas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Athanasiou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning to reconstruct 3D human pose and shape via model-fitting in the loop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Kolotouros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Convolutional mesh regression for single-image human shape reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Kolotouros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">MoSh: Motion and shape capture from sparse markers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Loper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naureen</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<idno>220:1-220:13</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>ACM ToG</publisher>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">SMPL: a skinned multiperson linear model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Loper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naureen</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<publisher>ACM ToG</publisher>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="248" to="249" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">3D human motion estimation via motion compression and refinement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Alireza</forename><surname>Zhengyi Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kris</forename><forename type="middle">M</forename><surname>Golestaneh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kitani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">AMASS: Archive of motion capture as surface shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naureen</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nima</forename><surname>Ghorbani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Nikolaus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Troje</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Monocular 3D human pose estimation in the wild using improved CNN supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dushyant</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helge</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleksandr</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weipeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3DV</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">I2L-MeshNet: Image-to-lixel prediction network for accurate 3D human pose and mesh estimation from a single RGB image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gyeongsik</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kyoung Mu Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Neural Body Fitting: Unifying deep learning and model-based human pose and shape estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Lassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">V</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Automatic differentiation in PyTorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS Workshop on Autodiff</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning to estimate 3D human pose and shape from a single color image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luyang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Improving language understanding by generative pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">OpenAI</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Recalibrating fully convolutional networks with spatial and channel &apos;squeeze &amp; excitation&apos; blocks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nassir</forename><surname>Abhijit Guha Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wachinger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE T-MI</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="540" to="549" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Human mesh recovery from monocular images via a skeleton-disentangled representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenpeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yili</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">BodyNet: Volumetric inference of 3D human body shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G?l</forename><surname>Varol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duygu</forename><surname>Ceylan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ersin</forename><surname>Yumer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><forename type="middle">M</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Recovering accurate 3D human pose in the wild using IMUs and a moving camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Timo Von Marcard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Henschel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Bodo Rosenhahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pons-Moll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Joon-Young Lee, and In So Kweon. CBAM: Convolutional block attention module</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyun</forename><surname>Woo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongchan</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Combining local convolution with global self-attention for reading comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adams</forename><forename type="middle">Wei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Dohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Qanet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">PyMAF: 3D human pose and shape regression with pyramidal mesh alignment feedback loop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongwen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yating</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinchi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yebin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenan</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV, 2021</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
