<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Neural Relation Extraction Within and Across Sentence Boundaries</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pankaj</forename><surname>Gupta</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Corporate Technology, Machine-Intelligence (MIC-DE)</orgName>
								<orgName type="institution">Siemens AG Munich</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">CIS</orgName>
								<orgName type="institution" key="instit2">University of Munich (LMU) Munich</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subburam</forename><surname>Rajaram</surname></persName>
							<email>subburam.rajaram@siemens.com|pankaj.gupta@campus.lmu.de</email>
							<affiliation key="aff0">
								<orgName type="department">Corporate Technology, Machine-Intelligence (MIC-DE)</orgName>
								<orgName type="institution">Siemens AG Munich</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Andrassy</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Corporate Technology, Machine-Intelligence (MIC-DE)</orgName>
								<orgName type="institution">Siemens AG Munich</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Sch?tze</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">CIS</orgName>
								<orgName type="institution" key="instit2">University of Munich (LMU) Munich</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Runkler</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Corporate Technology, Machine-Intelligence (MIC-DE)</orgName>
								<orgName type="institution">Siemens AG Munich</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Neural Relation Extraction Within and Across Sentence Boundaries</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T17:20+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Past work in relation extraction mostly focuses on binary relation between entity pairs within single sentence. Recently, the NLP community has gained interest in relation extraction in entity pairs spanning multiple sentences. In this paper, we propose a novel architecture for this task: inter-sentential dependency-based neural networks (iDepNN). iDepNN models the shortest and augmented dependency paths via recurrent and recursive neural networks to extract relationships within (intra-) and across (inter-) sentence boundaries. Compared to SVM and neural network baselines, iDepNN is more robust to false positives in relationships spanning sentences. We evaluate our models on four datasets from newswire (MUC6) and medical (BioNLP shared task) domains that achieve state-of-the-art performance and show a better balance in precision and recall for inter-sentential relationships. We perform better than 11 teams participating in the BioNLP shared task 2016 and achieve a gain of 5.2% (0.587 vs 0.558) in F1 over the winning team. We also release the crosssentence annotations for MUC6.</p><p>Paul Allen has started a company and named [Vern Raburnse1 its President. The company, to be called [Paul Allen Group] e2 will be based in Bellevue, Washington.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>The task of relation extraction (RE) aims to identify semantic relationship between a pair of nominals or entities e1 and e2 in a given sentence S. Due to a rapid growth in information, it plays a vital role in knowledge extraction from unstructured texts and serves as an intermediate step in a variety of NLP applications in newswire, web and high-valued biomedicine <ref type="bibr" target="#b0">(Bahcall 2015)</ref> domains. Consequently, there has been increasing interest in relation extraction, particularly in augmenting existing knowledge bases.</p><p>Progress in relation extraction is exciting; however most prior work <ref type="bibr">(Zhang et al. 2006;</ref><ref type="bibr" target="#b13">Kambhatla 2004;</ref><ref type="bibr" target="#b28">Vu et al. 2016a;</ref><ref type="bibr" target="#b9">Gupta, Sch?tze, and Andrassy 2016)</ref> is limited to single sentences, i.e., intra-sentential relationships, and ignores relations in entity pairs spanning sentence boundaries, i.e., inter-sentential. Thus, there is a need to move beyond single sentences and devise methods to extract relationships spanning sentences. For instance, consider the sentences:</p><p>The two sentences together convey the fact that the entity e1 is associated with e2, which cannot be inferred from either sentence alone. The missed relations impact the system performance, leading to poor recall. But precision is equally important; e.g., in high-valued biomedicine domain, significant inter-sentential relationships must be extracted, especially in medicine that aims toward accurate diagnostic testing and precise treatment, and extraction errors can have severe negative consequences. In this work, we present a neural network (NN) based approach to precisely extract relationships within and across sentence boundaries, and show a better balance in precision and recall with an improved F 1 .</p><p>Previous work on cross-sentence relation extraction used coreferences to access entities that occur in a different sentence <ref type="bibr" target="#b5">(Gerber and Chai 2010;</ref><ref type="bibr" target="#b31">Yoshikawa et al. 2011)</ref> without modeling inter-sentential relational patterns. <ref type="bibr" target="#b27">Swampillai and Stevenson (2011)</ref> described a SVM-based approach to both intra-and inter-sentential relations. Recently, <ref type="bibr" target="#b22">Quirk and Poon (2016)</ref> applied distant supervision to cross-sentence relation extraction of entities using binary logistic regression (non-neural network based) classifier and <ref type="bibr" target="#b20">Peng et al. (2017)</ref> applied sophisticated graph long short-term memory networks to cross-sentence n-ary relation extraction. However, it still remains challenging due to the need for coreference resolution, noisy text between the entity pairs spanning multiple sentences and lack of labeled corpora. <ref type="bibr" target="#b2">Bunescu and Mooney (2005)</ref>, Nguyen, <ref type="bibr" target="#b19">Matsuo, and Ishizuka (2007)</ref> and <ref type="bibr" target="#b17">Mintz et al. (2009)</ref> have shown that the shortest dependency path (SDP) between two entities in a dependency graph and the dependency subtrees are the most useful dependency features in relation classification. Further, <ref type="bibr" target="#b14">Liu et al. (2015)</ref> developed these ideas using Recursive Neural Networks (RecNNs, <ref type="bibr" target="#b25">Socher et al. (2014)</ref>) and combined the two components in a precise structure called Augmented Dependency Path (ADP), where each word on a SDP is attached to a dependency subtree; however, limited to single sentences. In this paper, we aspire from these methods to extend shortest dependency path across sentence boundary and effectively combine it with dependency subtrees in NNs that can capture semantic representation of the structure and boost relation extraction spanning sentences.</p><p>The contributions are: (1) Introduce a novel dependencybased neural architecture, named as inter-sentential Dependency-based Neural Network (iDepNN) to extract re- lations within and across sentence boundaries by modeling shortest and augmented dependency paths in a combined structure of bidirectional RNNs (biRNNs) and RecNNs.</p><p>(2) Evaluate different linguistic features on four datasets from newswire and medical domains, and report an improved performance in relations spanning sentence boundary. We show amplified precision due to robustness towards false positives, and a better balance in precision and recall. We perform better than 11 teams participating in in the BioNLP shared task 2016 and achieve a gain of 5.2% (0.587 vs 0.558) in F 1 over the winning team.</p><p>(3) Release relation annotations for the MUC6 dataset for intra-and inter-sentential relationships. Code, data and supplementary are available at https://github.com/pgcool/ Cross-sentence-Relation-Extraction-iDepNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methodology</head><p>Inter-sentential Dependency-Based Neural Networks (iDepNN)</p><p>Dependency-based neural networks (DepNN) <ref type="bibr" target="#b2">(Bunescu and Mooney 2005;</ref><ref type="bibr" target="#b14">Liu et al. 2015)</ref> have been investigated for relation extraction between entity pairs limited to single sentences, using the dependency information to explore the semantic connection between two entities. In this work, we introduce iDepNN, the inter-sentential Dependency-based Neural Network, an NN that models relationships between entity pairs spanning sentences, i.e., inter-sentential within a document. We refer to the iDepNN that only models the shortest dependency path (SDP) spanning sentence boundary as iDepNN-SDP and to the iDepNN that models augmented dependency paths (ADPs) as iDepNN-ADP; see below. biRNNs (bidirectional RNNs, <ref type="bibr" target="#b23">Schuster and Paliwal (1997)</ref>) and RecNNs (recursive NNs, <ref type="bibr" target="#b24">Socher et al. (2012)</ref>) are the backbone of iDepNN.</p><p>Modeling Inter-sentential Shortest Dependency Path (iDepNN-SDP): We compute the inter-sentential Shortest Dependency Path (iSDP) between entities spanning sentence boundaries for a relation. To do so, we obtain the dependency parse tree for each sentence using the Stanford-CoreNLP dependency parser . We then use NetworkX <ref type="bibr" target="#b11">(Hagberg, Swart, and S Chult 2008)</ref> to represent each token as a node and the dependency relation as a link between the nodes. In the case of multiple sentences, the root node of the parse tree of a sentence is connected to the root of the subsequent tree, leading to the shortest path from one entity to another across sentences. <ref type="figure" target="#fig_0">Figure 1</ref> (Left) shows dependency graphs for the example sentences where the two entities e1 and e2 appear in nearby sentences and exhibit a relationship. <ref type="figure" target="#fig_0">Figure 1</ref> (Right) illustrates that the dependency trees of the two adjacent sentences and their roots are connected by NEXTS to form an iSDP, an inter-Sentential Dependency Path, (highlighted in gray) between the two entities. The shortest path spanning sentence boundary is seen as a sequence of words between two entities. <ref type="figure" target="#fig_1">Figure 2</ref> shows how a biRNN <ref type="bibr" target="#b23">(Schuster and Paliwal 1997;</ref><ref type="bibr" target="#b29">Vu et al. 2016b</ref>) uses iSDP to detect relation between e1 and e2, positioned one sentence apart.</p><p>Modeling Inter-sentential Dependency Subtrees: To effectively represent words on the shortest dependency path within and across sentence boundary, we model dependency subtrees assuming that each word w can be seen as the word itself and its children on the dependency subtree. The notion of representing words using subtree vectors within the dependency neural network (DepNN) is similar to <ref type="bibr" target="#b14">(Liu et al. 2015)</ref>; however, our proposed structures are based on iSDPs and ADPs that span sentences.</p><p>To represent each word w on the subtree, its word embedding vector x w P R d and subtree representation c w P R d 1 are concatenated to form its final representation p w P R d`d 1 . We use 200-dimensional pretrained GloVe embeddings <ref type="bibr" target="#b21">(Pennington, Socher, and Manning 2014)</ref>. The subtree representation of a word is computed through recursive transformations of the representations of its children words. A RecNN is used to construct subtree embedding c w , traversing bottom-up from its leaf words to the root for entities spanning sentence boundaries, as shown in <ref type="figure" target="#fig_1">Figure 2</ref>. For a word which is a leaf node, i.e., it does not have a subtree, we set its subtree representation as c LEAF . <ref type="figure" target="#fig_1">Figure 2</ref> illustrates how subtree-based word representations are constructed via iSDP.</p><p>Each word is associated with a dependency relation r, e.g., r = dobj, during the bottom-up construction of the subtree. For each r, a transformation matrix W r P R d 1?p d`d 1 q is learned. The subtree embedding is computed as:</p><p>cw " f p ? qPChildrenpwq WR pw,qq?p q`bq and pq " rxq, cqs  where R pw,qq is the dependency relation between word w and its child word q and b P R d 1 is a bias. This process continues recursively up to the root word such as the word "named" on the iSDP in the figure.</p><p>Modeling Inter-sentential Augmented Dependency Path (iDepNN-ADP): Following <ref type="bibr" target="#b14">Liu et al. (2015)</ref>, we combine the two components: iSDP and dependency subtrees spanning sentence boundaries to form a combined structure which we name as inter-sentential Augmented Dependency Path (iDepNN-ADP). As shown in <ref type="figure" target="#fig_1">Figure 2</ref>, each word on iSDP is attached to its subtree representation c w . An attached subtree enriches each word on the iSDP with additional information about how this word functions in specific sentence to form a more precise structure for classifying relationships within and across sentences.</p><p>To capture the semantic representation of iDepNN-ADP, we first adopt a RecNN to model the dependency subtrees for each word on the iSDP. Then, we design a biRNN to obtain salient semantic features on the iSDP. The overall structure of iDepNN-ADP ( <ref type="figure" target="#fig_1">Figure 2)</ref> is built upon the combination of recursive and recurrent NNs spanning sentences.</p><p>Learning: We develop a biRNN over the two structures: iDepNN-SDP and iDepNN-ADP, and pass the last hidden vector h N (in the iSDP word sequence, <ref type="figure" target="#fig_1">Figure 2</ref>) to a softmax layer whose output is the probability distribution y over relation labels R, as y " softmaxpU?h N`by q where U P R R?H is the weight matrix connecting hidden vector of dimension H to output of dimension R and b y P R R is the bias. h N is the last hidden vector of the biRNN.</p><p>To compute semantic representation h w for each word w on the iSDP, we adopt the Connectionist biRNN <ref type="bibr" target="#b28">(Vu et al. 2016a</ref>) that combines the forward and backward pass by adding their hidden layers (h ft and h bt ) at each time step t and also adds a weighted connection to the previous combined hidden layer h t?1 to include all intermediate hidden layers into the final decision.</p><formula xml:id="formula_0">h ft " f pV?i t`W?hft?1 q h bt " f pV?i N?t`1`W?hbt`1 q h t " f ph ft`hbt`W?ht?1 q</formula><p>where V P R H?|i| , N is the total number of words on iSDP and i t the input vector at t, defined by:</p><formula xml:id="formula_1">iDepNN-SDP : i t " rx t , L t s iDepNN-ADP : i t " rp t , L t s</formula><p>where L t are lexical level features (e.g., part-of-speech tag, position indicators, entity types) for each word at t. Observe, in order to minimize the number of parameters, we share the same weight matrix W in three parts: forward pass, backward pass and combination of both. The optimization objective is to minimize the cross-entropy error between the ground-truth label and softmax output. The parameters are learned using backpropagation <ref type="bibr" target="#b30">(Werbos 1990</ref>). Key Features: The features focus on characteristics of the full sentence, dependency path or individual entities. The various features used in our experiments are: (1) Position-Indicator (PI): A one-hot vector for SVM which indicates the position of the entity in the vocabulary. Four additional words (?e 1 ?, ?{e 1 ?, ?e 2 ?, ?{e 2 ?) to mark start and end of entity mentions e1 and e2, used in NNs. See details about PI in <ref type="bibr" target="#b10">Gupta (2015)</ref>.    <ref type="bibr" target="#b7">(Grishman and Sundheim 1996)</ref> dataset contains information about management succession events from newswire. The task organizers provided a training corpus and a set of templates that contain the management succession events, the names of people who are starting or leaving management posts, the names of their respective posts and organizations and whether the named person is currently in the job. Entity Tagging: We tag entities Person (Per), Organization (Org) using Stanford NER tagger <ref type="bibr" target="#b4">(Finkel, Grenager, and Manning 2005)</ref>. The entity type Position (Post) is annotated based on the templates. Relation Tagging: We have three types of relations: Per-Org, Per-Post and Post-Org. We follow <ref type="bibr" target="#b26">Swampillai and Stevenson (2010)</ref> and annotate binary relations (within and across sentence boundaries) using management succession events between two entity pairs. We randomly split the collection 60/20/20 into train/dev/test.</p><p>Experimental Setup. For MUC6, we use the pretrained 1 the official evaluation is not accessible any more and therefore, the annotations for their test sets are not available  GloVe <ref type="bibr">(Pennington, Socher, and Manning 2014) embeddings (200-dimension)</ref>. For the BioNLP datasets, we use 200-dimensional embedding 2 vectors from six billion words of biomedical text <ref type="bibr" target="#b18">(Moen and Ananiadou 2013)</ref>. We randomly initialize a 5-dimensional vectors for PI and POS. We initialize the recurrent weight matrix to identity and biases to zero. We use the macro-averaged F 1 score (the official evaluation script by SemEval-2010 Task 8 <ref type="bibr" target="#b12">(Hendrickx et al. 2010</ref>)) on the development set to choose hyperparameters (see supplementary). To report results on BioNLP ST 2016 test set, we use the official web service 3 .</p><p>Baselines. <ref type="bibr" target="#b26">Swampillai and Stevenson's (2010)</ref> annotation of MUC6 intra-and inter-sentential relationships is not available. They investigated SVM with dependency and linguistic features for relationships spanning sentence boundaries. In BioNLP shared tasks, the top performing systems are SVM-based and limited to relationships within single sentences. As an NN baseline, we also develop Connectionist biRNN <ref type="bibr" target="#b28">(Vu et al. 2016a</ref>) that spans sentence boundaries; we refer to it as i-biRNN (architecture in supplementary). Similarly, we also investigate using a bidirectional LSTM (i-biLSTM). As a competitive baseline in the inter-sentential relationship extraction, we run 4 graphLSTM <ref type="bibr" target="#b20">(Peng et al. 2017</ref>). This work compares SVM and graphLSTM with i-biRNN, i-biLSTM, iDepNN-SDP and iDepNN-ADP for different values of the sentence range parameter k (the distance in terms of the number of sentences between the entity pairs for a relation) , i.e., k (" 0, ? 1, ? 2 and ? 3).</p><p>Contribution of different components. <ref type="table" target="#tab_4">Table 2</ref> shows the contribution of each feature, where both training and evaluation is performed over relationships within and across sentence boundaries for sentence range parameter k?1. Note: iSDP+Dependency refers to iDepNN-ADP structure train</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Evaluation for different values of sentence range k param  </p><formula xml:id="formula_2">k " 0 k ? 1 k ? 2 k ? 3 pr P R F 1 pr P R F 1 pr P R F 1 pr P R F 1 k " 0<label>SVM</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>State-of-the-Art Comparisons</head><p>BioNLP ST 2016 dataset: <ref type="table" target="#tab_6">Table 3</ref> shows the performance of {SVM, graphLSTM} vs {i-biRNN, iDepNN-SDP, iDepNN-ADP} for relationships within and across sentence boundaries. Moving left to right for each training parameter, the recall increases while precision decreases due to increasing noise with larger k. In the inter-sentential evaluations (k ? 1, ? 2, ? 3 columns) for all the training parameters, the iDepNN variants outperform both SVM and graphLSTM in terms of F 1 and maintain a better precision as well as balance in precision and recall with increasing k; e.g., at k ? 1 (train/eval), precision and F 1 are (.402 vs .226) and (.449 vs .336), respectively for (iDepNN-ADP vs graphLSTM). We find that SVM mostly leads in recall. In comparison to graphLSTM, i-biRNN and i-biLSTM, we observe that iDepNN-ADP offers precise structure in relation extraction within and across sentence boundaries. For instance, at training k ? 1 and evaluation k " 0, iDepNN-ADP reports precision of .570 vs .489 and .561 in i-biRNN and iDepNN-SDP, respectively. During training at k ? 1, iDepNN-SDP and iDepNN-ADP report better F 1 than i-biRNN for evaluations at all k, suggesting that the shortest threshold ensemble (train k ? 1 and evaluation k " 0) Dev (official scores)</p><p>Test <ref type="formula">(</ref>  <ref type="table">Table 4</ref>: Ensemble scores at various thresholds for BioNLP ST 2016 dataset. p: output probability and augmented paths provide useful dependency features via recurrent and recursive compositions, respectively. Between the proposed architectures iDepNN-SDP and iDepNN-ADP, the former achieves higher recall for all k. We find that the training at k ? 1 is optimal for intra-and inter-sentential relations over development set (see supplementary). We also observe that i-biRNN establishes a strong NN baseline for relationships spanning sentences. The proposed architectures consistently outperform graphLSTM in both precision and F 1 across sentence boundaries.</p><p>Ensemble: We exploit the precision and recall bias of the different models via an ensemble approach, similar to TurkuNLP <ref type="bibr" target="#b16">(Mehryary et al. 2016</ref>) and UMS <ref type="bibr" target="#b3">(Del?ger et al. 2016</ref>) systems that combined predictions from SVM and NNs. We aggregate the prediction outputs of the neural (i-  where pr is the count of predictions. This work demonstrates a better balance in precision and recall, and achieves the highest F 1 and recall. We extract 419 predictions within and across sentence boundaries, which is closer to the count of gold predictions, i.e., 340 <ref type="bibr" target="#b3">(Del?ger et al. 2016)</ref>.</p><p>biRNN, iDepNN-SDP and iDepNN-ADP) and non-neural (SVM) classifiers, i.e., a relation to hold if any classifier has predicted it. We perform the ensemble scheme on the development and official test sets for intra-and inter-sentential (optimal at k ? 1) relations. <ref type="table" target="#tab_6">Table 3</ref> shows the ensemble scores on the official test set for relations within and across sentence boundaries, where ensemble achieves the highest F 1 (.561) over individual models. Confident Extractions: We consider the high confidence prediction outputs by the different models participating in the ensemble, since it lacks precision (.478). Following <ref type="bibr" target="#b20">Peng et al. (2017)</ref>, we examine three values of the output probability p, i.e., (? 0.85, 0.90 and 0.95) of each model in the ensemble. <ref type="table">Table 4</ref> shows the ensemble performance on the development and official test sets, where the predictions with p ? 0.85 achieve the state-of-the-art performance and rank us at the top out of 11 systems <ref type="figure">(Figure 3, right)</ref>.</p><p>This Work vs Competing Systems in BioNLP ST 2016: As shown in <ref type="figure">Figure 3 (right)</ref>, we rank at the top and achieve a gain of 5.2% (.587 vs .558) in F 1 compared to VERSE. We also show a better balance in precision and recall, and report the highest recall compared to all other systems. Most systems do not attempt to predict relations spanning sentences. The most popular algorithms are SVM (VERSE, HK, UTS, LIMSI) and NNs (TurkuNLP, WhuNlpRE, DUTIR). UMS combined predictions from an SVM and an NN. Most systems rely on syntactic parsing, POS, word embeddings and entity recognition features (VERSE, TurkuNLP, UMS, HK, DUTIR, UTS). VERSE and TurkuNLP obtained top scores on intra-sentential relations relying on the dependency path features between entities; however they are limited to intrasentential relations. TurkuNLP employed an ensemble of 15 different LSTM based classifiers. DUTIR is based on CNN for intra-sentential relationsips. LIMSI is the only system that considers inter-sentential relationships during training; however it is SVM-based and used additional manually annotated training data, linguistic features using biomedical resources (PubMed, Cocoa web API, OntoBiotope ontology, etc.) and post-processing to annotate biomedical abbreviations. We report a noticeable gain of 21% (.587 vs .485) in F 1 with an improved precision and recall over LIMSI.</p><p>BioNLP ST 2011 and 2013 datasets: Following the BioNLP ST 2016 evaluation, we also examine two additional datasets from the same domain. iDepNN-ADP (Table 5) is the leading performer in terms of precision and F 1 within and across boundaries for BioNLP ST 2013. Examining BioNLP ST 2011, the iDepNN variants lead both SVM and i-biRNN for k ? 1 and k ? 2.</p><p>MUC6 dataset: Similar to BioNLP ST 2016, we perform training and evaluation of SVM, i-biRNN, iDepNN-SDP and iDepNN-ADP for different sentence range with best feature combination (Table 2) using MUC6 dataset. <ref type="table">Table 6</ref> shows that both iDepNN variants consistently outperform graphLSTM and SVM for relationships within and across sentences. For within (k"0) sentence evaluation, iDepNN-ADP reports .963 F 1 , compared to .779 and .783 by SVM and graphLSTM, respectively. iDepNN-ADP is observed more precise than iDepNN-SDP and graphLSTM with in- <ref type="bibr">Dataset: BioNLP ST 2013</ref> Dataset: BioNLP ST 2011</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><formula xml:id="formula_3">k " 0 k ? 1 k ? 2 k ? 3 k " 0 k ? 1 k ? 2 k ? 3 P R F 1 P R F 1 P R F 1 P R F 1 P R F 1 P R F 1 P R F 1 P R F 1 SVM</formula><p>.  Table 5: BioNLP ST 2011 and 2013 datasets: Performance for training (k ? 1) and evaluation for different k. Underline: Better precision in iDepNN-ADP than iDepNN-SDP, graphLSTM, i-biLSTM, i-biRNN and SVM. Bold: best in column.  k ? 1</p><formula xml:id="formula_4">train Model Evaluation for different k param k " 0 k ? 1 k ? 3 P R F 1 P R F 1 P R F 1 k " 0 SVM .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SVM</head><p>.  k ? 3</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SVM</head><p>.  Table 6: MUC6 Dataset: Performance over the intra-and inter-sentential training and evaluation for different k. Underline signifies better precision by iDepNN-ADP over iDepNN-SDP, graphLSTM, i-biLSTM, i-biRNN and SVM. Bold indicates the best score column-wise. creasing k, e.g., at k?3. Training at sentence range k?1 is found optimal in extracting inter-sentential relationships.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Error Analysis and Discussion</head><p>In <ref type="figure">Figure 3</ref> (left), we analyze predictions using different values of sentence range k (=0, ?1, ?2 and ?3) during both training and evaluation of SVM, graphLSTM and iDepNN-ADP for BioNLP ST 2016 dataset. For instance, an SVM (top-left) is trained for intra-sentential (same sentence) relations, while iDepNN-ADP (bottom-right) for both intra-and inter-sentential spanning three sentences (three sentences apart). We show how the count of true positives (TP), false negatives (FN) and false positives (FP) varies with k.</p><p>Observe that as the distance of the relation increases, the classifiers predict larger ratios of false positives to true positives. However, as the sentence range increases, iDepNN-ADP outperforms both SVM and graphLSTM due to fewer false positives (red colored bars). On top, the ratio of FP to TP is better in iDepNN-ADP than graphLSTM and SVM for all values of k. Correspondingly in <ref type="table" target="#tab_6">Table 3</ref>, iDepNN-ADP reports better precision and balance between precision and recall, signifying its robustness to noise in handling intersentential relationships.</p><p>iDepNN vs graphLSTM: <ref type="bibr" target="#b20">Peng et al. (2017)</ref> focuses on general relation extraction framework using graphLSTM with challenges such as potential cycles in the document graph leading to expensive model training and difficulties in convergence due to loopy gradient backpropagation. Therefore, they further investigated different strategies to backpropagate gradients. The graphLSTM introduces a number of parameters with a number of edge types and thus, requires abundant supervision/training data. On other hand, our work introduces simple and robust neural architectures (iDepNN-SDP and iDepNN-ADP), where the iDepNN-ADP is a special case of document graph in form of a parse tree spanning sentence boundaries. We offer a smooth gradient backpropagation in the complete structure (e.g., in iDepNN-ADP via recurrent and recursive hidden vectors) that is more efficient than graphLSTM due to non-cyclic (i.e., tree) architecture. We have also shown that iDepNN-ADP is robust to false positives and maintains a better balance in precision and recall than graphLSTM for inter-sentential relationships <ref type="figure">(Figure 3)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>We have proposed to classify relations between entities within and across sentence boundaries by modeling the inter-sentential shortest and augmented dependency paths within a novel neural network, named as inter-sentential Dependency-based Neural Network (iDepNN) that takes advantage of both recurrent and recursive neural networks to model the structures in the intra-and inter-sentential relationships. Experimental results on four datasets from newswire and medical domains have demonstrated that iDepNN is robust to false positives, shows a better balance in precision and recall and achieves the state-of-the-art performance in extracting relationships within and across sentence boundaries. We also perform better than 11 teams participating in the BioNLP shared task 2016.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Parameter</head><p>Value <ref type="formula">(</ref>     <ref type="figure" target="#fig_0">Figure 1</ref>: Architecture of i-biRNN. In our structures, we share the weights in forward, backward and combined networks, in order to reduce parameters, i.e., W is being shared in the three networks. <ref type="bibr">Zhang, M.;</ref><ref type="bibr">Zhang, J.;</ref><ref type="bibr">Su, J.;</ref><ref type="bibr">and Zhou, G. 2006</ref>. A composite kernel to extract relations between entities with both flat and structured features. In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics, 825-832. Association for Computational Linguistics.</p><formula xml:id="formula_5">V V V V V V V V V V V V V V V V</formula></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Left: Sentences and their dependency graphs. Right: Inter-sentential Shortest Dependency Path (iSDP) across sentence boundary. Connection between the roots of adjacent sentences by NEXTS.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Inter-sentential Dependency-based Neural Network variants: iDepNN-SDP and iDepNN-ADP</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>(2) Entity Types (ET): A one-hot vector to represent the entity type in SVM and embedding vectors in NNs. (3) Part-of-speech (POS): A bag-of-words (BoW) in SVM and embedding vector for each POS type in NNs. (4) Dependency: In SVM, the specific edge types in the dependency path are captured with a BoW vector, similar to</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Vern Raburn] e1 named [Paul Allen Group] e2 called iSDP</head><label></label><figDesc></figDesc><table><row><cell cols="2">Bi-directional hidden layer vector</cell><cell cols="2">Word embedding vector</cell><cell>Subtree embedding vector</cell><cell>Softmax layer</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Inter-sentential Bi-directional Recurrent Neural Network</cell></row><row><cell>[W poss president its Inter-sentential Dependency Subtrees W dobj</cell><cell cols="2">started and company a W dobj W cc W compound Allen Paul W nsubj W det</cell><cell cols="3">W company the W nsubjpass based will be W auxpass Washington in W nmod Bellevue to W aux W aux W det W case W compound be W auxpass Word V Neural Network U h N Recursive Inter-sentential Embedding (x w ) Subtree Embedding (c w ) p w</cell><cell>iDepNN-SDP</cell><cell>iDepNN-ADP</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1</head><label>1</label><figDesc>We evaluate our proposed methods on four datasets from medical and news domain.Table 1shows counts of intra-and inter-sentential relationships. The three medical domain datasets are taken from the BioNLP shared task (ST) of relation/event extraction<ref type="bibr" target="#b1">(Bossy et al. 2011;</ref><ref type="bibr" target="#b19">N?dellec et al. 2013;</ref><ref type="bibr" target="#b3">Del?ger et al. 2016)</ref>. We compare our proposed techniques with the systems published at these venues. The Bacteria Biotope task<ref type="bibr" target="#b1">(Bossy et al. 2011</ref>) of the BioNLP ST 2011 focuses on extraction of habitats of bacteria, which is extended by the BioNLP ST 2013<ref type="bibr" target="#b19">(N?dellec et al. 2013)</ref>, while the BioNLP ST 2016 focuses on extraction of Lives in events. We have standard train/dev/test splits for the BioNLP ST 2016 dataset, while we perform 3-fold crossvalidation 1 on BioNLP ST 2011 and 2013. For BioNLP ST 2016, we generate negative examples by randomly sampling co-occurring entities without known interactions. Then we sample the same number as positives to obtain a balanced dataset during training and validation for different sentence range. See supplementary for further details.The MUC6</figDesc><table><row><cell>: Count of intra-and inter-sentential relationships in datasets (train+dev) from two domains</cell></row><row><cell>Grouin (2016). In NNs, it refers to iDepNN-ADP. (5) [inter-sentential-]Shortest-Dependency-Path ([i-]SDP): Sequence of Words on the [i-]SDP.</cell></row><row><cell>Evaluation and Analysis</cell></row><row><cell>Dataset.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Dataset: BioNLP ST 2016   iSDP  .217 .816 .344 .352 .574 .436  + PI + ET  .218 .819 .344 .340 .593 .432  + POS  .269 .749 .396 .348 .568 .431  + Dependency .284 .746 .411 .402 .509 .449   Dataset: MUC6</figDesc><table><row><cell>Features</cell><cell>P</cell><cell>SVM R</cell><cell>F 1</cell><cell>P</cell><cell>iDepNN R</cell><cell>F 1</cell></row><row><cell>Features</cell><cell>P</cell><cell>SVM R</cell><cell>F 1</cell><cell>P</cell><cell>iDepNN R</cell><cell>F 1</cell></row><row><cell>iSDP</cell><cell cols="6">.689 .630 .627 .916 .912 .913</cell></row><row><cell>+ PI</cell><cell cols="6">.799 .741 .725 .912 .909 .909</cell></row><row><cell>+ POS</cell><cell cols="6">.794 .765 .761 .928 .926 .926</cell></row><row><cell cols="7">+ Dependency .808 .768 .764 .937 .934 .935</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>SVM vs iDepNN: Features in inter-sentential (k?1) training and inter-sentential (k ? 1) evaluation. iSDP+Dependency refers to iDepNN-ADP structure.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>363 .474 .512 .492 821 .249 .606 .354 1212 .199 .678 .296 1517 .153 .684 .250  graphLSTM  473 .472 .668 .554 993 .213 .632 .319 1345 .166 .660 .266 2191 .121 .814 .218  i-biLSTM  480 .475 .674 .556 998 .220 .652 .328 1376 .165 .668 .265 1637 .640 .219 i-biRNN 286 .517 .437 .474 425 .301 .378 .335 540 .249 .398 .307 570 .239 .401 .299 iDepNN-SDP 297 .519 .457 .486 553 .313 .510 .388 729 .240 .518 .328 832 .209 .516 .298 iDepNN-ADP 266 .526 .414 .467 476 .311 .438 .364 607 .251 .447 .320 669 .226 .447 .300 k ? 1 SVM 471 .464 .645 .540 888 .284 .746 .411 1109 .238 .779 .365 1196 .221 .779 .344 graphLSTM 406 .502 .607 .548 974 .226 .657 .336 1503 .165 .732 .268 2177 .126 .ADP 292 .570 .491 .527 428 .402 .509 .449 497 .356 .522 .423 517 .341 .521 .412 ADP 313 .553 .512 .532 541 .355 .568 .437 654 .315 .601 .415 687 .300 .601 .401 k ? 1 ensemble 480 .478 .680 .561 837 .311 .769 .443 1003 .268 .794 .401 1074 .252 .797 .382</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>813 .218</cell></row><row><cell></cell><cell>i-biLSTM</cell><cell cols="2">417 .505 .628 .556 1101 .224 .730 .343 1690 .162 .818 .273 1969 .132 .772 .226</cell></row><row><cell></cell><cell>i-biRNN</cell><cell>376 .489 .544 .515 405 .393 .469 .427 406 .391 .469 .426 433</cell><cell>.369 .472 .414</cell></row><row><cell></cell><cell>iDepNN-SDP</cell><cell>303 .561 .503 .531 525 .358 .555 .435 660 .292 .569 .387 724</cell><cell>.265 .568 .362</cell></row><row><cell cols="2">SVM graphLSTM iDepNN-k ? 2 i-biLSTM</cell><cell cols="2">495 .461 .675 .547 1016 .259 .780 .389 1296 .218 .834 .345 1418 .199 .834 .321 442 .485 .637 .551 1016 .232 .702 .347 1334 .182 .723 .292 1758 .136 .717 .230 404 .487 .582 .531 940 .245 .682 .360 1205 .185 .661 .289 2146 .128 .816 .222</cell></row><row><cell></cell><cell>i-biRNN</cell><cell cols="2">288 .566 .482 .521 462 .376 .515 .435 556 .318 .524 .396 601 ..296 .525 .378</cell></row><row><cell></cell><cell>iDepNN-SDP</cell><cell>335 .537 .531 .534 633 .319 .598 .416 832 .258 .634 .367 941</cell><cell>.228 .633 .335</cell></row><row><cell></cell><cell cols="2">iDepNN-ADP 309 .538 .493 .514 485 .365 .525 .431 572 .320 .542 .402 603</cell><cell>.302 .540 .387</cell></row><row><cell></cell><cell>SVM</cell><cell cols="2">507 .458 .686 .549 1172 .234 .811 .363 1629 .186 .894 .308 1874 .162 .897 .275</cell></row><row><cell></cell><cell>graphLSTM</cell><cell cols="2">429 .491 .624 .550 1082 .230 .740 .351 1673 .167 .833 .280 2126 .124 .787 .214</cell></row><row><cell>k ? 3</cell><cell>i-biLSTM</cell><cell cols="2">417 .478 .582 .526 1142 .224 .758 .345 1218 .162 .833 .273 2091 .128 .800 .223</cell></row><row><cell></cell><cell>i-biRNN</cell><cell>405 .464 .559 .507 622 .324 .601 .422 654 .310 .604 .410 655</cell><cell>.311 .607 .410</cell></row><row><cell></cell><cell>iDepNN-SDP</cell><cell>351 .533 .552 .542 651 .315 .605 .414 842 .251 .622 .357 928</cell><cell>.227 .622 .333</cell></row><row><cell></cell><cell>iDepNN-</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table /><note>BioNLP ST 2016 Dataset: Performance of the intra-and-inter-sentential training/evaluation for different k. Underline: Better precision by iDepNN-ADP over iDepNN-SDP, graphLSTM and SVM. Bold: Best in column. pr: Count of predictions that exhibits a better precision, F 1 and balance in precision and recall, compared to SVM. See supplementary for feature analysis on BioNLP ST 2011 / 2013.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>.85 160 .694 .514 .591 419 .530 .657 .587 p ? 0.90 151 .709 .496 .583 395 .539 .630 .581 p ? 0.95 123 .740 .419 .535 293 .573 .497 .533</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">official scores)</cell></row><row><cell>pr</cell><cell>P</cell><cell>R</cell><cell>F 1</cell><cell>pr</cell><cell>P</cell><cell>R</cell><cell>F 1</cell></row><row><cell>p ? 0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 1 :</head><label>1</label><figDesc>Hyperparameters for BRNN and iDepNN | HostPart | Geographic | Environment | Food | Soil | Water), while PartOf has HostPart and Host entity types. The BioNLP ST 2016 focuses on extraction of Lives in events among Bacteria, Habitat and Geographical entities. Hyperparameters See Table 1. We run graphLSTM for 100 iterations with learning rate of 0.02, batch size 10 and 150 hidden dimension. ET .853 .786 .808 .915 .921 .918 + POS .934 .904 .917 .938 .945 .942 + Dependency .937 .926 .931 .970 .938 .951 Dataset: BioNLP ST 2011 Dependency .930 .931 .930 .928 .957 .942</figDesc><table><row><cell>Feature Analysis on BioNLP ST 2011 and 2013 datasets</cell></row><row><cell>See Table 2.</cell></row><row><cell>Development Scores (official) on BioNLP ST 2016 data set to determine the value of training parameter, k</cell></row><row><cell>See Table 3.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 2 :</head><label>2</label><figDesc>SVM Vs iDepNN: Performance of different features used in inter-sentential (k ? 1) training and inter-sentential (k ? 1) evaluation. iSDP + dependency refers to iDepNN-ADP. .497 .378 .726 132 .591 .475 .781 iDepNN-SDP 139 .524 .431 .669 167 .641 .568 .737 iDepNN-ADP 138 .536 .438 .688 161 .630 .549 .740 k ? 2 i-biRNN 129 .493 .393 .659 145 .613 .511 .764 iDepNN-SDP 149 .531 .449 .651 180 .598 .545 .661 iDepNN-ADP 142 .534 .443 .675 161 .619 .539 .726 k ? 3 i-biRNN 112 .514 .388 .759 129 .603 .482 .806 iDepNN-SDP 142 .506 .417 .641 173 .610 .548 .688 iDepNN-ADP 139 .530 .435 .676 158 .626 .539 .747</figDesc><table><row><cell>train param</cell><cell>Model</cell><cell>pr</cell><cell>Development scores at different values of k k 0 k ? 1 F1 R P pr F1 R</cell><cell>P</cell></row><row><cell></cell><cell>i-biRNN</cell><cell>113</cell><cell></cell><cell></cell></row><row><cell>k ? 1</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 3 :</head><label>3</label><figDesc>BioNLP ST 2016 development set: Performance comparison to determine the value of training parameter k. Observe that the parameter k ? 1 is optimal during training and evaluation (on development set) for relationships within and across sentence boundaries. P , R and F 1 are official scores.</figDesc><table><row><cell>h b h 1 h f w 2 4 + + Forward direction h b h 2 h f W b W W b &lt;e2&gt;w 2 3 &lt;/e2&gt; W + + W f W f v 1 1 &lt;e1&gt;v 1 2 &lt;/e1&gt;</cell><cell>h b h 3 h f w 2 2 + + v 1 3</cell><cell>W b W W f</cell><cell>h b h 4 h f w 2 1 + + v 1 4</cell><cell>h b h 5 h f Backward direction h b h 6 h f v 1 4 v 1 3 W b W b W + + W W f W f + + w 2 1 w 2 2</cell><cell>h b h 7 h f &lt;e1&gt;v 1 2 &lt;/e1&gt; v 1 h b h N h f 1 W b W b + W + W W f + + W f &lt;e2&gt;w 2 3 4 w 2 &lt;/e2&gt;</cell><cell>U</cell><cell>R E L A T I O N PerOrg</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">http://bio.nlplab.org/ 3 http://bibliome.jouy.inra.fr/demo/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank anonymous reviewers for their review comments. This research was supported by Bundeswirtschaftsministerium (bmwi.de), grant 01MD15010A (Smart Data Web) at Siemens AG-CT Machine Intelligence, Munich Germany.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Material AAAI Press</head><p>Association for the Advancement of Artificial Intelligence 2275 East Bayshore Road, Suite 160 Palo Alto, California 94303</p><p>Architecture of i-biRNN <ref type="figure">Figure 1</ref> illustrates the architecture of inter-sentential Connectionist Bi-directional Recurrent Neural Network (i-biRNN). Consider two sentences S1 and S2, where S1 consists of words . v 1 2 and w 2 3 are entity1 and entity2 respectively spanning sentence boundary.</p><p>The input to the i-biRNN is the concatenation of S1 and S2 where;</p><p>? V : the weights matrix between hidden units and input in forward and backward network used to condition the input word vector, x t ? W f : the weights matrix connecting hidden units in forward network</p><p>? W b : the weights matrix connecting hidden units in backward network</p><p>? h f : the forward hidden unit computed at time step, t, accumulating the semantic meaning given the input word x t and history</p><p>? h b : the backward hidden unit computed at time step, t, accumulating the semantic meaning given the input word x t and the future context, h bt 1 , that is accumulated and conditioned on the future words.</p><p>? W : the recurrent weight matrix connecting combined hidden units, h t through time;</p><p>? h t : the hidden vector that accumulates the semantics obtained from the combination or sum of forward and backward units at time step, t</p><p>? N : total number of words in S1 and S2</p><p>? U: the output weight matrix, connecting to the softmax layer</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data Description</head><p>The BioNLP shared task (2011 and 2013) consists of extracting bacteria localization events of given species i.e., Localization or PartOf relations. For Location, the participating entity types are Bacterium and type Localization (Host Copyright c 2019, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Precision medicine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Bahcall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">526</biblScope>
			<biblScope unit="page">335</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Bionlp shared task 2011: bacteria biotope</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bossy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jourde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bessieres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Van De Guchte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>N?dellec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the BioNLP Shared Task 2011 Workshop</title>
		<meeting>the BioNLP Shared Task 2011 Workshop</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="56" to="64" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A shortest path dependency kernel for relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Bunescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the conference on human language technology and empirical methods in natural language processing</title>
		<meeting>the conference on human language technology and empirical methods in natural language processing</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="724" to="731" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Overview of the bacteria biotope task at bionlp shared task 2016</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Del?ger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bossy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Chaix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ferr?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bessieres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>N?dellec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4th BioNLP Shared Task Workshop</title>
		<meeting>the 4th BioNLP Shared Task Workshop</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="12" to="22" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Incorporating non-local information into information extraction systems by gibbs sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Finkel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Grenager</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd annual meeting on association for computational linguistics</title>
		<meeting>the 43rd annual meeting on association for computational linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="363" to="370" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Beyond nombank: A study of implicit arguments for nominal predicates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gerber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Chai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 48th</title>
		<meeting>the 48th</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<title level="m">Annual Meeting of the Association for Computational Linguistics</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<biblScope unit="page" from="1583" to="1592" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Message understanding conference-6: A brief history</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Grishman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sundheim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 16th International Conference on Computational Linguistics</title>
		<imprint>
			<date type="published" when="1996" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note>COLING</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Identification of mentions and relations between bacteria and biotope from pubmed abstracts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Grouin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">64</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Table filling multitask recurrent neural network for joint entity and relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sch?tze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Andrassy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers</title>
		<meeting>COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2537" to="2547" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Deep Learning Methods for the Extraction of Relations in Natural Language Text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gupta</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<pubPlace>Munich, Germany</pubPlace>
		</imprint>
	</monogr>
	<note>Master&apos;s thesis, Technical University of</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Exploring network structure, dynamics, and function using networkx</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hagberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Swart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chult</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
		<respStmt>
			<orgName>Los Alamos National Laboratory (LANL</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Semeval-2010 task 8: Multi-way classification of semantic relations between pairs of nominals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Hendrickx</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">N</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Kozareva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Nakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">O</forename><surname>S?aghdha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pad?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pennacchiotti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Romano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Szpakowicz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page">33</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Combining lexical, syntactic, and semantic features with maximum entropy models for extracting relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kambhatla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL 2004 on Interactive poster and demonstration sessions</title>
		<meeting>the ACL 2004 on Interactive poster and demonstration sessions</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2004" />
			<biblScope unit="volume">22</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1507.04646</idno>
		<title level="m">A dependency-based neural network for relation classification</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">The stanford corenlp natural language processing toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Finkel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bethard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mcclosky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL (System Demonstrations)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="55" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep learning with minimal training data: Turkunlp entry in the bionlp shared task 2016</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Mehryary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bj?rne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pyysalo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Salakoski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ginter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4th BioNLP Shared Task Workshop</title>
		<meeting>the 4th BioNLP Shared Task Workshop</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="73" to="81" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Distant supervision for relation extraction without labeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mintz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bills</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Snow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP</title>
		<meeting>the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1003" to="1011" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Distributional semantics resources for biomedical text processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Moen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S S</forename><surname>Ananiadou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Relation extraction from wikipedia using subtree mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>N?dellec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bossy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ohta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pyysalo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zweigenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Matsuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ishizuka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the National Conference on Artificial Intelligence</title>
		<meeting>the National Conference on Artificial Intelligence<address><addrLine>Bulgaria; Menlo Park, CA; Cambridge, MA; London</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="1414" />
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
	<note>Proceedings of the BioNLP Shared Task 2013 Workshop</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Cross-sentence n-ary relation extraction with graph lstms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Poon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Quirk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="101" to="115" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 conference on empirical methods in natural language processing</title>
		<meeting>the 2014 conference on empirical methods in natural language processing</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Distant supervision for relation extraction beyond the sentence boundary</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Quirk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Poon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.04873</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Bidirectional recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">K</forename><surname>Paliwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2673" to="2681" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Semantic compositionality through recursive matrix-vector spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Huval</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 joint conference on empirical methods in natural language processing and computational natural language learning</title>
		<meeting>the 2012 joint conference on empirical methods in natural language processing and computational natural language learning</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1201" to="1211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Grounded compositional semantics for finding and describing images with sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="207" to="218" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Inter-sentential relations in information extraction corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Swampillai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stevenson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">LREC</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Extracting relations within and across sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Swampillai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stevenson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">RANLP</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="25" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Combining recurrent and convolutional neural networks for relation classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">T</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sch?tze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the NAACL-HLT</title>
		<meeting>the NAACL-HLT<address><addrLine>San Diego, California USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="534" to="539" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Bidirectional recurrent neural network with ranking loss for spoken language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">T</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sch?tze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Acoustics, Speech and Signal Processing (ICASSP)</title>
		<meeting>the Acoustics, Speech and Signal Processing (ICASSP)<address><addrLine>Shanghai, China</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="6060" to="6064" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Backpropagation through time: what it does and how to do it</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Werbos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1550" to="1560" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Coreference based event-argument relation extraction on biomedical text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yoshikawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hirao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Asahara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Matsumoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Biomedical Semantics</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
