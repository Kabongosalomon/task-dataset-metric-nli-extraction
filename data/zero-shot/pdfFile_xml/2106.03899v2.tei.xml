<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Shifting Transformation Learning for Out-of-Distribution Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sina</forename><surname>Mohseni</surname></persName>
							<email>smohseni@nvidia.com</email>
							<affiliation key="aff0">
								<orgName type="institution">NVIDIA</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arash</forename><surname>Vahdat</surname></persName>
							<email>avahdat@nvidia.com</email>
							<affiliation key="aff0">
								<orgName type="institution">NVIDIA</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jay</forename><surname>Yadawa</surname></persName>
							<email>jyadawa@nvidia.com</email>
							<affiliation key="aff0">
								<orgName type="institution">NVIDIA</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Shifting Transformation Learning for Out-of-Distribution Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T17:14+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Detecting out-of-distribution (OOD) samples plays a key role in open-world and safety-critical applications such as autonomous systems and healthcare. Recently, self-supervised representation learning techniques (via contrastive learning and pretext learning) have shown effective in improving OOD detection. However, one major issue with such approaches is the choice of shifting transformations and pretext tasks which depends on the in-domain distribution. In this paper, we propose a simple framework that leverages a shifting transformation learning setting for learning multiple shifted representations of the training set for improved OOD detection. To address the problem of selecting optimal shifting transformation and pretext tasks, we propose a simple mechanism for automatically selecting the transformations and modulating their effect on representation learning without requiring any OOD training samples. In extensive experiments, we show that our simple framework outperforms state-of-the-art OOD detection models on several image datasets. We also characterize the criteria for a desirable OOD detector for real-world applications and demonstrate the efficacy of our proposed technique against state-of-the-art OOD detection techniques.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Despite advances in representation learning and their generalization to unseen samples, learning algorithms are bounded to perform well on source distribution and vulnerable to outof-distribution (OOD) or outlier samples. For example, it has been shown that the piece-wise linear decision boundaries in deep neural network (DNN) with ReLU activation are prune to OOD samples as they can assign arbitrary high confidence values to samples away from the training distribution <ref type="bibr" target="#b15">(Hein, Andriushchenko, and Bitterwolf 2019)</ref>. Recent work on machine learning trustworthiness and safety have shown that OOD detection plays a key role in open-world and safetycritical applications such as autonomous systems <ref type="bibr" target="#b28">(Mohseni et al. 2019</ref>) and healthcare . However, OOD detection in high dimensional domains like image data is a challenging task and often requires great computational resource <ref type="bibr" target="#b9">(Gal and Ghahramani 2016)</ref>.</p><p>The recent surge in self-supervised learning techniques shows that learning pretext tasks can result in better semantic understanding of data by learning invariant representa-Work in progress.</p><p>tions <ref type="bibr" target="#b8">(Dosovitskiy et al. 2014</ref>) and can increase model performance in different setups <ref type="bibr" target="#b10">(Gidaris, Singh, and Komodakis 2018)</ref>. Self-supervised learning has also been shown effective in OOD detection. For example, <ref type="bibr" target="#b11">Golan and El-Yaniv (2018)</ref> and <ref type="bibr" target="#b20">Hendrycks et al. (2019b)</ref> show that simple geometric transformations improve OOD detection performance, and <ref type="bibr">Tack et al. (2020)</ref> leverage shifting data transformations and contrastive learning for OOD detection. However, these works manually design the transformations and pretext tasks.</p><p>Inspired by the recent works, we study the impact of representation learning on OOD detection when training a model on artificially transformed datasets. We observe that training on a diverse set of dataset transformations jointly, termed as shifting transformation learning here, further improves the model's ability to distinguish in-domain samples from outliers. However, we also empirically observe that the choice of effective data transformations for OOD detection depends on the in-domain training set. In other words, the set of transformations effective for one in-domain dataset may no longer be effective for another dataset.</p><p>To address this problem, we make the following contributions in this paper: (i) We propose a simple framework for transformation learning from multiple shifted views of the in-domain training set in both self-supervised and fullysupervised settings (when data labels are available) for OOD detection. (ii) We propose a framework that selects effective transformation and modulates their impact on representation learning. We demonstrate that the optimally selected transformations result in better representation for both main classification and OOD detection compared to data augmentation-based approaches. (iii) We propose an ensemble score for OOD detection that leverages multiple transformations trained with a shared encoder. In particular, our technique achieves new state-of-the-art results in OOD detection on multi-class classification by improving averaged area under the receiver operating characteristics (AUROC) +1.3% for CIFAR-10, +4.37% for CIFAR-100, and +1.02% for ImageNet-30 datasets. (iv) To the best of our knowledge, this paper is the first to introduce criteria for ideal OOD detection and to analyze a diverse range of techniques along with these criteria. Albeit the simplicity, we show that our proposed approach outperforms the state-of-the-art techniques on robustness and generalization criteria. arXiv:2106.03899v2 [cs.CV] 8 Oct 2021</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Work</head><p>Here, we review OOD detection methods related to this work:</p><p>Distance-based Detection: Distance-based methods use different distance measures between the unknown test sample and source training set in the representation space. These techniques involve preprocessing or test-time sampling of the source domain distribution to measure their averaged distance to the novel input sample. The popular distance measures include Mahalanobis distance <ref type="bibr" target="#b25">(Lee et al. 2018;</ref><ref type="bibr" target="#b41">Sehwag, Chiang, and Mittal 2021)</ref>, cosine similarity <ref type="bibr" target="#b43">(Techapanurak, Suganuma, and Okatani 2020;</ref><ref type="bibr">Tack et al. 2020)</ref> and others semantic similarity metrics <ref type="bibr" target="#b35">(Rafiee, Gholamipoor, and Kollmann 2020)</ref>. These techniques usually work well with unlabeled data in unsupervised and one-class classification setups. For example, <ref type="bibr" target="#b37">Ruff et al. (2018)</ref> present a deep learning one-class classification approach to minimize the representation hypersphere for source distribution and calculate the detection score as the distance of the outlier sample to the center of the hypersphere. Recently, <ref type="bibr" target="#b30">Mukhoti et al. (2021)</ref> proposed using distance measures for model features to better disentangle model uncertainty from dataset uncertainty. Distance-based methods can benefit from ensemble measurements over input augmentations <ref type="bibr">(Tack et al. 2020)</ref> or transformations <ref type="bibr" target="#b1">(Bergman and</ref><ref type="bibr">Hoshen 2020), network layers (Lee et al. 2018;</ref><ref type="bibr" target="#b40">Sastry and Oore 2019)</ref>, or source domain sub-distributions <ref type="bibr" target="#b33">(Oberdiek, Rottmann, and Fink 2020)</ref> to improve detection results. For instance, <ref type="bibr">Tack et al. (2020)</ref> present a detection score based on combining representation norm with cosine similarity between the outlier samples and their nearest training samples for one-class classification problem. They also show that OOD detection can be improved with ensembling over random augmentations, which carries a higher computational cost.</p><p>Classification-based Detection: These OOD detection techniques avoid costly distance-based and uncertainty estimation techniques (e.g., <ref type="bibr" target="#b9">Gal and Ghahramani (2016)</ref>) by seeking effective representation learning to encode normality together with the main classification task. Various detection scores have been proposed including maximum softmax probability <ref type="bibr" target="#b18">(Hendrycks and Gimpel 2016)</ref>, maximum logit scores <ref type="bibr" target="#b16">(Hendrycks et al. 2019a)</ref>, prediction entropy <ref type="bibr" target="#b29">(Mohseni et al. 2020)</ref>, and KL-divergence score <ref type="bibr" target="#b20">(Hendrycks et al. 2019b)</ref>. To improve the detection performance, <ref type="bibr" target="#b25">(Lee et al. 2017;</ref><ref type="bibr" target="#b21">Hsu et al. 2020</ref>) proposed a combination of temperature scaling and adversarial perturbation of input samples to calibrate the model to increase the gap between softmax confidence for the inlier and outlier samples. Another line of research proposed using auxiliary unlabeled and disjoint OOD training set to improve OOD detection for efficient OOD detection without architectural changes <ref type="bibr" target="#b19">(Hendrycks, Mazeika, and Dietterich 2018;</ref><ref type="bibr" target="#b29">Mohseni et al. 2020)</ref>.</p><p>Recent work on self-supervised learning shows that adopting pretext tasks results in learning more invariant representations and better semantic understanding of data <ref type="bibr" target="#b8">(Dosovitskiy et al. 2014)</ref> and which significantly improves OOD detection <ref type="bibr" target="#b11">(Golan and El-Yaniv 2018)</ref>. <ref type="bibr" target="#b20">Hendrycks et al. (Hendrycks et al. 2019b</ref>) extended self-supervised techniques with a combination of geometric transformation prediction tasks. Self-supervised contrastive training <ref type="bibr" target="#b3">(Chen et al. 2020)</ref> is also shown to be effective to leverage from multiple random transformations to learn in-domain invariances, resulting in better OOD detection <ref type="bibr" target="#b47">(Winkens et al. 2020;</ref><ref type="bibr" target="#b41">Sehwag, Chiang, and Mittal 2021;</ref><ref type="bibr">Tack et al. 2020</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>In this paper, we propose shifting transformation learning for explicit and efficient training of in-domain representation for improved OOD detection. Intuitively, we simultaneously train a base encoder on multiple shifting transformations of the training data using auxiliary self-supervised objectives for unlabeled data and fully-supervised objectives for labeled data. To illustrate the impact of our approach on OOD detection, <ref type="figure" target="#fig_0">Figure 1</ref>  In this section, we first present our shifting transformation learning framework. Then, we present our model for selecting optimal transformation for a given in-domain set. Finally, we present our detection score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Shifting Transformation Learning</head><p>Our transformation learning technique trains a multi-tasked network using self-supervised and fully-supervised training objectives. We consider a set of geometric (translation, rotation) and non-geometric (blurring, sharpening, color jittering, Gaussian noise, cutout) shifting transformations and we train the network with dedicated loss functions for each transformation. For the self-supervised transformation learning, given an unlabeled training set of S = {(x i )} M i=1 , we denote the set of domain invariant transformations T n by T = {T n } N n=1 . We generate a self-labeled training set</p><formula xml:id="formula_0">S Tn = {(T n (x i ),? i )} M i=1</formula><p>for each self-supervised transformation T n where? i are the transformation labels. For example, we consider the image rotation task with four levels of {0 ? , 90 ? , 180 ? , 270 ? } self- labeled rotations and? i ? {0, 1, 2, 3} in this case. The selfsupervised loss L ssl is the weighted average of loss across all transformations in T :</p><formula xml:id="formula_1">L ssl (?, ?) = 1 N N n=1 ? n (Tn(xi),?i)?S Tn (f (n) ? (T n (x i )),? i ), (1) where f (n) ?</formula><p>is a classification network with parameters ? for the n th task, ? = {? n } N n=1 are transformation weights, and l is the multi-class cross-entropy loss. When labels are available, given the labeled training set of S = {(x i , y i )} M i=1 , we generate transformed copies of the original training sets</p><formula xml:id="formula_2">S Tn = {(T n (x i ), y i )} M i=1</formula><p>where training samples retain their original class labels. The supervised loss L sup is defined by:</p><formula xml:id="formula_3">L sup (? , ?) = 1 N N n=1 ? n (Tn(xi),yi)?S Tn (f (n) ? (T n (x i )), y i ),</formula><p>(2) which measures the classification loss for transformed copies of the data with ? = {? n } N n=1 as transformation coefficients in L sup . In labeled setup, we combine L ssl and L sup with the main supervised learning loss L main (e.g., the cross-entropy loss for classifying the in-domain training set):</p><formula xml:id="formula_4">L total (?, ? , ?) = L main (?) + L ssl (?, ?) + L sup (? , ?) (3)</formula><p>In all unlabeled detection setups, we define L total. := L ssl and discard the main classification task. In the rest of the paper, for the ease of notation, we use ? to refer to all the coefficients {?, ? }, and we drop ?, ? when it is clear from the context.</p><p>Instead of training a separate network f (n) ? or f (n) ? for each task, all the auxiliary tasks and the main classification task share a feature extraction network and each only introduces an additional 2-layer fully-connected head for each task. Training is done in a multi-task fashion in which the network is simultaneously trained for the main classification (if applicable) and all weighted auxiliary tasks using standard cross-entropy loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Learning to Select Optimal Transformations</head><p>Previous work on self-supervised learning used ad-hoc heuristics for choosing data transformations for the training set <ref type="bibr" target="#b20">(Hendrycks et al. 2019b;</ref><ref type="bibr" target="#b11">Golan and El-Yaniv 2018;</ref><ref type="bibr">Tack et al. 2020</ref>). However, the optimal choice of effective transformations depends on the source distribution and heuristic approaches cannot scale up to diverse training distributions when there are many potential transformations. To illustrates this, we train a ResNet-18 <ref type="bibr" target="#b14">(He et al. 2016)</ref> with one or two self-supervised transformations that are selected from a pool of seven transformations. Here, we use the training objective in Eq. 1 with equal weights for all transformations. The OOD detection results are reported in <ref type="figure" target="#fig_2">Figure 2</ref>-top with CIFAR-10, CIFAR-100, and ImageNet-30 <ref type="bibr" target="#b20">(Hendrycks et al. 2019b)</ref> datasets as in-distribution and CIFAR-100, CIFAR-10, and Pets <ref type="bibr" target="#b34">(Parkhi et al. 2012</ref>) as example OOD test sets, respectively. The heatmap visualization presents a clear view of how different transformations (and the combinations of two) have a different impact on the OOD detection performance depending on the source distribution. For example, although rotation is the most effective transformation on CIFAR-10 and ImageNet-30, it is among the least effective ones for CIFAR-100. On the other hand, sharpening and color jittering are among the most effective transformations for CIFAR-100, but they perform worse on CIFAR-10.</p><p>To tackle the problem of selecting optimal transformations, we propose a simple two-step transformation selection framework. Our approach relies on Bayesian optimization to first select effective transformation set T . It then uses metalearning to learn ? for OOD detection as discussed next. Sample a new T set with ? = 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3:</head><p>Train a classifier with L total loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4:</head><p>Calculate L main on D in val as fitness measure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5:</head><p>Update the acquisition function. 6: end while</p><p>Step 2: ? Weights Optimization 1: Initialize with ? = 1. 2: while not converged do 3:</p><p>for K steps do 4:</p><formula xml:id="formula_5">? = ? ? ?? ? L total (?, ?) on D in train 5: end for 6: ? = ? ? ?? ? L main (?) on D in val 7: end while</formula><p>Optimizing Transformations Set T : We use Bayesian optimization to identify effective transformations for each indomain training set as the first step shown in Alg. 1. Here, we assume that transformation weights ? are equal to one and we only search for effective transformations set from a pool of available transformations. Due to the small T search space (i.e., 2 n for n transformations), we use a low-cost off-theshelf Bayesian optimization library <ref type="bibr" target="#b0">(Akiba et al. 2019)</ref> with Tree-Parzen estimators to find the optimum self-supervised task set. The Bayesian optimization objective seeks to minimize the main classification loss L main on D in val , the validation set for the in-domain training data.</p><p>Optimizing Transformations Weights ?: Next, we optimize ? coefficients for the selected transformation from the previous step to improve the effect of shifting transformation on representation learning. This step is important because the ? coefficients modulate the impact of different transformations in the training objective in Eq. 1 and Eq. 2. Here, we assume that ? is a "meta-parameter" and we use a differentiable hyperparameter optimization algorithm <ref type="bibr" target="#b27">(Maclaurin, Duvenaud, and Adams 2015)</ref> for optimizing it as the second step shown in Alg. 1. Our optimization algorithm consists of inner training updates that trains network parameters ? using L total on D in train for K steps. Given the current state of parameters ?, we update ? in the outer loop such that L main (?) is minimized on D in val . Note that the gradient of L main (?) w.r.t. ? is defined only through the gradient updates in the inner loop. Thus, the ? updates in the outer loop require backpropagating through the gradients updates in the inner loop which can be done easily using differentiable optimizers <ref type="bibr" target="#b13">(Grefenstette et al. 2019)</ref>. We use K = 1 step for the inner-loop optimization with SGD when updating ? and we use Adam <ref type="bibr" target="#b24">(Kingma and Ba 2014)</ref> to update ? with small ? learning rate of 0.01. <ref type="figure" target="#fig_2">Figure 2</ref>-bottom presents ? values during optimization from a study on three training sets.</p><p>Because the choice of effective shifting transformations de-pends on the in-domain distribution, our optimization framework avoids the need for D out test samples and only relies on in-domain validation loss as a proxy for representation learning. Our ablation studies show that multi-task training of shifting transformations with this objective function is an effective proxy for selecting optimal transformations for both OOD detection and in-domain generalization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>OOD Detection Scores</head><p>In multi-class detection, we consider two ways for computing the detection score: (i) since all supervised heads are trained on the same task, we get the ? weighted sum of the softmax predictions from the main task and all auxiliary supervised transformation heads to compute an ensemble score. (ii) Alternatively, to reduce the test-time computational complexity, a faster detection score can be computed using only the main classification head. Given softmax scores obtained from either (i) or (ii), in all experiments we use KL-divergence loss between the softmax scores and uniform distribution as the OOD detection score.</p><p>In unlabeled and one-class detection with only selfsupervised heads, we first get the KL-divergence between each auxiliary head and its self-labeled targets as done by <ref type="bibr" target="#b20">Hendrycks et al. (2019b)</ref>, then we calculate the final ensemble score using ? weighted sum of these scores from all auxiliary heads.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments and Results</head><p>We run our main experiments on ResNet-18 <ref type="bibr" target="#b14">(He et al. 2016)</ref> network to have a fair comparison with state-of-the-arts. We used seven different self-labeled transformations including rotation, translation, Gaussian noise, Gaussian blur, cutout, sharpening, and color distortion with details explained in Appendix 1. In all experiments, both transformations set T and their training weights ? are optimized using the proposed framework with the final (T , ?) sets presented in Appendix 1. Unless mentioned otherwise, our main evaluation results are based on the ensembled score from available auxiliary heads.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparison to State-of-the-Arts</head><p>Multi-class Classification <ref type="table" target="#tab_0">Table 1</ref> presents our main evaluation results for multi-class classification training with Eq. 3 on CIFAR-10, CIFAR-100, and ImageNet-30 <ref type="bibr" target="#b20">(Hendrycks et al. 2019b</ref>) datasets each with six disjoint D out test sets with details provided in Appendix 1. We compare our technique with the full supervised Baseline <ref type="bibr" target="#b18">(Hendrycks and Gimpel 2016)</ref> and current state-of-the-art methods including selfsupervised learning (Geometric) <ref type="bibr" target="#b20">(Hendrycks et al. 2019b</ref>), supervised contrastive learning (SupSimCLR) <ref type="bibr" target="#b23">(Khosla et al. 2020</ref>) and SSD <ref type="bibr" target="#b41">(Sehwag, Chiang, and Mittal 2021)</ref>, and contrasting shifted instances (CSI) (Tack et al. 2020) and with its ensembled version (CSI-ens). All techniques are trained on ResNet-18 network with equal training budget, and all except SSD+ use their softmax prediction as OOD detection score in multi-class classification. We compared the impact of both L ssl and L sup + L ssl training loss functions on OOD performance (in addition to L main for the main classification task which is used by all the techniques in <ref type="table" target="#tab_0">Table 1</ref>). Results show <ref type="table" target="#tab_0">Table 1</ref>: Comparison of OOD detection results (AUROC %) with the supervised Baseline, state-of-the-art self-supervised <ref type="bibr" target="#b20">(Hendrycks et al. 2019b</ref>), contrastive learning <ref type="bibr" target="#b23">(Khosla et al. 2020;</ref><ref type="bibr" target="#b41">Sehwag, Chiang, and Mittal 2021;</ref><ref type="bibr">Tack et al. 2020</ref>) and our technique with multi-task self-supervised (L ssl ) and hybrid (L ssl + L sup ) transformation learning tasks. our approach outperforms previous works with a large margin with both L sup and L ssl training objectives. The averaged standard deviation for detection AUROC over six test sets from 5 runs of our techniques shows 0.13% for CIFAR-10, 0.33% for CIFAR-100, and 0.18% for ImageNet-30. Moreover, <ref type="table" target="#tab_0">Table 1</ref> shows that training on the optimized transformation set T and ? weights only using an in-domain validation set consistently outperforms the previous work when testing on diverse D out test sets. This observation highlights the dependency of the optimal set of shifting transformations on the in-domain training set as opposed to prior work that manually selected the shifting transformation. In fact, we observe that all prior work based on rotation transformation perform worse than the suervised Baseline on the CIFAR-100 experiment when testing with CIFAR-10 as the D out test with the exception of CSI-ens.</p><p>Unlabeled Detection Next, we test our technique for multiclass unlabeled and one-class OOD detection trained with the L ssl loss (Eq. 1) using our proposed transformation optimization framework.  <ref type="bibr" target="#b12">(Goyal et al. 2020)</ref>, GOAD (Bergman and Hoshen 2020), Geometric, and SSD, with the exception of CSI which requires a far more computationally expensive distance-based detection score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ablation Studies</head><p>In this section, we provide additional ablation experiments to quantify contributions made by different parts of our model. <ref type="table" target="#tab_4">Table 3</ref>-top presents an analysis of the effects of our transformation set T and ? weights optimizations on OOD detection when training on CIFAR-10 and testing on CIFAR-100 as D out test . When training with all available transformations with equal ? = 1 weights (first row), the detection AUROC drops 2.76% compared to training with both T and ? optimized (forth row). We also observe that when only enabling ? or T optimizations (second and third rows, respectively), the OOD detection performance is sub-optimal. We hypothesize that the gradient-based ? optimization only has access to training signal from a few gradient updates on ? and it does not capture the full effect of mini-batch transformations on OOD detection for long training. In contrast, Bayesian optimization in T optimization can capture the effect of each transformation on the full training and improves OOD detection by a large margin. Nevertheless, we obtain the best OOD detection result (fourth row) with both ? and T optimizations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Transformation Optimization:</head><p>Our method avoids making any assumption on the avail-  <ref type="table" target="#tab_4">Table 3</ref>-bottom, we use a disjoint subset of 80 Million Tiny Images dataset <ref type="bibr" target="#b44">(Torralba, Fergus, and Freeman 2008)</ref> as the source of unlabeled D out and the KL divergence between D out predictions and uniform distribution as the optimization objective for both T and ? optimizations. Interestingly, we only observe a slight performance improvement when assuming access to an auxiliary D out for transformation optimization.</p><p>Advantage of Ensemble Detection Score: <ref type="table" target="#tab_5">Table 4</ref> presents OOD detection performance when using only clean samples for the main classification head compared to using transformed samples for generating an ensemble of detection scores from all auxiliary heads. Results are based on averaged OOD detection AUROC over six D out test sets. We observe a clear performance increase when using the ensemble of auxiliary heads compared to the main classification head.</p><p>Comparison to Data Augmentation Techniques: A natural question to ask is whether the improvements in OOD detection could also be obtained with the supervised baseline that is trained with data augmentations. In <ref type="table" target="#tab_6">Table 5</ref>, we compare our method with the supervised baseline trained with RandAugment <ref type="bibr" target="#b5">(Cubuk et al. 2020)</ref> and AutoAugment <ref type="bibr" target="#b4">(Cubuk et al. 2019</ref>) techniques as two popular augmentation techniques. We observe that both data augmentation techniques achieve competitive OOD detection performance on ResNet-18, improving upon the supervised baseline. However, they perform significantly lower than our proposed shifting transformation learning. Note that we could not use our shifting transformations as the data augmentation in supervised training as our transformations heavily change the input distribution and do not let training converge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>OOD Detection Generalizability</head><p>In real-world application, we characterize four main criteria required from an ideal OOD detection technique, including i)  zero-shot OOD training, ii) no hyperparameter dependency, and iii) generalization to various unseen OOD distributions and iv) robustness against test-time perturbations. In this section, we situate our proposed technique against a diverse range of state-of-the-art OOD detection techniques along these requirements. <ref type="table" target="#tab_7">Table 6</ref> presents results for training on CIFAR-10 and testing on six D out test sets used in <ref type="table" target="#tab_0">Table 1</ref>. Note that this is not intended to be a ranking of different OOD detection techniques; instead, we aim to review trade-offs and limitations among different detection approaches.</p><p>Hyperparameters Dependency: While hyperparameter tuning for the training of in-domain samples is done using a held-out validation set, the hyperparameter disentanglement is a crucial property for OOD detection. Specifically, an ideal detector should not be sensitive to hyperparameters tied to the target outlier distribution. <ref type="table" target="#tab_7">Table 6</ref> divides different techniques w.r.t their dependency on detection hyperparameters into three levels of high, low, and no dependency. Techniques with high dependency like ODIN <ref type="bibr" target="#b26">(Liang, Li, and Srikant 2017)</ref> and <ref type="bibr">Mahalanobis (Lee et al. 2018</ref>) use a validation set of D out for training, resulting in poor performance under unseen or diverse mixture of outlier distributions. <ref type="table" target="#tab_7">Table 6</ref> shows over 3% performance gap between the averaged detection performance on six D out test sets (Column 5) and detection performance under an equal mixture of the same test sets (Column 6) for these two detectors which indicates strong D out hyperparameter dependency.</p><p>Techniques with low dependency do not use a subset of D out test , however, they depend on hyperparameters such as the choice of D out train set (e.g., Outlier Exposure (Hendrycks, Mazeika, and Dietterich 2018)), or hand-crafted self-supervised tasks <ref type="bibr" target="#b11">(Golan and El-Yaniv 2018)</ref>, <ref type="bibr" target="#b20">(Hendrycks et al. 2019b</ref><ref type="bibr">), or data augmentation (Tack et al. 2020</ref>) that requires post training D out test for validation. These tech-  niques can suffer significantly in settings in which the new source training set is invariant to previous hand-crafted selfsupervised tasks and augmentations as seen in <ref type="figure" target="#fig_2">Figure 2</ref>. On the other hand, techniques with no hyperparameters like Gram Matrices (Sastry and Oore 2019), SSD <ref type="bibr" target="#b41">(Sehwag, Chiang, and Mittal 2021)</ref>, and our proposed framework bears no hyperparameter dependency on the choices of in-domain or outlier distribution. Note that many techniques, like ours, use a ? training hyperparameter to balance training between in-domain classification and auxiliary tasks. However, in our case these hyperparameters are tuned automatically without requiring OOD training samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Zero-shot Training:</head><p>A previous trend in OOD detection techniques considered using a subset of the target D out test for model tuning (e.g., ODIN <ref type="bibr" target="#b26">(Liang, Li, and Srikant 2017)</ref> and Mahalanobis (Lee et al. 2018)) or using an auxiliary D out train set as a part of model training (e.g., Outlier Exposure <ref type="bibr" target="#b19">(Hendrycks, Mazeika, and Dietterich 2018)</ref>). Although these techniques can achieve high detection performance with the right training set, having access to the specific D out tune for tuning or even any D out train for training the detector is not a realistic assumption in practical setups. An efficient proposal to use these techniques is to integrate them into zero-shot techniques as presented by <ref type="bibr" target="#b40">(Sastry and Oore 2019;</ref><ref type="bibr" target="#b20">Hendrycks et al. 2019b</ref>) when D out train is available or to benefit from taking semi-supervised or few-shot approaches as done by <ref type="bibr" target="#b38">(Ruff et al. 2019;</ref><ref type="bibr" target="#b41">Sehwag, Chiang, and Mittal 2021)</ref>.</p><p>Detection Generalizability: Recent work on OOD detection recognized the necessity of diverse D out test sets to evaluate the generalizability of OOD detection techniques <ref type="bibr" target="#b29">(Mohseni et al. 2020;</ref><ref type="bibr" target="#b47">Winkens et al. 2020;</ref><ref type="bibr" target="#b40">Sastry and Oore 2019)</ref>. Typically, near-OOD and far-OOD sets are chosen based on the semantic and appearance similarities between the indomain and outlier distributions and, in some cases, measured by relevant similarity metrics (e.g., confusion log probability <ref type="bibr" target="#b47">(Winkens et al. 2020)</ref>). Following the previous works, we chose CIFAR-100 as the near-OOD test distribution and SVHN as the far-OOD test distribution for detectors trained on CIFAR-10. While <ref type="table" target="#tab_7">Table 6</ref> shows high performance on far-OOD for all techniques, Gram Matrices, Mahalanobis, and ODIN show 20.5%, 10.9%, and 10.6% detection performance drop for near-OOD distribution compared to the far-OOD test distribution, respectively. In comparison, our technique shows 2.53% performance gap between far-OOD and near-OOD test distributions.</p><p>Detection Robustness: Evaluating the effects of distribution shift on predictive uncertainty have been previously studied in <ref type="bibr" target="#b12">Goyal et al. 2020</ref>) for real-world application. In Appendix 2, we investigate the effect of natural perturbations and corruptions proposed in <ref type="bibr" target="#b17">(Hendrycks and Dietterich 2019)</ref> on OOD detection performance. We measure averaged OOD detection results for all 15 image distortions on 5 levels of intensity where both D in test and D out test are treated with the same distortion type and level. <ref type="figure">Figure  4</ref> in Appendix 2 presents detailed OOD detection results in which all techniques show more performance drop at the higher levels of perturbation intensity. However, distancebased detectors <ref type="figure">(Figure 4-a</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>Developing reliable and trustworthy machine learning algorithms for open-world and safety-critical applications poses a great challenge. In this paper, we presented a simple framework for OOD detection that leverages representation learning with shifting data transformations, and we empirically demonstrated its efficacy on several image datasets. We showed that the optimal choice of shifting transformation depends on the in-domain training distribution and we propose a framework to automatically choose the optimal transformations for a given in-domain set without requiring any OOD training samples. Albeit its simplicity, our proposed method outperforms the state-of-the-art OOD detection techniques and exhibits strong generalization to different outlier distributions. A limitation of our work is longer training time and large memory requirement due to the large training batch size. Future work is focused on improving the efficiency and scalability of shifted transformation learning for larger datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Experiments Details</head><p>Dataset details: Our experiments are focused on image domain and we use CIFAR-10 <ref type="bibr" target="#b25">(Krizhevsky, Hinton et al. 2009</ref>), CIFAR-100 <ref type="bibr" target="#b25">(Krizhevsky, Hinton et al. 2009</ref>), and ImageNet-30 <ref type="bibr" target="#b20">(Hendrycks et al. 2019b</ref>) in multi-class and unlabeled detection. CIFAR-10 and CIFAR-100 consist of 50,000 training and 10,000 test samples, respectively. ImageNet-30 is a selection of 30 classes from ImageNet <ref type="bibr" target="#b7">(Deng et al. 2009</ref>) dataset that contains 39,000 training and 3,000 test samples.</p><p>In one-class classification, we only used single classes of CIFAR-10 as training source (D in train ) and the other classes as test set (D out test ). All D out test test images are resized to the D in train image size which is 32x32 in all CIFAR-10/100 experiments. In ImageNet-30 experiments, we first resize the images to 256x256 and then center crop to 224x224.</p><p>Training Details: All experiments are based on ResNet-18 network with mini-batch size of 64, SGD optimizer with momentum of 0.9, initial learning rate of 0.1 (decayed using a cosine annealing schedule). Despite the set of shifting transformations, we still use a few data invariant "native augmentations" including random horizontal flip and small random crop and padding for the main supervised head. We use cross-entropy loss for all supervised and self-supervised branches with labels generated for self-supervised tasks. We apply all transformation targets (e.g., all four rotations for the rotation transformation) from T on every mini-batch during the training, and therefore, the final mini-batch is the base mini-batch size multiplied by the total number of shifting transformations. We observe that learning multiple shifting transformations benefits from longer training time (similar to contrastive learning setups). So we train all multi-class classification models for 800 epochs (i.e., <ref type="table" target="#tab_0">Table 1</ref>) and unsupervised models for 200 epochs (i.e., <ref type="table" target="#tab_2">Table 2</ref>) . Ablation studies presented in <ref type="figure" target="#fig_2">Figure 2</ref> and <ref type="table" target="#tab_4">Table 3</ref> are trained for 200 epochs.</p><p>T and ? Optimization: As described in Algorithm 1, we first run the Bayesian optimization to find the optimum T set, followed by the meta-learning optimization to optimize all training ? weights. Despite efforts on solely using metalearning optimization for finding the optimum T set and ? weights, we found that gradient-based optimization is not able to capture long effect of shifting transformations on OOD performance. Instead, given the small search space for the small number of image transformations, Bayesian optimization served well for this task. Note that this is only the search step, and hence we use a short training with 50 epochs on each iteration.</p><p>Our framework finds the following transformation and weight pairs for the CIFAR-10 dataset {(Jitter, 3.2791), (Rotation, 2.7547), (Sharpening, 2.6906)} with ? 0 = 4.0760. In CIFAR-100 dataset: {(blur, 4.3051), (Jitter, 2.2612), (Translate, 2.9636), (Sharpening, 3.9634)} with ? 0 = 8.6546. In ImageNet-30: {(Noise, 5.3806), (Rotation, 3.3754), (Sharpening, 4.7626)} with ? 0 = 9.3599.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Transformations Details</head><p>In contrast to common data augmentations, we followed <ref type="bibr" target="#b8">(Dosovitskiy et al. 2014)</ref> to apply transformations to the extreme degree. We used </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Evaluation Setup and Metrics:</head><p>We evaluate the OOD detection performance using multiple diverse D out test sets to determine how well the detector can generalize on these unseen distributions, including test sets of SVHN <ref type="bibr" target="#b31">(Netzer et al. 2011</ref><ref type="bibr">), TinyImageNet (Russakovsky et al. 2015</ref>), Places365 <ref type="bibr" target="#b49">(Zhou et al. 2017)</ref>, LSUN <ref type="bibr" target="#b48">(Yu et al. 2015)</ref>, and CIFAR-10 (or CIFAR-100 when CIFAR-10 is the source training set) for CIFAR-10 and CIFAR-100 experiments and Pets <ref type="bibr" target="#b34">(Parkhi et al. 2012)</ref>, Flowers-101 <ref type="bibr" target="#b32">(Nilsback and Zisserman 2006)</ref>), <ref type="bibr">CUB-200 (Wah et al. 2011</ref><ref type="bibr">), Dogs (Khosla et al. 2011</ref>, Food <ref type="bibr" target="#b2">(Bossard, Guillaumin, and Van Gool 2014)</ref> for ImageNet-30 experiments. We choose the area under the receiver operating characteristic curve (AUROC) <ref type="bibr" target="#b6">(Davis and Goadrich 2006)</ref> as a threshold agnostic metric in all evaluations. The AUROC will be 100% for the perfect detector and 50% for a random detector. In all evaluations, we use D out test (test set of the outlier dataset) as positive OOD samples and the D in test (test set of source training dataset) as negative samples for detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Classification Accuracy</head><p>Since our technique enjoys from learning domain knowledge through multiple data transformations, we first compare the supervised classification accuracy of our technique with stateof-the-art self-supervised and contrastive learning techniques. CIFAR-100 datasets trained on WideResNet-40-2, ResNet-18, and ResNet-50 networks. Our technique outperforms other techniques across both datasets in WideResNet and ResNet-18 networks and achieves competitive performance in ResNet-50, which indicates the effectiveness of transformation learning for better generalization. The improvement is more visible in smaller network sizes like WideResNet-40-2 (e.g., 3.7% gain over Baseline in CIFAR-10) and the smaller training sets (e.g., 8.64% gain over Baseline in CIFAR-100).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Additional Robustness Results</head><p>Figure 4 presents a robustness evaluation for different distance-based and classification-based techniques under testtime perturbations. In this experiment, both in-domain and OOD samples are perturbed with 15 natural perturbations on 5 levels of intensity proposed by <ref type="bibr" target="#b17">(Hendrycks and Dietterich 2019)</ref>. Our results indicate that all techniques show more performance drop at the higher levels of perturbation intensity. Specifically, distance-based detectors show significantly less performance drop compared to classification-based detectors like Outlier Exposure and Geometric with over 14% AU-ROC drop. Noticeably, the Mahalanobis detector with access to perturbed D out test samples for tuning maintains fairly high detection performance under all perturbation types.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Computation Complexity:</head><p>We emphasize the importance of test-time computation costs in real-world applications with resource and time constraints. Classification-based techniques tend to perform faster as they only use the model prediction probability from the main classification or an ensemble of multiple tasks. For example, the Baseline, Outlier Exposure, and ODIN technique only use the model softmax prediction as the OOD detection score.  based on k-mean clustering which significantly improves the detection time. <ref type="table" target="#tab_9">Table 7</ref> presents a comparison between detection inference time on a ResNet-18 network trained on CIFAR-10 running on the same system with a single RTX 1080ti GPU. Inference time is measured for the entire D out test set based on equal mixture of five test set. We encourage future research to investigate opportunities for distance-based and classification-based detectors in different applications of OOD detection.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>The t-SNE visualization of the penultimate layer features in a ResNet-18 network trained on CIFAR-10 using (a) supervised learning with cross-entropy loss and (b) our method with shifting transformation learning. OOD samples (Place365 dataset) are presented in gray.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>shows t-SNE visualization (Van der Maaten and Hinton 2008) of the CIFAR-10 examples obtained from ResNet-18 (He et al. 2016) trained with the cross-entropy loss (left) compared to our multitask transformation learning (right) with Places365 examples as the OOD test set. The visualization intuitively shows how training with multiple shifted in-domain samples improves the separation between OOD and in-domain samples without the need for any OOD training set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Our studies show that the optimal transformation set T and their weights ? depend on the in-domain training set. Top: Ablation study to measure effects of individual and paired transformations on OOD detection performance. Bottom: Optimizing transformation weights (?) for auxiliary self-supervised tasks for each training set. Experiments are done in multi-class classification setup on different training sets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>ability of any D out for model training or hyperparameter optimization unlike prior work such as ODIN (Lee et al. 2017), Mahalanobis (Lee et al. 2018), Outlier Exposure (Hendrycks, Mazeika, and Dietterich 2018) detectors. In the absence of OOD samples for training, we utilize the in-domain validation loss as a proxy and we examine the generalization capability of our model to unseen examples to guide the optimization of shifting transformations in Alg. 1. In an ablation experiment in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>) like Gram and Mahalanobis show significantly less performance drop (4.23% and 5.24% AUROC drop, respectively) compared to classification-based detectors (Figure 4-b) like Outlier Exposure and Geometric with over 14% AUROC drop. Our experiments indicate the advantage of distance-based detection methods in OOD detection under test-time input perturbations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 :</head><label>3</label><figDesc>Classification accuracy on CIFAR-10 (Top) and CIFAR-100 (Bottom) datasets. 7 different self-labeled transformations including rotation ({0 ? , 90 ? , 180 ? , 270 ? }) , translation (combinations of ? 30 % horizontal and vertical translations) , Gaussian noise (with standard deviations of {0, 0.3, 0.5, 0.8}), Gaussian blur (with sigmas of {0, 0.3, 0.5, 0.8}), cutout (rectangular cut with sizes of {0, 0.3, 0.5, 0.8}), sharpening (image blended with its convolution-based edges with alphas of {0, 0.3, 0.6, 1.0}), and color distortion (jittered brightness, contrast, saturation, and hue by rates of {0, 0.4, 0.6, 0.8}).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>Figure 3presents classification accuracy of CIFAR-10 and (a) Distance-based techniques (b) Classification-based techniques Figure 4: OOD detection robustness results for (Left) distance-based and (Right) Classification-based techniques with both D out test and D in test perturbed under 5 levels of intensity. ? sign represents the mean AUROC.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Algorithm 1 :</head><label>1</label><figDesc>Transformations T and ? Optimization Input: Available transformation set T , learning rate ?, ?, inner steps K Output: Optimal T opt and ? opt sets Step 1: Transformations Selection 1: while not converged do</figDesc><table /><note>2:</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell>(a) presents results for unlabeled</cell></row><row><cell>multi-class detection in which averaged detection AUROC</cell></row><row><cell>over the six D out test sets is outperforming state-of-the-art meth-</cell></row><row><cell>ods with a large margin except for the CIFAR-10 experiment.</cell></row><row><cell>Table 2(b) shows detailed one-class classification results for</cell></row><row><cell>each of the CIFAR-10 classes as D in train and the remain-ing classes as D out test . Our technique with 90.9% averaged</cell></row><row><cell>AUROC on CIFAR-10 one-class detection outperforms pre-</cell></row><row><cell>vious works including DROCC</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Comparison of OOD detection results (AUROC %) with different one-class classification and unlabeled multiclass datasets on CIFAR-10, CIFAR-100, and ImageNet-30.(a) Unlabeled CIFAR-10, CIFAR-100, and ImageNet-30</figDesc><table><row><cell>D in</cell><cell cols="5">Geometric SimCLR SSD CSI-ens</cell><cell>Ours (L ssl )</cell></row><row><cell>CIFAR-10</cell><cell>86.04</cell><cell>77.84</cell><cell cols="2">84.54</cell><cell>91.99</cell><cell>89.8</cell></row><row><cell>CIFAR-100</cell><cell>75.28</cell><cell>48.81</cell><cell cols="2">66.41</cell><cell>71.91</cell><cell>83.95</cell></row><row><cell>ImageNet-30</cell><cell>85.11</cell><cell>65.27</cell><cell cols="2">87.42</cell><cell>92.13</cell><cell>96.57</cell></row><row><cell cols="6">(b) One-class Detection on CIFAR-10</cell></row><row><cell>D in</cell><cell cols="4">DROCC GOAD Geom. SSD</cell><cell>CSI -ens</cell><cell>Ours (L ssl )</cell></row><row><cell>Airplane</cell><cell>81.7</cell><cell>75.5</cell><cell>80.2</cell><cell cols="3">82.7 90.0 84.3</cell></row><row><cell>Automobile</cell><cell>76.7</cell><cell>94.2</cell><cell>96.6</cell><cell cols="3">98.5 99.1 96.0</cell></row><row><cell>Bird</cell><cell>66.6</cell><cell>82.4</cell><cell>85.9</cell><cell cols="3">84.2 93.3 87.7</cell></row><row><cell>Cat</cell><cell>67.2</cell><cell>72.1</cell><cell>81.7</cell><cell cols="3">84.5 86.4 82.3</cell></row><row><cell>Deer</cell><cell>73.6</cell><cell>83.7</cell><cell>91.6</cell><cell cols="3">84.8 94.8 91.0</cell></row><row><cell>Dog</cell><cell>74.4</cell><cell>84.8</cell><cell>89.8</cell><cell cols="3">90.9 94.4 91.5</cell></row><row><cell>Frog</cell><cell>74.4</cell><cell>82.8</cell><cell>90.2</cell><cell cols="3">91.7 94.4 91.1</cell></row><row><cell>Horse</cell><cell>71.3</cell><cell>93.4</cell><cell>96.1</cell><cell cols="3">95.2 95.2 96.3</cell></row><row><cell>Ship</cell><cell>80.0</cell><cell>92.6</cell><cell>95.1</cell><cell cols="3">92.9 98.2 96.3</cell></row><row><cell>Truck</cell><cell>76.2</cell><cell>85.1</cell><cell>92.8</cell><cell cols="3">94.4 97.9 92.3</cell></row><row><cell>Average</cell><cell>74.2</cell><cell>84.7</cell><cell>90.0</cell><cell cols="3">90.0 94.3 90.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Ablation study on T and ? n optimizations and optimization loss access to D out ? n opt. T opt. AUROC</figDesc><table><row><cell>-</cell><cell>-</cell><cell>-</cell><cell>90.36</cell></row><row><cell>-</cell><cell></cell><cell>-</cell><cell>91.72</cell></row><row><cell>-</cell><cell>-</cell><cell></cell><cell>92.90</cell></row><row><cell>-</cell><cell></cell><cell></cell><cell>93.12</cell></row><row><cell></cell><cell></cell><cell></cell><cell>93.24</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Detection using only the main classification head vs. ensemble of auxiliary heads.</figDesc><table><row><cell>D in</cell><cell>only main head</cell><cell>ensemble aux. heads</cell></row><row><cell>CIFAR-10</cell><cell>93.83</cell><cell>95.67</cell></row><row><cell>CIFAR-100</cell><cell>79.30</cell><cell>84.35</cell></row><row><cell>ImageNet-30</cell><cell>92.59</cell><cell>96.69</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Detection AUROC with our technique vs. data augmentation.</figDesc><table><row><cell>D in</cell><cell cols="4">Baseline AutoAug. RandAug. Ours</cell></row><row><cell>CIFAR-10</cell><cell>88.98</cell><cell>92.46</cell><cell>92.72</cell><cell>95.67</cell></row><row><cell>CIFAR-100</cell><cell>76.84</cell><cell>78.68</cell><cell>78.62</cell><cell>84.35</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>Review of OOD detection criteria, averaged detection performance, and generalizability to unseen OOD test distributions (AUROC %) for a diverse set of OOD detection techniques. We compare our technique with ODIN (Lee et al. 2017), Mahalanobis (Lee et al. 2018), Outlier Exposure<ref type="bibr" target="#b19">(Hendrycks, Mazeika, and Dietterich 2018)</ref>, Geometric<ref type="bibr" target="#b20">(Hendrycks et al. 2019b)</ref>,CSI (Tack  et al. 2020), and Gram<ref type="bibr" target="#b40">(Sastry and Oore 2019)</ref>.</figDesc><table><row><cell>Detection</cell><cell cols="2">OOD Detection Criteria</cell><cell></cell><cell>Averaged</cell><cell cols="2">Generalizability Tests</cell><cell></cell></row><row><cell>Technique</cell><cell>Hyp.-Para. Dependency</cell><cell>Generalizable</cell><cell>Zero Shot</cell><cell>Detection Performance</cell><cell>Mixed Distribution</cell><cell cols="2">Far-OOD Near-OOD</cell></row><row><cell>ODIN</cell><cell>High</cell><cell>-</cell><cell>-</cell><cell>91.15</cell><cell>88.10</cell><cell>96.70</cell><cell>85.80</cell></row><row><cell>Mahalanobis</cell><cell>High</cell><cell>-</cell><cell>-</cell><cell>95.35</cell><cell>92.24</cell><cell>99.10</cell><cell>88.51</cell></row><row><cell>Outlier Exposure</cell><cell>Low</cell><cell></cell><cell>-</cell><cell>96.24</cell><cell>96.88</cell><cell>98.76</cell><cell>93.41</cell></row><row><cell>Geometric</cell><cell>Low</cell><cell></cell><cell></cell><cell>94.05</cell><cell>94.29</cell><cell>97.96</cell><cell>91.91</cell></row><row><cell>CSI-ens</cell><cell>Low</cell><cell></cell><cell></cell><cell>94.37</cell><cell>94.10</cell><cell>97.38</cell><cell>92.06</cell></row><row><cell>SSD</cell><cell>No</cell><cell></cell><cell></cell><cell>92.45</cell><cell>92.70</cell><cell>93.80</cell><cell>90.40</cell></row><row><cell>Gram Matrices</cell><cell>No</cell><cell>-</cell><cell></cell><cell>94.17</cell><cell>95.08</cell><cell>99.50</cell><cell>79.01</cell></row><row><cell>Ours</cell><cell>No</cell><cell></cell><cell></cell><cell>95.67</cell><cell>95.55</cell><cell>96.60</cell><cell>94.07</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>However, distance-based methods carry an overhead to measure the unknown D out test samples' distance from the seen D in train set. For instance, the CSI-ens (Tack et al. 2020) technique uses an ensemble of distances (cosine similarity distance) from multiple augmented copies of the D out test to the entire or a subset of D in train . On the other hand, SSD (Sehwag, Chiang, and Mittal 2021) detector measures the Mahalanobis distance between D out test and a pre-trained representation of D in train</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 7 :</head><label>7</label><figDesc>Detection inference time averaged on five D out test sets.</figDesc><table><row><cell>Detector</cell><cell>Inference Time (s)</cell></row><row><cell>Baseline</cell><cell>9.3s</cell></row><row><cell>OE</cell><cell>9.3s</cell></row><row><cell>Geometric</cell><cell>39.1s</cell></row><row><cell>Ours</cell><cell>76.3s</cell></row><row><cell>SSD</cell><cell>23.6</cell></row><row><cell>CSI</cell><cell>18.7s</cell></row><row><cell>CSI-ens</cell><cell>163.2s</cell></row><row><cell>Gram</cell><cell>323.9</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Optuna: A next-generation hyperparameter optimization framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Akiba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yanase</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ohta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Koyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2623" to="2631" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Classification-based anomaly detection for general data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bergman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hoshen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.02359</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Food-101-mining discriminative components with random forests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bossard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Guillaumin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="446" to="461" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.05709</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Autoaugment: Learning augmentation strategies from data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="113" to="123" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Randaugment: Practical automated data augmentation with a reduced search space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="702" to="703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The relationship between Precision-Recall and ROC curves</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Goadrich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="233" to="240" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Discriminative unsupervised feature learning with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="766" to="774" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Dropout as a bayesian approximation: Representing model uncertainty in deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1050" to="1059" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Unsupervised representation learning by predicting image rotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.07728</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep anomaly detection using geometric transformations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Golan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>El-Yaniv</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="9758" to="9769" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">DROCC: Deep Robust One-Class Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Raghunathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">V</forename><surname>Simhadri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th International Conference on Machine Learning</title>
		<editor>III, H. D.</editor>
		<editor>and Singh, A.</editor>
		<meeting>the 37th International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">119</biblScope>
			<biblScope unit="page" from="3711" to="3721" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Amos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yarats</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">M</forename><surname>Htut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Molchanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Meier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kiela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.01727</idno>
		<title level="m">Generalized Inner Loop Meta-Learning</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Why ReLU networks yield high-confidence predictions far away from the training data and how to mitigate the problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriushchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bitterwolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Basart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mazeika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mostajabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Steinhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Song</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.11132</idno>
		<title level="m">A benchmark for anomaly segmentation</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Benchmarking Neural Network Robustness to Common Corruptions and Perturbations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Dietterich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">A baseline for detecting misclassified and out-of-distribution examples in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gimpel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.02136</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep Anomaly Detection with Outlier Exposure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mazeika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Dietterich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Using self-supervised learning can improve model robustness and uncertainty</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mazeika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kadavath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="15663" to="15674" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Generalized odin: Detecting out-of-distribution image without learning from out-of-distribution data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-C</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Kira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10951" to="10960" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Novel dataset for fine-grained image categorization: Stanford dogs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jayadevaprakash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F.-F</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR Workshop on Fine-Grained Visual Categorization (FGVC)</title>
		<meeting>CVPR Workshop on Fine-Grained Visual Categorization (FGVC)</meeting>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2011" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Supervised Contrastive Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Teterwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sarna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Maschinot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">33</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A simple unified framework for detecting out-of-distribution samples and adversarial attacks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.09325</idno>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="7167" to="7177" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Training confidence-calibrated classifiers for detecting out-ofdistribution samples</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Enhancing the reliability of out-of-distribution image detection in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Srikant</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02690</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Gradientbased hyperparameter optimization through reversible learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Maclaurin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2113" to="2122" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Practical solutions for machine learning safety in autonomous vehicles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mohseni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pitale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.09630</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Self-Supervised Learning for Generalizable Out-of-Distribution Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mohseni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pitale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yadawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Deterministic Neural Networks with Appropriate Inductive Biases Capture Epistemic and Aleatoric Uncertainty</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mukhoti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Van Amersfoort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.11582</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Reading digits in natural images with unsupervised feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Netzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bissacco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A visual vocabulary for flower classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-E</forename><surname>Nilsback</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR&apos;06)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2006" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1447" to="1454" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Detection and Retrieval of Out-of-Distribution Objects in Semantic Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Oberdiek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rottmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">A</forename><surname>Fink</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="328" to="329" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Cats and dogs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">M</forename><surname>Parkhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Jawahar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="3498" to="3505" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Unsupervised Anomaly Detection From Semantic Similarity Scores</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Rafiee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gholamipoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kollmann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.00461</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Likelihood ratios for out-of-distribution detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Fertig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Poplin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Depristo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lakshminarayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="14707" to="14718" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Deep one-class classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ruff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vandermeulen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Goernitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deecke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Siddiqui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Binder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kloft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4393" to="4402" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Deep Semi-Supervised Anomaly Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ruff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Vandermeulen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>G?rnitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Binder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-R</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kloft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Detecting Out-of-Distribution Examples with In-distribution Examples and Gram Matrices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">S</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Oore</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.12510</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">{SSD}: A Unified Framework for Self-Supervised Outlier Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sehwag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mittal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Can you trust your model&apos;s uncertainty? Evaluating predictive uncertainty under dataset shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ovadia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Fertig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lakshminarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sculley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">V</forename><surname>Dillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">; J</forename><surname>Nado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">34th Conference on Neural Information Processing Systems (NeurIPS) 2020. Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>CSI: Novelty Detection via Contrastive Learning on Distributionally Shifted Instances</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Hyperparameter-Free Out-of-Distribution Detection Using Cosine Similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Techapanurak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Suganuma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Okatani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Asian Conference on Computer Vision</title>
		<meeting>the Asian Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">80 million tiny images: A large data set for nonparametric object and scene recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1958" to="1970" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Visualizing data using t-SNE</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<title level="m">The caltech-ucsd birds-200-2011 dataset</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winkens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bunel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Stanforth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Natarajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Ledsam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Macwilliams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karthikesalingam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kohl</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.05566</idno>
		<title level="m">Contrastive training for improved out-of-distribution detection</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Seff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.03365</idno>
		<title level="m">Lsun: Construction of a large-scale image dataset using deep learning with humans in the loop</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Places: A 10 million image database for scene recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1452" to="1464" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
