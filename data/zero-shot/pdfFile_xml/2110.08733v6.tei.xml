<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">LoveDA: A Remote Sensing Land-Cover Dataset for Domain Adaptive Semantic Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjue</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">State Key Laboratory of Information Engineering in Surveying, Mapping, and Remote Sensing Wuhan University</orgName>
								<address>
									<postCode>430074</postCode>
									<settlement>Wuhan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuo</forename><surname>Zheng</surname></persName>
							<email>zhengzhuo@whu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">State Key Laboratory of Information Engineering in Surveying, Mapping, and Remote Sensing Wuhan University</orgName>
								<address>
									<postCode>430074</postCode>
									<settlement>Wuhan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ailong</forename><surname>Ma</surname></persName>
							<email>maailong007@whu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">State Key Laboratory of Information Engineering in Surveying, Mapping, and Remote Sensing Wuhan University</orgName>
								<address>
									<postCode>430074</postCode>
									<settlement>Wuhan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyan</forename><surname>Lu</surname></persName>
							<email>luxiaoyan@whu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">State Key Laboratory of Information Engineering in Surveying, Mapping, and Remote Sensing Wuhan University</orgName>
								<address>
									<postCode>430074</postCode>
									<settlement>Wuhan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanfei</forename><surname>Zhong</surname></persName>
							<email>zhongyanfei@whu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">State Key Laboratory of Information Engineering in Surveying, Mapping, and Remote Sensing Wuhan University</orgName>
								<address>
									<postCode>430074</postCode>
									<settlement>Wuhan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">LoveDA: A Remote Sensing Land-Cover Dataset for Domain Adaptive Semantic Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T08:23+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Deep learning approaches have shown promising results in remote sensing high spatial resolution (HSR) land-cover mapping. However, urban and rural scenes can show completely different geographical landscapes, and the inadequate generalizability of these algorithms hinders city-level or national-level mapping. Most of the existing HSR land-cover datasets mainly promote the research of learning semantic representation, thereby ignoring the model transferability. In this paper, we introduce the Land-cOVEr Domain Adaptive semantic segmentation (LoveDA) dataset to advance semantic and transferable learning. The LoveDA dataset contains 5987 HSR images with 166768 annotated objects from three different cities. Compared to the existing datasets, the LoveDA dataset encompasses two domains (urban and rural), which brings considerable challenges due to the: 1) multi-scale objects; 2) complex background samples; and 3) inconsistent class distributions. The LoveDA dataset is suitable for both land-cover semantic segmentation and unsupervised domain adaptation (UDA) tasks. Accordingly, we benchmarked the LoveDA dataset on eleven semantic segmentation methods and eight UDA methods. Some exploratory studies including multi-scale architectures and strategies, additional background supervision, and pseudo-label analysis were also carried out to address these challenges. The code and data are available at https://github.com/Junjue-Wang/LoveDA. * Equal contribution.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>With the continuous development of society and economy, the human living environment is gradually being differentiated, and can be divided into urban and rural zones <ref type="bibr" target="#b7">[8]</ref>. High spatial resolution (HSR) remote sensing technology can help us to better understand the geographical and ecological environment. Specifically, land-cover semantic segmentation in remote sensing is aimed at determining the land-cover type at every image pixel. The existing HSR land-cover datasets such as the Gaofen Image Dataset (GID) <ref type="bibr" target="#b37">[38]</ref>, DeepGlobe <ref type="bibr" target="#b8">[9]</ref>, Zeebruges <ref type="bibr" target="#b23">[24]</ref>, and Zurich Summer <ref type="bibr" target="#b42">[43]</ref> contain large-scale images with pixel-wise annotations, thus promoting the development of fully convolutional networks (FCNs) in the field of remote sensing <ref type="bibr" target="#b46">[46,</ref><ref type="bibr" target="#b49">49]</ref>. However, these datasets are designed only for semantic segmentation, and they ignore the diverse styles among geographic areas. For urban and rural areas, in particular, the manifestation of the land cover is completely different, in the class distributions, object scales, and pixel spectra. In order to improve the model generalizability for large-scale land-cover mapping, appropriate datasets are required.</p><p>In this paper, we introduce an HSR dataset for Land-cOVEr Domain Adaptive semantic segmentation (LoveDA) for use in two challenging tasks: semantic segmentation and UDA. Compared with the UDA datasets <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b38">39]</ref> that use simulated images, the LoveDA dataset contains real urban and rural remote sensing images. Exploring the use of deep transfer learning methods on this dataset will be a meaningful way to promote large-scale land-cover mapping. The major characteristics of this dataset are summarized as follows: 1) Multi-scale objects. The HSR images were collected from 18 complex urban and rural scenes, covering three different cities in China. The objects in the same category are in completely different geographical landscapes in the different scenes, which increases the scale variation. 2) Complex background samples. The remote sensing semantic segmentation task is always faced with the complex background samples (i.e., land-cover objects that are not of interest) <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b54">54]</ref>, which is particularly the case in the LoveDA dataset. The high-resolution and different complex scenes bring more rich details as well as larger intra-class variance for the background samples. 3) Inconsistent class distributions. The urban and rural scenes have different class distributions. The urban scenes with high population densities contain lots of artificial objects such as buildings and roads. In contrast, the rural scenes include more natural elements, such as forest and water. Compared with UDA datasets <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b41">42]</ref> in general computer vision, the LoveDA dataset focuses on the style differences of the geographical environments. The inconsistent class distributions pose a special challenge for the UDA task.</p><p>As the LoveDA dataset was built with two tasks in mind, both advanced semantic segmentation and UDA methods were evaluated. Several exploratory experiments were also conducted to solve the particular challenges inherent in this dataset, and to inspire further research. A stronger representational architecture and UDA method are needed to jointly promote large-scale land cover mapping. The abbreviations are: SS -semantic segmentation, UDA -unsupervised domain adaptation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Land-cover semantic segmentation datasets</head><p>Land-cover semantic segmentation, as a long-standing research topic, has been widely explored over the past decades. The early research relied on low-and medium-resolution datasets, such as MCD12Q1 <ref type="bibr" target="#b32">[33]</ref>, the National Land Cover Database (NLCD) <ref type="bibr" target="#b13">[14]</ref>, GlobeLand30 <ref type="bibr" target="#b14">[15]</ref>, LandCoverNet <ref type="bibr" target="#b0">[1]</ref>, etc. However, these studies all focused on large-scale mapping and analysis from a macro-level. With the advancement of remote sensing technology, massive HSR images are now being obtained on a daily basis from both spaceborne and airborne platforms. Due to the advantages of the clear geometrical structure and fine texture, HSR land-cover datasets are tailored for specific scenes at a micro-level. As is shown in <ref type="table" target="#tab_0">Table 1</ref>, datasets such as ISPRS Potsdam 3 , ISPRS Vaihingen 4 , Zurich Summer <ref type="bibr" target="#b42">[43]</ref>, and Zeebruges <ref type="bibr" target="#b23">[24]</ref> are designed for urban parsing. These datasets only contain a small number of annotated images and cover limited areas. In contrast, DeepGlobe <ref type="bibr" target="#b8">[9]</ref> and LandCover.ai <ref type="bibr" target="#b1">[2]</ref> focus on rural areas with a larger scale, in which the homogeneous areas contain few man-made structures. The GID dataset <ref type="bibr" target="#b37">[38]</ref> was collected with Gaofen-2 satellite from different cities in China. Although LandCoverNet and GID datasets contain both urban and rural areas, the geo-locations of these released images are private. Therefore, the urban and rural areas are not able to be divided. In addition, the identifications of cities in released GID images have been already removed so it is hard to perform UDA tasks. Considering limited coverage and annotation cost, the existing HSR datasets mainly promote the research of improving land-cover segmentation accuracy, ignoring its transferability. Compared with land-cover datasets, the iSAID dataset <ref type="bibr" target="#b48">[48]</ref> focuses on key objects semantic segmentation. The different study objects bring different challenges for different remote sensing tasks.</p><p>These HSR land-cover datasets have all promoted the development of semantic segmentation, and many variants of FCNs <ref type="bibr" target="#b18">[19]</ref> have been evaluated <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b46">46]</ref>. Recently, some UDA methods have been developed from the combination of two public datasets <ref type="bibr" target="#b50">[50]</ref>. However, directly utilizing combined datasets may result in two problems: 1) Insufficient common categories. Different datasets are designed for different purposes, and the insufficient common categories limit further exploration.</p><p>2) Inconsistent annotation granularity. The different spatial resolutions and labeling styles lead to different annotation granularities, which can result in unreliable conclusions. Compared with existing datasets, LoveDA dataset encompasses two domains (urban and rural), representing a novel UDA task for land-cover mapping.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Unsupervised domain adaptation</head><p>For natural images, UDA is aimed at transferring a model trained on the source domain to the target domain. Some conventional image classification studies <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b39">40]</ref> have directly minimized the discrepancy of the feature distributions to extract domain-invariant features. The recent works have mainly proceeded in two directions, i.e., adversarial training and self-training.</p><p>Adversarial training. In adversarial training, the architecture includes a feature extractor and a discriminator. The extractor aims to learn domain-invariant features, while the discriminator attempts to distinguish these features. For semantic segmentation, Tsai et al. <ref type="bibr" target="#b38">[39]</ref> considered the semantic outputs containing spatial similarities between the different domains, and adapted the structured output space for segmentation (AdaptSeg) with adversarial learning. Luo et al. <ref type="bibr" target="#b21">[22]</ref> introduced a category-level adversarial network (CLAN) to align each class with an adaptive adversarial loss. Differing from the binary discriminators, Wang et al. <ref type="bibr" target="#b43">[44]</ref> proposed a fine-grained adversarial learning framework for domain adaptive semantic segmentation (FADA), aligning the class-level features. From the aspect of structure, the transferable normalization (TransNorm) method <ref type="bibr" target="#b47">[47]</ref> was proposed to enhance the transferability of the FCN-based feature extractors. All these advanced adversarial learning methods were implemented on the LoveDA dataset for evaluation.</p><p>Self-training. Self-training involves alternately generating pseudo-labels on the target data and fine-tuning the model. Recently, the self-training UDA methods have focused on improving the quality of the pseudo-labels <ref type="bibr" target="#b51">[51,</ref><ref type="bibr" target="#b57">57]</ref>. Lian et al. <ref type="bibr" target="#b17">[18]</ref> designed the self-motivated pyramid curriculum (PyCDA) to observe the target properties, and fused multi-scale features. Zou et al. <ref type="bibr" target="#b56">[56]</ref> proposed a class-balanced self-training (CBST) strategy to sample pseudo-labels, thus avoiding the dominance of the large classes. Mei et al. <ref type="bibr" target="#b24">[25]</ref> used an instance adaptive self-training (IAST) selector for sample balance. In addition to testing these self-training methods on the LoveDA dataset, we also performed the pseudo-label analysis for the CBST.</p><p>UDA in the remote sensing community. The early UDA methods focused on scene classification tasks <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b27">28]</ref>. Recently, adversarial training <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b34">35]</ref> and self-training <ref type="bibr" target="#b37">[38]</ref> have been studied for UDA land-cover semantic segmentation. These methods follow the general UDA approach in the computer vision field, with some improvements. However, with only the public datasets, the advancement of the UDA algorithms has been limited by the insufficient shared categories and the inconsistent annotation granularity. To this end, the LoveDA dataset is proposed for a more challenging benchmark, promoting future research of remote sensing UDA algorithms and applications.</p><p>3 Dataset Description</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Image Distribution and Division</head><p>The LoveDA dataset was constructed using 0.3 m images obtained from Nanjing, Changzhou and Wuhan in July 2016, totally covering 536.15km 2 <ref type="figure" target="#fig_0">(Figure 1</ref>). The historical images were obtained from the Google Earth platform. As each research area has its own planning strategy, the urban-rural ratio is inconsistent <ref type="bibr" target="#b52">[52]</ref>. Data from the rural and urban areas were collected referring to the "Urban and Rural Division Code" issued by the National Bureau of Statistics. There are nine urban areas selected from different economically developed districts, which are all densely populated (&gt; 1000 people/km 2 ) <ref type="bibr" target="#b52">[52]</ref>. The other nine rural areas were selected from undeveloped districts. The spatial resolution is 0.3 m, with red, green, and blue bands. After geometric registration and pre-processing, each area is covered by 1024 ? 1024 images, without overlap. Considering Tobler's First Law, i.e., everything is related to everything else, but near things are more related than distant things <ref type="bibr" target="#b36">[37]</ref>, the training, validation, and test sets were split so that they were spatially independent ( <ref type="figure" target="#fig_0">Figure 1</ref>), thus enhancing the difference between the split sets. There are two tasks that can be evaluated on the LoveDA dataset: 1) Semantic segmentation. There are eight areas for training, and the others are for validation and testing. The training, validation, and test sets cover both urban and rural areas.2) Unsupervised domain adaptation. The UDA process considers two cross-domain adaptation sub-tasks: a) Urban ? Rural. The images from the Qinhuai, Qixia, Jianghan, and Gulou areas are included in the source training set. The images from Liuhe and Huangpi are included in the validation set. The Jiangning, Xinbei, and Liyang images included in the test set. The Oracle setting is designed to test the upper limit of accuracy in a single domain <ref type="bibr" target="#b30">[31]</ref>. Hence, the training images were collected from the Pukou, Lishui, Gaochun, and Jiangxia areas. b) Rural ? Urban. The images from the Pukou, Lishui, Gaochun, and Jiangxia areas are included in the source training set. The images from Yuhuatai and Jintan are used for the validation set. The Jiangye, Wuchang, and Wujin images are used for the test set. In the Oracle setting, the training images cover the Qinhuai, Qixia, Jianghan, and Gulou areas.</p><p>With the division of these images, a comprehensive annotation pipeline was adopted, including professional annotators and strict inspection procedures <ref type="bibr" target="#b48">[48]</ref>. Further details of the data division and annotation can be found in ?A.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Statistics for LoveDA</head><p>Some statistics of the LoveDA dataset are analyzed in this section. With the collection of public HSR land-cover datasets, the number of labeled objects and pixels has been counted. As is shown in the <ref type="figure">Figure 2</ref>(a), our proposed LoveDA dataset contains the largest number of labeled pixels as well as land-cover objects, which shows the advantage in data diversity. There are a lot of buildings because urban scenes have large populations <ref type="figure">(Figure 2(b)</ref>). As is shown in <ref type="figure">Figure 2</ref>(c), the background class contains the most pixels with complex samples <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b54">54]</ref>. The complex background samples have larger intra-class variance in the complex scenes and cause serious false alarms. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Differences Between Urban and Rural Scenes</head><p>During the process of urbanization, cities differentiate into rural and urban forms. In this section, we list the main differences, which reveal the meaning and challenges of the remote sensing UDA task. For the Nanjing City, the main differences come from the shape, layout, scale, spectra, and class distribution. As is shown in <ref type="figure" target="#fig_0">Figure 1</ref>, the buildings in the urban area are neatly arranged, with various shapes, while the buildings in the rural area are disordered, with simpler shapes. The roads are wide in the urban scenes. In contrast, the roads are narrow in the rural scenes. Water is often presented in the form of large-scale rivers or lakes in the urban scenes, while small-scale ponds and ditches are common in the rural scenes. The agricultural is found in the gaps between the buildings in the urban scenes, but occurs in a large-scale and continuously distributed form in the rural scenes.</p><p>For the class distribution, spectra, and scale, the related statistics are reported in <ref type="figure">Figure 3</ref>. The urban areas always contain more man-made objects such as buildings and roads due to their high population density <ref type="figure">(Figure 3(a)</ref>). In contrast, the rural areas have more agricultural land. The inconsistent class distributions between the urban and rural scenes increases the difficulty of model generalization.</p><p>For the spectral statistics, the mean values are similar <ref type="figure">(Figure 3(b)</ref>). Because of the large-scale homogeneous geographical areas, such as agriculture and water, the rural images have lower standard deviations. As is shown in <ref type="figure">Figure 3</ref>(c), most of the buildings have relatively small scales in the rural areas, representing the "long tail" phenomenon. However, the buildings in the urban scenes have a larger size variance. Scale differences also exist in the other categories, as shown in <ref type="figure" target="#fig_0">Figure 1</ref>. The multi-scale objects require the models to have multi-scale capture capabilities. When faced with large-scale land cover mapping tasks, the differences between urban and rural scenes bring new challenges to the model transferability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Semantic Segmentation</head><p>For the semantic segmentation task, the general architectures as well as their variants, and particularly those most often used in remote sensing, were tested on the LoveDA dataset. Specifically, the selected networks were: UNet <ref type="bibr" target="#b31">[32]</ref>, UNet++ <ref type="bibr" target="#b55">[55]</ref>, LinkNet <ref type="bibr" target="#b2">[3]</ref>, DeepLabV3+ <ref type="bibr" target="#b4">[5]</ref>, PSPNet <ref type="bibr" target="#b53">[53]</ref>, FCN8S <ref type="bibr" target="#b18">[19]</ref>, PAN <ref type="bibr" target="#b16">[17]</ref>, Semantic-FPN <ref type="bibr" target="#b15">[16]</ref>, HRNet <ref type="bibr" target="#b45">[45]</ref>, FarSeg <ref type="bibr" target="#b54">[54]</ref>, and FactSeg <ref type="bibr" target="#b22">[23]</ref>. Following the common practice <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b45">45]</ref>, we use the intersection over union (IoU) to report the semantic segmentation accuracy. With respect to the IoU for each class, the mIoU represents the mean of the IoUs over all the categories. The inference speed is reported with a single 512 ? 512 input (repeated 500 times), using frames per second (FPS).   <ref type="table" target="#tab_8">Table 8</ref> in ?A.1. During the training, we used the Stochastic Gradient Descent (SGD) optimizer with a momentum of 0.9 and a weight decay of 10 ?4 . The learning rate was initially set to 0.01, and a 'poly' schedule with power 0.9 was applied. The number of training iterations was set to 15k with a batch size of 16. For the data augmentation, 512 ? 512 patches were randomly cropped from the raw images, with random mirroring and rotation.</p><p>The backbones used in all the networks were pre-trained on ImageNet.</p><p>Multi-scale architectures and strategies. As ground objects show considerable scale variance, especially in complex scenes ( ?3.3), we have analyzed the multi-scale architectures and strategies.</p><p>There are three noticeable observations from  is shown in <ref type="table" target="#tab_2">Table 3</ref>, multi-scale augmentation (with scale = {0.5, 0.75, 1.0, 1.25, 1.5, 1.75}) was conducted during the training (MSTr), significantly improving the performance of different methods.</p><p>In the implementation, the multi-scale inference adopts multi-scale inputs and ensembles the rescaled multiple outputs using a simple mean function. With further use in the testing process, all methods were further improved. As for multi-scale fusion, hierarchical multi-scale architecture search <ref type="bibr" target="#b46">[46]</ref> may also become an effective solution.</p><p>Additional background supervision. The complex background samples cause serious false alarms in HRS imagery semantic segmentation <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b54">54]</ref>. As is shown in <ref type="figure" target="#fig_2">Figure 4</ref>, the confusion matrices show that lots of objects were misclassified into background, which is consistent with our analysis in ?3.2. Based on Semantic-FPN, we designed the additional background supervision to address this problem. Dice loss <ref type="bibr" target="#b25">[26]</ref> and binary cross-entropy loss were utilized with the corresponding modulation factors. We calculated the total loss as: L total = L ce + ?L bce + ?L dice , where L ce denotes the original cross-entropy loss. <ref type="table" target="#tab_4">Table 4</ref> and <ref type="table" target="#tab_5">Table 5</ref> additionally report the precision (P), recall (R) and F1-score (F1) of the background class with varying modulation factors. Besides, the standard deviations are reported after 3 runs. <ref type="table" target="#tab_4">Table 4</ref> shows that the addition of binary cross-entropy loss improves the background accuracy and the overall performance. The combination of L dice and L bce performs well because they optimize the background class from different directions. In the future, the spatial attention mechanism <ref type="bibr" target="#b26">[27]</ref> may improve the background with adaptive weights.  Visualization. Some representative results are shown in <ref type="figure" target="#fig_3">Figure 5</ref>. With the shallow backbone (VGG16), FCN8S can hardly recognize the road due to its lack of feature extraction capability. The other methods which utilize deep layers can produce better results. Because of the disorderly arrangement and varied scales, the edges of the buildings are hard to extract accurately. Some small-scale objects such as buildings and scattered trees are easy to miss. In contrast, water class achieves higher accuracies for all methods. This because water have strong spectral homogeneity and low intra-class variance <ref type="bibr" target="#b37">[38]</ref>. The forest is easy to misclassify into agriculture because these classes have similar spectra. Because of the high-resolution retention and multi-scale fusion, HRNet produces the best visualization result, especially in the details. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Unsupervised Domain Adaptation</head><p>The advanced UDA methods were evaluated on the LoveDA dataset. In addition to the original metric-based approach of DDC <ref type="bibr" target="#b39">[40]</ref>, two mainstream UDA approaches were tested, i.e., adversarial training (AdaptSeg <ref type="bibr" target="#b38">[39]</ref>, CLAN <ref type="bibr" target="#b21">[22]</ref>, TransNorm <ref type="bibr" target="#b47">[47]</ref>, FADA <ref type="bibr" target="#b43">[44]</ref>) and self-training (CBST <ref type="bibr" target="#b56">[56]</ref>, PyCDA <ref type="bibr" target="#b17">[18]</ref>, IAST <ref type="bibr" target="#b24">[25]</ref>). Implementation details. All the UDA methods adopted the same feature extractor and discriminator, following the common practice <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b43">44]</ref>. Specifically, DeepLabV2 <ref type="bibr" target="#b3">[4]</ref> with ResNet50 was utilized as the extractor, and the discriminator was constructed by fully convolutional layers <ref type="bibr" target="#b38">[39]</ref>. For the adversarial training (AT), the classification and discriminator learning rates were set to 5 ? 10 ?3 and 10 ?4 , respectively. The Adam optimizer was used for the discriminator with the momentum of 0.9 and 0.99. The number of training iterations was set to 10k, with a batch size of 16. The eight source images and eight target images were alternatively input. The other settings are the same in the semantic segmentation. and the learning schedule is the same as in semantic segmentation settings. For the self-training (ST), the classification learning rate was set to 10 ?2 . Full implementation details are provided in the ?A.4.</p><p>Benchmark results. As is shown in <ref type="table" target="#tab_6">Table 6</ref>, the Oracle setting obtains the best overall performances. However, DeepLabV2 has lost its effectiveness due to the domain divergence, referring to the result of Source only setting. In the Rural ? Urban experiments, the accuracies of artificial classes (building and road) drop more than natural classes (forest and agricultural). Because of the inconsistent class distribution, the Urban ? Rural experiments show the opposite results. The transfer learning methods relatively improve the model transferability. Noticeably, TransNorm obtains the lowest mIoUs. This is because the source and target images were obtained by the same sensor, and their spectral statistics are similar <ref type="figure">(Figure 3(2)</ref>). These rural and urban domains require similar normalization weights, so that the adaptive normalization can lead to optimization conflicts (more analysis are provided in ?A.6). The ST methods achieve better performances because they address the class imbalance problem with pseudo-label generation.</p><p>Inconsistent class distributions. It is noticeable to find that the ST methods surpass AT methods in cross-domain adaptation experiments. We conclude that the main reason for this is the extremely inconsistent class distribution <ref type="figure">(Figure 3(a)</ref>). The rural scenes only contain a few artificial samples and large-scale natural objects. In contrast, the urban scenes have a mixture of buildings and roads with few natural objects. The AT methods cannot address this difficulty, so that they report lower accuracies. However, differing from the AT methods, the ST methods generate pseudo-labels on the target images. With the addition of diverse target samples, the class distribution divergence is eliminated during the training. Overall, the ST methods show more potential in the UDA land-cover semantic segmentation task. In Urban ? Rural experiments, all UDA methods show negative transfer effects for the road class. Hence, more tailored UDA methods are worth exploring faced with these special challenges.</p><p>Visualization. The qualitative results for the Rural ? Urban experiments are shown in <ref type="figure" target="#fig_5">Figure 6</ref>.</p><p>The Oracle result successfully recognizes the buildings and roads, and is the closest to the ground truth. According to the <ref type="table" target="#tab_1">Table 2</ref>, it can be further improved by using a more robust backbone. The ST methods (j)-(l) produce better results than AT methods (f)-(i), but there is still much room for improvement. The large-scale mapping visualizations are provided in ?A.7.  Pseudo-label analysis for CBST. As pseudo samples are important for addressing inconsistent class distribution problem, we varied the target class proportion in CBST, which is a hyper-parameter controlling the number of pseudo samples. The mean F1-score (mF1) and mIoU are reported in <ref type="table" target="#tab_7">Table 7</ref>. Without pseudo-label learning (t = 0), the model degenerated into Source only setting and achieved low accuracy. The optimal range of t is relatively large (0.05 ? t ? 0.5), which proves that it is not sensitive to the remote sensing UDA task. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>The differences between urban and rural scenes limit the generalization of deep learning approaches in land-cover mapping. In order to address this problem, we built an HSR dataset for Land-cOVEr Domain Adaptive semantic segmentation (LoveDA). The LoveDA dataset reflects three challenges in large-scale remote sensing mapping, including multi-scale objects, complex background samples, and inconsistent class distributions. The state-of-the-art methods were evaluated on the LoveDA dataset, revealing the challenges of LoveDA. In addition, multi-scale architectures and strategies, additional background supervision and pseudo-label analysis were conducted to find alternative ways to address these challenges.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Broader Impact</head><p>This work offers a free and open dataset with the purpose of advancing land-cover semantic segmentation in the area of remote sensing. We also provide two benchmarked tasks with three considerable challenges. This will allow other researchers to easily build on this work and create new and enhanced capabilities. The authors do not foresee any negative societal impacts of this work. A potential positive societal impact may arise from the development of generalizable models that can produce large-scale high-spatial-resolution land-cover mapping accurately. This could help to reduce the manpower and material resource consumption of surveying and mapping.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Appendix</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Annotation Procedure and Data Division</head><p>The seven common land-cover types were developed according to the "Data Regulations and Collection Requirements for the General Survey of Geographical Conditions", i.e., buildings, road, water, forest, agriculture, and background classes. Based on the advanced ArcGIS geo-spatial software , all the images were annotated by professional remote sensing annotators. With the division of these images, a comprehensive annotation pipeline was adopted referring to <ref type="bibr" target="#b48">[48]</ref>. The annotators labeled all objects belonging to six categories (except background) using polygon features. As for the 18 selected areas, it took approximately 24.6 h to finish the single-area annotations, resulting in a time cost of 442.8 man hours in total. After the first round of labeling, self-examination and cross-examination were conducted, correcting the false labels, missing objects, and inaccurate boundaries. The team supervisors then randomly sampled 600 images for quality inspection. The unqualified annotations were then refined by the annotators. Finally, several statistics (e.g. object numbers per image, object areas, etc.) were computed to double check the outliers. Based on DeepLabV3, preliminary experiments were conducted to ensure the validity of the annotations. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Top Performances Compared with Other Datasets</head><p>In order to support the "challeangability" of the proposed dataset compared to other land-cover datasets. By investigating the current researches, the top performances on different datasets have been reported in <ref type="table" target="#tab_9">Table 9</ref>. The advanced method (HRNet) only achieved the lowest performance on the LoveDA dataset, showing the difficulty of this dataset </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Instance Differences Between Urban and Rural Areas</head><p>For the LoveDA dataset, the differences between urban and rural areas at the instance level are shown in the <ref type="figure" target="#fig_6">Figure 7</ref>. Similar with the pixel analysis in ?3.3, the instances across domains are imbalanced. Specifically, the urban areas have more buildings and fewer instances of agricultural land. The rural areas have more instances of agricultural land. This also highlights the inconsistent class distribution problem between different domains. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 Implementation Details</head><p>All the networks were implemented under the PyTorch framework, using an NVIDIA 24 GB RTX TITAN GPU. The backbones used in all the networks were pre-trained on ImageNet. The number of training iterations was set to 10k with a batch size of 16. The eight source images and eight target images were alternately input. The other settings were the same as in the semantic segmentation. As for self-training (ST), the pseudo-generation hyper-parameters remained the same as in the original literature. The classification learning rate was set to 10 ?2 . All the ST-based networks were trained for 10k steps including two stages: 1) for the first 4k steps, the models were trained only on the source images for initialization; and 2) the pseudo-labels were then updated every 1k steps during the remaining training process. Considering the training stability, IAST method was set 8k steps for initialization in the Urban ? Rural experiments.</p><p>All the networks were then re-implemented following the original literature. The segmentation models followed the default settings in <ref type="bibr" target="#b38">[39]</ref>, including a modified ResNet50 and atrous spatial pyramid pooling (ASPP) <ref type="bibr" target="#b3">[4]</ref>. By using dilated convolutions, the stride of the last two convolution layers was modified from 2 to 1. The final output stride of the feature map was 16.</p><p>Following <ref type="bibr" target="#b38">[39]</ref>, the discriminator was made up of five convolutional layers with a kernel of 4 ? 4 and a stride of 2, where the channel numbers were {64, 128, 256, 512, 1}, respectively. Each convolution was followed with a Leaky ReLU, and the parameter was set to 0.2. Bilinear interpolation was used for re-scaling the output to the size of the input.</p><p>As for the hyperparameter settings, the adversarial scale factor ? was set to 0.001 following <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b43">44]</ref>. With respect to the two segmentation outputs in <ref type="bibr" target="#b38">[39]</ref>, ? 1 and ? 2 were set to 0.001 and 0.002, respectively. The weight discrepancy loss was used in CLAN <ref type="bibr" target="#b21">[22]</ref>, and the default settings were adopted, i.e., ? w = 0.01, ? local = 10, and = 0.4. FADA <ref type="bibr" target="#b43">[44]</ref> adopts the temperature T to encourage a soft probability distribution over the classes, which was set to 1.8 by default. The confidence of pseudo-label ? in PyCDA <ref type="bibr" target="#b17">[18]</ref> was set to 0.5 by default. The pseudo-label related hyperparameters for IAST remained the same as in <ref type="bibr" target="#b24">[25]</ref>. The target proportion p in CBST was set to 0.1 and 0.5 when transferring to the rural and urban domains, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5 Error Bar Visualization for the UDA Experiments</head><p>In order to make the results more convincing and reproducible, we ran all UDA methods five times using a random seed. The error bar visualization for the UDA experiments is shown in <ref type="figure" target="#fig_7">Figure 8</ref>. The adversarial training methods achieve smaller error fluctuations than the self-training methods. This is because the self-training methods assign and update the pseudo-labels alternately, which brings greater randomness. Hence, for the self-training methods, we suggest that three times more repeats are preferred to provide more convincing results. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.6 Batch Normalization Statistics in the Different Domains</head><p>The batch normalization (BN) statistics are shown in <ref type="figure" target="#fig_8">Figure 9</ref>. We observe that in the Oracle source and target settings, the model has similar BN statistics in both mean and variance. This demonstrates that the gap between the source and target domains does not lie in the BNs, which is different from the conclusion in <ref type="bibr" target="#b47">[47]</ref>. Hence, the modification of the BN statistics may have a negative effect, as in TransNorm <ref type="bibr" target="#b47">[47]</ref>, where the target BN statistics are far different from those of the Oracle target model. This observation is consistent with the results listed in <ref type="table" target="#tab_6">Table 6</ref>. We speculate that the cause of this failure in the combined simulation dataset UDA experiments <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b47">47]</ref> is that the source and target domains have large spectral differences, and thus require domain-specific BN statistics. However, the LoveDA dataset is real data obtained from the same sensor at the same time. The spectral difference in the source and target domains is very small <ref type="figure">(Figure 3(b)</ref>), so the BN statistics are very similar <ref type="figure" target="#fig_8">(Figure 9</ref>). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.7 Large-scale Visualizations on UDA Test Set</head><p>The large-scale visualizations are shown in the <ref type="figure" target="#fig_0">Figure 10</ref>. Compared with the baseline, CBST can produce better results on large-scale mapping, which highlights the importance of developing UDA methods. However, CBST still has a lot of room for improvement. More tailored UDA algorithms requires to be developed on the LoveDA dataset. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Overview of the dataset distribution. The images were collected from Nanjing, Changzhou, and Wuhan cities, covering 18 different administrative districts.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :Figure 3 :</head><label>23</label><figDesc>Statistics for the pixels and objects in LoveDA dataset. (a) Number of objects vs. number of pixels. The radius of the circles represents the number of classes. (b) Histogram of the number of objects for each class. (c) Histogram of the number of pixels for each class.Urban Rural (a) Class distributions (b) Spectral values (c) Building scales Statistics for the urban and rural scenes in Nanjing City. (a) Class distribution. (b) Spectral statistics. The mean and standard deviation (?) for 5 urban and 5 rural areas are reported. (c) Distribution of the building sizes. The Jianye (urban) and Lishui (rural) scenes are reported.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Representative confusion matrices for the semantic segmentation experiments.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Semantic segmentation results on images from the LoveDA Test set in the Liuhe (Rural) area. Some small-scale objects such as buildings and scattered trees are hard to recognize. The forest and agricultural classes are easy to misclassify due to their similar spectra.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Visual results for the Rural ? Urban experiments. (f)-(i) and (j)-(l) were obtained from the AT and ST methods, respectively. The ST methods produce better results than the AT methods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Instance differences between urban and rural areas.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 :</head><label>8</label><figDesc>Error bar visualization for the UDA experiments.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 9 :</head><label>9</label><figDesc>Statistics of the running mean (RM) and running var (RV) of the batch normalization in the different layers of ResNet50. Two Oracle models and TransNorm in the Urban ? Rural experiments are shown.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 10 :</head><label>10</label><figDesc>Large-scale visualizations on UDA Test set (Rural ? Urban).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Comparison between LoveDA and the main land-cover semantic segmentation datasets.</figDesc><table><row><cell>Image level</cell><cell cols="2">Resolution (m) Dataset</cell><cell>Year Sensor</cell><cell cols="4">Area (km 2 ) Classes Image width Images</cell><cell>Task</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>SS UDA</cell></row><row><cell>Meter level</cell><cell>10 4</cell><cell>LandCoverNet [1] GID [38]</cell><cell>2020 Sentinel-2 2020 GF-2</cell><cell>30000 75900</cell><cell>7 5</cell><cell cols="2">256 4800?6300 150 1980</cell></row><row><cell></cell><cell>0.25?0.5</cell><cell>LandCover.ai [2]</cell><cell>2020 Airborne</cell><cell>216.27</cell><cell>3</cell><cell cols="2">4200?9500 41</cell></row><row><cell></cell><cell>0.6</cell><cell cols="2">Zurich Summer [43] 2015 QuickBird</cell><cell>9.37</cell><cell>8</cell><cell>622?1830</cell><cell>20</cell></row><row><cell></cell><cell>0.5</cell><cell>DeepGlobe [9]</cell><cell cols="2">2018 WorldView-2 1716.9</cell><cell>7</cell><cell>2448</cell><cell>1146</cell></row><row><cell>Sub-meter level</cell><cell>0.05</cell><cell>Zeebruges [24]</cell><cell>2018 Airborne</cell><cell>1.75</cell><cell>8</cell><cell>10000</cell><cell>7</cell></row><row><cell></cell><cell>0.05</cell><cell>ISPRS Potsdam 3</cell><cell>2013 Airborne</cell><cell>3.42</cell><cell>6</cell><cell>6000</cell><cell>38</cell></row><row><cell></cell><cell>0.09</cell><cell>ISPRS Vaihingen 4</cell><cell>2013 Airborne</cell><cell>1.38</cell><cell>6</cell><cell cols="2">1887?3816 33</cell></row><row><cell></cell><cell>0.07</cell><cell>AIRS [6]</cell><cell>2019 Airborne</cell><cell>475</cell><cell>2</cell><cell>10000</cell><cell>1047</cell></row><row><cell></cell><cell>0.5</cell><cell>SpaceNet [41]</cell><cell cols="2">2017 WorldView-2 2544</cell><cell>2</cell><cell>406?439</cell><cell>6000</cell></row><row><cell></cell><cell>0.3</cell><cell>LoveDA (Ours)</cell><cell>2021 Spaceborne</cell><cell>536.15</cell><cell>7</cell><cell>1024</cell><cell>5987</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Semantic segmentation results obtained on the Test set of LoveDA.</figDesc><table><row><cell>Method</cell><cell>Backbone</cell><cell></cell><cell></cell><cell cols="2">IoU per category (%)</cell><cell></cell><cell></cell><cell cols="2">mIoU (%) Speed (FPS)</cell></row><row><cell></cell><cell></cell><cell cols="6">Background Building Road Water Barren Forest Agriculture</cell><cell></cell><cell></cell></row><row><cell>FCN8S [19]</cell><cell>VGG16</cell><cell>42.60</cell><cell>49.51</cell><cell cols="2">48.05 73.09 11.84</cell><cell>43.49</cell><cell>58.30</cell><cell>46.69</cell><cell>86.02</cell></row><row><cell>DeepLabV3+ [5]</cell><cell>ResNet50</cell><cell>42.97</cell><cell>50.88</cell><cell cols="2">52.02 74.36 10.40</cell><cell>44.21</cell><cell>58.53</cell><cell>47.62</cell><cell>75.33</cell></row><row><cell>PAN [17]</cell><cell>ResNet50</cell><cell>43.04</cell><cell>51.34</cell><cell cols="2">50.93 74.77 10.03</cell><cell>42.19</cell><cell>57.65</cell><cell>47.13</cell><cell>61.09</cell></row><row><cell>UNet [32]</cell><cell>ResNet50</cell><cell>43.06</cell><cell>52.74</cell><cell cols="2">52.78 73.08 10.33</cell><cell>43.05</cell><cell>59.87</cell><cell>47.84</cell><cell>71.35</cell></row><row><cell>UNet++ [55]</cell><cell>ResNet50</cell><cell>42.85</cell><cell>52.58</cell><cell cols="2">52.82 74.51 11.42</cell><cell>44.42</cell><cell>58.80</cell><cell>48.20</cell><cell>27.22</cell></row><row><cell cols="2">Semantic-FPN [16] ResNet50</cell><cell>42.93</cell><cell>51.53</cell><cell cols="2">53.43 74.67 11.21</cell><cell>44.62</cell><cell>58.68</cell><cell>48.15</cell><cell>73.98</cell></row><row><cell>PSPNet [53]</cell><cell>ResNet50</cell><cell>44.40</cell><cell>52.13</cell><cell>53.52 76.50</cell><cell>9.73</cell><cell>44.07</cell><cell>57.85</cell><cell>48.31</cell><cell>74.81</cell></row><row><cell>LinkNet [3]</cell><cell>ResNet50</cell><cell>43.61</cell><cell>52.07</cell><cell cols="2">52.53 76.85 12.16</cell><cell>45.05</cell><cell>57.25</cell><cell>48.50</cell><cell>67.01</cell></row><row><cell>FarSeg [54]</cell><cell>ResNet50</cell><cell>43.09</cell><cell>51.48</cell><cell>53.85 76.61</cell><cell>9.78</cell><cell>43.33</cell><cell>58.90</cell><cell>48.15</cell><cell>66.99</cell></row><row><cell>FactSeg [23]</cell><cell>ResNet50</cell><cell>42.60</cell><cell>53.63</cell><cell cols="2">52.79 76.94 16.20</cell><cell>42.92</cell><cell>57.50</cell><cell>48.94</cell><cell>65.58</cell></row><row><cell>HRNet [45]</cell><cell>W32</cell><cell>44.61</cell><cell>55.34</cell><cell cols="2">57.42 73.96 11.07</cell><cell>45.25</cell><cell>60.88</cell><cell>49.79</cell><cell>16.74</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table><row><cell>Method</cell><cell></cell><cell>mIoU(%)</cell><cell></cell></row><row><cell></cell><cell cols="3">Baseline +MSTr +MSTrTe</cell></row><row><cell>DeepLabV3+</cell><cell>47.62</cell><cell>49.97</cell><cell>51.18</cell></row><row><cell>UNet</cell><cell>48.00</cell><cell>50.21</cell><cell>51.13</cell></row><row><cell>SFPN</cell><cell>48.15</cell><cell>50.80</cell><cell>51.82</cell></row><row><cell>HRNet</cell><cell>49.79</cell><cell>51.51</cell><cell>52.14</cell></row></table><note>Multi-Scale augmentation during Training and Testing (MSTrTe).Implementation details. The data splits followed the</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2</head><label>2</label><figDesc></figDesc><table /><note>: 1) UNet++ outperforms UNet due to its nested cross-scale connections between different scales. 2) Among the different fusion strategies, UNet++, Semantic-FPN, LinkNet and HRNet outperform DeepLabV3+. This demonstrates that the cross-layer fusion works better than the in-module fusion. 3) HRNet outperforms the other methods, due to its sophisticated architecture, where the features are repeatedly exchanged across different scales. As</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Varied ? for L bce 57.70 63.36 60.39 48.50 ? 0.13 0.5 56.92 65.86 61.06 48.85 ? 0.15 0.7 57.73 64.62 61.98 48.74 ? 0.19 0.9 57.30 64.05 60.48 48.26 ? 0.14 1.0 58.43 62.64 60.46 48.14 ? 0.18</figDesc><table><row><cell>?</cell><cell>Background</cell><cell>mIoU (%)</cell></row><row><cell></cell><cell>P (%) R (%) F1(%)</cell><cell></cell></row><row><cell>0</cell><cell cols="2">55.46 61.01 59.86 48.15 ? 0.17</cell></row><row><cell>0.2</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Varied ? for L dice (w. optimal ?) .5 56.68 64.82 60.47 48.97 ? 0.16 0.5 0.5 56.88 65.16 60.96 49.23 ? 0.09 0.7 0.5 57.13 65.31 60.93 49.68 ? 0.14 0.2 0.7 56.91 66.03 61.13 49.69 ? 0.17 0.5 0.7 57.14 66.21 61.34 50.08 ? 0.15 0.7 0.7 56.68 65.52 60.78 49.48 ? 0.13</figDesc><table><row><cell>?</cell><cell>?</cell><cell>Background</cell><cell>mIoU (%)</cell></row><row><cell></cell><cell></cell><cell>P (%) R (%) F1(%)</cell><cell></cell></row><row><cell cols="2">0.2 0</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Unsupervised domain adaptation results obtained on the Test set of the LoveDA dataset. The abbreviations are: AT -adversarial training methods. ST -self-training methods.</figDesc><table><row><cell cols="2">Domain Method</cell><cell>Type</cell><cell></cell><cell></cell><cell></cell><cell>IoU (%)</cell><cell></cell><cell></cell><cell>mIoU(%)</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="7">Background Building Road Water Barren Forest Agriculture</cell></row><row><cell></cell><cell>Oracle</cell><cell>-</cell><cell>48.18</cell><cell>52.14</cell><cell cols="3">56.81 85.72 12.34</cell><cell>36.70</cell><cell>35.66</cell><cell>46.79</cell></row><row><cell></cell><cell>Source only</cell><cell>-</cell><cell>43.30</cell><cell>25.63</cell><cell cols="3">12.70 76.22 12.52</cell><cell>23.34</cell><cell>25.14</cell><cell>31.27</cell></row><row><cell></cell><cell>DDC [40]</cell><cell>-</cell><cell>43.60</cell><cell>15.37</cell><cell cols="3">11.98 79.07 14.13</cell><cell>33.08</cell><cell>23.47</cell><cell>31.53</cell></row><row><cell>Rural</cell><cell>AdaptSeg [39]</cell><cell>AT</cell><cell>42.35</cell><cell>23.73</cell><cell cols="3">15.61 81.95 13.62</cell><cell>28.70</cell><cell>22.05</cell><cell>32.68</cell></row><row><cell>?</cell><cell>FADA [44]</cell><cell>AT</cell><cell>43.89</cell><cell>12.62</cell><cell cols="3">12.76 80.37 12.70</cell><cell>32.76</cell><cell>24.79</cell><cell>31.41</cell></row><row><cell>Urban</cell><cell>CLAN [22]</cell><cell>AT</cell><cell>43.41</cell><cell>25.42</cell><cell cols="3">13.75 79.25 13.71</cell><cell>30.44</cell><cell>25.80</cell><cell>33.11</cell></row><row><cell></cell><cell>TransNorm [47]</cell><cell>AT</cell><cell>38.37</cell><cell>5.04</cell><cell>3.75</cell><cell cols="2">80.83 14.19</cell><cell>33.99</cell><cell>17.91</cell><cell>27.73</cell></row><row><cell></cell><cell>PyCDA [18]</cell><cell>ST</cell><cell>38.04</cell><cell>35.86</cell><cell cols="2">45.51 74.87</cell><cell>7.71</cell><cell>40.39</cell><cell>11.39</cell><cell>36.25</cell></row><row><cell></cell><cell>CBST [56]</cell><cell>ST</cell><cell>48.37</cell><cell>46.10</cell><cell cols="3">35.79 80.05 19.18</cell><cell>29.69</cell><cell>30.05</cell><cell>41.32</cell></row><row><cell></cell><cell>IAST [25]</cell><cell>ST</cell><cell>48.57</cell><cell>31.51</cell><cell cols="3">28.73 86.01 20.29</cell><cell>31.77</cell><cell>36.50</cell><cell>40.48</cell></row><row><cell></cell><cell>Oracle</cell><cell>-</cell><cell>37.18</cell><cell>52.74</cell><cell cols="3">43.74 65.89 11.47</cell><cell>45.78</cell><cell>62.91</cell><cell>45.67</cell></row><row><cell></cell><cell>Source only</cell><cell>-</cell><cell>24.16</cell><cell>37.02</cell><cell cols="3">32.56 49.42 14.00</cell><cell>29.34</cell><cell>35.65</cell><cell>31.74</cell></row><row><cell></cell><cell>DDC [40]</cell><cell>-</cell><cell>25.61</cell><cell>44.27</cell><cell cols="3">31.28 44.78 13.74</cell><cell>33.83</cell><cell>25.98</cell><cell>31.36</cell></row><row><cell>Urban</cell><cell>AdaptSeg [39]</cell><cell>AT</cell><cell>26.89</cell><cell>40.53</cell><cell cols="3">30.65 50.09 16.97</cell><cell>32.51</cell><cell>28.25</cell><cell>32.27</cell></row><row><cell>?</cell><cell>FADA [44]</cell><cell>AT</cell><cell>24.39</cell><cell>32.97</cell><cell cols="3">25.61 47.59 15.34</cell><cell>34.35</cell><cell>20.29</cell><cell>28.65</cell></row><row><cell>Rural</cell><cell>CLAN [22]</cell><cell>AT</cell><cell>22.93</cell><cell>44.78</cell><cell cols="3">25.99 46.81 10.54</cell><cell>37.21</cell><cell>24.45</cell><cell>30.39</cell></row><row><cell></cell><cell>TransNorm [47]</cell><cell>AT</cell><cell>19.39</cell><cell>36.30</cell><cell cols="3">22.04 36.68 14.00</cell><cell>40.62</cell><cell>3.30</cell><cell>24.62</cell></row><row><cell></cell><cell>PyCDA [18]</cell><cell>ST</cell><cell>12.36</cell><cell>38.11</cell><cell cols="3">20.45 57.16 18.32</cell><cell>36.71</cell><cell>41.90</cell><cell>32.14</cell></row><row><cell></cell><cell>CBST [56]</cell><cell>ST</cell><cell>25.06</cell><cell>44.02</cell><cell cols="2">23.79 50.48</cell><cell>8.33</cell><cell>39.16</cell><cell>49.65</cell><cell>34.36</cell></row><row><cell></cell><cell>IAST [25]</cell><cell>ST</cell><cell>29.97</cell><cell>49.48</cell><cell cols="2">28.29 64.49</cell><cell>2.13</cell><cell>33.36</cell><cell>61.37</cell><cell>38.44</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 :</head><label>7</label><figDesc>Varied p for target class proportion (Rural ? Urban) 45.24 48.50 50.93 56.30 51.23 51.03 49.43 mIoU(%) 32.94 32.18 34.46 36.84 41.32 37.12 37.02 35.47</figDesc><table><row><cell>t</cell><cell>0.</cell><cell>0.01</cell><cell>0.05</cell><cell>0.1</cell><cell>0.5</cell><cell>0.7</cell><cell>0.9</cell><cell>1.0</cell></row><row><cell>mF1(%)</cell><cell>46.81</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 8 :</head><label>8</label><figDesc>The division of the LoveDA dataset</figDesc><table><row><cell>Domain</cell><cell>City</cell><cell>Region</cell><cell cols="2">#Images Train</cell><cell>Val</cell><cell>Test</cell></row><row><cell></cell><cell></cell><cell>Qixia</cell><cell>320</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>Gulou</cell><cell>320</cell><cell></cell><cell></cell></row><row><cell></cell><cell>Nanjing</cell><cell>Qinhuai</cell><cell>336</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>Yuhuatai</cell><cell>357</cell><cell></cell><cell></cell></row><row><cell>Urban</cell><cell></cell><cell>Jianye</cell><cell>357</cell><cell></cell><cell></cell></row><row><cell></cell><cell>Changzhou</cell><cell>Jintan Wujin</cell><cell>320 320</cell><cell></cell><cell></cell></row><row><cell></cell><cell>Wuhan</cell><cell>Jianghan Wuchang</cell><cell>180 143</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>Pukou</cell><cell>320</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>Gaochun</cell><cell>336</cell><cell></cell><cell></cell></row><row><cell></cell><cell>Nanjing</cell><cell>Lishui</cell><cell>336</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>Liuhe</cell><cell>320</cell><cell></cell><cell></cell></row><row><cell>Rural</cell><cell></cell><cell>Jiangning</cell><cell>336</cell><cell></cell><cell></cell></row><row><cell></cell><cell>Changzhou</cell><cell>Liyang Xinbei</cell><cell>320 320</cell><cell></cell><cell></cell></row><row><cell></cell><cell>Wuhan</cell><cell>Jiangxia Huangpi</cell><cell>374 672</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>Total</cell><cell>5987</cell><cell cols="3">2522 1669 1796</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 9 :</head><label>9</label><figDesc>Top performances compared with other datasets</figDesc><table><row><cell>Dataset</cell><cell>Top mIoU (%)</cell></row><row><cell>GID [46]</cell><cell>93.54</cell></row><row><cell>DeepGlobe [36]</cell><cell>52.24</cell></row><row><cell>ISPRS Potsdam [27]</cell><cell>82.38</cell></row><row><cell cols="2">ISPRS Vaihingen [27] 79.76</cell></row><row><cell>LoveDA</cell><cell>49.79</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">http://www2.isprs.org/commissions/comm3/wg4/2d-sem-label-potsdam.html 4 http://www2.isprs.org/commissions/comm3/wg4/2d-sem-label-vaihingen.html</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Acknowledgments</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Alemohammad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Booth</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.03111</idno>
		<title level="m">LandCoverNet: A global benchmark land cover classification training dataset</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">LandCover.ai: Dataset for automatic mapping of buildings, woodlands, water and roads from aerial imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Boguszewski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batorski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ziemba-Jankowska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Dziedzic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zambrzycka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1102" to="1110" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Linknet: Exploiting encoder representations for efficient semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chaurasia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Culurciello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Visual Communications and Image Processing (VCIP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="834" to="848" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Encoder-decoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="801" to="818" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Aerial imagery for roof segmentation: A large-scale dataset towards automatic mapping of buildings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">L</forename><surname>Waslander</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPRS Journal of Photogrammetry and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">147</biblScope>
			<biblScope unit="page" from="42" to="55" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Collaborative global-local networks for memory-efficient segmentation of ultra-high resolution images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8924" to="8933" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">A recommendation on the method to delineate cities, urban and rural areas for international statistical comparisons</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><forename type="middle">S</forename><surname>Commission</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>European Commission</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deepglobe 2018: A challenge to parse the earth through satellite images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Demir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Koperski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lindenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Basu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hughes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tuia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Raskar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="172" to="181" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Spectral-spatial weighted kernel manifold embedded distribution alignment for remote sensing image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Du</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Cybernetics</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="3185" to="3197" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Local manifold-based sparse discriminant learning for feature extraction of hyperspectral image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on cybernetics</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The pascal visual object classes challenge: A retrospective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="98" to="136" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Weakly-supervised domain adaptation for built-up region segmentation in aerial and satellite imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ali</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPRS Journal of Photogrammetry and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">167</biblScope>
			<biblScope unit="page" from="263" to="275" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">National land cover database (nlcd) 2016 science research products</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Homer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dewitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Danielson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Howard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AGU Fall Meeting Abstracts</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2019</biblScope>
			<biblScope unit="page" from="11" to="2301" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Open access to earth land-cover map</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">514</biblScope>
			<biblScope unit="issue">7523</biblScope>
			<biblScope unit="page" from="434" to="434" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Panoptic feature pyramid networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6399" to="6408" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Pyramid attention network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.10180</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Constructing self-motivated pyramid curriculums for cross-domain semantic segmentation: A non-adversarial approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6758" to="6767" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning transferable features with deep adaptation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="97" to="105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Multisource compensation network for remote sensing cross-domain scene classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="2504" to="2515" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Taking a closer look at domain shift: Category-level adversaries for semantics consistent domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2507" to="2516" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Factseg: Foreground activation-driven small object semantic segmentation in large-scale remote sensing imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Land cover mapping at very high resolution with rotation equivariant cnns: Towards small yet accurate models. ISPRS journal of photogrammetry and remote sensing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Marcos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Volpi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kellenberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tuia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">145</biblScope>
			<biblScope unit="page" from="96" to="107" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Instance adaptive self-training for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2020: 16th European Conference</title>
		<meeting><address><addrLine>Glasgow, UK</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="415" to="430" />
		</imprint>
	</monogr>
	<note>Proceedings, Part XXVI 16</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">V-net: Fully convolutional neural networks for volumetric medical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Milletari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-A</forename><surname>Ahmadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 fourth international conference on 3D vision (3DV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="565" to="571" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A relation-augmented fully convolutional network for semantic segmentation in aerial scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">X</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="12416" to="12425" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Domain adaptation network for cross-scene classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Othman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bazi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Melgani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Alhichri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Alajlan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zuair</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="4441" to="4456" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">R 2 -CNN: Fast tiny object detection in large-scale remote sensing images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="5512" to="5524" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Moment matching for multi-source domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1406" to="1415" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Visda: A synthetic-to-real benchmark for visual domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Usman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kaushik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2021" to="2026" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical image computing and computer-assisted intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">User guide to collection 6 modis land cover (mcd12q1 and mcd12c1) product</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sulla-Menashe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Friedl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">USGS</title>
		<imprint>
			<biblScope unit="page" from="1" to="18" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Deep coral: Correlation alignment for deep domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="443" to="450" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Standardgan: Multi-source domain adaptation for semantic segmentation of very high resolution satellite images by data standardization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Tasar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tarabalka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Giros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Alliez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Clerc</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="192" to="193" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Dense fusion classmate network for land cover classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="192" to="196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A computer movie simulating urban growth in the Detroit region</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">R</forename><surname>Tobler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Economic geography</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">sup1</biblScope>
			<biblScope unit="page" from="234" to="240" />
			<date type="published" when="1970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Land-cover classification with high-resolution remote sensing images using transferable deep models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-Y</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-S</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote Sensing of Environment</title>
		<imprint>
			<biblScope unit="volume">237</biblScope>
			<biblScope unit="page">111322</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Learning to adapt structured output space for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-H</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-C</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schulter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chandraker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7472" to="7481" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.3474</idno>
		<title level="m">Deep domain confusion: Maximizing for domain invariance</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Etten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lindenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Bacastow</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.01232</idno>
		<title level="m">Spacenet: A remote sensing dataset and challenge series</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Deep hashing network for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Venkateswara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Eusebio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chakraborty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Panchanathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5018" to="5027" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Semantic segmentation of urban scenes by learning local class interactions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Volpi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Classes matter: A fine-grained adversarial approach to cross-domain semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-Y</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<biblScope unit="page" from="642" to="659" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Springer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Deep high-resolution representation learning for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">RSNet: the search for remote sensing deep neural networks in recognition tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="2520" to="2534" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Transferable normalization: Towards improving transferability of deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1953" to="1963" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">isaid: A large-scale dataset for instance segmentation in aerial images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-S</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="28" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Satellite video super-resolution via multiscale deformable convolution alignment and temporal grouping projection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="page" from="1" to="19" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Triplet adversarial domain adaptation for pixel-level classification of vhr remote sensing images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="3558" to="3573" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Category anchor-guided unsupervised domain adaptation for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="435" to="445" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">National urban population and construction land in 2016 (by cities)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>China Statistics Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2881" to="2890" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Foreground-aware relation network for geospatial object segmentation in high spatial resolution remote sensing imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4096" to="4105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Unet++: A nested u-net architecture for medical image segmentation. In Deep learning in medical image analysis and multimodal learning for clinical decision support</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M R</forename><surname>Siddiquee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Tajbakhsh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="3" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation for semantic segmentation via class-balanced self-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="289" to="305" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Confidence regularized self-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5982" to="5991" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
