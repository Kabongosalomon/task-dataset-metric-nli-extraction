<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Zero-Shot Logit Adjustment</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dubing</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Nanjing University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuming</forename><surname>Shen</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Oxford</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haofeng</forename><surname>Zhang</surname></persName>
							<email>zhanghf@njust.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Nanjing University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
							<email>philip.torr@eng.ox.ac.uk</email>
							<affiliation key="aff1">
								<orgName type="institution">University of Oxford</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Zero-Shot Logit Adjustment</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T16:15+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Semantic-descriptor-based Generalized Zero-Shot Learning (GZSL) poses challenges in recognizing novel classes in the test phase. The development of generative models enables current GZSL techniques to probe further into the semantic-visual link, culminating in a two-stage form that includes a generator and a classifier. However, existing generation-based methods focus on enhancing the generator's effect while neglecting the improvement of the classifier. In this paper, we first analyze of two properties of the generated pseudo unseen samples: bias and homogeneity. Then, we perform variational Bayesian inference to back-derive the evaluation metrics, which reflects the balance of the seen and unseen classes. As a consequence of our derivation, the aforementioned two properties are incorporated into the classifier training as seen-unseen priors via logit adjustment. The Zero-Shot Logit Adjustment further puts semantic-based classifiers into effect in generation-based GZSL. Our experiments demonstrate that the proposed technique achieves state-of-the-art when combined with the basic generator, and it can improve various generative Zero-Shot Learning frameworks. Our codes are available on https://github.com/cdb342/</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In recognition tasks, it is challenging when classes for training and test are different, known as Zero-Shot Learning (ZSL) problems, which call for knowledge generalization from seen to unseen classes. The goal of ZSL is to correctly recognize unseen samples with a classifier trained on seen classes. To bridge the gap between training and test domains, it counts on the similarity of semantic descriptors <ref type="bibr" target="#b9">[Lampert et al., 2009]</ref> corresponding to each class. Through the semantic descriptors, ZSL can transfer knowledge from seen to unseen classes without accessing the data of unseen classes. * Equal contribution.</p><p>Corresponding author. This work was supported by the National Natural Science Foundation of China <ref type="bibr">(NSFC) under Grants No. 61872187, No. 62077023 and No. 62072246</ref>.  Early ZSL works focus on end-to-end classifier learning, particularly the embedding models <ref type="bibr" target="#b9">[Lampert et al., 2013;</ref><ref type="bibr" target="#b9">Li et al., 2019]</ref> which match visual features and semantic descriptors in a shared embedding space. These methods appear effective in ZSL but perform poorly in the more challenging Generalized Zero-Shot Learning (GZSL) setting <ref type="bibr" target="#b1">[Chao et al., 2016;</ref><ref type="bibr" target="#b12">Xian et al., 2017]</ref>. More recent efforts <ref type="bibr" target="#b12">[Xian et al., 2018;</ref><ref type="bibr" target="#b11">Shen et al., 2020]</ref> aim to improve GZSL performance by decomposing the learning process into generator learning and classifier learning. Such a two-stage strategy compensates for the feature expression of unseen classes during the classifier learning by the generated samples. A typical family of studies has focused on the improvement of the generator, exploring alternative architectures, or introducing various inductive biases. On the contrary, there is a scarcity of research on classifiers under the generative paradigm.</p><p>A principled classifier design is required to uncap the performance of the two-stage approach. The intuitive notion is introducing semantic information to classifier learning. However, both semantic and visual embedding strategies learn the same knowledge as generative models, i.e., semantic-visual links. It implies that, by directly replacing the classifier, the generated unseen samples can only serve to deliver the generator's learned links to the classifier, with little effect. As a result, classic ZSL classifiers perform worse than vanilla softmax classifiers in the generative setting <ref type="bibr" target="#b12">[Xian et al., 2018]</ref>.</p><p>To determine design directions for the classifier, we first in-vestigate the distribution of synthetic unseen data. As shown in <ref type="figure" target="#fig_1">Figure 1</ref>, the synthetic unseen samples deviate severely from the real distribution. This bias is theoretically inevitable since the domain shift problem <ref type="bibr" target="#b4">[Fu et al., 2014]</ref>, which leads to the misalignment of training and test domains. Nevertheless, we further explore the working mechanism of generated samples in <ref type="table" target="#tab_0">Table 1</ref>, finding that the information contained in various synthetic unseen samples is quite homogenous for the classifier. Existing methods employ a large sampling size from generated unseen distribution to magnify the influence of modest valid information included in synthetic unseen samples on feature expression. However, such a resampling strategy has been proven to result in overfitting of the classifier on certain feature patterns (the biased generated unseen distribution in this setting) in other fields <ref type="bibr" target="#b0">[Buda et al., 2018;</ref><ref type="bibr" target="#b10">Menon et al., 2020]</ref>, which harms the recognition of both seen and unseen classes. These findings imply that there is a need to restrict the generation number of unseen samples. Can we directly transmit class imbalance information to the classifier during training rather than inefficiently increasing the expression of unseen classes by resampling? In this work, we regard the bias and homogeneity of generated unseen samples as special prior knowledge. In light of the success of loss modification in class imbalance research <ref type="bibr" target="#b9">[Lin et al., 2017;</ref><ref type="bibr" target="#b10">Menon et al., 2020]</ref>, we incorporate this seen-unseen prior into the classifier training process in the form of logit adjustment. Specifically, we establish the lower bound of the seenunseen balanced accuracy <ref type="bibr" target="#b12">[Xian et al., 2017]</ref> with variational Bayesian inference and obtain an adjusted posterior. Then the prior takes part in training via adjusting the logits of vanilla cross-entropy loss. This approach, termed Zero-Shot Logit Adjustment (ZLA), allows for a lower number of generated unseen samples while producing more balanced results. By establishing a semantic-prototype mapping, we further introduce the semantic information to the classifier. Notably, our proposed ZLA allows the generated unseen samples to play an adjustment role rather than supplying unseen class information, which overcomes the embedding-based classifier's previous ineffectiveness in the generative scenario (see <ref type="bibr">Sec. 4.5)</ref>. Our contributions are summarized as follows:</p><p>? We mathematically derive the lower bound of the seenunseen balanced accuracy, allowing us to include generated unseen samples's bias and homogeneity as a seenunseen prior in cross-entropy via logit adjustment.</p><p>? Based on ZLA, we break the previous classifier's inconsistency in training objectives and test metrics and the inability to exploit semantic priors in classifier learning.</p><p>? Our proposed classifier enables greatly reducing the generation number of unseen samples. It outperforms So-TAs when combined with the base generator, and can be a plug-in to augment various generation-based methods.</p><p>2 Related Work</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Zero-Shot Learning</head><p>Zero-Shot Learning (ZSL) <ref type="bibr" target="#b9">[Lampert et al., 2009;</ref><ref type="bibr">Farhadi et al., 2009]</ref> has become a popular research area in recent years.</p><p>Classic ZSL excludes seen classes during the test, while Generalized Zero-Shot Learning (GZSL) <ref type="bibr" target="#b1">[Chao et al., 2016;</ref><ref type="bibr" target="#b12">Xian et al., 2017]</ref> considers both seen and unseen classes, attracting more current interest. The typical embedding-based ZSL methods <ref type="bibr" target="#b9">[Li et al., 2019;</ref><ref type="bibr">Skorokhodov and Elhoseiny, 2021]</ref> learn the semantic-visual links for classification, but with little effectiveness in the GZSL scenario. The progress of GZSL was once driven by the development of generative models <ref type="bibr" target="#b8">[Kingma and Welling, 2013;</ref><ref type="bibr" target="#b5">Goodfellow et al., 2014]</ref>, which allowed converting the GZSL problem to common supervised classification using a generator-classifier architecture. Until recently, research on generators <ref type="bibr" target="#b11">[Shen et al., 2020;</ref><ref type="bibr" target="#b6">Han et al., 2021]</ref> gradually saturated, whereas classifier design is rarely examined in such a two-stage framework.</p><p>To break the bottleneck of generation-based approaches, the principle design of a classifier is required.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Posterior Modification</head><p>Posterior modification, which has been deeply studied in class imbalance learning <ref type="bibr" target="#b9">[Lin et al., 2017;</ref><ref type="bibr" target="#b10">Menon et al., 2020]</ref>, aims at producing a class-balanced prediction. Post-hoc correction <ref type="bibr" target="#b3">[Collell et al., 2016]</ref>, loss re-weighting <ref type="bibr" target="#b10">[Menon et al., 2013]</ref>, and logit adjustment <ref type="bibr" target="#b10">[Menon et al., 2020]</ref> are its representative strategies. A similar procedure has been adopted in certain ZSL research. DCN <ref type="bibr" target="#b10">[Liu et al., 2018]</ref> utilizes entropy regularization to calibrate the predictions of seen and unseen classes. The post-hoc correction, known as calibrated stacking <ref type="bibr" target="#b1">[Chao et al., 2016]</ref> in ZSL, is also employed. However, a more general strategy in generation-based GZSL is to sample a large number of unseen class samples from the generated distribution. Although the re-sampling technique <ref type="bibr" target="#b2">[Chawla et al., 2002]</ref> has been proven to produce overfitting in long-tail learning <ref type="bibr" target="#b3">[Collell et al., 2016]</ref>, its shortcoming in generation-based GZSL is a lack of exploration. In this paper, we mathematically introduce the more advanced logit adjustment strategy into GZSL for a better balance between seen and unseen predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head><p>Considering two disjoint label sets, Y s and Y u , GZSL aims at recognizing instances that belong to Y s ? Y u , while only accessing samples with labels in Y s during training. Define the visual space X ? R dx and the semantic set</p><formula xml:id="formula_0">A = {a y |y ? Y s ? Y u } ? R da ,</formula><p>where d x and d a are dimensions of these two spaces. Then the goal of GZSL is to learn such a classifier, i.e., f gzsl : X ? Y s ? Y u given the training set D tr = {x, y|x ? X , y ? Y s } and the global semantic set A.</p><p>The two-stage framework typically processes this problem with two main components: the generator G and the classifier C . The generator G, defined as an arbitrary generative model <ref type="bibr" target="#b8">[Kingma and Welling, 2013;</ref><ref type="bibr" target="#b5">Goodfellow et al., 2014]</ref>, is first trained with the seen visual features and their corresponding semantics for conditionally mapping the Gaussian noise to fit the real visual distribution. The instances generated by unseen class semantics and the real seen instances are then fed into the classifier C together to fit the posterior probability:</p><formula xml:id="formula_1">x = G(z, a),z ? N (0, I), p(y| x) := softmax[C ( x)], x ? X ? {x},<label>(1)</label></formula><p>wherex denotes the synthetic instances, z represents random Gaussian noises, and x is either real or synthetic instances. p(y| x) denotes the posterior probability derived from the classifier. Then the model predicts the class label by taking C out = argmax y (p(y| x)). In this work, we focus on the design of the classifier under the GZSL setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Preliminary: Logit Adjustment</head><p>Logit adjustment strategy is commonly employed in class imbalance tasks <ref type="bibr" target="#b9">[Lin et al., 2017;</ref><ref type="bibr" target="#b10">Menon et al., 2020]</ref>, which weights the logit in softmax cross-entropy, i.e.,</p><formula xml:id="formula_2">L LA = log[1 + y =y ?(y, y )exp(C y (x) ? C y (x))],<label>(2)</label></formula><p>where C y (?) is the logit corresponding to class y , and ?(y, y ) represents the adjustment weight. The larger ?(y, y ) results in the network focusing more on optimizing the logit of y , allowing control of the prediction probabilities of different categories. Existing class imbalance works typically associate the weights with the class prior of y or y <ref type="bibr" target="#b1">[Cao et al., 2019;</ref><ref type="bibr" target="#b11">Tan et al., 2020;</ref><ref type="bibr" target="#b10">Menon et al., 2020]</ref>. Regardless the bias problem ( <ref type="figure" target="#fig_1">Figure 1</ref>) of generated unseen samples, generation-based methods achieve a certain success in GZSL. Thus, we empirically investigate the working mechanism of the generated unseen samples. As shown in <ref type="table" target="#tab_0">Table 1</ref>, we compare the single class center point (semantic to visualcenter mapping trained with MSE loss) resampling technique with two generative models, i.e., VAE <ref type="bibr" target="#b8">[Kingma and Welling, 2013]</ref> and WGAN <ref type="bibr" target="#b6">[Gulrajani et al., 2017]</ref> (detailed in supplementary material). Two phenomena can be intuitively observed by comparing the results in <ref type="table" target="#tab_0">Table 1</ref>: <ref type="formula" target="#formula_1">(1)</ref> the key success of generation-based models in GZSL relies on unseen class feature expression enhancement by a large number of generated unseen samples; and (2) the more diversified samples (generated from generative models) produce a limited performance improvement compared to replicating a single point. We forego a deeper study due to its orthogonal nature to our work, but these modest findings imply that the synthetic unseen samples are rather homogenous than real ones. Despite the difficulty of eliminating bias and homogeneity, can we include them in classifier training as the seen-unseen prior? Below, we'll illustrate how, by changing the classifier's optimization target, we can integrate this prior into the learning process in the form of the logit adjustment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Empirical Analysis on Generated Samples</head><formula xml:id="formula_3">Method G.N. T1 A U A S A</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">From the Statistical View</head><p>The nature of GZSL is an extreme case of class imbalance, as measured by a class balanced metric A H (detailed in Sec. 4.1). Assuming the class space has been completed by a set of pseudo unseen class samples generated by the trained generator, existing classifiers optimize the global accuracy A G by modeling the base posterior probability (Eq. 1):</p><formula xml:id="formula_4">A G = E x?p(x) q(C out = y x |x),<label>(3)</label></formula><p>where p(x) is defined as a uniform distribution over all data, y x is the true label of x, and q(C out = y|x) is the probability to predict class y with the classifier C. However, Eq. 3 neglects the imbalance between seen and unseen domains, which is inconsistent with the test metric A H . To find balanced results across classes, we in turn employ evaluation indicators to guide the design of the classifier. Indeed, let A(y) denote the accuracy in class y, we have <ref type="bibr" target="#b3">[Collell et al., 2016</ref>]</p><formula xml:id="formula_5">A(y) = E x?p(x) q(C out = y|x)p(y|x) p(y) ,<label>(4)</label></formula><p>where p(y) represents the statistics frequency of class y, and p(y|x) denotes the real posterior probability. Then the average accuracy of seen classes is</p><formula xml:id="formula_6">A S = 1 |Y s | y?Y s A(y) = 1 |Y s | y?Y s E x?p(x) q(C out = y|x)p(y|x) p(y) = E y?Y s E x?p(x) q(C out = y|x)p(y|x) p(Y s )p(y|y ? Y s ) ,<label>(5)</label></formula><p>where p(y|y ? Y s ) denotes the frequency of class y in Y s , and p(Y s ) is a theoretically derived data-independent probability which will be served as a hyperparameter. Analogously, the average accuracy of unseen classes is</p><formula xml:id="formula_7">A U = E y?Y u E x?p(x) q(C out = y|x)p(y|x) p(Y u )p(y|y ? Y u ) .<label>(6)</label></formula><p>Then we consider the harmonic mean, A H , which serves the target of attaining high accuracy for both seen and unseen classes, and empirically reaches its maximum when the accuracy of seen and unseen classes is balanced <ref type="bibr" target="#b12">[Xian et al., 2017]</ref>. We have</p><formula xml:id="formula_8">A H = 2/( 1 A S + 1 A U ).<label>(7)</label></formula><p>Based on the convex property of the inversely proportional function, we have the upper bound of 1/A S with Jensen Inequality:</p><formula xml:id="formula_9">1 A S = 1/E y?Y s E x?p(x) q(C out = y|x)p(y|x) p(Y s )p(y|y ? Y s ) ? E y?Y s E x?p(x) p(Y s )p(y|y ? Y s ) q(C out = y|x)p(y|x) .<label>(8)</label></formula><p>Using the same inequality on A U , we have the lower bound of A H : where Y is Y s (Y u ) when y belongs to seen (unseen) classes.</p><formula xml:id="formula_10">A H ? 2/E y?Y s ?Y u E x?p(x) |Y s ? Y u |p(Y)p(y|y ? Y) |Y|q(C out = y|x)p(y|x) ,<label>(9)</label></formula><p>To simplify the symbols, we designate |Y s ? Y u |p(Y)/|Y| as p 0 (Y) in the following. Despite the difficulty in determining the Bayesian optimal of A H , maximizing its lower bound achieves an approximate effect, which is equivalent to minimizing its reciprocal. Intuitively, the denominator term of Eq. 9 is minimized if</p><formula xml:id="formula_11">q(C out = y|x) = 1, if y = argmax i p(i|x) p0(Y)p(i|i?Y) 0, otherwise<label>(10)</label></formula><p>for each x in p(x). So, given a datum (x, y x ), we change the modeling objective in Eq. 1 to the adjusted posterior probability, i.e.,</p><formula xml:id="formula_12">p(y x |x) p 0 (Y)p(y x |y x ? Y) ,<label>(11)</label></formula><p>where p 0 (Y) refers to the seen-unseen prior (Sec. 3.2) which reflects the bias and homogeneity of pseudo unseen samples. Eq. 11 theoretically gives a more balanced predicted probability distribution than the base posterior, as shown in <ref type="figure" target="#fig_2">Figure  2</ref>. Next, we will show the estimation of the adjusted posterior.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">ZLA-Based Classifier</head><p>Adjusted Cross-Entropy The base posterior probability in Eq. 1 is typically estimated with the cross-entropy loss, i.e.,</p><formula xml:id="formula_13">L CE = log y =y [1 + exp(C y (x) ? C y (x))].<label>(12)</label></formula><p>Referring to researches on class imbalance [Tan et al., 2020; Menon et al., 2020], we directly model Eq. 11 with C(?) and the posterior becomes</p><formula xml:id="formula_14">p(y|x) := p 0 (Y)p(y|y ? Y) ? softmax[C(x)].<label>(13)</label></formula><p>This enables integrating the conditional class prior p(y|y ? Y) and the seen-unseen prior p 0 (Y) into the softmax crossentropy in a logit adjustment manner. Then the weights in Eq. 2 are replaced with ?(y, y ) :</p><formula xml:id="formula_15">= p 0 (Y )p(y |y ? Y ) p 0 (Y)p(y|y ? Y) .<label>(14)</label></formula><p>The final adjusted cross-entropy is</p><formula xml:id="formula_16">L ZLA = log[1 + y =y p 0 (Y )p(y |y ? Y ) p 0 (Y)p(y|y ? Y) exp(C y (x) ? C y (x))].<label>(15)</label></formula><p>In contrast to the standard cross-entropy form, we consider the specific prior information in the generation-based GZSL setting, which contributes to the balancing results across classes. In practice, we make p 0 (Y s ) much bigger than p 0 (Y u ). This is intuitively explainable from two perspectives: first, small p 0 (Y u )/p 0 (Y s ) promotes seen samples to focus on learning decision boundaries between seen classes; and second, large p 0 (Y s )/p 0 (Y u ) encourages large prediction probabilities for unseen classes, which serves the same purpose as a large generation number of unseen samples <ref type="bibr" target="#b12">[Xian et al., 2018;</ref><ref type="bibr" target="#b6">Han et al., 2021]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Semantic Prior Inclusion</head><p>Embedding-based methods <ref type="bibr" target="#b9">[Li et al., 2019;</ref><ref type="bibr">Skorokhodov and Elhoseiny, 2021]</ref> work by learning a semantic-visual direct link. In this case, an extra generator is hard to aid in embedder learning (see Sec. 4.5 for experimental results) because the overlap between the knowledge (i.e., semantic-visual link) learned by the generator and the embedder results in semantic information crucial for knowledge transfer not being fully exploited. The proposed ZLA, in contrast, allows for supporting the learning of the semantic-based classifier through an adjustment mechanism. No longer teaching the net the semantic-visual link of unseen classes, the pseudo unseen samples adjust the decision boundaries between seen and unseen classes by weighting the logits, avoiding knowledge overlapping to an extent. Specifically, we adopt a prototype learner P [Li et al., 2019; Skorokhodov and Elhoseiny, 2021] to directly map semantics to visual prototypes, and then the adjusted posterior probability of a datum x is estimated through cosine similarity, i.e.,</p><formula xml:id="formula_17">C(x) := cos(x, P(a))/?,<label>(16)</label></formula><p>where ? is the temperature <ref type="bibr" target="#b7">[Hinton et al., 2015]</ref>. In the test phase, the prediction class y * corresponds to the prototype that achieves the maximum similarity.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implementation Details</head><p>Since our work focuses on studying a plug-in classifier, we test it on WGAN <ref type="bibr" target="#b6">[Gulrajani et al., 2017]</ref>, with the same generator and discriminator structures as <ref type="bibr" target="#b12">[Xian et al., 2018]</ref>. Our prototype learner P consists of a multi-layer perceptron (MLP) with a single 1024-D hidden layer activated by LeakyReLU and no activation function in the output. The Adam optimizer is employed with a learning rate of 1?10 ?3 , and the batch size is set at 512 for evaluating our design. When plugging into CE-GZSL [Han et al., 2021], we employ a batch size of 256 in SUN and 512 in other datasets instead of the default 4096 (due to device limitations) while maintaining all other settings in the published paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Comparison with State-of-the-Arts</head><p>We apply the proposed classifier to the vanilla WGAN (compare to f-CLSWGAN <ref type="bibr" target="#b12">[Xian et al., 2018]</ref>) and the more advanced CE-GZSL [Han et al., 2021] to show its effect and compatibility in different generative frameworks. The baseline results are obtained from the official codes. As shown in <ref type="table" target="#tab_2">Table 2</ref>, the combination of the proposed classifier and the basic generative framework (WGAN) outperforms So-TAs, demonstrating the excellent class balancing capability of ZLA. We note that the performance gain provided by our module is not as significant in the highly fine-grained SUN dataset as it is in other datasets. It is mainly due to the minor bias problem of generated unseen samples in the case of multiple classes and modest feature variations that it takes the limited strengths of our approach. Moreover, our classifier improves the performance of both f-CLSWGAN and CE- GZSL, even though CE-GZSL has already produced decent results, proving its plug-in ability in two-stage frameworks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Hyperparameters</head><p>Three key factors are involved in our work, i.e., the generation number (per unseen class) N g , p 0 (Y) in Eq. 11, and the temperature ? in <ref type="bibr">Eq. 16. Following [Skorokhodov and Elhoseiny, 2021]</ref>, ? is fixed at 0.04 for all experiments, since it has a slight bearing on our study. We first examine p 0 (Y) in the form of the ratio of p 0 (Y s ) to p 0 (Y u ), which more intuitively reflects the seen-unseen dichotomy. The ratio is denoted as ?, and its effect is plotted in <ref type="figure" target="#fig_4">Figure 3 (left)</ref>, where a reverse variation of seen and unseen accuracy can be observed, demonstrating ZLA's capacity to moderate the prediction between classes. <ref type="figure" target="#fig_4">Figure 3</ref> (right) depicts the influence carried by N g . Although N g also posses the ability to affect the accuracy, a large generation number lowers the performance cap (72.8 with 10 generated vs. 69.7 with 1000 generated), indicating the harm of re-sampling the biased samples. In the major experiment <ref type="table" target="#tab_2">(Table 2)</ref>, we generate 10 samples per unseen class for all datasets to contrast with f- <ref type="bibr">CLSWGAN [Xian et al., 2018]</ref>, and the best results are obtained when ? is set to 1000, 30, 60, and 300 for AWA2, CUB, SUN, and APY, respectively (better results are possible by trading off between N g   and ?). In comparison to CE-GZSL <ref type="bibr" target="#b6">[Han et al., 2021]</ref>, we keep the published generated number and take the value of ? as 28, 9, 7, and 920 for the above datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Ablation Study</head><p>In this section, we perform ablation studies to validate our design and display the key elements in our implementation. Function of ZLA <ref type="figure" target="#fig_4">Figure 3</ref> (right) shows the function of ZLA, where log ? = 0 means p 0 (Y s ) = p 0 (Y u ), i.e., without major adjustments. The effect of ZLA is reflected in the comparison of different ? values. Intuitively, an adequate ? value produces considerable performance gains in varying generation numbers, especially when the number is minimal. Furthermore, ZLA enables the prototype learner to be effective in generative scenarios, as illustrated in <ref type="table" target="#tab_5">Table 4</ref> (second lines in two baselines reflect previous ineffectiveness), which further reduces the reliance on the generation number and thereby resolving the previously discussed bias problem.</p><p>Beyond the Standard Prototype Learner Existing prototype leaners <ref type="bibr" target="#b9">[Li et al., 2019;</ref><ref type="bibr">Skorokhodov and Elhoseiny, 2021]</ref> are simply established on the semantic-visual links of seen classes, generalizing to unseen classes based on semantic similarities. In this paper, we find the last ReLU layer is crucial to these approaches' hitherto unseen class performances. To investigate the effect of the ReLU function, we compare the latest pure prototype learner <ref type="bibr">[Skorokhodov and Elhoseiny, 2021]</ref> with the WGAN-based ZLAP in <ref type="table" target="#tab_4">Table 3</ref>. When the ReLU layer is removed, as shown in <ref type="table" target="#tab_4">Table 3</ref>, the seen accuracy improves in both baselines, whereas the unseen accuracy decreases if pseudo unseen samples are not accessible. Zeroing the negative output layer values intuitively affects (seen class) prototype expression. However, it provides a regularization which narrows the gap between the unseen class prototypes and the instances when (pseudo) unseen in-stances are unavailable in training, since the visual feature components are likewise larger than or equal to zero <ref type="bibr" target="#b12">[Xian et al., 2017]</ref>. In this sense, our proposed ZLA allows the model to remove the ReLU layer by adjusting the unseen class expression using the synthetic unseen instances, resulting in a win-win situation for both seen and unseen class accuracy.</p><p>Beyond the Vanilla Softmax Classifier In <ref type="table" target="#tab_5">Table 4</ref>, we compare the prototype-based classifier to the vanilla softmax classifier to validate its necessity. Results reveal that the prototype learner beats the vanilla softmax classifier thoroughly when ZLA is employed. The explanations are as follows:</p><p>(1) the prototype learner enables further regularization on unseen class prototypes by learning semantic-prototype relations with real-world data, whereas the vanilla softmax classifier learns to distinguish unseen classes solely by generated samples; and (2) classifier weights mapped from semantics focus more on category-distinctive information, i.e., semantic information. Specifically, in coarse-grained datasets like AWA2, the generated unseen class samples meet a more serious bias problem, causing the classifier to incorrectly detect unseen classes. The prototype learner, on the other hand, can provide supplemental information to unseen class weights by comprehending the semantic-visual links in seen classes. In fine-grained datasets such as CUB, samples from different classes are relatively close together, making it challenging to separate them correctly. The semantics-based classifier, which contains intrinsically category-distinctive information, aids in increased discrimination.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Time Complexity Analysis</head><p>We note that some recent generation-based methods <ref type="bibr" target="#b6">[Han et al., 2021;</ref><ref type="bibr">Chen et al., 2021b]</ref> also mine the semantic discriminant information of samples. However, these methods are typically built on class contrast during generator training, which yields a time complexity of O(N |Y s |) in the training phase (N is the data size). In comparison, the proposed classifier combined with the vanilla WGAN achieves a comparable result with O(N ) time complexity. Moreover, the proposed method allows for a much smaller (10 vs. 4000 in AWA2) synthetic number in the classifier training phase, resulting in further time savings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this work, we theoretically include the logit adjustment tech in the generation-based GZSL. We begin by examining the bias and homogeneity of the generated unseen samples, which build the seen-unseen prior. Then we derive an adjusted posterior from the seen-unseen balanced metric, which enables integrating the seen-unseen prior into the original cross-entropy via logit adjustment. Considering the zero-shot setting, we call our approach Zero-Shot Logit Adjustment. Based on ZLA, we inject the semantic information into the classifier, which always fails in existing two-stage methods. Our work explores the underutilized potential of the generation-based GZSL by breaking the previous inconsistency between the classifier's training objectives and testing metrics. This approach allows for greatly fewer generated unseen samples, achieving SoTA with little time consumption.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>t-SNE visualization of the synthetic-real unseen data (left) and the real train-test seen data (right) in AWA2, we sample the same number for each class pair. The synthetic unseen data shows obvious domain bias from real test unseen data, while the training domain of the seen classes is consistent with the test domain.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>A seen (s) and unseen (u) class-prediction-probability example of modeling the base and the adjusted posteriors. The unseen class probabilities are suppressed when modeling the base posterior p(y|x), while the adjusted posterior p(y|x)/[p0(Y)p(y|y ? Y)] provides a more balanced distribution.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>P(a i )).(17)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Left: hyperparemeter analysis of ? (see 4.4 for its definition) on AWA2. Right: log ? varies w.r.t. generation number per unseen class. Large generation number lowers the performance cap (72.8 with 10 generated vs. 69.7 with 1000 generated).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>ZSL (T1) and GZSL (A H ) results of the simple semanticvisual mapping net (denoted as MSE) and two different generative models, VAE and WGAN on AWA2. G.N. denotes the generation number per unseen class (588 is the class averaged number of real seen samples).</figDesc><table><row><cell></cell><cell>H</cell></row><row><cell>MSE</cell><cell>588 67.9 17.1 71.6 27.6 4000 67.5 57.1 59.7 58.4</cell></row><row><cell>VAE</cell><cell>588 68.0 25.8 67.6 37.3 4000 68.6 57.2 68.8 62.9</cell></row><row><cell>WGAN</cell><cell>588 68.7 20.9 83.2 33.5 4000 68.3 57.7 71.0 63.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Li et al. ICCV [Li et al., 2019] 56.4 81.4 66.7 47.4 47.6 47.5 36.3 2.8 39.3 26.5 74.0 39.0 DVBE CVPR [Xu et al., 2020] 63.6 70.8 67.0 53.2 60.2 56.5 45.0 37.2 40.7 32.6 58.3 41.8 RGEN ECCV[Xie et al., 2020] 67.1 76.5 71.5 60.0 73.5 66.1 44.0 31.7 36.8 30.4 48.1 37.2 APN NeurIPS [Min et al., 2020] 56.5 78.0 65.5 65.3 69.3 67.2 41.9 34.</figDesc><table><row><cell></cell><cell>Method</cell><cell>Reference</cell><cell>A U</cell><cell>AWA2 A S</cell><cell>A H</cell><cell>A U</cell><cell>CUB A S</cell><cell>A H</cell><cell>A U</cell><cell>SUN A S</cell><cell>A H</cell><cell>A U</cell><cell>APY A S</cell><cell>A H</cell></row><row><cell>?</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">0 37.6</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>Li et al.</cell><cell>AAAI [Li et al., 2021]</cell><cell cols="9">56.9 80.2 66.6 51.1 58.2 54.4 47.6 36.6 41.4</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>GCM-CF</cell><cell>CVPR [Yue et al., 2021]</cell><cell cols="12">60.4 75.1 67.0 61.0 59.7 60.3 47.9 37.8 42.2 37.1 56.8 44.9</cell></row><row><cell></cell><cell>AGZSL</cell><cell>ICLR [Chou et al., 2021]</cell><cell cols="12">65.1 78.9 71.3 41.4 49.7 45.2 29.9 40.2 34.3 35.1 65.5 45.7</cell></row><row><cell></cell><cell>FREE</cell><cell cols="10">ICCV [Chen et al., 2021a] 60.4 75.4 67.1 55.7 59.9 57.7 47.4 37.2 41.7</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>?</cell><cell>SDGZSL</cell><cell cols="7">ICCV [Chen et al., 2021b] 64.6 73.6 68.8 59.9 66.4 63.0</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="3">38.0 57.4 45.7</cell></row><row><cell></cell><cell>f-CLSWGAN</cell><cell>CVPR [Xian et al., 2018]</cell><cell cols="12">57.7 71.0 63.7 59.4 63.3 61.3 46.2 35.2 40.0 32.5 57.2 41.5</cell></row><row><cell></cell><cell>WGAN+ZLAP</cell><cell>Proposed</cell><cell cols="12">65.4 82.2 72.8 73.0 64.8 68.7 50.1 38.0 43.2 40.2 53.8 46.0</cell></row><row><cell></cell><cell>CE-GZSL</cell><cell>CVPR [Han et al., 2021]</cell><cell cols="12">65.3 75.0 69.9 66.9 65.9 66.4 52.4 34.3 41.5 28.3 65.8 39.6</cell></row><row><cell></cell><cell cols="2">CE-GZSL+ZLAP Proposed</cell><cell cols="12">64.8 80.9 72.0 71.2 66.2 68.6 50.9 35.7 42.0 38.3 60.9 47.0</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">4 Experiments</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="6">4.1 Datasets and Metrics</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="10">Benchmark Datasets We study GZSL performed in An-</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="10">imals with Attributes 2 (AWA2) [Lampert et al., 2013],</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="10">Attribute Pascal and Yahoo (APY) [Farhadi et al., 2009],</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="10">Caltech-UCSD Birds-200-2011 (CUB) [Wah et al., 2011],</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="10">and SUN Attribute (SUN) [Patterson and Hays, 2012], fol-</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="10">lowing the common split (version 2) proposed in [Xian et al.,</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="10">2017]. AWA2 includes 50 animal species and 85 attribute</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="10">annotations, accounting 37,322 samples. APY contains 32</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="10">classes of 15,339 samples and 64 attributes. CUB consists</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="10">of 11,788 samples with 200 bird species, annotated by 312</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="10">attributes. SUN carries 14,340 images from 717 different</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="6">scenario-style with 102 attributes.</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="10">Visual Representations and Semantic Descriptors We</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="10">follow [Xian et al., 2017] to represent images as the 2048-D</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="10">ResNet-101 [He et al., 2016] features. Moreover, we regard</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="10">the artificial attribute annotations that come with the datasets</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="10">as the semantic descriptors of AWA2, APY, and SUN, and the</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="10">1024-dimensional character-based CNN-RNN features [Reed</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>GZSL performance comparisons with state-of-the-art methods. A U and A S denote the per-class accuracy (%) on unseen and seen classes, respectively, and A H is their harmonic mean. The best results are bolded, and the underlines indicate the second-place results. ? and ? represent whether a generator is employed to obtain the pseudo unseen samples, respectively ( ? indicates yes, and ? is the opposite). ZLAP is the proposed zero-shot logit adjustment prototype learner.</figDesc><table><row><cell>et al., 2016] generated from textual descriptions as the seman-</cell></row><row><cell>tics of CUB.</cell></row><row><cell>Evaluation Protocol We calculate the average per-class</cell></row><row><cell>top-1 accuracy for unseen and seen classes, respectively, de-</cell></row><row><cell>noted as A</cell></row></table><note>U and A S . Then the metric A H for GZSL is rep- resented as their harmonic mean.[Xian et al., 2017].</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Comparison of the pure prototype learner and the generation-based ZLA prototype learner.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">Non Gen.: a latest pro-</cell></row><row><cell cols="8">posed pure prototype learner, implemented by the official code (with</cell></row><row><cell cols="8">the post-hoc correction removed for a fair comparison). Gen.:</cell></row><row><cell cols="7">WGAN-based zero-shot logit adjustment prototype learner.</cell></row><row><cell cols="2">Classifier ZLA L.N.</cell><cell>A U</cell><cell>AWA2 A S</cell><cell>A H</cell><cell>A U</cell><cell>CUB A S</cell><cell>A H</cell></row><row><cell></cell><cell>?</cell><cell cols="6">40.2 82.5 54.1 61.4 52.3 56.5</cell></row><row><cell>Vanilla</cell><cell>?</cell><cell cols="6">57.7 71.0 63.7 59.4 63.3 61.3</cell></row><row><cell></cell><cell></cell><cell cols="6">61.2 74.6 67.3 66.8 63.5 65.1</cell></row><row><cell></cell><cell>?</cell><cell cols="6">65.4 82.2 72.8 73.0 64.8 68.7</cell></row><row><cell>Proto.</cell><cell>?</cell><cell cols="6">50.7 75.8 60.8 60.0 63.4 61.4</cell></row><row><cell></cell><cell></cell><cell cols="6">64.1 73.1 68.3 72.4 63.0 67.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Vanilla softmax classifier vs. prototype learner, based on WGAN, where L.N. represents a large generation number.</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A systematic study of the class imbalance problem in convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>References</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Buda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">106</biblScope>
			<biblScope unit="page" from="249" to="259" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Boqing Gong, and Fei Sha. An empirical study and analysis of generalized zero-shot learning for object recognition in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="52" to="68" />
		</imprint>
	</monogr>
	<note>Learning imbalanced datasets with label-distribution-aware margin loss</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Free: Feature refinement for generalized zero-shot learning. ICCV, 2021</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Chawla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
	<note>Semantics disentangling for generalized zero-shot learning</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Reviving threshold-moving: a simple plug-in bagging ensemble for binary and multiclass imbalanced data. CoRR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Chou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<editor>Farhadi et al., 2009] Ali Farhadi, Ian Endres, Derek Hoiem, and David Forsyth</editor>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1778" to="1785" />
		</imprint>
	</monogr>
	<note>CVPR</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Transductive multi-view embedding for zero-shot recognition and annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="584" to="599" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Goodfellow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Generative adversarial nets. NeurIPS</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Contrastive embedding for generalized zero-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Gulrajani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.00028</idno>
	</analytic>
	<monogr>
		<title level="m">Xiangyu Zhang, Shaoqing Ren, and Jian Sun</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>CVPR</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Auto-encoding variational bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Welling ; Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Rethinking zero-shot learning: A conditional visual classification perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Lampert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<editor>Li et al., 2019] Kai Li, Martin Renqiang Min, and Yun Fu</editor>
		<meeting><address><addrLine>Ross Girshick</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1966" />
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
	<note>ICCV</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">On the statistical consistency of algorithms for binary classification under class imbalance</title>
		<idno type="arXiv">arXiv:2007.07314</idno>
	</analytic>
	<monogr>
		<title level="m">Sadeep Jayasumana, Ankit Singh Rawat, Himanshu Jain, Andreas Veit, and Sanjiv Kumar. Long-tail learning via logit adjustment</title>
		<editor>Menon et al., 2020] Aditya Krishna Menon</editor>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="603" to="611" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>NeurIPS</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Ivan Skorokhodov and Mohamed Elhoseiny. Class normalization for (continual)? generalized zero-shot learning. ICLR, 2021</title>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>Patterson and Hays</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="11662" to="11671" />
		</imprint>
	</monogr>
	<note>CVPR</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Yongqin Xian, Bernt Schiele, and Zeynep Akata. Zero-shot learning-the good, the bad and the ugly</title>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<editor>Xian, Tobias Lorenz, Bernt Schiele, and Zeynep Akata</editor>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="5542" to="5551" />
		</imprint>
	</monogr>
	<note>CVPR</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Region graph embedding network for zero-shot learning</title>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="562" to="580" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Attribute prototype network for zeroshot learning</title>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Counterfactual zero-shot and open-set visual recognition</title>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="15404" to="15414" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
