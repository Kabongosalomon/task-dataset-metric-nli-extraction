<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Logic-Driven Context Extension and Data Augmentation for Logical Reasoning of Text</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyuan</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Data Science</orgName>
								<orgName type="institution">Fudan University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanjun</forename><surname>Zhong</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Sun Yat-Sen University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duyu</forename><surname>Tang</surname></persName>
							<email>dutang@microsoft.com</email>
							<affiliation key="aff2">
								<orgName type="institution">Microsoft</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongyu</forename><surname>Wei</surname></persName>
							<email>zywei@fudan.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Data Science</orgName>
								<orgName type="institution">Fudan University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhihao</forename><surname>Fan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Data Science</orgName>
								<orgName type="institution">Fudan University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daxin</forename><surname>Jiang</surname></persName>
							<email>djiang@microsoft.com</email>
							<affiliation key="aff2">
								<orgName type="institution">Microsoft</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
							<email>zhouming@chuangxin.com</email>
							<affiliation key="aff3">
								<orgName type="institution">Sinovation Ventures</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Duan</surname></persName>
							<email>nanduan@microsoft.com</email>
							<affiliation key="aff2">
								<orgName type="institution">Microsoft</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Logic-Driven Context Extension and Data Augmentation for Logical Reasoning of Text</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T16:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Logical reasoning of text requires understanding critical logical information in the text and performing inference over them. Largescale pre-trained models for logical reasoning mainly focus on word-level semantics of text while struggling to capture symbolic logic. In this paper, we propose to understand logical symbols and expressions in the text to arrive at the answer. Based on such logical information, we not only put forward a context extension framework but also propose a data augmentation algorithm. The former extends the context to cover implicit logical expressions following logical equivalence laws. The latter augments literally similar but logically different instances to better capture logical information, especially logical negative and conditional relationships. We conduct experiments on Re-Clor dataset. The results show that our method achieves the state-of-the-art performance, and both logic-driven context extension framework and data augmentation algorithm can help improve the accuracy. And our multi-model ensemble system is the first to surpass human performance on both EASY set and HARD set of ReClor. 1</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Recent years have witnessed a growing interest in logical reasoning of text, which aims to follow a logic-level analysis over a given text to arrive at the correct answer <ref type="bibr" target="#b15">(McCarthy, 1989;</ref><ref type="bibr" target="#b16">Nilsson, 1991)</ref>. It is a challenging task since it requires the ability to extract critical information from text and perform logical inference over them <ref type="bibr" target="#b21">(Williams et al., 2018;</ref><ref type="bibr" target="#b4">Habernal et al., 2018;</ref><ref type="bibr" target="#b12">Liu et al., 2020)</ref>.</p><p>An example of logical reasoning problem is shown in <ref type="figure">Figure 1</ref>. It takes a context, a related * Work is done during internship at Microsoft Research Asia. <ref type="bibr">1</ref> Codes are publicly available at https://github. com/WangsyGit/LReasoner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Logical Symbols :</head><p>! : have keyboarding skills " : be able to use a compute # : be able to write your essays using a word processing program</p><p>Extend the Implicit Logical Expressions by Laws:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Context:</head><p>If you have no keyboarding skills at all, you will not be able to use a computer. And if you are not able to use a computer, you will not be able to write your essays using a word processing program. Question If the statements above are true, which one of the following must be true? Options A. If you are not able to write your essays using a word processing program, you have no keyboarding skills. B. If you are able to write your essays using a word processing program, you have at least some keyboarding skills. C. If you are not able to write your essays using a word processing program, you are not able to use a computer. D. If you have some keyboarding skills, you will be able to write your essays using a word processing program.</p><p>Logical Expressions :</p><formula xml:id="formula_0">(? ! ? ? " ) (? " ? ? # ) (? ! ? ? " ) ? (" ? ! ) (? " ? ? # ) ? (# ? " ) (? ! ? ? " ) ? (? " ? ? # ) ? (? ! ? ? # ) (" ? ! ) ? (# ? " ) ? (# ? ! ) (? # ? ? ! ) (# ? ! ) (! ? # ) (? # ? ? " )</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Contrapostion</head><p>Contrapostion Transitive Law Transitive Law <ref type="figure">Figure 1</ref>: A logical reasoning example from ReClor dataset <ref type="bibr" target="#b25">(Yu et al., 2020)</ref> which includes a context, a question and four options and the correct answer is marked by . To find the answer, it need to extract logical symbols, identify logical expressions and perform logical inference to extend the implicit logical expressions. The underlined phrases represent logical symbols. The colored rectangles are corresponding logical expressions of options.</p><p>question and four options as the input, and needs to analyze which option follows logically from the context. In order to solve such a problem, the reasoning system first needs to extract the critical constituents from the context as logical symbols like ?, ?, ? and identify the existing logical expressions that are composed of logical symbols, like (?? ? ??). Then according to logical equivalence laws, it performs inference to extend the logi-cal expressions that are not explicitly mentioned in the context and compare them with the logical expressions of each option to find the most plausible answer.</p><p>Although large-scale pre-trained language models <ref type="bibr" target="#b3">(Devlin et al., 2019;</ref><ref type="bibr" target="#b13">Liu et al., 2019;</ref><ref type="bibr" target="#b24">Yang et al., 2019)</ref> have achieved promising results on logical reasoning of text, they mainly leverage word-level semantics but hardly capture symbolic logic. In responding to such a problem, we propose to identify logical symbols and expressions explicitly mentioned in the context which serve as the elementary components of logical inference process. Based upon such logical information, we propose both a context extension framework and a data augmentation algorithm. The whole system is built on top of the pre-trained model considering its promising performance. Our logic-driven context extension framework performs inference following logical equivalence laws to extend the logical expressions in the context to cover the implicit ones. For better utilization of extended logical expressions at symbolic space into neural model, we verbalize them into natural language and feed as an extended context into a pre-trained model to match the options and find the answer. Our logic-driven data augmentation algorithm employs modification operations over logical expressions to produce challenging instances with literally similar but logically different contexts. It trains our model to distinguish different contexts to better capture logical information, especially negative and conditional relationships in logical expressions.</p><p>We take both RoBERTa <ref type="bibr" target="#b13">(Liu et al., 2019)</ref> and ALBERT <ref type="bibr" target="#b9">(Lan et al., 2020)</ref> as our backbone pretrained models. The experiments are conducted on the benchmark dataset ReClor <ref type="bibr" target="#b25">(Yu et al., 2020)</ref> and our system achieves the state-of-the-art performance and reaches the top of the leaderboard. The further results also show the effectiveness of both logic-driven context extension framework and data augmentation algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Task and Background</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Task Definition</head><p>We study the problem of logical reasoning of text on a multiple-choice question answering task. The task is described as following: given a context c, a question q, and four associated options {o 1 , o 2 , o 3 , o 4 }, we aim to select the most appropriate option as the answer o a .</p><p>We adopt a challenging dataset ReClor <ref type="bibr" target="#b25">(Yu et al., 2020)</ref> which is collected from the logical reasoning questions of standardized examinations including Law School Admission Test (LSAT) 2 and Graduate Management Admission Test (GMAT) 3 . Although many questions need to be answered through complicated logical reasoning, there are still some biased instances which can be solved even without knowing contexts and questions. It therefore splits the unbiased instances from the test data as the HARD set to fully evaluate the logical reasoning ability. And the other biased ones are taken as the EASY set. A leaderboard 4 is also hosting for public evaluation on ReClor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Base Model</head><p>In this paper, we follow the leading methods on the leaderboard to take pre-trained models as our base model. Pre-trained models for multiple-choice question answering concatenate the context, the question and each option as the input and encode the sequence for calculating its score. Given four options, four concatenated sequences are constructed to calculate four scores, and the one with the highest score is chosen as the answer. Specifically, the concatenated sequence is formulated as</p><formula xml:id="formula_1">[CLS] c [SEP ] q || o [SEP ]</formula><p>, where c is the context and q || o is the concatenation of the question and each option. The representations of the special token [CLS] in the four sequences are fed into a linear layer with a softmax function to get the probability distribution of options as P ({o 1 , o 2 , o 3 , o 4 }|c, q). The cross entropy loss is calculated as:</p><formula xml:id="formula_2">L A = ? log P (o a |c, q)<label>(1)</label></formula><p>where o a is the correct option.</p><p>Since previous work <ref type="bibr" target="#b25">(Yu et al., 2020)</ref> has shown that RoBERTa <ref type="bibr" target="#b13">(Liu et al., 2019)</ref> achieves superior results on the leaderboard, and ALBERT <ref type="bibr" target="#b9">(Lan et al., 2020)</ref> is proved to be even more powerful than RoBERTa on many tasks, we utilize both RoBERTa and ALBERT in the experiments.</p><p>RoBERTa is a robustly optimized BERT <ref type="bibr" target="#b3">(Devlin et al., 2019)</ref> with more training data, which uses 2 https://www:lsac:org/lsat/ taking-lsat/testformat/logical-reasoning 3 https://www.mba.com/exams/gmat/ about-the-gmat-exam/gmat-exam-structure/ verbal 4 https://eval.ai/web/challenges/ challenge-page/503/leaderboard/1347</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Context:</head><p>If you have no keyboarding skills at all, you will not be able to use a computer. And if you are not able to use a computer, you will not be able to write your essays using a word processing program.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Options:</head><p>A. If you are not able to write your essays using a word processing program, you have no keyboarding skills. B. If you are able to write your essays using a word processing program, you have at least some keyboarding skills. C. If you are not able to write your essays using a word processing program, you are not able to use a computer. D. If you have some keyboarding skills, you will be able to write your essays using a word processing program.</p><p>symbol ? symbol ? symbol ? Implicit Logical Expressions:</p><formula xml:id="formula_3">( ? ? ? ? ? ) ? ( ? ? ? ) ( ? ? ? ? " ) ? ( " ? ? ) ( ? ? ? ? ? ) ? ( ? ? ? ? " ) ? ( ? ? ? ? " ) ( ? ? ? ) ? ( " ? ? ) ? ( " ? ? )</formula><p>Extended Logical Expressions related to each option:</p><formula xml:id="formula_4">A. ( ? ? ? ? " ) ; B. ( " ? ? ) ; ( " ? ? ) ; C. ( ? ? ? ? " ) ; D. ( ? ? ? ) ; ( " ? ? ) ; ( " ? ? ) ;</formula><p>Logical Expressions in the context:</p><formula xml:id="formula_5">( ? ? ? ? ? ) ; ( ? ? ? ? " ) ;</formula><p>Logical Expressions in each option:</p><formula xml:id="formula_6">A. ( ? " ? ? ? ) ; B. ( " ? ? ) ; C. ( ? " ? ? ? ) ; D. ( ? ? " ) ;</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Extended contexts of each option:</head><p>A. If you do not have keyboarding skills, then you will not be able to write your essays ? B. If you be able to write your essays ? , then you will be able to use a computer. If you be able to write your essays ? , then you will have keyboarding skills. C. If you do not have keyboarding skills, then you will not be able to write your essays ? D. If you be able to use a computer, then you will have keyboarding skills. If you be able ?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Logic Identification Logic Extension</head><p>Pre-trained Encoder</p><formula xml:id="formula_7">[CLS] c [SEP] q || # ! [EXT] $ ! [SEP]</formula><p>?? score ? ! Logic Verbalization <ref type="figure">Figure 2</ref>: The overall architecture of our proposed logic-driven context extension framework. c, q, o i and e i are the context, question, i-th option and the extended context for i-th option, respectively. The texts in green mean that the option B is matched against its extended context which has the highest score. a more dynamic sentence masking method and removes the next sentence prediction loss.</p><p>ALBERT is a lite BERT with two parameterreduction techniques including factorized embedding parameterization and cross-layer parameter sharing. And it replaces the next sentence prediction loss with the inter-sentence coherence loss. However, such pre-trained models for logical reasoning of text directly encode the triplet of context, question and options, which only leverage word-level semantics but struggle to capture symbolic logic in the text. Therefore, we propose a framework on top of a pre-trained model to understand logical symbols and expressions in the text and perform logical inference over them to predict the answer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Logic-Driven Context Extension</head><p>In this section, we present a logic-driven context extension framework for logical reasoning of text, and the overall architecture is illustrated in <ref type="figure">Figure 2</ref>. The framework can be divided into three steps as follow. It first identifies the logical symbols and expressions explicitly mentioned in the context and options ( ? 3.1). Then it performs logical inference over them to extend the logical expressions implicit in the context ( ? 3.2). Finally, it verbalizes the extended logical expressions related to each option as an extended context and utilizes it in the pretrained model to match the answer ( ? 3.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Logic Identification</head><p>In order to perform logical reasoning, we first need to identify the elementary components for reasoning as logical expressions. We identify the existing logical expressions for each sentence in the context and each option. To show the format of the logical expression, we introduce some notations:</p><p>(1) {?, ?, ?, ...}: the logical symbols, which are the basic constituents in the context to constitute the logical expressions, such as the "have keyboarding skills" in <ref type="figure">Figure 2</ref>.</p><p>(2) {?, ?}: the logical connectives set. ? means the negation operation upon a specific logical symbol and ? acts as a conditional relationship between two logical symbols.</p><p>(3) {(? ? ?), ...}: the logical expressions which are composed of logical symbols and connectives. (? ? ?) means that ? is the condition of ?.</p><p>Because logical expressions are composed of logical symbols, we first employ constituency parser <ref type="bibr" target="#b6">(Joshi et al., 2018)</ref> to extract constituents including noun phrases and gerundial phrases as basic symbols. Then the logical symbols in each sentence are combined by logical connectives to constitute the logical expressions as follow-up. If any negative word is related to a logical symbol ?, we add the negation connective ? before ? as a new logical symbol ? ?. We define a set of negative words as {"not", "n't", "unable", "no", "few", "little", "neither", "none of "}. Then if there is a conditional relationship between two logical symbols ? and ? in a sentence, we can construct the corresponding logical expression as (? ? ?). We simply recognize the conditional relationship between symbol ? and ? as (? ? ?) according to conditional indicators such as "if ?, then ?", "? in order for ?", "? due to ?", "?? unless ?", etc.</p><p>And if an active voice occurs between ? and ?, we also have (? ? ?). As illustrated in <ref type="figure">Figure 2</ref>, given the context with two sentences, we can extract three logical symbols {?, ?, ?} and identify two existing logical expressions as (?? ? ??) and (?? ? ??).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Logic Extension</head><p>In addition to the logical expressions explicitly mentioned in the context, there are still some other implicit ones which we need to logically infer and extend. We combine the identified logical expressions existing in all sentences of the context as a logical expression set S, and perform logical inference over them to further extend the implicit logical expressions according to logical equivalence laws. Here we follow two logical equivalence laws including contraposition <ref type="bibr" target="#b19">(Russel et al., 2013)</ref> and transitive law <ref type="bibr" target="#b26">(Zhao et al., 1997)</ref>:</p><formula xml:id="formula_8">Contraposition : (? ? ?) =? (?? ? ??) (2) Transitive Law : (? ? ?) ? (? ? ?) =? (? ? ?) (3)</formula><p>Then the extended implicit logical expressions form an extension set of the current logical expression set S as S E . As the example in <ref type="figure">Figure 2</ref>, the set of existing logical expressions is S = {(?? ? ??), (?? ? ??)} and the logic extension set is</p><formula xml:id="formula_9">S E = {(? ? ?), (? ? ?), (?? ? ??), (? ? ?)}.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Logic Verbalization</head><p>After inferring the extended logical expressions set S E , we verbalize them into natural language for better utilization in the pre-trained model. We first select the related expressions from S E for each option. A logical expression is regarded as related to an option if it has the same logical symbols with the option judged by the text overlapping, and whether a negation connective exists also needs to be considered. For example, (?? ? ??) in <ref type="figure">Figure 2</ref> is related to option C because they both contain ??. Then we transform all logical expressions related to the option at symbolic space into natural language by filling them into a template and concatenate them into a sentence. We take such a sentence as an extended context for this option. The template is designed as shown in <ref type="table" target="#tab_0">Table 1</ref>.</p><p>We feed extended contexts into the pre-trained model to match the options and predict the answer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>logic</head><p>(?? ? ??) template If do not ?, then will not ?. extended context If you do not have keyboarding skills, then you will not be able to write your essays using a word processing program. We take an extended context as sentence e, and introduce a special token [EXT ] to represent context extension. Then we reformulate the input sequence</p><formula xml:id="formula_10">as [CLS] c [SEP ] q || o [EXT ] e [SEP ]</formula><p>for encoding and feed the [CLS] representation into a classification layer to get each option's score and find the most appropriate answer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Logic-Driven Data Augmentation</head><p>In order to make the pre-trained model put more focus on logical information in the context, especially logical negative and conditional relationships, we further introduce a logic-driven data augmentation algorithm. Inspired by SimCLR , it employs contrastive learning to augment challenging instances with literally similar but logically different contexts based on logical expressions. It then trains our model to predict the correct context supporting the answer. We first introduce the background of SimCLR and then describe our logicdriven constrative learning.</p><p>SimCLR As a paradigm of self-supervised representation learning by comparing different samples, contrastive learning <ref type="bibr" target="#b23">(Wu et al., 2018;</ref><ref type="bibr" target="#b10">Le-Khac et al., 2020;</ref><ref type="bibr" target="#b5">He et al., 2020)</ref> aims to make the representations of similar samples be mapped close together, while that of dissimilar samples be further away in the encoding space. The goal can be described as following.</p><formula xml:id="formula_11">s(f (x), f (x + )) s(f (x), f (x ? ))<label>(4)</label></formula><p>x + is a positive sample similar to the data point x while x ? is a negative sample dissimilar to x. f (?) is an encoder to learn a representation and the s(?) is a metric function that measures the similarity between two representations. Over this, SimCLR  bulids a classifier to distinguish positive from negative samples and learns to capture what makes two samples different.</p><p>Logic-Driven Contrastive Learning In our multiple-choice question answering setting, we alter the score function from measuring the similarity between two representations to calculating the score that the question can be solved by the correct answer under a given context:</p><formula xml:id="formula_12">s (c + , q, o a ) s (c ? , q, o a )<label>(5)</label></formula><p>where (c + , q, o a ) and (c ? , q, o a ) are the positive and negative sample, c + and c ? are the positive and negative context, respectively, and s is the score function. The contrastive loss can be formulated as a classification loss for predicting the most plausible context that supports the answer:</p><formula xml:id="formula_13">L C = ? log exp(s (+)) exp(s (+)) + exp(s (?))<label>(6)</label></formula><p>where s (+) and s (?) are short for s (c + , q, o a ) and s (c ? , q, o a ) respectively. Aware of symbolic logical expressions, we can construct logical negative samples including negative contexts that are literally similar but logical dissimilar to the positive one. We take the original context to construct the positive sample. Then we generate a negative sample by modifying the existing logical expressions in the context and verbalizing the modified logical expressions into a negative context as ? 3.3. During the modification operations, we randomly choose a logical expression and randomly delete, reverse or negate such a expression. The delete, reverse or negate operations are respectively to delete a logical expression in the context, reverse the conditional order of a logical expression and negate a logical symbol in a logical expression. The constructing procedure of a logical negative sample is illustrated in <ref type="figure" target="#fig_0">Figure 3</ref>. Then the model can be trained to better capture logical information, especially negative and conditional relationships in logical expressions.</p><p>In the logic-driven data augmentation algorithm, our framework is trained with a combined loss as L = L A + L C . And the classification of positive and negative context for the correct answer is also implemented in the logic-driven context extension framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experimental Setup</head><p>Our experiments are conducted on ReClor <ref type="bibr" target="#b25">(Yu et al., 2020)</ref>, a question answering dataset extracted (!"#$%&amp;$, '(%)$*"#, -#).%/) (0 ? 2), (2 ? 3), ? from standardized exams including GMAT and LSAT which requires complicated logical reasoning. It consists of 6, 138 questions and each question is collected with a context and four answer options, in which only one is correct. It is divided into training, validation and test sets with 4, 638, 500 and 1, 000 data points, and test set is split into EASY set and HARD set with 440 and 560 data points. For automatic evaluation, we adopt the accuracy as the metric to compare performance of models over test set, EASY set and HARD set. We take both RoBERTa-large <ref type="bibr" target="#b13">(Liu et al., 2019</ref>) and ALBERT-xxlarge-v2 <ref type="bibr" target="#b9">(Lan et al., 2020)</ref> as our backbone models and implement them using Huggingface <ref type="bibr" target="#b22">(Wolf et al., 2019)</ref>. We use a batch size of 8 and fine-tune on ReClor for 10 epochs. The AdamW <ref type="bibr" target="#b14">(Loshchilov and Hutter, 2017)</ref> with ?1 = 0.9 and ?2 = 0.98 is taken as the optimizer and the learning rate is set to 1e-5. We use a linear learning rate scheduler with 10% warmup proportion. We select at most two extended logical expressions related to each option to construct the extended context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Logic Identification</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Randomly delete, reverse or negate a logical expression</head><formula xml:id="formula_14">(!"#$%&amp;$ ! , '(%)$*"#, -#).%/) delete (2 ? 3), ? reverse (2 ? 0), (2 ? 3), ? negate (0 ? ?2), (2 ? 3), ? (?0 ? 2), (2 ? 3), ?</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Logic Verbalization</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Overall Performance</head><p>We compare our systems with several baseline models and human performance, which are described as follows.</p><p>Baseline Models The compared baseline pretrained models for multiple-choice question answering include GPT <ref type="bibr" target="#b17">(Radford et al., 2018)</ref>, <ref type="bibr">GPT-2 (Radford et al., 2019)</ref>, BERT <ref type="bibr" target="#b3">(Devlin et al., 2019)</ref>, XLNet <ref type="bibr" target="#b24">(Yang et al., 2019)</ref>, RoBERTa <ref type="bibr" target="#b13">(Liu et al., 2019)</ref> and ALBERT <ref type="bibr" target="#b9">(Lan et al., 2020)</ref>. In this paper, BERT, XLNet, RoBERTa and ALBERT respec-tively refer to BERT-large, XLNet-large, RoBERTalarge and ALBERT-xxlarge-v2.</p><p>Our Systems LReasoner RoBERTa is our proposed logic-driven reasoner built on top of RoBERTa, which utilizes both logic-driven context extension framework and data augmentation algorithm. LReasoner ALBERT is also our proposed logic-driven reasoner but taking ALBERT as the backbone model.</p><p>Human Performance The average score of ten graduate students over randomly chosen 100 samples from test set is taken as human performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Test EASY HARD  The evaluation results are shown in <ref type="table" target="#tab_2">Table 2</ref>. We have several findings: -Our models outperform all baseline models by a considerable margin. LReasoner ALBERT even surpasses the human performance in Test and EASY sets. This indicates the effectiveness of our method for predicting more accurate answer. -LReasoner RoBERTa and LReasoner ALBERT both perform better than the corresponding baseline models RoBERTa and ALBERT. It demonstrates that our proposed method is robust to be effective for logical reasoning on top of different pretrained models, even the base model is already of great power, such as ALBERT. -Our models generate large improvement on both HARD set and EASY set by comparing our models with RoBERTa and ALBERT, respectively. This not only follows our intuition that our system is designed for logical reasoning problems, but also shows that it is capable of solving easier problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Ablation Study</head><p>To dive into the effectiveness of different components in our logic-driven reasoner, we conduct ablation study which takes RoBERTa as our backbone model on both validation and test sets.  As shown in <ref type="table" target="#tab_4">Table 3</ref>, RoBERTa+CE and RoBERTa+DA both outperform the baseline model RoBERTa and perform worse than our final system RoBERTa+CE+DA. It indicates that both logicdriven context extension framework and logicdriven data augmentation algorithm are beneficial for question answering involving logical reasoning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Further Analysis on Negative Sample Construction Strategies</head><p>To further analyze the effectiveness of our logical negative samples in logic-driven contrastive learning, we compare several different negative sample construction strategies in contrastive learning on top of RoBERTa.  From the results in <ref type="table" target="#tab_6">Table 4</ref>, we can see that all models with contrastive learning outperform the Context : Everyone sitting in the clubhouse of the golf course today at ten o' clock had just registered for a beginner' s golf lesson. Gerald, Robert, and Shirley were sitting in the clubhouse this morning at ten o' clock. No accomplished golfer would register for a beginner' s golf lesson. Question : If the statements above are true, which one of the following must also be true on the basis of them?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Options :</head><p>A. Gerald, Robert, and Shirley were the only people who registered for a beginner s golf lesson this morning. ( ! ? Others ) B. None of the people sitting in the clubhouse this morning at ten o' clock had ever played golf. ( ? ? ? Others ) C. Neither Gerald nor Shirley is an accomplished golfer. ( ! ? ? " ) D. Everyone sitting in the clubhouse this morning at ten o' clock registered only for a beginner's golf lesson. ( ? ? Others ) Answer : C</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Logical Symbols &amp; Expressions</head><p>? : sitting in the clubhouse of the golf course today at ten o' clock; ? : registered for a beginner' s golf lesson ; ! : Gerald, Robert, and Shirley; ": accomplished golfer ;</p><formula xml:id="formula_15">? ? ? ; ! ? ? ; " ? ? ? ;</formula><p>Extending the Implicit Logical Expressions <ref type="figure">Figure 4</ref>: A case of the reasoning process of our logic-driven reasoner LReasoner ALBERT . Phrases underlined represent other symbols (called Others) different from the logical symbols in context and tokens in bold make them different. The option marked by is our predicted answer. model without it, which demonstrates that contrastive learning can help to better predict the answer. Our logic-driven contrastive learning RoBERTa(w/ CLR-L) performs best. It reveals that logical negative samples are more effective than negative samples constructed by other methods which make the model to better capture the logical negative and conditional relationships in the context for logical reasoning.</p><formula xml:id="formula_16">( ? ? ? ) ? (? ? ? ? ? ) ; ( ! ? ? ) ? (? ? ? ? ! ) ; ( " ? ? ? ) ? ( ? ? ? " ) ; ( ? ? ? ) ? ( ! ? ? ) ? ( ! ? ? ) ; (? ? ? ? ? ) ? (? ? ? ? ! ) ? (? ? ? ? ! ) ; ( ? ? ? ) ? ( ? ? ? " ) ? ( ? ? ? " ) ; ( " ? ? ? ) ? (? ? ? ? ? ) ? ( " ? ? ? ) ; ( ! ? ? ) ? ( ? ? ? " ) ? ( ! ? ? " ) ; ( " ? ? ? ) ? (? ? ? ? ! ) ? ( " ? ? ! ) ;</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implicit Logical Expressions related to each option</head><formula xml:id="formula_17">A. ( ! ? ? ) ; ( ! ? ? " ) ; B. ( ? ? ? " ) ; C. ( ! ? ? ) ; ( ! ? ? " ) ; D. ( ? ? ? " ) ;</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Case Study</head><p>A case study is presented in <ref type="figure">Figure 4</ref> to show the reasoning process of our system. At first the logical symbols are correctly extracted from the context and the logical expressions are identified based on them considering logical negative and conditional relationships. Then we extend the logical expressions by inferring implicit ones in the context. For each option we recognize its logical expression and find the related extended expressions. We verbalize them into natural language to feed into pre-trained model as an extended context to compute a matching score. Finally we take option C which exactly matches an extended implicit logical expression as the most plausible answer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">Error Analysis</head><p>Although our system achieves the best performance, there still exists some instances that can not be solved. ReClor dataset integrates various logical reasoning skills which can be categorized into 17  types. We can investigate the detailed performance with respect to different logical reasoning types of our system LReasoner ALBERT and the baseline model ALBERT, to analyse which type of questions tend to be more challenging. As shown in <ref type="table" target="#tab_8">Table 5</ref>, we can see that all models perform relative poorly on certain logical reasoning types, such as Most Strongly Supported and Implication. The first type is less straight-forward and requires deeper logical reasoning considering degree. Specifically, Most Strongly Supported aims to find the choice that is most strongly supported by a stimulus. For Implication, all candidate options prefer to be more similar to each other or the context which are more difficult to be classified, like the examples in <ref type="figure">Figure 1</ref> and 4. But our system is still able to make progress on them which prove the effectiveness of our system for logical reasoning.</p><p>Our model achieves great improvement in most of the logical reasoning types compared to the baseline model but performs comparably or even worse in following question types: Sufficient Assumption, Evaluation, Conclusion/Main Point, Role, Match flaws and Weaken. The first four types only take up a small proportion of the whole data, which results in no improvement over them. Match flaws needs to understand more abstract concept flaw and find an option exhibiting the same flaw as the context. Weaken aims to find the opposite statement that weaken the argument. Our system to first extract logical expressions and then imply the implicit logical expressions is not suitable for matching flaw and identifying weaken statement. How to deal with different logical reasoning types is our further research direction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related Work</head><p>In recent years, an increasing number of tasks and datasets have been introduced targeting on logical reasoning of text in NLP area. Natural Language Inference (NLI) (Dagan et al., 2005) is a typical task requiring logical reasoning, which aims to determine whether a hypothesis can reasonably be entailed from a premise. <ref type="bibr">SNLI (Bowman et al., 2015)</ref>, MultiNLI <ref type="bibr" target="#b21">(Williams et al., 2018)</ref>, QNLI <ref type="bibr" target="#b20">(Wang et al., 2018)</ref> and SciTail <ref type="bibr" target="#b7">(Khot et al., 2018)</ref> are widely used benchmarks in evaluating the performance of NLI. However, these datasets handle the task at sentence-level and the required logical reasoning ability is simple. <ref type="bibr" target="#b11">Liu et al. (2021)</ref> propose ConTRoL dataset investigating NLI for long texts and examining more complex contextual reasoning types. Argument reasoning comprehen-sion <ref type="bibr" target="#b4">(Habernal et al., 2018)</ref> is a task closer to NLI which also concerns passage-level logical reasoning. Given a premise and a claim, it aims to identify the correct implicit warrant from two opposing options. But it is limited to only one logical reasoning type, i.e., warrant identification.</p><p>Another task involving logical reasoning of text is question answering. Several multiple-choice question answering datasets, which need to select an answer from candidate options given a context and a question, have been proposed for promoting the development of logical reasoning. These datasets are all sourced from public standardized exams so that they partly mitigate the noise brought by crowd-sourcing or automaticallygeneration <ref type="bibr" target="#b8">(Lai et al., 2017)</ref>. LogiQA <ref type="bibr" target="#b12">(Liu et al., 2020)</ref> is collected from National Civil Servants Examination of China which covers 5 types of logical reasoning. <ref type="bibr" target="#b25">Yu et al. (2020)</ref> propose ReClor dataset from the GMAT and LSAT tests. It examines more complicated logical reasoning integrating 17 question types. In this paper, we conduct experiments on top of ReClor for investigating diverse logical reasoning skills.</p><p>Large-scale pre-trained language models <ref type="bibr" target="#b3">(Devlin et al., 2019;</ref><ref type="bibr" target="#b13">Liu et al., 2019;</ref><ref type="bibr" target="#b24">Yang et al., 2019;</ref><ref type="bibr" target="#b9">Lan et al., 2020)</ref> have been widely adopted for logical reasoning of text, which directly encode the given texts to predict the output. Without understanding symbolic logic, they can still achieve a promising performance. We therefore take a pre-trained model as our backbone model and identify logical expressions for our proposed logic-driven system .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion and Future Work</head><p>In this paper, we focus on the task of logical reasoning of text and propose to understand the logical symbols and expressions in the context to find the answer. Taking them as elementary components of logical inference, we first propose a logic-driven context extension framework to extend the logical expressions to cover the implicit ones and verbalize them as an extended context to match the answer. We also introduce a logic-driven data augmentation algorithm which employs contrastive learning to augment literally similar but logically different instances to help our model better capture logical information, especially logical negative and conditional relationships. Experimental results on ReClor dataset confirm the effectiveness of our logic-driven reasoner with both logic-driven con-text extension framework and data augmentation algorithm.</p><p>In the future, we will explore how to take different logical reasoning types into consideration to deal with the question types that still can not be well solved. We also would like to design a model to directly encode the symbolic logic rather than utilize them by verbalizing into natural language. Besides, we plan to evaluate our model on more datasets. As our codes are publicly available, anyone interested could also have a try.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 :</head><label>3</label><figDesc>Procedure to construct a logical negative sample.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>An example of verbalizing a logical expression into text.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Experimental results of different models and human performance. The results in bold are the best performance of each column and * indicates that the results come from<ref type="bibr" target="#b25">Yu et al. (2020)</ref>.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Ablation study of our system built on top of RoBERTa. CE and DA are respectively our logic-driven context extension framework and data augmentation algorithm. RoBERTa+CE+DA is our proposed LReasoner RoBERTa .</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Comparison of different negative sample construction approaches on top of RoBERTa. CLR represents contrastive learning. RS means randomly selecting a context from in-batch data while RD means randomly deleting a sentence from the original context as the negative sample. L denotes our logical negative samples in logic-driven contrastive learning.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 :</head><label>5</label><figDesc>Detailed results on different logical reasoning types. Numbers in parentheses are percentages of different reasoning types. Base is the ALBERT model while Ours means our LReasoner ALBERT system. ?, ? and ? respectively mean that our performance is better, worse than and equal to the baseline model.</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A large annotated corpus for learning natural language inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabor</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Angeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Potts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="632" to="642" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In International conference on machine learning</title>
		<imprint>
			<biblScope unit="page" from="1597" to="1607" />
			<date type="published" when="2020" />
			<publisher>PMLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The pascal recognising textual entailment challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Ido Dagan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernardo</forename><surname>Glickman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Magnini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning Challenges Workshop</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="177" to="190" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Long and Short Papers</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The argument reasoning comprehension task: Identification and reconstruction of implicit warrants</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Habernal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henning</forename><surname>Wachsmuth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benno</forename><surname>Stein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>New Orleans, Louisiana</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1930" to="1940" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9729" to="9738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Extending a parser to distant domains using a few dozen partially annotated examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vidur</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Hopkins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1190" to="1199" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Scitail: A textual entailment dataset from science question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tushar</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Sabharwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">RACE: Large-scale ReAding comprehension dataset from examinations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guokun</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qizhe</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="785" to="794" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Albert: A lite bert for self-supervised learning of language representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenzhong</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingda</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piyush</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Soricut</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Contrastive representation learning: A framework and review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Phuc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Le-Khac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">F</forename><surname>Healy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smeaton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>IEEE Access</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Natural language inference in contextinvestigating contextual reasoning over long texts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanmeng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leyang</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Logiqa: A challenge dataset for machine reading comprehension with logical reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leyang</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanmeng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dandan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yile</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence</title>
		<meeting>the Twenty-Ninth International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno>abs/1907.11692</idno>
		<title level="m">Roberta: A robustly optimized bert pretraining approach. CoRR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Fixing weight decay regularization in adam</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<idno>abs/1711.05101</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Artificial intelligence, logic and formalizing common sense</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Mccarthy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Philosophical logic and artificial intelligence</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1989" />
			<biblScope unit="page" from="161" to="190" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Logic and artificial intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nils</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nilsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial intelligence</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">1-3</biblScope>
			<biblScope unit="page" from="31" to="56" />
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Improving language understanding by generative pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Tim Salimans, and Ilya Sutskever</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">OpenAI blog</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Artificial intelligence: a modern approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stuart</forename><surname>Russel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Norvig</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>Pearson Education Limited</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Glue: A multi-task benchmark and analysis platform for natural language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel R</forename><surname>Bowman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A broad-coverage challenge corpus for sentence understanding through inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adina</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikita</forename><surname>Nangia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Bowman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>New Orleans, Louisiana</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1112" to="1122" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Huggingface&apos;s transformers: State-of-the-art natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clement</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierric</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Rault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R?mi</forename><surname>Louf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Morgan</forename><surname>Funtowicz</surname></persName>
		</author>
		<idno>abs/1910.03771</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Unsupervised feature learning via nonparametric instance discrimination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhirong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Stella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3733" to="3742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Xlnet: Generalized autoregressive pretraining for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Russ</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5753" to="5763" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Reclor: A reading comprehension dataset requiring logical reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanfei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Static logic implication with application to redundancy identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J-K</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elizabeth</forename><forename type="middle">M</forename><surname>Rudnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janak H</forename><surname>Patel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings. 15th IEEE VLSI Test Symposium (Cat. No. 97TB100125)</title>
		<meeting>15th IEEE VLSI Test Symposium (Cat. No. 97TB100125)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1997" />
			<biblScope unit="page" from="288" to="293" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
