<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">May the Force Be with Your Copy Mechanism: Enhanced Supervised-Copy Method for Natural Language Generation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyuk</forename><surname>Choi</surname></persName>
							<email>sanghyuk@ncsoft.com</email>
							<affiliation key="aff0">
								<orgName type="department">NCSOFT NLP Center</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeong-In</forename><surname>Hwang</surname></persName>
							<email>jihwang@ncsoft.com</email>
							<affiliation key="aff0">
								<orgName type="department">NCSOFT NLP Center</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyungjong</forename><surname>Noh</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">NCSOFT NLP Center</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yeonsoo</forename><surname>Lee</surname></persName>
							<email>yeonsoo@ncsoft.com</email>
							<affiliation key="aff0">
								<orgName type="department">NCSOFT NLP Center</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">May the Force Be with Your Copy Mechanism: Enhanced Supervised-Copy Method for Natural Language Generation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T20:53+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recent neural sequence-to-sequence models with a copy mechanism have achieved remarkable progress in various text generation tasks. These models addressed out-ofvocabulary problems and facilitated the generation of rare words. However, the identification of the word which needs to be copied is difficult, as observed by prior copy models, which suffer from incorrect generation and lacking abstractness. In this paper, we propose a novel supervised approach of a copy network that helps the model decide which words need to be copied and which need to be generated. Specifically, we re-define the objective function, which leverages source sequences and target vocabularies as guidance for copying. The experimental results on datato-text generation and abstractive summarization tasks verify that our approach enhances the copying quality and improves the degree of abstractness.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Natural language generation is a task in any setting in which we generate new text, such as machine translation, summarization, and dialogue systems <ref type="bibr" target="#b6">(Gatt and Krahmer, 2018)</ref>. Approaches using neural networks known as sequence-to-sequence (seq2seq) have achieved reliable results on such tasks <ref type="bibr" target="#b33">(Sutskever et al., 2014)</ref>. However, basic seq2seq models exhibit weaknesses in dealing with rare and out-of-vocabulary (OOV) words. <ref type="bibr" target="#b35">Vinyals et al. (2015)</ref> resolved this problem by copying words from the source sequence and directly inserting them into the output generation. This idea has been successfully applied to abstractive summarization tasks <ref type="bibr" target="#b10">(Gulcehre et al., 2016;</ref><ref type="bibr" target="#b9">Gu et al., 2016;</ref><ref type="bibr" target="#b30">See et al., 2017;</ref><ref type="bibr" target="#b38">Xu et al., 2020)</ref> and has been further adopted by <ref type="bibr" target="#b36">Wiseman et al. (2017)</ref> and <ref type="bibr" target="#b25">Puduppully et al. (2019a)</ref> to help the model copy correct values in data-to-text generation tasks. <ref type="bibr">TEAM</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pointer-Generator</head><p>The Atlanta Hawks defeated the host New York Knicks, 142-139, at Philips Arena on Wednesday. The Hawks came into this game as a sizable favorite and they didn't disappoint. In fact, Atlanta led for over 40 minutes of this game, as they led by double -digits for the entirety of the second half. ... Our Force-copy-unk The Atlanta Hawks (28-20) defeated the New York Knicks (21-28) 142-139 at Phillips Arena in Atlanta. The Hawks were led by Paul Millsap, who scored 37 points (13-29 FG, 3-8 3Pt, 8-10 FT) to go with 19 rebounds, seven assists and one steal in 60 minutes ... <ref type="figure">Figure 1</ref>: Comparison of output of data-to-text models on a ROTOWIRE dataset. Text that accurately reflects a record is highlighted in blue, and erroneous text is highlighted in red.</p><p>Unfortunately, the generation results obtained from copying mechanisms can be suboptimal. <ref type="bibr" target="#b40">Zhou et al. (2018)</ref> observed that some unrelated words appear unexpectedly in the middle of the phrase, or the phrase is not copied completely and some words are missing. <ref type="bibr" target="#b30">See et al. (2017)</ref> and <ref type="bibr" target="#b8">Gehrmann et al. (2018b)</ref> reported the lack of abstractness in the excessive copying of source words, resulting in insufficient novel expressions of generated summarization. For example, the CNN/DailyMail dataset  contains approximately 17% of novel words on gold summaries, but most copy models create less than 1%. Moreover, most data-to-text models based on copy mechanisms still suffer from poor factual inconsistencies. Accurately conveying the facts is especially important in informational communication, such as news, and low levels of veracity make these models unreliable and useless in practice <ref type="bibr">(Kryscin-ski et al., 2020)</ref>. <ref type="bibr" target="#b36">Wiseman et al. (2017)</ref> indicated that there is a significant gap between neural models and template systems in terms of generating text containing factual (i.e., correct) records. Despite the efforts of the dedicated research community <ref type="bibr" target="#b25">(Puduppully et al., 2019a;</ref><ref type="bibr">Rebuffel et al., 2020</ref><ref type="bibr" target="#b27">Rebuffel et al., , 2021</ref> there are still many challenges to narrow the gap.</p><p>In our experiments, we found that the probability of the copying words crucially affects the decoding process, resulting in fallacious generation when calculated inaccurately. Therefore, we argue that deciding between copying or generating can benefit from distinct guiding. Accordingly, we propose a force-copy method that forces the model to copy every word when it appears in source and target sequences simultaneously so as to increase copy precision. Furthermore, to relieve the lacking abstractness of the generated text, we present the force-copy-unk method, which forces model to copy a word only if it does not exist in the target vocabulary even if it appears in both the source and target sequences.</p><p>Our contributions are three-fold:</p><p>? We analyzed and compared the characteristics of prior copy models. On the basis of the analysis, we present force-copy and forcecopy-unk methods that promote accurate copy of source sequence.</p><p>? The force-copy and force-copy-unk methods improve the RG Precision score in data-totext, which indicates that the text generation based on the source data is conducted more correctly.</p><p>? The force-copy-unk achieves an improvement in copy precision, which not only increases the ROUGE score but also shows better abstraction in abstractive summarization task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head><p>As our work builds on the prior copy mechanism, we introduce Pointer-Generator Network <ref type="bibr" target="#b30">(See et al., 2017)</ref> as a baseline. In this study, the source text x is fed into a bidirectional LSTM (BiLSTM) encoder. However <ref type="bibr" target="#b13">Klein et al. (2020)</ref> reported that there is no significant difference in the copying performances of Transformer <ref type="bibr" target="#b34">(Vaswani et al., 2017)</ref> encoder compared to that of BiLSTM; thus we replaced it with a Transformer encoder producing a sequence of encoded hidden states h i . At each timestep t, the decoder receives the representation of the previously generated word to produce the decoder hidden states s t . From the hidden states, a context vector c t is calculated based on the attention distribution <ref type="bibr" target="#b0">(Bahdanau et al., 2015)</ref> as follows:</p><formula xml:id="formula_0">e t,i = v T tanh(W h h i + W s s t ) (1) ? t = softmax(e t )<label>(2)</label></formula><formula xml:id="formula_1">c t = i ? t,i h i<label>(3)</label></formula><p>where v, W h and W s are learnable parameters. The vocabulary distribution P vocab over all words in the target vocabulary is computed from s t , c t , and the learnable parameters W v , W v , b and b :</p><formula xml:id="formula_2">P vocab = softmax(W v (W v [s t ; c t ] + b) + b ) (4)</formula><p>The generation probability p gen is used as a soft switch to either generate from vocabulary or copy from the source words by sampling from the attention distribution ? t . Additionally, p gen for timestep t is calculated from the context vector c t , decoder state s t , and decoder input y t :</p><formula xml:id="formula_3">p gen = sigmoid(w T h c t +w T s s t +w T y y t +b ptr ) (5)</formula><p>where vectors w T h , w T s , w T y and the scalar bias b ptr are learnable parameters. Let a word w be an element in the set of extended vocabulary, which refers to the union of the target vocabulary and all words appearing in the input sequence; then, the final probability distribution P (w) is computed as:</p><formula xml:id="formula_4">P (w) = p gen P vocab (w) + (1 ? p gen ) i:w i =w ? t,i<label>(6)</label></formula><p>If w is an OOV word, then P vocab (w) is zero. Similarly, if w does not exist in the input sequence, then i:w i =w ? t,i is zero. During training, the loss at timestep t is the negative log-likelihood of the target word w * t for that timestep:</p><formula xml:id="formula_5">loss t = ?logP (w * t )<label>(7)</label></formula><p>3 Model</p><p>We next consider techniques for incorporating supervised learning of the copying decision into the model. To identify the number of occasions at which every copy and generation occurs, Eq. (6) is split into three cases: (i) No word is copied from the source sequence, i.e., words are sampled from the vocabulary probability distribution. In this case, p gen is guided to take higher value during the training time; (ii) The word copied from the source sequence does not exist in the target vocabulary. In this case, p gen is guided to take lower value during the training time; (iii) The word copied from the source sequence exists in the target vocabulary. In this case, p gen could have a scalar range in [0, 1] and is learned implicitly. We augment the decoder by supervising a soft switch p gen that determines whether the model generates or copies. First, we re-define the loss function as loss t = loss t vocab + loss t attn + loss t pgen (8)</p><p>where loss t vocab is the maximum likelihood estimation (MLE), which is used as a standard training criterion in the sequence-to-sequence model:</p><formula xml:id="formula_6">loss t vocab = ?log(P vocab (w * t ))<label>(9)</label></formula><p>It should be noted that P vocab (w * t ) indicates the probability of generating the unknown token if w * t is OOV. Similarly to the pointer network <ref type="bibr" target="#b35">(Vinyals et al., 2015)</ref>, we use attention distribution as a guide for copying, and train it to minimize the negative log-likelihood:</p><formula xml:id="formula_7">loss t attn = ?log( i:w i =w ? t,i )<label>(10)</label></formula><p>Finally, we adopt loss t pgen to train p gen explicitly. In fact, the optimal way is to feed the gold copy answer for every target word; However, there rarely exist supervised data for this task. Hence, we suggest two methods leveraging the source sequences and target vocabularies.</p><p>Force-copy Given a source sequence X = (x 1 , x 2 , ..., x Tx ), if a target word w * t appears in X, w * t can be a copy-candidate. For example, the word 'the' and 'COVID-19' are both copy-candidates in <ref type="figure" target="#fig_0">Figure 2</ref>. In the force-copy model, we assume that every copy-candidate word is copied from the source sequence. Therefore, p gen can be derived from the loss function:</p><formula xml:id="formula_8">loss t pgen = ?log(1 ? p gen ) if w * t ? X ?log(p gen ) otherwise<label>(11)</label></formula><p>This forces the copy switch p gen to perform a copy all copy-candidate words. However, we do not penalize the generating ability even on copy circumstances. We always train loss t vocab on the loss function regardless of the copy process.</p><p>Force-copy-unk Whereas the force-copy model tries to copy every copy-candidate word, it is also possible to generate words from the vocabulary distribution instead of copying if the word exists in the target vocabulary. For example, the copycandidate word 'the' could be generated instead of being copied in <ref type="figure" target="#fig_0">Figure 2</ref>. In this way, we restrict the scope of copy to unknown words in the forcecopy-unk model. Therefore, p gen can be derived from the loss function:  <ref type="table">Table 1</ref>: Brief comparison between prior copy models. Use explicit switch denotes that the model calculates an explicit switch probability. Recycle attention denotes that the model recycles the attention distribution as the copy distribution. Conditional activate denotes that the model activates the copy mechanism only for unknown or named-entity words or keywords. Mix copy, gen prob denotes that the model combines the probabilities from the vocabulary distribution with probabilities from the copy distribution. Sum src duplicated word denotes that the model adds all probabilities of the same words from the attention distribution when a word appears multiple times in the source sequence. Force train switch denotes that the model trains the switch probability explicitly. Tradeoff between copy/gen denotes that the model has a loss trade-off between copy and generation during training.</p><formula xml:id="formula_9">loss t pgen = ?log(1 ? p gen ) if w * t ? X, / ? V ?log(p gen ) otherwise<label>(</label></formula><p>word only if it is an unknown token. For other copy-candidate words that are not OOV, we found that it is effective to utilize the copy information as well (see Section 6.1). Inspired by work in guided alignment training for machine translation <ref type="bibr" target="#b2">(Chen et al., 2016)</ref>, we retain the loss t attn on the loss function to inform the decoder that the word is a copycandidate, thus inducing copy-like generation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Related Work</head><p>Abstractive Summarization Abstractive summarization aims to generate accurate and concise summaries that contain novel words in contrast to extractive summarization that extracts almost entire sentences from the input. Most prior works on abstractive summarization that employed neural networks achieved inspiring results <ref type="bibr" target="#b29">(Rush et al., 2015;</ref><ref type="bibr" target="#b30">See et al., 2017)</ref>. Especially after <ref type="bibr" target="#b35">Vinyals et al. (2015)</ref>, <ref type="bibr" target="#b10">Gulcehre et al. (2016)</ref> showed dramatic improvement by applying copy mechanisms, it has become a primary module for abstractive summarization. Recent work leveraging such pre-training models <ref type="bibr" target="#b4">(Dong et al., 2019;</ref><ref type="bibr" target="#b32">Song et al., 2019;</ref><ref type="bibr" target="#b17">Lewis et al., 2020;</ref><ref type="bibr" target="#b39">Zhang et al., 2020)</ref> has presented impressive advancements in performance when fine-tuned for text generation tasks. Applying the copy mechanism to these pre-training models has extended the success <ref type="bibr" target="#b38">(Xu et al., 2020;</ref><ref type="bibr" target="#b1">Bi et al., 2020)</ref>. Data-to-text Generation Data-to-text generation tasks aim to produce texts from non-linguistic input data <ref type="bibr" target="#b28">(Reiter, 2007)</ref>, including the generation of weather forecasts <ref type="bibr" target="#b18">(Liang et al., 2009</ref>) and biographical content from Wikipedia <ref type="bibr" target="#b16">(Lebret et al., 2016)</ref>. Specifically, Wiseman et al. <ref type="bibr">(2017)</ref> showed that a neural encoderdecoder model powered with the copy mechanism <ref type="bibr" target="#b9">(Gu et al., 2016;</ref><ref type="bibr" target="#b10">Gulcehre et al., 2016)</ref> can generate fluent multi-sentence summaries from game statistics without explicit templates or rules. Based on their approach, various promising neural approaches have been proposed for ROTOWIRE tasks <ref type="bibr">(Puduppully et al., 2019a,b;</ref><ref type="bibr">Rebuffel et al., 2020;</ref><ref type="bibr" target="#b12">Iso et al., 2019)</ref>. Regarding the copy mechanism, <ref type="bibr">Puduppully et al. (2019a,b)</ref> and <ref type="bibr" target="#b12">Iso et al. (2019)</ref>  Pointer / Copy Mechanism The fundamental structure of the copy mechanism was first introduced in the form of the pointer network <ref type="bibr" target="#b35">(Vinyals et al., 2015)</ref>. Expanding their work, various approaches combining attention probability and generation probability have been proposed <ref type="bibr" target="#b22">(Miao and Blunsom, 2016;</ref><ref type="bibr" target="#b9">Gu et al., 2016;</ref><ref type="bibr" target="#b10">Gulcehre et al., 2016;</ref>. In particular, <ref type="bibr" target="#b30">See et al. (2017)</ref> proposed PGNet, where two distributions are weighted and merged into a single mixture distribution with the aid of a soft switch mechanism. PGNet has become de facto standard for copy mechanisms in various tasks, including summarization <ref type="bibr" target="#b8">Gehrmann et al., 2018b)</ref>, data-to-text <ref type="bibr" target="#b7">(Gehrmann et al., 2018a;</ref><ref type="bibr">Rebuffel et al., 2020)</ref>, and question answering <ref type="bibr">(Mc-Cann et al., 2018)</ref>. <ref type="table">Table 1</ref> shows the characteristics of the copy models. While our models bear a resemblance to models that trains the switch probability, ours are considerably different from theirs in two aspects: (i) Compared to <ref type="bibr" target="#b10">Gulcehre et al. (2016)</ref>; Nallapati et al. <ref type="formula" target="#formula_0">(2016)</ref>; ; , those works train their copy components to activate only for certain words (i.e., OOV words, named entites, keywords, or values of structured data) whereas we activate it for all copy-candidates. Our force-copy-unk model also seems like it trains to copy only for unknown words, but it works for all copy-candidates by maintaining attention loss (see Eq. 10). (ii) We retain the vocabulary loss (see Eq. 9) even if the model copies a word whereas the other models induce a trade-off between copy and generation. Models that has a trade-off between copy and generation definitely weaken the effect of vocabulary loss when they activate the copy process. However, we believe it's necessary to keep vocabulary loss to prevent mistaken prediction from the vocabulary distribution because the switch probability p gen takes value in scalar range of [0, 1], not a binary value.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Data-to-text Generation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experimental Settings</head><p>We perform experiments on using two datasets, i.e., ROTOWIRE <ref type="bibr" target="#b36">(Wiseman et al., 2017)</ref> and MLB <ref type="bibr" target="#b26">(Puduppully et al., 2019b)</ref>    <ref type="bibr" target="#b30">(See et al., 2017)</ref> which we re-implemented based on the hierarchical-attention architecture of RBF-2020. Note that our (FC: force-copy, FCU: force-copyunk) models are also based on RBF-2020, in which we modified the copy module from the original. More details can be found in the Appendix A.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>Following prior work <ref type="bibr" target="#b36">(Wiseman et al., 2017;</ref><ref type="bibr" target="#b25">Puduppully et al., 2019a</ref><ref type="bibr" target="#b26">Puduppully et al., , 2019b</ref><ref type="bibr">Rebuffel et al., 2020)</ref>, we evaluate our models using both extractive (relation generation (RG), content selection (CS), and content ordering (CO)) and n-gram metrics (BLEU). For extractive metrics, we use the Information Extraction (IE) system suggested by <ref type="bibr" target="#b36">Wiseman et al. (2017)</ref>. Given a data record r, gold summary y and generated text?, the IE system identifies the entity (e.g., Knicks) and value (e.g., 28) pairs from? and then predicts the pair relation (e.g., WIN). RG measures precision and the number of unique relations extracted from? that also appear in r. CS estimates precision and recall of the relations extracted from? that are also extracted from y. Finally, CO computes the normalized Damerau-Levenshtein distance (DLD) between the sequences of relations extracted from? and y. We used the pretrained IE model developed by <ref type="bibr" target="#b36">Wiseman et al. (2017)</ref> and <ref type="bibr" target="#b26">Puduppully et al. (2019b)</ref> for ROTOWIRE and MLB, respectively. <ref type="table" target="#tab_3">Table 2</ref> shows our main results on the ROTOWIRE and MLB datasets. Our force-copy and force-copy-unk models achieve higher RG precision for both corpora than any other neural models. Specifically, the RG precision of the force-copyunk model is as high as that of GOLD results for the ROTOWIRE dataset. With respect to the loss function, the intervention of the copy mechanism increases in the order of PGNet, RBF-2020 and our models. We observe that RG precision performance increases along with this order. This effect is observed in both datasets.</p><p>The ENT system yields the highest scores in terms of CO on ROTOWIRE. Considering the model architectures, we conjecture that ENT leverages entity-specific representations as it specialized on data-to-text tasks, whereas the other neural models feature vanilla sequence-to-sequence structures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Abstractive Summarization</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experimental Settings</head><p>We use the non-anonymized version of the CNN/DailyMail dataset <ref type="bibr" target="#b11">(Hermann et al., 2015;</ref> provided by Harvard NLP and conform all experimental conditions presented by <ref type="bibr" target="#b30">See et al. (2017)</ref>. More details can be found in the Appendix A.2.</p><p>Since the experimental purpose is not recording the highest ROUGE score but validating the performance of copy mechanism itself, we confine the experiments to encoder-decoder abstractive baselines trained with cross-entropy and do  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method Copy Precision</head><p>Pointer-Generator 47.80% Force-copy 47.81% Force-copy-unk 48.84% not adopt any additional techniques such as coverage or selector mechanisms. For that same reason, we re-implemented the Pointer-Generator network to exquisitely compare the effect of copy mechanism. Therefore, we compare our model against (i) Abstractive model , a pointer-based encoder-decoder model using two softmax layers; (ii) ML+Intra-Attention , an intra-attention model based on the encoder-decoder network; and (iii) Pointer-Generator <ref type="bibr" target="#b30">(See et al., 2017)</ref> and our reimplemented version comprising the modification of the original LSTM encoder to a Transformer encoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Automatic Evaluation Results</head><p>We adopt the ROUGE <ref type="bibr" target="#b19">(Lin, 2004)</ref> and Copy Precision (CP) for the evaluation metric. Given a source article x, gold summary y, and generated text?, CP estimates how well? are matched to y for those words that appear in x. <ref type="table" target="#tab_5">Table 3</ref>, 4 shows our ROUGE and CP evaluation results on the CNN/DailyMail corpus. The results of the baseline and the force-copy model are close, where the force-copy is better at R-1 and the baseline is better at R-2 and R-L. The force-copyunk model outperforms the baseline in all ROUGE criteria. This is consistent with the fact that forcecopy-unk outperforms on CP than the baseline.</p><p>To investigate the copy and generation ratio, we report the novel n-gram scores in <ref type="table" target="#tab_8">Table 5</ref> for the ground-truth and model-generated summaries,   where it represents the level of abstractness <ref type="bibr" target="#b15">(Kry?ci?ski et al., 2018)</ref>. Our force-copy model produces less novel expression than the baseline. As the final generation probability is calculated by mixing the probabilities from the vocabulary and copy distributions, it is obscure to exactly determine that a word is copied or generated. However, we conjecture that force-copy model tends to favor copying since 83% of the target words are copy-candidates in the dataset that are trained to be copied. At inference time, it becomes more skewed owing to unprovided word-by-word supervision. Practically, the average p copy (equivalent to 1 ? p gen ) of the force-copy model for all words of test set is 0.848, whereas that of the Pointer-Generator is 0.773. Contrarily, the force-copy-unk model with an average p copy of 0.018 tends to favor generation. This effect makes the force-copy-unk model produce significantly more novel expressions than the baseline. In fact, <ref type="bibr" target="#b15">Kry?ci?ski et al. (2018)</ref> reported that there exists an inverse correlation between the ROUGE and novelty scores in all model types. Hence, the main benefit of our force-copy-unk model is the increase in novel expression, as the model enhanced the ROUGE score simultaneously.  and numeric values of ROTOWIRE) for our model and baseline depending on copy probability. Percentage numbers represent the ratio of the number of words that p copy &gt; 0.5 and p gen &gt; 0.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Human Evaluation Results</head><p>We conduct a human evaluation to measure the quality of the summaries of each model. The instructions are related to three different aspects, as follows. (i) Readability: How well-written (fluent and grammatical) is the summary? (ii) Factuality: Is the summary factually consistent with the source document? (iii) Abstractness: How does the summary generate novel expressions (not entirely copy the source sentences)?</p><p>As shown in <ref type="table" target="#tab_9">Table 6</ref>, our force-copy model outperforms the others on readability and factuality and force-copy-unk model outperforms the others on abstractness. It seems that more copy operations make the summary be good at readability and factuality levels but results in poor abstractness. We discuss this further in Section 6.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Analysis and Discussion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Effects of Copy Probability on Precision</head><p>To examine the effects of copy probability on generation precision, we investigate the word-level p copy on the ROTOWIRE test set. Based on the IE system, we report the RG precision depending on the copy probability on <ref type="table" target="#tab_11">Table 7</ref>. To couple the IE system and generated summary, we limit this study only to unique and numeric words. Hence, this RG precision is different from that shown in <ref type="table" target="#tab_3">Table 2</ref>.</p><p>We first observe that high copy probability results in high precision in both force-copy model and the baseline, which adheres to the goals of the copy mechanism. Although the precision gap between two models for p gen &gt; 0.5 is only 0.4%p, it increases to 2.1%p for p copy &gt; 0.5. These findings confirm that our force-copy model shows high confidence on copy probability than the baseline. Furthermore, it generates 62.3% of p copy &gt; 0.5 words compared to 57.1% of the baseline, further widening the precision gap. Meanwhile, the forcecopy-unk model does not generate p copy &gt; 0.5  words for the values of the ROTOWIRE test set as the numeric words exist in the target vocabulary.</p><p>Although the force-copy-unk model results in low p copy , attention loss (Eq. 10) prompts the generation to look up the values to copy during training. This copy-like generation corresponds to 6.1%p precision increase compared to the baseline for the words p gen &gt; 0.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Impact of Vocabulary Size on the Force-copy-unk Model</head><p>As copy mechanism of our force-copy-unk model relies on a target vocabulary, we study the effects of vocabulary size on the force-copy-unk model. In Section 5.2, we experimented on the abstractive summarization task on the CNN/DailyMail corpus with 50k vocabularies. The vocabulary coverage is 72.1%, and 1.9% words of the gold test set summaries are OOV. When we reduce the vocabulary size to 25k, the percentages become 53.3% and 3.5%, respectively. In contrast when we increase the vocabulary size to 100k, the percentages become 83.6% and 1.1%, respectively. <ref type="table" target="#tab_13">Table 8</ref> shows a relative comparison in terms of vocabulary size. As reported by <ref type="bibr" target="#b30">See et al. (2017)</ref>, large vocabulary size does not enhance the performance of attention-based sequence-to-sequence models. This is consistent with our observations that large (100k) vocabulary models perform poorly in terms of ROUGE score. Even 50k-vocabulary model falls behind the 25k model on ROUGE score in practice.</p><p>In contrast, we observe novel n-gram expression increases as vocabulary size grows. This disparity is likely associated with less vocabulary size during training, which prompts the model to copy more (see Eq. 12). According to the above results, the force-copy-unk model can control this phenomenon by adjusting the vocabulary size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Tradeoff between Copy and Abstractness</head><p>The human evaluation results presented in Section 5.2 indicate that our force-copy model outperforms other models in terms of readability and factuality. However, the force-copy-unk model exhibits low</p><p>Article no other scoreline could have emphasised how seriously this new arsenal team are taking their prospects of winning major trophies again (. . . ) olivier giroud wants arsenal to continue their premier league winning streak and challenge for the title. (. . . ) arsenal to earn a 1-0 victory over burnley on saturday. (. . . ) PGNet arsenal beat burnley 1-0 in the premier league on saturday. olivier giroud wants arsenal to continue their premier league winning streak and challenge for the title.</p><p>Force-copy no other scoreline could have emphasised how seriously this new arsenal team are taking their prospects of winning major trophies again. olivier giroud wants arsenal to continue their premier league winning streak and challenge for the title. readability and factuality in abstractive summarization tasks. This is a conflicting outcome given that the force-copy-unk model enhanced the RG precision of data-to-text tasks. It likely attributed to the difference of two tasks and datasets. As shown in <ref type="figure">Figure 4</ref>, only values of structured data (i.e., names and records) can be copied in data-to-text task. In contrast, almost every word can be copied in summarization task. This is consistent with <ref type="bibr" target="#b38">Song et al. (2020)</ref>, which found that the proportion of copy-candidate words used for training impacts the model to depend on copying the words, leading to pure extracts and less abstraction. The examples in <ref type="figure">Figure 3</ref> show that the more the model activates copy during training, the more it behaves like an extractive summarization model. Abstraction introduces even more choice of phrasing, negatively affecting factuality and readability compared to the extracted source sentence. In any case, encouraging the model to write more abstractly while retaining the factuality and readability is an interesting subject for the future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>In this work we analyzed the prior copy models in detail. Based on the analysis, we presented a novel copy mechanisms that leverages Pointer-Generator network. We showed that our models conduct more accurate copy than baselines via data-to-text experiment. In addition, our force-copy-unk model outperformed the baselines in both ROUGE score and the novel n-gram score in abstractive summarization task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Experimental Details</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Data-to-text Generation</head><p>We used the official train/valid/test splits of both datasets for our experiments <ref type="bibr">(3,</ref><ref type="bibr">398/727/728 for ROTOWIRE and 22,</ref><ref type="bibr">821/1,</ref><ref type="bibr">739/1,</ref><ref type="bibr">744 for MLB)</ref>. <ref type="bibr">3</ref> For ROTOWIRE, We adopt word embedding size of 300, and feature embedding size of 27. We employed a Transformer encoder based on the hierarchical architecture of RBF-2020. The encoder consists of 3 low-level (unit) layers and 3 highlevel (chunk) layers, where each layer comprises feed-forward layer with 1024-dimensional hidden states. For decoder, We adopt 2-layer LSTM network. The number of parameters of the model in this setting is 14,152,201. We additionally tried a bigger embedding size (600 and 100 for word embedding and feature embedding by max, respectively) and bigger encoder layer size (6 by max), resulting in inconsistent or ineffective outcomes in terms of RG precision. We train the model using Adam with learning rate of 1e-3 and weight decay value of 1e-5. Training took 13 hours for 18k training iterations on a single V100 GPU with a batch size of 96. During inference, we use beam search with beam size of 10 and block n-gram repeat over 10 words. Every experiment has been conducted at least twice, and we report the best results among them.</p><p>For MLB, we adopt word embedding size of 512, and feature embedding size of 32. The encoder consists of 4 low-level layers and 4 high-level layers, where each layer comprises feed-forward layer with 1024-dimensional hidden states. The number of parameters of the model in this setting is 69,562,999. Training took 7 days and 8 hours for 42k training iterations on a single V100 GPU with a batch size of 96. Other settings are the same as those of the ROTOWIRE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Abstractive Summarization</head><p>We used the 287,227/13,368/11,490 train/valid/test splits of CNN/DailyMail datasets for our experiments, which is provided by Harvard NLP. 4 Additionally, we truncate the article to 400 tokens and control the length of the summary to less than 100 <ref type="bibr">3</ref> The complete ROTOWIRE datasets that we used can be obtained from https://github.com/harvardnlp/ boxscore-data, and MLB datasets from https:// github.com/ratishsp/mlb-data-scripts. 4 the official splits of CNN/DailyMail datasets can be obtained from https://github.com/harvardnlp/ sent-summary. tokens for training and less than 120 tokens for inference. According to the original paper, we use 50K vocabulary for the Pointer-Generator, where the author noted the best result. We applied 50K, 25K vocabulary size for the force-copy, force-copyunk respectively, for the best performance. We include more detailed analysis about the effect of vocabulary size change in Section 6.2 of the main paper.</p><p>We use word embedding size of 512 and adopt a 6-layer Transformer encoder and a 3-layer LSTM decoder without any pre-trained word embeddings. Note that we shared the source and target embeddings since the same vocabularies are used in the the source and the target data in this task. The number of parameters of the model in this setting is 51,136,213. We train using Adam with learning rate of 2e-4 and weight decay value of 1e-5. We train our models and baseline models for approximately 40 epochs. Training took 2 days and 15 hours on a single V100 GPU with a batch size of 96. During decoding, we implement beam search with beam size of 10, and block n-gram repeat over 3 words.</p><p>For the evaulation metric, we used official ROUGE-1.5.5 perl script (ROUGE-1.5.5.pl) provided by a python wrapper package. 5 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Examples</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Abstractive Summarization</head><p>Article (truncated) concerns are raised about labour 's policy under shadow education secretary tristram hunt . the heads of some of britain 's best state schools today warn of the dangers of a labour government reversing radical education reforms . in a letter to the daily mail , 80 current and former leaders say there is clear evidence that academy-style freedoms are benefiting a generation of children . but they say labour -and some senior lib dems -appear to be threatening to reimpose state controls . the letter , signed by the heads of good and outstanding autonomous schools , was backed yesterday by david cameron . in it , they claim there is evidence that the most successful education systems benefit from schools with academy-style freedoms . they say such schools are more likely to be ranked ' outstanding ' by ofsted and more likely to improve . ' secondary schools which have converted to academy status outperform other schools -by a margin of almost 10 per cent , ' they wrote . but the heads expressed alarm at comments by ed miliband that labour would reimpose ' a proper local authority framework for all schools ' . senior lib dems were also accused of suggesting they no longer support freedom for acdemies , which are able to control pay , conditions and the curriculum . ' this is not the time to stop something that is working to the benefit of so many children in schools , ' wrote the heads . schools on the letter include torquay boys ' grammar school , ranked in the top 100 for gcse results this year . (...)</p><p>Pointer-Generator 80 current and former leaders say there is clear evidence that academy-style freedoms are benefiting a generation of children . but they say labour and some senior lib dems appear to be threatening to reimpose state controls .</p><p>Force-copy 80 current and former leaders say there is clear evidence academy-style freedoms are benefiting a generation of children . senior lib dems appear to be threatening to reimpose state controls . Force-copy-unk shadow education secretary tristram hunt said labour 's policy is ' not the time to stop something that is working to the benefit of so many children in schools ' but the heads of good and outstanding autonomous schools have converted to academy status outperform other schools -by a margin of almost 10 per cent . </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Article (truncated)</head><p>it 's t20 season on the sub-continent and the world 's best players are about the pad up for the latest edition of the indian premier league , cricket 's most exciting and richest domestic tournament . eight teams will play a total of 60 games over almost seven weeks across 12 venues all round india in a battle to be crowned champions of the tournament 's eighth edition , with the final taking place at eden gardens in kolkata on may 24 . can kolkata knight riders retain their title ? will virat kohli lead the royal challengers bangalore to their first title ? can ms dhoni 's chennai super kings win their third crown ? and who are the players to watch out for ? sportsmail tells you all you need to know in our guide to the 2015 indian premier league as india prepares for the spectacular cricket roadshow . ms dhoni , pictured in the 2011 champions league , is looking to guide chennai super kings to a third title . chennai super kings . the bright yellow jerseys of the super kings are one of the iconic sights of the indian premier league . led by the superstar indian duo of ms dhoni and suresh raina , chennai are the most successful team in ipl history . as well as their back-to-back victories in 2010 and 2011 , csk have been losing finalists three times and never failed to reach the last four . in international players dhoni , raina , ravi ashwin , ravi jadeja and mohit sharma , the super kings have probably the best pool of indian talent in the tournament , which is key given that seven of the starting xi have to be domestic players . the foreign talent is also strong , though , and includes new zealand captain brendon mccullum , south african faf du plessis and west indian all-rounder dwyane bravo . one to watch : there are so many . dhoni needs no introduction , raina is the top scorer in ipl history , but mccullum is one of the most exciting players in world cricket at the moment (...) Pointer-Generator india face india in the indian premier league on may 24. can kolkata knight riders retain their first title of the tournament . can ms dhoni 's chennai super kings win their third crown ? Force-copy eight teams will play a total of 60 games across 12 venues all round india in a battle to be crowned champions of the tournament 's eighth edition , with the final taking place at eden gardens in kolkata on may 24 . sportsmail tells you all you need to know in our guide to the 2015 indian premier league as india prepares for the spectacular cricket roadshow . senior lib dems appear to be threatening to reimpose state controls . Force-copy-unk eight teams will play a total of 60 games over almost seven weeks across 12 venues all round india in a battle to be crowned champions of the tournament 's eighth edition . chennai super kings are one of the iconic sights of the indian premier league . brendon mccullum is one of the most exciting players in world cricket at the moment . </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Architecture of our models. A generation probability p gen at each decoder timestep is trained through explicit loss function. (a) Force-copy model trains to copy if the target word exists in the source context. On the other hands, (b) Force-copy-unk model trains to generate from the vocab distribution unless the target word is an OOV. Note that COVID-19 is an out-of-vocabulary in this example. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>adopted the same method as Wiseman et al. (2017), and Rebuffel et al. (2020) utilized Pointer-Generator Network (PGNet) (See et al., 2017) 1 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Force-copy-unk arsenal beat burnley 1-0 in their premier league clash on saturday. olivier giroud feels the new arsenal team (. . . ) Figure 3: Examples of generated summaries of CNN/DailyMail of each model. (red color denotes copied sentences from the source article)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Comparison of output of abstractive generation models on CNN/DailyMail test sets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Comparison of output of abstractive generation models on CNN/DailyMail test sets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>WIN LOSS PTS FG_PCT RB ...</figDesc><table><row><cell>Hawks</cell><cell>28</cell><cell>20</cell><cell>142</cell><cell></cell><cell>44</cell><cell>64</cell><cell>...</cell></row><row><cell>Knicks</cell><cell>21</cell><cell>28</cell><cell>139</cell><cell></cell><cell>40</cell><cell>63</cell><cell>...</cell></row><row><cell cols="2">PLAYER</cell><cell cols="2">AS RB</cell><cell>PT</cell><cell cols="3">FG FGA ...</cell></row><row><cell cols="2">Paul Millsap</cell><cell>7</cell><cell>19</cell><cell>37</cell><cell>13</cell><cell>29</cell><cell>...</cell></row><row><cell cols="2">Kent Bazemore</cell><cell>0</cell><cell>10</cell><cell>37</cell><cell>13</cell><cell>29</cell><cell>...</cell></row><row><cell>...</cell><cell></cell><cell>...</cell><cell>...</cell><cell>...</cell><cell>...</cell><cell>...</cell><cell>...</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>for data-to-text generation tasks. ROTOWIRE contains professionally written articles summarizing NBA basketball games paired with corresponding game statistics, whereas MLB contains summaries paired with MLB baseball games. Both datasets consist of table-structured data records and relatively long multi-sentence documents (337 and 542 words on average for ROTOWIRE and MLB, respectively). Note that MLB dataset we used to train and test our model slightly differs from Puduppully et al. in two aspects. 2 First, part of the official data has been</figDesc><table><row><cell>RW</cell><cell>RG P%</cell><cell>#</cell><cell>CS P% R%</cell><cell cols="2">CO BLEU</cell></row><row><cell>GOLD</cell><cell cols="4">96.11 17.31 ._100 ._100 ._100</cell><cell>100</cell></row><row><cell>Templ</cell><cell cols="4">99.95 54.15 23.74 72.36 11.68</cell><cell>8.9</cell></row><row><cell>WS-2017</cell><cell cols="4">75.62 16.83 32.80 39.93 15.62</cell><cell>14.2</cell></row><row><cell cols="5">RBF-2020 89.46 21.17 39.47 51.64 18.90</cell><cell>17.5</cell></row><row><cell>ENT</cell><cell cols="4">92.69 30.11 38.64 48.50 20.17</cell><cell>16.2</cell></row><row><cell>PGNet</cell><cell cols="4">87.14 20.98 40.53 48.06 19.80</cell><cell>16.3</cell></row><row><cell cols="5">RBF-2020  ? 89.31 22.07 36.88 49.37 17.87</cell><cell>16.6</cell></row><row><cell>Ours(FC)</cell><cell cols="4">93.27 24.28 34.34 48.85 17.26</cell><cell>15.8</cell></row><row><cell cols="5">Ours(FCU) 95.40 27.37 30.65 48.39 15.14</cell><cell>14.2</cell></row><row><cell>MLB</cell><cell>RG P%</cell><cell>#</cell><cell>CS P% R%</cell><cell cols="2">CO BLEU</cell></row><row><cell>GOLD</cell><cell cols="4">92.07 21.02 98.84 99.91 98.76</cell><cell>100</cell></row><row><cell>Templ</cell><cell cols="4">97.96 59.93 68.46 22.82 10.64</cell><cell>3.8</cell></row><row><cell>PGNet</cell><cell cols="4">79.12 20.94 47.06 47.73 20.07</cell><cell>9.9</cell></row><row><cell cols="5">RBF-2020  ? 81.71 19.53 47.75 47.11 19.37</cell><cell>9.7</cell></row><row><cell>Ours(FC)</cell><cell cols="4">82.50 21.71 46.91 49.01 19.28</cell><cell>10.4</cell></row><row><cell cols="5">Ours(FCU) 84.50 21.05 49.39 50.89 21.16</cell><cell>10.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table /><note>Evaluation on the ROTOWIRE and MLB test sets using RG count (#) and precision (P%), CS preci- sion (P%) and recall (R%), CO in normalized Damerau- Levenshtein distance (DLD%), and BLEU. The bottom section corresponds to our implementation of the corre- sponding methods. ? denotes the duplicate model.modified since Puduppully et al. Second, we modi- fied the script to preprocess the summaries to adjust to our model. Hence, we only compare the models that were trained on our version of the dataset. We compare our model against (i) Template-based gen- erators from Wiseman et al. (2017) for ROTOWIRE and from Puduppully et al. (2019b) for MLB; (ii) WS-2017, a standard encoder-decoder system with copy mechanism (Wiseman et al., 2017); (iii) RBF- 2020, a Transformer architecture with a hierarchi- cal attention mechanism over entities and records within entities (Rebuffel et al., 2020) based on OpenNMT + force copy attn (Klein et al., 2020) (See Table 1); (iv) ENT, the entity-based model of Puduppully et al. (2019b) that creates dynam- ically updated entity-specific representations; (v) PGNet, a Pointer-Generator Network</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell>: Evaluation on CNN/DailyMail test set using</cell></row><row><cell>full-length ROUGE-F1 metric. The Bottom section cor-</cell></row><row><cell>responds to our implementation, and  ? denotes the du-</cell></row><row><cell>plicated model. * marked model used the anonymized</cell></row><row><cell>dataset, so it is not strictly comparable to our results.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table /><note>Evaluation on CNN/DailyMail test set using Copy Precision.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>Ground truth Summary 16.99 56.19 73.98 82.38</figDesc><table><row><cell>Method</cell><cell cols="3">NN-1 NN-2 NN-3 NN-4</cell></row><row><cell>Pointer-Generator</cell><cell>0.25</cell><cell cols="2">5.37 11.45 16.85</cell></row><row><cell>Force-copy</cell><cell>0.03</cell><cell>3.54</cell><cell>8.10 12.36</cell></row><row><cell>Force-copy-unk</cell><cell>0.28</cell><cell cols="2">6.62 13.60 19.54</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5</head><label>5</label><figDesc></figDesc><table><row><cell>:</cell><cell cols="4">Comparison of novel n-gram (NN-)</cell></row><row><cell cols="5">test results for our model and baselines on the</cell></row><row><cell cols="2">CNN/DailyMail test set.</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Comparison</cell><cell></cell><cell cols="2">Preference(%)</cell></row><row><cell></cell><cell>(A vs B)</cell><cell>A</cell><cell>B</cell><cell>Tie</cell></row><row><cell></cell><cell>PGNet vs. FC</cell><cell>6.0</cell><cell>12.0</cell><cell>82.0</cell></row><row><cell>Readability</cell><cell>PGNet vs. FCU</cell><cell>10.0</cell><cell>8.7</cell><cell>81.3</cell></row><row><cell></cell><cell>FC vs. FCU</cell><cell>12.6</cell><cell>6.7</cell><cell>80.7</cell></row><row><cell></cell><cell>PGNet vs. FC</cell><cell>4.7</cell><cell>16.0</cell><cell>79.3</cell></row><row><cell>Factuality</cell><cell>PGNet vs. FCU</cell><cell>20.7</cell><cell>11.3</cell><cell>68.0</cell></row><row><cell></cell><cell>FC vs. FCU</cell><cell>28.7</cell><cell>2.0</cell><cell>69.3</cell></row><row><cell></cell><cell>PGNet vs. FC</cell><cell>27.3</cell><cell>9.3</cell><cell>63.3</cell></row><row><cell>Abstractness</cell><cell>PGNet vs. FCU</cell><cell>12.7</cell><cell>20.7</cell><cell>66.7</cell></row><row><cell></cell><cell>FC vs. FCU</cell><cell>4.0</cell><cell>31.3</cell><cell>64.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6 :</head><label>6</label><figDesc></figDesc><table /><note>Human evaluation results on the 50 ran- domly sampled articles of CNN/DailyMail test sets. Each summary pair is reviewed by 3 human evaluators. Agreement scores by Fleiss' kappa (Fleiss et al., 1971) are 0.19 for Readability, 0.44 for Factuality and 0.29 for Abstractness, respectively.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 7 :</head><label>7</label><figDesc>Comparison of RG Precision (for only unique</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head></head><label></label><figDesc>25k 39.31 17.13 36.25 0.28 6.62 13.60 19.54 50k 38.52 16.62 35.58 0.46 7.96 15.85 22.42 100k 38.11 16.21 35.28 0.73 10.47 20.14 27.84</figDesc><table><row><cell>V</cell><cell>R-1</cell><cell>R-2</cell><cell>R-L NN-1 NN-2 NN-3 NN-4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 8 :</head><label>8</label><figDesc>ROUGE and novel n-gram comparison in terms of vocabulary size for our force-copy-unk model.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head></head><label></label><figDesc>B.1 Data-to-text Generation</figDesc><table><row><cell>TEAM</cell><cell cols="2">HOME WIN</cell><cell>LOSS</cell><cell>PTS</cell><cell cols="4">FG_PCT FG3_PCT RB</cell><cell></cell><cell cols="4">PTS_QTR1 PTS_QTR4 ...</cell></row><row><cell>Boston Celtics</cell><cell>YES</cell><cell>21</cell><cell>19</cell><cell>117</cell><cell>44</cell><cell></cell><cell>47</cell><cell>56</cell><cell></cell><cell>36</cell><cell>21</cell><cell></cell><cell>...</cell></row><row><cell>Pheonix Suns</cell><cell>NO</cell><cell>13</cell><cell>28</cell><cell>103</cell><cell>39</cell><cell></cell><cell>37</cell><cell>47</cell><cell></cell><cell>29</cell><cell>25</cell><cell></cell><cell>...</cell></row><row><cell>PLAYER</cell><cell>AS</cell><cell>RB</cell><cell>PT</cell><cell cols="10">FGM FGA FG3M FG3A FTM FTA BLK STL MIN ...</cell></row><row><cell>Isaiah Thomas</cell><cell>5</cell><cell>2</cell><cell>19</cell><cell>5</cell><cell>11</cell><cell>3</cell><cell>5</cell><cell>6</cell><cell>6</cell><cell>0</cell><cell>0</cell><cell>24</cell><cell>...</cell></row><row><cell>Sonny Weems</cell><cell>5</cell><cell>6</cell><cell>10</cell><cell>4</cell><cell>6</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>2</cell><cell>2</cell><cell>1</cell><cell>13</cell><cell>...</cell></row><row><cell>Jarad Sullinger</cell><cell>2</cell><cell>10</cell><cell>11</cell><cell>4</cell><cell>8</cell><cell>1</cell><cell>2</cell><cell>2</cell><cell>4</cell><cell>0</cell><cell>0</cell><cell>24</cell><cell>...</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">We analyzed released code (if available) to identify which copy mechanism they employed in case it was not mentioned in their paper.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">In table 2, CS and CO scores of the MLB GOLD test set are under 100. This result indicates the difference of dataset between ours and<ref type="bibr" target="#b26">Puduppully et al. (2019b)</ref> </note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">https://github.com/falcondai/pyrouge/</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyung</forename><forename type="middle">Hyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">PALM: Pre-training an autoencod-ing&amp;autoregressive language model for contextconditioned generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenliang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songfang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luo</forename><surname>Si</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.700</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="8681" to="8691" />
		</imprint>
	</monogr>
	<note>Online</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Guided alignment training for topic-aware neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeny</forename><surname>Matusov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shahram</forename><surname>Khadivi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan-Thorsten</forename><surname>Peter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AMTA</title>
		<imprint>
			<biblScope unit="page">121</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Few-shot NLG with pre-trained language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harini</forename><surname>Eavani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinyin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">Yang</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.18</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="183" to="190" />
		</imprint>
	</monogr>
	<note>Online. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Unified language model pre-training for natural language understanding and generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hsiao-Wuen</forename><surname>Hon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Measuring nominal scale agreement among many raters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fleiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Bulletin</title>
		<imprint>
			<biblScope unit="volume">76</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="378" to="382" />
			<date type="published" when="1971" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Survey of the state of the art in natural language generation: Core tasks, applications and evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Albert</forename><surname>Gatt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emiel</forename><surname>Krahmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page" from="65" to="170" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">End-to-end content and plan selection for data-to-text generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Gehrmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Falcon</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henry</forename><surname>Elder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Rush</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W18-6505</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th International Conference on Natural Language Generation</title>
		<meeting>the 11th International Conference on Natural Language Generation<address><addrLine>Tilburg University, The Netherlands</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="46" to="56" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Bottom-up abstractive summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Gehrmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuntian</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Rush</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D18-1443</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4098" to="4109" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Incorporating copying mechanism in sequence-to-sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiatao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">K</forename><surname>Victor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P16-1154</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1631" to="1640" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Pointing the unknown words</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungjin</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramesh</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P16-1014</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="140" to="149" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Teaching machines to read and comprehend</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Moritz Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Kocisky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lasse</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mustafa</forename><surname>Suleyman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">28</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning to select, track, and generate for data-to-text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hayate</forename><surname>Iso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yui</forename><surname>Uehara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tatsuya</forename><surname>Ishigaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroshi</forename><surname>Noji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eiji</forename><surname>Aramaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ichiro</forename><surname>Kobayashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusuke</forename><surname>Miyao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naoaki</forename><surname>Okazaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroya</forename><surname>Takamura</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1202</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2102" to="2113" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The OpenNMT neural machine translation toolkit: 2020 edition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fran?ois</forename><surname>Hernandez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Senellart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th Conference of the Association for Machine Translation in the Americas</title>
		<meeting>the 14th Conference of the Association for Machine Translation in the Americas</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="102" to="109" />
		</imprint>
	</monogr>
	<note>Research Track). Virtual. Association for Machine Translation in the Americas</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Evaluating the factual consistency of abstractive text summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Kryscinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Mccann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.750</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9332" to="9346" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Improving abstraction in text summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Kry?ci?ski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Romain</forename><surname>Paulus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D18-1207</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1808" to="1817" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Neural text generation from structured data with application to the biography domain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R?mi</forename><surname>Lebret</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D16-1128</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1203" to="1213" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">BART: Denoising sequence-to-sequence pretraining for natural language generation, translation, and comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal ; Abdelrahman Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.703</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="7871" to="7880" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning semantic correspondences with less supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP</title>
		<meeting>the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP<address><addrLine>Suntec</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
	<note>Singapore</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">ROUGE: A package for automatic evaluation of summaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Text Summarization Branches Out</title>
		<meeting><address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page" from="74" to="81" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">The natural language decathlon</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Mccann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Shirish Keskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.08730</idno>
	</analytic>
	<monogr>
		<title level="m">Multitask learning as question answering</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Pointer sentinel mixture models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Merity</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.07843</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Language as a latent variable: Discrete generative models for sentence compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yishu</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D16-1031</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="319" to="328" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Abstractive text summarization using sequence-to-sequence RNNs and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramesh</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?aglar</forename><surname>Cicero Dos Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Gul?ehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xiang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/K16-1028</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 20th SIGNLL Conference on Computational Natural Language Learning</title>
		<meeting>The 20th SIGNLL Conference on Computational Natural Language Learning<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="280" to="290" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A deep reinforced model for abstractive summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Romain</forename><surname>Paulus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Data-to-text generation with content selection and planning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ratish</forename><surname>Puduppully</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI conference on artificial intelligence</title>
		<meeting>the AAAI conference on artificial intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="6908" to="6915" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Data-to-text generation with entity modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ratish</forename><surname>Puduppully</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1195</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2023" to="2035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Cl?ment Rebuffel, Laure Soulier, Geoffrey Scoutheeten, and Patrick Gallinari. 2020. A hierarchical model for data-to-text generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cl?ment</forename><surname>Rebuffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Roberti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laure</forename><surname>Soulier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Scoutheeten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rossella</forename><surname>Cancelliere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Gallinari</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.02810</idno>
	</analytic>
	<monogr>
		<title level="m">European Conference on Information Retrieval</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="65" to="80" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Controlling hallucinations at word level in data-to-text generation</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">An architecture for data-to-text systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehud</forename><surname>Reiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eleventh European Workshop on Natural Language Generation (ENLG 07)</title>
		<meeting>the Eleventh European Workshop on Natural Language Generation (ENLG 07)<address><addrLine>Saarbr?cken, Germany. DFKI GmbH</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="97" to="104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A neural attention model for abstractive sentence summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D15-1044</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="379" to="389" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Get to the point: Summarization with pointergenerator networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abigail</forename><surname>See</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P17-1099</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1073" to="1083" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Controlling the amount of verbatim copying in abstractive summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiqiang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingqing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ren</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2020-02-07" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="8902" to="8909" />
		</imprint>
	</monogr>
	<note>The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Mass: Masked sequence to sequence pre-training for language generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaitao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5926" to="5936" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Neural Information Processing Systems</title>
		<meeting>the 27th International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Pointer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meire</forename><surname>Fortunato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Neural Information Processing Systems</title>
		<meeting>the 28th International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="2692" to="2700" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Challenges in data-to-document generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Wiseman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stuart</forename><surname>Shieber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Rush</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D17-1239</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2253" to="2263" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A question type driven and copy loss enhanced frameworkfor answer-agnostic neural question generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiuyu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunfang</forename><surname>Wu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.ngt-1.8</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourth Workshop on Neural Generation and Translation</title>
		<meeting>the Fourth Workshop on Neural Generation and Translation</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="69" to="78" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Self-attention guided copy mechanism for abstractive summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoran</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youzheng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.125</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1355" to="1362" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Pegasus: Pre-training with extracted gap-sentences for abstractive summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingqing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Saleh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="11328" to="11339" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Sequential copying networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingyu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Force-copy The Boston Celtics ( 21 -19 ) defeated the Phoenix Suns ( 13 -28 ) 117 -103 on Wednesday at the US Airways Center in Phoenix . The Celtics were the superior shooters in this game , going 44 percent from the field and 47 percent from the three -point line , while the Suns went 39 percent from the floor and a meager 37 percent from beyond the arc . The Celtics were led by the duo of Isaiah Thomas , who went 5 -for -11 from the field and 3 -for -5 from the three -point line to score 19 points , while also adding five assists . It was his second double -double in a row (. . . ) Force-copy-unk The Boston Celtics ( 21 -19 ) defeated the Phoenix Suns ( 13 -28 ) 117 -103 on Wednesday at the TD Garden in Phoenix . The Celtics got off to a quick start in this one , outscoring the Suns 36 -29 in the first quarter alone . The Celtics were the superior shooters in this one , going 44 percent from the field and 47 percent from the three -point line , while the Suns went just 39 percent from the floor and 37 percent from beyond the arc</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pointer-Generator</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">This was a tight game throughout the first half , as the Celtics outscored the Sixers 36 -6 in the final 12 minutes</title>
		<imprint>
			<biblScope unit="page" from="56" to="62" />
		</imprint>
	</monogr>
	<note>RBF-2020 The Boston Celtics ( 21 -19 ) defeated the Phoenix Suns ( 13 -28 ) 117 -103 on Sunday at the TD Garden in Boston . The Celtics got off to a quick start in this one , out -scoring the Suns 36 -29 in the first quarter . The Celtics were able to out -score the Suns 21 -25 in the fourth quarter to secure the victory in front of their home crowd . The Celtics were the superior shooters in this one , going 44 percent from the field and 47 percent from the three -point line , while the Suns went 39 percent from the floor and a meager 37 percent from beyond the arc. Jared Sullinger was the only other starter to reach double figures in points , as he finished with 11 points ( 4 -8 FG , 1 -2 3Pt , 2 -4 FT ) and 10 rebounds in 24 minutes .(. . . )</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">We highlight text in blue if it accurately reflects a record, in red if it is inconsistent with the records</title>
	</analytic>
	<monogr>
		<title level="m">Comparison of output of data-to-text models on a ROTOWIRE test set</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
		</imprint>
	</monogr>
	<note>in green if it can be inferred indirectly from the records (e.g., &quot;at the TD Garden in Boston&quot; can be inferred from the HOME column in the Boston Celtics row. and in orange if there are no conflicting or supporting records at all (even in the train set). Best viewed in color</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
