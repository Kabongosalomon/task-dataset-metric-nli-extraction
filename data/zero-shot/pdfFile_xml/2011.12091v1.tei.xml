<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SEA: Sentence Encoder Assembly for Video Retrieval by Textual Queries</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE</roleName><forename type="first">Xirong</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangming</forename><surname>Zhou</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaoxi</forename><surname>Xu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaqi</forename><surname>Ji</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yang</surname></persName>
						</author>
						<title level="a" type="main">SEA: Sentence Encoder Assembly for Video Retrieval by Textual Queries</title>
					</analytic>
					<monogr>
						<title level="j" type="main">IEEE TRANSACTIONS ON MULTIMEDIA</title>
						<imprint>
							<biblScope unit="page">1</biblScope>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T20:58+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Ad-hoc video search</term>
					<term>cross-modal representation learning</term>
					<term>sentence encoder assembly</term>
					<term>multiple space learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Retrieving unlabeled videos by textual queries, known as Ad-hoc Video Search (AVS), is a core theme in multimedia data management and retrieval. The success of AVS counts on cross-modal representation learning that encodes both query sentences and videos into common spaces for semantic similarity computation. Inspired by the initial success of previously few works in combining multiple sentence encoders, this paper takes a step forward by developing a new and general method for effectively exploiting diverse sentence encoders. The novelty of the proposed method, which we term Sentence Encoder Assembly (SEA), is two-fold. First, different from prior art that use only a single common space, SEA supports text-video matching in multiple encoder-specific common spaces. Such a property prevents the matching from being dominated by a specific encoder that produces an encoding vector much longer than other encoders. Second, in order to explore complementarities among the individual common spaces, we propose multi-space multi-loss learning. As extensive experiments on four benchmarks (MSR-VTT, TRECVID AVS 2016-2019, TGIF and MSVD)  show, SEA surpasses the state-of-the-art. In addition, SEA is extremely ease to implement. All this makes SEA an appealing solution for AVS and promising for continuously advancing the task by harvesting new sentence encoders.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>table indoors", see <ref type="figure">Fig. 1</ref>. A cross-modal similarity model that effectively computes the semantic relevance of the unlabeled videos with respect to a given query is crucial. Also, due to the ad-hoc nature of the query, the model has to be generalizable to handle novel queries unseen when the model is built.</p><p>For building such a model, both queries and videos have to be encoded into real-valued vectors via cross-modal representation learning. Earlier efforts struggle to detect semantic concepts from the two modalities and use the detected concepts as an intermediate representation <ref type="bibr" target="#b5">[6]</ref>- <ref type="bibr" target="#b10">[11]</ref>. Now, it is becoming increasingly evident that learning cross-modal representations in an end-to-end and concept-free manner is preferred, as manifested via major benchmarks for the AVS task including TRECVID <ref type="bibr" target="#b11">[12]</ref>- <ref type="bibr" target="#b13">[14]</ref> and MSR-VTT <ref type="bibr" target="#b14">[15]</ref>- <ref type="bibr" target="#b16">[17]</ref>.</p><p>We concentrate on end-to-end query representation learning, an essential component for AVS. Typically, the component is composed of a sentence encoder that vectorizes a textual query into a constant-sized vector and a feed-forward neural network that projects the vector into a common latent space <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>. Varied types of sentence encoders have been investigated in the growing literature. The vanilla Bag-of-Words (BoW) model is employed by <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b19">[20]</ref>, with word2vec (w2v) in <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b20">[21]</ref>, GRU / bi-GRU in <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b21">[22]</ref>, NetVLAD in <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b15">[16]</ref>, and BERT in <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b22">[23]</ref>. While the existing works mainly count on a single sentence encoder, the importance of exploiting multiple sentence encoders for addressing ad-hoc queries has been recognized by few recent works <ref type="bibr" target="#b11">[12]</ref>- <ref type="bibr" target="#b13">[14]</ref>. The W2VV++ model proposed by Li et al. <ref type="bibr" target="#b11">[12]</ref> processes a given query by three encoders, i.e., BoW, w2v and GRU, in parallel, and then merges the three encoding results by vector concatenation. Dong et al. <ref type="bibr" target="#b13">[14]</ref> and their follow-up <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b23">[24]</ref> develop multi-level encoding, where specific sentence encoders are selectively used at distinct levels. Again, vector concatenation is used to combine encodings from the multiple levels. Despite their state-of-theart performance, we argue that such a concatenation-based method is suboptimal due to the following two reasons. First, the overall encoding could be easily dominated by a specific encoder that produces an encoding vector much longer than the others. For instance, the size of a BoW vector goes up to ten thousand with ease, while encodings of word2vec, GRU or BERT are more compact, with a typical size of a few hundreds. Second, varied encodings by distinct encoders are fed as a whole into the subsequent feed-forward network, meaning the exploration of complementarities among the encoders is limited to a single common space.</p><p>In this paper we advance AVS with the following contributions: arXiv:2011.12091v1 [cs.CV] 24 Nov 2020 a person holding a tool and cutting something a man and a woman hugging each other a person wearing shorts outdoors coral reef underwater one or more people eating food at a table indoors a sewing machine a woman wearing glasses one or more people hiking a dog playing outdoors <ref type="figure">Fig. 1</ref>. Top-5 videos per query sentence retrieved from V3C1 <ref type="bibr" target="#b0">[1]</ref>, a large collection of one million unlabeled web video clips, by the proposed SEA model. Queries are from the TRECVID Ad-hoc Video Search benchmarks <ref type="bibr" target="#b1">[2]</ref>- <ref type="bibr" target="#b4">[5]</ref>. <ref type="bibr">?</ref> We propose Sentence Encoder Assembly (SEA), a new and general method for effectively exploiting varied sentence encoders. SEA bypasses the issues of vector concatenation by learning common spaces per encoder. ? To derive a cross-modal similarity from multiple common spaces, we propose multi-space multi-loss learning, an effective mechanism to explore complementarities among the individual common spaces. ? Our solution surpasses the state-of-the-art on four benchmarks, i.e., MSR-VTT, TRECVID AVS 2016-2019, TGIF and MSVD. Moreover, our solution is easy to implement. With its generality, effectiveness and simplicity, SEA has opened up a promising avenue for harnessing novel sentence encoders for continuous performance improvement of AVS. Code and data are available at https://github.com/li-xirong/sea. II. RELATED WORK Earlier methods for AVS follow a concept-based approach <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b24">[25]</ref>- <ref type="bibr" target="#b28">[29]</ref>, with both queries and videos represented in a pre-defined concept space. An intrinsic drawback of the concept-based approach is that concepts in use have to be specified in advance, typically according to their occurrence in training data. Such a hand-crafted common space is suboptimal for cross-modal similarity computation <ref type="bibr" target="#b11">[12]</ref>. In order to overcome the drawback, end-to-end learning of concept-free and cross-modal representations has been the mainstream <ref type="bibr" target="#b13">[14]</ref>- <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b30">[31]</ref>. As we target at query representation learning, in what follows we discuss recent progress in this direction.</p><p>The classical Bag-of-Words (BoW) representation is commonly used for its simplicity. In <ref type="bibr" target="#b19">[20]</ref>, for instance, a query is first encoded as a BoW vector, and then projected into a latent space through a fully connected layer. However, the BoW encoder has two intrinsic issues. First, it cannot handle semantic relatedness between words. In a BoW feature space, the distance of "a beagle is running" to "a dog is running" is the same as to "a person is running", even though the former pair is visually and semantically more close. Second, it fully ignores word order. To resolve the first issue, Dong et al. <ref type="bibr" target="#b31">[32]</ref> employ a pre-trained word2vec model to encode each word in a given query into a dense vector and consequently obtain the query vector by mean pooling over the word-level vectors.</p><p>Later in Liu et al. <ref type="bibr" target="#b15">[16]</ref>, NetVLAD <ref type="bibr" target="#b32">[33]</ref> is adopted to exploit second-order statistics of the word-level vectors. To overcome the limit of BoW in sequential modeling, varied forms of sequence-aware deep neural networks are investigated. For instance, GRU is used in <ref type="bibr" target="#b14">[15]</ref>, bi-LSTM in <ref type="bibr" target="#b33">[34]</ref>, relational GCN in <ref type="bibr" target="#b29">[30]</ref>, and more recently BERT in <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b22">[23]</ref>.</p><p>While more advanced sentence encoders are being actively exploited for AVS, it appears to us that no specific encoder is ready to rule them all. We attribute this to the variety and complexity of AVS queries, which can be short phrases or detailed descriptions of multiple-object actions in specific scenes. For the former case, a BoW encoder will suffice, while the latter case requires a complicated encoder to effectively capture fine-grained information. In the context of image/video caption retrieval, Dong et al. <ref type="bibr" target="#b31">[32]</ref> make an initial endeavor to combine multiple encoders including BoW, w2v and GRU for query representation. In particular, they concatenate the output of the individual encoders into a lengthy vector. Based on <ref type="bibr" target="#b31">[32]</ref>, Li et al. <ref type="bibr" target="#b11">[12]</ref> develop W2VV++, the winning entry for the TRECVID AVS 2018 evaluation <ref type="bibr" target="#b34">[35]</ref>. Contemporarily, Dong et al. <ref type="bibr" target="#b13">[14]</ref> propose the Dual Encoding network, wherein three encoders, i.e., BoW, bi-GRU and 1-d CNN, are employed to build a multi-level query representation. Follow-ups of <ref type="bibr" target="#b13">[14]</ref>, e.g., <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b30">[31]</ref> also leverage multiple encoders, and again merge the output of the encoders in advance to cross-modal representation learning. By contrast, our multispace learning mechanism makes our model more flexible to harness the complementarities between distinct sentence encoders. Consequently, even with common 2D-CNN features as video representation, our proposed model compares favorably against the state-of-the-art.</p><p>Note that at a high level, the idea of sentence encoder assembly is similar to the conventional ensemble methods <ref type="bibr" target="#b35">[36]</ref>. However, ensemble learning is a very general idea, typically studied in the context of a classification task. Therefore, a gap naturally exists between the idea itself and putting it to work on AVS. This paper is an initial attempt to bridge the gap.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. PROPOSED METHOD A. Problem Formalization</head><p>We formalize an ad-hoc video search process as follows. We denote a specific video clip as v and a large collection  Proposed Sentence Encoder Assembly (SEA) method for exploiting multiple sentence encoders {e t,1 , . . . , e t,k } for computing cross-modal similarities between a given query sentence s and a specific unlabeled video v. Instead of concatenating the output of the individual sentence encoders as in previous works <ref type="bibr" target="#b11">[12]</ref>- <ref type="bibr" target="#b13">[14]</ref>, our SEA model simultaneously learns k common spaces for the k encoders. Rather than minimizing a single loss computed based on the combined similarity k i=1 cms i (s, v), SEA is trained to minimize a combine loss k i=1 loss i (s). Such a multi-space multi-loss learning mechanism is novel and crucial for AVS, meanwhile easy to implement. of n unlabeled video clips as V = {v 1 , . . . , v n }. For an adhoc query in the form of a sentence s, let cms(s, v) be a cross-modal similarity function that measures the semantic relevance between the query and a specific video. Accordingly, the search process boils down to sorting V in descending order in terms of cms(s, v) and returning the top ranked items for the given query. The computation of cms(s, v) requires proper embeddings of both s and v into a common cross-modal space. While visual CNNs are prerequisites for video embedding, sentence encoders are required for query embedding. Let e t be a specific sentence encoder, which encodes the given query into a d t -dimensional real-valued vector, i.e., e t (s) ? R dt . Having k distinct sentence encoders {e t,1 , . . . , e t k } shall give us k vectors of varied dimensions {d t,1 , . . . , d t,k }. We aim for a model that effectively exploit the multiple sentence encoders for computing cms(s, v).</p><p>Next, we describe in brief sentence encoders investigated in this work in Section III-B, followed by the proposed sentence encoder assembly (SEA) model in Section III-C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Sentence Encoders in Use</head><p>We consider five present-day sentence encoders, i.e., Bagof-Words (BoW), word2vec (w2v), GRU, bi-GRU and BERT. Among them, the first two are unigram, while the others are sequential models. Their main properties are summarized in <ref type="table" target="#tab_1">Table I.</ref> 1) BoW. As a classical text encoder, BoW simply quantizes a given sentence s of l words with respect to a pre-specified  <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b31">[32]</ref> w2v 1.7 millions 500 pre-trained 1 and fixed <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b31">[32]</ref> GRU m+ 1,024 trained from scratch <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b31">[32]</ref> bi-GRU m+ 2,048 trained from scratch <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref> BERT 30,000 768 pre-trained 2 and fixed <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b22">[23]</ref> vocabulary of m words. Let c(s, j) be a function that counts the occurrence of the j-th word in the sentence. Accordingly, we have the BoW encoding e BoW (s) as </p><p>Note that an AVS query is relatively short, often containing less than 10 words. Meanwhile, the vocabulary size is much larger, with a typical order of 10 4 . As a consequence, e BoW (s) is a long and sparse vector.</p><p>2) w2v. The w2v model <ref type="bibr" target="#b36">[37]</ref> learns to produce word-level dense and semantic vectors by training a two-layer neural network on a large text corpus, with the goal to reconstruct linguistic contexts of words in the training text. As computing the reconstruction loss requires no extra manual annotation, w2v encodes millions of words with ease. We adopt a 500dimensional w2v model 1 from <ref type="bibr" target="#b31">[32]</ref>. We also tried alternatives such as GloVe <ref type="bibr" target="#b37">[38]</ref>, and found it less effective in our preliminary experiments. Let w2v(s[i]) be a lookup function that returns the embedding vector for the i-th word of s, we obtain w2v based sentence encoding by mean pooling, i.e., e w2v (s) :</p><formula xml:id="formula_1">= 1 l l i=1 w2v(s[i]).<label>(2)</label></formula><p>3) GRU. The Gated Recurrent Unit (GRU) network <ref type="bibr" target="#b38">[39]</ref> models the sequential information within a sentence by iteratively generating a sequence of recurrent hidden state vectors { h 1 , . . . , h l }. In particular, the hidden state vector at timestep i, h i , is jointly determined by the word embedding of the current word s[i] and h i?1 , the hidden state vector at the previous time-step. Similar to LSTM <ref type="bibr" target="#b39">[40]</ref>, the GRU network effectively prevents the vanishing gradient problem by introducing a gating mechanism to modulate the flow of information inside the unit. Meanwhile, as GRU has no separate memory cell, it has a simplified architecture and thus with less parameters to be trained. Following <ref type="bibr" target="#b11">[12]</ref>, we obtain the GRU-based sentence encoding by mean pooling over the hidden vector sequence, i.e.,</p><formula xml:id="formula_2">e gru (s) := 1 l l i=1 h i .<label>(3)</label></formula><p>4) bi-GRU. The bi-directional GRU (bi-GRU) network extends the forward GRU by including a backward GRU that encodes the sequence in a reverse order. Given { h 1 , . . . , h l } as hidden state vectors of the backward GRU, our bi-GRU based sentence encoding is obtained by</p><formula xml:id="formula_3">e bigru (s) := 1 l l i=1 h i ? h i ,<label>(4)</label></formula><p>where ? denotes vector concatenation. Note that given forward and backward hidden vectors of the same size, e bigru provides a richer representation than e gru at the cost of doubled parameters. Hence, we shall use either e gru or e bigru , but not both.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5)</head><p>BERT. The BERT model, built by stacking a number of L bi-directional Transformer blocks <ref type="bibr" target="#b40">[41]</ref>, generates word embeddings for a given sentence by progressively passing encodings through the multiple blocks. A Transformer block consists of a self-attention network and a feed-forward network <ref type="bibr" target="#b41">[42]</ref>. The self-attention network accepts encodings of individual tokens from the previous Transformer block, weighs their importance to each other by a self-attention mechanism, and accordingly generates new encodings. These encodings are then fed in parallel into the feed-forward network to produce the output encodings of this block. In this work, we adopt the base version of BERT containing L = 12 blocks, which has been pre-trained on English Wikipedia and book corpora for masked language modeling and next sentence prediction 2 . We obtain the BERT-based sentence encoding by mean pooling as</p><formula xml:id="formula_4">e bert (s) = 1 l l i=1 token-emb(s[i], L ? 1),<label>(5)</label></formula><p>where token-emb(s[i], L ? 1) denotes the embedding of the ith word produced by the second-last block. Note that we tried max pooling or using the embedding of the first / last token, and found these alternatives less effective than mean pooling. With the sentence encoders introduced, we proceed to describe how to effectively combine them in an end-to-end framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Sentence Encoder Assembly</head><p>We propose to combine k distinct sentence encoders {e t,i |i = 1, . . . , k} in a generic multi-space multi-loss learning framework.</p><p>Multiple common spaces. Our framework consists of k cross-modal matching subnetworks, each corresponding to a specific sentence encoder and learning its own common space. Each subnetwork, indexed by i, consists of two fully connected (FC) layers, one on the text side to transform e t,i (s) into a d c,i -dimensional vector, and the other on the video side that transforms the video feature vector f (v) into another d c,idimensional vector. Consequently, the sentence-video semantic relevance, denoted as cms i (s, v), is computed as the cosine similarity between the two embedding:</p><formula xml:id="formula_5">cms i (s, v) := cosine-sim( F C t,i (e t,i (s)) text embedding , F C v,i (f (v)) video embedding ),<label>(6)</label></formula><p>where F C t,i and F C v,i indicate the two FC layers, each followed by a tanh function to increase their learning capacity. We choose the cosine similarity as it is a widely used similarity metric for cross-modal matching <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b42">[43]</ref>, <ref type="bibr" target="#b43">[44]</ref>. We also tried a Euclidean distance based similarity, which is however less effective <ref type="bibr" target="#b2">3</ref> .</p><p>By simply averaging the similarities computed in the individual common spaces, we have the overall cross-modal similarity as</p><formula xml:id="formula_6">cms(s, q) := 1 k k i=1 cms i (s, v).<label>(7)</label></formula><p>Note that we do not go for more complicated alternatives, e.g., weighing the individual similarities by self-attention mechanisms. Rather, we opt for this simple combination strategy, not only for preventing the risk of over-fitting. Such a strategy also encourages the individual common spaces to be good enough to be combined, as they are set to be equally important.</p><p>Multi-loss learning. We develop our loss function based on the improved triplet ranking loss (ITRL) by Faghri et al. <ref type="bibr" target="#b42">[43]</ref>.</p><p>While originally proposed for image-text matching, ITRL is now found to be effective for text-video matching <ref type="bibr" target="#b11">[12]</ref>- <ref type="bibr" target="#b15">[16]</ref>. Unlike the classical triplet ranking loss that selects negative training examples by random, ITRL considers the negative that violates the ranking constraint the most (within a mini-batch) and thus deemed to be the most informative for improving the model being trained. Given a training sentence s with v + as a video relevant w.r.t s and v ? as irrelevant, we express ITRL as</p><formula xml:id="formula_7">v ? * = argmax v ? ?batch (cms(s, v ? ) ? cms(s, v + )) IT RL(s) = max(0, ? + cms(s, v ? * ) ? cms(s, v + )),<label>(8)</label></formula><p>where ? is a positive hyper-parameter concerning the margin.</p><p>We argue that such a single loss is suboptimal for multispace learning. Given a specific mini-batch, hard negative examples selected in terms of the combined similarity are not necessarily the most effective for learning the individual common spaces. Therefore, we choose to compute IT RL i (s) per space, and accordingly learn to minimize their combined loss, i.e.,</p><formula xml:id="formula_8">k i=1 IT RL i (s).<label>(9)</label></formula><p>In a similar spirit to similarity combination, we again treat all the sub losses equally. As exemplified in <ref type="figure" target="#fig_2">Fig. 3</ref>, the combined loss lets the model be exposed to more diverse hard negatives. We empirically find that compared to the single loss, the combined loss provides around 30% extra hard negatives per training epoch.</p><p>commentary on a horse race on a grass track the guy in red leads the race an asian man in black and white is smiling and waving a female giving a nail art tutorial a man is touching a woman's neck Using the combined loss allows the model to be exposed to more diverse hard negatives in a given batch. To sum up, the multi-space strategy provides a more flexible mechanism to exploit complementarities among the distinct sentence encoders. Meanwhile, given a specific mini-batch during training, the multi-loss strategy allows each common space to select its own hard negative example. More flexibility in encoder ensemble and more effectiveness for training together contributes to the superior performance of the proposed SEA method against the state-of-the-art.</p><formula xml:id="formula_9">(b) (a) (c) (d)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EVALUATION</head><p>We first conduct experiments on two major benchmarks, MSR-VTT <ref type="bibr" target="#b44">[45]</ref> and TRECVID AVS <ref type="bibr" target="#b1">[2]</ref>. While originally developed for video captioning, MSR-VTT has been adopted by recent works for text-based video retrieval <ref type="bibr" target="#b13">[14]</ref>- <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b33">[34]</ref>. TRECVID AVS is a leading benchmark for ad-hoc video search at large-scale since 2016 [2]- <ref type="bibr" target="#b4">[5]</ref>. The two benchmarks have their own characteristics. As shown in <ref type="table" target="#tab_1">Table II</ref>, while MSR-VTT has a relatively small amount of 2,990 test videos, it has over 59k query sentences. As for TRECVID, it has a much larger number of test videos, over 335k in the 2016 / 2017 / 2018 editions and over one million in the 2019 edition. Hence, a joint evaluation on the two benchmarks provides a comprehensive assessment of the state-of-the-art. In addition, we report performance on TGIF <ref type="bibr" target="#b45">[46]</ref> and MSVD <ref type="bibr" target="#b46">[47]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Experimental Setup</head><p>We first describe experimental setups unique to MSR-VTT and TRECVID, and then introduce common implementations.</p><p>1) Setup for MSR-VTT: We follow the official data split, which divides MSR-VTT into three disjoint subsets used for training, validation and test, respectively. Note that in <ref type="bibr" target="#b33">[34]</ref> and its follow-ups <ref type="bibr" target="#b15">[16]</ref>- <ref type="bibr" target="#b17">[18]</ref>, a smaller test set of 1,000 videos randomly sampled from the full test set is used, which we refer to as test-1k.</p><p>Performance metrics. Following the previous works, we report R@k, k = 1, 5, 10, the percentage of test queries that have at least one relevant video covered in the top k returned items, and Median rank (Med r), the median rank of the first relevant video in the search results. Mean Average Precision (mAP) is also reported to assess the overall ranking quality.</p><p>2) Setup for TRECVID: We evaluate on the TRECVID AVS testbed from the last four years. The test video collection for TV16 / TV17 / TV18 is IACC.3 <ref type="bibr" target="#b1">[2]</ref>, containing 335,944 web video clips. The test collection for TV19 is V3C1 <ref type="bibr" target="#b47">[48]</ref>, which contains 1,082,649 web video clips, with even more diverse content, no predominant characteristics and low self-similarity <ref type="bibr" target="#b0">[1]</ref>. As no training data is provided by the organizers, we adopt the setup of the winning entry of TV18 <ref type="bibr" target="#b34">[35]</ref>, using MSR-VTT and TGIF <ref type="bibr" target="#b45">[46]</ref> for training and the development set of the TV16 video-to-text matching task <ref type="bibr" target="#b1">[2]</ref> for validation.</p><p>Performance metric. The official metric, i.e., inferred average precision (infAP) <ref type="bibr" target="#b48">[49]</ref>, is used.</p><p>3) Common Implementations: We use public feature data 4 , where each video is represented by a 4,096-d feature vector, obtained by using two pre-trained CNNs, i.e., ResNet-152 and ResNeXt-101, to extract 2,048-d features from video frames. Frame-level features are concatenated and aggregated to videolevel features by mean pooling. We refer to <ref type="bibr" target="#b11">[12]</ref> for details.</p><p>Training. The margin parameter ? in the loss is set to 0.2 according to <ref type="bibr" target="#b42">[43]</ref>. The dimensionality of all the common spaces d c,i is set to 2,048, which achieves a good balance between model performance and model complexity. In fact, our model is highly robust to the choice of the common space dimensionality, see the Appendix. We perform SGD based training, with a mini-batch size of 128 and RMSProp as the optimizer. The learning rate is initially set to 10 ?4 , decayed by a factor of 0.99 per epoch. Following <ref type="bibr" target="#b49">[50]</ref>, we half the learning rate if the validation performance does not increase in three consecutive epochs. Early stop occurs when no validation performance increase is achieved in ten consecutive epochs. For each model with specific configurations of sentence encoders, we repeat training three times and pick the version that maximizes the validation performance. All experiments were done with PyTorch (1.2.0) <ref type="bibr" target="#b50">[51]</ref> on an Nvidia GEFORCE GTX 1080Ti GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Experiment 1. Which Sentence Encoders to Use?</head><p>We compare with the state-of-the-art W2VV++ <ref type="bibr" target="#b11">[12]</ref>, which combines multiple sentence encoders by concatenating their output into a long feature vector and then embeds the concatenated vector into a common space by an FC layer. While originally developed for automated search, W2VV++ has been used with success by Kratochv?l et al. <ref type="bibr" target="#b51">[52]</ref> and Loko? et al. <ref type="bibr" target="#b22">[23]</ref> in the Video Browser Showdown, a leading benchmark for interactive video retrieval <ref type="bibr" target="#b52">[53]</ref>, <ref type="bibr" target="#b53">[54]</ref>. For a fair comparison, we use author-provided source code 5 with the same setup as described in Section IV-A3.</p><p>The performance of W2VV++ and the proposed SEA model is presented in <ref type="table" target="#tab_1">Table III</ref>. For all configurations of sentence encoders, SEA consistently outperforms its W2VV++ counterpart. Specifically, on MSR-VTT our model obtains a relative improvement ranging from 3.6% to 7.3% in terms of mAP. While on TRECVID, the relative improvement w.r.t the overall performance ranges from 7.3% to 25.7%. The advantage becomes even more clear when four sentence encoders are combined, see the last two rows in <ref type="table" target="#tab_1">Table III</ref>. These results justify the effectiveness of the multi-space mechanism.</p><p>As more sentence encoders are included, we observe different phenomenons on the two benchmarks. For MSR-VTT, adding sequential encoders is helpful. Compared to SEA ({BoW, w2v}), SEA ({BoW, w2v, GRU}) improves mAP from 21.3 to 22.1, while substituting BERT for GRU obtains higher mAP of 23.0. The peak performance, mAP of 23.3, is reached by SEA ({BoW, w2v, GRU, BERT}) and SEA ({BoW, w2v, bi-GRU, BERT}). By contrast, the inclusion of GRU and BERT has a negative impact on TRECVID. Compared to SEA ({BoW, w2v}) which has the best overall infAP of 17.1, adding GRU results in an overall infAP of 16.8, while adding BERT results in a lower value of 16.3. By analyzing the query sentences of the two benchmarks, we find that an MSR-VTT sentence tend to be longer, containing 9.3 words on average, while the corresponding number of TRECVID is 7.1. We attribute this difference to the fact that MSR-VTT was originally meant for video captioning, so its sentences are more detailed. This is furthered confirmed by part-of-speech statistics, where we find that an MSR-VTT query has 3.3 nouns, 1.8 verbs and 0.6 adjective on average, while a TRECVID query has 2.7 nouns, 1.0 verb and 0.4 adjective. TRECVID queries are more keyword-oriented, e.g., "a newspaper", "people shopping", and "a blond female indoors". Hence, for answering keywordoriented queries, SEA ({BoW, w2v}) is most suited, while a full setup, e.g., SEA ({BoW, w2v, bi-GRU, BERT}), is preferred for addressing description-oriented queries.</p><p>We further analyze the complementarity between the distinct sentence encoders by inspecting how they behave when used individually. To that end, we train a cross-modal matching network per encoder on MSR-VTT. To obtain an intuitive understanding of what each network has learned as its common space, we perform sentence-to-sentence retrieval, using all the 200k captions in MSR-VTT as a sentence pool. As w2v and BERT are pre-trained on large-scale corpora with a large vocabulary, they better handle subjects of low occurrence ('beagle') or zero occurrence ('rottweiler') in the training data, see <ref type="figure">Fig. 4</ref>. Interestingly, for the query 'a is running on lawn' where we have intentionally remove the subject, BoW returns sentences describing person running, while some of the top-ranked sentences by BERT are still related to dogs. Moreover, we perform a per-query comparison between the matching networks for video retrieval. Among all the 59,800 test queries, the network with BoW is better than the others for 15.2% of the test queries, while the numbers corresponding to w2v, GRU, bi-GRU and BERT are 14.5%, 12.9%, 14.0% and 19.6%, respectively. The results clearly show the complementarity between the encoders.  <ref type="figure">Fig. 4</ref>. Visualization of sentence-to-sentence retrieval results. Given a query sentence, e.g., "a dog is running on lawn", we retrieve top-20 sentences from MSR-VTT (which has 200k sentences in total), using common spaces learned by cross-modal matching networks with respect to specific sentence encoders. A yellow grid indicates sentences related to dogs. For the last query, we intentionally remove the subject. Encoders pre-trained on large-scale corpora, i.e., w2v and BERT, better handle subjects of low occurrence ('beagle') or zero occurrence ('rottweiler') in the training data. As the output size of the individual sentence encoders ranges from 500 (e w2v ) up to over 10k (e BoW ), one might naturally challenge the deficiency of the concatenation operation used by W2VV++. For a more comprehensive comparison, we further implement two more alternatives:</p><p>? Transformed W2VV++. We modify W2VV++ by adding an FC layer after each encoder to transform all encodings into 2,048-d vectors in advance to concatenation. This allows the size of the concatenated vector to be invariant with respect to the encodings. The new variant is also end-to-end trained.</p><p>? Model averaging. The cross-modal subnetworks w.r.t the individual encoders are trained separately. Cross-modal similarities computed by the subnetworks are equally combined. <ref type="table" target="#tab_1">Table IV</ref> shows the results of these alternatives on TRECVID. The lower performance of Transformed W2VV++ suggests that adjusting the encodings causes loss in the original information produced by the individual encoders. Model averaging outperforms W2VV++, again suggesting the benefit of using multiple common spaces against a single common space. The result that model averaging is less effective than SEA verifies the necessity of learning multiple common spaces in a unified framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Experiment 3. Combined loss versus Single loss</head><p>We have qualitatively illustrate the benefit of the combined loss against the single loss in <ref type="figure" target="#fig_2">Fig. 3</ref>. Now we provide more quantitative evidence. As <ref type="table" target="#tab_1">Table IV</ref> shows, SEA trained with the single loss does not outperform W2VV++. We can also observe similar results from the learning curves in <ref type="figure" target="#fig_4">Fig. 5</ref>. For both W2VV++ and SEA, we use {BoW, w2v, GRU} as their sentence encoders. Note that the number of epochs each model takes is not pre-specified. Due to the early stop strategy, the number of training epochs actually took varies among the models. As shown in <ref type="figure" target="#fig_4">Fig. 5</ref>, SEA with the single loss (the blue curve) quickly converged to a suboptimal state. These results proof the importance of the combined loss for training the multi-space network.  Validation is performed after each epoch. The number of epochs a model takes is not pre-specified. After a model reaches its peak performance, as indicated by yellow markers, early stopping occurs in ten epochs. So the number of training epochs actually took varies among the models. Both W2VV++ and SEA use {BoW, w2v, GRU} as their sentence encoders. For training the multispace network, the combined loss is preferred over the single loss.</p><p>E. Comparison to the State-of-the-Art 1) On MSR-VTT: We compare with 11 recent models as follows, which have been evaluated on the test-1k set <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b33">[34]</ref>, the full set <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b54">[55]</ref> or both <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b42">[43]</ref>, <ref type="bibr" target="#b43">[44]</ref>. We highlight their choices of sentence encoders:</p><p>? JSFusion <ref type="bibr" target="#b33">[34]</ref>: Use bi-LSTM as its sentence encoder.</p><p>? VSE++ <ref type="bibr" target="#b42">[43]</ref>: Use GRU as its sentence encoder.  ? Dual Encoding <ref type="bibr" target="#b13">[14]</ref>: Hierarchical encoding that combines BoW, bi-GRU and 1D-CNN.</p><p>? W2VV++ <ref type="bibr" target="#b11">[12]</ref>: Concatenate encodings of BoW, w2v and GRU ? CE <ref type="bibr" target="#b15">[16]</ref>: Use NetVLAD as its sentence encoder.</p><p>? TCE <ref type="bibr" target="#b43">[44]</ref>: Use a latent semantic tree for query representation learning.</p><p>? HGR <ref type="bibr" target="#b29">[30]</ref>: Encode by hierarchical semantic graph including three levels of events, actions, entities and relationships across levels.</p><p>? CF-GNN <ref type="bibr" target="#b30">[31]</ref>: Graph neural network based search result reranking, with Dual Encoding as its sentence encoder.</p><p>? UniViLM <ref type="bibr" target="#b16">[17]</ref>: BERT as its sentence encoder. Note that all the models were trained on the official training set of MSR-VTT except for <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>, where the authors pretrained their model on 100 million narrated video clips and then fine-tuned on MSR-VTT.</p><p>Results. <ref type="table" target="#tab_5">Table V</ref> shows the performance of the distinct models on the MSR-VTT full test set and test-1k. For the ease of comparison, the performance of the baselines is directly cited from the original papers except for W2VV++ 5 , VSE++ <ref type="bibr" target="#b5">6</ref> and Dual Encoding 7 , which we have re-trained using their public code with the same video feature as used in this work. Among the baselines, CE is the best on test-1k, while W2VV++ is the best on the full test set. On both sets, the proposed SEA model is the top performer. Notice that the good performance of CE is obtained by representing videos with many features including appearance, scene, motion, face, OCR, speech and audio. Given the simplicity of our video feature, the advantage of the new model is clearly justified.</p><p>2) On TRECVID AVS 2016-2019: We compare with the top-3 finalist of the TRECVID AVS evaluation each year,  <ref type="figure">Fig. 6</ref>. Performance of varied models in the TRECVID 2019 (TV19) AVS task, which is to find amidst a set of one million unlabeled videos those relevant with respect to 30 test queries. For result analysis, each query is preceded by the TRECVID-specified query type, e.g., [person + object] or [person + action + location], and ID. The queries are sorted in descending order in terms of their infAP scores by SEA ({BoW,w2v,bi-GRU}). As the key difference of the varied models is whether multiple sentence encoders are used and how they are combined, the leading performance of the SEA series verifies the effectiveness of the proposed method, namely multi-space network plus multi-loss training. which naturally reflects the state-of-the-art. We again compare with W2VV++, VSE++ and Dual Encoding, re-training them using the TRECVID setup as described in Section IV-A2. We also include VideoStory <ref type="bibr" target="#b19">[20]</ref> which uses BoW as its sentence encoder, and Extended Dual Encoding <ref type="bibr" target="#b23">[24]</ref>, a very recent work which makes use of more than one encodings of the visual and textual content and two distinct attention mechanisms.</p><p>Results. The performance on the TRECVID test data is shown in <ref type="table" target="#tab_1">Table VI</ref>. The proposed SEA model surpasses the prior art. While the Extended Dual Encoding network <ref type="bibr" target="#b23">[24]</ref> appears to be on par with the SEA models, <ref type="bibr" target="#b23">[24]</ref> has factually used the ground truth of the test set, which shall be unavailable in real applications, to select the best performing models. By contrast, our model selection is performed exclusively based on an independent validation set (see <ref type="table" target="#tab_1">Table II</ref>), and thus more practical.</p><p>Late average fusion of Dual Encoding and SEA boosts the performance further, see the last row. Note that previous top-performing submissions boost their performance by late (average) fusion of a handful of models <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b34">[35]</ref> or nearly hundred models <ref type="bibr" target="#b59">[60]</ref>. In this context, the capability of SEA to advance the state-of-the-art with a single model is a big advantage for AVS at large-scale. <ref type="figure">Fig. 6</ref> shows how each model performs on the individual queries from the TV19 task, by searching over the one-million V3C1 collection. Each query is preceded by a TRECVIDspecified query type that reflects the query complexity to some extent <ref type="bibr" target="#b4">[5]</ref>. A query comprised of person, action, object and location tends to be more complex and thus more difficult to address than a query of person. While such a pattern can largely be observed from <ref type="figure">Fig. 6</ref>, exceptions are not uncommon. Consider query #639, for instance. Although the top-ranked videos show small airplane flying, they are mostly "external view", rather than "inside view" as required. Such a geometric property has not been effectively captured by the current sentence encoders that are fully data-driven. We consider the SEA model, with its flexibility to harvest new encoders, promising to attack the deficiency.</p><p>3) On TGIF and MSVD: For both datasets, we follow the data partition specified by their developers. That is, training / validation / test is 78,799 / 10,705 / 11,351 for TGIF and 1,200 / 100 / 670 for MSVD. All captions are used. The state-of-the- art following such a setting is HGR <ref type="bibr" target="#b29">[30]</ref> and CF-GNN <ref type="bibr" target="#b30">[31]</ref> on TGIF and CF-GNN on MSVD. Therefore, we compare with these two models. Dual Encoding and W2VV++ are also included.</p><p>Results. As shown in <ref type="table" target="#tab_1">Table VII and Table VIII</ref>, our SEA model is again the best. Given that the amount of the training data in MSVD is substantially less than that of TGIF, the peak performance of SEA on MSVD is reached with less sentence encoders.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Efficiency Analysis</head><p>We report in <ref type="table" target="#tab_1">Table IX</ref> the amount of trainable parameters, training time and inference time of the SEA models with varied setups on MSR-VTT, TGIF and MSVD. Two state-ofthe-art methods, i.e., Dual Encoding <ref type="bibr" target="#b13">[14]</ref> and W2VV++ <ref type="bibr" target="#b11">[12]</ref>, are included as well. For a fair comparison, all models use the same size of 2,048 for their common spaces. For all models, the computational cost of video embedding is excluded from the inference time as this step is done once in an offline mode. Concerning the training time, Dual Encoding is slower than W2VV++ and SEA on MSR-VTT and MSVD, while faster on TGIF. In particular, SEA ({BoW,w2v,biGRU,BERT}) requires the longest training time of 4.9 hours on TGIF, as we find that the model needs more training epochs to trigger the early stop mechanism on this dataset.</p><p>For each model, its inference time to answer a given query consists of two parts: 1) query embedding that projects the query into a common space (for Dual Encoding and W2VV++) or multiple common spaces (for the SEA models), and 2) ranking that performs cross-modal matching between the query and all videos in a test set and sorting the videos accordingly. The main computational overhead is due to the online inference of the BERT encoder. Still, query embedding can be done within 19 milliseconds. As the cross-modal matching is executed in parallel on GPU, the ranking is extremely fast, costing around one millisecond. The inference time per query is around 20 milliseconds. Hence, our model is sufficiently fast to support real-time interactive video retrieval.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSIONS</head><p>We have described a method for exploiting diverse sentence encoders for ad-hoc video search. Our experiments show the importance of building a query representation learning network that supports text-video matching in multiple encoderspecific common spaces. Nonetheless, the multi-space network architecture alone is inadequate. In order to effectively utilize complementaries among the individual common spaces, the network has to be end-to-end trained with a combined loss. On four benchmark datasets including MSR-VTT, TRECVID AVS 2016-2019, TGIF and MSVD, our proposed SEA model with multi-space multi-loss learning surpasses the prior art.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX</head><p>The impact of the common space dimensionality. As shown in <ref type="table">Table X</ref>, except for using a relatively small value of 256, the dimensionality of the common space has a marginal impact on the performance of the proposed method. We recommend to use 2,048 to strikes a proper balance between performance and model complexity.</p><p>The role of pretraining corpus. As noted in Section III-B, we use w2v and BERT which were pre-trained on Flickr tags <ref type="bibr" target="#b31">[32]</ref> and web documents <ref type="bibr" target="#b40">[41]</ref>, respectively. To investigate if better performance can be obtained by pre-training the two encoders on the same corpus, we have re-trained w2v on Wikipedia dumps and book corpus as used for BERT. We do not try the opposite direction, i.e., re-training BERT on the Flickr data, since Flickr tags are not natural-language text and thus unsuited for training BERT. As shown in <ref type="table" target="#tab_1">Table XI</ref>, the Flickr version of w2v is slightly better. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Proposed Sentence Encoder Assembly (SEA) method for exploiting multiple sentence encoders {e t,1 , . . . , e t,k } for computing cross-modal similarities between a given query sentence s and a specific unlabeled video v. Instead of concatenating the output of the individual sentence encoders as in previous works [12]-[14], our SEA model simultaneously learns k common spaces for the k encoders. Rather than minimizing a single loss computed based on the combined similarity k i=1 cms i (s, v), SEA is trained to minimize a combine loss k i=1 loss i (s). Such a multi-space multi-loss learning mechanism is novel and crucial for AVS, meanwhile easy to implement.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>e</head><label></label><figDesc>BoW (s) := (c(s, 1), . . . , c(s, m)).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Examples of hard negative videos automatically selected for specific sentences during training. The first column (a) is selection based on the combined similarity in a single common space. The other columns indicate selections made based on individual similarities w.r.t (b) e BoW , (c) e w2v , and (d) egru within the proposed multi-space and multi-loss framework.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Learning curves of distinct models in the TRECVID experiments.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>?</head><label></label><figDesc>Mithun et al. [55]: Use GRU as its sentence encoder. ? Miech et al. [18]: Use a 1D-CNN as its sentence encoder.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I</head><label>I</label><figDesc>FIVE SENTENCE ENCODERS USED IN THIS PAPER. THE SPECIFIC VALUE OF THE VOCABULARY SIZE m IS DATASET-DEPENDENT, WHICH IS 7,676 FOR MSR-VTT, 3,981 FOR TGIF AND 2,917 FOR MSVD. THE NOTATION m+ MEANS THE VOCABULARY OF GRU AND BI-GRU IS SLIGHTLY BIGGER THAN m DUE TO THE INCLUSION OF STOPWORDS AND SPECIAL TOKENS FOR SEQUENTIAL MODELING.</figDesc><table><row><cell cols="3">Encoder Vocabulary Dim. d t Training</cell><cell>Prior work</cell></row><row><cell>BoW</cell><cell>m</cell><cell>m Not trainable</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE II DATASETS</head><label>II</label><figDesc>USED IN OUR EVALUATION. FOR ALL EXPERIMENTS WE TRAIN MODELS ON THE SPECIFIED TRAINING SET AND USE THE CORRESPONDING VALIDATION SET FOR MODEL SELECTION.</figDesc><table><row><cell>Data split</cell><cell>Data sources</cell><cell>Video clips</cell><cell>Frames</cell><cell>Queries</cell></row><row><cell cols="2">MSR-VTT experiments:</cell><cell></cell><cell></cell><cell></cell></row><row><cell>train set</cell><cell></cell><cell>6,513</cell><cell>197,648</cell><cell>-</cell></row><row><cell>val. set test-full</cell><cell>MSR-VTT [45]</cell><cell>497 2,990</cell><cell>15,347 92,467</cell><cell>9,940 59,800</cell></row><row><cell>test-1k [34]</cell><cell></cell><cell>1,000</cell><cell>30,932</cell><cell>1,000</cell></row><row><cell cols="2">TRECVID experiments:</cell><cell></cell><cell></cell><cell></cell></row><row><cell>train set</cell><cell>MSR-VTT [45] TGIF [46]</cell><cell>10,000 100,855</cell><cell>305,462 1,045,268</cell><cell>--</cell></row><row><cell>val. set</cell><cell>TV16-VTT-dev [2]</cell><cell>200</cell><cell>5,941</cell><cell>200</cell></row><row><cell>test set for TV16/17/18</cell><cell>IACC.3 [2]</cell><cell>335,944</cell><cell>3,845,221</cell><cell>90</cell></row><row><cell>test set for TV19</cell><cell>V3C1 [1]</cell><cell>1,082,649</cell><cell>7,839,450</cell><cell>30</cell></row><row><cell cols="2">TGIF experiments:</cell><cell></cell><cell></cell><cell></cell></row><row><cell>train set</cell><cell></cell><cell>78,799</cell><cell>818,140</cell><cell>-</cell></row><row><cell>val. set</cell><cell>TGIF [46]</cell><cell>10,705</cell><cell>110,252</cell><cell>10,828</cell></row><row><cell>test set</cell><cell></cell><cell>11,351</cell><cell>116,876</cell><cell>34,074</cell></row><row><cell cols="2">MSVD experiments:</cell><cell></cell><cell></cell><cell></cell></row><row><cell>train set</cell><cell></cell><cell>1,200</cell><cell>23,313</cell><cell>-</cell></row><row><cell>val. set</cell><cell>MSVD [47]</cell><cell>100</cell><cell>2,415</cell><cell>4,291</cell></row><row><cell>test set</cell><cell></cell><cell>670</cell><cell>15,429</cell><cell>27,767</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE III JOINT</head><label>III</label><figDesc>EVALUATION OF SENTENCE ENCODERS AND THEIR ASSEMBLY MODELS, i.e., W2VV++ [12] AND THE PROPOSED SEA, ON MSR-VTT AND TRECVID. NUMBERS ARE SHOWN IN PERCENTAGES, WITH BEST SCORES SHOWN IN BOLD FONT. FOR A GIVEN SETUP OF SENTENCE ENCODERS, RELATIVE IMPROVEMENT OF SEA OVER ITS W2VV++ COUNTERPART IS GIVEN IN PARENTHESES. SEA IS CONSISTENTLY BETTER.</figDesc><table><row><cell>Sentence encoders</cell><cell>Model</cell><cell cols="4">MSR-VTT (the full test set) R@1 R@5 R@10 Med r mAP</cell><cell cols="4">TRECVID (metric: infAP) TV16 TV17 TV18 TV19 MEAN</cell></row><row><cell>{BoW, w2v}</cell><cell>W2VV++ SEA</cell><cell>10.9 11.6</cell><cell>29.1 30.6</cell><cell>39.9 41.6</cell><cell>19 20.2 17 21.3 (?5.4%)</cell><cell>14.4 15.7</cell><cell>21.8 23.4</cell><cell>11.1 12.8</cell><cell>14.3 15.4 16.6 17.1 (?11.2%)</cell></row><row><cell>{BoW, w2v, GRU}</cell><cell>W2VV++ SEA</cell><cell>11.1 12.2</cell><cell>29.6 31.9</cell><cell>40.5 43.1</cell><cell>18 20.6 15 22.1 (?7.3%)</cell><cell>16.2 15.0</cell><cell>22.3 23.4</cell><cell>10.1 12.2</cell><cell>13.9 15.6 16.6 16.8 (?7.5%)</cell></row><row><cell>{BoW, w2v, bi-GRU}</cell><cell>W2VV++ SEA</cell><cell>11.3 12.4</cell><cell>29.9 32.1</cell><cell>40.6 43.3</cell><cell>18 20.8 15 22.3 (?7.2%)</cell><cell>16.1 16.4</cell><cell>21.7 22.8</cell><cell>10.4 12.5</cell><cell>13.5 15.4 16.7 17.1 (?10.9%)</cell></row><row><cell>{BoW, w2v, BERT}</cell><cell>W2VV++ SEA</cell><cell>12.3 12.8</cell><cell>31.8 33.1</cell><cell>43.0 44.6</cell><cell>15 22.2 14 23.0 (?3.6%)</cell><cell>15.1 15.3</cell><cell>22.5 22.8</cell><cell>10.2 12.1</cell><cell>12.8 15.2 14.8 16.3 (?7.3%)</cell></row><row><cell>{BoW, w2v, GRU, BERT}</cell><cell>W2VV++ SEA</cell><cell>12.1 13.0</cell><cell>31.7 33.6</cell><cell>42.7 44.9</cell><cell>16 22.0 14 23.3 (?5.9%)</cell><cell>14.3 16.0</cell><cell>19.3 23.1</cell><cell>9.3 12.1</cell><cell>10.1 13.3 15.4 16.7 (?25.7%)</cell></row><row><cell>{BoW, w2v, bi-GRU, BERT}</cell><cell>W2VV++ SEA</cell><cell>12.0 13.1</cell><cell>31.3 33.4</cell><cell>42.3 45.0</cell><cell>16 21.8 14 23.3 (?6.9%)</cell><cell>15.8 15.9</cell><cell>20.6 22.9</cell><cell>9.0 11.7</cell><cell>10.5 14.0 15.5 16.5 (?18.1%)</cell></row><row><cell cols="2">a dog is running on lawn</cell><cell></cell><cell></cell><cell></cell><cell cols="5">Word frequency in the top-20 ranked sentences</cell></row><row><cell>BoW</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">dog:20 running:19 forest:4 field:4 around:3</cell></row><row><cell>w2v</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">running:19 dog:18 field:5 forest:4 around:3</cell></row><row><cell>GRU</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">dog:20 running:14 field:11 runs:6 around:4</cell></row><row><cell>biGRU</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">dog:19 running:12 field:11 around:5 runs:4</cell></row><row><cell>BERT</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">dog:20 running:7 runs:6 field:5 street:5</cell></row><row><cell cols="2">a beagle is running on lawn</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>BoW</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">running:10 lawn:10 woman:6 girl:3 grass:3</cell></row><row><cell>w2v</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">running:17 dog:14 field:7 around:4 dogs:3</cell></row><row><cell>GRU</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">dog:20 field:11 runs:10 running:9 around:4</cell></row><row><cell>biGRU</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">dog:20 running:9 field:8 runs:4 playing:3</cell></row><row><cell>BERT</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">dog:11 running:9 runs:8 around:6 street:6</cell></row><row><cell cols="2">a rottweiler is running on lawn</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>BoW</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">running:17 person:8 lawn:5 grass:4 girl:3</cell></row><row><cell>w2v</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">running:18 dog:16 field:8 around:4 dogs:3</cell></row><row><cell>GRU</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">running:16 man:7 field:5 back:3 ground:3</cell></row><row><cell>biGRU</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">running:12 field:9 man:6 grass:6 runs:5</cell></row><row><cell>BERT</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">dog:11 grass:9 around:6 kitten:4 playing:4</cell></row><row><cell>a is running on lawn</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>BoW</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">running:17 person:8 lawn:5 grass:4 girl:3</cell></row><row><cell>w2v</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">running:20 man:7 grass:7 field:4 kid:2</cell></row><row><cell>GRU</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">running:16 field:8 man:8 back:3 people:2</cell></row><row><cell>biGRU</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">running:15 field:12 man:10 ball:4 grass:4</cell></row><row><cell>BERT</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">running:15 field:12 dog:8 man:3 yard:3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE IV EVALUATING</head><label>IV</label><figDesc>DIFFERENT METHODS FOR FUSING MULTIPLE SENTENCE ENCODERS, i.e., {BOW, W2V, GRU}. THE MOST EFFECTIVE METHOD IS TO TRAIN THE SEA MODEL WITH THE COMBINED LOSS.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>C. Experiment 2. Other Alternatives for Encoder Assembly?</cell></row><row><cell>Fusion method</cell><cell>TV16</cell><cell>TV17</cell><cell>TV18</cell><cell>TV19</cell><cell>MEAN</cell></row><row><cell>W2VV++</cell><cell>16.2</cell><cell>22.3</cell><cell>10.1</cell><cell>13.9</cell><cell>15.6</cell></row><row><cell>Transformed W2VV++</cell><cell>13.9</cell><cell>20.2</cell><cell>10.2</cell><cell>13.5</cell><cell>14.5</cell></row><row><cell>Model averaging</cell><cell>14.9</cell><cell>21.9</cell><cell>11.6</cell><cell>15.4</cell><cell>16.0</cell></row><row><cell>SEA single loss</cell><cell>14.7</cell><cell>21.8</cell><cell>11.2</cell><cell>14.7</cell><cell>15.6</cell></row><row><cell>SEA combined loss</cell><cell>15.0</cell><cell>23.4</cell><cell>12.2</cell><cell>16.6</cell><cell>16.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE V</head><label>V</label><figDesc>STATE-OF-THE-ART ON MSR-VTT FOR TEXT-BASED VIDEO RETRIEVAL. BEST SCORES FROM THE CITED PAPERS ARE USED, WHERE APPLICABLE. ON BOTH THE test-1k SET AND THE full TEST SET, OUR PROPOSED SEA({BOW, W2V,BI-GRU, BERT}) IS THE BEST.</figDesc><table><row><cell>Test set</cell><cell>Model</cell><cell cols="3">R@1 R@5 R@10</cell><cell>Med r</cell><cell>mAP</cell></row><row><cell></cell><cell>JSFusion [34]</cell><cell>10.2</cell><cell>31.2</cell><cell>43.2</cell><cell>13</cell><cell>n.a.</cell></row><row><cell>1k</cell><cell>VSE++ [43]</cell><cell>15.2</cell><cell>37.7</cell><cell>50.1</cell><cell>10</cell><cell>26.0</cell></row><row><cell>[34]</cell><cell>TCE [44]</cell><cell>16.1</cell><cell>38.0</cell><cell>51.5</cell><cell>10</cell><cell>n.a.</cell></row><row><cell></cell><cell>Miech et al. [18]</cell><cell>14.9</cell><cell>40.2</cell><cell>52.8</cell><cell>9</cell><cell>n.a.</cell></row><row><cell></cell><cell>UniViLM [17]</cell><cell>15.4</cell><cell>39.5</cell><cell>52.3</cell><cell>9</cell><cell>n.a.</cell></row><row><cell></cell><cell>Dual Encoding [14]</cell><cell>18.8</cell><cell>44.4</cell><cell>57.2</cell><cell>7</cell><cell>31.6</cell></row><row><cell></cell><cell>W2VV++ [12]</cell><cell>18.9</cell><cell>45.3</cell><cell>57.5</cell><cell>8</cell><cell>31.6</cell></row><row><cell></cell><cell>CE [16]</cell><cell>20.9</cell><cell>48.8</cell><cell>62.4</cell><cell>6</cell><cell>n.a.</cell></row><row><cell></cell><cell>SEA</cell><cell>23.8</cell><cell>50.3</cell><cell>63.8</cell><cell>5</cell><cell>36.6</cell></row><row><cell></cell><cell>Mithun et al. [55]</cell><cell>7.3</cell><cell>21.7</cell><cell>30.9</cell><cell>34</cell><cell>n.a.</cell></row><row><cell></cell><cell>TCE</cell><cell>7.7</cell><cell>22.5</cell><cell>32.1</cell><cell>30</cell><cell>n.a.</cell></row><row><cell>Full</cell><cell>CF-GNN [31]</cell><cell>8.0</cell><cell>23.2</cell><cell>32.6</cell><cell>31</cell><cell>16.0</cell></row><row><cell></cell><cell>VSE++</cell><cell>8.7</cell><cell>24.3</cell><cell>34.1</cell><cell>28</cell><cell>16.9</cell></row><row><cell></cell><cell>HGR [30]</cell><cell>9.2</cell><cell>26.2</cell><cell>36.5</cell><cell>24</cell><cell>n.a</cell></row><row><cell></cell><cell>CE</cell><cell>10.0</cell><cell>29.0</cell><cell>41.2</cell><cell>16</cell><cell>n.a.</cell></row><row><cell></cell><cell>Dual Encoding</cell><cell>11.1</cell><cell>29.4</cell><cell>40.3</cell><cell>19</cell><cell>20.5</cell></row><row><cell></cell><cell>W2VV++</cell><cell>11.1</cell><cell>29.6</cell><cell>40.5</cell><cell>18</cell><cell>20.6</cell></row><row><cell></cell><cell>SEA</cell><cell>13.1</cell><cell>33.4</cell><cell>45.0</cell><cell>14</cell><cell>23.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE VI</head><label>VI</label><figDesc>STATE-OF-THE-ART ON TRECVID AVS. SEA SURPASSES THE PRIOR ART. LATE AVERAGE FUSION OF TWO SEA MODELS OR SEA ({BOW,W2V}) AND DUAL ENCODING BOOSTS THE PERFORMANCE.</figDesc><table><row><cell>Model</cell><cell>TV16</cell><cell>TV17</cell><cell>TV18</cell><cell>TV19</cell><cell>MEAN</cell></row><row><cell>Top-3 TRECVID finalists</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Rank 1</cell><cell cols="4">05.4 [25] 20.6 [26] 12.1 [35] 16.3 [13]</cell><cell>n.a.</cell></row><row><cell>Rank 2</cell><cell cols="4">05.1 [27] 15.9 [28] 08.7 [56] 16.0 [57]</cell><cell>n.a.</cell></row><row><cell>Rank 3</cell><cell cols="4">04.0 [58] 12.0 [29] 08.2 [59] 12.3 [60]</cell><cell>n.a.</cell></row><row><cell>VideoStory [20], [61]</cell><cell>08.7</cell><cell>15.0</cell><cell>n.a.</cell><cell>n.a.</cell><cell>n.a.</cell></row><row><cell>VSE++ [43]</cell><cell>13.5</cell><cell>16.3</cell><cell>10.6</cell><cell>09.8</cell><cell>12.6</cell></row><row><cell>W2VV++ [12]</cell><cell>16.2</cell><cell>22.3</cell><cell>10.1</cell><cell>13.9</cell><cell>15.6</cell></row><row><cell>Dual Encoding [14]</cell><cell>16.5</cell><cell>22.8</cell><cell>11.7</cell><cell>15.2</cell><cell>16.6</cell></row><row><cell>Extended Dual Encoding [24]</cell><cell>15.9</cell><cell>24.4</cell><cell>12.6</cell><cell>n.a.</cell><cell>n.a.</cell></row><row><cell>SEA({BoW,w2v})</cell><cell>15.7</cell><cell>23.4</cell><cell>12.8</cell><cell>16.6</cell><cell>17.1</cell></row><row><cell cols="2">SEA({BoW,w2v,bi-GRU}) 16.4</cell><cell>22.8</cell><cell>12.5</cell><cell>16.7</cell><cell>17.1</cell></row><row><cell>SEA({BoW,w2v}) + SEA({BoW,w2v,bi-GRU})</cell><cell>16.6</cell><cell>23.5</cell><cell>12.6</cell><cell>17.2</cell><cell>17.5</cell></row><row><cell>SEA({BoW,w2v}) + Dual Encoding</cell><cell>17.3</cell><cell>25.0</cell><cell>12.8</cell><cell>17.1</cell><cell>18.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE VII</head><label>VII</label><figDesc>STATE-OF-THE-ART ON TGIF FOR TEXT-BASED VIDEO RETRIEVAL.</figDesc><table><row><cell>Model</cell><cell cols="3">R@1 R@5 R@10</cell><cell>Med r</cell><cell>mAP</cell></row><row><cell>HGR [30]</cell><cell>4.5</cell><cell>12.4</cell><cell>17.8</cell><cell>160</cell><cell>n.a.</cell></row><row><cell>Dual Encoding [14]</cell><cell>9.1</cell><cell>21.3</cell><cell>28.6</cell><cell>50</cell><cell>15.7</cell></row><row><cell>W2VV++ [12]</cell><cell>9.4</cell><cell>22.3</cell><cell>29.8</cell><cell>48</cell><cell>16.2</cell></row><row><cell>CF-GNN [31]</cell><cell>10.2</cell><cell>23.0</cell><cell>30.7</cell><cell>44</cell><cell>n.a.</cell></row><row><cell>SEA ({BoW,w2v,GRU})</cell><cell>10.2</cell><cell>23.6</cell><cell>31.3</cell><cell>41</cell><cell>17.2</cell></row><row><cell>SEA ({BoW,w2v,BERT})</cell><cell>10.7</cell><cell>24.4</cell><cell>31.9</cell><cell>37</cell><cell>17.9</cell></row><row><cell>SEA ({BoW,w2v,GRU,BERT})</cell><cell>11.1</cell><cell>25.2</cell><cell>32.7</cell><cell>36</cell><cell>18.4</cell></row><row><cell>SEA ({BoW,w2v,bi-GRU,BERT})</cell><cell>11.1</cell><cell>25.2</cell><cell>32.8</cell><cell>35</cell><cell>18.5</cell></row><row><cell cols="2">TABLE VIII</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">STATE-OF-THE-ART ON MSVD FOR TEXT-BASED VIDEO RETRIEVAL.</cell></row><row><cell>Model</cell><cell cols="3">R@1 R@5 R@10</cell><cell>Med r</cell><cell>mAP</cell></row><row><cell>Dual Encoding [14]</cell><cell>20.3</cell><cell>46.8</cell><cell>59.7</cell><cell>6</cell><cell>32.9</cell></row><row><cell>CF-GNN [31]</cell><cell>22.8</cell><cell>50.9</cell><cell>63.6</cell><cell>6</cell><cell>n.a.</cell></row><row><cell>W2VV++ [12]</cell><cell>22.4</cell><cell>51.6</cell><cell>64.8</cell><cell>5</cell><cell>36.1</cell></row><row><cell>SEA ({BoW,w2v,GRU})</cell><cell>23.2</cell><cell>52.9</cell><cell>66.2</cell><cell>5</cell><cell>37.2</cell></row><row><cell>SEA ({BoW,w2v,BERT})</cell><cell>24.6</cell><cell>55.0</cell><cell>67.9</cell><cell>4</cell><cell>38.7</cell></row><row><cell>SEA ({BoW,w2v,GRU,BERT})</cell><cell>24.4</cell><cell>54.1</cell><cell>67.6</cell><cell>5</cell><cell>38.3</cell></row><row><cell>SEA ({BoW,w2v,bi-GRU,BERT})</cell><cell>23.9</cell><cell>53.9</cell><cell>67.3</cell><cell>5</cell><cell>38.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE IX TRAINING</head><label>IX</label><figDesc>AND INFERENCE TIME OF DUAL ENCODING, W2VV++ AND OUR PROPOSED SEA MODELS ON DISTINCT DATASETS. THE DIMENSIONALITY OF THE COMMON SPACE FOR ALL MODELS IS 2,048. EXPERIMENTS ARE DONE WITH PYTORCH (1.2.0) ON AN NVIDIA 1080TI GPU. THE INFLUENCE OF THE COMMON SPACE DIMENSIONALITY d c,i ON THE MODEL PERFORMANCE. WE EVALUATE SEA ({BOW,W2V,GRU}) ON MSR-VTT.TABLE XI PERFORMANCE OF SEA WITH W2V PRE-TRAINED ON DISTINCT CORPUS. WE EVALUATE SEA ({BOW,W2V,BI-GRU,BERT}) ON MSR-VTT.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">MSR-VTT</cell><cell></cell><cell></cell><cell></cell><cell cols="2">TGIF</cell><cell></cell><cell></cell><cell>MSVD</cell></row><row><cell>Model</cell><cell></cell><cell cols="2">#parameters (million)</cell><cell>training time (hr)</cell><cell cols="2">query embedding (ms)</cell><cell>ranking (ms)</cell><cell>#parameters (million)</cell><cell>training time (hr)</cell><cell>query embedding (ms)</cell><cell>ranking (ms)</cell><cell>#parameters (million)</cell><cell>training time (hr)</cell><cell>query embedding (ms)</cell><cell>ranking (ms)</cell></row><row><cell>Dual Encoding [14]</cell><cell></cell><cell></cell><cell>95.9</cell><cell>2.9</cell><cell></cell><cell>14.8</cell><cell>0.5</cell><cell>86.5</cell><cell>2.2</cell><cell>16.6</cell><cell>0.9</cell><cell>83.7</cell><cell>0.7</cell><cell>13.7</cell><cell>0.4</cell></row><row><cell>W2VV++ [12]</cell><cell></cell><cell></cell><cell>35.8</cell><cell>1.2</cell><cell></cell><cell>2.1</cell><cell>0.5</cell><cell>26.4</cell><cell>2.4</cell><cell>2.2</cell><cell>0.9</cell><cell>23.7</cell><cell>0.3</cell><cell>1.8</cell><cell>0.4</cell></row><row><cell>SEA(BoW,w2v)</cell><cell></cell><cell></cell><cell>33.5</cell><cell>0.9</cell><cell></cell><cell>1.2</cell><cell>0.6</cell><cell>26.0</cell><cell>3.1</cell><cell>1.0</cell><cell>1.0</cell><cell>23.8</cell><cell>0.1</cell><cell>0.9</cell><cell>0.6</cell></row><row><cell>SEA(BoW,w2v,GRU)</cell><cell></cell><cell></cell><cell>52.6</cell><cell>2.7</cell><cell></cell><cell>2.4</cell><cell>0.8</cell><cell>43.2</cell><cell>4.4</cell><cell>2.4</cell><cell>1.1</cell><cell>40.5</cell><cell>0.2</cell><cell>2.2</cell><cell>0.8</cell></row><row><cell>SEA(BoW,w2v,bi-GRU)</cell><cell></cell><cell></cell><cell>59.4</cell><cell>2.4</cell><cell></cell><cell>2.8</cell><cell>0.8</cell><cell>50.0</cell><cell>4.9</cell><cell>3.0</cell><cell>1.1</cell><cell>47.3</cell><cell>0.4</cell><cell>2.6</cell><cell>0.8</cell></row><row><cell>SEA(BoW,w2v,BERT)</cell><cell></cell><cell></cell><cell>43.5</cell><cell>1.0</cell><cell></cell><cell>15.8</cell><cell>0.8</cell><cell>35.9</cell><cell>3.6</cell><cell>15.9</cell><cell>1.1</cell><cell>33.8</cell><cell>0.2</cell><cell>15.5</cell><cell>0.8</cell></row><row><cell cols="2">SEA(BoW,w2v,GRU,BERT)</cell><cell></cell><cell>62.6</cell><cell>2.0</cell><cell></cell><cell>17.4</cell><cell>0.9</cell><cell>53.2</cell><cell>4.4</cell><cell>17.1</cell><cell>1.2</cell><cell>50.4</cell><cell>0.3</cell><cell>17.1</cell><cell>1.0</cell></row><row><cell cols="2">SEA(BoW,w2v,bi-GRU,BERT)</cell><cell></cell><cell>69.4</cell><cell>2.5</cell><cell></cell><cell>18.4</cell><cell>0.9</cell><cell>59.9</cell><cell>4.9</cell><cell>18.2</cell><cell>1.2</cell><cell>57.2</cell><cell>0.3</cell><cell>17.4</cell><cell>0.9</cell></row><row><cell></cell><cell></cell><cell cols="3">TABLE X</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="7">d c,i R@1 R@5 R@10 Med r mAP</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>256</cell><cell>11.0</cell><cell>29.5</cell><cell cols="2">40.4</cell><cell>18</cell><cell>20.4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>512</cell><cell>12.0</cell><cell>31.5</cell><cell cols="2">42.7</cell><cell>16</cell><cell>21.9</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>1,024</cell><cell>12.1</cell><cell>31.6</cell><cell cols="2">42.9</cell><cell>15</cell><cell>22.0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>2,048</cell><cell>12.2</cell><cell>31.9</cell><cell cols="2">43.1</cell><cell>15</cell><cell>22.1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>4,096</cell><cell>12.2</cell><cell>31.8</cell><cell cols="2">43.1</cell><cell>15</cell><cell>22.1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>8,192</cell><cell>12.1</cell><cell>31.4</cell><cell cols="2">42.6</cell><cell>16</cell><cell>21.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/danieljf24/w2vv Note that while<ref type="bibr" target="#b31">[32]</ref> performs imageto-text matching experiments on Flickr30k, its w2v model was trained on English tags of 30 million Flickr images, using the skip-gram algorithm.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://github.com/google-research/bert 3 For SEA ({BoW,w2v}) trained with the Euclidean distance based similarity, its infAP scores on TV16/17/18/19 are 12.8/18.9/11.8/10.4, clearly lower than the cosine similarity counterpart (15.7/23.4/12.8/16.6).</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">https://github.com/li-xirong/avs 5 https://github.com/li-xirong/w2vvpp</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">https://github.com/fartashf/vsepp 7 https://github.com/danieljf24/dual encoding</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">V3C1 dataset: An evaluation of content characteristics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Berns</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Rossetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schoeffmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Beecks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Awad</surname></persName>
		</author>
		<editor>ICMR</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Trecvid 2016: Evaluating video search, video event detection, localization, and hyperlinking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Awad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fiscus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Joy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Smeaton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kraaij</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Qu?not</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Eskevich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Aly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ordelman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Huet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Larson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">TRECVID Workshop</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Trecvid 2017: Evaluating ad-hoc and instance video search, events detection, video captioning and hyperlinking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Awad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Butt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fiscus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Joy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Delgado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Smeaton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J F</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kraaij</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Qu?not</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Eskevich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ordelman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Huet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">TRECVID Workshop</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Trecvid 2018: Benchmarking video activity detection, video captioning and matching, video storytelling linking and video search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Awad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Butt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Curtis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fiscus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Godil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Joy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Delgado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">F</forename><surname>Smeaton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kraaij</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Qu?not</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Magalhaes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Semedo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Blasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">TRECVID Workshop</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Trecvid 2019: An evaluation campaign to benchmark video activity detection, video captioning and matching, and video search &amp; retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Awad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Butt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Curtis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fiscus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Godil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Delgado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">F</forename><surname>Smeaton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kraaij</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Qu?not</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">TRECVID Workshop</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Large-scale concept ontology for multimedia</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Naphade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Te?i?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kennedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hauptmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Curtis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Multimedia</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="86" to="91" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Can high-level concepts fill the semantic gap in video retrieval? a case study with broadcast news</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hauptmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Christel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wactlar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">T-MM</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="958" to="966" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Concept-based video retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">G M</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Worring</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations and Trends in Information Retrieval</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="215" to="322" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Event detection with zero example: Select the right and suppress the wrong concepts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Boer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-W</forename><surname>Ngo</surname></persName>
		</author>
		<editor>ICMR</editor>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Query and keyframe representations for ad-hoc video search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Markatopoulou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Galanopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mezaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Patras</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Video indexing, search, detection, and description with focus on trecvid</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Awad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-D</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-W</forename><surname>Ngo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V.-T</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Qu?not</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satoh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICMR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">W2VV++: Fully deep learning for ad-hoc video search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACMMM</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Hybrid sequence encoder for text based video retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Mao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">TRECVID Workshop</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Dual encoding for zero-example video retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning joint embedding with multimodal cues for cross-modal video-text retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mithun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Metze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Roy-Chowdhury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ICMR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Use what you have: Video retrieval using representations from collaborative experts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Albanie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nagrani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">UniViLM: A unified video and language pre-training model for multimodal understanding and generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.06353</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Howto100M: Learning a text-video embedding by watching hundred million narrated video clips</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Miech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhukov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Alayrac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tapaswi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Jointly modeling embedding and translation to bridge video and language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Video2vec embeddings recognize events when examples are scarce</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Habibian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mensink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">G M</forename><surname>Snoek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">T-PAMI</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2089" to="2103" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Fine-grained action retrieval through multiple parts-of-speech embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Larlus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Csurka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Damen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Cross-modal and hierarchical modeling of video and text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A W2VV++ case study with automated and interactive text-to-video retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Loko?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sou?ek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vesel?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Mejzl?k</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACMMM</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Attention mechanisms, signal encodings and fusion strategies for improved ad-hoc video search with dual encoding networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Galanopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mezaris</surname></persName>
		</author>
		<editor>ICMR</editor>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-D</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Phan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V.-T</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Renoust</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V.-N</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ngo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-T</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Klinkigt</surname></persName>
		</author>
		<idno>NII-HITACHI-UIT at TRECVID 2016</idno>
		<title level="m">TRECVID Workshop</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">University of Amsterdam and Renmin university at TRECVID 2017: Searching video, detecting events and describing video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">G M</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">C</forename><surname>Koelma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">TRECVID Workshop</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">ITI-CERTH participation in TRECVID 2016</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Markatopoulou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Moumtzidou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Galanopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mironidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kaltsa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ioannidou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Symeonidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Avgerinakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Andreadis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">TRECVID Workshop</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Waseda Meisei at TRECVID 2017: Ad-hoc video search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ueki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hirakawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kikuchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ogawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kobayashi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">TRECVID Workshop</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">VIREO@TRECVID 2017: Video-to-text, ad-hoc video search and video hyperlinking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-Q</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-W</forename><surname>Ngo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">TRECVID Workshop</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Fine-grained video-text retrieval with hierarchical graph reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning coarse-to-fine graph neural networks for video-text retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">T-MM</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>in press</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Predicting visual features from text for image and video caption retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">G M</forename><surname>Snoek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">T-MM</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="3377" to="3388" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">NetVLAD: CNN architecture for weakly supervised place recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Arandjelovi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gronat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pajdla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">T-PAMI</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1437" to="1451" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A joint sequence fusion model for video question answering and retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Renmin University of China and Zhejiang Gongshang University at TRECVID 2018: Deep Cross-Modal Embeddings for Video-Text Retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">TRECVID Workshop</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">M</forename><surname>Bishop</surname></persName>
		</author>
		<title level="m">Pattern Recognition and Machine Learning</title>
		<meeting><address><addrLine>Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Van Merri?nboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">BERT: pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>Attention is all you need,&quot; in NIPS</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">VSE++: Improving visual-semantic embeddings with hard negatives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Faghri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Treeaugmented cross-modal encoding for complex-query video retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">MSR-VTT: A large video description dataset for bridging video and language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">TGIF: A new dataset and benchmark on animated GIF description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tetreault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jaimes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Collecting highly parallel data for paraphrase evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">B</forename><surname>Dolan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">V3c -a research video collection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Rossetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Schuldt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Awad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Butt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MMM</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Estimating average precision with incomplete and imperfect judgments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yilmaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Aslam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Learning visual features from large weakly supervised data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jabri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasilache</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">PyTorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<pubPlace>NeurIPS</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">SOM-Hunter: Video browsing with relevance-to-som feedback loop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kratochv?l</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vesel?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Mejzl?k</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Loko?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MMM</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">On influential trends in interactive video retrieval: Video browser showdown</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Loko?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Bailer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schoeffmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Muenzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Awad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">T-MM</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="3361" to="3376" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Interactive video retrieval in the age of deep learning -detailed evaluation of VBS</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Rossetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gasser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lokoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Bailer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schoeffmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Muenzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Soucek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">A</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bolettieri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Leibetseder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vrochidis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>T-MM, 2020, in press</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Joint embeddings with multimodal cues for video-text retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mithun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Metze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Roy-Chowdhury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Multimedia Information Retrieval</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3" to="18" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Informedia@TRECVID 2018: Ad-hoc video search with discrete and continuous representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vaibhav</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hauptmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">TRECVID Workshop</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Renmin University of China and Zhejiang Gongshang University at TRECVID 2019: Learn to search and describe videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">TRECVID Workshop</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">TRECVID Workshop</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">NTU ROSE lab at TRECVID 2018: Ad-hoc video search and video to text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bastan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Heng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">TRECVID Workshop</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Waseda Meisei SoftBank at TRECVID 2019: Ad-hoc video search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ueki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kobayashi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">TRECVID Workshop</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Query understanding is key for zero-example video search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">C</forename><surname>Koelma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">G M</forename><surname>Snoek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">TRECVID Workshop</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">The Netherlands, in 2012, all in computer science. He is currently an Associate Professor with the</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xirong</forename><surname>Li Received The</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">S</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<pubPlace>Beijing, China; Amsterdam; Beijing, China</pubPlace>
		</imprint>
		<respStmt>
			<orgName>degrees from Tsinghua University ; Key Lab of Data Engineering and Knowledge Engineering, Renmin University of China</orgName>
		</respStmt>
	</monogr>
	<note>, respectively, and the Ph.D. degree from the University of Amsterdam. His research is multimedia intelligence</note>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dr</surname></persName>
		</author>
		<title level="m">Li was recipient of the ACMMM 2016 Grand Challenge Award, the ACM SIGMM Best Ph.D. Thesis Award 2013, the IEEE TRANSACTIONS ON MULTIMEDIA Prize Paper Award 2012, and the Best Paper Award of ACM CIVR 2010. He served as program co-chair of Multimedia Modeling 2021. He is associate editor of ACM TOMM and the Multimedia Systems journal</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">He is currently a graduate student at School of Information</title>
		<imprint>
			<date type="published" when="2019" />
			<pubPlace>Nanjing, China</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Fangming Zhou received his B.S. degree in Nano Materials in Nanjing University of Science and Technology ; Renmin University of China</orgName>
		</respStmt>
	</monogr>
	<note>pursuing his master degree on multimedia retrieval</note>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
			</analytic>
	<monogr>
		<title level="m">respectively. He is currently an assistant engineer at the Institute of Computing Technology</title>
		<meeting><address><addrLine>Beijing, China; Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
		<respStmt>
			<orgName>Chaoxi Xu received his B.S. and M.E. degrees in Computer Science from Renmin University of China</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">He is currently a graduate student at the School of Information, Renmin University of china</title>
		<imprint>
			<date type="published" when="2019" />
			<pubPlace>Taiyuan, China</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Jiaqi Ji received his B.S degree in Software Engineering from Taiyuan University of Technology</orgName>
		</respStmt>
	</monogr>
	<note>pursuing his master degree on video retrieval</note>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">He is currently an Associate Professor at School of Information</title>
	</analytic>
	<monogr>
		<title level="m">His research interests include computational intelligence, multimedia computing and machine learning</title>
		<meeting><address><addrLine>Toyama, Japan; Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
		<respStmt>
			<orgName>Gang Yang received his Ph.D. degree in Innovative Life Science from University of Toyama ; Renmin University of China</orgName>
		</respStmt>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
