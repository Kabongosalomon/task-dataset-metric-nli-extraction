<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">BAPose: Bottom-Up Pose Estimation with Disentangled Waterfall Representations</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bruno</forename><surname>Artacho</surname></persName>
							<email>bmartacho@mail.rit.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Rochester Institute of Technology Rochester</orgName>
								<address>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Savakis</surname></persName>
							<email>andreas.savakis@rit.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Rochester Institute of Technology Rochester</orgName>
								<address>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">BAPose: Bottom-Up Pose Estimation with Disentangled Waterfall Representations</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T15:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose BAPose, a novel bottom-up approach that achieves state-of-the-art results for multi-person pose estimation. Our end-to-end trainable framework leverages a disentangled multi-scale waterfall architecture and incorporates adaptive convolutions to infer keypoints more precisely in crowded scenes with occlusions. The multiscale representations, obtained by the disentangled waterfall module in BAPose, leverage the efficiency of progressive filtering in the cascade architecture, while maintaining multi-scale fields-of-view comparable to spatial pyramid configurations. Our results on the challenging COCO and CrowdPose datasets demonstrate that BAPose is an efficient and robust framework for multi-person pose estimation, achieving significant improvements on state-of-the-art accuracy.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Locating humans and estimating their pose in crowded scenes is a challenging task of high interest for computer vision researchers and practitioners. Successful human pose estimation enables applications in action recognition, sports analysis, human-computer interactions, rehabilitation, and sign language recognition. Various methods have focused on tackling specific aspects of human pose estimation, including 2D pose estimation <ref type="bibr" target="#b39">[40]</ref>, <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b42">[43]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b38">[39]</ref>; 3D pose estimation <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b46">[47]</ref>, <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b4">[5]</ref>; single frame detection <ref type="bibr" target="#b5">[6]</ref>; pose detection in videos <ref type="bibr" target="#b11">[12]</ref>; dealing with a single person <ref type="bibr" target="#b42">[43]</ref> or multiple people <ref type="bibr" target="#b6">[7]</ref>.</p><p>The task of multi-person pose estimation is notorious for the challenges caused by the high occurrence of joint occlusions, combined with the large number of degrees of freedom in the human body movements. Common approaches to overcome these challenges include the deployment of statistical and geometric models to estimate occluded joints <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b28">[29]</ref> and the use of anchor poses <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b41">[42]</ref>, although the latter approach is limited by the number of poses in its library, making it difficult to generalize and handle unforeseen poses.</p><p>State-of-the-art (SOTA) methods for multi-person 2D pose estimation can be divided in two distinct approaches: top-down and bottom-up. Top-down methods initially detect the instances of persons in the image and then perform single person pose estimation for each individual. Bottomup methods for multi-person pose estimation either detect all keypoints and group them by affinity relations <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b21">[22]</ref>, or directly regress the keypoint locations to each person in the image <ref type="bibr" target="#b14">[15]</ref>. Overall, top-down approaches achieve high accuracy, although they require an extra step for person detection, resulting in a slower and more costly process.</p><p>On the other hand, bottom-up approaches are based on a single-stage for multi-person pose estimation that is generally more efficient.</p><p>In this paper, we propose BAPose, a bottom up framework that is named after "Basso verso l'Alto" (bottom up in Italian). The BAPose method is a single-stage, end-toend trainable network that extends recent successful approaches by UniPose <ref type="bibr" target="#b2">[3]</ref> , UniPose+ <ref type="bibr" target="#b4">[5]</ref>, and OmniPose <ref type="bibr" target="#b3">[4]</ref> to bottom-up multi-person 2D pose estimation. BA-Pose achieves state-of-the-art (SOTA) results in two large datasets without requiring post-processing, intermediate supervision, multiple iterations or anchor poses. The main contributions of BAPose are the following:</p><p>? We propose the novel BAPose method, a single-pass, end-to-end trainable, multi-scale approach for bottomup multi-person 2D pose estimation, that achieves SOTA results for two large benchmark datasets, COCO and CrowdPose.</p><p>? In our bottom-up approach, we combine multi-scale waterfall features with disentangled adaptive convolutions and an integrated multi-scale decoder to disambiguate the joints of individuals in crowded scenes.</p><p>? The enhanced multi-scale capability of BAPose specializes the network for human pose estimation in images with a large number of person instances, drastically increasing the SOTA performance for the Crowd-Pose dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>The advent of Convolutional Neural Networks (CNNs) for deep learning methods enabled leaping advances for the task of human pose estimation <ref type="bibr" target="#b39">[40]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b4">[5]</ref>. The Convolutional Pose Machines (CPM) <ref type="bibr" target="#b42">[43]</ref> approach uses a sequence of CNN stages in the network to refine joint detection. Furthering the work of <ref type="bibr" target="#b42">[43]</ref> Yan et al. integrated Part Affinity Fields (PAF) in their framework to better capture the context and relationships between joints for improved 2D human pose estimation. The resulting Open-Pose method <ref type="bibr" target="#b6">[7]</ref> is widely used in various applications.</p><p>The Stacked Hourglass (HG) network <ref type="bibr" target="#b27">[28]</ref> utilizes a multi-stage approach by cascading hourglass structures through the network to refine the resulting pose estimation. The HG work was further expanded to incorporate the multi-context approach in <ref type="bibr" target="#b12">[13]</ref> by augmenting the backbone with residual units in order to increase the receptive Fieldof-View (FOV). Postprocessing with Conditional Random Fields (CRFs) is applied to refine the location of detected joints. A downside of this approach is the increase in complexity by the addition of another stage of postprocessing and the associated increase in computational load.</p><p>Aiming to offer a multi-scale approach to feature representations, the High-Resolution Network (HRNet) includes both the high and low resolutions to obtain a larger FOV. The Multi-Stage Pose Network (MSPN) <ref type="bibr" target="#b23">[24]</ref> follows a similar approach to HRNet by combining the cross-stage feature aggregation and coarse-to-fine supervision. In further work, <ref type="bibr" target="#b10">[11]</ref> combined the HRNet structure with multiresolution pyramids to obtain multi-scale features. Building upon the work of HRNet, the Distribution-Aware coordinate Representation of Keypoints (DARK) method <ref type="bibr" target="#b43">[44]</ref> incorpo-rates a refined approach to their decoder in order to reduce the inference error at the decoder stage.</p><p>Developments in graphical components for CNNs inspired the Cascade Prediction Fusion (CPF) approach <ref type="bibr" target="#b44">[45]</ref> that applies graphs in other to further extract the contextual information for pose estimation. In a similar fashion, Cascade Feature Aggregation (CFA) <ref type="bibr" target="#b37">[38]</ref> applied the cascade approach into the semantic information for pose estimation. Generative Adversarial Networks (GANs) were used in <ref type="bibr" target="#b7">[8]</ref> in order to learn dependencies and contextual information for pose.</p><p>A limitation of top-down approaches is the requirement of an independent module for the detection of instances of humans in the frame. LightTrack <ref type="bibr" target="#b31">[32]</ref>, for instance, requires a separate YOLOv3 <ref type="bibr" target="#b35">[36]</ref> architecture to detect subjects prior to the detection of joints for pose estimation. In a slightly different approach, LCR-Net <ref type="bibr" target="#b36">[37]</ref> applies multiple branches for detection by using Detectron <ref type="bibr" target="#b15">[16]</ref> and the arrangement of joints during classification.</p><p>With the goal of developing a unified framework to overcome the limitation of top-down approaches, UniPose <ref type="bibr" target="#b2">[3]</ref> combines the bounding box generation and pose estimation in a single, one-pass network. This approach is possible due to the larger FOV and significant increase in the multiscale representation obtained by the Waterfall Atrous Spatial Pooling (WASP) module <ref type="bibr" target="#b1">[2]</ref>, which allows for greater FOV and results in better representation of contextual information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Bottom-Up Approaches</head><p>The most common approach for bottom-up estimation is to associate detected keypoints with each person present in the image. One approach is to cast the problem in terms of integer linear programming <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b17">[18]</ref>. A clear drawback of this approach is the high processing time required, inhibiting the possibility of real-time performance.</p><p>OpenPose <ref type="bibr" target="#b6">[7]</ref> is considered a breakthrough approach for grouping keypoints with the introduction of PAF. Other methods further developed PAF, such as Pif-Paf <ref type="bibr" target="#b21">[22]</ref> and associative embedding <ref type="bibr" target="#b26">[27]</ref>. In a similar vein, PersonLab <ref type="bibr" target="#b33">[34]</ref> adopted Hough voting, and <ref type="bibr" target="#b19">[20]</ref> used hierarchical graphical clustering.</p><p>The dense regression of pose candidates is adopted by several recent works <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b29">[30]</ref>. A limitation of this approach is the lower regression accuracy in the localization process, that usually requires an additional post-processing step in order to improve the regression results. Aiming to bridge the gap, <ref type="bibr" target="#b40">[41]</ref> applied a mixture density network to better handle uncertainty in the network before regression. The recent Disentangled Keypoint Regression (DEKR) method <ref type="bibr" target="#b14">[15]</ref>, on the other hand, learns disentangled representations for each keypoint and utilizes adaptively activated pixels, ensuring that each representation fo- cuses on the corresponding keypoint area.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Multi-Scale Feature Representations</head><p>The reduction of resolution that takes place in CNNbased methods is an ongoing challenge for pose estimation or semantic segmentation methods. Fully Convolutional Networks (FCN) <ref type="bibr" target="#b25">[26]</ref> initially addressed resolution reduction by adopting upsampling strategies across deconvolution layers to increase the size of the features maps, reverting it back to the original input image dimensions. Further, DeepLab <ref type="bibr" target="#b8">[9]</ref> deployed dilated or atrous convolutions to achieve a multi-scale framework and increase the size of the receptive fields, avoiding downsampling in the network with the introduction of the Atrous Spacial Pyramid Pooling (ASPP). The ASPP architecture applies atrous convolutions in four parallel branches with different rates, and combines them via bilinear interpolation in order to recover the feature maps at the original image resolution.</p><p>Improving upon ASPP <ref type="bibr" target="#b8">[9]</ref>, the WASP module incorporates multi-scale features without immediately parallelizing the input stream <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>. The WASP module creates a waterfall flow by initially processing through a filter and later creating a new branch. The waterfall scheme extends the cascade approach by combining the streams from all its branches to achieve a multi-scale representation. The OmniPose framework <ref type="bibr" target="#b3">[4]</ref> recently introduced the enhanced WASPv2 module, that improves upon the multi-scale feature extraction from the backbone and includes the decoder features of the network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">BAPose Architecture</head><p>The proposed BAPose bottom-up method, illustrated in <ref type="figure" target="#fig_1">Figure 2</ref>, consists of a single-pass, single output branch network that is particularly effective for multi-person 2D pose estimation in crowded scenes. BAPose integrates improvements in multi-scale feature representations <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b14">[15]</ref>, an encoder-decoder structure combined with the spatial pyra-mid pooling of the waterfall configuration, and disentangled adaptive regression for person localization and parts association.</p><p>The processing pipeline of the BAPose architecture is shown in <ref type="figure" target="#fig_1">Figure 2</ref>. The input image is initially processed by the HRNet feature extractor. The extracted multi-scale feature maps are then processed by the WASPv2 module with integrated decoder, that extracts the location of keypoints as well as contextual information for the localization regression. The network generates K heatmaps, one for each joint, with the corresponding confidence maps as well as 2 offset maps for the identification of person instances and association of keypoints to each instance. The integrated WASPv2 decoder in our network generates detections from all scales of the feature extraction for both visible and occluded joints while maintaining the image resolution through the network.</p><p>Our architecture includes several innovations that contribute to increased accuracy. In the WASPv2 module, BA-Pose combines atrous convolutions and the waterfall architecture to increase the network's capacity to represent multi-scale contextual information by the probing of feature maps at multiple rates of dilation. This configuration achieves a larger FOV in the encoder. Our architecture also integrates disentangled adaptive convolutions in the decoding process, enabling the single-pass detection of multiple person instances and their keypoint estimation. Additionally, our network demonstrates superior ability to deal with a large number of subjects by the enhanced extraction of features at multiple scales, as indicated by state-of-the-art results for the CrowdPose dataset presented in Section 6. Finally, the modular nature of BAPose facilitates the easy implementation and training of the network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Disentangled Waterfall Module</head><p>The proposed enhanced "Disentangled Waterfall Atrous Spatial Pyramid" module, or D-WASP, is shown in <ref type="figure" target="#fig_2">Figure  3</ref>. The D-WASP module processes all four levels of fea- ture maps from the backbone through the waterfall branches with different dilation rates. Low-level and high-level features are represented at the same resolution, achieving a refined localization for joint estimation. Furthermore, the D-WASP module uses adaptive convolution blocks to infer the final heatmaps for joint localization and offset maps for person instance regression. The module generates both the keypoints and offset heatmaps for each person, through their respective heads illustrated in <ref type="figure" target="#fig_2">Figure 3</ref>. The D-WASP architecture helps to more effectively discern multiple people in a crowded setting due to its multi-level and multiscale representations, contributing to SOTA performance.</p><p>The design of the D-WASP module relies on a combination of atrous and adaptive convolutions. Atrous convolutions are utilized in the initial stages to expand the FOV by performing a filtering cascade at increasing rates to gain efficiency. The waterfall modules are designed to create a waterfall flow, initially processing the input and then creating a new branch. D-WASP goes beyond the cascade approach of <ref type="bibr" target="#b9">[10]</ref> by combining all streams from all its branches and the average pooling layer from the original input. Additionally, our module incorporates a larger number of scales compared to WASPv2 <ref type="bibr" target="#b3">[4]</ref> by adopting all 480 feature maps from all levels of the HRNet feature extractor. Adaptive convolutions are used to better infer the individual keypoints and offset heatmaps during the regression process by providing context around the vicinity of each detected joint and strengthening the relationship between associated joints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Waterfall Features and Adaptive Convolutions</head><p>The D-WASP module operation begins with the concatenation g 0 of all feature maps f i from the HRNet feature extractor, where i = 0, 1, 2, 3 indicates the levels at different scales of the feature extractor and summation is overloaded for concatenation:</p><formula xml:id="formula_0">g 0 = 3 i=0 (f i )<label>(1)</label></formula><p>Following the concatenation of all feature maps, the waterfall processing is described as follows:</p><formula xml:id="formula_1">f W aterf all = W 1 ( 4 i=1 (W di g i?1 ) + AP (g 0 )) (2) f maps = W 1 (W 1 (W 1 f LLF + f W aterf all ) (3)</formula><p>where represents convolution, g 0 is the input feature map, g i is the feature map resulting from the i th atrous convolution, AP is the average pooling operation, f LLF are the low-level feature maps, and W 1 and W di represent convolutions of kernel size 1?1 and 3?3 with dilations of d i = <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b17">18]</ref>, as shown in <ref type="figure" target="#fig_2">Figure 3</ref>. After concatenation, the feature maps are combined with low level features. The last 1?1 convolution brings the number of feature maps down to one quarter of the number in the combined input feature maps.</p><p>Finally, the D-WASP module output f D?W ASP is obtained from the multi-scale adaptive convolutional regression, where adaptive convolution is defined as:</p><formula xml:id="formula_2">y(c) = 9 i=1 (w i x(g c i + c))<label>(4)</label></formula><p>where c is the center pixel of the convolution, y(c) represents the output of the convolution for input x, w i are the kernel weights for the the center pixel its neighbors, and g c i is the offset of the i th activated pixel. In the adaptive convolutions, the offsets g c i are adopted in a parametric manner as an extension of spatial transformer networks <ref type="bibr" target="#b18">[19]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Disentangled Adaptive Regression</head><p>The regression stage for multi-person pose estimation is considered the most challenging and a bottleneck in performance for bottom-up methods. To address the limitation of regression, additional processing may utilize pose candidates, post-processing matching schemes, proximity matching, and statistical methods, however these may be computationally expensive or limited in effectiveness. D-WASP expands on the idea of regression by focus, by not only learning disentangled representations for each of the K joints, but also using multiple scales to infer each representation for all keypoints from multiple adaptively activated pixels. This configuration gives each regression a more robust contextual information of the keypoint region, and results in a more accurate spatial representation.</p><p>The multi-scale approach proposed by the D-WASP module, allows BAPose to regress person detections and keypoints with a larger FOV, increasing the network capability to infer joints association through the use of adaptive convolutions. Differently than the WASPv2 <ref type="bibr" target="#b3">[4]</ref> decoder stage that only extracts the heatmaps for joints, the D-WASP multi-scale disentangled adaptive regression determines both the keypoint heatmaps and the final offset heatmaps that are used to regress the position of each individual in the image and their respective joints.</p><p>In addition, the integration of the multi-scale feature maps in the disentangled adaptive regression utilizes multiple resolutions at the regression stage, allowing the network to better infer the locations of people and their joints in the image. As a consequence, BAPose demonstrates superior performance (see Section 6), especially in challenging scenarios that include large numbers of people in close proximity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Datasets</head><p>We evaluated the BAPose method on two datasets for 2D multi-person pose estimation: Common Objects in Context (COCO) <ref type="bibr" target="#b24">[25]</ref> and CrowdPose <ref type="bibr" target="#b22">[23]</ref>. The large and most commonly adopted COCO dataset <ref type="bibr" target="#b24">[25]</ref> consists of over 200K images with more than 250K instances of labelled people keypoints. The keypoint labels consist of 17 keypoints including all major joints in the torso and limbs, as well as facial landmarks of nose, eyes, and ears. The dataset is considered a challenging dataset due to the large number of images in a diverse set of scales and occlusion for poses in the wild.</p><p>The CrowdPose dataset <ref type="bibr" target="#b22">[23]</ref> is a more challenging dataset since it includes many images with crowds and low separation among individuals. The dataset contains 10K images for training, 2K images for validation, and 20K images for testing. The dataset contains frames with joints annotations, head and torso orientations, and body part occlusions.</p><p>We follow evaluation procedures adopted by <ref type="bibr" target="#b10">[11]</ref> and <ref type="bibr" target="#b14">[15]</ref>. We adopted the generation of ideal Gaussian maps for the joints ground truth locations in order to train our network more effectively. The Gaussian maps are a more effective strategy for loss assessment during training compared to single points at the joint locations. As a consequence, the BAPose was trained to generate heatmaps as output locations for each joint. The value of ? = 3 was adopted, generating a well define Gaussian response for both the ground truth and keypoint predictions, while maintaining a decent separation of keypoints and avoiding large overlapping of keypoints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>BAPose experiments followed standard metrics set by each dataset, and same procedures applied by <ref type="bibr" target="#b10">[11]</ref>, and <ref type="bibr" target="#b14">[15]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Metrics</head><p>For the evaluation of BAPose, the evaluation is done based on the Object Keypoint Similarity metric (OKS).</p><formula xml:id="formula_3">OKS = ( i e ?d 2 i /2s 2 k 2 i )?(v i &gt; 0) i ?(v i &gt; 0)<label>(5)</label></formula><p>where, d i is the Euclidian distance between the estimated keypoint and its ground truth, v i indicates if the keypoint is visible, s is the scale of the corresponding target, and k i is the falloff control constant. Since the OKS measurement is adopted by both COCO and CrowdPose dataset and is similar to the intersection over the union (IOU), we report our OKS results as the Average Precision (AP) for the IOUs for all instances between 0.5 and 0.95 (AP ), at 0.5 (AP 50 ) and 0.75 (AP 75 ), as well as instances of medium (AP M ) and large size (AP L ) for the COCO dataset. For the Crowd-Pose dataset, we report easy (AP E ), medium (AP M ,) and hard size (AP H ) instances. We also report the overall Average Recall (AR) as well as AR for AR M medium and AR L large instances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Parameter Selection</head><p>We use a set of dilation rates of r = {1, 6, 12, 18} for the D-WASP module, similar to <ref type="bibr" target="#b3">[4]</ref>. The network was trained for 140 epochs. The learning rate is calculated based on the step method, where the rate is initialized at 10 ?3 and is reduced by an order of magnitude in two steps at 90 and 120 epochs. The training procedure includes random rotation between ?30 ? and 30 ? , random scale from 0.75 to 1.5, and random translation between ?40 and 40, mirroring procedures followed by <ref type="bibr" target="#b14">[15]</ref>. All experiments were performed using PyTorch on Ubuntu 16.04. The workstation has an Intel i5-2650 2.20GHz CPU with 16GB of RAM and an NVIDIA Tesla V100 GPU.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Results</head><p>This section presents BAPose results on two large datasets and provides comparisons with state-of-the art methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Experimental results on the CrowdPose dataset</head><p>We performed training and testing on the CrowdPose dataset, which presents a difficult challenge due to the high occurrence of crowds in the images. The CrowdPose results are shown in <ref type="table" target="#tab_0">Table 1</ref>.</p><p>Our BAPose method significantly improves upon the performance of SOTA methods for 512?512 input resolution, achieving 72.2% accuracy, and significantly outperforms other bottom-up approaches, even those that utilized higher input resolutions. It is noticeable that BAPose achieves most of its gains by more precise joint estimations, increasing the performance from 70.4% to 78.0% for AP 75 when compared to the previous SOTA, HRNet-w32 <ref type="bibr" target="#b14">[15]</ref>. Additionally, BAPose outperforms networks that utilize top-down approaches. Differently than top-down methods, BAPose does not rely on ground truth for person detection and has to infer the location of all individuals in a modular, single-pass process. For the CrowdPose dataset, BA-Pose's performance is superior to networks utilizing higher resolution inputs of 640?640 <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b10">[11]</ref> while processing the less computationally expensive 512?512 resolution.  detections at different scales.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Experimental results on the COCO dataset</head><p>We next performed training and testing on the COCO dataset, which is challenging due to the large number of diverse images with multiple people in close proximity, and additionally includes images lacking a person instance.</p><p>We first compared BAPose with SOTA methods for the COCO validation and test-dev datasets, with results presented in <ref type="table" target="#tab_3">Tables 3 and 4</ref> respectively. The validation results in <ref type="table" target="#tab_3">Table 3</ref> show that BAPose achieves significant improvement over the previous SOTA for input resolution of 512?512 and 640?640. The BAPose results at the lower 512?512 resolution are obtained with a significantly lower computational cost compared to methods with higher resolution inputs, as shown in <ref type="table" target="#tab_1">Table 2</ref>, while achieving comparable results to the higher resolution.</p><p>The combination of the HRNet backbone with the D-WASP module achieves an increased overall accuracy of 69.1% when using single-scale testing, and 71.9% when using multi-scale testing, compared to the previous SOTA, HRNet-w32 <ref type="bibr" target="#b14">[15]</ref>, of 68% and 70.7%, respectively. Overall BAPose achieves a significant increase of 1.6% and 1.7% for single-scale and multi-scale testing, respectively.</p><p>BAPose improves the accuracy of the previous SOTA in all keypoint estimation metrics and IOU for the COCO dataset. Most of the performance improvements of BA-Pose are attributed to performing better on harder detections and more refined predictions at AP 75 . The results on the COCO validation dataset, in <ref type="table" target="#tab_3">Table 3</ref>, show the greater capability of BAPose to detect more complex and harder poses while still using a smaller resolution in the input image. We also trained and tested BAPose-W48 at 640?640 resolution. BAPose-W48 achieved 71.5% accuracy for the COCO validation set with single scale testing and 72.7% with multi-scale testing, improving the previous SOTA by 0.7% and 0.6%, respectively. However, larger resolution models require much higher computational resources, as illustrated by the GFLOPs and memory requirements for different methods shown in <ref type="table" target="#tab_1">Table 2</ref>. Compared to BAPose-W32, HRNet-W48 requires 249.1% the number of GFLOPs and HigherHRNet-W48 requires 271.7% the number of GFLOPs, demonstrating that BAPose-W32 results in a better trade-off between accuracy and computational cost.   the COCO dataset. It is noticeable from the sample images that BAPose effectively locates symmetric body joints and avoids confusion due to occlusion between individuals. This is illustrated in harder to detect joints such as ankles and wrists. Overall, the BAPose results demonstrate its robustness for pose estimation in various challenging conditions, such as images that include detections of individuals with high overlapping ratio combined with shadows or darker images, or partial pose present in the image. We next compared BAPose with SOTA methods on the larger COCO test-dev dataset, with results shown in Table 4. BAPose again achieved new SOTA performance over methods using input resolutions of 512?512. Our method obtained an overall precision of 68.0% when using single-scale testing and 70.4% when using multi-scale testing, which are improvements of 1.0% and 1.1% over SOTA for single and multi scale testing, respectively. When training and testing at the 640?640 resolution, BAPose-W48 achieved accuracies of 70.3% for single-scale testing and 71.2% when using single-scale multi-scale testing, an improvement of 0.4% and 0.3% to the previous SOTA, respectively. These results further confirm that BAPose demonstrates most significant improvements in smaller and harder targets consistent with the findings from the validation dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>We presented the BAPose method for bottom-up multiperson pose estimation. The BAPose network includes the novel D-WASP module that combines multi-scale features obtained from the waterfall flow with the person detection capability of the disentangled adaptive regression. BAPose is a end-to-end trainable, single-pass architecture that does not require anchor poses, prior person detections, or postprocessing. The results demonstrate state-of-the-art performance for both the COCO and CrowdPose datasets using various metrics, and the superior capability of person detection and pose estimation in densely populated images.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Pose estimation examples with our BAPose method.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>BAPose architecture for bottom-up multi-person pose estimation. The input color image is fed through the HRNet backbone for initial feature extraction, followed by the D-WASP module and an adaptive convolution based decoder to generate one heatmap per joint (17 joints in the figure) and offset regression for the localization of each person instance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>The proposed D-WASP disentangled waterfall module. The inputs are 32, 64, 128, and 256 features maps from all four levels of the HRNet backbone, respectively, and low-level features from the initial layers of the framework. The module outputs both the keypoints and offsets heatmaps.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Pose estimation examples using BAPose with the CrowdPose dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4</head><label>4</label><figDesc>illustrates successful detections of pose for multiple people in images from the CrowdPose test set. The examples demonstrate how effectively BAPose deals with occlusions and close proximity of individuals, as well as</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 .</head><label>5</label><figDesc>Pose estimation results using BAPose with the COCO dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5</head><label>5</label><figDesc>presents examples of pose estimation results for</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>BAPose results and comparison with SOTA methods for the CrowdPose dataset for testing.</figDesc><table><row><cell>Method</cell><cell>Input Size</cell><cell>Approach</cell><cell>AP</cell><cell>AP 50</cell><cell>AP 75</cell><cell>AP E</cell><cell>AP M</cell><cell>AP H</cell></row><row><cell></cell><cell></cell><cell cols="3">Single-Scale Testing</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>BAPose (W32)</cell><cell>512</cell><cell cols="7">Bottom-Up 72.2% 89.6% 78.0% 79.9% 73.4% 61.3%</cell></row><row><cell>HRNet-W48 [15]</cell><cell>640</cell><cell cols="7">Bottom-Up 67.3% 86.4% 72.2% 74.6% 68.1% 58.7%</cell></row><row><cell>HigherHRNet-W48 [11]</cell><cell>640</cell><cell cols="7">Bottom-Up 65.9% 86.4% 70.6% 73.3% 66.5% 57.9%</cell></row><row><cell>MIPNet [21]</cell><cell>512</cell><cell cols="2">Top-Down 70.0%</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Joint-candidate SPPE [23]</cell><cell>-</cell><cell cols="2">Top-Down 66.0%</cell><cell>84.2</cell><cell>71.5</cell><cell cols="3">75.5% 66.3% 57.4%</cell></row><row><cell>HRNet-W32 [15]</cell><cell>512</cell><cell cols="7">Bottom-Up 65.7% 85.7% 70.4% 73.0% 66.4% 57.5%</cell></row><row><cell>AlphaPose [14]</cell><cell>-</cell><cell>Bottom-Up</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="3">71.2% 61.4% 51.1%</cell></row><row><cell>Mask R-CNN [17]</cell><cell>-</cell><cell cols="2">Bottom-Up 60.3%</cell><cell>-</cell><cell>-</cell><cell cols="3">69.4% 57.9% 45.8%</cell></row><row><cell>OpenPose [7]</cell><cell>-</cell><cell>Bottom-Up</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="3">62.7% 48.7% 32.3%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>GFLOPs and number of parameters comparison.</figDesc><table><row><cell>Method</cell><cell cols="2">Input GFLOPs Size</cell><cell>Params (M)</cell></row><row><cell>HRNet-W32 [15]</cell><cell>512</cell><cell>45.4</cell><cell>29.6</cell></row><row><cell>BAPose-W32</cell><cell>512</cell><cell>56.8</cell><cell>30.3</cell></row><row><cell>HRNet-W48 [15]</cell><cell>640</cell><cell>141.5</cell><cell>65.7</cell></row><row><cell>HigherHRNet-W48 [11]</cell><cell>640</cell><cell>154.3</cell><cell>63.8</cell></row><row><cell>BAPose-W48</cell><cell>640</cell><cell>183.2</cell><cell>67.4</cell></row><row><cell>AE [27]</cell><cell>512</cell><cell>206.9</cell><cell>227.8</cell></row><row><cell>PersonLab [34]</cell><cell>1401</cell><cell>405.5</cell><cell>68.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>5% 88.7% 77.8% 67.2% 78.8% 76.2% 71.0% 84.0% HRNet-W48 [15] 640 71.0% 88.3% 77.4% 66.7% 78.5% 76.0% 70.6% 84.0%</figDesc><table><row><cell>Method</cell><cell>Input Size</cell><cell>AP</cell><cell>AP 50</cell><cell>AP 75</cell><cell>AP M</cell><cell>AP L</cell><cell>AR</cell><cell>AR M</cell><cell>AR L</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">Single-Scale Testing</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">BAPose (W48) 71.HigherHRNet-W48 [11] 640 640 69.9% 87.2% 76.1%</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="2">65.4% 76.4%</cell></row><row><cell>PersonLab [34]</cell><cell cols="9">1401 66.5% 86.2% 71.9% 62.3% 73.2% 70.7% 65.6% 77.9%</cell></row><row><cell>PersonLab [34]</cell><cell>601</cell><cell cols="8">54.1% 76.4% 57.7% 40.6% 73.3% 57.7% 43.5% 77.4%</cell></row><row><cell>BAPose (W32)</cell><cell>512</cell><cell cols="8">69.1% 87.0% 75.6% 63.1% 78.6% 73.7% 66.9% 83.4%</cell></row><row><cell>HRNet-W32 [15]</cell><cell>512</cell><cell cols="8">68.0% 86.7% 74.5% 62.1% 77.7% 73.0% 66.2% 82.7%</cell></row><row><cell>HigherHRNet-W32 [11]</cell><cell>512</cell><cell cols="3">67.1% 86.2% 73.0%</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="2">61.5% 76.1%</cell></row><row><cell>HGG [20]</cell><cell>512</cell><cell cols="3">60.4% 83.0% 66.2%</cell><cell>-</cell><cell>-</cell><cell>64.8%</cell><cell>-</cell><cell>-</cell></row><row><cell>CenterNet-HG [46]</cell><cell>512</cell><cell>64.0%</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>CenterNet-DLA [46]</cell><cell>512</cell><cell>58.9%</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Multi-Scale Testing</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>BAPose (W48)</cell><cell>640</cell><cell cols="8">72.7% 88.6% 79.1% 69.3% 78.4% 77.9% 73.4% 84.7%</cell></row><row><cell>HRNet-W48 [15]</cell><cell>640</cell><cell cols="8">72.3% 88.3% 78.6% 68.6% 78.6% 77.7% 72.8% 84.9%</cell></row><row><cell>HigherHRNet-W48 [11]</cell><cell>640</cell><cell cols="3">72.1% 88.4% 78.2%</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="2">67.8% 78.3%</cell></row><row><cell>BAPose (W32)</cell><cell>512</cell><cell cols="8">71.9% 88.3% 77.8% 67.2% 79.1% 76.6% 71.3% 84.5%</cell></row><row><cell>HRNet-W32 [15]</cell><cell>512</cell><cell cols="8">70.7% 87.7% 77.1% 66.2% 77.8% 75.9% 70.5% 83.6%</cell></row><row><cell>HigherHRNet-W32 [11]</cell><cell>512</cell><cell cols="3">69.9% 87.1% 76.0%</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="2">65.3% 77.0%</cell></row><row><cell>HGG [20]</cell><cell>512</cell><cell cols="3">68.3% 86.7% 75.8%</cell><cell>-</cell><cell>-</cell><cell>72.0%</cell><cell>-</cell><cell>-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>BAPose results and comparison with SOTA methods for the COCO dataset for validation. 7% 89.9% 76.3% 64.8% 75.3% 74.8% 69.6% 82.1% PersonLab [34] 1401 68.7% 89.0% 75.4% 64.1% 75.5% 75.4% 69.7% 83.0% BAPose (W32) 512 70.4% 89.3% 77.4% 66.0% 76.9% 75.6% 70.1% 83.2% HRNet-W32 [15] 512 69.6% 89.0% 76.6% 65.2% 76.5% 75.1% 69.5% 82.8% HGG [20] 512 67.6% 85.1% 73.7% 62.7% 74.6% 71.3% --AE [27] 512 65.5% 86.8% 72.3% 60.6% 72.6% 70.2% 64.6% 78.1% Table 4. BAPose results and comparison with SOTA methods for the COCO dataset for test-dev.</figDesc><table><row><cell>Method</cell><cell>Input Size</cell><cell>AP</cell><cell>AP 50</cell><cell>AP 75</cell><cell>AP M</cell><cell>AP L</cell><cell>AR</cell><cell>AR M</cell><cell>AR L</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">Single-Scale Testing</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>BAPose (W48)</cell><cell>640</cell><cell cols="8">70.3% 89.6% 77.5% 65.9% 77.1% 75.4% 69.8% 83.2%</cell></row><row><cell>HRNet-W48 [15]</cell><cell>640</cell><cell cols="8">70.0% 89.4% 77.3% 65.7% 76.9% 75.4% 69.7% 83.2%</cell></row><row><cell>HigherHRNet-W48 [11]</cell><cell>640</cell><cell cols="3">68.4% 88.2% 75.1%</cell><cell>64.4</cell><cell>74.2</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>PersonLab [34]</cell><cell cols="9">1401 66.5% 88.9% 72.6% 62.4% 72.3% 71.0% 66.1% 77.7%</cell></row><row><cell>BAPose (W32)</cell><cell>512</cell><cell cols="8">68.0% 88.0% 74.8% 62.4% 76.6% 72.9% 66.1% 82.4%</cell></row><row><cell>HRNet-W32 [15]</cell><cell>512</cell><cell cols="8">67.3% 87.9% 74.1% 61.5% 76.1% 72.4% 65.4% 81.9%</cell></row><row><cell>SPM [31]</cell><cell>-</cell><cell cols="5">66.9% 88.5% 72.9% 62.6% 73.1%</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>PifPaf [22]</cell><cell>-</cell><cell>66.7%</cell><cell>-</cell><cell>-</cell><cell cols="2">62.4% 72.9%</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>M DN 3 [41]</cell><cell>-</cell><cell cols="5">62.9% 85.1% 69.4% 58.8% 71.4%</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>CenterNet-HG [46]</cell><cell>512</cell><cell cols="5">63.0% 86.8% 69.6% 58.9% 70.4%</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>OpenPose [7]</cell><cell>-</cell><cell cols="6">61.8% 84.9% 67.5% 57.1% 68.2% 66.5%</cell><cell>-</cell><cell>-</cell></row><row><cell>CenterNet-DLA [46]</cell><cell>512</cell><cell cols="5">57.9% 84.7% 63.1% 52.5% 67.4%</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>AE [27]</cell><cell>512</cell><cell cols="5">56.6% 81.8% 61.8% 49.8% 67.0%</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">Multi-Scale Testing</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>BAPose (W48)</cell><cell>640</cell><cell cols="8">71.2% 89.4% 78.1% 67.4% 76.8% 76.8% 71.6% 84.0%</cell></row><row><cell>HRNet-W48 [15]</cell><cell>640</cell><cell cols="8">71.0% 89.2% 78.0% 67.1% 76.9% 76.7% 71.5% 83.9%</cell></row><row><cell>HigherHRNet-W48 [11]</cell><cell>640</cell><cell cols="5">70.5% 89.3% 77.2% 66.6% 75.8%</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Point-set Anchors [42]</cell><cell>640</cell><cell>68.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Acknowledgements</head><p>This research was supported in part by the National Science Foundation grant 1749376.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Densepose: Dense human pose estimation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Riza Alp Guler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Neverova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kokkinos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7297" to="7306" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Waterfall atrous spatial pooling architecture for efficient semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bruno</forename><surname>Artacho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Savakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">24</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Unipose: Unified human pose estimation in single images and videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bruno</forename><surname>Artacho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Savakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2020. 1</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Omnipose: A multiscale framework for multi-person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bruno</forename><surname>Artacho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Savakis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Unipose+: A unified framework for 2d and 3d human pose estimation in images and videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bruno</forename><surname>Artacho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Savakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Recurrent human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vasileios</forename><surname>Belagiannis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Automatic Face &amp; Gesture Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="468" to="475" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Realtime multi-Person 2d pose estimation using part affinity fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-En</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Improving human pose estimation with selfattention generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongzheng</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoqiang</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE International Conference on Multimedia &amp; Expo Workshops (ICMEW)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="567" to="572" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution and fully connected cfrs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="834" to="845" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Encoder-decoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Liang-Chieh Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="801" to="818" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Higherhrnet: Scale-aware representation learning for bottom-up human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honghui</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Occlusion-aware networks for 3d human pose estimation in video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wending</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robby</forename><forename type="middle">T</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Multi-context attention for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1831" to="1840" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Rmpe: Regional multi-person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuqin</forename><surname>Hao-Shu Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Wing</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2017-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Bottom-up human pose estimation via disentangled keypoint regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zigang</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoxiang</forename><surname>Zhang Bin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilija</forename><surname>Radosavovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Piotr Doll?r, and Kaiming He. Detectron</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Piotr Doll?r, and Ross Girshick. Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2961" to="2969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deepercut: A deeper, stronger, and faster multi-person pose estimation model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eldar</forename><surname>Insafutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bjoern</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhaylo</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="34" to="50" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Spatial transformer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">28</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Differentiable hierarchical graph grouping for multi-person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wentao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enze</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Multi-instance pose networks: Rethinking top-down pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rawal</forename><surname>Khirodkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Visesh</forename><surname>Chari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amit</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ambrish</forename><surname>Tyagi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2021-10" />
			<biblScope unit="page" from="3122" to="3131" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Pifpaf: Composite fields for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sven</forename><surname>Kreiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Bertoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Alahi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Crowdpose: Efficient crowded scenes pose estimation and A new benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiefeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Can</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihuan</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoshu</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Lu</surname></persName>
		</author>
		<idno>abs/1812.00324</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Binyi</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixiang</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuming</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianzi</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongtao</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.00148</idno>
		<title level="m">Rethinking on multi-stage networks for human pose estimation</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubomir</forename><forename type="middle">D</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<idno>abs/1405.0312</idno>
		<title level="m">Microsoft COCO: common objects in context. CoRR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Associative embedding: End-to-end learning for joint detection and grouping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.05424</idno>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiyu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="483" to="499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">3d human pose estimation with 2D marginal heatmaps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aiden</forename><surname>Nibali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stuart</forename><surname>Morgan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Prendergast</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1477" to="1485" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Pose partition networks for multi-person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuecheng</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junliang</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the european conference on computer vision (eccv)</title>
		<meeting>the european conference on computer vision (eccv)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="684" to="699" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Single-stage multi-person pose machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuecheng</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="6951" to="6960" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Lighttrack: A generic framework for online top-down human pose tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanghan</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.02822</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Personlab: Person pose estimation and instance segmentation with a bottom-up, part-based, geometric embedding model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tyler</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spyros</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Personlab: Person pose estimation and instance segmentation with a bottom-up, part-based, geometric embedding model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tyler</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spyros</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="269" to="286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Deepcut: Joint subset partition and labeling for multi person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eldar</forename><surname>Insafutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bjoern</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhaylo</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4929" to="4937" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<title level="m">Yolov3: An incremental improvement. arXiv</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">LCR-Net: Localization-classification-regression for human pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Rogez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philippe</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting><address><addrLine>Honolulu, HI, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Cascade feature aggregation for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhihui</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guohui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianda</forename><surname>Sheng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.07837</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Deep high-resolution representation learning for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Deeppose: Human pose estimation via deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1653" to="1660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Mixture dense regression for object detection and human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Varamesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinne</forename><surname>Tuytelaars</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="13086" to="13095" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Point-set anchors for object detection, instance segmentation and pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangyun</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="527" to="544" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Convolutional pose machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shih-En</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ramakrishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Distribution-aware coordinate representation for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiatian</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanbin</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mao</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ce</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Human pose estimation with spatial contextual information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruigang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.01760</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dequan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Kr?henb?hl</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.07850</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">Objects as points. In arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">MonoCap: Monocular human motion capture using a cnn coupled with a geometric prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spyridon</forename><surname>Leonardos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Konstantinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Derpanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="901" to="914" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
