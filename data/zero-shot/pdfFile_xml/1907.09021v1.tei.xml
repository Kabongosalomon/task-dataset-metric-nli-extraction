<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">TARN: Temporal Attentive Relation Network for Few-Shot and Zero-Shot Action Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mina</forename><surname>Bishay</surname></persName>
							<email>m.a.t.bishay@qmul.ac.uk</email>
							<affiliation key="aff0">
								<address>
									<settlement>Ioannis Patras</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Zoumpourlis</surname></persName>
							<email>g.zoumpourlis@qmul.ac.uk</email>
							<affiliation key="aff1">
								<orgName type="laboratory">Multimedia and Vision Research Group Queen Mary University of London London</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">TARN: Temporal Attentive Relation Network for Few-Shot and Zero-Shot Action Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>BISHAY, ZOUMPOURLIS, PATRAS: TEMPORAL ATTENTIVE RELATION NETWORK 1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T04:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper we propose a novel Temporal Attentive Relation Network (TARN) for the problems of few-shot and zero-shot action recognition. At the heart of our network is a meta-learning approach that learns to compare representations of variable temporal length, that is, either two videos of different length (in the case of few-shot action recognition) or a video and a semantic representation such as word vector (in the case of zero-shot action recognition). By contrast to other works in few-shot and zero-shot action recognition, we a) utilise attention mechanisms so as to perform temporal alignment, and b) learn a deep-distance measure on the aligned representations at video segment level. We adopt an episode-based training scheme and train our network in an end-toend manner. The proposed method does not require any fine-tuning in the target domain or maintaining additional representations as is the case of memory networks. Experimental results show that the proposed architecture outperforms the state of the art in few-shot action recognition, and achieves competitive results in zero-shot action recognition.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Human action recognition has received significant attention in the last decade due to its application in areas like video surveillance, human-computer interaction, and video retrieval <ref type="bibr" target="#b12">[13]</ref>, as is the trend in most Computer Vision problems, using Deep Neural Networks <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b35">35,</ref><ref type="bibr" target="#b39">39]</ref>. However, training deep architectures requires a large amount of annotated data, something that is not easily available for new action classes. By contrast, humans are able to recognize new actions using a few labelled examples or just an action-related description.</p><p>For this reason, Few-Shot Learning (FSL) and Zero-Shot Learning (ZSL) have recently received a lot of attention. Most of the FSL works follow the meta-learning approach where a high-level transferable knowledge is learned on a collection of different tasks. This knowledge, that helps to perform classification on the target few-shot task(s), can be good initial network weights <ref type="bibr" target="#b6">[7]</ref>, embedding functions <ref type="bibr" target="#b36">[36,</ref><ref type="bibr" target="#b40">40]</ref>, or an external memory with useful information <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b34">34]</ref>. Some works treat the few-shot problem as a similarity problem, that is, a c 2019. The copyright of this document resides with its authors. It may be distributed unchanged freely in print or electronic forms. arXiv:1907.09021v1 [cs.CV] 21 Jul 2019 similarity model is trained to classify a query example by comparing it to labelled examples in the training set. These works are simple and need no additional memory or cost, however, most of them used a fixed distance metric to calculate the matching score. In <ref type="bibr" target="#b38">[38]</ref>, Sung et al. proposed a relation network that learns to calculate embeddings and a transferable deep measure of similarity between them. <ref type="bibr" target="#b38">[38]</ref> achieved state-of-the-art results in image-based FSL, and shows competitive performance when applied to image-based ZSL.</p><p>However, most of the works have focused on image-based FSL and ZSL problems, like object recognition <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b36">36,</ref><ref type="bibr" target="#b38">38,</ref><ref type="bibr" target="#b48">48]</ref>, while relatively few works were aimed at video-based FSL and ZSL problems like action recognition <ref type="bibr" target="#b42">[42,</ref><ref type="bibr" target="#b47">47,</ref><ref type="bibr" target="#b49">49]</ref>. Applying FSL and ZSL in videos is more challenging compared to images, due to the additional temporal dimension in videos and the variations that are introduced. To the best of our knowledge only one work has been proposed for few-shot action recognition <ref type="bibr" target="#b49">[49]</ref>. However, this work is based on memory networks that require extra computational and space resources, and use a single embedding vector to represent the entire video. This might not capture well the temporal structure of the action and is challenging due to the amount of information existing in it. Xu et al. <ref type="bibr" target="#b45">[45]</ref> proposed a method for action recognition from videos in limited data scenarios (i.e. when only a portion of the examples is available), however, training and testing is done on all the classes which is different from the few-shot protocol of <ref type="bibr" target="#b40">[40]</ref> that is typically used in such problems. All ZSL approaches for action recognition use a single vector as a visual representation of entire videos, obtained either from handcrafted <ref type="bibr" target="#b50">[50]</ref> or from deep <ref type="bibr" target="#b23">[24]</ref> features. Working on video level, thus being incapable to explicitly leverage time-specific information, they miss more detailed visual cues that appear on the fine-grained level of the segments that form a video.</p><p>Our network (TARN) addresses the few-shot problem by working at video-segment level to calculate the relation scores between a query video and other sample videos -the query video is then assigned with the label of the most related video in the sample set. The relation/similarity is calculated in two stages: the embedding stage and the relation stage. In the embedding stage, a C3D <ref type="bibr" target="#b39">[39]</ref> network followed by a layer of bidirectional Gated Recurrent Unit (GRU) <ref type="bibr" target="#b2">[3]</ref>, extract features from short segments of videos. The GRU learns an embedding function that is general and transferable over different tasks. In the relation stage, a segment-by-segment attention mechanism is used to align segment embeddings for a pair of query and sample videos, and then the aligned segments are compared. That is, we introduce segment-to-segment comparisons and model their temporal evolution, as a prior step towards video-to-video matching. The comparison outputs are fed to a deep neural network that learns a general deep distance measure for video matching, and gives at its output the final relation score for a pair of videos. Our approach can generalise to video-based ZSL, where no videos are available for the training in the sample set, but instead class descriptions (e.g. attribute vectors) are given. In this case, the query and the sample set have different types of data (visual and semantic, respectively) and therefore we use two different embedding modules, one for each domain. TARN (excluding the C3D model) is trained in an end-to-end manner in both FSL and ZSL cases. The main contributions of our work are three-fold:</p><p>1. We propose a relation network for few-shot and zero-shot action recognition. The proposed architecture compares either segment-wise visual features from a pair of videos (in FSL), or segment-wise visual features from a video with a class-wise semantic representation (in ZSL), retaining temporal information to finally perform video-wise classification.</p><p>2. The proposed architecture needs no additional resources like memory networks and does not require training or fine-tuning on the target problem like <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b29">30]</ref> 3. We test the proposed architecture on different benchmark datasets, achieving state-ofthe-art results in FSL and very competitive performance in ZSL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>Our approach aims to tackle the problems of few-shot and zero-shot action recognition, that are related to the following research directions: Few-shot learning: Since the seminal work of Fei-Fei et al. in <ref type="bibr" target="#b4">[5]</ref>, several works have been proposed on learning from few examples, focusing mostly on image classification. In <ref type="bibr" target="#b6">[7]</ref>, Finn et al. addressed the problem of FSL by focusing on fast adaptability through proper initialization conditions. The danger of overfitting on few-shot tasks when adopting finetuning <ref type="bibr" target="#b29">[30]</ref> has been noted in <ref type="bibr" target="#b40">[40]</ref>. A remedy to this has been the episode-based strategy of <ref type="bibr" target="#b40">[40]</ref>, where at each episode K support examples from each one of C classes (where K and C are typically small), and one query example, are randomly chosen and the network weights are updated according to a loss defined over them. In this way, the generalization is improved without the need to perform weight updates on the support set during inference. Other works <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b36">36,</ref><ref type="bibr" target="#b40">40]</ref> treated FSL as a metric-learning problem, where the query samples are classified using either pre-defined or learned distance measures on learned embeddings. The closest work to our FSL method is <ref type="bibr" target="#b38">[38]</ref>, where query images are classified by comparing them to images from the support set. In <ref type="bibr" target="#b38">[38]</ref>, a relation module learns a non-linear similarity function to match images. In our work, we propose a relation module that first uses Euclidean distance and cosine similarity for comparing video segments, and then a trainable deep network for modeling the temporal distances/similarities across different segments and inferring a relation score for each pair of videos, is subsequently learned.</p><p>Zero-shot learning was initially defined as a problem in the works of Palatucci et al. <ref type="bibr" target="#b26">[27]</ref> and Larochelle et al. <ref type="bibr" target="#b20">[21]</ref>. Action recognition in the ZSL scenario typically requires bridging the semantic gap between the distributions of the semantic representations and the visual representations from the unseen classes <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b42">42,</ref><ref type="bibr" target="#b46">46]</ref>. The semantic representation of a class is a single vector, which can be either class-related attributes or word2vec embedding of the class label <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b23">24]</ref>. The visual representations used in existing ZSL approaches are either handcrafted <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b47">47,</ref><ref type="bibr" target="#b50">50]</ref> based on the Improved Dense Trajectories (IDT) method <ref type="bibr" target="#b41">[41]</ref>, or deep <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b42">42]</ref> features extracted with C3D <ref type="bibr" target="#b39">[39]</ref> network. IDT features represent a video with a single vector, by default. C3D features refer to 16-frame video segments, but most of the ZSL methods average them over the entire video to obtain a single visual video representation. In ZSL, the embeddings of the semantic representations are mapped to the embeddings of the visual representations, to establish relationships between the class-related attributes and the visual features. In <ref type="bibr" target="#b7">[8]</ref>, a ZSL method was proposed, leveraging the interclass semantic relationships between the known and unknown actions. Other works have investigated using auxiliary data that are relevant to the unseen classes <ref type="bibr" target="#b47">[47]</ref>, using Wikipedia as an external ontology, to calculate semantic correlations between class labels <ref type="bibr" target="#b9">[10]</ref>, or constructing universal representations to achieve cross-dataset generalization <ref type="bibr" target="#b50">[50]</ref>. However, even in datasets of trimmed videos, factors such as camera motion, viewpoint or action structure complexity can obstruct the extraction of meaningful features at some video parts. Hence, building deep networks that are capable of processing segment-level visual features, is a promising direction for ZSL. In an C-way K-shot task (where K &gt; 1), the relation score of the query video to each class of the support set, is the average of the sample relation scores of that class.</p><p>Sequence matching: We claim that matching video sequences can benefit from comparing video segments as a first step. Previously, Fernando et al. <ref type="bibr" target="#b5">[6]</ref> have proposed a method to match video segments from a pair of videos, that share similar temporal evolutions. We also draw our inspiration from works on text matching, where representations of sequence parts are compared and aggregated to match an entire text sequence <ref type="bibr" target="#b44">[44]</ref>. An attention mechanism similar to that of <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b30">31]</ref> is used to align the segments of each video in the support set, with respect to the query video. This is done to semantically align the features of the support set videos to the features of the query video. It also transforms the number of segments for each video of the support set to be equal to that of the query video.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Proposed Architecture</head><p>In this section we introduce a novel deep architecture, named Temporal Attentive Relation Network (TARN) for the problems of Few-Shot Learning (FSL) and Zero-Shot Learning (ZSL) for video-based tasks. <ref type="figure" target="#fig_0">Figure 1</ref> shows an overview of the network. TARN learns to compare a query video against a sample set of videos in FSL, or semantic attributes in ZSL, representing a group of actions. In the FSL case, the inputs are segment-wise visual representations of the query and sample videos, and in the ZSL case the inputs are segmentwise visual representations of a query video on the one hand and semantic representations of the unseen classes on the other. The output is a relation score, either for each pair of videos, or for each pair of video and semantic attributes. More specifically, TARN consists of two modules: the embedding module and the relation module. First, the embedding module processes the visual or semantic representations, retaining the temporal structure of visual features, and produces embeddings that are later compared. Second, the relation module initially applies segment-by-segment attention. By doing so, it either transforms the visual representations of the sample set to have the same number of segments as the query video (FSL), or transforms the semantic representations of the unseen classes to allow a segment-wise comparison between them and the visual representations (ZSL). Afterwards, a per-segment comparison is performed. Finally the relation module produces the matching score by taking into account the variations of the comparisons across all segments of the query video. These modules are explained in detail in the following subsections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Embedding Module</head><p>TARN uses a single embedding module to embed both the query and sample videos in FSL, while two different embedding modules are used in ZSL. That is, one is used to embed the visual data while the other is used to embed the semantic data. Video embedding: A pre-trained C3D network is used to extract spatio-temporal features across short segments of videos. This network acts as a local feature extractor in the embedding module. Then, a bi-directional GRU uses the local features to learn more globally-aware features, allowing each time step to access both backward and forward information across the whole video. Moreover, it reduces the dimension of the C3D features. This visual embedding module is applied in the same way to embed video segments in both the FSL and ZSL cases. Semantic embedding: ZSL first compares visual representations extracted from segments of the query video to semantic representations of the sample set classes. The original semantic information needs to be encoded into a representation that allows a feature-rich comparison of video segments to class-related attributes, and therefore needs to have the same dimensions with the visual representation of the video segments. The semantic embedding module consists of two stacked fully-connected (FC) layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Relation Module</head><p>In this section we will explain the relation module from the FSL perspective, and then we will show how it is extended for ZSL. In FSL, in order to match a query video to a sample set of videos, firstly we pair the query video with each video in the sample set. Given pairs of videos, secondly we align segments in the videos using a segment-by-segment attention layer. The attention layer maps the sample video to have the same number of segment embeddings as the query. Third, each segment in the query is compared to the corresponding aligned sample segment. Fourth, the comparison outputs of the different segments are fed to a deep neural network, that learns a deep metric for video matching, and gives at its output a relation score for each pair. Finally, a softmax layer is used to map the relation scores to a probability distribution over the sample classes.</p><p>Segment-by-segment attention: Several recent works in text sequence matching and textual entailment use an attention mechanism, named word-by-word attention, to align the words of two given sentences <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b44">44]</ref>. Similarly, as shown in the corresponding block of <ref type="figure" target="#fig_0">Fig. 1</ref>, we adopt the word-by-word attention in our architecture to align the sample and query segment-embeddings (i.e. segment-by-segment attention). Given a sample video S S S ? R N?d and a query video Q Q Q ? R M?d , where each row in S S S and Q Q Q represents a segmentembedding vector of dimension d, and where N and M denote the number of segments in videos S S S and Q Q Q respectively. The segment-by-segment attention is calculated as follows:</p><formula xml:id="formula_0">A A A = so f tmax((S S SW W W + b b b ? e e e N )Q Q Q T ), H H H = = = A A A T T T S S S,<label>(1)</label></formula><p>where W W W ? R d?d and b b b ? R d are parameters to be learned, and the operator "?e e e N " repeats the bias vector b b b, N times to form a matrix of dimension Deep metric learning: The relation module performs deep similarity/distance metric learning by using a comparison layer and a non-linear classifier on the top of it. The comparison layer calculates a similarity measure between each of the M segments of the row vectors of Q Q Q ? R M?d and H H H ? R M?d . This measure, as described in <ref type="bibr" target="#b44">[44]</ref>, can be based on one of the following operations: multiplication (Mult), subtraction (Subt), neural network (NN), subtraction and multiplication followed by a neural network (SubMultNN), or Euclidean distance and cosine similarity (EucCos). Since the measure is estimated at each of the M pairs of segments, the output of this layer has also M dimensions. This layer acts as an intermediate stage that produces low-level representations of the comparisons between the sample and the query segments. As we will show in the experimental section, decomposing the query-sample matching problem into several comparisons across segments performs better than just a single comparison of two vectors representing the sample and query videosthis coincides with the findings in text sequence matching problems <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b43">43]</ref>.</p><p>Unlike other works in FSL which used a linear classifier or a fixed metric to match query and sample examples <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b36">36]</ref>, we follow <ref type="bibr" target="#b38">[38]</ref> and use a deep neural network for deep metric learning. That is, the outputs of the comparison layer are passed to the deep network that learns a global deep metric over the entire videos. For FSL, we use a uni-directional GRU to learn temporal information across different segment comparisons and a Fully-Connected (FC) layer for giving the final relation score. The final relation scores coming from different sample examples are passed to a softmax layer, so that they can be mapped to a probability distribution over the sample classes. The query video is assigned with the label of the most related video in the sample set. In the case of multiple shots (K &gt; 1) per class in the sample set, the mean of the relation scores over the shots of each class is taken as the relation score of the query video with that class.</p><p>The full architecture (excluding the C3D model) is trained in an end-to-end fashion. We use episode-based training scheme proposed in <ref type="bibr" target="#b36">[36,</ref><ref type="bibr" target="#b40">40]</ref> to train our architecture, and binary cross-entropy as the cost function. The total batch/episode cost is:</p><formula xml:id="formula_1">L(t, q) = ? 1 KC K ? k=1 C ? c=1 (t kc log q kc + (1 ? t kc ) log(1 ? q kc ))<label>(2)</label></formula><p>where K denotes the number of shots, C the number of classes in each episode, t the target relation score, and q the predicted relation value.</p><p>In zero-shot learning, each of the sample set classes is expressed by a semantic attribute/word vector. In this case, the attention mechanism estimates the similarity between the semantic vector and each segment of the query video, instead of aligning video segments as in the FSL case. The semantic embedding module encodes class attributes in representations that allow finding relations between the underlying class attributes and query video segments. We perform several segment-to-attribute comparisons, leveraging this finegrained information to improve the training process and reduce overfitting. The comparison outputs are aggregated over all query video segments using two FC layers and an average pooling layer that produces the final relation score. The comparison, FC, and pooling layers represent in this case the deep network for metric learning. Note that the uni-directional GRU of the FSL case has been replaced, as there is no temporal alignment information to learn in the deep network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Few-shot action recognition</head><p>Implementation details. In <ref type="bibr" target="#b49">[49]</ref>, Zhu and Yang introduced a dataset for few-shot video classification, that is a modification of the original Kinetics dataset <ref type="bibr" target="#b15">[16]</ref>. In this work, we follow <ref type="bibr" target="#b49">[49]</ref> and use their dataset and evaluation protocol. The dataset videos are sliced into fixed length segments of 16 frames each, and then these segments are fed to a C3D network pre-trained on Sports-1M <ref type="bibr" target="#b14">[15]</ref>. Visual features are extracted from the last FC layer (i.e. FC7) of the C3D network, and used as input to the embedding bidirectional GRU. The dimension of the C3D features is 4096, and the GRU has a hidden state of size 256. In the relation module, the output of the comparison layer is used as input to a unidirectional GRU layer of size 256, and the GRU output at the last time step is fed to an FC layer with a single neuron for predicting the final relation score. We use 20, 000 episodes for training, 500 for validation, and 1, 000 for testing. Each episode has a sample set of 5 classes. We evaluate the performance of our architecture on the validation set every 500 training episodes. The best-performing model on the validation set is used for testing. We train our architecture using Stochastic Gradient Descent (SGD) with momentum m = 0.9, and learning rate equal to 10 ?3 for the first 10k training episodes, and 10 ?4 for the last 10k episodes.</p><p>Results. In our first experiment, we investigate the impact of the various functions that can be used as distance/similarity measure in the comparison layer, on the TARN performance. Following <ref type="bibr" target="#b44">[44]</ref>, we compare five different distance measures (Mult, Subt, NN, Sub-MultNN, EucCos). <ref type="table" target="#tab_2">Table 1</ref> shows the accuracy obtained by the TARN model over the different measures. EucCos leads to the best accuracy over all shots. Although EucCos is a fixed measure with no learnable parameters, the following layers in the relation module are trainable and non-linear.</p><p>In the next experiment, we investigate the benefits of using segment-by-segment attention and comparing segment-wise representations in our architecture. To do so, we compare the TARN model to another model that has no attention layer and performs a single comparison for each video pair. Specifically, we modify the embedding module by replacing the bidirectional GRU with a unidirectional GRU of size 256, and the relation module by replacing the unidirectional GRU with a FC layer of size 256 for deep metric learning. In this case, the embedding GRU output at the last time step summarizes the entire video into a single vector. We call this model "TARN-single".   <ref type="table" target="#tab_1">Table 2</ref>: Accuracies of the the state-of-the-art method <ref type="bibr" target="#b49">[49]</ref>, as well as the TARN model at different settings.</p><p>by both the TARN and TARN-single models. Aligning video segments through attention and comparing segment-wise embeddings leads to consistent gains over the different shots. Furthermore, TARN model achieves better accuracy than the state-of-the-art method <ref type="bibr" target="#b49">[49]</ref>. The accuracy gains hold for all shots, with significant boosts in the more difficult one-shot case, showing that even with a single sample per class in the sample set, the TARN model can still perform well. In the last experiment, we compare the TARN model to the architecture proposed in <ref type="bibr" target="#b49">[49]</ref> when using the same visual features. In the comparison, we use features extracted from a ResNet-50 <ref type="bibr" target="#b11">[12]</ref> model pretrained on ImageNet <ref type="bibr" target="#b32">[33]</ref>. ResNet features are extracted every 3 frames. We modify the TARN model to use the frame-level ResNet-50 features instead of the C3D ones, by replacing the bidirectional GRU in the embedding module by a unidirectional GRU, that is applied over video segments and gives an output at the last time step of each segment. The rest of the network remains the same to that of the original TARN model. This model is called "TARN-f". In this comparison, we also show the effect of not using attention and segment-level comparisons. To do so, the unidirectional GRU is applied over the entire video, and then the GRU output at the last time step is fed to the comparison layer. This model is called "TARN-f-single". <ref type="table" target="#tab_1">Table 2</ref> shows the results obtained by both the TARN-f and TARN-f-single models. First, we can see that TARN-f performs better than CMN <ref type="bibr" target="#b49">[49]</ref> when using the same visual features (ResNet-50). Second, using attention and segment-wise comparisons improves the performance of our architecture. Finally, the C3D features perform better than the ResNet features, when trying to compare video segments in a few-shot scenario. This is probably due to the fact that C3D features are spatiotemporal, while ResNet features are static.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Zero-shot action recognition</head><p>Datasets and settings: We use two action recognition datasets to evaluate our architecture, namely UCF-101 <ref type="bibr" target="#b37">[37]</ref> and HMDB51 <ref type="bibr" target="#b19">[20]</ref>. UCF-101 has 13320 video clips from 101 classes, while HMDB51 has 6766 clips from 51 classes. Following <ref type="bibr" target="#b42">[42]</ref>, we divide the 101 actions in UCF-101 into 51/50 and 81/20 (seen/unseen) classes. For HMDB51, we divide the 51 actions into 26/25 classes. In the literature, different numbers of splits (ranging between 5-50) are randomly generated for the seen/unseen classes, and the mean accuracy and standard deviation are reported over them. In this work, we follow <ref type="bibr" target="#b42">[42]</ref> and use 30 random splits used by <ref type="bibr" target="#b46">[46]</ref> for the 51/50 and 81/20 cases in UCF-101, and for the 26/25 case in HMDB51.</p><p>Video/class representations: In our experiments, we use two types of semantic representations, both of which are widely used in the literature. First, we use the 115 binary semantic attributes (denoted as "Attr") that are manually annotated by <ref type="bibr" target="#b13">[14]</ref> for UCF-101. To the best of our knowledge, there are no semantic attributes available for HMDB51. Second, we use 300-dimensional Word Vectors (mentioned as "WV") generated by the skip-gram model of <ref type="bibr" target="#b22">[23]</ref>, that is trained on the Google News dataset. While there are other ways of representing semantic information, an extensive analysis of their influence is beyond the scope of this work. For example, <ref type="bibr" target="#b28">[29]</ref> showed better performance using Error-Correcting Output Codes (ECOC). Here, we follow the majority of the works and use attributes and/or word vectors. Similar to FSL, we use C3D for extracting visual features.</p><p>Implementation details: The C3D features are embedded using a bidirectional GRU layer with a hidden state of size 256. Hence, the output dimension of the GRU at each time step (i.e. video segment) is of size 512. The semantic information is embedded using two FC layers with 4096 and 512 nodes. The deep network used for metric learning has two FC layers of size 256 and 1. We train our architecture in an end-to-end fashion using episode-based training strategy <ref type="bibr" target="#b36">[36,</ref><ref type="bibr" target="#b40">40]</ref>. Adam optimizer <ref type="bibr" target="#b16">[17]</ref> with an initial learning rate set to 10 ?4 and gradient clipping to 0.5 is used in the training. The architecture is trained for 3, 000 episodes and tested for 100 episodes. During training, episodes/batches of size 16 for UCF-101 and 8 for HMDB51 are used, while in testing episodes are formed from all the unseen classes in the target split. We evaluate our architecture every 50 training episodes.</p><p>Ablation studies: In <ref type="table" target="#tab_4">Table 3</ref> we show the results obtained by our architecture on ZSL using different settings. The first and second settings (first two rows in <ref type="table" target="#tab_4">Table 3</ref>) show the performance of our architecture when having a single representation for an entire video, instead of multiple segment representations. In the first setting, we use a unidirectional GRU to summarize the video segments into a single vector, obtained from the last time step of the GRU, and perform a single comparison between the semantic and the video vectors. In the second setting, we use a bidirectional GRU that gives an output at each time step. The attention mechanism is applied to the visual and semantic embedding, so as to map the multiple segment representations of the query video into a single representation. The single aligned query representation is then compared to the semantic embedding. The third setting (third row in <ref type="table" target="#tab_4">Table 3</ref>) is a network that performs per-segment comparisons between the visual and semantic features. The attention mechanism maps the single semantic embedding of each class into multiple representations, as many, as the number of segments in the query video. In this way, each segment of the query video is compared to a semantic representation, allowing us to find relations between class-related semantic attributes and the visual features of each segment. Regarding the single comparison cases, the results show that the attention mechanism (second row in <ref type="table" target="#tab_4">Table 3</ref>) effectively encodes multiple segment features into a single representation. The best performance is achieved when performing multiple segmentto-attribute comparisons.</p><p>Comparison to state of the art in ZSL: Methods proposed in the literature for ZSL have used a wide range of testing settings. In order to have a common setting across the majority of works, we do not compare with works that: (1) use auxiliary data to augment the training set <ref type="bibr" target="#b46">[46,</ref><ref type="bibr" target="#b47">47,</ref><ref type="bibr" target="#b50">50]</ref>; (2) fuse different semantic or visual features <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b42">42]</ref>; or (3) require access to the testing (unseen) classes during training (also known as "transductive" setting) <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b42">42,</ref><ref type="bibr" target="#b46">46,</ref><ref type="bibr" target="#b47">47]</ref>. This allows us to have a clear and model-based comparison to   <ref type="table">Table 4</ref>: Accuracies of the TARN model as well as other state-of-the-art methods on zeroshot action recognition on the UCF-101 and HMDB51 datasets. Results marked with " ?" are reported as reproduced by <ref type="bibr" target="#b47">[47]</ref>.</p><p>the literature. <ref type="table">Table 4</ref> summarizes the comparison results over the UCF-101 (51/50), UCF-101 (81/20), and HMDB51 (26/25) splits. We state in <ref type="table">Table 4</ref> the type of semantic and visual representation used in each method. Our architecture achieves the best results over the UCF-101 51/50 and 81/20 splits with almost 0.5% and 3%, respectively, in comparison to the second best performing methods. For HMDB51, we only get lower results than works that use either Improved Dense Trajectories (IDT) as visual features <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b50">50]</ref>, or ECOC as semantic representation <ref type="bibr" target="#b28">[29]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this work, we propose a deep network (called TARN) for addressing the problem of fewshot and zero-shot action recognition. TARN includes an embedding module for encoding the query and sample set examples, and a relation module that utilize attention for performing temporal alignment and a deep network for learning deep distance measure on the aligned representations at video segment level. The proposed network requires no additional resources or fine-tuning on the target problem. Our experimental results show that using attention and comparing segment-wise representations benefit the video-to-video or videoto-vector matching, compared to using video-wise representations. Furthermore, our method achieves the state-of-the-art results in FSL and very competitive performance in ZSL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Acknowledgments</head><p>The work of Mina Bishay is supported by the Newton-Mosharafa PhD scholarship, which is jointly funded by the Egyptian Ministry of Higher Education and the British Council. This research has also been supported by EPSRC under grant No. EP/R026424/1. We gratefully acknowledge NVIDIA for the donation of the GTX Titan X GPU used for this research.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>The proposed TARN architecture, consisting of the embedding module and the relation module.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>N ? d. A A A ? R N?M is the attention weight matrix and H H H is the aligned version of S S S. Each row vector in H H H is a weighted sum of the S S S segment-embeddings, and represents the parts of S S S that are most similar to the corresponding row vector (segment-embedding) of Q Q Q. The row vectors of Q Q Q and H H H are used as inputs to a comparison layer.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2</head><label>2</label><figDesc>shows the obtained accuracies</figDesc><table><row><cell>Method</cell><cell>Measure</cell><cell>1-shot</cell><cell>2-shot</cell><cell>3-shot</cell><cell>4-shot</cell><cell>5-shot</cell></row><row><cell>TARN</cell><cell>Mult*</cell><cell>63.10</cell><cell>71.16</cell><cell>74.08</cell><cell>76.36</cell><cell>75.64</cell></row><row><cell>TARN</cell><cell>Subt*</cell><cell>64.82</cell><cell>70.70</cell><cell>73.90</cell><cell>76.26</cell><cell>77.54</cell></row><row><cell>TARN</cell><cell>NN*</cell><cell>63.26</cell><cell>70.46</cell><cell>72.70</cell><cell>75.62</cell><cell>75.58</cell></row><row><cell cols="2">TARN SubMultNN*</cell><cell>66.10</cell><cell>73.74</cell><cell>75.44</cell><cell>77.08</cell><cell>78.20</cell></row><row><cell>TARN</cell><cell>EucCos*</cell><cell>66.55</cell><cell>74.56</cell><cell>77.33</cell><cell>78.89</cell><cell>80.66</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table><row><cell>Method</cell><cell>Features</cell><cell>1-shot</cell><cell>2-shot</cell><cell>3-shot</cell><cell>4-shot</cell><cell>5-shot</cell></row><row><cell>CMN [49] -ECCV 2018</cell><cell></cell><cell>60.5</cell><cell>70.0</cell><cell>75.6</cell><cell>77.3</cell><cell>78.9</cell></row><row><cell>TARN-f-single</cell><cell>ResNet-50</cell><cell>57.92</cell><cell>63.96</cell><cell>65.90</cell><cell>68.52</cell><cell>70.64</cell></row><row><cell>TARN-f</cell><cell></cell><cell>64.83</cell><cell>72.94</cell><cell>76.22</cell><cell>78.02</cell><cell>78.52</cell></row><row><cell>TARN-single TARN</cell><cell>C3D</cell><cell>62.84 66.55</cell><cell>69.80 74.56</cell><cell>73.96 77.33</cell><cell>74.88 78.89</cell><cell>76.88 80.66</cell></row></table><note>TARN model accuracy when using different similarity/distance measures in the comparison layer.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>TARN (with attention, single comparison) 20.0?2.5 38.1?5.6 17.4?2.8 TARN (with attention, multi comparison) 23.2?2.9 42.7?5.4 19.5?4.2</figDesc><table><row><cell>Method</cell><cell cols="2">UCF-101 UCF-101 HMDB51 (51/50) (81/20) (26/25)</cell></row><row><cell>TARN (w/o attention, single comparison)</cell><cell>16.7?4.0 35.8?5.9</cell><cell>16.6?3.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Accuracies of the TARN model at different settings on zero-shot action recognition on the UCF-101 and HMDB51 datasets.</figDesc><table><row><cell>Method</cell><cell cols="2">Visual Semantic Repr. Repr.</cell><cell cols="3">UCF-101 UCF-101 HMDB51 (51/50) (81/20) (26/25)</cell></row><row><cell>ESZSL ? [32] -ICML 2015</cell><cell>IDT</cell><cell>WV</cell><cell>15.0?1.3</cell><cell>-</cell><cell>18.5?2.0</cell></row><row><cell>SJE ? [1] -CVPR 2015</cell><cell>IDT</cell><cell>Attr WV</cell><cell>12.0 ? 1.2 9.9 ? 1.4</cell><cell>--</cell><cell>-13.3 ? 2.4</cell></row><row><cell>UDICA [9] -CVPR 2016 KDICA [9] -CVPR 2016</cell><cell>C3D</cell><cell>Attr</cell><cell>--</cell><cell>29.6?1.2 31.1?0.8</cell><cell>--</cell></row><row><cell>MTE [47] -ECCV 2016</cell><cell>IDT</cell><cell>Attr WV</cell><cell>18.3?1.7 15.8?1.3</cell><cell>--</cell><cell>-19.7?1.6</cell></row><row><cell></cell><cell></cell><cell>Attr</cell><cell>3.2?0.7</cell><cell>-</cell><cell>-</cell></row><row><cell>ZSECOC [29] -CVPR 2017</cell><cell>IDT</cell><cell>WV</cell><cell>13.7?0.5</cell><cell>-</cell><cell>16.5?3.9</cell></row><row><cell></cell><cell></cell><cell>ECOC</cell><cell>15.1?1.7</cell><cell>-</cell><cell>22.6?1.2</cell></row><row><cell>BiDiLEL [42] -IJCV 2017</cell><cell>C3D</cell><cell>Attr WV</cell><cell>20.5?0.5 18.9?0.4</cell><cell>39.2?1.0 38.3?1.2</cell><cell>-18.6?0.7</cell></row><row><cell>GMM [24] -WACV 2018</cell><cell>C3D</cell><cell>Attr WV</cell><cell>22.7?1.2 17.3?1.1</cell><cell>--</cell><cell>-19.3?2.1</cell></row><row><cell>UAR [50] -CVPR 2018</cell><cell>IDT</cell><cell>WV</cell><cell>17.5?1.6</cell><cell>-</cell><cell>24.4?1.6</cell></row><row><cell>TARN</cell><cell>C3D</cell><cell>Attr WV</cell><cell>23.2?2.9 19.0?2.3</cell><cell>42.7?5.4 36.0?5.3</cell><cell>-19.5?4.2</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Evaluation of output embeddings for fine-grained image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeynep</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Walter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2927" to="2936" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations</title>
		<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-05-07" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014-10" />
			<biblScope unit="page" from="1724" to="1734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Long-term recurrent convolutional networks for visual recognition and description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lisa</forename><forename type="middle">Anne</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhashini</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="677" to="691" />
			<date type="published" when="2017-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">One-shot learning of object categories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="594" to="611" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Unsupervised human action detection by action matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Basura</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sareh</forename><surname>Shirazi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Gould</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Model-agnostic meta-learning for fast adaptation of deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="1126" to="1135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Exploring semantic inter-class relationships (SIR) for zero-shot action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yueting</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence</title>
		<meeting>the Twenty-Ninth AAAI Conference on Artificial Intelligence<address><addrLine>Austin, Texas, USA.</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3769" to="3775" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning attributes equals multi-source domain generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianbao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boqing</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Las Vegas, NV, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-06-27" />
			<biblScope unit="page" from="87" to="97" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Recognizing an action using its name: A knowledge-based approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linchao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deli</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yueting</forename><surname>Zhuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">120</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="61" to="77" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Pairwise word interaction modeling with deep neural networks for semantic similarity measurement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="937" to="948" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Identity mappings in deep residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2016 -14th European Conference</title>
		<meeting><address><addrLine>Amsterdam, The Netherlands</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="630" to="645" />
		</imprint>
	</monogr>
	<note>Proceedings, Part IV</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Going deeper into action recognition: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samitha</forename><surname>Herath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehrtash</forename><surname>Harandi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fatih</forename><surname>Porikli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and vision computing</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="page" from="4" to="21" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-G</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<title level="m">THUMOS challenge: Action recognition with a large number of classes</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Large-scale video classification with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanketh</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Computer Vision and Pattern Recognition</title>
		<meeting>International Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chloe</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudheendra</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Back</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Natsev</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.06950</idno>
		<title level="m">The kinetics human action video dataset</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations</title>
		<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-05-07" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Siamese neural networks for one-shot image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML deep learning workshop</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation for zero-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elyor</forename><surname>Kodirov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenyong</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaogang</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2452" to="2460" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">HMDB: A large video database for human motion recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hildegard</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hueihan</forename><surname>Jhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Est?baliz</forename><surname>Garrote</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tomaso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Poggio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Serre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision, ICCV 2011</title>
		<meeting><address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2556" to="2563" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Zero-data learning of new tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno>978-1-57735-368-3</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd National Conference on Artificial Intelligence</title>
		<meeting>the 23rd National Conference on Artificial Intelligence</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2008" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="646" to="651" />
		</imprint>
	</monogr>
	<note>AAAI&apos;08</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Recognizing human actions by attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kuipers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<idno>978-1-4577- 0394-2</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2011 IEEE Conference on Computer Vision and Pattern Recognition, CVPR &apos;11</title>
		<meeting>the 2011 IEEE Conference on Computer Vision and Pattern Recognition, CVPR &apos;11</meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="3337" to="3344" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A generative approach to zero-shot and few-shot action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vinay Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Verma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shiva Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piyush</forename><surname>Arulkumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Rai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mittal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE Winter Conference on Applications of Computer Vision</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="372" to="380" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Meta networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsendsuren</forename><surname>Munkhdalai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="2554" to="2563" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Discriminative convolutional fisher vector network for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petar</forename><surname>Palasek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Patras</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.06119</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Zero-shot learning with semantic output codes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Palatucci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dean</forename><surname>Pomerleau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom M</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2009" />
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="1410" to="1418" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A decomposable attention model for natural language inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankur</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>T?ckstr?m</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dipanjan</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2016-11" />
			<biblScope unit="page" from="2249" to="2255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Zero-shot action recognition with error-correcting output codes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fumin</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingbing</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2833" to="2842" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Optimization as a model for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sachin</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">5th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Toulon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-04-24" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Reasoning about entailment with neural attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Rockt?schel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><forename type="middle">Moritz</forename><surname>Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Kocisky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">An embarrassingly simple approach to zero-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernardino</forename><surname>Romera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-</forename><surname>Paredes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2152" to="2161" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
		<imprint>
			<pubPlace>Alexander C</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">ImageNet Large Scale Visual Recognition Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Meta-learning with memory-augmented neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Bartunov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Botvinick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">P</forename><surname>Lillicrap</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33nd International Conference on Machine Learning</title>
		<meeting>the 33nd International Conference on Machine Learning<address><addrLine>New York City, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-06-19" />
			<biblScope unit="page" from="1842" to="1850" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="568" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Prototypical networks for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jake</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Ucf101: A dataset of 101 human actions classes from videos in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khurram</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Amir Roshan Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shah</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.0402</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Learning to compare: Relation network for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Flood</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongxin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">M</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1199" to="1208" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3d convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubomir</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4489" to="4497" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Matching networks for one shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th International Conference on Neural Information Processing Systems, NIPS&apos;16</title>
		<meeting>the 30th International Conference on Neural Information Processing Systems, NIPS&apos;16</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3637" to="3645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Action recognition with improved trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3551" to="3558" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Zero-shot visual recognition via bidirectional latent embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Chen</surname></persName>
		</author>
		<idno>0920-5691</idno>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vision</title>
		<imprint>
			<biblScope unit="volume">124</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="356" to="383" />
			<date type="published" when="2017-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Machine comprehension using match-lstm and answer pointer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuohang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">5th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Toulon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-04-24" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">A compare-aggregate model for matching text sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuohang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">5th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Toulon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-04-24" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Dense dilated network for few shot action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baohan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingbin</forename><surname>Hao Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Gang</forename><surname>Luwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 ACM on International Conference on Multimedia Retrieval</title>
		<meeting>the 2018 ACM on International Conference on Multimedia Retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="379" to="387" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Semantic embedding space for zero-shot action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Hospedales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaogang</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on Image Processing (ICIP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="63" to="67" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Multi-task zero-shot action recognition with prioritised data augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaogang</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="343" to="359" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Learning a deep embedding model for zero-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaogang</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2021" to="2030" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Compound memory networks for few-shot video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linchao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="751" to="766" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Towards universal representation for unseen action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shawn</forename><surname>Newsam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
