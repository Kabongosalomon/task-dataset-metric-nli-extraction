<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">RDP-Net: Region Detail Preserving Network for Change Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongjia</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE</roleName><forename type="first">Fangling</forename><surname>Pu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Yang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Tang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Xu</surname></persName>
						</author>
						<title level="a" type="main">RDP-Net: Region Detail Preserving Network for Change Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T05:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-change detection</term>
					<term>deep learning</term>
					<term>training strat- egy</term>
					<term>optical remote sensing images</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Change detection (CD) is an essential earth observation technique. It captures the dynamic information of land objects. With the rise of deep learning, neural networks (NN) have shown great potential in CD. However, current NN models introduce backbone architectures that lose detailed information during learning. Moreover, current NN models are heavy in parameters, which prevents their deployment on edge devices such as UAVs. In this work, we tackle this issue by proposing RDP-Net: a region detail preserving network for CD. We propose an efficient training strategy that constructs the training tasks during the warmup period of NN training and lets the NN learn from easy to hard. The training strategy enables NN to learn more powerful features with fewer FLOPs and achieve better performance. Next, we propose an effective edge loss that increases the penalty for errors on details and improves the network's attention to details such as boundary regions and small areas. Furthermore, we provide a NN model with a brand new backbone that achieves the state-of-the-art empirical performance in CD with only 1.70M parameters. We hope our RDP-Net would benefit the practical CD applications on compact devices and could inspire more people to bring change detection to a new level with the efficient training strategy.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>C HANGE detection (CD) reports the temporal dynamics of the studied area by observing it at different times <ref type="bibr" target="#b0">[1]</ref>. In geoscience, the observation is conducted through the remote sensing technique <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr">[3]</ref>. With the information interpreted from multi-temporal satellite images, CD benefits applications such as urban planning <ref type="bibr" target="#b3">[4]</ref>, environment monitoring <ref type="bibr" target="#b4">[5]</ref>- <ref type="bibr" target="#b6">[7]</ref>, disaster assessment <ref type="bibr" target="#b7">[8]</ref>- <ref type="bibr" target="#b10">[11]</ref> and resource management <ref type="bibr" target="#b11">[12]</ref>.</p><p>Traditional CD methods can be divided into two categories: Pixel-based and Object-based <ref type="bibr" target="#b12">[13]</ref>. The Pixel-based CD methods generate a change map by comparing multi-temporal remote sensing images pixel by pixel. The representatives are change vector analysis (CVA) <ref type="bibr" target="#b13">[14]</ref>, principal component analysis (PCA) <ref type="bibr" target="#b14">[15]</ref>, independent component analysis (ICA) <ref type="bibr" target="#b15">[16]</ref>, etc. Recently, some researchers use neural networks to extract deep change vectors (DCVA) <ref type="bibr" target="#b16">[17]</ref>- <ref type="bibr" target="#b18">[19]</ref> and have made some  achievements, especially in the field of unsupervised learning. However, Pixel-based CD methods have limited capability in capturing spatial context information and complex visual features. The Object-based methods segment the raw image into different categories and then obtain the change map via comparison <ref type="bibr" target="#b19">[20]</ref>. However, it is usually difficult to set suitable parameters for segmentation, limiting the performance. Traditional CD methods tend to have high requirements for image registration, leading to a strong dependence on pre-processing approaches.</p><p>Recently, neural networks (NN) have demonstrated empirical success in plenty of geoscience applications such as seismic imaging <ref type="bibr" target="#b20">[21]</ref>, scene classification <ref type="bibr" target="#b1">[2]</ref>, texture evaluation <ref type="bibr" target="#b21">[22]</ref>, etc. In the field of CD, NN has also demonstrated strong capabilities. Current NN models can be mainly divided into two categories according to their backbone, U-Net-like <ref type="bibr" target="#b22">[23]</ref> methods and ResNet/VGG-based <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref> methods. Daudt et al. <ref type="bibr" target="#b25">[26]</ref> combined Siamese network with U-Net and proposed FC-EF, FC-Siam-conc and FC-Siam-diff. Siamese network was used to extract feature maps from the multitemporal, and U-Net was used to sense the combination of feature maps. These networks are usually recognized as the cornerstone of deep learning-based change detection. Peng et al. <ref type="bibr" target="#b26">[27]</ref> and Fang et al. <ref type="bibr" target="#b27">[28]</ref> proposed UNet++ MSOF and SNUNet-CD based on Daudt's work. They replaced U-Net with U-Net++ <ref type="bibr" target="#b28">[29]</ref>, and fused the multiple side outputs of U-Net++, which achieves a better performance in the field of change detection. Chen et al. <ref type="bibr" target="#b29">[30]</ref> proposed DASNet. ResNet and VGG are used as backbone to extract feature maps from the multi-temporal. A dual attention mechanism was proposed to fuse the feature map from different depths. However, existing methods cannot tackle the major challenges of CD: characterizing the dynamics of the boundary regions and small areas. An example is demonstrated in <ref type="figure" target="#fig_1">Fig. 1</ref>. The changed area contains acres of buildings and a few country roads. Although the up-to-date methods have good intuition on general changed areas, they failed to identify the boundary regions and had poor results in modeling the changes in the roads. Only our region detail preserving network (RDP-Net) can detect the changed buildings and roads well.</p><p>Can we build a compact neural network with better modeling of details for change detection?</p><p>In this paper, we answer this question positively by proposing RDP-Net: a region detail preserving network for CD. We provide an efficient training strategy, an effective edge loss and a brand new NN backbone focusing on preserving regions' details. These three factors strengthen the performance of our method in CD.</p><p>First of all, CD in remote sensing images faces various scenes. The difficulty of detection varies for different scenes. From the perspective of model training, network training becomes more difficult. Curriculum learning <ref type="bibr" target="#b30">[31]</ref> suggests that simple, basic tasks are more useful for network training. Specifically, in this paper, an efficient training strategy is proposed to split the training dataset into different subsets by difficulty level, and let the NN learn from easy to hard. We argue that it is beneficial to warm up the NN with samples from easier areas. Next, our model would learn harder subtle areas better with previous training knowledge. Besides, one idea for curriculum learning is to reduce the impact of hard samples during training <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b32">[33]</ref>. We believe that in CD, hard samples can help further improve the performance of the network in the last training stage, so our efficient training strategy can achieve better results in CD.</p><p>Then, the boundary regions and small areas are not wellprocessed and are still a challenging problem in remote sensing change detection. The bad boundary performance turns out to be the bottleneck of change detection accuracy. However, according to the analysis, the boundary regions and small areas are quite undervalued in the current NN. In the existing methods, the most commonly used loss functions are CrossEntropy, Balanced Cross Entropy, Focal Loss <ref type="bibr" target="#b33">[34]</ref>, Dice Loss <ref type="bibr" target="#b34">[35]</ref> and IOU Loss <ref type="bibr" target="#b35">[36]</ref>, which mainly aim at dealing with class imbalance problems without well considering the boundary regions and small areas. Obviously, determining the boundaries of the changed areas is more valuable than determining the internal regions in the field of CD. There should be a greater penalty for errors in the boundary regions. Yet little attention is currently paid to this issue. The Boundary Loss proposed by Hoel et al. <ref type="bibr" target="#b36">[37]</ref> is designed for medical image segmentation, which is not suitable for change detection using remote sensing images, especially when it comes to complex scenarios. Thus, an Edge Loss by increasing penalty for errors on the boundary regions is proposed. We argue that our edge loss helps the network to pay attention to the details such as boundary regions and small areas, leading to accurate detection of the details.</p><p>At last, we argue that current NN models have limited success in CD as their networks focus more on the global context than on the local information. The state-of-the-art (SOTA) NN models in CD use U-Net <ref type="bibr" target="#b22">[23]</ref> or ResNet <ref type="bibr" target="#b23">[24]</ref> as their backbone. While both backbone networks are designed for image recognition, they focus on the global context and discard a lot local information via pooling operations <ref type="bibr" target="#b37">[38]</ref>. If U-Net and ResNet are directly applied as the backbone, it is natural that modeling details such as boundary regions and small areas would be difficult. In this paper, a new NN with a brand new backbone is proposed, which is more suitable for CD tasks. It can sense the input images by region, focus on the local changes between the input multi-temporal images and ensure that the information is fully utilized. It is worth mentioning that this new NN is quite compact, with a small number of parameters, which allows it to be deployed on edge devices such as UAVs.</p><p>The RDP-Net has achieved a better result in the field of remote sensing change detection. The major contributions of RDP-Net can be summarized as follows:</p><p>? An Efficient Training Strategy: We propose a strategy to let the network learn from easy to hard. The datasets are split by difficulty level and training tasks are constructed in the early stage of training. Experiments show that this efficient training strategy can achieve better performance. ? An Effective Edge Loss: We propose an edge loss, which increases the penalty for errors on the boundary regions when calculating loss. We demonstrate that this loss improves the network's attention on the details such as boundary regions and small areas, and increases the detection accuracy. ? A Compact Network: We perform parameter tuning following the proposed efficient training strategy and edge loss. Experimental results indicate that our method leads us to find out a network that achieves the SOTA accuracy in remote sensing change detection with a brand new backbone. Moreover, the number of parameters in our RDP-Net is 1.70M, which enables its deployment in more democratized devices.</p><p>This paper is organized as follows. Section II describes the change detection method proposed in this paper. Section III contains a series of quantitative comparisons and analyses through experiments. Finally, the conclusion is drawn in Section IV.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. METHODOLOGY</head><p>In this section, we first introduce our efficient training strategy, splitting the dataset by difficulty level and training the NN model from easy to hard. Then, we propose an edge loss to improve the network's attention on the details such as boundary regions and small areas. At last, the architecture of the proposed RDP-Net with a brand new backbone is presented.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Efficient Training Strategy</head><p>Change detection in remote sensing images faces various challenges and situations. Some changed areas may contain thousands of pixels while some of them only take few pixels. Meanwhile, seasonal changes are regarded as interference factors, which brings difficulties to change detection. As shown in <ref type="figure" target="#fig_2">Fig. 2</ref>, the changed area between <ref type="figure" target="#fig_2">Fig. 2</ref>(a) and <ref type="figure" target="#fig_2">Fig. 2(b)</ref> is a building covering a large area, the changed area between <ref type="figure" target="#fig_2">Fig.  2(d)</ref> and <ref type="figure" target="#fig_2">Fig. 2</ref>(e) is much smaller and under the influence of seasonal changes (the growth of trees). Obviously, <ref type="figure" target="#fig_2">Fig. 2</ref> However, most current deep learning methods treat different samples equally. A more adaptive strategy should be considered, since some samples may be more difficult and affect the effectiveness of learning. As shown in <ref type="figure" target="#fig_3">Fig. 3</ref>, for such a classification task, the batches used for training are randomly generated. If all the samples are directly used for training, the network can learn some features in the situations demonstrated in <ref type="figure" target="#fig_3">Fig. 3</ref>(b) and 3(c). But the situations demonstrated in <ref type="figure" target="#fig_3">Fig.  3</ref>(d) and 3(e) may also occur, which are not conducive for the network to learn the features and build a good foundation, and would drag down the learning process.</p><formula xml:id="formula_0">(f) is more difficult to detect than Fig. 2(c). (a) (b) (c) (d) (e) (f)</formula><p>Therefore, we propose an efficient training strategy. The dataset is split into different subsets by difficulty level and fed into the network at different training stages. At the early training stage, the network is trained with an easy subset (demonstrated in <ref type="figure" target="#fig_3">Fig. 3(f)</ref>). After the network has learned some feature distribution, the difficulty of training dataset would be increased. The efficient training strategy allows the network to learn from easy to hard, which we believe is more conducive to the network's learning. Specifically, the entire dataset is used for pre-training to obtain an initial model. The training difficulties of samples are described using the detection loss of this initial model. The dataset is split into three subsets: easy subset, medium subset and hard subset. Meanwhile, the efficient training strategy can significantly reduce FLOPs in training process, because the number of samples used in each epoch is less. The efficient training strategy can also improve the performance and convergence speed of the network. Different colors represent different classes.</p><formula xml:id="formula_1">(a) (b) (c) (d) (e) (f)</formula><p>In addition, one idea for curriculum learning is to reduce the impact of hard samples in the training process, which suggests reducing the probability of hard samples appropriately during training and proposes a strategy to randomly select samples based on their difficulties. In this paper, we argue that in the early training stage, training the network with only easy samples can build a good foundation for learning strong features. While in the last training stage, hard samples can improve the detection in subtle areas and further enhance the network's performance. In the experiments, we compare our efficient training strategy with the random sampling strategy in CD tasks, and the results show the efficient training strategy can achieve better performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Edge Loss</head><p>Edge loss increases the penalty for errors on the boundary regions by increasing the weights of points located on the boundary during the loss calculation. It can improve the network's attention to the details in remote sensing CD tasks. The closer the point is to the boundary, the larger its weight should be, and vice versa. For a straight boundary, we can roughly divide the points around the boundary into five cases, as shown in <ref type="figure" target="#fig_4">Fig. 4(a)</ref>. The weight can be determined based on the distance between the point and its boundary. The weights of each point w edge (point) can be compared as follows:</p><formula xml:id="formula_2">w edge (A) &gt; w edge (B) ? w edge (C) &gt; w edge (D) ? w edge (E).</formula><p>In practice, the scenarios are much more complex. As shown in <ref type="figure" target="#fig_4">Fig. 4(b)</ref>, in this case, we cannot determine the weight based on its distance from the nearest boundary. The weight of corner point G should be larger than that of point F . There are many similar cases, as shown in <ref type="figure" target="#fig_4">Fig. 4(c) and 4(d)</ref>. Obviously, the weight of canyon point H should be larger than point I, and the weight of gap point K should be larger than point J.  Therefore, we design a weight w edge describing the changes in the neighborhood of the point, as shown in <ref type="figure" target="#fig_5">Fig. 5</ref>. It can be described as:</p><formula xml:id="formula_3">w edge (c) = ? n k?N |L(k) ? L(c)|<label>(1)</label></formula><p>where w edge (c) represents the weight of point c, ? &gt; 0 is a coefficient, N represents the neighborhood of point c which contains n points, c represents the center of area N and L[k] represents the label value of point k. Since the label value can only be 0 or 1, the above equation is equivalent to:</p><formula xml:id="formula_4">w edge (c) = ? 1 n k?N L(k) ? L(c)<label>(2)</label></formula><p>According to Eq. (2), w edge can be indicated by the difference between the point and the average of its neighborhood. We combine w edge with the cross entropy loss and define the edge loss as:</p><formula xml:id="formula_5">EL(p t ) = ?w edge log(p t )<label>(3)</label></formula><p>where p t represents the probability of correct classification. <ref type="figure">Fig. 6</ref> shows an example where ? is set to 1. It can be seen that w edge can quantify the boundary, making the network focus on the details such as boundary regions. The attention of the network for the boundary regions is likewise increased as ? is increased. According to experiments, ? can generally be set between 0.5 and 5. In this paper, ? is set to 1.</p><p>(a) (b) <ref type="figure">Fig. 6. (a)</ref> is the ground truth. The white pixels represent the changed area. (b) is a map of w edge .</p><p>In this paper, we use a hybrid loss function, which is defined as:</p><formula xml:id="formula_6">L = L edge + L focal + L dice<label>(4)</label></formula><p>where L focal represents Focal loss <ref type="bibr" target="#b33">[34]</ref> and L dice represents Dice loss <ref type="bibr" target="#b34">[35]</ref>. Edge loss mainly focuses on the details such as boundary regions. Focal loss and dice loss mainly aim at dealing with class imbalance problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Network Architecture</head><p>Our model, based on a brand new backbone, named RDP-Net, mainly consists of four components, including a region division layer, a fully-convolutional block named ConvMixer, and a region composition layer, as shown in <ref type="figure" target="#fig_6">Fig. 7(a)</ref>. For change detection tasks, detailed local information is more valuable than global context, especially for the change detection of boundary regions and small areas. So, we design a region division layer to slice the input image into several small patches by region. ConvMixer allows us to explore the local information in the patch while perceiving the global context. The region composition layer will compose the patches. Finally, a tiny depth attention module is added to fuse the multiple depth outputs and obtain the change detection result.</p><p>Specifically, the region division layer is used to slice the input image into several patches according to the region. In order to maintain the amount of data without loss and preserve all the detailed local information, the region division layer with region size (p ? p) and input image size (c ? h ? w) can be implemented as convolution with c in input channels, c out (= c in ? (h/p ? w/p)) output channels, kernel size p, and stride p:</p><formula xml:id="formula_7">? {SN(Conv (c in , c out , stride = p, kernel = p))}</formula><p>where c, h, w represent the channels, height and width of the input image, SN() represents Switchable Normalization <ref type="bibr" target="#b38">[39]</ref>, ?{} represents Gaussian Error Linear Units <ref type="bibr" target="#b39">[40]</ref>. Switchable Normalization allows the network to choose a better normalization method for each layer, so that we can pay attention to the module design of the network. GELU combines the idea of dropout <ref type="bibr" target="#b40">[41]</ref> and ReLU <ref type="bibr" target="#b41">[42]</ref>, making the network training more robust.</p><p>The ConvMixer block <ref type="bibr" target="#b42">[43]</ref> is used as the backbone of our network. As shown in <ref type="figure" target="#fig_6">Fig. 7(b)</ref>, it consists of depthwise convolution followed by pointwise convolution. The depthwise convolution is used to explore information between different (a) Regional Sensing Network (RDP-Net) (b) ConvMixer patches. The pointwise convolution is used to explore the information of each patch. The residual connection ensures that it still mainly focuses on the details inside a path, when the network explores information between patches, which is important for detail-sensitive change detection tasks. In this paper, we introduce ConvMixer into the change detection task.</p><p>Existing methods gradually lose information as the network deepens. The ConvMixer makes the resolution of the network remain the same and retains detailed local information, so it is suitable for change detection tasks that require more attention to detail. The region composition layer is used to compose patches together according to the region and obtain the pixel-level classification feature map. The region composition layer can be implemented as convolution-transpose with c out (= c in ? (h/p ? w/p)) input channels, out ch output channels, kernel size p, and stride p: ? {SN(ConvTrans (c out , out ch, stride = p, kernel = p))}</p><p>The tiny depth attention module is used to suppress semantic gaps and localization differences. It is a 1d learnable weight vector with length out ch ? depth, where depth represents the depth of the backbone.</p><p>RDP-Net source code is released at https://github.com/ Chnja/RDPNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. EXPERIMENT AND RESULT ANALYSIS</head><p>The experiment was conducted on two datasets named CDD <ref type="bibr" target="#b43">[44]</ref> and LEVIR-CD <ref type="bibr" target="#b44">[45]</ref>, two of the most common datasets in remote sensing change detection.</p><p>CDD dataset consists of seven image pairs of 4725 ? 2200 pixels and four image pairs of 1900 ? 1000 pixels. The spatial resolution ranges from 3 to 100 cm per pixel, and the seasons vary widely. We cut each image pair into 256 ? 256 pixel patches without overlapping and ultimately obtained 10000 training sets and 3000 validation sets.</p><p>LEVIR-CD dataset consists of 637 image pairs of 1024 ? 1024 pixels. We cut each image pair into 256 ? 256 pixel patches without overlapping and ultimately obtained 3167 training sets and 436 validation sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Implementation Details</head><p>We implement RDP-Net using Pytorch framework. The depth of ConvMixer is set to 6, out ch is set to 32, and the size of the neighborhood in edge loss is set to 7. The training dataset is divided into three subsets: easy subset, medium subset and hard subset. The ratio of the three subsets is set to 4:2:3 empirically. Different subsets are fed to the network at the 30th, 60th and 90th epochs. The learning rate is set to 1e-3 and decays by 0.8 every 15 epochs. In the training process, the batch size is set to 16, and AdamW <ref type="bibr" target="#b45">[46]</ref> is applied as an optimizer. We conduct experiments on a single NVIDIA RTX3090 and train for 200 epochs.</p><p>And in the experiments, we use three indicators for the evaluation of quantitative metrics: Precision, Recall, and F1-Score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Comparison With State-of-the-Art Methods</head><p>We compare our method with FC-EF, FC-Siam-conc, FC-Siam-diff <ref type="bibr" target="#b25">[26]</ref>, UNet++ MSOF <ref type="bibr" target="#b26">[27]</ref>, DASNet <ref type="bibr" target="#b29">[30]</ref> and SNUNet-CD <ref type="bibr" target="#b27">[28]</ref>. They are representative of deep learningbased methods in the field of change detection. FC-EF, FC-Siam-conc, FC-Siam-diff <ref type="bibr" target="#b25">[26]</ref> is the baseline model for change detection, it is a simple combination of U-Net <ref type="bibr" target="#b22">[23]</ref> and Siamese network. Unet++ MSOF uses the multiple side outputs fusion (MSOF) from U-Net++ <ref type="bibr" target="#b28">[29]</ref> for deep supervision. DASNet uses a contrastive method with a dual attention mechanism. SNUNet-CD uses the ensemble channel attention module to fuse the multiple side outputs of U-Net++, and its inputs come from a Siamese network. <ref type="table" target="#tab_0">Table I</ref> and II report the comparisons of detection accuracy and the number of parameters. Our proposed RDP-Net can get better performance than other SOTA change detection methods, and it only has 1.70 M parameters. CDD dataset contains some detailed changes. Although the overall task is not difficult, it is still a challenge to further improve accuracy. It can be seen that our network has a significant improvement. Our RDP-Net only uses 1.7M parameters (? 12.9% ? 13.2M) to achieve a better result (F1 = 0.972), which undoubtedly has advantages. Compared with SNUNet-CD / 16 with twice parameters, the F1 is increased by 3.6%. LEVIR-CD dataset is mainly for building change detection tasks. The label in LEVIR-CD dataset mainly refers to the general location and shape of the building, which is not as fine as the CDD dataset. Our RDP-Net does not perform well on this dataset, but still achieves better performance with fewer parameters. We believe that on datasets with more accurate labels, our method will have a more obvious improvement effect. In addition, <ref type="figure" target="#fig_7">Fig. 8</ref> shows some detection results from the validation set of the CDD and LEVIR-CD datasets. The false positives and false negatives are indicated by red and green, respectively. Other colors represent true positives.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Ablation Study</head><p>To evaluate the RDP-Net, edge loss and efficient training strategy, a couple of ablation experiments were conducted. <ref type="table" target="#tab_0">Table III</ref> and IV report the detection accuracy.  The experimental results show that the addition of the Edge loss is beneficial to improving the detection accuracy by 0.5% recall and 0.2% F1 on CDD dataset; 1% recall and 0.5% F1 on LEVIR-CD dataset. <ref type="figure">Fig. 9</ref> shows the contribution of edge loss, that the model can achieve a better performance in boundary regions with edge loss. The efficient training strategy allows the network to learn from easy to hard. When we use it to train the network, the detection accuracy is improved by 0.7% recall and 0.5% F1 on CDD dataset; 1% precision, 0.6% recall and 0.9% F1 on LEVIR-CD dataset.</p><p>We can infer from the experiment that our improvements can increase recall without reducing precision, which enables the network to detect as many change areas as possible without increasing the false detection rate. Meanwhile, the experiment can also prove that the architecture of RDP-Net is very efficient, which enables us to achieve a performance close to SOTA even without edge loss and the efficient training strategy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Experiment in Efficient Training Strategy</head><p>To test the efficient training strategy further, we trained the network with datasets of different difficulties. <ref type="figure" target="#fig_1">Fig. 10</ref> shows epoch versus F1 on the validation set. <ref type="figure" target="#fig_1">Fig. 10(a)</ref> and (c) indicate that different subsets are input for training in the early stage of training. It can be seen that the red line rises the fastest and achieves the highest F1, while the blue line performs worst. It can be proved that in the early stage of network training, feeding easy tasks to the network can not only reduce the amount of computation and speed up the network operation, but also achieve better training results. <ref type="figure" target="#fig_1">Fig. 10(b)</ref> and (d) indicate that different subsets are input for training after some training epochs with the easy subset. It can be seen that the orange line rises faster and achieves higher F1, while the blue line performs worse. It can be proved that in the middle stage of network training, feeding moderate tasks to the network can reduce the amount of computation, speed up the network operation, and achieve better training results. These couple of experiments further prove the effectiveness of our proposed efficient training strategy. We also compare our efficient training strategy with the random sampling strategy. The probability of each sample is set to e ?L , where L represents the detection loss of the initial     learn more powerful features from easy to hard with fewer FLOPs (about 86.5%) and achieve better performance.</p><formula xml:id="formula_8">(a) (b) (c) (d) (e) (f) (g) (h) (i) (j) (k) (l) (m) (n)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. CONCLUSION</head><p>In this paper, we propose RDP-Net, an effective method for change detection in remote sensing. It contains an efficient training strategy, an effective edge loss and a brand new NN backbone focusing on preserving regions' details. The efficient training strategy splits the dataset by difficulty level, allows the network to learn from easy to hard and achieve a better performance compared with reducing the probability of hard samples in the training process. Edge loss increases the penalty for errors on the boundary regions when calculating loss, which can improve the network's attention on the details such as boundary regions and small areas, and enhance the detection performance. The proposed RDP-Net achieves the state-of-theart empirical performance with only 1.70M parameters. The F1-score is 3.6% higher than that of a comparable size network (3.31M parameters). The number of parameters is only 12.9% of SOTA (13.21M parameters). We performed experiments on CDD and LEVIR-CD datasets. The experimental results show that the proposed change detection method can significantly improve the accuracy of the task of remote sensing change detection, and the efficient training strategy can also benefit other methods.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible.Thiswork was supported in part by National Natural Science Foundation of China (Grant: 62071336) and in part by the National Key Research and Development Program of China (Grant: 2018YFB2100503). (Corresponding author: Fangling Pu) H. Chen, F. Pu, R. Yang, R. Tang and X. Xu are with the Collaborative Sensing Laboratory, School of Electronic Information, Wuhan University, Wuhan 430079, China (e-mail: chj1997@whu.edu.cn; flpu@whu.edu.cn; ruiyang@whu.edu.cn; tangr@whu.edu.cn; xinxu@whu.edu.cn).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 .</head><label>1</label><figDesc>Multi-temporal remote sensing images in CDD dataset. (a) and (b) are the original images, (c) is the ground truth. The result of (d) FC-Siam-diff, (e) SNUNet-CD, (f) our method. The white pixels represent the changed area.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>(a), (b), (d) and (e) are the original input images. (c) and (f) are ground truth. The white pixels represent the changed area.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>An abstract image of a classification task. (a) represents the whole dataset of the task. (b)-(e) represent some possible batches during the training process. (f) represents an easy subset of the task.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .</head><label>4</label><figDesc>(a) shows five cases of points around the straight boundary, where the gray represents the changed area. (b), (c) and (d) show the other three practical cases.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 .</head><label>5</label><figDesc>The green represents the neighborhood of a point, and the gray represents the changed area.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7 .</head><label>7</label><figDesc>The architecture of the proposed RDP-Net. (a) is the backbone of RDP-Net. (b) is ConvMixer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 8 .</head><label>8</label><figDesc>(a)-(g) are results on CDD dataset. (h)-(n) are results on LEVIR-CD dataset. (a), (b), (h) and (i) are the original images. (c) and (j) are the ground truth. The results of (d) (k) FC-Siam-diff, (e) (l) UNet++ MSOF with 32 channels, (f) (m) SNUNet-CD with 32 channels, (g) (n) our RDP-Net. The false positives and false negatives are indicated by red and green, respectively. Other colors represent true positives.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 9 .Fig. 10 .</head><label>910</label><figDesc>(a), (b), (h) and (i) are the original input images. (c) and (j) are the ground truth. The results of (d) (k) SNUNet-CD with 32 channels, (e) (l) our RDP-Net without edge loss and efficient training strategy, (f) (m) our RDP-Net without efficient training strategy. (g) (n) our RDP-Net. The false positives and false negatives are indicated by red and green, respectively. Other colors represent true positives. Epoch versus F1. (a) and (b) are on CDD dataset. (c) and (d) are on LEVIR-CD dataset. The blue line indicates that the easy, medium and hard subsets are used for training. The orange line indicates that the easy and medium subsets are used for training. The red line indicates that only the easy subset is used for training. (a) and (c) are in the early stage of training. (b) and (d) are in the middle stage of training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I RESULTS</head><label>I</label><figDesc></figDesc><table><row><cell></cell><cell cols="2">ON CDD DATASET</cell><cell></cell><cell></cell></row><row><cell>Method / Channel</cell><cell cols="3">Params(M) Precision Recall</cell><cell>F1</cell></row><row><cell>FC-EF</cell><cell>1.83</cell><cell>0.598</cell><cell>0.708</cell><cell>0.648</cell></row><row><cell>FC-Siam-conc</cell><cell>2.03</cell><cell>0.679</cell><cell>0.723</cell><cell>0.700</cell></row><row><cell>FC-Siam-diff</cell><cell>1.83</cell><cell>0.715</cell><cell>0.685</cell><cell>0.699</cell></row><row><cell>UNet++ MSOF / 16</cell><cell>2.75</cell><cell>0.924</cell><cell>0.879</cell><cell>0.901</cell></row><row><cell>UNet++ MSOF / 32</cell><cell>11.00</cell><cell>0.946</cell><cell>0.939</cell><cell>0.943</cell></row><row><cell>DASNet</cell><cell>16.25</cell><cell>0.914</cell><cell>0.925</cell><cell>0.919</cell></row><row><cell>SNUNet-CD / 16</cell><cell>3.31</cell><cell>0.938</cell><cell>0.935</cell><cell>0.936</cell></row><row><cell>SNUNet-CD / 32</cell><cell>13.21</cell><cell>0.961</cell><cell>0.965</cell><cell>0.963</cell></row><row><cell>RDP-Net</cell><cell>1.70</cell><cell>0.967</cell><cell>0.977</cell><cell>0.972</cell></row><row><cell></cell><cell>TABLE II</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">RESULTS ON LEVIR-CD DATASET</cell><cell></cell><cell></cell></row><row><cell>Method / Channel</cell><cell cols="3">Params(M) Precision Recall</cell><cell>F1</cell></row><row><cell>FC-EF</cell><cell>1.83</cell><cell>0.776</cell><cell>0.712</cell><cell>0.735</cell></row><row><cell>FC-Siam-conc</cell><cell>2.03</cell><cell>0.899</cell><cell>0.775</cell><cell>0.828</cell></row><row><cell>FC-Siam-diff</cell><cell>1.83</cell><cell>0.883</cell><cell>0.785</cell><cell>0.826</cell></row><row><cell>UNet++ MSOF / 16</cell><cell>2.75</cell><cell>0.907</cell><cell>0.880</cell><cell>0.893</cell></row><row><cell>UNet++ MSOF / 32</cell><cell>11.00</cell><cell>0.906</cell><cell>0.886</cell><cell>0.894</cell></row><row><cell>DASNet</cell><cell>16.25</cell><cell>0.811</cell><cell>0.788</cell><cell>0.799</cell></row><row><cell>SNUNet-CD / 16</cell><cell>3.31</cell><cell>0.917</cell><cell>0.871</cell><cell>0.892</cell></row><row><cell>SNUNet-CD / 32</cell><cell>13.21</cell><cell>0.907</cell><cell>0.887</cell><cell>0.896</cell></row><row><cell>RDP-Net</cell><cell>1.70</cell><cell>0.915</cell><cell>0.888</cell><cell>0.901</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE III</head><label>III</label><figDesc></figDesc><table><row><cell cols="3">ABLATION EXPERIMENT ON CDD DATASET</cell><cell></cell></row><row><cell>Method</cell><cell cols="2">Precision Recall</cell><cell>F1</cell></row><row><cell>w/o Edge loss, efficient training</cell><cell>0.966</cell><cell>0.965</cell><cell>0.965</cell></row><row><cell>w/o efficient training</cell><cell>0.965</cell><cell>0.970</cell><cell>0.967</cell></row><row><cell>RDP-Net</cell><cell>0.967</cell><cell>0.977</cell><cell>0.972</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE IV</head><label>IV</label><figDesc></figDesc><table><row><cell cols="3">ABLATION EXPERIMENT ON LEVIR-CD DATASET</cell><cell></cell></row><row><cell>Method</cell><cell cols="2">Precision Recall</cell><cell>F1</cell></row><row><cell>w/o Edge loss, efficient training</cell><cell>0.906</cell><cell>0.872</cell><cell>0.887</cell></row><row><cell>w/o efficient training</cell><cell>0.905</cell><cell>0.882</cell><cell>0.892</cell></row><row><cell>RDP-Net</cell><cell>0.915</cell><cell>0.888</cell><cell>0.901</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE V EXPERIMENT</head><label>V</label><figDesc></figDesc><table><row><cell cols="4">BETWEEN EFFICIENT TRAINING STRATEGY AND RANDOM</cell></row><row><cell cols="3">SAMPLING STRATEGY ON CDD DATASET</cell><cell></cell></row><row><cell>Method</cell><cell cols="2">Precision Recall</cell><cell>F1</cell></row><row><cell>Efficient Training Strategy</cell><cell>0.967</cell><cell>0.977</cell><cell>0.972</cell></row><row><cell>Random Sampling Strategy</cell><cell>0.962</cell><cell>0.973</cell><cell>0.967</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE VI EXPERIMENT</head><label>VI</label><figDesc>BETWEEN EFFICIENT TRAINING STRATEGY AND RANDOM SAMPLING STRATEGY ON LEVIR-CD DATASET</figDesc><table><row><cell>Method</cell><cell cols="2">Precision Recall</cell><cell>F1</cell></row><row><cell>Efficient Training Strategy</cell><cell>0.915</cell><cell>0.888</cell><cell>0.901</cell></row><row><cell>Random Sampling Strategy</cell><cell>0.888</cell><cell>0.873</cell><cell>0.878</cell></row></table><note>model. Three-quarters of the samples are randomly selected for each training epoch, according to their probabilities. The results are shown in Table V and VI. It can be proved that our efficient training strategy can achieve better results compared with the random sampling strategy. Furthermore, we use the efficient training strategy to train</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE VII</head><label>VII</label><figDesc></figDesc><table><row><cell>(a)</cell><cell>(b)</cell><cell>(c)</cell><cell>(d)</cell><cell>(e)</cell><cell>(f)</cell><cell></cell><cell>(g)</cell></row><row><cell>(h)</cell><cell>(i)</cell><cell>(j)</cell><cell>(k)</cell><cell>(l)</cell><cell>(m)</cell><cell></cell><cell>(n)</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="5">EXPERIMENT WITH EFFICIENT TRAINING STRATEGY COMBINED WITH</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="5">DIFFERENT METHODS ON CDD DATASET</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Method</cell><cell></cell><cell cols="3">Efficient Training Precision Recall</cell><cell>F1</cell></row><row><cell></cell><cell></cell><cell></cell><cell>FC-Siam-diff</cell><cell></cell><cell>-</cell><cell>0.774 0.847</cell><cell>0.631 0.607</cell><cell>0.688 0.697</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">UNet++ MSOF</cell><cell>-</cell><cell>0.946 0.959</cell><cell>0.939 0.963</cell><cell>0.943 0.961</cell></row><row><cell></cell><cell></cell><cell></cell><cell>SNUNet-CD</cell><cell></cell><cell>-</cell><cell>0.961 0.965</cell><cell>0.965 0.973</cell><cell>0.963 0.969</cell></row></table><note>the existing SOTA methods. The results are shown in Table VII and VIII. It can be seen that our proposed efficient training strategy is also helpful for existing methods. The NN could</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE VIII EXPERIMENT</head><label>VIII</label><figDesc>WITH EFFICIENT TRAINING STRATEGY COMBINED WITH DIFFERENT METHODS ON LEVIR-CD DATASET</figDesc><table><row><cell>Method</cell><cell cols="3">Efficient Training Precision Recall</cell><cell>F1</cell></row><row><cell>FC-Siam-diff</cell><cell>-</cell><cell>0.883 0.878</cell><cell>0.785 0.804</cell><cell>0.826 0.834</cell></row><row><cell>UNet++ MSOF</cell><cell>-</cell><cell>0.906 0.923</cell><cell>0.886 0.878</cell><cell>0.894 0.899</cell></row><row><cell>SNUNet-CD</cell><cell>-</cell><cell>0.907 0.908</cell><cell>0.887 0.892</cell><cell>0.896 0.900</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Review Article Digital Change Detection Techniques using Remotely-Sensed Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="989" to="1003" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">DA2Net: Distraction-Attention-Driven Adversarial Network for Robust Remote Sensing Image Scene Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geoscience and Remote Sensing Letters</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">AMN: Attention Metric Network for One-Shot Remote Sensing Image Scene Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">24</biblScope>
			<biblScope unit="page">4046</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Fully Convolutional Networks for Multisource Building Extraction From an Open Aerial and Satellite Imagery Data Set</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="574" to="586" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The Role of Remote Sensing in Process-Scaling Studies of Managed Forest Ecosystems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">G</forename><surname>Masek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Hayes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Hughes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">P</forename><surname>Healey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Turner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Forest Ecology and Management</title>
		<imprint>
			<biblScope unit="volume">355</biblScope>
			<biblScope unit="page" from="109" to="123" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A Systematic Review and Assessment of Algorithms to Detect, Characterize, and Monitor Urban Land Change</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Reba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">C</forename><surname>Seto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote Sensing of Environment</title>
		<imprint>
			<biblScope unit="volume">242</biblScope>
			<biblScope unit="page">111739</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Climate Change now Detectable from any Single Day of Weather at Global Scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sippel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Meinshausen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">M</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Sz?kely</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Knutti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Climate Change</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="35" to="41" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Landslide Inventory Mapping From Bitemporal Images Using Deep Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Nandi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geoscience and Remote Sensing Letters</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="982" to="986" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">An Automatic Procedure for Early Disaster Change Mapping Based on Optical Remote Sensing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">272</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Optical Remote Sensing Image Change Detection Based on Attention Mechanism and Image Difference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A Learning-Based Semi-Autonomous Controller for Robotic Exploration of Unknown Disaster Scenes While Searching for Victims</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Doroodgar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Nejat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Cybernetics</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2719" to="2732" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Grid Commerce, Market-Driven G-Negotiation, and Grid Resource Management</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Sim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Systems, Man, and Cybernetics</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1381" to="1394" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
	<note>Part B (Cybernetics)</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Change Detection from Remotely Sensed Images: From Pixel-Based to Object-Based Approaches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hussain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Stanley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPRS Journal of photogrammetry and remote sensing</title>
		<imprint>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="91" to="106" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Change-vector analysis in multitemporal space: A tool to detect and categorize land-cover change processes using high temporal-resolution satellite data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">F</forename><surname>Lambin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">H</forename><surname>Strahlers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote sensing of environment</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="231" to="244" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Unsupervised change detection in satellite images using principal component analysis and k-means clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Celik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE geoscience and remote sensing letters</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="772" to="776" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Ica and kernel ica for change detection in multispectral remote sensing images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Marchesi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bruzzone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE International Geoscience and Remote Sensing Symposium</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">980</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Unsupervised deep change vector analysis for multiple-change detection in vhr images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Saha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bovolo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bruzzone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="3677" to="3693" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Unsupervised deep transfer learning-based change detection for hr multispectral images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Saha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">T</forename><surname>Solano-Correa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bovolo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bruzzone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geoscience and Remote Sensing Letters</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="856" to="860" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Building change detection in vhr sar images via unsupervised deep transcoding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Saha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bovolo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bruzzone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1917" to="1929" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Image objects and geographic objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Castilla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Object-based image analysis</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="91" to="110" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Raw Nav-merge Seismic Data to Subsurface Properties with MLP based Multi-Modal Information Unscrambler</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Desai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vial-Aussavy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">3D Pavement Data Decomposition and Texture Level Evaluation based on Step Extraction and Pavement-Transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Measurement</title>
		<imprint>
			<biblScope unit="page">110399</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">U-Net: Convolutional Networks for Biomedical Image Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deep Residual Learning for Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Very Deep Convolutional Networks for Large-Scale Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Fully Convolutional Siamese Networks for Change Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Daudt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">Le</forename><surname>Saux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Boulch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 25th IEEE International Conference on Image Processing (ICIP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4063" to="4067" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">End-to-End Change Detection for High Resolution Satellite Images Using Improved UNet++</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Guan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">1382</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">SNUNet-CD: A Densely Connected Siamese Network for Change Detection of VHR Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note>Remote Sensing Letters</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">UNet++: A Nested U-Net Architecture for Medical Image Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M R</forename><surname>Siddiquee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Tajbakhsh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Deep Learning in Medical Image Analysis and Multimodal Learning for Clinical Decision Support</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">DASNet: Dual Attentive Fully Convolutional Siamese Networks for Change Detection in High-Resolution Satellite Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1194" to="1206" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Curriculum learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Louradour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th annual international conference on machine learning</title>
		<meeting>the 26th annual international conference on machine learning</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="41" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Curriculum learning for natural language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<ptr target="https://aclanthology.org/2020.acl-main.542" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020-07" />
			<biblScope unit="page" from="6095" to="6104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Easy samples first: Self-paced reranking for zero-example multimedia search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mitamura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM international conference on Multimedia</title>
		<meeting>the 22nd ACM international conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="547" to="556" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Focal Loss for Dense Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">V-Net: Fully Convolutional Neural Networks for Volumetric Medical Image Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Milletari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-A</forename><surname>Ahmadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 fourth International Conference on 3D Vision (3DV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="565" to="571" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">UnitBox: An Advanced Object Detection Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM International Conference on Multimedia</title>
		<meeting>the 24th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="516" to="520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Boundary Loss for Highly Unbalanced Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kervadec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bouchtiba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Desrosiers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Granger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dolz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">B</forename><surname>Ayed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Imaging with Deep Learning. PMLR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="285" to="296" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Benchmarking Graph Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">P</forename><surname>Dwivedi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Laurent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.00982</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.10779</idno>
		<title level="m">Differentiable Learning-to-Normalize via Switchable Normalization</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gimpel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.08415</idno>
		<title level="m">Gaussian Error Linear Units (GELUs)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Improving neural networks by preventing co-adaptation of feature detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1207.0580</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Icml</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Anonymous</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=TVHS5Y4dNvM" />
		<title level="m">Patches Are All You Need?&quot; in Submitted to The Tenth International Conference on Learning Representations, 2022, under review</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Change Detection in Remote Sensing Images using Conditional Adversarial Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lebedev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">V</forename><surname>Vizilter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vygolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Knyaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Rubis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote Sensing &amp; Spatial Information Sciences</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>International Archives of the Photogrammetry</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">A Spatial-Temporal Attention-Based Method and a New Dataset for Remote Sensing Image Change Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shi</surname></persName>
		</author>
		<ptr target="https://www.mdpi.com/2072-4292/12/10/1662" />
	</analytic>
	<monogr>
		<title level="j">Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">10</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Decoupled Weight Decay Regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05101</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
