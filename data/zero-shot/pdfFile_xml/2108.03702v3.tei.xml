<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">BIGRoC: Boosting Image Generation via a Robust Classifier</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roy</forename><surname>Ganz</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Elad</surname></persName>
						</author>
						<title level="a" type="main">BIGRoC: Boosting Image Generation via a Robust Classifier</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T09:54+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The interest of the machine learning community in image synthesis has grown significantly in recent years, with the introduction of a wide range of deep generative models and means for training them. In this work, we propose a general model-agnostic technique for improving the image quality and the distribution fidelity of generated images, obtained by any generative model. Our method, termed BIGRoC (Boosting Image Generation via a Robust Classifier), is based on a post-processing procedure via the guidance of a given robust classifier and without a need for additional training of the generative model. Given a synthesized image, we propose to update it through projected gradient steps over the robust classifier, in an attempt to refine its recognition. We demonstrate this post-processing algorithm on various image synthesis methods and show a significant improvement of the generated images, both quantitatively and qualitatively, on CIFAR-10 and ImageNet. Specifically, BIGRoC improves the image synthesis state of the art on ImageNet 128 ? 128 by 14.81%, attaining an FID score of 2.53 and on 256 ? 256 by 7.87%, achieving an FID of 3.63.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Deep generative models (DGMs) are a class of deep neural networks trained to model complicated high-dimensional data <ref type="bibr" target="#b2">(Bond-Taylor et al., 2021)</ref>. Such models receive a large number of samples that follow a certain data distribution, x ? P D (x), and aim to produce samples from the same statistics. One of the most fascinating generative tasks is image synthesis, which is notoriously hard, due to the complexity of the natural images' manifold. Nevertheless, deep generative models for image synthesis have gained tremen-1 Department of EE, Technion -Israel Institute of Technology, Haifa, Israel 2 Department of CS, Technion -Israel Institute of Technology, Haifa, Israel. Correspondence to: Roy Ganz &lt;ganz.campus@technion.ac.il&gt;. dous popularity in recent years, revolutionized the field, and became state-of-the-art in various tasks <ref type="bibr" target="#b38">Zhu et al., 2017;</ref><ref type="bibr" target="#b16">Karras et al., 2018;</ref><ref type="bibr" target="#b34">2019;</ref><ref type="bibr" target="#b3">Brock et al., 2019;</ref><ref type="bibr" target="#b19">Karras et al., 2020b)</ref>. Energy-based models (EBMs), variational autoencoders (VAEs), generative adversarial networks (GANs), autoregressive likelihood models, normalization flows, diffusion-based algorithms, and more, all aim to synthesize natural-looking images, ranging from relatively simple to extremely complicated generators, often containing millions of parameters <ref type="bibr" target="#b20">(Kingma &amp; Welling, 2014;</ref><ref type="bibr" target="#b9">Goodfellow et al., 2014;</ref><ref type="bibr" target="#b30">Rezende &amp; Mohamed, 2015;</ref><ref type="bibr" target="#b28">Oord et al., 2016;</ref><ref type="bibr" target="#b14">Ho et al., 2020)</ref>.</p><p>When operating on a multiclass labeled dataset, as considered in this paper, image synthesis can be either conditional or unconditional. In the unconditional setup, the generative model aims to produce samples from the target data distribution without receiving any information regarding the target class of the synthesized images, i.e., a sample from P D (x). In contrast, in the conditional setup, the generator goal is to synthesize images from a designated class, i.e., a sample from P D (x|y) where y is the label. As such, conditional generative models receive additional class-related information.</p><p>Most of the work in the deep generative models' field has been focusing on improving the quality and the variety of the images produced by such models, tackled by seeking novel architectures and training procedures. In this work, while still aiming to improve the performance of trained generative models, we place a different emphasis than in most of these studies and propose a method for boosting generative models without any re-training or fine-tuning. More specifically, our method improves the perceptual quality of the images synthesized by any given model via an iterative post-processing procedure driven by a robust classifier.</p><p>With the introduction of learning-based machines into "realworld" applications, the interest in the robustness of such models has become a central concern. While there are abundant definitions for robustness, the most common and studied is the adversarial one. This definition upholds if a classifier is robust to a small perturbation of its input, made by an adversary to fool it. Previous work <ref type="bibr" target="#b33">(Szegedy et al., 2014;</ref><ref type="bibr" target="#b10">Goodfellow et al., 2015;</ref><ref type="bibr" target="#b22">Kurakin et al., 2017)</ref> has demonstrated that deep neural networks are not robust at all arXiv:2108.03702v3 [cs.CV] 4 Feb 2022 and can be easily fooled by an adversary. In light of this observation, many robustification methods were proposed, but the most popular among these is adversarial training <ref type="bibr" target="#b10">(Goodfellow et al., 2015;</ref><ref type="bibr" target="#b24">Madry et al., 2018)</ref>. According to this method, in order to train a robust classifier, one should generate adversarial examples and incorporate them into the training process. While examining the properties of such classifiers, researchers have revealed a fascinating phenomenon, called perceptually aligned gradients . According to this tendency, a modification of an image that sharpens such a classifier's decision yields visual features that are perceptually aligned with the target class. In other words, when drifting an image content to be better classified, the changes obtained are visually pleasing and faithful to natural image content.</p><p>In this work we harness and utilize the above-described phenomenon -we propose to iteratively modify the images created by a trained generative model, so as to maximize the conditional probability of a certain target class, approximated by a given robust classifier. This modification can potentially improve the quality of the synthesized images, since it emphasizes visual features that are aligned with images of the target class, thus boosting the generation process both in terms of perceptual quality and distribution faithfulness. We hypothesize that given an image dataset, the supervised training of a robust classifier is much simpler and effective than the unsupervised training of a generative model, thus enabling an indirect yet powerful improvement of generative models. We term this method "BIGRoC" -Boosting Image Generation via a Robust Classifier.</p><p>The method presented in this article is general and modelagnostic, and it can be applied to any image generator, both conditional or unconditional, without requiring access to its weights. In the unconditional case, since we do not have a target class to guide the boosting process, we propose to estimate it via the trained robust classifier. The marked performance improvement achieved by our proposed method is demonstrated in a series of experiments on a wide range of image generators on CIFAR-10 and ImageNet datasets. We show that this approach enables us to significantly improve the quality of images synthesized by relatively simple models, boosting them to a level of more sophisticated and complex ones. Furthermore, we demonstrate the ability of our method to enhance the performance of higher-quality state-of-the-art (SOTA) generative architectures, both qualitatively and quantitatively. Specifically, applying BIGRoC on the outputs of guided diffusion, <ref type="bibr" target="#b7">(Dhariwal &amp; Nichol, 2021)</ref>, leads to a new SOTA in image synthesis over Im-ageNet 128 ? 128 and 256 ? 256 -We achieve FIDs of 2.53 and 3.63, improving the previous SOTA by 14.81% and 7.87%, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Background</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Adversarial Examples</head><p>Adversarial examples are instances that are intentionally designed by an attacker to cause a false prediction by a machine learning-based classifier <ref type="bibr" target="#b33">(Szegedy et al., 2014;</ref><ref type="bibr" target="#b10">Goodfellow et al., 2015;</ref><ref type="bibr" target="#b22">Kurakin et al., 2017)</ref>. The generation procedure of such examples is based on applying modifications to given training examples, while restricting the allowed perturbations ?. Ideally, the "threat model" ? should include all the possible perturbations that are unnoticeable to a human observer. As it is impossible to rigorously define such a set, in practice a simple subset of the ideal threat model is used, where the most common choices are the 2 and the ? balls: ? = {? : ? 2/? ? }. Given ?, the attacker receives an instance x and generatesx = x + ? s.t. ? ? ?, while aiming to fool the classifier. Adversarial attacks can be both untargeted or targeted: An untargeted attack perturbs the input in a way that minimizes p(y|x) with respect to ?. In contrast, a targeted attack receives in addition the target class?, and perturbs x to maximize p (? |x). There are diverse techniques for generating adversarial examples, yet, in this work, we focus on targeted attacks using the Projected Gradient Descent (PGD) method <ref type="bibr" target="#b24">(Madry et al., 2018)</ref>an iterative method for creating adversarial examples that operates as shown in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Targeted Projected Gradient Descent (PGD)</head><p>Input: classifier f ? , input x, target class?, , step size ?, number of iterations T ? 0 ? 0 for t from 0 to T do</p><formula xml:id="formula_0">? t+1 = ? (? t ? ?? ? (f ? (x + ? t ),?)); end x adv = x + ? T Output: x adv</formula><p>The operation ? stands for a projection operator onto ?, and (?) is the classification loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Adversarial Robustness</head><p>Adversarial robustness is a property of classifiers, according to which, applying small perturbation on a classifier's input in order to fool it does not affect its prediction <ref type="bibr" target="#b10">(Goodfellow et al., 2015)</ref>. To attain such classifiers, one should solve the following optimization problem:</p><formula xml:id="formula_1">min ? x,y?D max ??? (f ? (x + ?), y)<label>(1)</label></formula><p>Namely, train the classifier to accurately predict the class labels of the "toughest" perturbed images, allowed by the threat model ?. In practice, solving this optimization problem is challenging, and there are several ways to attain an approximated solution. The most simple yet effective method is based on approximating the solution of the innermaximization via adversarial attacks, such as PGD <ref type="bibr" target="#b24">(Madry et al., 2018)</ref>. According to this strategy, the above optimization is performed iteratively, fixing the classifier's parameters ? and optimizing the perturbation ? for each example via PGD, and then fixing these and updating ?. Repeating these steps results in a robust classifier, as we use in this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Perceptually Aligned Gradients</head><p>Perceptually aligned gradients (PAG) is a phenomenon that occurs in adversarially trained models when modifying an image to maximize the probability assigned to a target class.  show that performing the above PGD process on such models yields meaningful visual features that are perceptually aligned to the target class. It is important to note that this phenomenon does not occur in nonrobust models. The perceptually aligned gradients property indicates that the features learned by robust models are more aligned with human perception. <ref type="figure">Figure 1</ref> presents a visual demonstration of this fascinating phenomenon.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Improving Image Generation</head><p>There are two main lines of work that aim to improve the quality of generated images. One is based on rejection sampling -improving the generation quality of GANs by discarding low-quality images, identified by the GAN's discriminator <ref type="bibr" target="#b1">Azadi et al., 2019)</ref>. In contrary to such work that does not enhance the generated images but rather acts as a selector, BIGRoC does not discard any of the synthesized images and improves their quality by modifying them.</p><p>Another line of work <ref type="bibr" target="#b34">(Tanaka, 2019;</ref><ref type="bibr" target="#b4">Che et al., 2021;</ref><ref type="bibr" target="#b0">Ansari et al., 2021)</ref>, which is closely related to ours, addresses the task of sample refinement -modifying the generated images to attain improved perceptual quality. These papers propose methods for improving synthesized images using the guidance of the GAN's discriminator. More precisely, given a latent code of a generated image, their strategy is to modify it to maximize the score given by the GAN's discriminator. Therefore, in order to enhance the perceptual quality of a set of generated images, these approaches require access to the trained models of both the generator and the discriminator, and the corresponding latent code of the generated images. Note that this constraint prevents such methods from operating on a standalone set of synthesized images. In opposition to these, our work offers a much simpler and different way of boosting generated images by an external robust classifier, without any need to have access to the latent vectors generating the images, nor to the GAN's generators or the discriminators. Thus, BIGRoC can be applied to standalone images -a setup where none of the existing methods can operate. In addition, while <ref type="bibr" target="#b34">(Tanaka, 2019;</ref><ref type="bibr" target="#b4">Che et al., 2021)</ref> are limited to GANs only, our method is model-agnostic and capable of improving generated images of any source, e.g. diffusion-based methods. In Section 5.3 we empirically demonstrate that although the existing methods have much stricter requirements than ours, our method leads to improved performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Perceptually Aligned Gradients (PAG) in Computer Vision</head><p>PAG phenomenon was previously utilized for solving various computer vision tasks, such as inpainting, image translation, super-resolution, and image generation . In image synthesis, they reach 7.5 Inception Score (IS) on the CIFAR-10 dataset, which is relatively good, but still far away from SOTA. This raises the question of whether this performance limit is due to the capabilities of robust image classifiers. In contrast, in our work, we harness PAG to a completely different task -image refinement.</p><p>To this end, we build upon any existing generative model, including high-performing ones, and empirically show that a robust classifier can boost the performance of image generators well beyond 7.5 IS. As such, our work exposes a much stronger force that does exist in adversarially robust classifiers in capturing high perceptual quality features. In addition, close inspection of the visual quality of the generated images in ) reveals a weakness in producing visually pleasing results, while our qualitative results are far better.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Boosting Image Generation via a Robust Classifier</head><p>We propose a method for improving the quality of images synthesized by trained generative models, named BIGRoC: Boosting Image Generation via a Robust Classifier. Our method is model agnostic and does not require additional training or fine-tuning of the generative model, and can be viewed as a post-processing step. Thus, BIGRoC can be easily applied to any generative model, both conditional or unconditional. This mechanism harnesses the perceptually aligned gradients phenomenon to further tweak the generated images to improve their visual quality. To do so, we perform an iterative process of modifying the generated image x to maximize the posterior probability of a given target class?, p ? (?|x), where p ? is modeled by an adversarially trained classifier. This can be achieved by performing a PGD-like process, but instead of adversarially changing an image x of class y to a different class? = y, we propose to modify it in a way that maximizes the probability that x <ref type="figure">Figure 1</ref>. Demonstration of the perceptually aligned gradients phenomenon on CIFAR-10: the original image (left) is perturbed via a targeted PGD attack to maximize the probability of different target classes. This experiment is done with a robust and a non-robust classifiers of the same architecture -ResNet50 <ref type="bibr" target="#b11">(He et al., 2016)</ref>. The top row shows the results for the robust classifier and the bottom for the non-robust one. We specify below each image the relevant classifier's certainty (denoted as P), and the effective 2 norm of the perturbation ? (denoted as ). For each of the generated images, the attacked classifier reaches absolute certainty (P = 1). As can be seen, an adversarial attack (PGD) on a robust classifier leads to a significant addition of perceptual features aligned with the target class. This phenomenon does not occur at all on the non-robust classifier. Although both networks use the same threat model ?, we see that the attack on the non-robust classifier does not utilize the entire threat model and perturbs the image in a seemingly insignificant way, but is still able to completely fool it. Experimental settings: ? = {? : ? 2 ? }, = 30, T = 60, ? = T = 0.5.</p><p>belongs to class y. Therefore, our method requires a trained robust classifier that operates on the same data source as the generated images we aim to improve.</p><p>In the conditional generation process, the generator G receives the class label y, from which it suppose to draw samples. Hence, in this setup, we have information regarding the class affiliation of the image and we can maximize the corresponding conditional probability. In the unconditional generation process, the generator does not receive class labels at all and its goal is to draw samples from p(x). Thus, in this case, we cannot directly maximize the desired posterior probability, as our method suggests. To bridge this gap, we propose to estimate the most likely class via our robust classifier f ? , and afterward modify the image via the suggested method to maximize its probability. The proposed image generation boosting is described in Algorithm 2, for both the conditional and the unconditional schemes.</p><p>While the above-described approach for unconditional sampling works well, it could be further improved. We have noticed that in this case, estimating the target classes Y of X gen via f ? leads to unbalanced labels. For example, in the CIFAR-10, when generating 50,000 samples, we expect approximately 5,000 images per each of the 10 classes, and yet the labels' estimation does not distribute uniformly at all. This imbalance causes a bias in the target labels estimation of the boosting algorithm, affects the visual content of X boost , and limits the quantitative improvement attained by BIGRoC. We emphasize that this issue is manifested only in the quantitative metrics, and when qualitatively evaluating the boosted images, the improvement is significant, as can be seen in <ref type="figure" target="#fig_0">Figure 11</ref> in Appendix D.</p><p>To further enhance the quantitative results of our algorithm in the unconditional case, we propose to de-bias the target class estimation of X gen , and attain close to uniform class estimations. A naive solution to this can be achieved by generating more samples and extracting a subset of these images with a labels-balance. This approach is computationally heavy and does not use the generated images as-is, which raises questions regarding the fairness of the quantitative comparison. Thus, we propose a different debiasing technique -we modify the classifier's class estimation to become more balanced by calibrating its logits. More specifically, we shift the classifier's logits by adding a per-class pre-calculated value, d ci , that induces equality of the mean logits value across all classes. We define d c as a vector containing all d ci values:</p><formula xml:id="formula_2">d c = [d c0 , . . . d c N ?1 ]</formula><p>where N is the number of classes. For simplicity, we denote logit ci as the logit of class c i corresponding to a generated sample x gen .</p><p>We approximate E xgen [logit ci ] for each class c i , using a validation set of generated images, and calculate a per-class debiasing factor: d ci = a ?? xgen [logit ci ] (WLOG, a = 1), where? xgen [logit ci ] is a mean estimator. After calculating d ci , given a generated image x gen , we calculate its logits and add d ci to it to obtain debiased logits (logit ci ), from which we derive the unbiased class estimation via softmax.</p><p>The following equation shows that, given a correct estimation of the per-class logits' mean, the per-class means of the debiased logits are equal: As can be seen in Algorithm 2, it receives as input the generated images and their designated labels (if exist) and returns an improved version of them. As such, this method can be applied at the inference phase of generative models to enhance their performance, in a manner totally separated from their training. Furthermore, BIGRoC does not require an access to the generative models at all. As can be seen from the algorithm's description, it has several hyperparameters that determine the modification process of the image: sets the maximal size of the perturbation allowed by the threat model ?, ? controls the step size at each update step and T is the number of updates. Another choice is the norm used to define the threat model ?.</p><formula xml:id="formula_3">E xgen [logit ci ] = E xgen [d ci + logit ci ] = E xgen [a ?? xgen [logit ci ] + logit ci ] = a ?? xgen [logit ci ] + E xgen [logit ci ] ? a</formula><p>The hyperparameter is central in our scheme -when is too large, the method overrides the input and modifies the original content in an unrecognizable way, as can be seen in <ref type="figure">Figure 1</ref>. On the other hand, when is too small, the boosted images remain very similar to the input ones, leading to a minor enhancement. As our goal is to obtain a significant enhancement to the synthesized images, a careful choice of should be practiced, which restricts the allowed perturbations in the threat model.</p><p>Another important choice is the threat model ? itself. Two of the most common choices of ? for adversarial attacks are the ? and the 2 balls. Due to the desired behavior of our method, using the ? ball is less preferable: it allows a change of ? to every pixel, and as such, it will not focus on meaningful specific locations and might not preserve the existing structure of the synthesized input image. Thus, we choose the 2 ball as our threat model, with relatively small . Such a choice restricts the allowed perturbations and leads to changes that may concentrate in specific locations while preserving most of the existing content in the generated images. A visual demonstration of these considerations is given in <ref type="figure" target="#fig_3">Figure 4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experimental Results</head><p>In this section, we present experiments that demonstrate the effectiveness of our method on the most common datasets for image synthesis -CIFAR-10 <ref type="bibr" target="#b21">(Krizhevsky, 2012)</ref> and ImageNet <ref type="bibr" target="#b6">(Deng et al., 2009)</ref>. Given a generative model, we use it to synthesize a set of images X gen and apply our method to generate X boost , according to Algorithm 2. We conduct extensive experiments to demonstrate the performance improvement achieved by the proposed method. Since BIGRoC is model-agnostic, it can be easily applied to any generative model, given a robust classifier, trained on the same data source. We utilize the model-agnostic property to examine the effects of applying the proposed boosting over a wide variety of image generators of different qualities: from relatively simple to sophisticated and complex ones. We test our method on both conditional and unconditional generative models to validate that the proposed scheme can enhance different synthesis procedures. An application of our method without the ground truth labels is termed BIGRoC PL , as it generates pseudo labels (PL) using our robust classifier. In contrast, BIGRoC GT refers to the case where ground truth (GT) labels are available (in the conditional image synthesis).</p><p>In all the conducted experiments on a certain dataset, we use the same pretrained adversarial robust classifier to boost all the generative models. The only needed adjustment is at tuning , which defines the allowed size of the visual modifications, done by BIGRoC. The fact that a single robust classifier is capable of improving both low and high-quality models strongly demonstrates the versatility of our approach and the surprising refinement capabilities possessed by such a model. We analyze BIGRoC performance both qualitatively and quantitatively. For quantitative evaluation, we use Fr?chet Inception Distance (FID, <ref type="bibr">(Heusel et al., 2017)</ref>, lower values are better) and Inception Score (IS, <ref type="bibr" target="#b31">(Salimans et al., 2016)</ref>, higher values are better) to assess the quality of X gen and X boost . In addition, we provide a comparison of our approach with other image refinement SOTA methods, mentioned in Section 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">CIFAR-10</head><p>In this section, we evaluate the performance of the proposed BIGRoC on the CIFAR-10 dataset.</p><p>Tested Architectures In the conditional image generation, we experiment with cGAN <ref type="bibr" target="#b25">(Mirza &amp; Osindero, 2014)</ref>, cGAN-PD , BigGAN <ref type="bibr" target="#b3">(Brock et al., 2019)</ref>, Diff BigGAN <ref type="bibr" target="#b37">(Zhao et al., 2020)</ref> and Style-GAN2 ADA <ref type="bibr" target="#b18">(Karras et al., 2020a)</ref>. In the unconditional case, we experiment with Variational AutoEncoder (VAE) <ref type="bibr" target="#b20">(Kingma &amp; Welling, 2014)</ref>, DCGAN <ref type="bibr" target="#b29">(Radford et al., 2015)</ref>, WGAN-GP <ref type="bibr" target="#b31">(Salimans et al., 2016)</ref>, SNGAN , InfoMaxGAN <ref type="bibr">(Lee et al., 2021)</ref> and SSGAN (Chen <ref type="figure">Figure 2</ref>. The effect of the threat model's choice: generated images via a conditional GAN (left) are perturbed via a targeted PGD attack to maximize the probability of their target classes (planes, cars and birds) using either ? or 2 threat models. The boosted outputs attained by the ? entirely change the structure of the images and lead to unnatural results. Using the 2 threat model leads to pleasing and better results, as claimed. Experimental settings: In the 2 case, we use = 10, T = 30, ? = T = 1 3 , which leads to good visual results. In order to make a sensible comparison with the ?, we measure the perturbation of each pixel made by the 2-based boosting algorithm and calculate the 70% percentile of the perturbations, denoted as q0.7. Namely, q0.7 is larger than 70% of the perturbations done by our method using the 2 threat model. For the ? case, we set to be equal to the q0.7. Implementation details In all of our experiments, we use a single pretrained adversarially trained ResNet-50 on CIFAR-10 as the robust classifier . We further elaborate regarding the experimental setups and implementation details in Appendix B.1. <ref type="table" target="#tab_0">Table 1</ref> contains our quantitative results for conditional image synthesis and <ref type="table" target="#tab_1">Table 2</ref> for the unconditional case. These results indicate that BIGRoC achieves a substantial improvement across a wide range of tested generator architectures, both conditional and unconditional, demonstrating the method's versatility and validity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Quantitative Results</head><p>Qualitative Results We show in <ref type="figure" target="#fig_2">Figure 3</ref>(a) and Appendix C qualitative results that indicate that the "boosted" results are indeed more pleasing and clear to human observers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">ImageNet</head><p>We turn to evaluate the performance of the proposed BI-GRoC on the ImageNet 128 ? 128 and 256 ? 256 datasets.</p><p>Tested Architectures For ImageNet 128 ? 128, we experiment with SNGAN , SSGAN (Chen  <ref type="bibr" target="#b3">(Brock et al., 2019)</ref> and Guided Diffusion <ref type="bibr" target="#b7">(Dhariwal &amp; Nichol, 2021)</ref>, which is the current state-of-the art (SOTA) in image synthesis on ImageNet dataset. For ImageNet 256 ? 256, we use BigGAN-deep and Guided Diffusion.</p><p>Implementation details In all of our experiments, we use a single adversarially trained ResNet-50 on ImageNet as the robust classifier . We further elaborate regarding the experimental setups and implementation details in Appendix B.2. <ref type="table" target="#tab_2">Tables 3 and 4</ref> summarize our quantitative results on ImageNet 128 ? 128 and 256 ? 256, respectively. These results strongly indicate that BIGRoC is also highly beneficial on higher-resolution images from richer datasets. Specifically, BIGRoC improves the previous SOTA <ref type="bibr" target="#b7">(Dhariwal &amp; Nichol, 2021)</ref> on ImageNet 128 ? 128 by 14.81%, leading to FID of 2.53, and on 256 ? 256 by 7.87%, leading to FID of 3.63. The obtained results show a similarly marked improvement in IS. As can be seen in these tables, our approach outperforms the previous SOTA even when the ground truth labels are disregarded (BIGRoC PL ).   Qualitative Results We show qualitative results on Ima-geNet 128 ? 128 in <ref type="figure" target="#fig_2">Figure 3(b)</ref>. In <ref type="figure" target="#fig_3">Figure 4</ref> we present a qualitative comparison between images generated by a guided-diffusion trained on ImageNet 256 ? 256 and the outputs of BIGRoC applied upon them. In addition, we show the images' differences (after contrast stretching) to better grasp the perceptual modifications applied by our method. As can be seen, BIGRoC focuses on the edges and textures and leads to sharper and more high-contrast images, which are more pleasing to a human observer. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Quantitative Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Comparison with other methods</head><p>Previous works, such as DOT <ref type="bibr" target="#b34">(Tanaka, 2019)</ref>, DDLS <ref type="bibr" target="#b4">(Che et al., 2021)</ref> and DG-f low <ref type="bibr" target="#b0">(Ansari et al., 2021)</ref>, address image generation refinement, as we do. As mentioned in Section 3, we propose a much simpler approach than our competitors and require much less information, since BI-GRoC can operate without access to the image generator, the discriminator, and without knowing the latent codes corresponding with the generated images. In this section, we aim to demonstrate that although our method utilizes less information than other methods, and can operate in setups in which the other approaches can not, BIGRoC performs on par and even better. To this end, we compare our method with current SOTA image refinement methods on CIFAR-10 and ImageNet 128 ? 128. For both these setups, we adopt the same publicly available pretrained models of  SN-ResNet-GAN as used in <ref type="bibr" target="#b34">(Tanaka, 2019;</ref><ref type="bibr" target="#b4">Che et al., 2021;</ref><ref type="bibr" target="#b0">Ansari et al., 2021)</ref> and apply our algorithm upon the generated images and evaluate its performance quantitatively (see Appendix B.3 for additional implementation details). In <ref type="table" target="#tab_4">Table 5</ref> we compare the quantitative results of DOT, DDLS and DG-f low with BIGRoC using IS, as this is the common measure reported in these papers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Ablation Study</head><p>In this section we conduct a series of experiments to further understand the performance improvements obtained, and analyze the effects of the central hyperparameters in our algorithm. Additional ablations appear in Appendix A.</p><p>The effect of the robust classifier's threat model In all of our experiments, we use an adversarially trained (AT) robust classifier with a threat model ? based on 2 norm with a predefined value. In this section, we study the effect of in the training of the AT robust classifier on BIGRoC's performance. In <ref type="table" target="#tab_5">Table 6</ref> we compare the influence of using a non-adversarial classifier (i.e. = 0) and adversarial classifiers trained with different threat models' sizes on our proposed method, while the rest of the hyperparameters are fixed. As can be seen, using AT classifiers in BIGRoC enhances the results significantly.</p><p>The effect of BIGRoC's size As stated in Section 4, the hyperparameter has a significant effect on our proposed method, since it defines the allowed perturbation. In <ref type="figure" target="#fig_4">Figure  5</ref>, we demonstrate the effect of when applying BIGRoC GT over images generated by guided diffusion, trained on Im-ageNet 256 ? 256. As can be seen, affects the trade-off between diversity and fidelity, demonstrated by IS versus FID values. Our method attains lower FID scores in a range of tested values of , while achieving better IS, leading to much better trade-offs. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Discussion and Conclusions</head><p>In this work we propose a novel method that leverages the perceptually aligned gradients phenomenon for enhancing the visual quality of synthesized images. Due to the core ability of such a robust classifier to better capture significant visual features, it is capable of effectively and efficiently improving the output of generative models. Our approach does not require additional training of the generative model and it is completely model agnostic. Thus, it can be applied during the inference phase of any model. Moreover, it can operate on sets of images without requiring access to the generative model themselves, a setup where competing methods can not be applied. In a line of experiments, we show that our method is highly effective and capable of substantially enhancing the qualitative and quantitative results of a wide range of generative models over multiple datasets. Specifically, BIGRoC leads to SOTA in both ImageNet 128 ? 128 (FID of 2.53) and 256 ? 256 (FID of 3.63). When is fixed, increasing the number of steps leads to smaller steps. Fine-grained steps can lead to better performance, but with a computational cost. In <ref type="table" target="#tab_6">Table 7</ref>, we summarize the effect of the number of steps w.r.t a fixed . As can be concluded, 7 is a plausible value since it is a good trade-off between refinement performance and computational cost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Visual demonstration of BIGRoC's iterations</head><p>BIGRoC is an iterative boosting algorithm, and as such, it performs several update steps. In <ref type="figure" target="#fig_5">Figure 6</ref>, we visualize the optimization algorithm performed by our method. As can be seen, the perceptual quality of the images obtained by BIGRoC gradually improves during its application. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Implementation Details</head><p>In this work, except for three basic generative models (see description in B.1), we did not train models and used only available pretrained ones from verified sources. For quantitative evaluation, we use common implementations of IS and FID metrics. In all of our experiments, we use the same robust classifier to boost all the generative models that operate on the same dataset. We use the pretrained generators to synthesize sets of images and BIGRoC to refine them. The specific details regarding the experimental results are listed in the sections below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1. CIFAR-10</head><p>Adversarial Robust Classifier We use a pretrained robust ResNet-50 on CIFAR-10, provided in . This model is adversarially trained with a threat model ? = {? : ? 2 ? 0.5} with step size of 0.1. Image Generators Besides VAE, DCGAN, and cGAN that we trained from scratch, using the relevant available codebases, we did not train any other generator and used only publicly available ones. For cGAN-PD, WGAN-GP, SNGAN, InfoMax-GAN, and SSGAN, we use the ones from mimicry repository 1 . We use the pretrained versions of BigGAN and Differential Augmentation CR BigGAN (Diff BigGAN) from data-efficient GANs repository 2 . In addition, we use the pretrained model of StyleGAN2 with adaptive discriminator augmentation (StyleGAN2 ADA) available at the official repository 3 .</p><p>BIGRoC hyperparameters As stated above, we use the same robust classifier in all our experiments. Thus, the remaining hyperparameters to tune are , step size, and the number of steps. We empirically find out that the number of steps is relatively marginal (see Section A.1), and thus we opt to use seven steps. In all of the experiments, we fix the step size to be 1.5 * num steps . Since we test various image generators of different qualities, each requires a particular amount of visual enhancement, defined by the value of . Low-quality generators require substantial improvement and therefore benefit from high values, while better ones benefit from smaller values. In the below table, we summarize the value of for each tested architecture. Where normalization is referred to images in <ref type="bibr">[-1, 1]</ref>. To better interpret the meaning of in terms of pixels modification, the average change of a pixel value is expressed by ? 32?32?3 . For example, in DCGAN, = 5 is equivalent to an average change of ? 0.1. We note that in this example, the pixels are in a range between -1 to 1 and therefore, the mean change is ? 5%</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2. ImageNet</head><p>Adversarial Robust Classifier We use a pretrained robust ResNet-50 on ImageNet, provided in , on both 128 ? 128 and 256 ? 256. This model is adversarially trained with a threat model ? = {? : ? 2 ? 3} with step size of 0.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Image Generators</head><p>We did not train any generator and utilize the publicly available ones. For SNGAN, SSGAN, and InfoMaxGAN, we use the ones from the mimicry repository. As for BigGAN-deep (truncation= 1.0) and guided diffusion, we utilize the fact that BIGRoC can operate on standalone images and utilize the sets of generated images, published in guided diffusion's repository 4 , and we apply our method upon these. We test our method using the aforementioned sets of generated images using two setups -with and without the ground truth labels. When operating without the labels, we produce pseudo labels using our robust classifier and then apply BIGRoC.</p><p>BIGRoC hyperparameters As in CIFAR-10, we only tune the value of . In the below table, we report the used values for our tested architectures. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3. Comparison with other methods</head><p>Adversarial Robust Classifier For CIFAR-10 we use the same robust classifier as described in Appendix B.1 and for ImageNet we use the same model as in Appendix B.2.</p><p>Image Generators To fairly compare between BIGRoC and the competitive methods, we use BIGRoC to refine the outputs of the same pretrained model as in DOT, DDLS, and DG-f low -SN-ResNet-GAN 5 . We experiment in both CIFAR-10 and ImageNet and compare our results with the reported ones of the other methods. The missing values in <ref type="table" target="#tab_4">Table 5</ref> stem from the fact that DOT did not report its results on SN-ResNet-GAN on CIFAR-10, and DG-f low was not tested on ImageNet or any other high-resolution dataset.</p><p>BIGRoC hyperparameters We report in the table below the value of epsilon used to attain the results in <ref type="table" target="#tab_4">Table 5</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Qualitative Results</head><p>In this section, we show additional qualitative results to further demonstrate the qualitative enhancement attained by our method. We use image generators of different qualities, both conditional and unconditional, and show the generated images and the boosted ones in <ref type="figure" target="#fig_7">Figures 7, 8, 9</ref> and 10. The images below are simply the 100 first synthesized images from each class.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Debiasing</head><p>In Section 4 we describe our debiasing algorithm, which aims to induce uniform class distribution over the outputs of BIGRoC. The rationale behind this procedure is expressed in Equation 2. To better understand the outcome of our debiasing, we present in <ref type="figure" target="#fig_0">Figure 11</ref> its effect visually. We compare the results of applying BIGRoC in the unconditional case, with and without debiasing. One can clearly see that it reduces the amount of the majority class and leads to a more uniform class distribution, by modifying the images accordingly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Robust Classifier based Image Interpolation</head><p>In addition to our contribution to enhancing image generation performance, we leverage the same robust classifier and its perceptually aligned gradient property for proposing an adversarial technique for image interpolation, which further demonstrates the refining capabilities of such models. In image synthesis, image interpolation is an operation that aims to generate intermediate pictures between two given images, while remaining meaningful and of gradual change. We propose to use features extracted by our robust classifier at different scales in order to guide a PGD-based interpolation process. More specifically, given a source image x s and a target image x t , we extract their corresponding features {F 1 xs , . . . , F k xs } and {F 1 xt , . . . , F k xt } from our trained robust classifier f ? . To perform interpolation, we modify x s such that its features will be more similar to those of x t at the different scales, using PGD. Mathematically, we aim to solve Equation 2:</p><formula xml:id="formula_4">arg min ??? k i=1 w i ? F i xs+? ? F i xt 2<label>(2)</label></formula><p>where {w i } k i=1 is an importance vector to the different features' scales. Namely, this optimization aims to find a perturbation ? ? ? that minimizes the 2 -distance between the images' representations. In the above formulation, the choice of the determines the level of change allowed in the process. Thus, solving the above optimization with different values leads to different intermediary images -as we increase , the interpolation resembles the target image more.</p><p>We conduct image interpolation experiments using the ImageNet dataset <ref type="bibr" target="#b6">(Deng et al., 2009</ref>) with a resolution of 224 ? 224 and the same adversarial robust ResNet-50 classifier, used for boosting image generation. We compare the visual results attained by our interpolation method using both robust and non-robust classifiers of the same architecture. As can be seen in <ref type="figure" target="#fig_10">Figure 12</ref>, the interpolation results using a robust classifier are much more convincing and the intermediary images are perceptually plausible. In contrast, the interpolation using a non-robust model leads to much inferior results, and the final interpolation does not look like the target image at all. This experiment further expresses the perceptual fidelity of features that adversarially robust classifiers capture.     <ref type="figure" target="#fig_0">Figure 11</ref>. Demonstration of the debiasing technique: We show 100 generated images by an unconditional SNGAN and the results of the BIGRoC algorithm, with and without the proposed debiasing. As can be seen, the outputs of the boosting algorithms are perceptually superior, while the histograms expose the fact that the suggested debiasing algorithm induces a more uniform labels distribution. In the "Boosting without Debiasing" experiment, 36 out of 100 images are classified as deers, and only 3 are horses. The most prominent deer images are marked in red. However, when applying the debiased boosting, the number of deers is reduced to 9, and the number of horses is increased to 15. We mark the boosted images that remain deer in red, and images that are modified to other minority classes in green. As can be seen, many of the deers were changed to be horses, a perceptually similar class. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 11</head><label>11</label><figDesc>in Appendix D presents an empirical demonstration that verifies the validity of this method and shows its qualitative effects on unconditional generation boosting.Algorithm 2 BIGRoC: Boosting Image Generation via a Robust Classifier Input: Robust classifier f ? , x gen , y gen , , step size ?, number of iterations T , d c if y gen is None then y gen = argmax(f ? (x gen ) + d c ) endx boost = T argeted P GD(f ? , x gen , y gen , , ?, T ) Output: x boost</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>BIGRoC's qualitative results: We present pairs of images -the left columns contain generated images, and the right ones contain the boosted results.Figure 3(a): We show six pairs of sets of images where each contains ten images, one per CIFAR-10 class. Each pair contains the generated images (left) by different CFAR-10 GANs, and their "boosted version" (right), attained by applying BIGRoC. Figure 3(b): We show pairs of images generated by different GANs, trained on ImageNet 128 ? 128 and their "boosted" version.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Qualitative comparison on ImageNet 256 ? 256. left: Images generated by guided diffusion. Middle: Images boosted by BIGRoCPL. Right: The difference after contrast stretching.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Trade-off effects of BIGRoC's on ImageNet 256?256.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 .</head><label>6</label><figDesc>Demonstration of BIGRoC's steps, demonstrated using VAE trained on CIFAR-10: On the left column we show the generated images of the model. The righmost column corresponds with the final output of BIGRoC. The middle 9 columns are the results obtained after each intermediate step of our algorithm.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>(a) A comparison between BigGAN generated images of class automobile and the corresponding boosted ones.(b) A comparison between BigGAN generated images of class bird and the corresponding boosted ones.(c) A comparison between BigGAN generated images of class dog and the corresponding boosted ones.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 .</head><label>7</label><figDesc>A qualitative comparison between BigGAN generated images of CIFAR-10 samples and the proposed BIGRoC algorithm.BIGRoC: Boosting Image Generation via a Robust ClassifierFigure 8. A qualitative comparison between an unconditional VAE generated images of CIFAR-10 samples and the proposed BIGRoC algorithm.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 9 .</head><label>9</label><figDesc>A qualitative comparison between an unconditional WGAN-GP generated images of CIFAR-10 samples and the proposed BIGRoC algorithm.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 10 .</head><label>10</label><figDesc>A qualitative comparison between an unconditional SSGAN generated images of CIFAR-10 samples and the proposed BIGRoC algorithm.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 12 .</head><label>12</label><figDesc>Image interpolation demonstration on ImageNet images. The interpolation process is achieved by an alternation of according to = c ? xt ? xs 2, c ? (0, 1].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>BIGRoCGT quantitative results on CIFAR-10.</figDesc><table><row><cell>ARCHITECTURE</cell><cell>FID</cell><cell>IS</cell></row><row><cell>CGAN</cell><cell>29.26 ? 0.10</cell><cell>6.95 ? 0.03</cell></row><row><cell>W/ BIGROC</cell><cell>8.89 ? 0.05</cell><cell>8.57 ? 0.05</cell></row><row><cell>CGAN-PD</cell><cell>11.10 ? 0.07</cell><cell>8.54 ? 0.03</cell></row><row><cell>W/ BIGROC</cell><cell>8.33 ? 0.06</cell><cell>8.76 ? 0.05</cell></row><row><cell>BIGGAN</cell><cell>7.45 ? 0.08</cell><cell>9.38 ? 0.05</cell></row><row><cell>W/ BIGROC</cell><cell>6.79 ? 0.02</cell><cell>9.47 ? 0.02</cell></row><row><cell>DIFF BIGGAN</cell><cell>4.37 ? 0.03</cell><cell>9.48 ? 0.03</cell></row><row><cell>W/ BIGROC</cell><cell>3.95 ? 0.02</cell><cell>9.61 ? 0.03</cell></row><row><cell>STYLEGAN2 ADA</cell><cell>2.44 ? 0.01</cell><cell>10.02 ? 0.04</cell></row><row><cell>W/ BIGROC</cell><cell>2.41 ? 0.01</cell><cell>10.07 ? 0.04</cell></row><row><cell>et al., 2019).</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>BIGRoCPL quantitative results on CIFAR-10.</figDesc><table><row><cell>ARCHITECTURE</cell><cell>FID</cell><cell>IS</cell></row><row><cell>VAE</cell><cell cols="2">152.04 ? 0.19 3.05 ? 0.01</cell></row><row><cell>W/ BIGROC</cell><cell>88.68 ? 0.37</cell><cell>6.27 ? 0.04</cell></row><row><cell>DCGAN</cell><cell>38.34 ? 0.11</cell><cell>6.10 ? 0.01</cell></row><row><cell>W/ BIGROC</cell><cell>29.93 ? 0.05</cell><cell>7.28 ? 0.04</cell></row><row><cell>WGAN-GP</cell><cell>22.62 ? 0.09</cell><cell>7.49 ? 0.03</cell></row><row><cell>W/ BIGROC</cell><cell>16.28 ? 0.08</cell><cell>8.15 ? 0.03</cell></row><row><cell>SNGAN</cell><cell>17.19 ? 0.07</cell><cell>8.04 ? 0.02</cell></row><row><cell>W/ BIGROC</cell><cell>13.25 ? 0.10</cell><cell>8.61 ? 0.04</cell></row><row><cell>INFOMAXGAN</cell><cell>15.41 ? 0.12</cell><cell>8.09 ? 0.05</cell></row><row><cell>W/ BIGROC</cell><cell>11.27 ? 0.11</cell><cell>8.48 ? 0.03</cell></row><row><cell>SSGAN</cell><cell>15.05 ? 0.06</cell><cell>8.20 ? 0.02</cell></row><row><cell>W/ BIGROC</cell><cell>10.77 ? 0.06</cell><cell>8.61 ? 0.03</cell></row><row><cell cols="3">et al., 2019), InfoMaxGAN (Lee et al., 2021), BigGAN-</cell></row><row><cell>deep</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Quantitative results on ImageNet 128 ? 128.</figDesc><table><row><cell>ARCHITECTURE</cell><cell>FID</cell><cell>IS</cell></row><row><cell>SNGAN</cell><cell>62.28</cell><cell>13.05</cell></row><row><cell>W/ BIGROCPL</cell><cell>40.40</cell><cell>71.67</cell></row><row><cell>SSGAN</cell><cell>63.60</cell><cell>13.75</cell></row><row><cell>W/ BIGROCPL</cell><cell>38.93</cell><cell>73.94</cell></row><row><cell>INFOMAXGAN</cell><cell>60.61</cell><cell>13.79</cell></row><row><cell>W/ BIGROCPL</cell><cell>37.70</cell><cell>75.49</cell></row><row><cell>BIGGAN-DEEP</cell><cell>6.02</cell><cell>145.83</cell></row><row><cell>W/ BIGROCPL</cell><cell>5.69</cell><cell>176.42</cell></row><row><cell>W/ BIGROCGT</cell><cell>5.71</cell><cell>226.17</cell></row><row><cell>GUIDED DIFFUSION</cell><cell>2.97</cell><cell>141.37</cell></row><row><cell>W/ BIGROCPL</cell><cell>2.77</cell><cell>150.43</cell></row><row><cell>W/ BIGROCGT</cell><cell>2.53</cell><cell>169.73</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Quantitative results on ImageNet 256 ? 256.</figDesc><table><row><cell>ARCHITECTURE</cell><cell>FID</cell><cell>IS</cell></row><row><cell>BIGGAN-DEEP</cell><cell cols="2">7.03 202.65</cell></row><row><cell>W/ BIGROCPL</cell><cell cols="2">6.93 221.78</cell></row><row><cell>W/ BIGROCGT</cell><cell cols="2">6.84 228.23</cell></row><row><cell cols="3">GUIDED DIFFUSION 3.94 215.84</cell></row><row><cell>W/ BIGROCPL</cell><cell cols="2">3.69 249.91</cell></row><row><cell>W/ BIGROCGT</cell><cell cols="2">3.63 260.02</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 .</head><label>5</label><figDesc>Quantitative comparison between BIGRoC and competitive methods on CIFAR-10 and ImageNet 128 ? 128, using IS.</figDesc><table><row><cell>ARCHITECTURE</cell><cell cols="2">INCEPTION SCORE CIFAR-10 IMAGENET</cell></row><row><cell>SN-RESNET-GAN</cell><cell>8.38</cell><cell>36.8</cell></row><row><cell>W/ DOT</cell><cell>-</cell><cell>37.29</cell></row><row><cell>W/ DDLS</cell><cell>9.09</cell><cell>40.2</cell></row><row><cell>W/ DG-f LOW</cell><cell>9.35</cell><cell>-</cell></row><row><cell>W/ BIGROC</cell><cell>9.31</cell><cell>44.68</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 .</head><label>6</label><figDesc>The effects of the robust classifier's threat model on BI-GRoC's performance, measured in FID, using WGAN-GP trained on CIFAR-10 dataset.</figDesc><table><row><cell>W/O BIGROC</cell><cell>2 = 0</cell><cell cols="2">W/ BIGROC 2 = 0.25 2 = 0.5</cell><cell>2 = 1</cell></row><row><cell>22.32</cell><cell>19.71</cell><cell>18.79</cell><cell>16.19</cell><cell>15.87</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 .</head><label>7</label><figDesc>The effect of the number of steps in BIGRoC's algorithm, measured in FID, using WGAN-GP trained on CIFAR-10 dataset.</figDesc><table><row><cell>W/O BIGROC</cell><cell cols="4">W/ BIGROC 1 STEP 7 STEPS 20 STEPS 30 STEPS</cell></row><row><cell>22.32</cell><cell>17.48</cell><cell>16.28</cell><cell>16.15</cell><cell>16.27</cell></row><row><cell>A. Additional Ablations</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>A.1. Effect of BIGRoC's number of steps</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 8 .</head><label>8</label><figDesc>BIGRoC</figDesc><table><row><cell cols="2">'s values in CIFAR-10 experiments</cell></row><row><cell>ARCHITECTURE</cell><cell>NORMALIZATION</cell></row><row><cell>VAE</cell><cell>25</cell></row><row><cell>DCGAN</cell><cell>5</cell></row><row><cell>WGAN-GP</cell><cell>5</cell></row><row><cell>SNGAN</cell><cell>3</cell></row><row><cell>SSGAN</cell><cell>3</cell></row><row><cell>CGAN</cell><cell>5</cell></row><row><cell>CGAN-PD</cell><cell>2</cell></row><row><cell>BIGGAN</cell><cell>1</cell></row><row><cell>DIFF BIGGAN</cell><cell>1</cell></row><row><cell cols="2">STYLEGAN2 ADA 0.28</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 9 .</head><label>9</label><figDesc>BIGRoC's values in ImageNet experiments</figDesc><table><row><cell cols="2">RESOLUTION ARCHITECTURE</cell><cell>NORMALIZATION</cell></row><row><cell></cell><cell>SNGAN</cell><cell>40</cell></row><row><cell>128</cell><cell>SSGAN INFOMAXGAN BIGGAN-DEEP</cell><cell>40 40 5</cell></row><row><cell></cell><cell cols="2">GUIDED DIFFUSION 1.5</cell></row><row><cell>256</cell><cell cols="2">BIGGAN-DEEP GUIDED DIFFUSION 1.5 1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 10 .</head><label>10</label><figDesc>BIGRoC's values inTable 5 experiments</figDesc><table><row><cell>DATASET</cell><cell>NORMALIZATION</cell></row><row><cell>CIFAR-10</cell><cell>1.8</cell></row><row><cell>IMAGENET</cell><cell>15</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/kwotsin/mimicry 2 https://github.com/mit-han-lab/data-efficient-gans/tree/master/DiffAugment-biggan-cifar 3 https://github.com/NVlabs/stylegan2-ada 4 https://github.com/openai/guided-diffusion</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">https://github.com/pfnet-research/sngan_projection</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Refining deep generative models via discriminator gradient flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">F</forename><surname>Ansari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">L</forename><surname>Ang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Soh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Discriminator rejection sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Azadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Olsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Odena</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Deep generative modelling: A comparative review of VAEs, GANs, Normalizing Flows, Energy-Based and Autoregressive Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bond-Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Leach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">G</forename><surname>Willcocks</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Large Scale GAN Training for High Fidelity Natural Image Synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">7th International Conference on Learning Representations</title>
		<imprint>
			<publisher>ICLR</publisher>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Your gan is secretly an energy-based model and you should use discriminator driven latent sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Paull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Self-Supervised GANs via Auxiliary Rotation Loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lucic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Houlsby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition, CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition, CVPR</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nichol</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.05233</idno>
		<title level="m">Diffusion models beat gans on image synthesis</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Engstrom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ilyas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Salman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Santurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Robustness</surname></persName>
		</author>
		<ptr target="https://github.com/MadryLab/robustness" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>python library</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Generative Adversarial Nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<meeting><address><addrLine>NeurIPS</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Explaining and Harnessing Adversarial Examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations, ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep Residual Learning for Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition, CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Nessler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gans</surname></persName>
		</author>
		<imprint/>
		<respStmt>
			<orgName>Scale Update Rule Converge to a Local Nash Equilibrium</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<title level="m">Advances in Neural Information Processing Systems</title>
		<meeting><address><addrLine>NeurIPS</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Denoising diffusion probabilistic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<meeting><address><addrLine>NeurIPS</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Image-toimage translation with conditional adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5967" to="5976" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Progressive growing of gans for improved quality, stability, and variation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lehtinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<publisher>ICLR</publisher>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A style-based generator architecture for generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aila</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition, CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4401" to="4410" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Training Generative Adversarial Networks with Limited Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Aittala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hellsten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lehtinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aila</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<meeting><address><addrLine>NeurIPS</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Analyzing and Improving the Image Quality of StyleGAN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Aittala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hellsten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lehtinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aila</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition, CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Auto-Encoding Variational Bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<publisher>ICLR</publisher>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
		<respStmt>
			<orgName>University of Toronto</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Adversarial examples in the physical world</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Infomax-gan: Improved adversarial image generation via information maximization and contrastive learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">T</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">M</forename><surname>Cheung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Towards Deep Learning Models Resistant to Adversarial Attacks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Madry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Makelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vladu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations, ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Conditional Generative Adversarial Nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">cGANs with Projection Discriminator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Koyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations, ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Spectral Normalization for Generative Adversarial Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kataoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshida</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations, ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Pixel Recurrent Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">V</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning, ICML</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1747" to="1756" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations, ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Variational Inference with Normalizing Flows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning, ICML</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1530" to="1538" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Improved Techniques for Training GANs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<meeting><address><addrLine>NeurIPS</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Image Synthesis with a Single (Robust) Classifier</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Santurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ilyas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Engstrom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Madry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<meeting><address><addrLine>NeurIPS</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Intriguing properties of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fergus</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations, ICLR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Discriminator optimal transport</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tanaka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems, NeurIPS</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Be at Odds with Accuracy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Santurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Engstrom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Turner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Madry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Robustness</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations, ICLR</title>
		<imprint>
			<date type="published" when="2019-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Metropolis-Hastings generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Turner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Saatchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yosinski</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning, ICML</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Differentiable augmentation for data-efficient GAN training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing System</title>
		<meeting><address><addrLine>NeurIPS</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Unpaired Imageto-Image Translation Using Cycle-Consistent Adversarial Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision, ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
