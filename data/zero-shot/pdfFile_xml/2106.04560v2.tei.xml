<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Scaling Vision Transformers</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
							<email>xzhai@google.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
							<email>akolesnikov@google.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
							<email>neilhoulsby@google.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
							<email>lbeyer@google.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Google</forename><surname>Research</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brain</forename><surname>Team</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z?rich</forename></persName>
						</author>
						<title level="a" type="main">Scaling Vision Transformers</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T19:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Attention-based neural networks such as the Vision Transformer (ViT) have recently attained state-of-the-art results on many computer vision benchmarks. Scale is a primary ingredient in attaining excellent results, therefore, understanding a model's scaling properties is a key to designing future generations effectively. While the laws for scaling Transformer language models have been studied, it is unknown how Vision Transformers scale. To address this, we scale ViT models and data, both up and down, and characterize the relationships between error rate, data, and compute. Along the way, we refine the architecture and training of ViT, reducing memory consumption and increasing accuracy of the resulting models. As a result, we successfully train a ViT model with two billion parameters, which attains a new state-of-the-art on ImageNet of 90.45% top-1 accuracy. The model also performs well for few-shot transfer, for example, reaching 84.86% top-1 accuracy on ImageNet with only 10 examples per class.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Attention-based Transformer architectures <ref type="bibr" target="#b44">[45]</ref> have taken computer vision domain by storm <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b15">16]</ref> and are becoming an increasingly popular choice in research and practice. Previously, Transformers have been widely adopted in the natural language processing (NLP) domain <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b14">15]</ref>. Optimal scaling of Transformers in NLP was carefully studied in <ref type="bibr" target="#b21">[22]</ref>, with the main conclusion that large models not only perform better, but do use large computational budgets more efficiently. However, it remains unclear to what extent these findings transfer to the vision domain, which has several important differences. For example, the most successful pre-training schemes in vision are supervised, as opposed to unsupervised pre-training in the NLP domain.</p><p>In this paper we concentrate on scaling laws for transfer performance of ViT models pre-trained on image classificaequal contribution tion tasks. In particular, we experiment with models ranging from five million to two billion parameters, datasets ranging from one million to three billion training images and compute budgets ranging from below one TPUv3 core-day to beyond 10 000 core-days. Our main contribution is a characterization of the performance-compute frontier for ViT models, on two datasets. Along the way, we create an improved large-scale training recipe. We investigate training hyper-parameters and discover subtle choices that make drastic improvements in few-shot transfer performance. The few-shot transfer evaluation protocol has also been adopted by previous large-scale pre-training efforts in NLP domain <ref type="bibr" target="#b5">[6]</ref>. Specifically, we discover that very strong L2 regularization, applied to the final linear prediction layer only, results in a learned visual representation that has very strong few-shot transfer capabilities. For example, with just a single example per class on the ImageNet dataset (which has 1 000 classes), our best model achieves 69.52% accuracy; and with 10 examples per class it attains 84.86%. In addition, we substantially reduce the memory footprint of the original ViT model proposed in <ref type="bibr" target="#b15">[16]</ref>. We achieve this by hardware-specific architecture changes and a different optimizer. As a result, we train a model with two billion parameters and attain a new state-of-the-art 90.45% accuracy on ImageNet. 10 0 10 1 10 2 10 3 <ref type="bibr">10 4</ref> Compute (TPUv3 core days) <ref type="bibr" target="#b9">10</ref>  Model Size (Gflops) 10 2 <ref type="bibr">10 3</ref> Dataset Size (M) <ref type="figure" target="#fig_1">Figure 2</ref>. Left/Center: Representation quality, measured as ImageNet finetune and linear 10-shot error rate, as a function of total training compute. A saturating power-law approximates the Pareto frontier fairly accurately. Note that smaller models (blue shading), or models trained on fewer images (smaller markers), saturate and fall off the frontier when trained for longer. Top right: Representation quality when bottlenecked by model size. For each model size, a large dataset and amount of compute is used, so model capacity is the main bottleneck. Faintly-shaded markers depict sub-optimal runs of each model. Bottom Right: Representation quality by datasets size. For each dataset size, the model with an optimal size and amount of compute is highlighted, so dataset size is the main bottleneck.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Core Results</head><p>We first present our main results on scaling trends, before presenting detailed architecture and training protocol improvements in Section 3. In the following experiments, we train several ViT models on both public ImageNet-21k <ref type="bibr" target="#b13">[14]</ref> dataset and privately gathered images, up to three billion weakly-labelled images. We vary the architecture size, number of training images, and training duration. All models are trained on TPUv3, thus total compute is measured in TPUv3 core-days. To evaluate the quality of the representation learned by the models, we measure (i) few-shot transfer via training a linear classifier on frozen weights, (ii) transfer via fine-tuning the whole model on all data, both to multiple benchmark tasks. <ref type="figure" target="#fig_1">Figure 2</ref> shows both the 10-shot linear evaluation and finetuning evaluation on ImageNet <ref type="bibr" target="#b13">[14]</ref>. Similar trends on other datasets, Oxford IIIT Pets <ref type="bibr" target="#b27">[28]</ref>, CIFAR-100 <ref type="bibr" target="#b23">[24]</ref>, and Caltech-UCSD Birds <ref type="bibr" target="#b46">[47]</ref> are presented in the Appendix, <ref type="figure">Figure 9</ref>. For each combination of model size and data size we pre-train for various numbers of steps. In <ref type="figure" target="#fig_1">Figure 2</ref>, connected points represent the same model trained for a different number of steps. We make the following observations. First, scaling up compute, model and data together improves representation quality. In the left plot and center plot, the lower right point shows the model with the largest size, dataset size and compute achieving the lowest error rate. However, it appears that at the largest size the models starts to saturate, and fall behind the power law frontier (linear relationship on the log-log plot in <ref type="figure" target="#fig_1">Figure 2</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Scaling up compute, model and data together</head><p>Second, representation quality can be bottlenecked by model size. The top-right plot shows the best attained performance for each model size. Due to limited capacity, small models are not able to benefit from either the largest dataset, or compute resources. <ref type="figure" target="#fig_1">Figure 2</ref>, left and center, show the Ti/16 model tending towards a high error rate, even when trained on a large number of images.</p><p>Third, large models benefit from additional data, even beyond 1B images. When scaling up the model size, the representation quality can be limited by smaller datasets; even 30-300M images is not sufficient to saturate the largest models. In <ref type="figure" target="#fig_1">Figure 2</ref>, center, the error rate of L/16 model on the the 30M dataset does not improve past 27%. On the larger datasets, this model attains 19%. Further, when increasing the dataset size, we observe a performance boost with big models, but not small ones. The largest models even obtain a performance improvement the training set size grows from 1B to 3B images <ref type="figure" target="#fig_1">(Figure 2</ref>, bottom right). For small models, however, such as Ti/16 or B/32, increasing the dataset size does not help. For example, in <ref type="figure" target="#fig_1">Figure 2</ref>, left and center, all of the curves for Ti/16 overlap, showing that this model achieves the same performance irrespective of the dataset size. For over two orders of magnitude of compute, the relation- ship between compute and performance follows a power-law (E = aC b ), resulting in a straight line on the log-log plot. However, we observe "saturation" at both ends of the compute spectrum. At the higher end of compute, the largest models do not tend towards zero error-rate. If we extrapolate from our observations, an infinite capacity model will obtain a non-zero error. This effect has also been observed for generative models <ref type="bibr" target="#b18">[19]</ref>. The authors of <ref type="bibr" target="#b18">[19]</ref> refer to this residual error as the "irreducible entropy" of the task. Since we plot error rate, the information-theoretic interpretation does not apply, but our observations support the notion of fundamental performance ceilings for ImageNet <ref type="bibr" target="#b3">[4]</ref>. In terms of the law, this saturation corresponds to an additive constant to the error rate:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Double-saturating power law</head><formula xml:id="formula_0">c in E = aC ?b + c.</formula><p>At the lower end of the compute spectrum, we see a saturation for smaller models; the performance of the smallest model is better than that would be predicted by a power-law. This saturation occurs because even trivial solutions can achieve non-zero error. For example, predicting the majority class (almost zero compute) will achieve an accuracy related to its occurence frequency in the test set. This lower bound is not observed in <ref type="bibr" target="#b18">[19]</ref>, either because their smallest model is large enough to avoid this region, or because log-loss saturates at worse performances than accuracy (it will saturate eventually). This saturation corresponds to a shift in the xaxis: d in E = a(C + d) ?b + c. This constant indicates that the zero-compute model will still obtain non-zero accuracy. <ref type="figure">Figure 3</ref> shows the representation quality with respect to the total number of images "seen" (batch size times number of steps) during pre-training. In addition to ImageNet finetuning and linear 10-shot results on the public validation set, we also report results of the ImageNet fine-tuned model on the ImageNet-v2 test set <ref type="bibr" target="#b32">[33]</ref> as an indicator of robust generalization. Three ViT models pre-trained on three billion images are presented in this plot.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Big models are more sample efficient</head><p>We observe that bigger models are more sample efficient, reaching the same level of error rate with fewer seen images. For 10-shot, the Ti/16 model needs to see nearly 100 times more images to match the representation quality of the L/16 model. When fine-tuning, this factor reduces from 100 to about 20. Our results suggest that with sufficient data, training a larger model for fewer steps is preferable. This observation mirrors results in language modelling and machine translation <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b25">26]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Do scaling laws still apply on fewer images?</head><p>We extend the study to much fewer images, ranging from one million to 13 millions on the public ImageNet-21k  dataset. In <ref type="figure">Figure 4</ref> left, we found that the double-saturation power law still applies, when varying model sizes, dataset sizes and compute resources. This indicates that the conclusions from the study generalizes well, and can guide future design choices for vision transformer architectures. In <ref type="figure">Fig</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.">ViT-G/14 results</head><p>We trained a large Vision Transformer, ViT-G/14, which contains nearly two billion parameters. Section 3.6 details the architecture's shape. We evaluate the ViT-G/14 model on a range of downstream tasks, and compare it to recent stateof-the-art results. We fine-tune on ImaegNet, and report Im-ageNet <ref type="bibr" target="#b33">[34]</ref>, ImageNet-v2 <ref type="bibr" target="#b32">[33]</ref>, ReaL <ref type="bibr" target="#b3">[4]</ref>, and ObjectNet <ref type="bibr" target="#b1">[2]</ref> accuracies. In addition, we report transfer learning result on the VTAB-1k benchmark consisting of 19 tasks <ref type="bibr" target="#b52">[53]</ref>. <ref type="figure" target="#fig_0">Figure 1</ref> shows the few-shot transfer results on Ima-geNet. ViT-G/14 outperforms the previous best ViT-H/14 model <ref type="bibr" target="#b15">[16]</ref> by a large margin (more than 5%), attaining 84.86% accuracy with 10 examples per class. Ten images per class is less than 1% of ImageNet data (13 examples per class), as commonly used in self-supervised and semisupervised learning <ref type="bibr" target="#b51">[52]</ref>. For reference, <ref type="figure" target="#fig_0">Figure 1</ref> shows three state-of-the-art self-supervised learning models, Sim-CLR v2 <ref type="bibr" target="#b9">[10]</ref> and BYOL <ref type="bibr" target="#b16">[17]</ref>, using 1% of ImageNet data, DINO <ref type="bibr" target="#b8">[9]</ref> using 20 examples per class. Note, however, that these approaches are quite different: ViT-G/14 uses large source of weakly-supervised data, and is pre-trained only once and transferred to different tasks. Meanwhile, the selfsupervised learning models use unlabeled but in-domain data for pre-training, and target a single task. <ref type="table" target="#tab_2">Table 1</ref> shows the results on the remaining benchmarks. ViT-G/14 achieves 90.45% top-1 accuracy on ImageNet, setting the new state-of-the art. On ImageNet-v2, ViT-G/14 improves 3% over the Noisy Student model <ref type="bibr" target="#b48">[49]</ref> based on EfficientNet-L2. For ReaL, ViT-G/14 outperforms ViT-H <ref type="bibr" target="#b15">[16]</ref> and BiT-L <ref type="bibr" target="#b22">[23]</ref> by only a small margin, which in-dicates again that the ImageNet classification task is likely reaching its saturation point. For ObjectNet, ViT-G/14 outperforms BiT-L [23] by a large margin, and is 2% better than Noisy Student, but is about 2% behind CLIP <ref type="bibr" target="#b30">[31]</ref>. Note that, unlike the other methods, CLIP does not fine-tune on ImageNet, and evaluates directly on ObjectNet, this likely improves its robustness. Finally, when transferring the ViT-G/14 model to VTAB, it gets consistently better results with just a single hyper parameter across all tasks. The state-ofthe-art on VTAB using a heavyweight per-task hyperparameter sweep is 79.99 <ref type="bibr" target="#b20">[21]</ref>, we leave running a heavy sweep with ViT-G/14 to future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method details</head><p>We present a number of improvements to the ViT model and training. These improvements are mostly simple to implement, and can significantly improve memory-utilization and model quality. They allow us to train ViT-G/14 using data-parallelism alone, with the entire model fitting on a single TPUv3 core.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Decoupled weight decay for the "head"</head><p>Weight decay has a drastic effect on model adaptation in the low-data regime. We conduct an study of this phenomena at a mid-size scale.</p><p>We find that one can benefit from decoupling weight decay strength for the final linear layer ("head"), and for the remaining weights ("body") in the model. <ref type="figure" target="#fig_4">Figure 5</ref> demonstrates this effect: we train a collection ViT-B/32 models on JFT-300M, each cell corresponds to the performance of different head/body weight decay values. The diagonal corresponds to using the same value for both decays. One can observe that the best performance appears off-diagonal (i.e. with a decoupled weight decay for the head and body). Interestingly, we observe that high weight decay in the head decreases performance on the pre-training (upstream) task (not shown), despite improving transfer performance.</p><p>We do not have a complete explanation of this phenomena. However, we hypothesize that a stronger weight decay in the  Normally, a single weight decay value is applied to all weights (corresponds to the diagonal on the heatmaps). We show that by using weight decay values for the "head" and the rest of the weights one significantly improves few-shot transfer performance. Right: Few-shot performance on ImageNet for different types of head. A high weight decay on the head works equally well for all of them.</p><p>head results in representations with larger margin between classes, and thus better few-shot adaptation. This is similar to the main idea behind SVMs <ref type="bibr" target="#b11">[12]</ref>. This large decay makes it harder to get high accuracy during upstream pre-training, but our main goal is high quality transfer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Saving memory by removing [class] token</head><p>The largest VIT model from <ref type="bibr" target="#b15">[16]</ref> uses 14 ? 14 patches with 224 ? 224 images. This results in 256 visual "tokens", where each one corresponds to an image patch. On top of this, ViT models have an extra [class] token, which is used to produce the final representation, bringing the total number of tokens to 257.</p><p>For ViT models, current TPU hardware pads the token dimension to a multiple of 128, which may result in up to a 50% memory overhead. To overcome this issue we investigate alternatives to using the extra [class] token. In particular, we evaluate global average pooling (GAP) and multihead attention pooling (MAP) <ref type="bibr" target="#b24">[25]</ref> to aggregate representation from all patch tokens. We set the number of heads in MAP to be equal to the number of attention heads in the rest of the model. To further simplify the head design we remove final non-linear projection before the final prediction layer, which was present in the original ViT paper.</p><p>To choose the best head, we perform a side-by-side comparison of a [class] token and GAP/MAP heads. Results are summarized in <ref type="figure" target="#fig_4">Figure 5</ref> (right). We find that all heads perform similarly, while GAP and MAP are much more memory efficient due to the aforementioned padding considerations. We also observe that non-linear projection can be safely removed. Thus, we opt for the MAP head, since it is the most expressive and results in the most uniform architecture. MAP head has also been explored in <ref type="bibr" target="#b41">[42]</ref>, in a different context for better quality rather than saving memory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Scaling up data</head><p>For this study, we use the proprietary JFT-3B dataset, a larger version of the JFT-300M dataset used in many previous works on large-scale computer vision models <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b36">37]</ref>. This dataset consists of nearly 3 billion images, annotated with a class-hierarchy of around 30k labels via a semiautomatic pipeline. Thus, the data and associated labels are noisy. We ignore the hierarchical aspect of the labels and use only the assigned labels as targets for multi-label classification via a sigmoid cross-entropy loss, following <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b22">23]</ref>.</p><p>We have conducted sensitive category association analysis as described in <ref type="bibr" target="#b0">[1]</ref>. We measured (per label) the distribution of sensitive categories across the raw data, the cleaned data, the models trained on this data, and labels that were verified by human raters. Human raters additionally assisted in removing offensive content from the dataset. <ref type="figure" target="#fig_5">Figure 6</ref> shows an ablation of the effect of changing from JFT-300M to JFT-3B on model performance, even when scale is not increased. <ref type="figure" target="#fig_5">Figure 6</ref>, left shows linear 10-shot Im-ageNet performance evaluated throughout. We observe that JFT-3B results in a better model, even before the model has completely one epoch of JFT-300M. Therefore, overfitting JFT-300M is not the sole cause of the improvement. This difference can be seen even for the small B/32 model as well as the larger L/16. We fine-tune the models to the full ImageNet dataset (right), and confirm that these improvements transfer to a full fine-tuning setup. Overall, the change in dataset improves transfer to ImageNet by about 1% for both small and large models. Other than the performance improvement, training behavior is similar on JFT-300M and JFT-3B. Most importantly, JFT-3B allows us to scale up further with fewer concerns about overfitting and regularization.</p><p>Deduplication. We remove all images from the JFT-3B dataset that are near-duplicates of images from both train set and test set of datasets we evaluate on. Overall we identified and removed 927k duplicate images from JFT-3B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Memory-efficient optimizers</head><p>When training large models, storage required for model parameters becomes a bottleneck. Our largest model, ViT-G, has roughly two billion parameters, which occupies 8 GiB of device memory. To make things much worse, the Adam optimizer that is commonly used for training Transformers, stores two additional floating point scalars per each parameter, which results in an additional two-fold overhead (extra 16 GiB). To tackle the overhead introduced by the Adam optimizer we explore two modifications.</p><p>Adam with half-precision momentum. We empirically observe that storing momentum in half-precision (bfloat16 type) does not affect training dynamics and has no effect on the outcome. This allows to reduce optimizer overhead from 2-fold to 1.5-fold. Notably, storing the second momentum using half-precision resulted in a significant performance deterioration.</p><p>Adafactor optimizer. The above optimizer still induces a large memory overhead. Thus, we turn our attention to the Adafactor optimizer <ref type="bibr" target="#b34">[35]</ref>, which stores second momentum using rank 1 factorization. From practical point of view, this results in the negligible memory overhead. However, the Adafactor optimizer did not work out of the box, so we make the following modifications:</p><p>? We re-introduce the first momentum in half-precision, whereas the recommended setting does not use the first momentum at all.</p><p>? We disable scaling of learning rate relative to weight norms, a feature that is part of Adafactor.</p><p>? Adafactor gradually increases the second momentum from 0.0 to 1.0 throughout the course of training. In our preliminary experiments, we found that clipping the second momentum at 0.999 (Adam's default value) results in better convergence, so we adopt it.</p><p>The resulting optimizer introduces only a 50% memory overhead on top the space needed to store model's parameters. We observe that both proposed optimizers perform on par with or slightly better than the original Adam optimizer. We are aware of other memory-efficient optimizers <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b39">40]</ref>, we leave the exploration to future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Learning-rate schedule</head><p>In our study we want to train each of the models for several different durations in order to measure the tradeoff between model size and training duration. When using linear decay, as in <ref type="bibr" target="#b15">[16]</ref>, each training duration requires its own training run starting from scratch, which would be an inefficient protocol.</p><p>Inspired by <ref type="bibr" target="#b26">[27]</ref>, we address this issue by exploring learning-rate schedules that, similar to the warmup phase in the beginning, include a cooldown phase at the end of training, where the learning-rate is linearly annealed toward zero. Between the warmup and the cooldown phases, the learning-rate should not decay too quickly to zero. This can be achieved by using either a constant, or a reciprocal squareroot schedule for the main part of training. <ref type="figure" target="#fig_7">Figure 7</ref> (bottom) depicts several of these options, with a cooldown after approximately 200 k, 400 k, and 500 k steps. The upper half of <ref type="figure" target="#fig_7">Figure 7</ref> shows the validation score (higher is better) for each of these options and their cooldowns, together with two linear schedules for reference. While the linear schedule is still preferable when one knows the training duration in advance and does not intend to train any longer, all three alternatives come reasonably close, with the advantage of allowing indefinite training and evaluating multiple training durations from just one run. For each of the schedules, we optimized the learning-rate and the exact shape. We have also briefly  tried cyclic learning-rate schedules, however they seemed to perform much worse and we have not investigated further. We therefore opt for the reciprocal square-root schedule.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.">Selecting model dimensions</head><p>ViT models have many parameters that control the model's shape, and we refer to the original publication for full details. Briefly, these include the patch-size, the number of encoder blocks (depth), the dimensionality of patch embeddings and self-attention (width), the number of attention heads, and the hidden dimension of MLP blocks (MLP-width). On top of this, we rely on the XLA compiler to optimize our models for runtime speed and memory footprint. Behind the scenes, XLA uses complex heuristics to compile a model into code for a specific hardware that trades off memory and speed optimally. As a result, it is hard to predict which model configurations will fit into memory on a single device.</p><p>Therefore we run an extensive simulation, where we instantiate a large amount of ViTs of various shapes, and attempt to train them for a few steps, without considering the quality. We vary the depth, width, heads, and MLP-width, but keep the patch-size at 14 px. In this way, we measure their speed and whether or not a given model fits into the device's memory. <ref type="figure" target="#fig_8">Figure 8</ref> summarizes the result of this simulation. Each block corresponds to one model configuration, the shade of the block corresponds to its training speed (brighter is faster). Orange blocks show which original ViT models, without any of our modifications, fit. Green blocks then further include the memory savings described in Section 3.2 coupled with the half-precision Adam described in Section 3.4. Finally, blue blocks are with our modified AdaFactor optimizer. The shapes in the white area were not able to fit into memory in any setting. For space reasons, we show here only the models pertaining to the experiments presented, but note that with our modifications we were able to fit thin ViT models of a depth up to 100 encoder blocks.</p><p>The original Vision Transformer publication contains a study in Appendix D2 about the trade-offs between scaling the different components, concluding that it is most effective to scale all aspects (depth, width, MLP-width, and patchsize) simultaneously and by a similar amount. We follow this recommendation, and select shapes for ViT-g and ViT-G at the limit of what fits in memory accordingly, as shown in <ref type="figure" target="#fig_8">Figure 8</ref> and summarized in <ref type="table" target="#tab_4">Table 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Related Work</head><p>Smaller Vision Transformers Early work on Transformers for vision focused on small networks for CIFAR-10 <ref type="bibr" target="#b10">[11]</ref>. The Vision Transformer <ref type="bibr" target="#b15">[16]</ref>, however, was proposed in the context of state-of-the-art medium and largescale image recognition; the smallest model (ViT-B) containing 86M parameters. <ref type="bibr" target="#b40">[41]</ref> present smaller ViT sizes for training from-scratch, down to ViT-Ti, with 5M parameters. New variants of ViT introduce smaller and cheaper architectures. For example, T2T-ViT <ref type="bibr" target="#b50">[51]</ref> reduces the number of parameters and compute using a new tokenization and narrower networks. Pyramidal ViTs <ref type="bibr" target="#b45">[46]</ref>, designed for dense prediction tasks, follow a CNN-like pyramidal structure, that also reduces the size of the model. Hybrids of CNNs and Transformers typically allow smaller models to perform well, such as the ViT-CNN hybrid in <ref type="bibr" target="#b15">[16]</ref>, BoTNet <ref type="bibr" target="#b35">[36]</ref>, and HaloNet <ref type="bibr" target="#b43">[44]</ref>. However, the other direction, increasing the scale of ViT, is less explored. While language Transformers are still much larger than Vision Transformers, understanding the scaling properties and the improvements introduced in this paper represent a step in this direction.</p><p>Scaling Laws <ref type="bibr" target="#b21">[22]</ref> present a thorough study of the empirical scaling laws of neural language models. The authors  fit power laws that describe the relationships between compute, data size, model size, and performance. Following these laws, GPT-3, a 175B parameter language model was successfully trained <ref type="bibr" target="#b6">[7]</ref>. <ref type="bibr" target="#b18">[19]</ref> presents laws for autoregressive generative modelling in other modalities, including the generation of images. Our paper contains the first study of scaling laws for the discriminative modelling of images.</p><p>Scaling-up Vision Models Many papers scale up CNNs to attain improved performance. EfficientNets <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b38">39]</ref> present a scaling strategy that balances compute between depth, width, and resolution and apply it to MobileNets. This strategy is revisited in <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b47">48]</ref> to further improve the performance of ResNets <ref type="bibr" target="#b17">[18]</ref>. Large CNNs have attained excellent performance in visual recognition, such as AmoebaNet-B(18, 512) (557M parameters) trained using GPipe pipeline parallelism <ref type="bibr" target="#b19">[20]</ref>, ResNeXt-101 32?48d (829M parameters) pre-trained on weakly-labelled Instagram images <ref type="bibr" target="#b26">[27]</ref>, EfficientNet-L2 (480M parameters) trained with ImageNet pseudo-labels on JFT-300M <ref type="bibr" target="#b49">[50]</ref>, and BiT-L-ResNet152x4 (928M parameters) pre-trained on JFT-300M <ref type="bibr" target="#b22">[23]</ref>. Recently, <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b53">54]</ref> explore strategies to scale the depth of ViTs. We are the first to scale Vision Transformers to even larger size and reache new state-of-the-art results doing so. The concurrent work <ref type="bibr" target="#b12">[13]</ref> focuses on CNN and ViT hybrid architectures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Discussion</head><p>Limitations. This work uses the proprietary JFT-3B dataset for the scaling laws study. To make our insights more reliable and generalizable, we verify that the scaling laws also apply on the public ImageNet-21k dataset.</p><p>Societal impact. A potential broader cost of this work is the energy required to perform the experiments in our scaling study, especially in training the largest ViT-G model. How-ever, this cost may be amortized in two ways. First, such studies of scaling laws need only be performed once; We hope future developers of ViT models may use our results to design models that can be trained with fewer compute resources. Second, the models trained are designed primarily for transfer learning. Transfer of pre-trained weights is much less expensive than training from scratch on a downstream task, and typically reaches higher accuracy. Therefore, by transferring our models to many tasks, the pre-training compute is further amortized.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>We demonstrate that the performance-compute frontier for ViT models with enough training data roughly follows a (saturating) power law. Crucially, in order to stay on this frontier one has to simultaneously scale compute and model size; that is, not increasing a model's size when extra compute becomes available is suboptimal. We also demonstrate that larger models are much more sample efficient and are great few-shot learners. Finally, we present a new training recipe, which allows one to efficiently train large and highperforming ViT models. Note, that our conclusions may not necessarily generalize beyond the scale we have studied and they may not generalize beyond the ViT family of models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. More few-shot transfer results</head><p>We observe similar scaling laws on more datasets, including Oxford IIIT Pets <ref type="bibr" target="#b27">[28]</ref>, CIFAR-100 <ref type="bibr" target="#b23">[24]</ref>, and Caltech-UCSD Birds <ref type="bibr" target="#b46">[47]</ref>. The results are presented in <ref type="figure">Figure 9</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Pre-training details</head><p>We pre-train all the ViT models using adafactor optimizer with half-precision momentum. We use the default ? 1 = 0.9 and ? 2 = 0.999 (clipping threshold) for adafactor. We use batch size 4096 for all the models smaller than ViT-g. For models ViT-g and ViT-G, to speed up the training, we scale up the batch size at most to 32 768 and distribute the training to 2048 TPUv3 chips. We set weight decay to 3.0 for the "head" and 0.03 for the "body". All the models are pretrained at resolution 224 ? 224, with inception crop followed by random horizontal flip pre-process. We use reciprocal square-root schedule with a linear learning rate warmup of 10k steps. We cooldown the training at multiple steps as noted in Tables from Section G.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Configuration file for pre-training ViT-g</head><p>We present the full configuration for training the ViT-g/14 model. It follows the big_vision codebase <ref type="bibr" target="#b4">[5]</ref> conventions. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Adaptation details</head><p>We report both the few-shot linear regression and finetune results on mutiple datasets. For few-shot linear regression, we simply solve the l2?regularized linear regression problem, using the frozen embeddings extracted from 224 ? 224 resolution images.</p><p>For finetune evaluation, we use SGD optimizer with momentum. We use batch size 512 and gradient clipping at global norm 1. We do not use weight decay for finetuning. Following <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b42">43]</ref>, we use higher resolution for finetuning. More specifically, we use 384 ? 384 resolution for ViT models smaller than ViT-g, and 518 ? 518 resolution for both ViT-g and ViT-G. We use Polyak averaging <ref type="bibr" target="#b29">[30]</ref> only for the ViT-G model during fine-tuning, similar to <ref type="bibr" target="#b15">[16]</ref>. We use a cosine learning rate schedule for 20k steps by default, except a flat learning rate for ViT-G with Polyak averaging. We linearly warm up the learning rate for 500 steps. We sweep over two learning rates {0.03, 0.01} and choose the better one using a held-out 2% training split. On VTAB tasks, we use a fixed 0.01 learning rate with a cosine learning rate schedule. We train for 2 500 steps in total.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Impact of resolution and patch size</head><p>In this section, we answer the question of "what happens if we scale up the resolution, while keeping number of tokens fixed?". We perform experiments on ImageNet-21K to verify this point, by scaling the resolution and patch size linearly together. We observed in <ref type="table" target="#tab_7">Table 3</ref> that the quality difference is pretty subtle if we increase patch and resolution together. What matters for ViT architecture is the total number of 10 0 10 1 10 2 10 3 10 4</p><p>Compute (TPUv3 core days) <ref type="bibr" target="#b19">20</ref>  Compute (TPUv3 core days)  <ref type="figure">Figure 9</ref>. Representation quality as a function of total training compute. Representation quality is measured as few-shot error rate on four datasets. Sometimes, like in pets 5/10shot, the law does not fit the evidence perfectly, maybe the models are not ideal or the law is not universal.</p><p>patches, which has already been covered in <ref type="table" target="#tab_4">Table 2</ref> with different patch sizes: 32, 28, 16, and 14. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Full table of few-shot results</head><p>We provide the 5-shot learning and 10-shot learning results, on the four datasets from <ref type="figure">Figure 9</ref>. Both ViT-g/14 and ViT-G/14 are summarized in <ref type="table">Table 4</ref>. Note that we use at most 32 768 batch size for ViT-g/14 and ViT-G/14. To make the following tables more readable, we normalize the number of steps in <ref type="table">Table 4</ref> assuming the batch size is always 4096, i.e. Images Seen/4096. All the other smaller ViT models are summarized in <ref type="table">Table 5</ref>, <ref type="table">Table 6, Table 7</ref>, <ref type="table">Table 8</ref>, <ref type="table">Table 9</ref>, <ref type="table" target="#tab_2">Table 10</ref>, <ref type="table" target="#tab_2">Table 11</ref>, <ref type="table" target="#tab_2">Table 12</ref>, <ref type="table" target="#tab_2">Table 13</ref>. We are aware of a few missing rows, which do not affect the trend for the scaling laws plot.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Full table of finetune results</head><p>We provide the finetune results on ImageNet, as well as the results evaluated on the other two ImageNet V2 and Ima-geNet ReaL test splits. Results for all the models could be found from <ref type="table" target="#tab_2">Table 14, Table 15, Table 16</ref>, <ref type="table" target="#tab_2">Table 17, Table 18</ref>, <ref type="table" target="#tab_2">Table 19</ref>, <ref type="table" target="#tab_4">Table 20</ref>, <ref type="table" target="#tab_2">Table 21, Table 22</ref>. We are aware of a few missing rows, which do not affect the trend for the scaling laws plot. We show the total steps and the cooldown steps for each model, as well as the best finetune learning rate selected on ImageNet held-out 2% training split. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Few-shot transfer results. Our ViT-G model reaches 84.86% top-1 accuracy on ImageNet with 10-shot linear evaluation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 ,</head><label>2</label><figDesc>left and center, show the Pareto frontier of representation quality versus training compute. The frontier contains the models with the best allocation of compute to model shape and training duration.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>10</head><label>10</label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Left and middle: The dependence of 5-shot ImageNet accuracy and upstream performance depends on the weight decay strength.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 .</head><label>6</label><figDesc>The effect of switching from JFT-300M to JFT-3B, without any further scaling. Both small and large models benefit from this change, by an approximately constant factor, both for linear few-shot evaluation (left) and transfer using the full dataset (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 .</head><label>7</label><figDesc>Various "infinite" learning-rate schedules, along with the finite linear one for reference.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 .</head><label>8</label><figDesc>Combined results of the "Shapefinder" simulation for the original ViT in orange, our improvements together with half-precision Adam (e.g. ViT-g) in green, and finally with our modified AdaFactor in blue. White areas ran out of memory. The brightness of the dot corresponds to its relative training speed.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>11 # 30 #Listing 1 .</head><label>11301</label><figDesc>Fits 32 images per TPUv3 core with ViT-g/14. 12 config.batch_size = 4096 * 4 13 14 pp_common = '|value_range(-1, 1)' 15 pp_common += f'|onehot({config.num_classes})' 16 pp_common += '|keep("image", "labels")' 17 config.pp_train = 'inception_crop(224)|flip_lr' Model section 31 config.model_name = 'vit' 32 config.model = mlc.ConfigDict() 33 config.model.variant = 'g/14' 34 config.model.pool_type = 'map' Full config for ViT-g/14 pre-training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Figure 3. Error rate on ImageNet, with respect to images seen during pre-training. Big models are more sample efficient, which is consistent across diverse setups: few-shot transfer on the frozen representations, fine-tune the network on ImageNet, and evaluate the fine-tuned models on the v2 test set.</figDesc><table><row><cell>20 30 40 50 60 70 80</cell><cell>10 2</cell><cell>10 3 Images Seen (M) ImageNet 10-shot error rate [%] 10 4 L/16 B/32 Ti/16</cell><cell>10 20 30 40</cell><cell>10 2</cell><cell>10 3 Images Seen (M) ImageNet finetune error rate [%] 10 4 Ti/16 B/32 L/16</cell><cell>20 30 40 50</cell><cell>10 2 ImageNet finetune V2 error rate [%] 10 3 Images Seen (M) 10 4 Ti/16 B/32 L/16</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 .</head><label>1</label><figDesc>The results for ViT-G/14, compared to the previous state-of-the-art models.</figDesc><table><row><cell></cell><cell></cell><cell>1.3M</cell><cell>Ti/16</cell><cell>2.6M</cell><cell>S/16</cell><cell>5.1M</cell><cell>B/32</cell><cell>10.2M</cell><cell>B/16</cell><cell>13M</cell></row><row><cell>80</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>30 40 50 60 70 ImageNet 10-shot error rate [%]</cell><cell cols="4">E = 0.24 + 0.96(C + 1.57) 0.61</cell><cell></cell><cell></cell><cell></cell><cell cols="2">10 1 Model Size (Gflops)</cell></row><row><cell></cell><cell>0</cell><cell cols="5">10 1 Compute (TPUv3 core days) 10 2</cell><cell></cell><cell cols="2">10 1 Dataset Size (M)</cell></row></table><note>Figure 4. Results on the ImageNet-21k dataset. Left: Representa- tion quality, measured as ImageNet linear 10-shot error rate, as a function of total training compute. The double-saturating power law still applies. Right: Representation quality by model sizes and dataset sizes.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 .</head><label>2</label><figDesc>Model architecture details.</figDesc><table><row><cell>Name</cell><cell>Width</cell><cell>Depth</cell><cell>MLP</cell><cell>Heads</cell><cell>Mio.</cell><cell>Param</cell><cell cols="2">GFLOPs 224 2 384 2</cell></row><row><cell>s/28</cell><cell cols="4">256 6 1024 8</cell><cell cols="2">5.4</cell><cell>0.7</cell><cell>2.0</cell></row><row><cell>s/16</cell><cell cols="4">256 6 1024 8</cell><cell cols="2">5.0</cell><cell>2.2</cell><cell>7.8</cell></row><row><cell>S/32</cell><cell cols="4">384 12 1536 6</cell><cell cols="2">22</cell><cell>2.3</cell><cell>6.9</cell></row><row><cell cols="4">Ti/16 192 12 768</cell><cell>3</cell><cell cols="2">5.5</cell><cell>2.5</cell><cell>9.5</cell></row><row><cell cols="5">B/32 768 12 3072 12</cell><cell cols="2">87</cell><cell>8.7</cell><cell>26.0</cell></row><row><cell>S/16</cell><cell cols="4">384 12 1536 6</cell><cell cols="2">22</cell><cell>9.2</cell><cell>31.2</cell></row><row><cell cols="5">B/28 768 12 3072 12</cell><cell cols="3">87 11.3</cell><cell>30.5</cell></row><row><cell cols="5">B/16 768 12 3072 12</cell><cell cols="4">86 35.1 111.3</cell></row><row><cell cols="5">L/16 1024 24 4096 16</cell><cell cols="4">303 122.9 382.8</cell></row><row><cell cols="9">g/14 1408 40 6144 16 1011 533.1 1596.4</cell></row><row><cell cols="9">G/14 1664 48 8192 16 1843 965.3 2859.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 3 .</head><label>3</label><figDesc>Results of different resolutions and patch sizes.</figDesc><table><row><cell>Model</cell><cell cols="3">B/32 B/48 B/64</cell><cell>S/16</cell><cell>S/24</cell><cell>S/32</cell></row><row><cell>Res.</cell><cell>224</cell><cell>336</cell><cell>448</cell><cell>224</cell><cell>336</cell><cell>448</cell></row><row><cell cols="7">INet10 64.43 64.65 64.67 63.42 63.79 63.50</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 4 .Table 5 .Table 6 .Table 7 .Table 8 .Table 9 .Table 10 .Table 12 .Table 14 .Table 15 .Table 16 .Table 17 .Table 18 .Table 19 .Table 20 .Table 21 .Table 22 .</head><label>4567891012141516171819202122</label><figDesc>Tabular representation of the few-shot results (%) for model ViT-g/14 and ViT-G/14. Tabular representation of the few-shot results (%) for model L/16. Tabular representation of the few-shot results (%) for model B/16. Tabular representation of the few-shot results (%) for model B/28. Tabular representation of the few-shot results (%) for model B/32. Tabular representation of the few-shot results (%) for model S/16. Tabular representation of the few-shot results (%) for model Ti/16. Tabular representation of the few-shot results (%) for model S/32. Tabular representation of the finetune results (%) for model ViT-L/16 on ImageNet, ImageNet V2 test set and ImageNet ReaL test set. Tabular representation of the finetune results (%) for model ViT-B/16 on ImageNet, ImageNet V2 test set and ImageNet ReaL test set. Tabular representation of the finetune results (%) for model ViT-B/28 on ImageNet, ImageNet V2 test set and ImageNet ReaL test set. Tabular representation of the finetune results (%) for model ViT-B/32 on ImageNet, ImageNet V2 test set and ImageNet ReaL test set. Tabular representation of the finetune results (%) for model ViT-S/16 on ImageNet, ImageNet V2 test set and ImageNet ReaL test set. Tabular representation of the finetune results (%) for model ViT-Ti/16 on ImageNet, ImageNet V2 test set and ImageNet ReaL test set. Tabular representation of the finetune results (%) for model ViT-s/16 on ImageNet, ImageNet V2 test set and ImageNet ReaL test set. Tabular representation of the finetune results (%) for model ViT-S/32 on ImageNet, ImageNet V2 test set and ImageNet ReaL test set. Tabular representation of the finetune results (%) for model ViT-s/28 on ImageNet, ImageNet V2 test set and ImageNet ReaL test set.</figDesc><table><row><cell>Data Size Steps INet5 INet10 Cifar5 Cifar10 Pets5 Pets10 Birds5 Birds10 ViT-g/14 3B 120K 74.0 76.4 75.3 79.2 93.4 94.7 79.6 84.0 3B 400K 79.1 81.3 79.1 82.9 96.1 96.8 84.2 87.7 3B 1.2M 81.3 83.3 82.8 85.3 97.1 97.5 85.6 88.9 3B 2M 82.0 83.9 82.7 86.1 97.2 97.3 86.6 89.1 3B 4M 82.4 84.3 83.0 86.5 97.0 97.6 87.0 89.2 3B 6.3M 82.7 84.5 84.6 86.5 97.3 97.7 86.7 89.8 ViT-G/14 3B 5M 83.0 84.9 84.7 87.5 97.0 97.5 87.6 88.8 Data Size Steps INet5 INet10 Cifar5 Cifar10 Pets5 Pets10 Birds5 Birds10 30M 20K 45.5 49.6 52.3 58.0 74.4 81.1 55.1 63.0 30M 30K 53.3 57.4 58.8 64.5 82.8 87.0 61.1 68.3 30M 60K 62.6 66.0 66.2 71.3 90.3 91.6 68.6 74.6 30M 120K 66.5 69.3 67.2 72.8 91.2 92.7 70.8 77.1 30M 400K 69.5 72.2 70.2 74.8 92.8 93.9 72.5 78.6 30M 1.2M 67.1 70.6 69.2 73.6 91.3 92.6 69.8 75.2 30M 2M 65.1 68.7 68.7 73.9 88.2 91.1 67.7 73.7 30M 4M 63.1 67.2 66.6 71.7 88.6 90.6 66.0 72.6 300M 20K 45.0 49.7 52.1 57.3 74.4 79.1 55.5 63.2 300M 30K 54.0 57.4 60.5 65.5 83.1 87.5 61.3 68.3 300M 60K 63.0 66.5 68.0 72.8 90.2 92.2 68.7 74.7 300M 120K 68.5 71.4 70.9 75.4 92.1 93.7 74.1 79.5 300M 400K 74.6 77.0 71.5 77.1 94.2 95.1 78.8 83.3 300M 1.2M 76.0 78.0 74.8 79.0 95.4 95.2 79.8 83.5 300M 2M 77.5 79.5 77.0 81.5 95.8 96.3 82.5 84.8 300M 4M 77.0 78.7 77.4 81.1 95.1 95.9 80.0 83.5 1B 20K 45.9 50.6 52.6 58.9 75.8 80.2 55.8 63.4 1B 30K 54.7 58.4 60.8 66.0 84.5 88.0 62.4 69.5 1B 60K 63.4 66.9 68.2 72.1 91.0 92.3 69.3 75.0 1B 120K 68.5 71.3 70.7 75.7 92.5 94.2 73.5 78.4 1B 400K 74.6 76.9 74.7 77.4 94.1 94.9 78.5 82.4 1B 1.2M 77.1 79.2 76.3 79.8 94.6 95.4 80.7 84.5 1B 2M 78.5 80.0 77.6 80.8 95.7 96.2 82.9 85.8 1B 4M 79.1 81.0 77.5 82.3 96.5 96.9 82.4 85.0 3B 20K 45.7 49.7 51.9 58.1 74.6 79.2 54.9 62.6 3B 30K 54.1 57.7 59.7 64.9 84.0 87.6 62.4 69.4 3B 60K 63.6 66.9 67.2 71.5 89.8 92.1 69.9 75.5 3B 120K 68.9 71.5 70.8 76.0 92.0 93.8 74.8 79.2 3B 400K 74.5 76.8 74.8 78.9 94.4 95.1 79.0 82.6 3B 1.2M 78.0 79.7 77.3 80.8 95.3 96.3 82.8 86.1 3B 2M 78.6 80.5 79.4 82.4 95.7 96.4 83.6 86.0 3B 4M 79.8 81.5 78.9 82.2 96.3 97.0 84.7 87.1 Data Size Steps INet5 INet10 Cifar5 Cifar10 Pets5 Pets10 Birds5 Birds10 30M 20K 40.8 45.4 47.2 53.5 68.6 74.8 49.3 57.8 30M 30K 48.0 52.2 53.1 58.7 79.1 81.4 56.7 64.5 30M 60K 56.6 60.5 60.1 64.8 87.0 88.6 64.1 71.3 30M 120K 61.6 65.4 63.4 68.4 89.1 90.8 69.0 75.1 30M 400K 67.1 70.4 66.0 70.8 91.5 93.3 72.8 78.2 30M 1.2M 68.8 71.4 65.2 70.5 92.0 93.8 73.8 79.4 30M 2M 68.2 71.1 65.7 69.7 92.5 93.8 73.5 78.4 30M 4M 67.2 70.5 64.4 70.0 92.7 93.6 71.8 77.7 300M 20K 41.0 45.6 48.4 54.6 69.7 76.1 49.9 57.6 300M 30K 48.6 52.6 53.5 58.7 78.5 83.1 56.5 64.2 300M 60K 57.3 60.6 60.0 65.9 86.8 89.8 63.7 70.8 300M 120K 62.3 65.5 63.8 69.4 89.3 91.1 68.9 75.3 300M 400K 69.3 72.1 68.8 73.1 92.3 94.1 75.5 80.5 300M 1.2M 72.3 74.9 68.4 72.2 93.1 94.4 77.7 81.1 300M 2M 73.3 76.0 71.2 74.7 94.5 95.5 78.4 82.7 300M 4M 73.6 76.3 70.8 74.9 94.4 95.4 79.2 82.7 1B 20K 40.9 45.0 47.0 53.0 69.3 75.8 49.4 58.1 1B 30K 48.5 52.6 53.2 59.1 78.3 83.4 56.6 63.6 1B 60K 57.3 61.0 60.0 65.2 87.9 89.6 65.0 71.0 1B 120K 62.3 65.9 63.4 68.8 89.5 90.6 69.8 75.2 1B 400K 69.2 72.2 67.9 71.1 93.0 93.6 74.6 80.5 1B 1.2M 71.9 74.4 71.5 75.7 94.3 94.8 77.1 81.6 1B 2M 73.8 75.9 72.2 76.2 94.8 95.2 79.0 82.8 1B 4M 74.3 76.7 71.1 75.2 93.8 95.3 79.3 83.4 3B 20K 41.3 45.7 46.4 53.5 68.3 75.4 50.0 58.9 3B 30K 49.2 53.1 54.0 59.5 78.2 83.6 57.9 65.8 3B 60K 57.4 61.0 61.0 65.3 87.0 89.6 65.0 71.7 3B 120K 62.5 66.0 63.8 68.6 90.0 92.0 68.9 75.8 3B 400K 69.8 72.4 68.2 72.9 93.0 93.8 75.3 81.0 3B 1.2M 72.3 74.9 71.3 75.3 94.3 94.7 78.9 83.2 3B 2M 73.8 76.3 72.9 74.6 94.7 95.3 79.0 82.9 3B 4M 74.3 76.8 71.9 76.8 95.1 95.9 79.4 82.8 Data Size Steps INet5 INet10 Cifar5 Cifar10 Pets5 Pets10 Birds5 Birds10 30M 20K 33.9 37.8 46.9 52.3 60.5 66.2 38.7 46.7 30M 30K 40.1 44.3 51.3 57.1 70.7 74.2 45.1 51.9 30M 60K 48.6 52.9 56.8 61.9 80.0 84.4 52.3 60.2 30M 120K 54.2 58.3 59.6 66.0 84.1 87.2 56.7 64.6 30M 400K 61.4 64.8 62.6 68.4 90.2 92.1 63.6 70.3 30M 1.2M 64.2 67.4 63.9 69.3 91.0 92.2 66.4 73.9 30M 2M 64.4 67.5 64.0 69.0 91.6 92.2 68.4 73.7 300M 20K 33.6 37.5 44.7 51.1 58.3 67.2 38.5 46.1 300M 30K 40.0 44.6 51.7 57.0 70.4 75.1 44.2 52.0 300M 60K 48.2 52.8 55.9 61.3 80.2 83.8 52.0 59.7 300M 120K 54.4 58.3 60.9 65.9 84.2 88.6 57.6 65.5 300M 400K 63.1 66.1 65.8 70.8 90.2 91.4 64.5 71.9 300M 1.2M 66.5 69.6 68.2 72.1 92.3 92.9 68.4 74.7 300M 2M 67.9 70.9 68.1 72.7 92.7 92.8 70.0 76.2 1B 20K 33.6 37.9 45.6 51.8 58.9 64.5 38.7 46.0 1B 30K 39.8 44.6 50.9 56.6 70.5 75.5 45.0 51.4 1B 60K 48.3 53.1 56.6 62.4 79.6 84.0 52.6 60.1 1B 120K 54.8 58.5 61.2 66.9 84.9 88.1 57.9 65.0 1B 400K 63.1 66.6 65.3 70.2 89.9 91.5 65.5 72.4 1B 1.2M 67.1 69.8 66.3 70.8 92.0 92.9 69.2 75.6 3B 20K 33.3 37.6 45.6 51.9 58.3 65.1 38.5 46.3 3B 30K 40.0 44.2 50.5 56.4 69.0 75.1 45.4 52.1 Data Size Steps INet5 INet10 Cifar5 Cifar10 Pets5 Pets10 Birds5 Birds10 30M 20K 30.6 34.8 44.2 50.2 54.7 62.6 34.7 42.0 30M 30K 37.5 41.4 49.5 55.0 66.9 72.0 41.9 49.1 30M 60K 46.1 50.0 55.9 60.7 77.0 81.0 48.8 57.1 30M 120K 51.7 55.8 59.7 64.2 82.2 85.2 54.1 62.1 30M 400K 59.5 63.4 63.3 68.3 88.5 90.6 62.3 68.3 30M 1.2M 63.2 66.7 64.1 68.8 90.8 91.8 64.3 70.7 30M 2M 63.8 66.7 63.6 68.8 90.6 91.3 64.7 70.9 30M 4M 63.2 67.3 62.4 68.5 89.9 91.6 64.0 70.6 300M 20K 30.5 35.2 45.1 50.0 54.7 60.6 35.8 42.7 300M 30K 37.3 41.0 50.2 55.6 64.4 70.2 40.8 48.0 300M 60K 45.7 49.9 56.6 62.2 75.6 80.8 48.8 56.1 300M 120K 51.9 55.8 61.2 66.0 82.3 86.2 53.5 61.2 300M 400K 60.1 64.1 65.4 70.6 88.9 90.7 61.5 68.1 300M 1.2M 64.0 67.7 66.3 71.4 90.9 91.9 65.1 71.2 300M 2M 66.4 69.3 67.7 73.1 91.6 92.1 67.5 73.7 300M 4M 67.5 70.1 68.4 73.1 91.7 92.2 68.3 74.1 1B 20K 30.6 35.2 43.9 50.4 57.1 61.9 35.1 41.9 1B 30K 37.1 41.9 49.5 55.4 65.3 71.8 41.6 48.5 1B 60K 46.3 50.2 56.0 61.3 76.2 80.3 48.3 56.7 1B 120K 51.9 55.9 60.8 65.2 81.5 85.7 54.6 62.3 1B 400K 60.8 64.5 65.7 70.3 88.6 90.7 62.5 69.1 1B 1.2M 65.1 68.1 66.6 72.3 90.7 91.7 66.6 72.9 1B 2M 66.1 69.4 68.1 71.8 91.4 92.7 66.9 74.1 1B 4M 67.5 70.7 67.4 73.3 91.7 93.1 68.2 74.2 3B 20K 31.5 35.2 44.9 51.0 53.9 62.7 35.1 43.6 3B 30K 37.6 41.8 50.1 55.9 63.7 70.0 42.0 48.8 3B 60K 46.2 50.1 56.7 62.3 77.0 80.7 49.6 57.1 3B 120K 52.0 56.4 60.5 66.6 81.7 85.6 55.2 62.5 3B 400K 61.6 64.3 65.7 70.3 88.0 90.9 62.5 69.8 3B 1.2M 65.3 68.7 67.7 72.6 90.6 92.0 67.3 73.0 3B 2M 66.2 69.1 68.7 73.5 91.9 92.7 68.3 74.0 3B 4M 67.6 70.6 70.0 72.9 92.7 93.4 68.1 75.1 Data Size Steps INet5 INet10 Cifar5 Cifar10 Pets5 Pets10 Birds5 Birds10 Data Size Steps INet5 INet10 Cifar5 Cifar10 Pets5 Pets10 Birds5 Birds10 Data Size Steps Cooldown LR ImageNet ImageNet V2 ImageNet ReaL Data Size Steps Cooldown LR ImageNet ImageNet V2 ImageNet ReaL Data Size Steps Cooldown LR ImageNet ImageNet V2 ImageNet ReaL Data Size Steps Cooldown LR ImageNet ImageNet V2 ImageNet ReaL Data Size Steps Cooldown LR ImageNet ImageNet V2 ImageNet ReaL Data Size Steps Cooldown LR ImageNet ImageNet V2 ImageNet ReaL Data Size Steps INet5 INet10 Cifar5 Cifar10 Pets5 Pets10 Birds5 Birds10 30M 20K 32.1 36.4 39.2 44.5 56.1 62.3 38.4 45.8 30M 30K 39.1 43.3 46.3 51.4 67.2 74.1 46.8 53.7 30M 60K 47.1 51.2 50.7 55.7 78.4 83.1 54.0 61.6 30M 120K 51.8 55.4 53.8 59.1 82.2 86.1 58.8 66.3 30M 400K 58.4 61.8 57.2 62.6 88.3 90.6 65.0 71.6 30M 1.2M 61.1 64.4 57.1 63.2 90.5 91.3 68.4 73.7 30M 2M 61.9 65.6 58.8 64.1 91.0 92.0 68.0 74.0 30M 4M 63.2 66.5 58.7 65.2 91.3 92.1 69.7 74.7 300M 20K 31.3 36.4 38.9 43.9 57.4 62.2 37.4 45.4 300M 60K 47.4 51.1 50.8 56.6 78.3 83.8 53.4 62.0 300M 120K 52.6 56.4 53.0 58.1 83.1 87.0 58.7 66.2 300M 400K 58.9 62.7 56.1 61.5 88.6 90.2 65.6 72.2 300M 1.2M 62.3 66.0 58.2 63.9 90.9 91.8 69.5 74.9 300M 2M 63.3 66.4 60.1 64.9 91.9 93.0 69.9 75.5 300M 4M 64.5 67.5 61.1 65.9 92.1 93.3 70.3 75.4 1B 20K 31.8 35.8 37.1 42.9 56.1 63.3 38.2 46.0 1B 30K 39.2 43.1 44.2 50.3 68.1 75.1 44.6 52.6 1B 60K 47.2 51.3 50.6 55.8 78.2 84.5 53.9 60.8 1B 120K 51.5 55.8 53.6 59.4 83.3 86.8 59.0 66.8 1B 400K 58.9 62.6 56.7 62.3 88.0 90.6 65.7 72.2 1B 1.2M 61.5 65.2 59.6 64.4 90.7 92.1 67.3 75.0 1B 2M 62.8 66.6 60.8 66.0 90.7 92.0 69.1 75.5 1B 4M 64.0 67.4 61.4 66.2 91.2 92.1 69.5 74.9 3B 20K 32.3 36.5 38.4 43.8 56.2 59.7 37.7 45.4 3B 30K 38.8 43.1 43.8 50.7 68.8 75.1 45.9 53.9 3B 120K 52.6 56.3 53.5 58.8 83.8 87.6 58.5 66.0 3B 400K 59.1 62.7 56.9 62.2 88.7 90.8 65.8 72.1 3B 1.2M 62.1 65.7 58.7 63.6 91.0 92.2 68.6 74.9 3B 2M 63.7 66.5 59.5 65.3 91.4 92.5 68.8 75.7 3B 4M 64.1 67.6 60.3 64.6 91.6 93.0 69.7 75.9 30M 20K 20.2 23.5 26.9 32.0 37.8 44.6 24.2 29.2 30M 30K 25.7 28.6 31.9 38.0 50.1 54.9 29.7 30M 20K 23.0 26.7 34.1 40.3 43.3 48.1 26.1 32.2 30M 20K 10K 0.03 75.4 63.3 82.1 30M 20K 10K 0.03 73.0 60.4 80.0 30M 20K 10K 0.03 66.6 53.8 73.8 30M 20K 10K 0.03 67.4 54.5 74.7 30M 20K 10K 0.03 55.5 43.6 62.5 30M 20K 10K 0.03 59.3 47.1 66.3 35.5 30M 60K 32.9 36.0 36.6 41.7 61.9 65.7 36.9 Table 11. Tabular representation of the few-shot results (%) for model s/16. 30M 30K 28.8 32.6 40.0 46.3 51.9 59.6 31.2 37.1 Table 13. Tabular representation of the few-shot results (%) for model s/28. 30M 30K 10K 0.03 78.8 67.5 85.0 30M 30K 10K 0.03 76.9 64.9 83.4 30M 30K 10K 0.03 71.0 57.9 78.0 30M 30K 10K 0.03 72.5 59.9 79.6 30M 30K 10K 0.03 61.8 49.2 69.2 30M 30K 10K 0.03 64.3 51.8 71.7 43.1 30M 120K 36.5 40.6 39.1 44.3 67.2 74.2 41.1 30M 60K 36.4 39.9 45.9 51.8 64.5 71.5 39.0 45.2 30M 60K 10K 0.03 82.4 72.5 87.6 30M 60K 10K 0.03 80.5 69.5 86.1 30M 60K 10K 0.03 75.6 63.5 82.3 30M 60K 10K 0.03 76.8 65.0 83.2 30M 60K 10K 0.03 67.8 55.2 75.5 30M 60K 10K 0.03 70.3 58.1 77.5 48.0 30M 400K 42.3 46.7 41.7 48.2 76.1 82.0 47.4 55.2 30M 1.2M 45.6 49.8 45.4 50.6 80.8 84.9 52.3 58.9 30M 2M 47.4 50.5 45.1 51.0 81.0 84.1 53.6 59.4 30M 4M 48.2 51.6 46.3 52.7 82.2 85.6 53.3 59.5 300M 20K 20.7 23.7 28.0 32.6 43.3 45.3 23.8 30.0 300M 30K 25.8 28.8 32.2 36.8 49.7 55.0 29.6 35.6 300M 60K 33.1 36.4 37.6 42.7 62.2 68.2 37.3 44.0 300M 120K 37.3 41.1 39.9 45.4 67.0 75.1 42.6 48.2 300M 400K 43.2 47.3 42.9 49.3 74.4 81.7 47.7 55.6 300M 1.2M 46.4 50.8 45.6 51.7 81.7 85.4 51.3 59.1 300M 2M 48.0 51.6 46.1 51.8 82.3 85.7 53.3 60.2 300M 4M 49.0 51.9 46.6 52.6 83.1 86.3 54.3 61.4 1B 20K 20.4 23.5 27.7 32.8 40.5 45.3 24.0 29.8 1B 30K 26.0 28.6 31.7 37.5 54.3 54.9 29.5 35.4 1B 60K 32.7 36.0 36.1 42.1 59.5 66.7 36.1 42.8 1B 120K 36.4 40.2 39.2 45.0 68.2 73.1 41.3 47.8 1B 400K 42.9 47.2 43.9 49.3 77.8 80.8 47.9 54.6 1B 1.2M 46.4 49.9 44.9 50.2 81.9 85.1 52.1 59.1 1B 2M 47.5 51.8 46.3 51.9 83.7 86.4 53.9 60.2 1B 4M 48.3 52.2 47.8 53.4 83.5 85.3 54.3 60.2 3B 20K 20.6 23.6 26.9 32.2 38.2 43.6 24.0 29.0 3B 30K 25.6 28.5 31.7 36.9 50.6 53.4 29.2 35.3 3B 60K 32.9 35.7 37.3 43.1 63.1 66.7 36.2 42.5 3B 120K 37.1 41.0 40.0 45.9 68.8 74.6 40.5 3B 400K 42.9 46.7 42.7 48.3 78.0 80.3 48.7 3B 1.2M 46.0 50.1 43.0 49.8 78.9 84.0 50.9 3B 2M 47.1 50.8 46.3 51.6 82.5 85.7 52.3 3B 4M 47.6 52.1 45.9 51.3 83.2 86.6 53.4 3B 4M 50K 0.01 88.5 80.4 90.4 3B 4M 50K 0.03 86.6 77.4 89.7 3B 4M 50K 0.03 84.1 74.4 88.2 3B 4M 50K 0.01 84.0 73.8 88.5 3B 4M 50K 0.01 77.6 65.6 84.3 3B 4M 50K 0.01 79.9 69.4 86.0 60.2 3B 4M 55.6 59.1 56.8 62.2 85.2 86.9 56.8 63.2 3B 2M 50K 0.03 87.9 79.6 90.0 3B 2M 50K 0.03 86.3 77.0 89.6 3B 2M 50K 0.03 83.8 74.0 88.2 3B 2M 50K 0.03 83.2 73.2 88.1 3B 2M 50K 0.03 76.9 64.7 83.7 3B 2M 50K 0.01 79.5 68.5 85.6 60.0 3B 2M 53.6 57.5 56.6 62.0 84.7 86.2 56.4 63.5 3B 1.2M 50K 0.03 87.8 79.4 90.0 3B 1.2M 50K 0.03 86.0 77.1 89.4 3B 1.2M 50K 0.03 83.5 73.5 87.9 3B 1.2M 50K 0.01 82.8 72.7 87.9 3B 1.2M 50K 0.03 76.4 64.7 83.4 3B 1.2M 50K 0.03 79.4 67.4 85.4 58.2 3B 1.2M 52.9 56.6 55.9 61.6 83.3 86.1 55.7 62.9 3B 400K 50K 0.03 87.0 78.5 90.1 3B 400K 50K 0.03 85.1 75.7 89.1 3B 400K 50K 0.03 82.2 71.8 87.1 3B 400K 50K 0.03 81.9 71.1 87.3 3B 400K 50K 0.03 75.0 62.8 82.1 3B 400K 50K 0.03 77.6 65.7 84.0 55.4 3B 400K 48.8 52.8 53.5 59.6 79.1 83.2 50.8 58.2 3B 120K 50K 0.03 84.7 75.6 89.0 3B 120K 50K 0.03 82.5 72.7 87.6 3B 120K 50K 0.03 78.3 67.3 84.6 3B 120K 50K 0.03 79.1 67.9 85.4 3B 120K 50K 0.03 71.2 58.6 78.7 3B 120K 50K 0.03 73.5 61.3 80.7 47.0 Data Size Steps INet5 INet10 Cifar5 Cifar10 Pets5 Pets10 Birds5 Birds10 30M 20K 20.3 23.8 27.2 32.7 36.8 47.1 24.2 30.0 30M 30K 26.3 29.6 32.3 37.9 47.6 54.4 30.2 37.4 30M 60K 32.7 36.0 36.9 43.6 58.9 65.3 36.6 44.0 30M 120K 36.2 39.7 39.0 45.7 65.0 69.9 42.2 48.7 30M 400K 40.9 44.9 41.9 48.1 74.2 79.0 48.7 55.3 30M 1.2M 43.2 47.6 43.7 49.5 75.9 78.7 51.3 57.3 30M 2M 43.7 48.2 43.8 49.2 76.3 81.6 51.2 59.0 300M 20K 20.4 23.7 28.1 32.7 42.3 43.9 24.6 29.7 300M 30K 25.5 29.3 33.5 39.2 49.6 55.0 30.2 36.1 300M 60K 31.4 35.3 37.5 43.4 59.3 66.7 37.0 43.8 300M 120K 35.8 39.4 38.4 44.6 65.3 71.2 41.6 48.9 300M 400K 40.7 44.7 42.5 48.8 72.6 79.2 48.6 55.6 300M 1.2M 43.5 47.3 43.8 50.1 77.2 80.0 51.8 57.8 300M 2M 44.3 48.1 44.4 50.2 77.3 80.6 51.2 57.8 1B 20K 20.6 24.0 27.8 32.8 38.5 44.1 23.7 30.1 1B 30K 26.1 29.9 31.7 37.7 49.3 55.9 30.3 37.3 1B 60K 32.1 36.3 36.8 42.6 60.3 66.1 37.0 43.6 1B 120K 35.5 40.1 40.0 46.1 66.0 72.2 41.6 48.9 1B 400K 41.0 45.2 43.0 49.4 73.3 79.1 48.8 55.0 1B 1.2M 42.8 47.2 45.0 51.8 76.3 81.5 50.7 57.2 3B 20K 20.7 24.5 28.4 33.8 39.7 44.9 24.8 29.8 3B 30K 26.2 30.0 33.5 39.6 51.2 56.2 29.5 36.4 30M 120K 40.3 44.4 49.8 55.7 71.0 76.9 43.7 50.7 30M 400K 47.9 51.7 54.8 60.1 79.5 83.1 50.5 57.6 30M 1.2M 51.6 55.9 55.2 60.7 84.0 86.6 54.3 60.6 30M 2M 52.5 56.6 56.0 60.1 84.9 88.1 55.7 62.5 30M 4M 54.5 57.6 55.3 61.3 86.0 87.9 56.7 63.2 300M 20K 23.1 27.0 36.1 41.5 42.6 47.0 25.8 32.0 300M 30K 28.4 31.9 42.2 47.1 48.9 58.9 30.3 36.6 300M 60K 35.0 39.3 47.7 52.5 62.9 69.5 36.9 44.8 300M 120K 40.8 44.9 50.4 55.4 71.4 75.6 43.4 50.3 300M 400K 48.4 52.4 54.5 59.9 79.3 83.9 50.6 57.5 300M 1.2M 52.9 56.2 57.3 62.6 83.0 85.5 54.6 61.8 300M 2M 53.4 57.4 57.1 62.9 84.5 87.7 55.2 62.4 300M 4M 55.2 58.5 57.6 62.8 85.4 87.1 55.5 62.6 1B 30K 28.3 32.2 41.4 47.1 50.2 56.2 29.9 36.6 1B 60K 35.7 39.7 47.1 53.1 63.4 70.0 36.9 44.7 1B 120K 40.8 44.7 50.7 56.0 68.6 75.3 43.2 50.1 1B 400K 48.3 52.4 54.0 59.6 80.2 83.4 50.5 57.7 1B 1.2M 52.6 56.7 55.8 61.1 83.2 86.4 55.7 62.4 1B 2M 54.3 58.0 56.7 61.2 84.9 86.6 56.3 63.7 1B 4M 55.4 58.8 56.3 61.6 86.4 88.6 56.5 64.0 3B 20K 22.5 26.3 36.8 41.7 43.4 46.8 25.1 31.5 3B 30K 28.2 32.0 40.4 46.2 49.6 56.9 31.2 37.2 3B 60K 36.0 39.6 46.5 52.0 63.1 71.2 37.5 44.8 3B 120K 40.5 44.5 50.6 56.0 70.0 75.9 42.1 49.1 Data Size Steps INet5 INet10 Cifar5 Cifar10 Pets5 Pets10 Birds5 Birds10 30M 20K 16.0 18.9 24.9 31.9 37.0 36.5 18.6 30M 120K 50K 0.03 83.8 74.8 88.3 30M 400K 50K 0.03 85.5 76.5 89.0 30M 120K 50K 0.03 82.2 72.3 87.4 30M 400K 50K 0.03 84.4 74.6 88.5 Data Size Steps Cooldown LR ImageNet ImageNet V2 ImageNet ReaL 30M 120K 50K 0.03 78.0 66.4 84.3 30M 400K 50K 0.03 81.4 70.8 86.8 30M 120K 50K 0.03 78.8 67.8 85.1 30M 400K 50K 0.03 81.5 70.9 87.1 30M 120K 50K 0.03 71.2 58.6 78.5 30M 400K 50K 0.03 74.9 62.8 82.1 Data Size Steps Cooldown LR ImageNet ImageNet V2 ImageNet ReaL 30M 120K 50K 0.03 73.4 61.2 80.5 30M 400K 50K 0.03 77.1 65.7 Data Size Steps Cooldown LR ImageNet ImageNet V2 ImageNet ReaL 83.6 24.6 30M 30K 20.3 23.4 30.5 35.9 40.4 46.8 23.0 30M 1.2M 50K 0.03 85.3 76.0 88.7 30M 1.2M 50K 0.03 84.9 75.0 88.7 30M 20K 10K 0.03 68.8 55.6 76.1 30M 1.2M 50K 0.03 82.7 72.4 87.5 30M 1.2M 50K 0.03 82.5 72.0 87.7 30M 1.2M 50K 0.01 76.5 64.5 83.4 30M 20K 10K 0.03 56.0 43.2 63.0 30M 1.2M 50K 0.03 79.0 67.3 84.9 30M 20K 10K 0.03 50.3 38.0 56.9 28.2 30M 60K 24.6 28.4 34.7 41.1 48.7 54.4 28.2 30M 2M 50K 0.03 85.1 76.2 88.7 30M 2M 50K 0.03 84.8 74.8 88.6 30M 30K 10K 0.03 72.8 59.6 79.8 30M 2M 50K 0.03 83.1 72.7 87.7 30M 2M 50K 0.03 82.8 72.2 87.8 30M 2M 50K 0.03 76.7 64.7 83.4 30M 30K 10K 0.03 62.2 49.4 69.5 30M 2M 50K 0.03 79.1 67.9 85.1 30M 30K 10K 0.03 55.8 43.4 62.8 34.3 30M 120K 27.7 32.0 37.4 43.3 51.6 58.5 29.7 30M 4M 50K 0.01 85.6 77.0 89.1 30M 4M 50K 0.01 84.9 75.3 88.8 30M 60K 10K 0.03 76.7 64.5 83.4 30M 4M 50K 0.01 83.0 72.8 87.7 30M 4M 50K 0.01 83.5 72.8 88.2 30M 4M 50K 0.01 77.5 65.6 84.2 30M 60K 10K 0.03 67.8 54.8 75.3 30M 4M 50K 0.01 79.7 68.2 85.6 30M 60K 10K 0.03 61.5 48.5 68.8 37.2 30M 400K 32.0 36.3 39.1 45.2 62.0 68.1 35.7 42.9 30M 1.2M 34.8 38.6 40.8 46.3 66.5 70.1 40.1 46.1 30M 2M 35.9 39.3 41.9 47.0 64.7 71.3 39.7 47.4 300M 20K 16.5 19.1 26.8 31.9 32.9 35.8 19.7 300M 20K 10K 0.03 75.1 63.5 81.9 300M 30K 10K 0.03 79.1 67.7 85.2 300M 60K 10K 0.03 82.7 72.9 87.9 300M 120K 50K 0.03 84.7 75.4 89.1 300M 20K 10K 0.03 73.5 61.0 80.5 300M 30K 10K 0.03 77.2 65.2 83.8 300M 60K 10K 0.03 80.6 69.9 86.3 300M 120K 50K 0.03 82.3 72.5 87.5 30M 120K 50K 0.03 79.1 68.3 85.3 30M 400K 50K 0.03 82.2 72.1 87.4 30M 1.2M 50K 0.03 83.3 73.1 87.8 30M 2M 50K 0.03 83.5 73.4 87.8 300M 20K 10K 0.03 66.6 53.4 73.9 300M 30K 10K 0.03 70.8 58.0 78.0 300M 60K 10K 0.03 75.5 63.2 82.2 300M 120K 50K 0.03 78.3 66.7 84.5 300M 20K 10K 0.03 67.8 54.8 75.0 300M 30K 10K 0.03 72.6 60.3 79.7 300M 60K 10K 0.03 76.8 65.3 83.4 300M 120K 50K 0.03 79.0 68.0 85.3 300M 20K 10K 0.03 55.9 43.7 62.9 300M 30K 10K 0.03 61.7 49.3 69.0 300M 60K 10K 0.03 68.5 55.7 76.0 300M 120K 50K 0.03 71.4 58.8 78.7 30M 120K 50K 0.03 70.0 57.5 77.7 30M 400K 50K 0.03 73.7 60.9 81.0 30M 1.2M 50K 0.03 75.0 62.4 82.0 30M 2M 50K 0.01 75.2 63.0 82.3 300M 20K 10K 0.03 59.3 47.1 30M 120K 50K 0.03 64.1 51.4 71.6 66.2 300M 30K 10K 0.03 64.2 51.0 30M 400K 50K 0.03 68.4 55.5 75.7 71.5 300M 60K 10K 0.03 70.1 57.6 30M 1.2M 50K 0.03 69.8 57.2 77.4 77.4 300M 120K 50K 0.03 73.4 60.5 30M 2M 50K 0.01 70.2 57.5 77.8 80.6 23.8 300M 30K 19.9 23.2 29.9 36.2 42.0 44.0 23.7 300M 400K 50K 0.03 86.5 77.5 89.8 300M 400K 50K 0.03 84.9 75.5 89.0 300M 20K 10K 0.03 68.9 56.0 76.2 300M 400K 50K 0.03 81.8 71.4 87.0 300M 400K 50K 0.03 81.7 71.2 87.3 300M 400K 50K 0.03 75.2 62.8 82.2 300M 20K 10K 0.03 56.3 43.2 63.3 300M 400K 50K 0.03 77.5 66.3 84.0 300M 20K 10K 0.03 50.3 38.2 56.9 29.1 300M 60K 24.8 28.4 34.9 41.3 50.3 56.0 28.5 300M 1.2M 50K 0.03 87.3 78.8 89.8 300M 1.2M 50K 0.03 86.0 76.7 89.4 300M 30K 10K 0.03 72.8 60.2 80.0 300M 1.2M 50K 0.03 83.3 73.4 87.9 300M 1.2M 50K 0.03 82.9 72.9 87.9 300M 1.2M 50K 0.03 76.7 64.7 83.7 300M 30K 10K 0.03 62.0 49.5 69.4 300M 1.2M 50K 0.03 79.0 67.9 85.1 300M 30K 10K 0.03 55.7 43.6 62.7 33.6 300M 120K 27.6 31.6 37.0 43.2 54.5 58.4 32.0 300M 2M 50K 0.03 87.7 78.6 89.8 300M 2M 50K 0.01 86.2 76.8 89.5 300M 60K 10K 0.03 77.0 65.0 83.5 300M 2M 50K 0.03 83.7 73.9 88.2 300M 2M 50K 0.01 83.3 73.4 88.3 300M 2M 50K 0.01 77.1 65.5 84.1 300M 60K 10K 0.03 67.4 54.3 75.0 300M 2M 50K 0.03 79.6 67.8 85.6 300M 60K 10K 0.03 61.1 48.8 68.5 37.5 300M 400K 32.9 36.6 39.4 45.4 63.9 65.5 37.3 300M 4M 50K 0.01 88.0 79.5 90.3 300M 4M 50K 0.01 86.7 77.6 89.7 300M 120K 50K 0.03 79.4 68.2 85.3 300M 4M 50K 0.01 83.9 74.3 88.3 300M 4M 50K 0.01 83.9 74.2 88.5 300M 4M 50K 0.01 77.8 66.1 84.4 300M 120K 50K 0.03 70.1 57.8 77.6 300M 4M 50K 0.03 79.9 68.5 85.8 300M 120K 50K 0.03 64.0 51.1 71.5 43.1 300M 1.2M 35.4 39.3 41.2 47.7 68.2 69.8 40.3 45.9 300M 2M 35.9 39.5 41.8 47.9 67.4 72.7 41.1 47.5 1B 20K 16.0 19.0 27.6 33.1 34.3 37.9 19.1 1B 20K 10K 0.03 75.9 63.9 82.7 1B 30K 10K 0.03 79.5 68.4 85.5 1B 60K 10K 0.03 82.5 72.6 87.8 1B 20K 10K 0.03 73.2 60.7 80.2 1B 30K 10K 0.03 77.0 65.7 83.6 1B 60K 10K 0.03 80.6 70.7 86.4 300M 400K 50K 0.03 82.8 72.6 87.7 300M 1.2M 50K 0.03 84.1 74.6 88.5 300M 2M 50K 0.03 84.4 74.6 88.5 1B 20K 10K 0.03 66.8 53.7 74.1 1B 30K 10K 0.03 71.1 58.5 78.1 1B 60K 10K 0.03 75.5 63.1 82.2 1B 20K 10K 0.03 67.3 54.5 74.6 1B 30K 10K 0.03 72.3 60.0 79.6 1B 60K 10K 0.03 76.6 64.9 83.4 1B 20K 10K 0.03 55.8 43.2 62.8 1B 30K 10K 0.03 61.6 49.1 69.0 1B 60K 10K 0.03 67.9 54.8 75.4 300M 400K 50K 0.03 73.6 61.2 80.6 300M 1.2M 50K 0.03 74.9 62.8 82.0 300M 2M 50K 0.01 75.4 63.4 82.6 1B 20K 10K 0.03 59.0 46.2 300M 400K 50K 0.03 68.6 55.5 76.0 66.2 1B 30K 10K 0.03 64.0 51.4 300M 1.2M 50K 0.03 70.1 57.1 77.6 71.4 1B 60K 10K 0.03 70.5 57.7 300M 2M 50K 0.03 70.5 57.1 77.9 77.7 24.1 1B 30K 20.2 23.3 30.1 35.9 41.3 45.8 23.2 1B 120K 50K 0.03 84.5 75.4 88.9 1B 120K 50K 0.03 82.3 72.0 87.5 1B 20K 10K 0.03 68.6 55.5 75.9 1B 120K 50K 0.03 78.5 66.9 84.7 1B 120K 50K 0.03 78.8 67.9 85.2 1B 120K 50K 0.03 71.1 58.3 78.5 1B 20K 10K 0.03 56.2 44.1 63.2 1B 120K 50K 0.03 73.6 60.8 80.7 1B 20K 10K 0.03 49.9 37.8 56.5 27.3 1B 60K 24.5 28.2 33.9 39.9 47.1 53.3 26.6 1B 400K 50K 0.03 86.7 78.3 89.8 1B 400K 50K 0.03 85.1 75.2 89.1 1B 30K 10K 0.03 72.8 60.1 79.9 1B 400K 50K 0.03 82.0 71.6 87.2 1B 400K 50K 0.03 81.9 70.6 87.3 1B 400K 50K 0.03 74.9 63.0 82.1 1B 30K 10K 0.03 62.4 49.7 69.8 1B 400K 50K 0.03 77.6 65.7 84.0 1B 30K 10K 0.03 55.2 42.8 62.3 32.8 1B 120K 27.6 31.8 36.5 43.3 53.6 60.3 30.2 1B 1.2M 50K 0.03 87.2 78.6 89.8 1B 1.2M 50K 0.03 86.0 77.0 89.5 1B 60K 10K 0.03 76.9 65.1 83.6 1B 1.2M 50K 0.03 83.4 73.5 87.9 1B 1.2M 50K 0.03 82.8 72.4 87.8 1B 1.2M 50K 0.03 76.7 64.6 83.6 1B 60K 10K 0.03 68.0 54.9 75.6 1B 1.2M 50K 0.03 79.5 68.0 85.5 1B 60K 10K 0.03 61.0 47.9 68.4 36.6 1B 400K 33.0 36.3 39.8 45.5 63.1 66.4 37.1 1B 2M 50K 0.03 87.9 78.9 90.0 1B 2M 50K 0.03 86.5 77.3 89.6 1B 120K 50K 0.03 79.4 69.0 85.5 1B 2M 50K 0.03 83.7 73.9 88.1 1B 2M 50K 0.01 83.2 72.8 88.2 1B 2M 50K 0.01 77.1 65.4 83.8 1B 120K 50K 0.03 70.5 57.5 77.8 1B 2M 50K 0.03 79.7 68.2 85.5 1B 120K 50K 0.03 64.0 51.1 71.5 43.0 1B 1.2M 35.2 38.9 40.8 46.3 65.4 71.2 40.2 1B 4M 50K 0.03 88.0 79.5 90.1 1B 4M 50K 0.01 86.8 77.5 89.8 1B 400K 50K 0.03 82.7 73.0 87.6 1B 4M 50K 0.03 84.1 74.4 88.4 1B 4M 50K 0.03 83.5 72.7 88.3 1B 4M 50K 0.01 77.7 66.2 84.4 1B 400K 50K 0.03 73.9 61.6 81.1 1B 4M 50K 0.03 80.2 68.1 85.9 1B 400K 50K 0.03 68.5 55.7 75.9 46.9 3B 20K 16.0 18.9 26.6 31.8 32.5 37.1 18.6 3B 20K 10K 0.03 75.5 63.0 82.2 3B 20K 10K 0.03 73.4 61.0 80.4 1B 1.2M 50K 0.03 84.0 74.4 88.3 3B 20K 10K 0.03 66.7 53.7 73.9 3B 20K 10K 0.03 67.5 54.5 74.9 3B 20K 10K 0.03 55.6 43.3 62.5 1B 1.2M 50K 0.03 75.1 63.2 82.1 3B 20K 10K 0.03 59.3 47.3 1B 1.2M 50K 0.03 70.0 57.3 77.3 66.4 23.2 3B 30K 20.4 23.3 30.4 36.0 43.0 46.2 21.7 3B 30K 10K 0.03 79.3 68.3 85.4 3B 30K 10K 0.03 77.1 65.5 83.7 3B 20K 10K 0.03 68.8 55.3 75.9 3B 30K 10K 0.03 71.0 58.4 78.1 3B 30K 10K 0.03 72.3 60.0 79.4 3B 30K 10K 0.03 61.4 49.2 68.7 3B 20K 10K 0.03 56.4 43.6 63.3 3B 30K 10K 0.03 64.3 51.5 71.6 3B 20K 10K 0.03 49.9 38.0 56.3 27.0 3B 60K 10K 0.03 82.7 73.5 87.7 3B 60K 10K 0.03 80.5 70.0 86.2 3B 30K 10K 0.03 72.6 60.2 79.7 3B 60K 10K 0.03 75.6 63.4 82.3 3B 60K 10K 0.03 76.7 64.8 83.3 3B 60K 10K 0.03 68.1 55.5 75.7 3B 30K 10K 0.03 62.6 49.9 70.1 3B 60K 10K 0.03 70.2 57.2 77.6 3B 30K 10K 0.03 55.4 43.5 62.4</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements We thank James Bradbury and Vivek Sharma for their help on using large-scale infrastructure; Alexey Dosovitskiy, Joan Puigcerver, Basil Mustafa, Carlos Riquelme for insightful discussions; Tom Duerig, Austin Tarango, Daniel Keysers, Howard Zhou, Wenlei Zhou, Yanan Bao for discussions on JFT; the Google Brain team at large for providing a supportive research environment.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Measuring model biases in the absence of ground truth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ken</forename><surname>Osman Aka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Burke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christina</forename><surname>B?uerle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margaret</forename><surname>Greer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mitchell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.03417</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Objectnet: A large-scale bias-controlled dataset for pushing the limits of object recognition models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><surname>Barbu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mayo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Alverio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Gutfreund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boris</forename><surname>Katz</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irwan</forename><surname>Bello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianzhi</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aravind</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zoph</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.07579</idno>
		<title level="m">Revisiting resnets: Improved training and scaling strategies</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Xiaohua Zhai, and A?ron van den Oord</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><forename type="middle">J</forename><surname>H?naff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.07159</idno>
	</analytic>
	<monogr>
		<title level="m">Are we done with imagenet</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Big vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<idno>2022. 11</idno>
		<ptr target="https://github.com/google-research/big_vision" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Language models are few-shot learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melanie</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jared</forename><forename type="middle">D</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ariel</forename><surname>Herbert-Voss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gretchen</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clemens</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Sigler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mateusz</forename><surname>Litwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Ilya Sutskever, and Dario Amodei</title>
		<meeting><address><addrLine>Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>NeurIPS</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Language models are few-shot learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Tom B Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melanie</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jared</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvind</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Askell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.14165</idno>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Endto-end object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.12872</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Emerging properties in self-supervised vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<idno>abs/2104.14294, 2021. 4</idno>
		<imprint>
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Big self-supervised models are strong semi-supervised learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.10029,2020.4</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">On the relationship between self-attention and convolutional layers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Baptiste</forename><surname>Cordonnier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Loukas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Jaggi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR, 2020</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Support-vector networks. Machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Corinna</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Vapnik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Coatnet: Marrying convolution and attention for all data sizes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<idno>abs/2106.04803</idno>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><forename type="middle">Toutanova</forename><surname>Bert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Bootstrap your own latent: A new approach to self-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Bastien</forename><surname>Grill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florent</forename><surname>Altch?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Corentin</forename><surname>Tallec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><forename type="middle">H</forename><surname>Richemond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elena</forename><surname>Buchatskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernardo</forename><surname>Avila Pires</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaohan</forename><forename type="middle">Daniel</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><forename type="middle">Gheshlaghi</forename><surname>Azar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bilal</forename><surname>Piot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R?mi</forename><surname>Munos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Valko</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.07733</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Scaling laws for autoregressive generative modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jared</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mor</forename><surname>Katz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Jackson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heewoo</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Tom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gray</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.14701</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Gpipe: Efficient training of giant neural networks using pipeline parallelism</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanping</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youlong</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankur</forename><surname>Bapna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orhan</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dehao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mia</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyoukjoong</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiquan</forename><surname>Ngiam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chen</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Scaling up visual and vision-language representation learning with noisy text supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinfei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zarana</forename><surname>Parekh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhsuan</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Duerig</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.05918</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Scaling laws for neural language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jared</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Mccandlish</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Tom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Chess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dario</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Amodei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.08361</idno>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Big Transfer (BiT): General Visual Representation Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Puigcerver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jessica</forename><surname>Yung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Houlsby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Set transformer: A framework for attention-based permutation-invariant neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juho</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoonho</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungtaek</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Kosiorek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungjin</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee Whye</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Gshard: Scaling giant models with conditional computation and automatic sharding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Lepikhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyoukjoong</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanzhong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dehao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orhan</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanping</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.16668</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Ashwin Bharambe, and Laurens van der Maaten. Exploring the limits of weakly supervised pretraining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Mahajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vignesh</forename><surname>Ramanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Cats and dogs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Parkhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">V</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jawahar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qizhe</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.10580,2020.4</idno>
		<title level="m">Meta pseudo labels</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Acceleration of stochastic approximation by averaging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">T</forename><surname>Polyak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B</forename><surname>Juditsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Control and Optimization</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Learning transferable visual models from natural language supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jong</forename><forename type="middle">Wook</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Hallacy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gretchen</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.00020</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Zero: memory optimizations toward training trillion parameter models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samyam</forename><surname>Rajbhandari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Rasley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olatunji</forename><surname>Ruwase</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxiong</forename><surname>He</surname></persName>
		</author>
		<idno>2020. 6</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis</title>
		<editor>Christine Cuicchi, Irene Qualters, and William T. Kramer</editor>
		<meeting>the International Conference for High Performance Computing, Networking, Storage and Analysis<address><addrLine>USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page">20</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rebecca</forename><surname>Roelofs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ludwig</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vaishaal</forename><surname>Shankar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.10811</idno>
		<title level="m">Do imagenet classifiers generalize to imagenet? arXiv preprint</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ImageNet Large Scale Visual Recognition Challenge. IJCV</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Adafactor: Adaptive learning rates with sublinear memory cost</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitchell</forename><surname>Stern</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aravind</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.11605</idno>
		<title level="m">Jonathon Shlens, Pieter Abbeel, and Ashish Vaswani. Bottleneck transformers for visual recognition</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Revisiting Unreasonable Effectiveness of Data in Deep Learning Era. ICCV</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">EfficientNet: Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Efficientnetv2: Smaller models and faster training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno>PMLR, 2021. 8</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th International Conference on Machine Learning, ICML 2021</title>
		<editor>Marina Meila and Tong Zhang</editor>
		<meeting>the 38th International Conference on Machine Learning, ICML 2021</meeting>
		<imprint>
			<publisher>Virtual Event</publisher>
			<date type="published" when="2021-07-24" />
			<biblScope unit="volume">139</biblScope>
			<biblScope unit="page" from="10096" to="10106" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Ce Zhang, and Yuxiong He. 1-bit adam: Communication efficient largescale training with adam&apos;s convergence speed</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanlin</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoduo</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ammar</forename><surname>Ahmad Awan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samyam</forename><surname>Rajbhandari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Conglong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangru</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Liu</surname></persName>
		</author>
		<idno>PMLR, 2021. 6</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th International Conference on Machine Learning, ICML 2021</title>
		<editor>Marina Meila and Tong Zhang</editor>
		<meeting>the 38th International Conference on Machine Learning, ICML 2021</meeting>
		<imprint>
			<date type="published" when="2021-07-24" />
			<biblScope unit="volume">139</biblScope>
			<biblScope unit="page" from="10118" to="10129" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Training data-efficient image transformers &amp; distillation through attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.12877</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Going deeper with image transformers. CoRR, abs/2103.17239</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.06423,2020.11</idno>
		<title level="m">Matthijs Douze, and Herv? J?gou. Fixing the train-test resolution discrepancy</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Scaling local self-attention for parameter efficient visual backbones</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prajit</forename><surname>Ramachandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aravind</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Blake</forename><surname>Hechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.12731</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.03762</idno>
		<title level="m">Attention is all you need</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Pyramid vision transformer: A versatile backbone for dense prediction without convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enze</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng-Ping</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaitao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ding</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.12122</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Caltech-UCSD Birds 200</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<idno>CNS-TR-2010-001</idno>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
		<respStmt>
			<orgName>California Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Resnet strikes back: An improved training procedure in timm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Wightman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
		<idno>abs/2110.00476</idno>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qizhe</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.04252</idno>
		<title level="m">Self-training with noisy student improves imagenet classification</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Self-training with noisy student improves imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qizhe</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunpeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujun</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">H</forename><surname>Francis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.11986</idno>
		<title level="m">Tokens-to-token vit: Training vision transformers from scratch on imagenet</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">S4l: Self-supervised semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avital</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1476" to="1485" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Sylvain Gelly, and Neil Houlsby. A large-scale study of representation learning with the visual task adaptation benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Puigcerver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Ruyssen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Riquelme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Lucic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josip</forename><surname>Djolonga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andre</forename><forename type="middle">Susano</forename><surname>Pinto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Bachem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Tschannen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcin</forename><surname>Michalski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Bousquet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.04867</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Deepvit: Towards deeper vision transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daquan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingyi</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojie</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjie</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaochen</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qibin</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<idno>abs/2103.11886</idno>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
