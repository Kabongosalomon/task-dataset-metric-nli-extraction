<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Joint Feature Learning and Relation Modeling for Tracking: A One-Stream Framework</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Botao</forename><surname>Ye</surname></persName>
							<email>botao.ye@vipl.ict.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computing Technology</orgName>
								<orgName type="laboratory">Key Lab of Intelligent Information Processing of Chinese Academy of Sciences (CAS)</orgName>
								<orgName type="institution">CAS</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100049</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Chang</surname></persName>
							<email>changhong@ict.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computing Technology</orgName>
								<orgName type="laboratory">Key Lab of Intelligent Information Processing of Chinese Academy of Sciences (CAS)</orgName>
								<orgName type="institution">CAS</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100049</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingpeng</forename><surname>Ma</surname></persName>
							<email>bpma@ucas.ac.cn</email>
							<affiliation key="aff1">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100049</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiguang</forename><surname>Shan</surname></persName>
							<email>sgshan@ict.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computing Technology</orgName>
								<orgName type="laboratory">Key Lab of Intelligent Information Processing of Chinese Academy of Sciences (CAS)</orgName>
								<orgName type="institution">CAS</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100049</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xilin</forename><surname>Chen</surname></persName>
							<email>xlchen@ict.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computing Technology</orgName>
								<orgName type="laboratory">Key Lab of Intelligent Information Processing of Chinese Academy of Sciences (CAS)</orgName>
								<orgName type="institution">CAS</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100049</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Joint Feature Learning and Relation Modeling for Tracking: A One-Stream Framework</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T14:54+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The current popular two-stream, two-stage tracking framework extracts the template and the search region features separately and then performs relation modeling, thus the extracted features lack the awareness of the target and have limited target-background discriminability. To tackle the above issue, we propose a novel one-stream tracking (OSTrack) framework that unifies feature learning and relation modeling by bridging the template-search image pairs with bidirectional information flows. In this way, discriminative target-oriented features can be dynamically extracted by mutual guidance. Since no extra heavy relation modeling module is needed and the implementation is highly parallelized, the proposed tracker runs at a fast speed. To further improve the inference efficiency, an in-network candidate early elimination module is proposed based on the strong similarity prior calculated in the one-stream framework. As a unified framework, OSTrack achieves stateof-the-art performance on multiple benchmarks, in particular, it shows impressive results on the one-shot tracking benchmark GOT-10k, i.e., achieving 73.7% AO, improving the existing best result (SwinTrack) by 4.3%. Besides, our method maintains a good performance-speed tradeoff and shows faster convergence. The code and models are available at</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Visual object tracking (VOT) aims at localizing an arbitrary target in each video frame, given only its initial appearance. The continuously changing and arbitrary nature of the target poses a challenge to learn a target appearance model that can effectively discriminate the specified target from the background. Current mainstream trackers typically address this problem with a common two-stream and two-stage pipeline, which means that the features of the template and the search region are separately extracted (two-stream), and the whole process is divided into two sequential steps: feature extraction and relation modeling (twostage). Such a natural pipeline employs the strategy of "divide-and-conquer" and achieves remarkable success in terms of tracking performance. However, the separation of feature extraction and relation modeling suffers from the following limitations. Firstly, the feature extracted by the vanilla twostream two-stage framework is unaware of the target. In other words, the extracted feature for each image is determined after off-line training, since there is no interaction between the template and the search region. This is against with the continuously changing and arbitrary nature of the target, leading to limited target-background discriminative power. On some occasions when the category of the target object is not involved in the training dataset (i.e., one-shot tracking), the above problems are particularly serious. Secondly, the two-stream, two-stage framework is vulnerable to the performance-speed dilemma. According to the computation burden of the feature fusion module, two different strategies are commonly utilized. The first type, shown in <ref type="figure" target="#fig_1">Fig 2(a)</ref>, simply adopts one single operator like cross-correlation <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b26">27]</ref> or discriminative correlation filter <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b6">7]</ref>, which is efficient but less effective since the simple linear operation leads to discriminative information loss <ref type="bibr" target="#b4">[5]</ref>. The second type, shown in <ref type="figure" target="#fig_1">Fig 2(b)</ref>, addresses the information loss by complicated non-linear interaction (Transformer <ref type="bibr" target="#b43">[44]</ref>), but is less efficient due to a large number of parameters and the use of iterative refinement (e.g., for each search image, STARK-S50 <ref type="bibr" target="#b49">[50]</ref> takes 7.5 ms for the feature extraction and 14.1 ms for relation modeling on an RTX2080Ti GPU).</p><p>In this work, we set out to address the aforementioned problems via a unified one-stream one-stage tracking framework. The core insight of the one-stream framework is to bridge a free information flow between the template and search region at the early stage (i.e., the raw image pair), thus extracting targetoriented features and avoiding the loss of discriminative information. Specifically, we concatenate the flattened template and search region and feed them into staked self-attention layers <ref type="bibr" target="#b43">[44]</ref> (widely used Vision Transformer (ViT) <ref type="bibr" target="#b11">[12]</ref> is chosen in our implementation), and the produced search region features can be directly used for target classification and regression without further matching. The staked self-attention operations enable iteratively feature matching between the template and the search region, thus allowing mutual guidance for targetoriented feature extraction. Therefore, both template and search region features can be extracted dynamically with strong discriminative power. Additionally, the proposed framework achieves a good balance between performance and speed because the concatenation of the template and the search region makes the onestream framework highly parallelizable and does not require additional heavy relational modeling networks.</p><p>Moreover, the proposed one-stream framework provides a strong prior about the similarity of the target and each part of the search region (i.e. candidates) as shown in <ref type="figure" target="#fig_3">Fig. 4</ref>, which means that the model can identify background regions even at the early stage. This phenomenon verifies the effectiveness of the onestream framework and motivates us to propose an in-network early candidate elimination module for progressively identifying and discarding the candidates belonging to the background in a timely manner. The proposed candidate elimination module not only significantly boosts the inference speed, but also avoids the negative impact of uninformative background regions on feature matching.</p><p>Despite its simple structure, the proposed trackers achieve impressive performance and set a new state-of-the-art (SOTA) on multiple benchmarks. Moreover, it maintains adorable inference efficiency and shows faster convergence compared to SOTA Transformer based trackers. As shown in <ref type="figure" target="#fig_0">Fig. 1</ref>, our method achieves a good balance between the accuracy and inference speed.</p><p>The main contributions of this work are three-fold: (1) We propose a simple, neat, and effective one-stream, one-stage tracking framework by combining the feature extraction and relation modeling. (2) Motivated by the prior of the early acquired similarity score between the target and each part of the search region, an in-network early candidate elimination module is proposed for decreasing the inference time. (3) We perform comprehensive experiments to verify that the one-stream framework outperforms the previous SOTA two-stream trackers in terms of performance, inference speed, and convergence speed. The resulting tracker OSTrack sets a new state-of-the-art on multiple tracking benchmarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>In this section, we briefly review different tracking pipelines, as well as the adaptive inference methods related to our early candidate elimination module.</p><p>Tracking Pipelines. Based on the different computational burdens of feature extraction and relation modeling networks, we compare our method with two different two-stream two-stage archetypes in <ref type="figure" target="#fig_1">Fig. 2</ref>. Earlier Siamese trackers <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b54">55]</ref> and discriminative trackers <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b6">7]</ref> belong to <ref type="figure" target="#fig_1">Fig. 2</ref>(a). They first extract the features of the template and the search region separately by a CNN backbone <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b23">24]</ref>, which shares the same structure and parameters. Then, a lightweight relation modeling network (e.g., the cross-correlation layer <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b25">26]</ref> in Siamese trackers and correlation filter <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b18">19]</ref> in discriminative trackers) takes responsibility to fuse these features for the subsequent state estimation task. However, the template feature cannot be adjusted according to the search region feature in these methods. Such a shallow and unidirectional relation modeling strategy may be insufficient for information interaction. Recently, stacked Transformer layers <ref type="bibr" target="#b43">[44]</ref> are introduced for better relation modeling. These methods belong to <ref type="figure" target="#fig_1">Fig. 2</ref>  and enables bi-directional information interaction. TransT <ref type="bibr" target="#b4">[5]</ref> proposes to stack a series of self-attention and cross-attention layers for iterative feature fusion. STARK <ref type="bibr" target="#b49">[50]</ref> concatenates the pre-extracted template and search region features and feeds them into multiple self-attention layers. The bi-directional heavy structure brings performance gain but inevitably slows down the inference speed. Differently, our one-stream one-stage design belongs to <ref type="figure" target="#fig_1">Fig. 2(c)</ref>. For the first time, we seamlessly combine feature extraction and relation modeling into a unified pipeline. The proposed method provides free information flow between the template and search region with minor computation costs. It not only generates target-oriented features by mutual guidance but also is efficient in terms of both training and testing time.</p><p>Adaptive Inference. Our early candidate elimination module can be seen as a progressive process of adaptively discarding potential background regions based on the similarity between the target and the search region. One related topic is the adaptive inference <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b51">52]</ref> in vision transformers, which is proposed to accelerate the computation of ViT. DynamicViT <ref type="bibr" target="#b38">[39]</ref> trains extra control gates with the Gumbel-softmax trick to discard tokens during inference. Instead of directly discarding non-informative tokens, EViT <ref type="bibr" target="#b28">[29]</ref> fuses them to avoid potential information loss. These works are tightly coupled with the classification task and are therefore not suitable for tracking. Instead, we treat each token as a target candidate and then discard the candidates that are least similar to the target by means of a free similarity score calculated by the self-attention operation. To the best of our knowledge, this is the first work that attempts to eliminate potential background candidates within the tracking network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>This section describes the proposed one-stream tracker (OSTrack). The input image pairs are fed into a ViT backbone for simultaneous feature extraction and relation modeling, and the resulting search region features are directly adopted  The structure of the encoder layer with early candidate elimination module, which is insert after the multi-head attention operation <ref type="bibr" target="#b43">[44]</ref>.</p><p>for subsequent target classification and regression. An overview of the model is shown in <ref type="figure" target="#fig_2">Fig. 3</ref>(a).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Joint Feature Extraction and Relation Modeling</head><p>We propose to combine the feature extraction and relation modeling modules and construct a free information flow between the contents of the template and the search region. The global contextual modeling capacity of self-attention <ref type="bibr" target="#b43">[44]</ref> operation perfectly fits our goal, therefore, vanilla ViT <ref type="bibr" target="#b11">[12]</ref> is selected as the main body of OSTrack. Adopting the existing Vision Transformer architecture also provides a bunch of publicly available pre-trained models <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b41">42]</ref>, freeing us from the time-consuming pre-training stage. The input of OSTrack is a pair of images, namely, the template image patch z ? R 3?Hz?Wz and the search region patch x ? R 3?Hx?Wx . They are first split and flattened into sequences of patches z p ? R Nz?(3?P 2 ) and x p ? R Nx?(3?P 2 ) , where P ? P is the resolution of each patch, and N z = H z W z /P 2 , N x = H x W x /P 2 are the number of patches of template and search region respectively. After that, a trainable linear projection layer with parameter E is used to project z p and x p into D dimension latent space as in Eq. 1 and Eq. 2, and the output of this projection is commonly called patch embeddings <ref type="bibr" target="#b11">[12]</ref>. Learnable 1D position embeddings P z and P x are added to the patch embeddings of the template and search region separately to produce the final template token embeddings H 0 z ? R Nz?D and search region</p><formula xml:id="formula_0">token embeddings H 0 x ? R Nx?D . H 0 z = z 1 p E; z 2 p E; ? ? ? ; z Nz p E + P z , E ? R (3?P 2 )?D , P z ? R Nz?D (1) H 0 x = x 1 p E; x 2 p E; ? ? ? ; x Nz p E + P x , P x ? R Nx?D<label>(2)</label></formula><p>To verify whether adding addition identity embeddings (to indicate a token belonging to the template or search region as in BERT <ref type="bibr" target="#b10">[11]</ref>) or adopting relative positional embeddings are beneficial to the performance, we also conduct ablation studies and observe no significant improvement, thus they are omitted for simplicity (details can be found in the supplementary material). Token sequences H 0 z and H 0 x are then concatenated as H 0</p><formula xml:id="formula_1">zx = [H 0 z ; H 0 x ]</formula><p>, and the resulting vector H 0 zx is then fed into several Transformer encoder layers <ref type="bibr" target="#b11">[12]</ref>. Unlike the vanilla ViT <ref type="bibr" target="#b11">[12]</ref>, we insert the proposed early candidate eliminating module into some of encoder layers as shown in <ref type="figure" target="#fig_2">Fig. 3</ref>(b) for inference efficiency, and the technical details are presented in Sec. 3.2. Notably, adopting the self-attention of concatenated features makes the whole framework highly parallelized compared to the cross-attention <ref type="bibr" target="#b4">[5]</ref>. Although template images are also fed into the ViT for each search frame, the impact on the inference speed is minor due to the highly parallel structure and the fact that the number of template tokens is small compared to the number of search region tokens.</p><p>Analysis. From the perspective of the self-attention mechanism <ref type="bibr" target="#b43">[44]</ref>, we further analyze the intrinsic reasons why the proposed framework is able to realize simultaneous feature extraction and relation modeling. The output of self-attention operation A in our approach can be written as:</p><formula xml:id="formula_2">A = Softmax QK ? ? d k ? V = Softmax [Q z ; Q x ][K z ; K x ] ? ? d k ? [V z ; V x ],<label>(3)</label></formula><p>where Q, K, and V are query, key and value matrices separately. The subscripts z and x denote matrix items belonging to the template and search region. The calculation of attention weights in Eq. 3 can be expanded to:</p><formula xml:id="formula_3">Softmax [Q z ; Q x ][K z ; K x ] ? ? d k = Softmax [Q z K ? z , Q z K ? x ; Q x K ? z , Q x K ? x ] ? d k ? [W zz , W zx ; W xz , W xx ],<label>(4)</label></formula><p>where W zx is a measure of similarity between the template and the search region, and the rest are similar. The output A can be further written as:</p><formula xml:id="formula_4">A = [W zz V z + W zx V x ; W xz V z + W xx V x ].<label>(5)</label></formula><p>In the right part of Eq. 5, W xz V z is responsible for aggregating the iter-image feature (relation modeling) and W xx V x aggregating the intra-image feature (feature extraction) based on the similarity of different image parts. Therefore, the feature extraction and relation modeling can be done with a self-attention operation. Moreover, Eq. 5 also constructs a bi-direction information flow that allows mutual guidance of target-oriented feature extraction through the similarity learning.</p><p>Comparisons with Two-Stream Transformer Fusion Trackers. 1) Previous two-stream Transformer fusion trackers <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b29">30]</ref> all adopt a Siamese framework, where the features of the template and search region are separately extracted first, and the Transformer layer is only adopted to fuse the extracted features. Therefore, the extracted features of these methods are not adaptive and may lose some discriminative information, which is irreparable. In contrast, OSTrack directly concatenates linearly projected template and search region images at the first stage, so feature extraction and relation modeling are seamlessly integrated and target-oriented features can be extracted through the mutual guidance of the template and the search region. 2) Previous Transformer fusion trackers only employ ImageNet <ref type="bibr" target="#b9">[10]</ref> pre-trained backbone networks <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b31">32]</ref> and leave Transformer layers randomly initialized, which degrades the convergence speed, while OSTrack benefits from pre-trained ViT models for faster convergence.</p><p>3) The one-stream framework provides the possibility of identifying and discarding useless background regions for further improving the model performance and inference speed as presented in Sec. 3.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Early Candidate Elimination</head><p>Each token of the search region can be regarded as a target candidate and each template token can be considered as a part of the target object. Previous trackers keep all candidates during feature extraction and relation modeling, while background regions are not identified until the final output of the network (i.e., classification score map). However, our one-stream framework provides a strong prior on the similarity between the target and each candidate. As shown in <ref type="figure" target="#fig_3">Fig. 4</ref>, the attention weights of the search region highlight the foreground objects in the early stage of ViT (e.g., layer 4), and then progressively focus on the target. This property makes it possible to progressively identify and eliminate candidates belonging to the background regions inside the network. Therefore, we propose an early candidate elimination module that progressively eliminates candidates belonging to the background in the early stages of ViT to lighten the computational burden and avoid the negative impact of noisy background regions on feature learning.</p><p>Candidate Elimination. Recall that the self-attention operation in ViT can be seen as a spatial aggregation of tokens with normalized importances <ref type="bibr" target="#b43">[44]</ref>, which is measured by the dot product similarity between each token pair. Specifically, each template token h i z , 1 ? i ? N z is calculated as:</p><formula xml:id="formula_5">h i z = Softmax q i ? [K z ; K x ] ? ? d ? V = [w i z ; w i x ] ? V ,<label>(6)</label></formula><p>where q i , K z , K x and V denote the query vector of token h i z , the key matrix corresponding to the template, the key matrix corresponding to the search region and the value matrix. The attention weight w i x determines the similarity between the template part h i z and all search region tokens (candidates). The j ? th item (1 ? j ? n, n is the number of input search region tokens) of w i x determines the similarity between h i z and the j ? th candidate. However, the input templates usually include background regions that introduce noise when calculating the similarity between the target and each candidate. Therefore, instead of summing up the similarity of each candidate to all template parts w i</p><formula xml:id="formula_6">x , i = 1, . . . , N z , we take w ? x , ? = ? Wz 2 ? + W z ? ? Hz 2 ? (? ?</formula><p>th token corresponding to the center part of the original template image) as the representative similarity. This is fairly reasonable as the center template part has aggregated enough information through self-attention to represent the target. In the supplementary, we compare the effect of different template token choices. Considering that multihead self-attention is used in ViT, there are multiple similarity scores w ? x (m), where m = 1, ..., M and M is the total number of attention heads <ref type="bibr" target="#b43">[44]</ref>. We average the similarity scores of all heads by w ? x = M m=1 w ? x (m)/M , which serves as the final similarity score of the target and each candidate. One candidate is more likely to be a background region if its similarity score with the target is relatively small. Therefore, we only keep the candidates corresponding to the k largest (top-k) elements in w ? x (k is a hyperparameter, and we define the token keeping ratio as ? = k/n), while the remaining candidates are eliminated. The proposed candidate elimination module is inserted after the multi-head attention operation <ref type="bibr" target="#b43">[44]</ref> in the encoder layer, which is illustrated in <ref type="figure" target="#fig_2">Fig. 3(b)</ref>. In addition, the original order of all remaining candidates is recorded so that it can be recovered in the final stage.</p><p>Candidate Restoration. The aforementioned candidate elimination module disrupts the original order of the candidates, making it impossible to reshape the candidate sequence back into the feature map as described in Sec. 3.3, so we restore the original order of the remaining candidates and then pad the missing positions. Since the discarded candidates belong to the irrelevant background regions, they will not affect the classification and regression tasks. In other words,  they just act as placeholders for the reshaping operation. Therefore, we first restore the order of the remaining candidates and then zero-pad in between them. Visualization. To further investigate the behavior of the early candidate elimination module, we visualize the progressive process in <ref type="figure" target="#fig_4">Fig. 5</ref>. By iteratively discarding the irrelevant tokens in the search region, OSTrack not only largely lightens the computation burden but also avoids the negative impact of noisy background regions on feature learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Head and Loss</head><p>We first re-interpret the padded sequence of search region tokens to a 2D spatial feature map and then feed it into a fully convolutional network (FCN), which consists of L stacked Conv-BN-ReLU layers for each output. Outputs of the FCN contain the target classification score map P ? [0, 1] Hx P ? Wx P , the local offset O ? [0, 1) 2? Hx P ? Wx P to compensate the discretization error caused by reduced resolution and the normalized bounding box size (i.e. width and height) S ? [0, 1] 2? Hx P ? Wx P . The position with highest classification score is considered to be target position, i.e., (x d , y d ) = arg max (x,y) P xy and the finial target bounding box is obtained as:</p><formula xml:id="formula_7">(x, y, w, h) = (x d + O(0, x d , y d ), y d + O(1, x d , y d ), S(0, x d , y d ), S(1, x d , y d )). (7)</formula><p>During training, both classification and regression losses are used. We adopt the weighted focal loss <ref type="bibr" target="#b24">[25]</ref> for classification (see the supplementary for more details). With the predicted bounding box, ? 1 loss and the generalized IoU loss <ref type="bibr" target="#b39">[40]</ref> are employed for bounding box regression. Finally, the overall loss function is:</p><formula xml:id="formula_8">L track = L cls + ? iou L iou + ? L1 L 1 ,<label>(8)</label></formula><p>where ? iou = 2 and ? L1 = 5 are the regularization parameters in our experiments as in <ref type="bibr" target="#b49">[50]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>After introducing the implementation details, this section first presents a comparison of OSTrack with other state-of-the-art methods on seven different benchmarks. Then, ablation studies are provided to analyze the impact of each component and different design choices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Implementation Details</head><p>Our trackers are implemented in Python using PyTorch. The models are trained on 4 NVIDIA A100 GPUs and the inference speed is tested on a single NVIDIA RTX2080Ti GPU.</p><p>Model. The vanilla ViT-Base <ref type="bibr" target="#b11">[12]</ref> model pre-trained with MAE [17] is adopted as the backbone for joint feature extraction and relation modeling. The head is a lightweight FCN, consisting of 4 stacked Conv-BN-ReLU layers for each of three outputs. The keeping ratio ? of each candidate elimination module is set as 0.7, and a total of three candidate elimination modules are inserted at layers 4, 7, and 10 of ViT respectively, following <ref type="bibr" target="#b38">[39]</ref>. We present two variants with different input image pair resolution for showing the scalability of OSTrack: Training. The training splits of COCO <ref type="bibr" target="#b30">[31]</ref>, LaSOT <ref type="bibr" target="#b13">[14]</ref>, GOT-10k <ref type="bibr" target="#b19">[20]</ref> (1k forbidden sequences from GOT-10k training set are removed following the convention <ref type="bibr" target="#b49">[50]</ref>) and TrackingNet <ref type="bibr" target="#b36">[37]</ref> are used for training. Common data augmentations including horizontal flip and brightness jittering are used in training. Each GPU holds 32 image pairs, resulting in a total batch size of 128. We train the model with AdamW optimizer <ref type="bibr" target="#b32">[33]</ref>, set the weight decay to 10 ?4 , the initial learning rate for the backbone to 4 ? 10 ?5 and other parameters to 4 ? 10 ?4 , respectively. The total training epochs are set to 300 with 60k image pairs per epoch and we decrease the learning rate by a factor of 10 after 240 epochs.</p><p>Inference. During inference, Hanning window penalty is adopted to utilize positional prior in tracking following the common practice <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b54">55]</ref>. Specifically, we simply multiply the classification map P by the Hanning window with the same size, and the box with the highest score after multiplication will be selected as the tracking result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Comparison with State-of-the-arts</head><p>To demonstrate the effectiveness of the proposed models, we compare them with state-of-the-art (SOTA) trackers on seven different benchmarks.</p><p>GOT-10k. GOT-10k <ref type="bibr" target="#b19">[20]</ref> test set employs the one-shot tracking rule, i.e., it requires the trackers to be trained only on the GOT-10k training split, and the object classes between train and test splits are not overlapped. We follow this protocol to train our model and evaluate the results by submitting them to <ref type="table">Table 1</ref>: Comparison with state-of-the-arts on four large-scale benchmarks: La-SOT, LaSOT ext , TrackingNet and GOT-10k 3 . The best two results are shown in red and blue fonts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Source LaSOT <ref type="bibr" target="#b13">[14]</ref> LaSOText <ref type="bibr" target="#b12">[13]</ref> TrackingNet <ref type="bibr" target="#b36">[37]</ref> GOT-10k * [20] AUC PNorm P AUC PNorm P AUC PNorm P AO SR0.5 SR0.   <ref type="bibr" target="#b5">6</ref>.5%, which verifies the capability of our trackers in both accurate target-background discrimination and bounding box regression. Moreover, the high performance on this one-shot tracking benchmark demonstrates that our one-stream tracking framework can extract more discriminative features for unseen classes by mutual guidance.</p><p>LaSOT. LaSOT <ref type="bibr" target="#b13">[14]</ref> is a challenging large-scale long-term tracking benchmark, which contains 280 videos for testing. We compare the result of the OS-Track with previous SOTA trackers in Tab. 1. The results show that the proposed tracker with smaller input resolution, i.e., OSTrack-256, already obtains comparable performance with SwinTrack-B <ref type="bibr" target="#b29">[30]</ref>. Besides, OSTrack-256 runs at a fast inference speed of 105.4 FPS, being 2x faster than SwinTrack-B (52 FPS), which indicates that OSTrack achieves an excellent balance between accuracy and inference speed. By increasing the input resolution, OSTrack-384 further improves the AUC on LaSOT to 71.1% and sets a new state-of-the-art. NFS, UAV123 and TNL2K. We also evaluate our tracker on three additional benchmarks: NFS <ref type="bibr" target="#b21">[22]</ref>, UAV123 <ref type="bibr" target="#b35">[36]</ref> and TNL2K <ref type="bibr" target="#b48">[49]</ref> includes 100, 123, and 700 video sequences, separately. The results in Tab. 2 show that OSTrack-384 achieves the best performance on all three benchmarks, demonstrating the strong generalizability of OSTrack.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablation Study and Analysis</head><p>The Effect of Early Candidate Elimination Module Tab. 1 shows that increasing the input resolution of the input image pairs can bring significant performance gain. However, the quadratic complexity with respect to the input resolution makes simply increasing the input resolution unaffordable in inference time. The proposed early candidate elimination module addresses the above problem well. We present the effect of the early candidate elimination module from the aspects of inference speed (FPS), multiply-accumulate computations (MACs), and tracking performance on multiple benchmarks in Tab. 3. The effect on different input search region resolutions is also presented. Tab. 3 shows that the early candidate elimination module can significantly decrease the calculation and increase the inference speed, while slightly boosting the performance in most cases. This demonstrates that the proposed module alleviates the negative impact brought by the noisy background regions on feature learning. For example, adding the early candidate elimination module in OSTrack-256 decreases the MACs by 25.9% and increases the tracking speed by 13.2%, and the LaSOT AUC is increased by 0.4%. Furthermore, larger input resolution benefits more from this module, e.g., OSTrack-384 shows a 40.3% increase in speed.  <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b49">50]</ref> random initialize the weights of Transformer layers, our joint feature learning and relation modeling module can directly benefit from the pre-trained weights. We further investigate the effect of different pre-training methods on the tracking performance by comparing four different pre-training strategies: no pre-training; ImageNet-1k <ref type="bibr" target="#b9">[10]</ref> pre-trained model provided by <ref type="bibr" target="#b42">[43]</ref>; ImageNet-21k <ref type="bibr" target="#b40">[41]</ref> pre-trained model provided by <ref type="bibr" target="#b41">[42]</ref>; unsupervised pre-training model MAE <ref type="bibr" target="#b16">[17]</ref>. As the results in Tab. 4 show, pre-training is necessary for the model weights initialization. Interestingly, we also observe that the unsupervised pretraining method MAE brings better tracking performance than the supervised pre-training ones using ImageNet. We hope this can inspire the community for designing better pre-training strategies tailored for the tracking task.</p><p>Aligned Comparison with SOTA Two-stream Trackers. One may wonder whether the performance gain is brought by the proposed one-stream structure or purely by the superiority of ViT. We thus compare our method with two SOTA two-stream Transformer fusion trackers <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b49">50]</ref> by eliminating the influencing factors of backbone and head structure. To be specific, we align two previous SOTA two-stream trackers (STRAK-S <ref type="bibr" target="#b49">[50]</ref> and SwinTrack <ref type="bibr" target="#b29">[30]</ref>) with ours for fair comparison as follows: replacing their backbones with the same pre-trained ViT and setting the same input resolution, head structure, and training objective as OSTrack-256. The remaining experimental settings are kept the same as in the original paper. As shown in Tab. 5, our re-implemented two-stream trackers show comparable or stronger performance compared to the initially published performance, but still lag behind OSTrack, which demonstrates the effectiveness of our one-stream structure. We also observe that OSTrack significantly outperforms the previous two-stream trackers on the one-shot benchmark GOT-10k, which further proves the advantage of our one-stream framework in the challenging scenario. Actually, the discriminative power of features extracted by the two-stream framework is limited since the object classes in the testing set are completely different from the training set. Whereas, by iterative interaction between the features of the template and search region, OSTrack can extract more discriminative features through mutual guidance. Different from the two-stream SOTA trackers, OSTrack neglects the extra heavy relation modeling module while still keeping the high parallelism of joint feature extraction and relation modeling module. Therefore, when the same backbone network is adopted, the proposed one-stream framework is much faster than STARK (40.2</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input</head><p>OSTrack SwinTrack-aligned Input OSTrack SwinTrack-aligned <ref type="figure">Fig. 6</ref>: Visualization of discriminative regions (i.e., activation maps) of backbone features extracted by OSTrack and two-stream tracker (SwinTrack-aligned). <ref type="table">Table 5</ref>: Comparison with re-implemented previous SOTA trackers aligned with OSTrack. Here OSTrack is trained without the early candidate elimination module for fair comparison and "aligned" denotes that the backbone, head, loss and input resolution are kept the same as OSTrack. Discriminative Region Visualization. To better illustrate the effectiveness of the proposed one-stream tracker, we visualize the discriminative regions of the backbone features extracted by OSTrack and a SOTA two-stream tracker (SwinTrack-aligned) in <ref type="figure">Fig. 6</ref>. As can be observed, due to the lack of target awareness, features extracted by the backbone of SwinTrack-aligned show limited target-background discriminative power and may lose some important target information (e.g., head and helmet in <ref type="figure">Fig. 6</ref>), which is irreparable. In contrast, OSTrack can extract discriminative target-oriented features, since the proposed early fusion mechanism enables relation modeling between the template and search region at the first stage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>This work proposes a simple, neat, and high-performance one-stream tracking framework based on Vision Transformer, which breaks out of the Siamese-like pipeline. The proposed tracker combines the feature extraction and relation modeling tasks, and shows a good balance between performance and inference speed. In addition, we further propose an early candidate elimination module that progressively discards search region tokens belonging to background regions, which significantly boosts the tracking inference speed. Extensive experiments show that the proposed one-stream trackers perform much better than previous methods on multiple benchmarks, especially under the one-shot protocol. We expect this work can attract more attention to the one-stream tracking framework.  discriminative regions of each feature map F . We first calculate the absolute mean values of each pixel along the channel dimension:</p><formula xml:id="formula_9">M (F ) = 1 C ? C c=1 |F c | ,<label>(10)</label></formula><p>where C is the number of channels. Then, the relative importance of each pixel is then calculated by:</p><formula xml:id="formula_10">D(F ) = M (F ) ? min(M (F )) max(M (F )) ? min(M (F )) ,<label>(11)</label></formula><p>where min(?) and max(?) return the maximum and minimum values of all pixels, respectively. As pointed out in Sec. 3.2, the goal of the early candidate elimination module is to identify and discard candidates belonging to background regions based on the ranking of similarity between the target and each candidate. However, the input template also contains background regions, which introduces noisy information when calculating the similarity score. Therefore, different choices of template parts (tokens) used for the similarity calculation may influence the candidate elimination results and consequently affect the tracking performance. We compare four different template token choices (the similarity scores of all chosen template tokens are summed up for the final ranking): 1) all template tokens; 2) all template tokens within the ground truth target bounding box; 3) template tokens within a 4x4 region around the center of the template image; 4) the template token corresponding to the center of the template image. The result comparison of these template choices is shown in Tab. A1. The results demonstrate that different template token choices do affect the quality of identifying background candidates. Since the input template contains background regions, directly using "All Template Tokens" clearly degrades the tracking performance compared with the baseline ("No Early Candidate Elimination"), i.e., 0.6% lower in LaSOT AUC. Compared to other choices, using the central template token shows better performance, probably because the central token does not contain any background region and has aggregated the entire target information through self-attention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Identity Embeddings and Relative Positional Embeddings</head><p>We additionally verify the effect of adding identity embeddings and relative positional embeddings. Specifically, for the identity embeddings, we add learnable identity embeddings (to indicate a token belonging to the template or search region as in BERT <ref type="bibr" target="#b10">[11]</ref>) to template tokens and search region tokens separately. For the relative positional embeddings, the same method as in SwinTrack <ref type="bibr" target="#b29">[30]</ref> is adopted. The results are presented in Tab. A2, these two components do not bring performance gain compared to the original design, thus not adopted in our model.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3 Additional Relation Modeling Module</head><p>To investigate whether our one-stream framework does not require an extra feature relation module, we add an additional transformer-based feature fusion module proposed in <ref type="bibr" target="#b29">[30]</ref>, which consists of 4 self-attention layers and 1 crossattention layer, to further fusion the extracted template and search region features. As the results in Tab. A3 show, adding such a relation modeling module instead degrades the tracking performance, indicating that the output search region features of the ViT backbone have been sufficiently fused with the template features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.4 Fewer Relation Modeling Layers</head><p>In the implementation of vanilla OSTrack, all encoder layers in ViT-Base (12 layers in total) are used for simultaneous feature extraction and relation modeling. In this subsection, we try to decrease the number of layers used for relation modeling. Specifically, only the last n encoder layers are used for simultaneous feature extraction and relation modeling, and the first 12 ? n layers are only used for the template and search region feature extraction. n is set to be 6 and 3 separately and the results are presented in Tab. A4. The results show that using fewer encoder layers for simultaneous feature extraction and relation modeling will degrade the tracking performance, showing the necessity of sufficient feature fusion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.5 Different Token Drop Rate</head><p>We also try to apply a different keeping ratio ? for the early candidate elimination module. As the results in Tab. A5 show, using ? &lt; 0.7 leads to performance drop on the LaSOT <ref type="bibr" target="#b13">[14]</ref> tracking benchmark since small ? may cause a significant information loss. However, the reduction in computational cost that comes with   large ? is limited. Setting ? = 0.7 shows a decent decrease in computational cost with a slight improvement in tracking performance. Therefore, we use ? = 0.7 in our experiments.</p><p>C Results on VOT2020 VOT2020 [23] is a challenging short-term tracking benchmark that is evaluated by target segmentation results. To evaluate OSTrack on VOT2020, we use Al-phaRefine <ref type="bibr" target="#b50">[51]</ref> to generate segmentation masks, and the results are shown in Tab. A6. Since the wide existence of distractors in VOT2020, updating the template during the tracking process has become a common practice to avoid tracking drift, which can bring significant performance gain (e.g., STARK-ST50 citestark raises the EAO from 0.462 to 0.505 by simply adding a dynamic template). OSTrack-256 obtains an EAO of 0.518, which already outperforms the STARK-ST50 with an online template updating mechanism. This demonstrates the great potential of OSTrack which serves as a neat and strong baseline model. <ref type="table">Table A7</ref>: Comparison with state-of-the-arts on ITB <ref type="bibr" target="#b27">[28]</ref> benchmark. mIoU(%) scores are reported. The best two results are shown in red and blue fonts.</p><p>SiamRPN++ <ref type="bibr" target="#b25">[26]</ref> Ocean <ref type="bibr" target="#b54">[55]</ref> GAT <ref type="bibr" target="#b15">[16]</ref> ATOM <ref type="bibr" target="#b6">[7]</ref> DiMP <ref type="bibr" target="#b1">[2]</ref> PrDiMP <ref type="bibr" target="#b8">[9]</ref> KYS <ref type="bibr" target="#b2">[3]</ref> TrDiMP <ref type="bibr" target="#b46">[47]</ref> TransT <ref type="bibr" target="#b4">[5]</ref> STARK <ref type="bibr" target="#b49">[50]</ref> OSTrack D Results on ITB ITB <ref type="bibr" target="#b27">[28]</ref> benchmark is a newly collected benchmark with 9 representative scenarios and 180 diverse videos, which contains more informative tracking sequences. Tab. A7 shows the results of OSTrack compared with other SOTA tackers. Our OSTrack-384 achieves 64.8% in mIoU, surpassing the previous best tracker STARK <ref type="bibr" target="#b49">[50]</ref> by a large margin (7.2%).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E More Visualization</head><p>We first provide more visualization results for attention weights of the search region corresponding to the center part of the template (which can be seen as the target) in <ref type="figure" target="#fig_1">Fig. A2</ref>. The results show that the model attends to the foreground objects at an early stage (see "Layer 4" in <ref type="figure" target="#fig_1">Fig. A2</ref>) and finally shows great discriminative power between the target and distractors (see "Layer 12" in <ref type="figure" target="#fig_1">Fig. A2</ref>). These phenomenons demonstrate that the proposed OSTrack can extract target-oriented features with strong target-distractor discriminability. In <ref type="figure" target="#fig_2">Fig. A3</ref>, more visualization results of the early candidate elimination module are presented. The results validate that the proposed method can effectively identify and discard background regions under various target categories and challenge scenarios (e.g., target deformation, occlusion, motion blur, etc.).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input</head><p>Layer 1 Layer 4 Layer 7 Layer 10 Layer 12 <ref type="figure" target="#fig_1">Fig. A2</ref>: Extended visualization for attention weights of the search region corresponding to the center part of the template after different ViT layers, the green rectangles indicate target objects. The results show that our one-stream framework is able to distinguish between targets and distractors and progressively focus on targets.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>A comparison of AO and speed of state-of-the-art trackers on GOT-10k under one-shot setting. Our OSTrack-384 sets a new SOTA of 73.7% AO on GOT-10k, showing impressive one-shot tracking performance. OSTrack-256 runs at 105.4 FPS while still outperforming all previous trackers. Speed on GOT-10k (One-shot Setting)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>Three different taxonomies of tracking pipeline. The height of each rectangular represents the relative model size.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 :</head><label>3</label><figDesc>(a) The overall framework of the proposed one-stream framework. The template and search region are split, flattened, and linear projected. Image embeddings are then concatenated and fed into Transformer encoder layers for joint feature extraction and relation modeling. (b)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 :</head><label>4</label><figDesc>Visualization of the attention weights of search region corresponding to the center part of template after different ViT layers, the green rectangles indicate target objects. It can be seen as an estimate of the similarity between the target and each position of the search region.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 :</head><label>5</label><figDesc>Visualization of the progressive early candidate elimination process. The main body of "Input" is the search region image, and the upper left corner shows the corresponding template image. The Green rectangles indicate target objects and the masked regions represent the discarded candidates. The results show that our method can gradually identify and discard the candidates belonging to the background regions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>-</head><label></label><figDesc>OSTrack-256. Template: 128?128 pixels; Search region: 256?256 pixels. -OSTrack-384. Template: 192?192 pixels; Search region: 384?384 pixels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. A1 :</head><label>A1</label><figDesc>(a) Our proposed one-stream framework without the early candidate elimination module, which combines feature extraction and relation modeling modules. (b) The aligned two-stream tracking framework, which extracts features of the template and search region separately and then models the feature relation with extra Transformer encoder layers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. A3 :</head><label>A3</label><figDesc>Extended visualization results of the progressive candidate elimination process. The main body of "Input" is the search region image, and the upper left corner shows the corresponding template image. The Green rectangles indicate target objects and the masked regions represent the discarded tokens. Our early candidate elimination module can effectively deal with different tracking targets and scenarios.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>(b) where the relation modeling module is relatively heavy</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Relation Modeling Relation Modeling</cell><cell>Joint Joint</cell></row><row><cell cols="2">Relation Modeling Relation Modeling</cell><cell></cell><cell></cell><cell>Feature Extraction Feature Extraction</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>&amp; &amp;</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Relation Modeling Relation Modeling</cell></row><row><cell>Template Template</cell><cell>Search Search</cell><cell>Template Template</cell><cell>Search Search</cell></row><row><cell>Feature Feature</cell><cell>Feature Feature</cell><cell>Feature Feature</cell><cell>Feature Feature</cell></row><row><cell>Extraction Extraction</cell><cell>Extraction Extraction</cell><cell>Extraction Extraction</cell><cell>Extraction Extraction</cell></row><row><cell cols="2">Template Search Region Template Search Region</cell><cell cols="2">Template Search Region Template Search Region</cell><cell>Template Search Region Template Search Region</cell></row><row><cell cols="2">(a) Two-stream two-stage</cell><cell cols="2">(b) Two-stream two-stage</cell><cell>(c) One-stream one-stage</cell></row><row><cell cols="2">with light relation modeling</cell><cell cols="2">with heavy relation modeling</cell><cell>without extra relation modeling</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table><row><cell></cell><cell>SiamFC</cell><cell>RT-MDNet</cell><cell>ECO</cell><cell>Ocean</cell><cell>ATOM</cell><cell>DiMP50</cell><cell>STMTrack</cell><cell>TransT</cell><cell>STARK</cell><cell>OSTrack</cell><cell>OSTrack</cell></row><row><cell></cell><cell>[1]</cell><cell>[21]</cell><cell>[8]</cell><cell>[55]</cell><cell>[7]</cell><cell>[2]</cell><cell>[15]</cell><cell>[5]</cell><cell>[50]</cell><cell>-256</cell><cell>-384</cell></row><row><cell>NFS</cell><cell>37.7</cell><cell>43.3</cell><cell cols="3">52.2 49.4 58.3</cell><cell>61.8</cell><cell>-</cell><cell>65.3</cell><cell>66.2</cell><cell>64.7</cell><cell>66.5</cell></row><row><cell cols="2">UAV123 46.8</cell><cell>52.8</cell><cell cols="3">53.5 57.4 63.2</cell><cell>64.3</cell><cell>64.7</cell><cell>68.1</cell><cell>68.2</cell><cell>68.3</cell><cell>70.7</cell></row><row><cell cols="2">TNL2K 29.5</cell><cell>-</cell><cell cols="3">32.6 38.4 40.1</cell><cell>44.7</cell><cell>-</cell><cell>50.7</cell><cell>-</cell><cell>54.3</cell><cell>55.9</cell></row></table><note>Comparison with state-of-the-arts on three benchmarks: NFS [22], UAV123 [36] and TNL2K [49]. AUC(%) scores are reported. The best two results are shown in red and blue fonts.the official evaluation server. As reported in Tab. 1, OSTrack-384 and OSTrack- 256 outperform SwinTrack-B [30] by 1.6% and 4.3% in AO. The SR 0.75 score of OSTrack-384 reaches 70.8%, outperforming SwinTrack-B by</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>The effect of our proposed early candidate elimination module on the inference speed, MACs and tracking performance on LaSOT, GOT-10k and Track-ingNet benchmarks, and w/o and w/ denote the models with or without early candidate elimination module separately.TrackingNet. The TrackingNet<ref type="bibr" target="#b36">[37]</ref> benchmark contains 511 sequences for testing, which covers diverse target classes. Tab. 1 shows that OSTrack-256 and OSTrack-384 surpass SwinTrack-B [30] by 0.6% and 1.4% in AUC separately. Moreover, both models are faster than SwinTrack-B. LaSOT ext . LaSOT ext [13] is a recently released extension of LaSOT, which consists of 150 extra videos from 15 object classes. Tab 1 presents the results. Previous SOTA tracker KeepTrack [35] designs a complex association network and runs at 18.3 FPS. In contrast, our simple one-stream tracker OSTrack-256 shows slightly lower performance but runs at 105.4 FPS. OSTrack-384 sets a new state-of-the-art AUC score of 50.5% while runs in 58.1 FPS, which is 2.3% higher in AUC score and 3x faster in speed.</figDesc><table><row><cell>Input</cell><cell></cell><cell>FPS</cell><cell cols="2">MACs (G)</cell><cell cols="6">LaSOT AUC (%) TrackingNet AUC (%) GOT-10k  *  AO (%)</cell></row><row><cell>Resolution</cell><cell>w/o</cell><cell>w/</cell><cell>w/o</cell><cell>w/</cell><cell>w/o</cell><cell>w/</cell><cell>w/o</cell><cell>w/</cell><cell>w/o</cell><cell>w/</cell></row><row><cell cols="8">256x256 93.1 105.4(+13.2%) 29.0 21.5(-25.9%) 68.7 69.1(+0.4) 82.9</cell><cell>83.1(+0.2)</cell><cell cols="2">71.0 71.0(+0.0)</cell></row><row><cell cols="8">384x384 41.4 58.1(+40.3%) 65.3 48.3(-26.0%) 71.0 71.1(+0.1) 83.5</cell><cell>83.9(+0.4)</cell><cell cols="2">73.5 73.7(+0.2)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>The effect of different pre-training methods. All the models are trained without the early candidate elimination module.</figDesc><table><row><cell>Trackers</cell><cell cols="3">LaSOT Success PNorm P Success PNorm P AO SR0.5 SR0.75 TrackingNet GOT-10k</cell></row><row><cell cols="2">No pre-training 60.4</cell><cell>70.0 62.8 77.5</cell><cell>83.0 73.8 62.7 72.8 53.7</cell></row><row><cell>ImageNet-1k</cell><cell>66.1</cell><cell>75.8 70.6 82.0</cell><cell>86.7 80.1 69.7 79.0 65.6</cell></row><row><cell cols="2">ImageNet-21k 66.9</cell><cell>76.3 71.2 82.4</cell><cell>86.9 80.1 70.2 80.7 65.4</cell></row><row><cell>MAE</cell><cell cols="3">68.7 78.1 74.6 82.9 87.5 81.6 73.6 83.0 71.7</cell></row><row><cell cols="4">Different Pre-training Methods. While previous Transformer fusion track-</cell></row><row><cell>ers</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table A1 :</head><label>A1</label><figDesc>Ablation study on different choices of template tokens used to identify candidates belonging to background.</figDesc><table><row><cell>Template Token Selection</cell><cell cols="3">LaSOT Success PNorm P Success PNorm P AO SR0.5 SR0.75 TrackingNet GOT-10k</cell></row><row><cell>No Early Candidate Elimination</cell><cell>68.7</cell><cell>78.1 74.6 82.9</cell><cell>87.5 81.6 73.6 83.0 71.7</cell></row><row><cell>All Template Tokens</cell><cell>68.1</cell><cell>77.4 73.5 82.8</cell><cell>87.5 81.6 72.9 82.3 70.1</cell></row><row><cell cols="2">All Template Tokens within GT Box 68.5</cell><cell cols="2">78.1 74.2 83.1 87.8 81.7 73.6 83.4 72.0</cell></row><row><cell>Center 4x4 Template Tokens</cell><cell>68.3</cell><cell>77.6 73.9 82.9</cell><cell>87.7 82.0 73.5 83.0 71.5</cell></row><row><cell>Center Template Token</cell><cell cols="3">69.1 78.7 75.2 83.1 87.8 82.0 73.6 82.8 71.4</cell></row><row><cell cols="2">B More Ablation Studies</cell><cell></cell><cell></cell></row><row><cell cols="4">B.1 The Effect of Different Template Token Choices.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table A2 :</head><label>A2</label><figDesc>The effect of adding additional identity embeddings to the template and search region embeddings and adding relative positional embeddings to the OSTrack-256 (without the early candidate elimination module). The results on LaSOT<ref type="bibr" target="#b13">[14]</ref>, TrackingNet<ref type="bibr" target="#b36">[37]</ref> and GOT10k<ref type="bibr" target="#b19">[20]</ref> benchmarks are presented.</figDesc><table><row><cell></cell><cell></cell><cell>LaSOT</cell><cell>TrackingNet</cell><cell>GOT-10k</cell></row><row><cell></cell><cell cols="3">Success PNorm P Success PNorm P AO SR0.5 SR0.75</cell></row><row><cell>Ours</cell><cell cols="3">68.7 78.1 74.6 82.9</cell><cell>87.5 81.6 73.6 83.0 71.7</cell></row><row><cell>+ Identity Embeddings</cell><cell>68.0</cell><cell cols="2">77.3 73.6 83.3 88.0 82.2 73.6 82.9 71.7</cell></row><row><cell cols="2">+ Relative Positional Embeddings 68.5</cell><cell cols="2">77.8 74.1 83.2</cell><cell>87.8 82.0 73.7 83.3 71.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table A3 :</head><label>A3</label><figDesc>Add additional relation modeling module to our OSTrack-256 (without the early candidate elimination module).</figDesc><table><row><cell></cell><cell></cell><cell>LaSOT</cell><cell>TrackingNet</cell><cell>GOT-10k</cell></row><row><cell></cell><cell cols="3">Success PNorm P Success PNorm P AO SR0.5 SR0.75</cell></row><row><cell>Ours</cell><cell cols="3">68.7 78.1 74.6 82.9 87.5 81.6 73.6 83.0 71.7</cell></row><row><cell cols="2">+ Relation Modeling 68.5</cell><cell cols="2">78.0 74.1 82.9</cell><cell>87.4 81.5 72.7 82.2 70.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table A4 :</head><label>A4</label><figDesc>Ablation studies on the number of encoder layers used for relation modeling.</figDesc><table><row><cell></cell><cell></cell><cell>LaSOT</cell><cell cols="2">TrackingNet</cell><cell>GOT-10k</cell></row><row><cell></cell><cell cols="4">Success PNorm P Success PNorm P AO SR0.5 SR0.75</cell></row><row><cell cols="4">12 (Ours) 68.7 78.1 74.6 82.9</cell><cell>87.5 81.6 73.6 83.0 71.7</cell></row><row><cell>6</cell><cell>67.9</cell><cell cols="3">77.3 73.6 83.0 87.5 81.5 73.3 82.9 71.4</cell></row><row><cell>3</cell><cell>67.8</cell><cell cols="2">77.0 73.5 82.7</cell><cell>87.4 80.7 72.8 82.5 70.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table A5 :</head><label>A5</label><figDesc>Different keeping ratio ? used in the early candidate elimination module (? = 1 means the early candidate elimination module is not adopted).</figDesc><table><row><cell>Keeping Ratio</cell><cell cols="3">LaSOT Success PNorm P Success PNorm P AO SR0.5 SR0.75 TrackingNet GOT-10k</cell><cell>MACs (G)</cell></row><row><cell>1</cell><cell>68.7</cell><cell>78.1 74.6 82.9</cell><cell>87.5 81.6 73.6 83.0 71.7</cell><cell>29.0</cell></row><row><cell>0.9</cell><cell>68.7</cell><cell>78.2 74.6 83.2</cell><cell>87.8 82.0 74.1 83.6 71.8</cell><cell>26.2</cell></row><row><cell>0.8</cell><cell>68.9</cell><cell cols="2">78.4 74.9 83.3 88.0 82.3 73.4 82.7 71.4</cell><cell>23.6</cell></row><row><cell>0.7</cell><cell cols="2">69.1 78.7 75.2 83.1</cell><cell>87.8 82.0 73.6 82.8 71.4</cell><cell>21.5</cell></row><row><cell>0.6</cell><cell>68.4</cell><cell>77.9 74.3 83.1</cell><cell>87.6 81.8 73.5 82.9 71.7</cell><cell>19.6</cell></row><row><cell>0.5</cell><cell>67.8</cell><cell>77.2 73.4 82.7</cell><cell>87.4 81.3 71.8 81.3 68.4</cell><cell>18.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table A6 :</head><label>A6</label><figDesc>Comparison on VOT2020 benchmark. The left part of the trackers adopts an online template update mechanism, while the right part of the trackers does not. The best two results are shown in red and blue fonts.</figDesc><table><row><cell></cell><cell>Ocean [55]</cell><cell>ATOM [7]</cell><cell>D3S [34]</cell><cell>AlphaRef [51]</cell><cell>STARK-ST50 [50]</cell><cell>STARK -ST101 [50]</cell><cell>SiamMask [48]</cell><cell>STARK-S50 [50]</cell><cell>OSTrack-256 OSTrack-384</cell></row><row><cell>EAO (?)</cell><cell cols="4">0.43 0.271 0.439 0.482</cell><cell>0.505</cell><cell>0.497</cell><cell>0.321</cell><cell>0.462</cell><cell>0.518</cell><cell>0.524</cell></row><row><cell cols="5">Accuracy (?) 0.693 0.462 0.699 0.754</cell><cell>0.759</cell><cell>0.763</cell><cell>0.624</cell><cell>0.761</cell><cell>0.762</cell><cell>0.767</cell></row><row><cell cols="5">Robustness (?) 0.754 0.734 0.769 0.777</cell><cell>0.817</cell><cell>0.789</cell><cell>0.648</cell><cell>0.749</cell><cell>0.814</cell><cell>0.816</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">We add the symbol * to GOT-10k if the corresponding models are trained following the one-shot protocol, otherwise they are trained with all training data.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments. This work is partially supported by Natural Science Foundation of China (NSFC): 61976203 and 61876171. Thanks Zhipeng Zhang for his helpful suggestions.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A More Implementation Details</head><p>Training Details. In OSTrack-256, the input sizes of templates and search regions are 128 ? 128 pixels and 256 ? 256 pixels respectively, corresponding to 2 2 and 4 2 times of the target bounding box area. In OSTrack-384, the input sizes of templates and search regions are 192 ? 192 pixels and 384 ? 384 pixels, corresponding to 2 2 and 5 2 times of the target bounding box area. For the GOT-10k test benchmark <ref type="bibr" target="#b19">[20]</ref>, which requires training the models with only the training split of GOT-10k (one-shot setting), we set the total training epoch to 100 with 60k image pairs per epoch, and we decrease the learning rate by a factor of 10 after 80 epochs. The other settings are kept consistent with the models trained with all datasets.</p><p>Classification Loss. We adopt the weighted focal loss <ref type="bibr" target="#b24">[25]</ref> for classification. Specifically, for each ground truth target centerp and its corresponding lowresolution equivalentp = [p x ,p y ], the ground truth heatmap can be generated using a Gaussian kernel asP xy = exp ?</p><p>where ? is an object size-adaptive standard deviation <ref type="bibr" target="#b24">[25]</ref>. The Gaussian weighted focal loss can be formulated as:</p><p>where ? and ? are hyper-parameters and we set ? = 2 and ? = 4 as in <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b55">56]</ref>. Position Embeddings. The length of the position embeddings in the pretrained ViT is different from the length of the input template and search region embeddings. Therefore, the pre-trained positional embeddings are interpolated (2D bicubic interpolation is adopted) to the sizes of the template and search region embeddings separately, which are further added to the patch embeddings.</p><p>Model Details. In Sec. 4.3, we compare our OSTrack (without the early candidate elimination module) with aligned two-stream trackers (i.e., STARKaligned and SwinTrack-aligned), and we further present the detailed structures in this section. The proposed one-stream framework, as shown in <ref type="figure">Fig. A1(a)</ref>, combines feature extraction and relation modeling modules into a single ViT backbone. The aligned two-stream framework, as shown in <ref type="figure">Fig. A1(b)</ref>, first extracts features of the template and the search region separately with the same ViT backbone and then models the feature relation with several extra Transformer encoder layers. As presented in Sec. 4.3, this relation modeling module is instantiated with the encoder structure proposed in STARK <ref type="bibr" target="#b49">[50]</ref> (STARKaligned) and SwinTrack <ref type="bibr" target="#b29">[30]</ref> (SwinTrack-aligned) separately.</p><p>Discriminative Regions Visualization. We show the method used to obtain the visualization of the discriminative regions in <ref type="figure">Fig. 6</ref>. Zagoruyko et al . <ref type="bibr" target="#b52">[53]</ref> show that the importance of a hidden neuron activation can be indicated by its absolute value. In this work, we adopt a similar approach to obtain the</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Fullyconvolutional siamese networks for object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bertinetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Valmadre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ECCV</publisher>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Learning discriminative model prediction for tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bhat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">23</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Know your surroundings: Exploiting scene information for object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bhat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">23</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Visual object tracking using adaptive correlation filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Bolme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Beveridge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">A</forename><surname>Draper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">M</forename><surname>Lui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Transformer tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">23</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">High-performance long-term tracking with meta-updater</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Atom: Accurate tracking by overlap maximization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bhat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Felsberg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">23</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">ECO: Efficient convolution operators for tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bhat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Shahbaz Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Felsberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR. pp</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Probabilistic regression for visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">23</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">ImageNet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">20</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Houlsby</surname></persName>
		</author>
		<title level="m">An image is worth 16x16 words: Transformers for image recognition at scale. ICLR (2021) 2, 5</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Lasot: A high-quality large-scale single object tracking benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">129</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Lasot: A high-quality benchmark for large-scale single object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">21</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Stmtrack: Template-free visual tracking with space-time memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Graph attention tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">23</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.06377</idno>
		<title level="m">Masked autoencoders are scalable vision learners</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">High-speed tracking with kernelized correlation filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Caseiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Martins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Batista</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Got-10k: A large high-diversity benchmark for generic object tracking in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">21</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Real-time mdnet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Baek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ECCV</publisher>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Need for speed: A benchmark for higher frame rate object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kiani</forename><surname>Galoogahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fagg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lucey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">The eighth visual object tracking vot2020 challenge results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kristan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Leonardis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Felsberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pflugfelder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>K?m?r?inen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">?</forename><surname>Zajc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Luke?i?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Drbohlav</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">22</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Cornernet: Detecting objects as paired keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">SiamRPN++: Evolution of siamese visual tracking with very deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR. pp</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">23</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">High performance visual tracking with siamese region proposal network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR. pp</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.06467</idno>
		<title level="m">An informative tracking benchmark</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">23</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">EViT: Expediting vision transformers via token reorganizations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Xie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022" />
			<publisher>ICLR</publisher>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.00995</idno>
		<title level="m">Swintrack: A simple and strong baseline for transformer tracking</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">21</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Guo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ICLR</publisher>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">D3s-a discriminative single shot segmentation tracker</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lukezic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kristan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">22</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Learning target candidate association to keep track of what not to track</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Paudel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">A benchmark and simulator for uav tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Trackingnet: A largescale dataset and benchmark for object tracking in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bibi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Giancola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Alsubaihi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ECCV</publisher>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">21</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Learning multi-domain convolutional neural networks for visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Dynamicvit: Efficient vision transformers with dynamic token sparsification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Hsieh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page">10</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Generalized intersection over union: A metric and a loss for bounding box regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rezatofighi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Tsoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sadeghian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ridnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ben-Baruch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Noy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zelnik-Manor</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.10972</idno>
		<title level="m">ImageNet-21k pretraining for the masses</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">How to train your vit? data, augmentation, and regularization in vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wightman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.10270</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Training data-efficient image transformers &amp; distillation through attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>J?gou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS. pp</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="5998" to="6008" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Siam r-cnn: Visual tracking by re-detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Voigtlaender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luiten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Tracking by instance detection: A meta-learning approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Transformer meets tracker: Exploiting temporal context for robust visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CVPR</publisher>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">23</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Fast online object tracking and segmentation: A unifying approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bertinetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">22</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Towards more flexible and accurate object tracking with natural language: Algorithms and benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Learning spatio-temporal transformer for visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">23</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Alpha-refine: Boosting tracking performance by precise bounding box estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">22</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Adavit: Adaptive tokens for efficient vision transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vahdat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mallya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Molchanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.07658</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Paying more attention to attention: Improving the performance of convolutional neural networks via attention transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Learn to match: Automatic matching network design for visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Ocean: Object-aware anchor-free tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV. pp</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">23</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kr?henb?hl</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.07850</idno>
		<title level="m">Objects as points</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
