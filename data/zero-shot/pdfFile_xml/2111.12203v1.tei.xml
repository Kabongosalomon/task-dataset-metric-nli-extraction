<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">KUIELab-MDX-Net: A Two-Stream Neural Network for Music Demixing</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minseok</forename><surname>Kim</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Korea University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Woosung</forename><surname>Choi</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Queen Mary University of London</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaehwa</forename><surname>Chung</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Korea National Open University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daewon</forename><surname>Lee</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">Seokyeong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soonyoung</forename><surname>Jung</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Korea University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">KUIELab-MDX-Net: A Two-Stream Neural Network for Music Demixing</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>License Authors of papers retain copyright and release the work under a Creative Commons Attribution 4.0 International License (CC BY 4.0). In partnership with</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T21:40+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recently, many methods based on deep learning have been proposed for music source separation. Some state-of-the-art methods have shown that stacking many layers with many skip connections improve the SDR performance. Although such a deep and complex architecture shows outstanding performance, it usually requires numerous computing resources and time for training and evaluation. This paper proposes a two-stream neural network for music demixing, called KUIELab-MDX-Net, which shows a good balance of performance and required resources. The proposed model has a time-frequency branch and a time-domain branch, where each branch separates stems, respectively. It blends results from two streams to generate the final estimation. KUIELab-MDX-Net took second place on leaderboard A and third place on leaderboard B in the Music Demixing Challenge at ISMIR 2021. This paper also summarizes experimental results on another benchmark, MUSDB18. Our source code is available online 1</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Recently, many methods have been proposed for music source separation. Notably, deep learning approaches <ref type="bibr" target="#b3">D?fossez et al. (2021)</ref> have become mainstream because of their excellent performance. Some state-of-the-art methods <ref type="bibr" target="#b2">(Choi et al., 2020;</ref><ref type="bibr" target="#b12">Takahashi et al., 2018;</ref> have shown that stacking many layers with many skip connections improve the SDR performance.</p><p>Although a deep and complex architecture shows outstanding performance, it usually requires numerous computing resources and time for training and evaluation. Such disadvantages make them not affordable in a restricted environment where limited resources are provided. For example, some deep models such as LaSAFT-Net  exceed the time limit of the Music Demixing Challenge (MDX) at ISMIR 2021  even if they are the current state of the art on the MUSDB18 <ref type="bibr" target="#b9">(Rafii et al., 2017b)</ref> benchmark. This paper presents a source separation model named KUIELab-MDX-Net. We empirically found a good balance of performance and required resources to design KUIElab-MDX-Net. For example, we replaced channel-wise concatenation operations with simple element-wise multiplications for each skip connection between encoder and decoder (i.e., for each U-connection in U-Net). In our prior experiments, it reduced parameters with negligible performance degradation.</p><p>Also, we removed the other skip connections, especially, skip connections used in dense blocks <ref type="bibr" target="#b2">(Choi et al., 2020;</ref><ref type="bibr" target="#b12">Takahashi et al., 2018;</ref>. We observed that stacked convolutional networks without dense connections followed by Time-Distributed Fully connected layers (TDF) <ref type="bibr" target="#b2">(Choi et al., 2020)</ref> could perform comparably to dense blocks without TDFs. TDF, proposed in <ref type="bibr" target="#b2">(Choi et al., 2020)</ref>, is a sequence of linear layers. It is applied to a given input in the frequency domain to capture frequency-to-frequency dependencies of the target source. Since a single TDF block has the whole receptive field in terms of frequency, injecting TDF blocks into a conventional U-Net <ref type="bibr" target="#b10">(Ronneberger et al., 2015)</ref> improves the SDR performance on singing voice separation even with a shallower structure.</p><p>By introducing such tricks, we found a computationally efficient and effective model design. As a result, the proposed architecture has a time-frequency branch and a time-domain branch, where each branch separates stems, respectively. It blends results from two streams to generate the final estimation. KUIELab-MDX-Net took second place on leaderboard A and third place on leaderboard B in the Music Demixing Challenge at ISMIR 2021. This paper also summarizes experimental results on another benchmark, MUSDB18.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Background</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Frequency Transformation for Source Separation</head><p>Some source separation methods <ref type="bibr" target="#b0">(Choi, 2021;</ref><ref type="bibr" target="#b2">Choi et al., 2020;</ref><ref type="bibr" target="#b16">Yin et al., 2020</ref>) have adopted Frequency Transformation (FT) to capture frequency-to-frequency dependencies of the target source. Both designed their FT blocks with fully connected layers, also known as linear layers. For example, <ref type="bibr" target="#b2">(Choi et al., 2020)</ref> proposed Time-Distributed Fully connected layers (TDF) to capture frequency patterns observed in spectrograms of a singing voice. A TDF block is a sequence of two linear layers. It is applied to a given input in the frequency domain. The first layer downsamples the features to R F/bn , where we denote the number of frequency bins in a given spectrogram feature by F and the bottleneck factor that controls the degree of downsampling by bn.</p><p>TFC-TDF-U-Net v1 <ref type="bibr" target="#b2">(Choi et al., 2020)</ref> proposed the original TFC-TDF-U-Net for singing voice separation. We call this architecture TFC-TDF-U-Net v1 for the rest of this paper. It adopted a Time-Frequency Convolutions followed by a TDF (TFC-TDF) block as a fundamental building block. By replacing fully connected 2-D convolutional building blocks, conventionally used in U-Net <ref type="bibr" target="#b10">(Ronneberger et al., 2015)</ref> with TFC-TDF blocks, it showed a promising performance on singing voice separation tasks of the MUSDB18 <ref type="bibr" target="#b9">(Rafii et al., 2017b)</ref> dataset. Also, injecting TDF blocks can enhance separation quality for the other tasks of MUSDB18, as shown in <ref type="bibr" target="#b0">(Choi, 2021)</ref>. <ref type="bibr" target="#b0">(Choi, 2021)</ref> presented how adding TDF blocks improves separation quality by visualizing trained weight matrixes of single-layered TDF blocks (they additionally trained U-Nets with single-layered TDF blocks for weight visualization). As shown in <ref type="figure" target="#fig_0">Figure 1</ref>, each matrix is trained to analyze timbre features uniquely observed in its instrument by capturing harmonic patterns (i.e., y = ? ? x). It is also observable that the TDF blocks still performs well on each scale. We summarized TFC-TDF-U-Net v1's performance reported in <ref type="bibr" target="#b0">(Choi, 2021)</ref> in the experiment section. Since the original TFC-TDF-U-Net v1 is computationally heavy to be evaluated within the time limit of the MDX challenge, we could not submit this, although its performance was promising on the MUSDB18 benchmark. To make an affordable model for the MDX challenge, we empirically found a good balance of performance and required resources. <ref type="figure" target="#fig_1">Figure 2</ref>, KUIELab-MDX-Net consists of six networks, all trained separately. 2 depicts the overall flow at inference time: the four U-Net-based separation models (TFC-TDF-U-Net v2) first estimate each source independently, then the Mixer model takes these estimated sources (+ mixture) and outputs enhanced estimated sources. Also, we extract sources with another network based on a time-domain approach, as shown on the right side of <ref type="figure" target="#fig_1">Figure 2</ref>. We used pretrained Demucs <ref type="bibr" target="#b3">(D?fossez et al., 2021)</ref> without fine-tuning. Finally, it takes the weighted average for each estimated source, also known as blending <ref type="bibr" target="#b15">(Uhlich et al., 2017)</ref>. On top of these architectural changes, we also use a different loss function (time-domain l 1 loss) as well as source-specific data preprocessing. As shown in <ref type="figure" target="#fig_3">Figure 3</ref>, high frequencies above the target source's expected frequency range were cut off from the mixture spectrogram. This way, we can increase n_fft while using the same input spectrogram size (which we needed to constrain for the separation time limit), and using a larger n_fft usually leads to better SDR. It is also why we did not use a multi-target model (a single model that is trained to estimate all four sources), where we could not use source-specific frequency cutting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method: KUIELab-MDX-Net</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>As in</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TFC-TDF-U-Net v2</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mixer</head><p>Although training one separation model for each source can benefit from source-specific preprocessing and model configurations, these models lack the knowledge that they are separating using the same mixture. We thought an additional network that could exploit this knowledge (which we call the Mixer) could further enhance the independently estimated sources. For example, estimated 'vocals' often have drum snare noises left. The Mixer can learn to remove sounds from 'vocals' that are also present in the estimated 'drums' or vice versa.</p><p>We only tried very shallow models (such as a single convolution layer) for the Mixer during the MDX Challenge due to the time limit. We look forward to trying more complex models in the future since even a single 1 ? 1 convolution layer was enough to make some improvement on total SDR (Section "Performance on the MUSDB18 Benchmark").</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experimental Results</head><p>This section describes the model configurations, STFT parameters, training procedure, and evaluation results on the MUSDB18 benchmark. For training, we used the MUSDB-HQ dataset with default 86/14 train and validation splits.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Configurations and Training</head><p>We present a comparison between configurations of TFC-TDF-U-Net v1 and v2 as follows. This applies to all models regardless of the target source (we did not explore different model configurations for each source). In short, v2 is a more shallow but wider model than v1. The number of intermediate channels is increased/decreased after down/upsampling layers with a linear factor of 32. Also, as mentioned in Section "TFC-TDF-U-Net v2," we used different n_fft for each source: <ref type="bibr">(6144,</ref><ref type="bibr">4096,</ref><ref type="bibr">16384,</ref><ref type="bibr">8192)</ref> for <ref type="bibr">(vocals, drums, bass, other)</ref>.</p><p>All five models (four separation models + Mixer) were optimized with RMSProp with no momentum. We used random chunking and mixing instruments from different songs for data augmentation <ref type="bibr" target="#b15">(Uhlich et al., 2017)</ref>. We also used data augmentation based on pitch shift and time stretch <ref type="bibr" target="#b3">(D?fossez et al., 2021)</ref>. The overall training procedure can be summarized into two steps:</p><p>1. Train single-target separation models (TFC-TDF-U-Net v2) for each source. 2. Train the Mixer while freezing the pretrained weights of the separation models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Performance on the MUSDB18 Benchmark</head><p>We compare our models with current state-of-the-art models on the MUSDB18 benchmark using the SiSEC2018 version of the SDR metric (BSS Eval v4 framewise multi-channel SDR). We report the median SDR over all 50 songs in the MUSDB18 test set. Only models for Leaderboard A were evaluated since our submissions for Leaderboard B uses the MUSDB18 test set as part of the training data.</p><p>We summarize the MUSDB18 benchmark performance of KUIELab-MDX-Net. We compare it to recent state-of-the-art models: TFC-TDF-U-Net v1 <ref type="bibr" target="#b2">(Choi et al., 2020)</ref>, X-UMX <ref type="bibr" target="#b11">(Sawata et al., 2021)</ref>, Demucs <ref type="bibr" target="#b3">(D?fossez et al., 2021)</ref>, D3Net , ResUNet-Decouple+ <ref type="bibr" target="#b4">(Kong et al., 2021)</ref>. We also include our baselines to validate our architectural design. Even though our models were downsized for the MDX Challenge, we can see that it gives superior performance over the state-of-the-art models and achieves the best SDR for every instrument except 'bass.' Also, it is notable that TFC-TDF-U-Net v2 with Mixer (i.e., v2 + Mixer) outperforms the existing methods except for 'vocals' even without blending with Demucs.</p><p>vocals drums bass other TFC-TDF-U-Net v1 <ref type="bibr" target="#b2">(Choi et al., 2020)</ref> 7.98 6.11 5.94 5.02 X-UMX <ref type="bibr" target="#b11">(Sawata et al., 2021)</ref> 6.61 6.47 5.43 4.64 Demucs <ref type="bibr" target="#b3">(D?fossez et al., 2021)</ref> 6.84 6.86 7.01 4.42 vocals drums bass other D3Net  7.24 7.01 5.25 4.53 ResUNetDecouple+ <ref type="bibr" target="#b4">(Kong et al., 2021)</ref>  We also compare three winning models' performance  on the MUSDB18 benchmark as follows. It should be noted that we only reported SDRs evaluated on MUSDB18 <ref type="bibr" target="#b7">(Rafii et al., 2017a)</ref>, not MUSDB-HQ <ref type="bibr" target="#b8">(Rafii et al., 2019)</ref>. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Weight matrixes visualization of single-layered TDF blocks</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>The Overall Architecture of KUIELab-MDX-Net</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure</head><label></label><figDesc>KUIELab-MDX-Net: A Two-Stream Neural Network for Music Demixing. Proceedings of the MDX Workshop, 2021.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>The architecture of TFC-TDF-U-Net v2 The following changes were made to the original TFC-TDF-U-Net architecture: -For "U" connections, we used multiplication instead of concatenation. -Other than U connections, all skip connections were removed. -In TFC-TDF-U-Net v1, the number of intermediate channels are not changed after down/upsampling layers. For v2, they are increased when downsampling and decreased when upsampling.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc># blocks # convs per block bn # freq bins # STFT frames hop</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">KUIELab-MDX-Net: A Two-Stream Neural Network for Music Demixing. Proceedings of the MDX Workshop, 2021.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Deep learning-based latent source analysis for source-aware audio manipulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Choi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note>PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Lasaft: Latent source attentive frequency transformation for conditioned source separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jung</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICASSP39728.2021.9413896</idno>
		<ptr target="https://doi.org/10.1109/ICASSP39728.2021.9413896" />
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<meeting>IEEE International Conference on Acoustics, Speech and Signal essing (ICASSP)</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="171" to="175" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Investigating u-nets with various intermediate blocks for spectrogram-based singing voice separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Society for Music Information Retrieval Conference (ISMIR)</title>
		<meeting>International Society for Music Information Retrieval Conference (ISMIR)</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Music source separation in the waveform domain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>D?fossez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bach</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1911.13254" />
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Decoupling magnitude and phase estimation with deep ResUNet for music source separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<idno>abs/2109.05418</idno>
		<ptr target="https://arxiv.org/abs/2109.05418" />
		<imprint>
			<date type="published" when="2021" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Dilated convolution with dilated GRU for music source separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-H</forename><surname>Yang</surname></persName>
		</author>
		<idno type="DOI">10.24963/ijcai.2019/655</idno>
		<ptr target="https://doi.org/10.24963/ijcai.2019/655" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence, IJCAI-19</title>
		<meeting>the Twenty-Eighth International Joint Conference on Artificial Intelligence, IJCAI-19</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4718" to="4724" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mitsufuji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Fabbro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Uhlich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F.-R</forename><surname>St?ter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.13559</idno>
		<title level="m">Music demixing challenge at ISMIR 2021</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv Preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">The MUSDB18 corpus for music separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Rafii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Liutkus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F.-R</forename><surname>St?ter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">I</forename><surname>Mimilakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bittner</surname></persName>
		</author>
		<idno type="DOI">10.5281/zenodo.1117372</idno>
		<ptr target="https://doi.org/10.5281/zenodo.1117372" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">MUSDB18-HQan uncompressed version of MUSDB18</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-Mdx-Net ;</forename><surname>Kuielab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Rafii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Liutkus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F.-R</forename><surname>St?ter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">I</forename><surname>Mimilakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bittner</surname></persName>
		</author>
		<idno type="DOI">10.5281/zenodo.3338373</idno>
		<ptr target="https://doi.org/10.5281/zenodo.3338373" />
	</analytic>
	<monogr>
		<title level="m">A Two-Stream Neural Network for Music Demixing. Proceedings of the MDX Workshop</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">MUSDB18-a corpus for music separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Rafii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Liutkus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F.-R</forename><surname>St?ter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">I</forename><surname>Mimilakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bittner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<title level="m">U-net: Convolutional networks for biomedical image segmentation. International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">All for one and one for all: Improving music separation by bridging networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sawata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Uhlich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Takahashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mitsufuji</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICASSP39728.2021.9414044</idno>
		<ptr target="https://doi.org/10.1109/ICASSP39728.2021.9414044" />
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<meeting>IEEE International Conference on Acoustics, Speech and Signal essing (ICASSP)</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="51" to="55" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Mmdenselstm: An efficient combination of convolutional and recurrent neural networks for audio source separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Takahashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Goswami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mitsufuji</surname></persName>
		</author>
		<idno type="DOI">10.1109/IWAENC.2018.8521383</idno>
		<ptr target="https://doi.org/10.1109/IWAENC.2018.8521383" />
	</analytic>
	<monogr>
		<title level="m">16th International Workshop on Acoustic Signal Enhancement, IWAENC 2018</title>
		<meeting><address><addrLine>Tokyo, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-09-17" />
			<biblScope unit="page" from="106" to="110" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Densely connected multi-dilated convolutional networks for dense prediction tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Takahashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mitsufuji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="993" to="1002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Multi-scale multi-band densenets for audio source separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Takahashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mitsufuji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Improving music source separation based on deep neural networks through data augmentation and network blending</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Uhlich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Porcu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Giron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enenkl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kemp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Takahashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mitsufuji</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICASSP.2017.7952158</idno>
		<ptr target="https://doi.org/10.1109/ICASSP.2017.7952158" />
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<meeting>IEEE International Conference on Acoustics, Speech and Signal essing (ICASSP)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="261" to="265" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">PHASEN: A phase-and-harmonics-aware speech enhancement network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="9458" to="9465" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-Mdx-Net</forename><surname>Kuielab</surname></persName>
		</author>
		<title level="m">A Two-Stream Neural Network for Music Demixing. Proceedings of the MDX Workshop</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
