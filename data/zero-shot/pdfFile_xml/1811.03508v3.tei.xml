<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A simple yet effective baseline for non-attributed graph classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-05-18">18 May 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Cai</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusu</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">The Ohio State University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">The Ohio State University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">A simple yet effective baseline for non-attributed graph classification</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-05-18">18 May 2022</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T15:11+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Graphs are complex objects that do not lend themselves easily to typical learning tasks. Recently, a range of approaches based on graph kernels or graph neural networks have been developed for graph classification and for representation learning on graphs in general. As the developed methodologies become more sophisticated, it is important to understand which components of the increasingly complex methods are necessary or most effective.</p><p>As a first step, we develop a simple yet meaningful graph representation, and explore its effectiveness in graph classification. We test our baseline representation for the graph classification task on a range of graph datasets. Interestingly, this simple representation achieves similar performance as the state-ofthe-art graph kernels and graph neural networks for non-attributed graph classification. Its performance on classifying attributed graphs is slightly weaker as it does not incorporate attributes. However, given its simplicity and efficiency, we believe that it still serves as an effective baseline for attributed graph classification. Our graph representation is efficient (linear-time) to compute. We also provide a simple connection with the graph neural networks.</p><p>Note that these observations are only for the task of graph classification while existing methods are often designed for a broader scope including node embedding and link prediction. The results are also likely biased due to the limited amount of benchmark datasets available. Nevertheless, the good performance of our simple baseline calls for the development of new, more comprehensive benchmark datasets so as to better evaluate and analyze different graph learning methods. Furthermore, given the computational efficiency of our graph summary, we believe that it is a good candidate as a baseline method for future graph classification (or even other graph learning) studies.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Graph-type data are ubiquitous across many scientific fields. For example, social networks, molecular graphs, biological protein-protein interaction networks, knowledge graphs, and recommender systems, are all graph objects naturally arise in a range of application domains, where edges can describe the interaction and relationship between individual entities.</p><p>Recently, there has been a surge of approaches that aim to learn representations that encode structure information about the graph. On the high level, the methods can be categorized to be graph-kernel based, or graph neural network based. The problem remains challenging. For example, the design of graph kernel and graph neural networks are often influenced by the (sub-)graph isomorphism problem, where one aims to make sure that features of non-isomorphic graphs are likely different. On the one hand, graph isomorphism is computationally hard 1 . On the other hand, there is no guarantee that graph kernels/neural networks with the strongest expressive power in terms of differentiating non-isomorphic graphs will generalize the best. In general, one also has to strike a balance between the flexibility and (over-)expressive power of a graph representation framework. In addition, the method should also be computationally efficient, scalable to large datasets. Several existing graph kernels rely on certain spectral structures of the adjacency matrix of the graph, and can be computationally expensive.</p><p>Nevertheless, much progress has been made in designing more sensitive graph kernels or more expressive graph neural networks. However, as the methodologies become more and more sophisticated, it becomes harder to understand which components of these methods are more crucial. For example, graph neural network (GNN) based approaches are flexible, and can scale to large datasets. They achieve state-ofthe-art performance in several tasks including node classification, link prediction, and graph classification. However, the architecture of graph neural networks mixes representation and optimization, making it hard to analyze its power and limitation rigorously.</p><p>In this paper, we are interested in evaluating existing approaches for graph classification tasks, in an attempt to gain a gradual understanding of their powers and limitations. In an effort to do so, we develop a simple graph representation based on local information for non-attributed graphs, which we refer to as LDP. One of our initial goals was to understand in which scenarios the simple summary shall fail. To this end, we collect all the graph datasets from existing literature on graph classification; see Section 7 for the description of the datasets we use.</p><p>Interestingly, this simple graph representation achieves similar performance as the state-of-the-art graph kernels and graph neural networks for non-attributed graph classification, and in fact, outperforms many existing more sophisticated representations. Its performance on classifying attributed graphs (mostly graphs representing biochemical compounds/molecules) is also competitive despite that it does not incorporate attributes. We report these results as we believe they make the following contributions:</p><p>? While our LDP graph representation is simple, it is intuitive and we show its competitive performance in graph classification for a range of graph datasets. This graph representation is computationally efficient (linear-time) to compute. We also provide a simple connection between our representation with the graph neural networks.</p><p>? We note that these observations are only for the task of graph classification; while existing methods (especially graph neural network based approaches) are often designed for a broader scope, including for node embedding and for link prediction purposes. The results are also likely biased due to the limited amount of benchmark datasets available, and thus do not form the basis to dismiss any existing graph classification methodology.</p><p>Nevertheless, the good performance of our simple graph representation raises concerns about the effectiveness of current benchmark datasets for evaluating different algorithms for non-attribute graph classification. It calls for the development of new, more comprehensive, benchmark datasets for the better evaluation of different graph learning methods as well as for more rigorous analysis of the power and limitation of graph representations.</p><p>? Furthermore, given the computational efficiency of our graph summary, we believe that it is a good candidate as a baseline method for future graph classification (or even other graph learning) studies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Graph Kernel</head><p>There are various graph kernels, many of which explore the R-convolutional framework <ref type="bibr" target="#b9">[Hau99]</ref>. The key idea is to decompose a whole graph into small substructures and build graph kernels based on the similarities defined for these components. Graph kernels following this line of work differ from each other in the way they decompose graphs. For example, graphlet kernels[SVP + 09] are based on small subgraphs up to a fixed size. Weisfeiler-Lehman graph kernels[SSL + 11] is based on subtree patterns. Shortest path kernel <ref type="bibr" target="#b2">[BK05]</ref> is derived by comparing the paths between graphs. Other graph kernels, such as <ref type="bibr" target="#b26">[VSKB10]</ref> and <ref type="bibr" target="#b8">[GFW03]</ref>, are developed by counting the number of common random walks on direct product graphs. However, all the above R-convolution based graph kernels suffer from a drawback. As pointed out in <ref type="bibr" target="#b31">[YV15b]</ref>, increasing the size of substructures will largely decrease the probability that two graphs contain similar substructures, which usually results in the "diagonal dominance issue" <ref type="bibr" target="#b13">[KGST03]</ref>. More recently, new methods have been proposed to compare graphs, which is done by quantifying the dissimilarity between the distributions of pairwise distances between nodes. [SCDG + 17] uses the shortest path distance, and[VZ17] uses the diffusion distance. In general, most graph kernels can handle label information, but there are a few exceptions[SVP + 09, VZ17] that purely use graph topology.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Graph Neural Network</head><p>Another way to tackle graph classification involves developing graph neural network (GNN). GNN broadly follows a recursive neighborhood aggregation(or message passing) scheme, where each node aggregates feature vectors of its neighbors to compute its new feature vector. Repeating the above procedure k times, a node is represented by its transformed feature vector. The representation of an entire graph can be obtained by pooling, for example, by summing the feature vectors over all nodes in the graph. Many GNN variants with different neighborhood aggregation and graph-level pooling schemes have been proposed [DBV16, DMI + 15, HYL17, LTBZ15, KMB + 16, KW16, VCC + 17, YYM + 18]. Empirically, these GNNs have achieved state-of-the-art performance in many tasks such as node classification, link prediction, and graph classification. However, the design of new GNNs is often based on empirical intuition, heuristics, and experimental trial-and-error. The theoretical understanding of the properties and limitations of GNNs is somewhat limited, although there exists very recent work starting to address this issue <ref type="bibr" target="#b29">[XHLJ18]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Baseline method</head><p>We remark that there has been a few works that draw attention to the potential problems of widely used graph datasets. Specifically, <ref type="bibr">[OAvL]</ref> observe that even by using simple graph features such as the number of nodes, they could achieve similar classification performance on common benchmark datasets compared to early graph kernels. Their results show that we cannot solely rely on these data sets to show the performance of a graph kernel. However, their work focuses on attributed graphs, while our work focuses on nonattributed graphs. Very recently, in an independent work by [WZSJ + 19], the authors hypothesize that the nonlinearity between GCN layers is not critical. They remove the non-linearities and develop a simple network called Simple Graph Convolution. This network works well on text classification, semi-supervised user geolocation, relation extract, zero-shot image classification, and graph classification. While our work shares some similarities with theirs, our paper was developed independently.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">A simple baseline for graph classification 3.1 Our model</head><p>We denote a graph as G(V, E) where V is the set of nodes and E is the set of edges in G. For each graph G we extract features for each node in the following way. For each node v ? G(V ), let DN (v) denote the multiset of the degree of all the neighboring nodes of v, i.e., DN (v) = {degree(u)|(u, v) ? E}. We take five node features, which are (degree(v), min(DN(v)), max(DN(v)),mean(DN(v)), std(DN(v))). In other words, each node feature summarizes the degree information of this node and its 1-neighborhood. We aggregate features over different nodes in the same graph by performing either a histogram or an empirical distribution function(edf) operation, i.e, mapping all node feature into a histogram or an empirical distribution.</p><p>We then repeat the same procedure for all five node features and concatenate all the feature vectors as the input feature for SVM classifier. For a fair comparison, we follow the convention in the graph kernel literature. We perform 10-fold cross validation ten times and report the average accuracy. For simplicity, in the rest of the paper, we denote our baseline as Local Degree Profile(LDP).</p><p>Computational complexity: In feature extraction, we only need to count the degree for each node and save the statistics of 1-neighborhood for each node. This can be done in O(E) time. To map V numbers into B bin takes O(V ) time so the total complexity is O(E) time. This matches the lower bound of reading a graph. In comparison, we attach the table summarizing the complexity of computing various graph kernels, both exactly and approximately. <ref type="bibr">2</ref> The more computationally expensive part of our algorithm is the SVM. For linear SVM it takes O(nd 2 ) time where n is the number of features and d is the dimension of features. For non-linear kernel it is O(n 2 d).</p><p>Although there is an efficient algorithm available to approximate the feature map[RR08], we can still afford running the original algorithm in a relatively short time.  <ref type="bibr" target="#b16">[KP16]</ref>,? is the number of sampled vertices and? &lt; V. For FGSD <ref type="bibr" target="#b27">[VZ17]</ref>, r the number of terms of polynomials used to approximate f (L).</p><formula xml:id="formula_0">Complexity WL/WL-OA GK RetGK MLG FGSD LDP Approximate - O(V d k?1 ) O(D + d) O(V 3 ) O(rE) - Worst-Case O(hE) O(V k ) O(V 2 ) O(? 3 ) O(V 3 ) O(E)</formula><p>Hyperparameter: Below we describe the hyper-parameters of our model. They arise naturally when discretizing continuous node features on graphs for the down-stream classifier. In practice, they are robust and easy to tune.</p><p>Bin size. In our experiment, we map the neighborhood degree distribution into a different number of n bins of uniform width. We try different sizes from {30, 50, 70, 100} to discretize the distribution.</p><p>Normalization. There are two natural ways of normalization for our method. The first one is to normalize every graph separately so that the value represents the relative degree. The second one is to normalize the whole dataset by finding the largest degree value across all the graphs. In practice, we do not see consistent advantage of one normalization over the other so we try both and pick the one gives best training accuracy.</p><p>Empirical Distribution versus Histogram. We try to represent node features over a graph by both a histogram or an empirical distribution because empirical distribution is more stable with respect with the particular choice of bin size. It indeed yields better results than the histogram on certain dataset so in experiments we treat this choice as a hyperparameter.</p><p>Linear vs logarithmic scales. The degree distribution of many real-life networks follows the power law, which is usually visualized by log-log scale. We try both log scale and linear scale and notice that log scale yields better results for REDDIT 5K and REDDIT 12K.</p><p>SVM parameter. The C parameter is selected from {10 ?3 , 10 ?2 , ..., 10 2 , 10 3 } and the Gaussian bandwidth is selected from {10 ?2 , 10 ?1 , 1, 10 1 , 10 2 }.</p><p>Remark. In general, all those hyper-parameters except the SVM parameters are not sensitive. Fine tuning of the above hyper-parameters usually yields about an improvement of 2 percentage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Relation to graph neural networks</head><p>Our baseline can be seen as a variant of GNN that is used to learn useful representations for graph classification in an end-to-end manner. GNNs use the graph structure G and node features X v to learn either node-level representation or graph-level representation. Formally, the k-th layer of a GNN is For our baseline, AGGREGATION is a simple function that summarizes the statistics of neighboring degree distribution by computing min, max, average, and standard deviation. We do not introduce trainable weights. The COMBINE function is a simple concatenation and READOUT is either histogram or empirical distribution operation. The number of iteration is K = 2 in our case. From this point of view, LDP captures the essential elements of GNN, and this may partially explain its effectiveness. However, it is quite surprising to us that without any learning we can still achieve results comparable to many GNNs. We suspect that the use of AGGREGATION or READOUT function may play an important role. Interestingly a recent paper <ref type="bibr" target="#b29">[XHLJ18]</ref> explore the expressiveness of different pooling strategies and conclude that sum pooling is more powerful than mean and max pooling. To leverage this, we introduce "sum(DN(v))" as an extra node feature in the hope of achieving better results. We did a preliminary experiment on REDDIT 5K and REDDIT 12K but only achieved marginal improvement(0.5 percent). This might be due to the fact that our baseline does not involve any learning. Thus in the reported results, this "sum(DN(v))" feature is not deployed.</p><formula xml:id="formula_1">a (k) v = AGGREGATE (k) ({h (k?1) u : u ? N (u)}), h (k) v = COMBINE (k) (h (k?1) v , a (k) v ) (1) where h (k) v is the feature vector of node v at the k-th layer. h (0) v = X v initially, and N (v) is neighbors of v.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discussion on comparison to Weisfeiler-Lehman Kernel</head><p>We can also see the similarity between LDP and the Weisfeiler-Lehman kernel/isomorphism test with two iterations. Both methods start from local node feature and build new feature from the previous step through graph topology. However, a key difference between WL kernel and LDP is that the hashing step(label compression) of WL kernel do not necessarily capture the local similarities: even two nodes with very similar neighborhood could be mapped to totally different labels, and perturbing the edges by a small amount will lead to completely different hashing features. LDP instead uses the statistics of degree distribution of local neighborhood that is more robust and able to capture the node similarity. Empirically, we observe that it appears to strike a good balance between discriminating different local structures versus being robust to small differences.</p><p>Interestingly, in a recent paper [XHLJ18] that explores the discriminative power of graph neural networks showed that GNNs are at most as powerful as the WL test in distinguishing graph structures. By choosing right aggregation function, they develop graph isomorphism network(GIN) that can achieve the same discriminative power as WL test. However, in experiments, it is observed that GIN outperform WL kernel on social network dataset. One explanation the authors provide is that WL-tests are one-hot encodings and thus cannot capture the "similarity" between subtrees (while they can capture that whether they are "the same" or not). In contrast, a GNN satisfying certain criteria (see Theorem 3 of <ref type="bibr" target="#b29">[XHLJ18]</ref>) generalizes the WL test by learning to embed the subtrees to a feature space continuously. This enables GNNs to not only discriminate different structures, but also learn to map similar graph structures to similar embeddings and capture dependencies between graph structures.</p><p>We can see in the experiment section that after incorporating the features that able to capture similarity of graph structures, LDP outperforms WL kernel by a large margin on social network dataset and achieves the result on par with GIN, even though no learning is involved in LDP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Results</head><p>Datasets. We collect commonly used graph datasets from existing graph classification literature as our benchmark data. For non-attribute graphs, we use the movie collaboration datasets IMDB BINARY and IMDB MULTI, the scientific collaboration data COLLAB, the social network datasets REDDIT BINARY, REDDIT 5K, and REDDIT 12K. For non-attribute graphs, we use the protein dataset ENZYMES, PRO-TEINS and D&amp;D, chemical compounds datasets MUTAG, PTC, and NCI1. See appendix for a more detailed description of these datasets, including their statistics and properties.  <ref type="bibr" target="#b0">[AT16]</ref>. In particular, among above graph kernels, only FGSD and GK does not utilize label information for chemical/protein graphs, and for GNNs, the size of chemical/protein graphs are too small and their performance is not reported for attribute graphs in the most paper. All the result is taken from published paper except the baseline. The code for LDP will is available on github. 3 .   margarin consistently, and achieve the results on par with more recent graph kernels such as RetGK and WL-OA(no more worse than 2.5 percent) and perform even better on certain datasets. On average, we are slightly better than RetGK for non-attribute graphs. However, our method is much simpler and faster than all of the previous graph kernels. What is more, we find that even using only linear kernel, which yielding our final model equivalent to local feature + linear SVM, performance on REDDIT BINARY, REDDIT 5K, and REDDIT 12K rarely degrades.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results. The results are shown in</head><p>Comparing with GNN, although many accuracy data is not available, we can still clearly see that the state-of-the-art GNNs do not show significant improvement over our baseline. It is well known that neural networks are data hungry, so one possible explanation is that the size of the current dataset is limiting the representation power of GNNs. Since our model can be seen as a simple variant of GNN where no end-to-end learning is involved, we interpret current result more as a dataset problem instead of an algorithm problem: current benchmark is no longer suitable for evaluating different algorithms. A Large-scale ImageNet-like dataset for graphs is strongly needed for the evaluation of GNNs.</p><p>For chemical graphs, we can treat them as graphs without the label, and apply the same method. We can also introduce one more feature for each node, which is simply the node label in the original data. Adding label information in this way is certainly not principled as our results are no longer invariant to label permutation. However, our goal in this paper is not to handle attribute graphs so we settle on this hack. As we can see, LDP is still quite good on MUTAG, PTC, and PROTEIN datasets that do not have rich label information. For graphs with rich labels such as ENZYME, DD, and NCI1(See details in appendix), LDP and its variant is much worse than the other kernels that can better utilize label information. This also suggests that MUTAG, PTC, and PROTEIN are not sufficient to evaluate different methods.</p><p>Variants and limitations of baseline: Surprised by the performance of our baseline that is based on purely local node feature, one natural extension is to incorporate more sophisticated 1) node features and 2) edge features in the hope of capturing the local and global graph topology better and therefore improving the accuracy.</p><p>To test this, we have experimented adding other node features(also more expansive to compute) such as closeness centrality, Fiedler vector(the second smallest eigenvectors of graph Laplacian), and Ricci curvature <ref type="bibr" target="#b18">[LLY11]</ref> of graphs. Interestingly, we observe that none of the above features yields any significant improvement consistently across all datasets.</p><p>For edge features, we compute all-pair-shortest path distance and add the histogram of distance distribution along with degree-based features for small chemical graphs(For the social networks dataset, it is too costly to compute all pair shortest path distance and we do not observe any improvement in the preliminary experiments). There is about 2 percentage improvement consistently over different datasets. 4 This indicates that our local method fails to capture more global information which is shown to be useful for chemical/protein classification 5 .  <ref type="table">Table 5</ref>: Adding distance distribution improves the accuracy. Compared with FGSD and GK, which are the only two models that also does not use label information, we can see after adding all pair shortest path distance, we can match their result on all datasets except the NCI1 for FGSD. This indicates that using only local degree-based features is not enough for chemicals/protein classification</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>FGSD GK LDP LDP + distance</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion and Discussion</head><p>Most graph kernels aim to capture graph topology and graph similarity in the hope of improving classification accuracy. Our experiment suggests that this is not yet well-reflected on current benchmark datasets for non-attribute graphs. This calls for the construction of better and more comprehensive benchmark datasets for understanding and analyzing graph representations. On the other hand, we emphasize that in scenarios where both graph nodes and edges have certain associated features, proper handling labels can significantly improve the classification accuracy, as is shown clearly for the NCI1 dataset. Graph kernels have been rather effective in incorporating node/edge attributes. It will be an interesting question to see how to incorporate attributes in a more effective yet still simple manner in our graph representation. Also, although there are large scale chemical graphs datasets available[HOAAE + 11, RVDBR12], a benchmark dataset that contains many large graphs is still missing. We plan to create such benchmark dataset for future use. In general, while not addressed in this paper, we note that understanding the power and limitation of various graph representations, as well as the types of datasets with respect to which they are most effective, are crucial, yet challenging and remain largely open.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Acknowledgement</head><p>We thank Shuaicheng Chang and Siyuan Ma for insightful comments. We thank Han Fu for proofreading.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Datasets description</head><p>The statistics of the benchmark graph datasets used in the paper are reported in <ref type="table" target="#tab_7">Table 6</ref>. We describe these datasets in detail in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Non-attributed graph datasets</head><formula xml:id="formula_2">IMDB-BINARY [YV15a]</formula><p>is a movie collaboration dataset that consists of the ego-networks of 1,000 actors/actresses who played roles in movies in IMDB. In each graph, nodes represent actors/actress, and there is an edge between them if they appear in the same movie. These graphs are derived from the Action and Romance genres.</p><p>IMDB-MULTI [YV15a] is generated in a similar way to IMDB-BINARY. The difference is that it is derived from three genres: Comedy, Romance, and Sci-Fi.</p><p>REDDIT-BINARY <ref type="bibr" target="#b30">[YV15a]</ref> consists of graphs corresponding to online discussions on Reddit. In each graph, nodes represent users, and there is an edge between them if at least one of them respond to the other's comment. There are four popular subreddits, namely, IAmA, AskReddit, TrollXChromosomes, and atheism. IAmA and AskReddit are two question/answer based subreddits, and TrollXChromosomes and atheism are two discussion-based subreddits. A graph is labeled according to whether it belongs to a question/answerbased community or a discussion-based community.</p><p>REDDIT-MULTI(5K) <ref type="bibr" target="#b30">[YV15a]</ref> is generated in a similar way to REDDIT-BINARY. The difference is that there are five subreddits involved, namely, worldnews, videos, AdviceAnimals, aww, and mildlyinteresting. Graphs are labeled with their corresponding subreddits.</p><p>REDDIT-MULTI(12K) <ref type="bibr" target="#b30">[YV15a]</ref> is generated in a similar way to REDDIT-BINARY and REDDIT-MULTI(5K). The difference is that there are eleven subreddits involved, namely, AskReddit, AdviceAnimals, atheism, aww, IAmA, mildlyinteresting, Showerthoughts, videos, todayilearned, worldnews, and TrollXChromosomes. Still, graphs are labeled with their corresponding subreddits.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Graphs with discrete attributes</head><p>MUTAG [DLdCD + 91] consists of graph representations of 188 mutagenic aromatic and heteroaromatic nitro chemical compounds. These graphs are labeled according to whether or not they have a mutagenic effect on the Gram negative bacterium Salmonella typhimurium.  consists of graph representations of chemical molecules. In each graph, nodes represent atoms, and edges represent chemical bonds. Graphs are labeled according to carcinogenicity on rodents, divided into male mice (MM), male rats (MR), female mice (FM), and female rats (FR).</p><p>ENZYMES and PROTEINS [BOS + 05] consist of graph representations of proteins. Nodes represent secondary structure elements (SSE), and there is an edge if they are neighbors along the amino acid sequence or one of three nearest neighbors in space. The discrete attributes are SSE types. The continuous attributes are the 3D length of the SSE. Graphs are labeled according to which EC top-level class they belong to.</p><p>DD <ref type="bibr" target="#b5">[DD03]</ref> consists of graph representations of 1,178 proteins. In each graph, nodes represent amino acids, and there is an edge if they are less than six Angstroms apart. Graphs are labeled according to whether they are enzymes or not. NCI1 [SSL + 11] consists of graph representations of 4,110 chemical compounds screened for activity against non-small cell lung cancer and ovarian cancer cell lines, respectively.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>The choices of AGGREGATE (k) (?) and COMBINE (k) (?) in GNNs are crucial. A number of architectures for AGGREGATE have been proposed. For example, one popular variant of GNN Graph Convolutional Networks (GCNs)<ref type="bibr" target="#b17">[KW16]</ref>, implements AGGREGATE as a(k) v = MEAN({ReLU(W ? h (k?1) u ), ?u ? N (v)}) where W is a learnable weight matrix.For node classification, the node representation h(K) v of the last layer is used for prediction. For graph classification task, an additional READOUT function is needed to aggregate node features into a graph representation h G = READOUT({h (K) v |v ? G}). READOUT can be a simple permutation invariant function such as summation and mean or a more sophisticated graph-level pooling function. [YYM + 18, ZCNC18]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Experimental setup All experiments are performed on a single Intel Xeon CPU E5-2630 v4@ 2.20GHz ? 40 and 64GB RAM machine. We compare our baseline with 6 state-of-the-art graph kernels: Weisfeiler-Lehman Kernel (WL)[SSL + 11], Graphlet kernel (GK)[SVP + 09], Deep Graph Kernel (DGK)[YV15a], RetGK[ZWX + 18], FGSD[VZ17], Weisfeiler-Lehman optimal assignment kernel (WL-OA)[KGW16], and 5 graph neural networks: PATCHYSAN (PSCN) [NAK16], GRAPHSAGE[HYL17], DIFFPOOL[YYM + 18], Graph Isomorphism Network GIN[XHLJ18], Diffusion-convolutional neural networks (DCNN)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Complexity of various graph kernels. With a slight abuse of notation, V is the number of nodes and E is the number of edges in the larger graph among two graphs. For WL[SSL + 11,<ref type="bibr" target="#b14">KGW16]</ref>, h is the number of iterations. For GK[SVP + 09], d &lt; V and k ? {3, 4, 5}. For RetGK[ZWX + 18], return probabilities of random walks need to be calculated before computing the kernel, which takes O(V 3 + (S + 1)V 2 ) exactly where S is the number of steps, and takes O(V SM ) approximately where M is number of Monte Carlo simulations used for simulation of random walks. D is the number of random Fourier features and d is the dimension of input feature. For MLG</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2</head><label>2</label><figDesc>-4. For non-attribute graphs, our results show that combining our simple degree-based features and the kernel machine, we can beat WL, GK, DGK kernels by a large</figDesc><table><row><cell></cell><cell>WL</cell><cell>GK</cell><cell cols="6">DGK RetGk FGSD WL-OA LDP LDP*</cell></row><row><cell>COLLAB</cell><cell>74.8</cell><cell>72.8</cell><cell>73.1</cell><cell>81.0</cell><cell>80.0</cell><cell>80.7</cell><cell>78.1</cell><cell>73.9</cell></row><row><cell>IMDB BINARY</cell><cell>70.8</cell><cell>65.9</cell><cell>67.0</cell><cell>71.9</cell><cell>71.0</cell><cell>-</cell><cell>75.4</cell><cell>67.7</cell></row><row><cell>IMDB MULTI</cell><cell>49.8</cell><cell>43.9</cell><cell>44.6</cell><cell>47.7</cell><cell>45.2</cell><cell>-</cell><cell>50.0</cell><cell>45.4</cell></row><row><cell cols="2">REDDIT BINARY 68.2</cell><cell>77.3</cell><cell>78.0</cell><cell>92.6</cell><cell>86.5</cell><cell>89.3</cell><cell>92.1</cell><cell>89.8</cell></row><row><cell>REDDIT 5K</cell><cell>51.2</cell><cell>41.0</cell><cell>41.3</cell><cell>56.1</cell><cell>47.8</cell><cell>-</cell><cell>55.9</cell><cell>54.2</cell></row><row><cell>REDDIT 12K</cell><cell>32.6</cell><cell>31.8</cell><cell>32.2</cell><cell>48.7</cell><cell>-</cell><cell>44.4</cell><cell>47.8</cell><cell>46.7</cell></row><row><cell>Average</cell><cell cols="5">57.90 55.45 56.03 66.33 -</cell><cell>-</cell><cell cols="2">66.55 62.95</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Comparison with Graph Kernel. LDP * means using only linear SVM.</figDesc><table><row><cell></cell><cell cols="7">PSCN GRAPHSAGE DIFFPOOL GIN DCNN DGCNN LDP LDP*</cell></row><row><cell>COLLAB</cell><cell>72.6</cell><cell>68.25</cell><cell>75.50</cell><cell cols="2">80.2 52.1</cell><cell>73.7</cell><cell>78.1 73.9</cell></row><row><cell>IMDB BINARY</cell><cell>71.0</cell><cell>-</cell><cell>-</cell><cell cols="2">75.1 49.1</cell><cell>70.0</cell><cell>75.4 67.7</cell></row><row><cell>IMDB MULTI</cell><cell>45.2</cell><cell>-</cell><cell>-</cell><cell cols="2">52.3 33.5</cell><cell>47.8</cell><cell>50.0 45.4</cell></row><row><cell cols="2">REDDIT BINARY 86.3</cell><cell>-</cell><cell>-</cell><cell cols="2">92.4 -</cell><cell>-</cell><cell>92.1 89.8</cell></row><row><cell>REDDIT 5K</cell><cell>49.1</cell><cell>-</cell><cell>-</cell><cell cols="2">57.5 -</cell><cell>-</cell><cell>55.9 54.2</cell></row><row><cell>REDDIT 12K</cell><cell>41.3</cell><cell>42.24</cell><cell>47.04</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>47.8 46.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Comparison with graph neural networks. LDP * means using only linear SVM.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>WL GK DGK PSCN RetGk FGSD WL-OA LDP LDP + Label</figDesc><table><row><cell>MUTAG</cell><cell cols="2">84.4 81.6 87.4</cell><cell>89.0</cell><cell>90.3</cell><cell>92.1</cell><cell>84.5</cell><cell>90.1 90.3</cell></row><row><cell>PTC</cell><cell cols="2">55.4 57.3 60.1</cell><cell>62.3</cell><cell>62.5</cell><cell>62.8</cell><cell>63.6</cell><cell>61.7 64.5</cell></row><row><cell cols="2">ENZYME 53.4 -</cell><cell>53.4</cell><cell>-</cell><cell>60.4</cell><cell>-</cell><cell>59.9</cell><cell>35.3 40.9</cell></row><row><cell cols="3">PROTEIN 71.2 71.7 75.7</cell><cell>75.0</cell><cell>75.8</cell><cell>73.4</cell><cell>76.4</cell><cell>72.7 73.7</cell></row><row><cell>DD</cell><cell cols="2">78.6 78.5 -</cell><cell>76.2</cell><cell>81.6</cell><cell>77.1</cell><cell>79.2</cell><cell>75.5 77.1</cell></row><row><cell>NCI1</cell><cell cols="2">85.4 62.3 80.3</cell><cell>76.3</cell><cell>84.5</cell><cell>79.8</cell><cell>86.1</cell><cell>73.0 74.3</cell></row><row><cell cols="8">Table 4: Comparison with other graph kernels for chemical graphs. LDP + Label means adding label as an</cell></row><row><cell>extra node feature.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>Datasets graph # class # average nodes # average edges # label #</figDesc><table><row><cell>MUTAG</cell><cell>188</cell><cell>2</cell><cell>17.93</cell><cell>19.79</cell><cell>7</cell></row><row><cell>PTC</cell><cell>344</cell><cell>2</cell><cell>14.29</cell><cell>14.69</cell><cell>19</cell></row><row><cell>ENZYME</cell><cell>600</cell><cell>6</cell><cell>32.63</cell><cell>64.14</cell><cell>3</cell></row><row><cell>PROTEIN</cell><cell>1113</cell><cell>2</cell><cell>39.06</cell><cell>72.82</cell><cell>3</cell></row><row><cell>DD</cell><cell>1178</cell><cell>2</cell><cell>284.32</cell><cell>715.66</cell><cell>81</cell></row><row><cell>NCI1</cell><cell>4110</cell><cell>2</cell><cell>29.87</cell><cell>32.30</cell><cell>37</cell></row><row><cell>IMDB BINARY</cell><cell>1000</cell><cell>2</cell><cell>19.77</cell><cell>96.53</cell><cell>-</cell></row><row><cell>IMDB MULTI</cell><cell>1500</cell><cell>3</cell><cell>13.00</cell><cell>65.94</cell><cell>-</cell></row><row><cell cols="2">REDDIT BINARY 2000</cell><cell>2</cell><cell>429.63</cell><cell>497.75</cell><cell>-</cell></row><row><cell>REDDIT 5K</cell><cell>4999</cell><cell>5</cell><cell>508.82</cell><cell>594.87</cell><cell>-</cell></row><row><cell>REDDIT 12K</cell><cell>12929</cell><cell>11</cell><cell>391.41</cell><cell>456.89</cell><cell>-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>Statistics of the benchmark graph datasets PTC [HKKS01]</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">The graph isomorphism testing itself has only very recently shown to be solvable in quasi-polynomial time<ref type="bibr" target="#b1">[Bab16]</ref>. The subgraph isomorphism, on the other hand, is NP-complete.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Deep graph kernel requires corpus generation to build the kernel and we do not find explicit computational complexity in the original paper so it is not listed in the table. The kernels in the table are WL(Weisfeiler-Lehman kernel)[SSL + 11], WL-OA (Weisfeiler-Lehman optimal assignment kernel)[KGW16], GK(Graphlet Kernel)[SVP + 09], RetGK(Graph Kernels based on Return Probabilities of Random Walks)[ZWX + 18], MLG(Multiscale Laplacian Kernel)<ref type="bibr" target="#b16">[KP16]</ref>, FGSD(family of graph spectral distances)<ref type="bibr" target="#b27">[VZ17]</ref>.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://github.com/Chen-Cai-OSU/LDP</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">We ignore MUTAG and PTC for the reason that the datasets are too small, and that even LDP + Label can achieve the best result. 5 For graphs, two vertex sets are called non-homometric if the multi-sets of distances determined by them are different. It is unknown whether there exists any distance metric under which two vertex sets of non-isomorphic graphs are always non-homometric; But it is easy to show that the shortest path distance does not satisfy the requirement: a cycle of four vertices and a triangle with a pendant edge are non-isomorphic but have the same multi-set of all pairwise shortest path distances, i.e., {1, 1, 1, 1, 2, 2}</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Diffusion-convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Atwood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Don</forename><surname>Towsley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Graph isomorphism in quasipolynomial time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L?szl?</forename><surname>Babai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the fortyeighth annual ACM symposium on Theory of Computing</title>
		<meeting>the fortyeighth annual ACM symposium on Theory of Computing</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="684" to="697" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Shortest-path kernels on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Karsten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans-Peter</forename><surname>Borgwardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kriegel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Data Mining, Fifth IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Protein function prediction via graph kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Karsten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Borgwardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soon</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Ong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sch?nauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">J</forename><surname>Svn Vishwanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans-Peter</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kriegel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="47" to="56" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
	<note>suppl</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Micha?l</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3844" to="3852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Distinguishing enzyme structures from non-enzymes without alignments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew J</forename><surname>Dobson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Doig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of molecular biology</title>
		<imprint>
			<biblScope unit="volume">330</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="771" to="783" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Structure-activity relationship of mutagenic aromatic and heteroaromatic nitro compounds. correlation with molecular orbital energies and hydrophobicity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rosa</forename><forename type="middle">L</forename><surname>Dldcd + 91 ; Asim Kumar Debnath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gargi</forename><surname>Lopez De Compadre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">J</forename><surname>Debnath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Corwin</forename><surname>Shusterman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hansch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of medicinal chemistry</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="786" to="797" />
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Convolutional networks on graphs for learning molecular fingerprints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dougal</forename><surname>David K Duvenaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jorge</forename><surname>Maclaurin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafael</forename><surname>Iparraguirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Bombarell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Al?n</forename><surname>Hirzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan P</forename><surname>Aspuru-Guzik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2224" to="2232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">On graph kernels: Hardness results and efficient alternatives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>G?rtner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Flach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Wrobel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Learning theory and kernel machines</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2003" />
			<biblScope unit="page" from="129" to="143" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Convolution kernels on discrete structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Haussler</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
		<respStmt>
			<orgName>Department of Computer Science, University of California at Santa Cruz</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The predictive toxicology challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Helma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">D</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Kramer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashwin</forename><surname>Srinivasan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="107" to="108" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The harvard clean energy project: large-scale computational screening and design of organic photovoltaics on the world community grid</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">+ 11] Johannes</forename><surname>Hachmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Olivares-Amaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sule</forename><surname>Atahan-Evrenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Amador-Bedolla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aryeh</forename><surname>S?nchez-Carrera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leslie</forename><surname>Gold-Parker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><forename type="middle">M</forename><surname>Vogt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Al?n</forename><surname>Brockway</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Aspuru-Guzik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Physical Chemistry Letters</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">17</biblScope>
			<biblScope unit="page" from="2241" to="2251" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1024" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Reducing kernel matrix diagonal dominance using semi-definite programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaz</forename><surname>Kandola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thore</forename><surname>Graepel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Shawe-Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Learning Theory and Kernel Machines</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2003" />
			<biblScope unit="page" from="288" to="302" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">On valid optimal assignment kernels and applications to graph classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nils</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre-Louis</forename><surname>Kriege</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Giscard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wilson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1623" to="1631" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Molecular graph convolutions: moving beyond fingerprints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Kearnes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Mccloskey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Berndl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Pande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Riley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of computer-aided molecular design</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="595" to="608" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The multiscale laplacian graph kernel</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Risi</forename><surname>Kondor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Horace</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2990" to="2998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Ricci curvature of graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linyuan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shing-Tung</forename><surname>Yau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Tohoku Mathematical Journal, Second Series</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="605" to="627" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Gated graph sequence neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Tarlow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Brockschmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.05493</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning convolutional neural networks for graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathias</forename><surname>Niepert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantin</forename><surname>Kutzkov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2014" to="2023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuliia</forename><surname>Orlova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Morteza</forename><surname>Alamgir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulrike</forename><forename type="middle">Von</forename><surname>Luxburg</surname></persName>
		</author>
		<title level="m">Graph kernel benchmark data sets are trivial! FEAST 2015: ICML Workshop on Features and Structures</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Random features for large-scale kernel machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Rahimi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Recht</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1177" to="1184" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Enumeration of 166 billion organic small molecules in the chemical universe database gdb-17</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><surname>Ruddigkeit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruud</forename><surname>Van Deursen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lorenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Louis</forename><surname>Blum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Reymond</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of chemical information and modeling</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2864" to="2875" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Efficient graphlet kernels for large graph comparison</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tiago</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Schieber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Albert</forename><surname>Carpi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>D?az-Guilera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Panos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristina</forename><surname>Pardalos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mart?n G Ravetti ; Nino</forename><surname>Masoller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Shervashidze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><forename type="middle">Jan</forename><surname>Schweitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Van Leeuwen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karsten</forename><forename type="middle">M</forename><surname>Mehlhorn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Borgwardt ; Nino Shervashidze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Vishwanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Petri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karsten</forename><surname>Mehlhorn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Borgwardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Artificial Intelligence and Statistics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="488" to="495" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
	<note>Journal of Machine Learning Research</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petar</forename><surname>Velickovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10903</idno>
		<title level="m">Graph attention networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Graph kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vichy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vishwanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Nicol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Risi</forename><surname>Schraudolph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karsten</forename><forename type="middle">M</forename><surname>Kondor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Borgwardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="1201" to="1242" />
			<date type="published" when="2010-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Hunt for the unique, stable, sparse and fast feature learning on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi-Li</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="88" to="98" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wzsj + 19] Felix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amauri</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Holanda De Souza</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Fifty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian Q</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weinberger</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.07153</idno>
		<title level="m">Simplifying graph convolutional networks</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.00826</idno>
		<title level="m">How powerful are graph neural networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Deep graph kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pinar</forename><surname>Yanardag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vishwanathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1365" to="1374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A structural smoothing framework for robust graph comparison</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pinar</forename><surname>Yanardag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vishwanathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2134" to="2142" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rex</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxuan</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Leskovec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.08804</idno>
		<title level="m">Hierarchical graph representation learning withdifferentiable pooling</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">An end-to-end deep learning architecture for graph classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicheng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marion</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixin</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI Conference on Artificial Inteligence</title>
		<meeting>AAAI Conference on Artificial Inteligence</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mianzhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yijian</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arye</forename><surname>Nehorai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Retgk</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.02670</idno>
		<title level="m">Graph kernels based on return probabilities of random walks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
