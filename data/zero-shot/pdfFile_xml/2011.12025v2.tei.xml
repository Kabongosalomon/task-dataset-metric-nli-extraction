<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SegBlocks: Block-Based Dynamic Resolution Networks for Real-Time Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Verelst</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinne</forename><surname>Tuytelaars</surname></persName>
						</author>
						<title level="a" type="main">SegBlocks: Block-Based Dynamic Resolution Networks for Real-Time Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1109/TPAMI.2022.3162528</idno>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T20:39+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Conditional execution</term>
					<term>convolutional neural networks</term>
					<term>model compression</term>
					<term>semantic segmentation !</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>SegBlocks reduces the computational cost of existing neural networks, by dynamically adjusting the processing resolution of image regions based on their complexity. Our method splits an image into blocks and downsamples blocks of low complexity, reducing the number of operations and memory consumption. A lightweight policy network, selecting the complex regions, is trained using reinforcement learning. In addition, we introduce several modules implemented in CUDA to process images in blocks. Most important, our novel BlockPad module prevents the feature discontinuities at block borders of which existing methods suffer, while keeping memory consumption under control. Our experiments on Cityscapes, Camvid and Mapillary Vistas datasets for semantic segmentation show that dynamically processing images offers a better accuracy versus complexity trade-off compared to static baselines of similar complexity. For instance, our method reduces the number of floating-point operations of SwiftNet-RN18 by 60% and increases the inference speed by 50%, with only 0.3% decrease in mIoU accuracy on Cityscapes.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>C OMPUTATIONAL demands of computer vision are constantly increasing, with new deep learning architectures growing in size and new datasets containing images of ever higher resolution. For instance, the Cityscapes dataset for semantic segmentation includes images of 2048 by 1024 pixels <ref type="bibr" target="#b0">[1]</ref>, and high-resolution images are also used in medical <ref type="bibr" target="#b1">[2]</ref> and remote sensing applications <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>. The sheer number of pixels results in a large number of computations due to the convolutions. In addition, many convolutional layers are required to achieve a large receptive field <ref type="bibr" target="#b4">[5]</ref>.</p><p>Deep learning applications using these images are often deployed on low-power devices such as phones, surveillance cameras, or car platforms <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>. The interest in embedded computer vision has lead to efficient neural network architectures <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref> and model compression methods <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>.</p><p>These approaches result in static neural networks, which apply identical operations to every pixel. Yet not every image region is equally complex. Therefore, static networks may not be allocating operations in the most optimal way. Various methods have been proposed to dynamically adapt network architectures based on image complexity. Most are designed for classification tasks, for instance by completely skipping network layers or channels <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref> and only few methods target dense pixel-wise classification tasks <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref>. Dynamic methods typically focus on the theoretical reduction in computational complexity, due to the implementation challenges of sparse operations <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref>. In this work, we speed up inference using block-based processing, of which efficient implementations have already been demonstrated on GPU and other platforms <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b26">[27]</ref>.</p><p>? T. <ref type="bibr">Verelst</ref>   <ref type="figure">Fig. 1</ref>. SegBlocks adjusts the processing resolution of image regions based on their complexity. A lightweight policy network decides which blocks should be processed in high resolution mode. The number of operations is reduced without significant loss of accuracy.</p><p>We propose a method to dynamically process lowcomplexity regions at a lower resolution, as illustrated in <ref type="figure">Figure 1</ref>. Our method splits an image into blocks, and then downsamples non-important blocks. Reducing the processing resolution not only reduces the computational cost, but also decreases the memory consumption. The policy network, selecting the regions to be processed in high resolution, is trained using reinforcement learning.</p><p>Efficiently processing images in blocks without losing accuracy is not trivial. One could treat each block as a separate image and then combine the individual block outputs. How-  <ref type="figure" target="#fig_0">(Figure 2b</ref>), leading to a loss in global context and significant decrease in accuracy. Existing works partially address this by including global features extracted by a separate network branch, requiring custom architectures <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref>.</p><p>Our method addresses this problem by replacing zeropadding with our BlockPad operation. This custom CUDA module copies features from neighboring blocks into the padding border and therefore preserves feature propagation between blocks, as if the network was never split into blocks. A comparison between zero-padding and the BlockPad module is given in <ref type="figure" target="#fig_1">Figure 3</ref>, showing that our module avoids artifacts at block borders.</p><p>The contributions of this work are as follows:</p><p>? We introduce the concept of dynamic block-based convolutional neural networks, where blocks are downsampled based on their complexity, to reduce their computational cost. In addition, we provide CUDA modules for PyTorch to efficiently implement block-based methods 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>?</head><p>We train the policy network with reinforcement learning, in order to select complex regions for highresolution processing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>?</head><p>We demonstrate our method using a state of the art semantic segmentation network, and show that our method reduces the number of floating-point operations (FLOPS) and increases the inference speed (FPS) with only a slight decrease in mIoU accuracy. Our method achieves better accuracy than static baseline networks of similar complexity.</p><p>This work is an extension of a 4-page short paper <ref type="bibr" target="#b27">[28]</ref>, where block-based processing was introduced in combination with a simple heuristic-based downsampling policy that selects regions where the loss of visual detail is most significant. Here, we provide more details on the blockbased processing framework and we introduce a superior reinforcement learning downsampling policy that is trained specifically for the task at hand. In addition, we provide results for Cityscapes, CamVid and Mapillary Vistas datasets with more comprehensive ablation studies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Semantic segmentation</head><p>Traditional works in semantic segmentation focus on improving segmentation accuracy, without taking into account  Features are visualized by summing activation magnitudes over the channel dimension and tiling the outputs of individual blocks into a single image. Standard zero-padding introduces noticeable artifacts at block borders. In addition, the output shows inconsistencies, e.g. in the car, due to the lack of global context for individual blocks. In contrast, our BlockPad module enables feature propagation as if the network was never evaluated in blocks. the network complexity <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b29">[30]</ref>. In contrast to other tasks such as classification or object detection, segmentation requires pixel-wise labels of the same resolution as the input. Preserving spatial information in the network requires high-resolution latent representations with a high computational and memory cost. At the same time, a large receptive field is needed to incorporate global context. In order to keep the network size under control, semantic segmentation networks use encoder-decoder architectures with skip connections <ref type="bibr" target="#b29">[30]</ref>, dilated convolutions <ref type="bibr" target="#b30">[31]</ref> and spatial pyramid pooling <ref type="bibr" target="#b4">[5]</ref>.</p><p>Applications such as driverless cars have raised interest in real-time inference on embedded devices. Several highresolution datasets for these applications have emerged. The Cityscapes dataset <ref type="bibr" target="#b0">[1]</ref> provides images of 2048?1024 pixels, with highly detailed annotations for 30 semantic classes. A more extensive dataset is Mapillary Vistas <ref type="bibr" target="#b31">[32]</ref> with 25000 high-resolution images and 152 object categories. As real-time inference is crucial for these applications and datasets, smaller and more efficient segmentation networks have been developed for this task specifically.</p><p>ICNet <ref type="bibr" target="#b9">[10]</ref> proposes a custom encoder architecture processing an image pyramid, with multi-resolution feature maps fused before the decoder. ERFNet <ref type="bibr" target="#b10">[11]</ref> factorizes convolution kernels into 1?3 and 3?1 kernels to reduce the computational cost. Bilateral Segmentation Network (BiSeNet) <ref type="bibr" target="#b32">[33]</ref> presents a network with two branches: a Spatial Path to encode high-resolution spatial information and a Context Path to achieve a high receptive field. A similar dual-branch architecture is proposed by Guided Upsampling Network (GUN) <ref type="bibr" target="#b33">[34]</ref>, where both branches share weights. A multi-branch network at different scales is demonstrated by ShelfNet <ref type="bibr" target="#b34">[35]</ref>. DABNet <ref type="bibr" target="#b35">[36]</ref> proposes a Depthwise Asymmetric Bottleneck module, which combines dilated convolutions with non-dilated ones, to capture local and more contextual information. ESPNetv2 <ref type="bibr" target="#b36">[37]</ref> introduces a building block with dilated depthwise convolutions for large receptive fields. Other methods use attention modules <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b39">[40]</ref>, <ref type="bibr" target="#b40">[41]</ref> to incorporate global context, for instance by weighting spatial features <ref type="bibr" target="#b41">[42]</ref>.</p><p>Many segmentation networks use classification back-bones. SwiftNet <ref type="bibr" target="#b42">[43]</ref> demonstrates that state of the art results can be achieved with a straightforward architecture: a ResNet-18 encoder <ref type="bibr" target="#b43">[44]</ref>, pretrained on ImageNet <ref type="bibr" target="#b44">[45]</ref>, is combined with a spatial pyramid pooling (SPP) module <ref type="bibr" target="#b45">[46]</ref> and basic decoder. Lightweight backbones such as MobileNetV2 <ref type="bibr" target="#b7">[8]</ref>, ShuffleNetV2 <ref type="bibr" target="#b8">[9]</ref> and EfficientNet <ref type="bibr" target="#b46">[47]</ref> use depthwise convolutions to reduce the computational cost, leading to models such as SwiftNetMN-V2 <ref type="bibr" target="#b42">[43]</ref>. Model compression techniques can further reduce the computational cost, by applying pruning <ref type="bibr" target="#b11">[12]</ref>, knowledge distillation <ref type="bibr" target="#b12">[13]</ref>, quantization <ref type="bibr" target="#b13">[14]</ref>, or factorization <ref type="bibr" target="#b47">[48]</ref>, <ref type="bibr" target="#b48">[49]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Dynamic segmentation</head><p>These efficient architectures and compression techniques achieve impressive performance, but do not exploit spatial properties of the image as they process each pixel with the same operation. Some recent methods address this by processing the image dynamically in blocks or patches. It is worth noting that dynamic methods are often complementary to static acceleration methods, and both can be combined to further reduce the computational cost. The method proposed by Huang et al. <ref type="bibr" target="#b19">[20]</ref> combines a fast segmentation model with a large model. First, the image is processed by a fast network, such as ICNet <ref type="bibr" target="#b9">[10]</ref>. Then, a selection criterion, based on the softmax certainty of the output, uses the rough predictions to decide which image regions should be re-evaluated by the large model (e.g. PSPNet <ref type="bibr" target="#b28">[29]</ref>). Such a two-stage approach has two drawbacks. First, the large network processes the image in blocks and the discontinuities at block borders stop feature propagation. Therefore, the method uses large patches of at least 256?256 pixels to include some global context. Secondly, the feature maps and predictions of the small network are not re-used by the large network, making the method less efficient and only applicable when the accuracy and cost difference between the small and large network is substantial.</p><p>Wu et al. <ref type="bibr" target="#b21">[22]</ref> propose a custom architecture where a high-resolution branch improves rough predictions of a lowresolution branch. Again, this method struggles with the feature propagation problem at patch borders and therefore uses large blocks of 256?256 pixels. In addition, they incorporate global features from the low-resolution network into the high-resolution branch to provide more global context. The Patch Proposal Network <ref type="bibr" target="#b20">[21]</ref> method of Wu et al. has a similar architecture, with a low-resolution global branch and high-resolution refinement branch. The local refinement branch only operates on selected patches, based on a trained selection criterion where regions with below-average accuracy are refined. Features of both branches are fused before generating final predictions.</p><p>In contrast to these methods, our dynamic method does not require modifications to existing network architectures. We address the discontinuities at block borders without requiring additional global features in the patch-based processing. Furthermore, by being applicable on existing architectures, our method benefits from further advancements in network architectures.</p><p>Another approach to reduce spatial redundancy is to warp the input image to enlarge important regions <ref type="bibr" target="#b49">[50]</ref>. The work of Marin et al. <ref type="bibr" target="#b50">[51]</ref> uses non-uniform image sampling to focus on complex regions. However, as the internal representation of the network is still a regular-spaced pixel grid, the flexibility of non-uniform downsampling is limited, and the network typically focuses on a single region.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Conditional execution</head><p>Adapting the network architecture based on the input image is also known as conditional execution <ref type="bibr" target="#b52">[52]</ref>, <ref type="bibr" target="#b53">[53]</ref>, <ref type="bibr" target="#b54">[54]</ref> or dynamic neural networks. For instance, some methods reduce the processing cost of simple images by skipping complete residual blocks <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref> or by dynamically pruning feature channels <ref type="bibr" target="#b17">[18]</ref>. However, these methods are intended for classification tasks, with a large variety in features between images of different classes. In segmentation, many images contain the same objects and features in different spatial arrangements.</p><p>Some dynamic methods rely on spatial properties to reduce the computational cost: they skip computations on low-complexity regions. Spatially Adaptive Computation Time <ref type="bibr" target="#b23">[24]</ref> and cascade-based methods <ref type="bibr" target="#b22">[23]</ref> halt the computation of features when features are 'good enough'. Dyn-Conv <ref type="bibr" target="#b18">[19]</ref> demonstrates inference speed improvements on human pose estimation tasks, where large regions of the image can be completely ignored by the network. The methods of Xie et al. <ref type="bibr" target="#b55">[55]</ref> and Figurnov et al. <ref type="bibr" target="#b56">[56]</ref> dynamically interpolate features in low-complexity regions. Requiring sparse convolutions, these methods either demonstrate no inference speed improvements <ref type="bibr" target="#b23">[24]</ref>, only on depthwise convolutions <ref type="bibr" target="#b18">[19]</ref>, or only on CPU <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b55">[55]</ref>, or are intended for specialized hardware <ref type="bibr" target="#b57">[57]</ref>. Block-based approaches are considered to be more feasible to implement efficiently on most platforms <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b26">[27]</ref>. In addition, methods completely skipping non-complex regions are less suitable for pixel-wise labeling tasks such as segmentation. Our method processes every image region, but reduces the cost of noncomplex regions.</p><p>An important aspect of conditional execution is the policy which determines the layers, channels or regions to execute. Often, this policy cannot be trained by standard backpropagation due to its discrete nature. Recently, the Gumbel-Softmax <ref type="bibr" target="#b58">[58]</ref>, <ref type="bibr" target="#b59">[59]</ref> trick gained popularity in dynamic methods <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>. In our case, it would require predictions for each region in both high and low-resolution, increasing the computational cost at training time. Instead, we show that the policy can be trained efficiently using reinforcement learning <ref type="bibr" target="#b60">[60]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">METHOD</head><p>SegBlocks splits an image into blocks and adjusts the processing resolution of each block based on its complexity. A lightweight policy network processes a low-resolution version of the image and outputs resolution decisions for each image region. For implementation simplicity, we restrict the resolution to two options, with either high-or lowresolution processing. Note that high-and low-resolution regions are processed by the same convolutional filters, which consequently perceive objects at different scales. This does not affect the performance negatively though, as segmentation images typically have large variations in object  The policy net's gradients are estimated using REINFORCE <ref type="bibr" target="#b61">[61]</ref> with a separate reward per block, based on the task loss in a block's region and the number of blocks processed at high resolution.</p><p>size, and convolutional neural networks are trained to be robust against scale variations. Since resolution decisions are discrete, the policy network is trained with reinforcement learning, using a reward function to determine the expected gradient.</p><p>First, we discuss how the reinforcement learning is incorporated using a reward taking into account both computational complexity and task accuracy. The policy is trained jointly with the main segmentation network. In the second part, we elaborate on custom modules making block-based adaptive processing of images possible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Downsampling policy</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Policy network</head><p>The policy network is a convolutional neural network that outputs a discrete decision per block. The network should be small compared to the main segmentation network. We use a simple architecture with 4 convolutional layers of 64 channels, batch normalization and ReLU non-linearities followed by a softmax layer over the channel dimension. A four times downsampled version of the image is given as input. The policy network f pn , parametrized by ?, outputs probabilities s b per block, indicating the likelihood that the block should be processed in high resolution based on the input image I:</p><formula xml:id="formula_0">s = f pn (I; ?),<label>(1)</label></formula><formula xml:id="formula_1">with s = [s 1 , . . . , s b , . . . , s B ] ? [0, 1] B .<label>(2)</label></formula><p>The soft probabilities are sampled to actions a, where a b = 1 results in high-resolution processing of block b:</p><formula xml:id="formula_2">s b = P (a b = 1).<label>(3)</label></formula><p>Standard backpropagation cannot be applied due to the sampling of discrete actions. Finding optimal decisions a for context I, with a constrained budget, is also referred to in literature as contextual bandits <ref type="bibr" target="#b60">[60]</ref>. This can be seen as a reinforcement learning scenario with a single time step: the policy network predicts all actions a b at once and directly obtains the reward. The probability of actions a, for image I and parameters ? is given by</p><formula xml:id="formula_3">? ? (a | I) = B b=1 p ? (a b | I).<label>(4)</label></formula><p>The objective is to find policy parameters ?, so that the predicted actions a for image I maximize the policy reward.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Policy reward</head><p>The goal of the policy network is to select the most important blocks for high-resolution processing. To avoid early convergence to a sub-optimal state where all blocks are processed at high resolution, the reward takes into account both the number of blocks processed at high resolution and the task accuracy. <ref type="figure" target="#fig_2">Figure 4</ref> illustrates how both rewards are integrated in the training process. We assume that a resolution decision a b only affects the segmentation output of block b and does not influence adjacent blocks. The total reward for an image can then be expressed as the sum of individual block rewards. A block reward consists of one term optimizing accuracy and second term keeping the number of operations under control, weighted by hyperparameter ? (set to 10 in our experiments). The reward per image is then written as</p><formula xml:id="formula_4">R(a) = B b R b (a) = B b R b,task (a) + ?R b,complexity (a) .<label>(5)</label></formula><p>The percentage of blocks processed in high-resolution is given by</p><formula xml:id="formula_5">?(a) = 1 B B b=1 a b .<label>(6)</label></formula><p>Then, the following reward minimizes the difference between ? and the desired percentage ? :</p><formula xml:id="formula_6">R b,complexity (a) = ?(?(a) ? ? ) if a b = 1, ?(a) ? ? if a b = 0.<label>(7)</label></formula><p>This reward is positive for blocks processed at high resolution when the actual percentage ? is smaller than the desired percentage ? , resulting in an incentive for the policy network to process more blocks at high resolution. Using a target ? instead of simply minimizing ? results in more stable training and lower sensitivity to hyperparameter ?.</p><p>The task reward encourages the policy network to select regions with a high segmentation loss for high resolution processing. In general, those regions correspond to the most complex ones in the image. The task criterion, e.g. pixelwise cross entropy, is denoted by L. The task loss per image is then given by</p><formula xml:id="formula_7">L task = L( y, y)<label>(8)</label></formula><p>where y and y denote the predictions and ground truth labels respectively. Our reward is based on the task loss per block. Values y b and y b are obtained by only considering values in the block region. The task reward of block b can then be defined as</p><formula xml:id="formula_8">R b,task (a) = L( y b , y b ) ? L task if a b = 1, ?(L( y b , y b ) ? L task ) if a b = 0.<label>(9)</label></formula><p>Subtracting L task is not strictly necessary but reduces the variance of the rewards for more stable training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3">Expected gradient</head><p>The policy network should predict actions that maximize the expected reward:</p><formula xml:id="formula_9">max J (?) = max E a?? ? [R(a)].<label>(10)</label></formula><p>The policy network's parameters ? can then be updated using gradient ascent with learning rate ?:</p><formula xml:id="formula_10">? ? ? + ?? ? [J (?)]<label>(11)</label></formula><p>The gradients of J can be derived similarly to the policy gradient method REINFORCE <ref type="bibr" target="#b61">[61]</ref>:</p><formula xml:id="formula_11">? ? J (?) = ? ? E a [R(a)] = ? ? a ? ? (a | I)R(a) = a ? ? ? ? (a | I)R(a) = a ? ? (a | I)? ? [log ? ? (a | I)R(a)] = a ? ? (a | I)? ? log B b=1 p ? (a b | I) R(a) = a ? ? (a | I)R(a) B b=1 ? ? log p ? (a b | I) = E a R(a) B b=1 ? ? log p ? (a b | I) = E a B b=1 R b (a)? ? log p ? (a b | I) .<label>(12)</label></formula><p>In practice, the expectation in Equation 12 is approximated with Monte-Carlo sampling using samples in the minibatch. The result can be interpreted as applying REIN-FORCE on each block individually with reward R b (a), giving unbiased but high-variance gradient estimates. In practice, we found the policy network stable to train for a large range of hyperparameters. The segmentation network and policy network are jointly trained. The hybrid loss to be minimized is then given by</p><formula xml:id="formula_12">L hybrid = L task + ? N N n=1 B b=1 ?R b (a)log p ? (a b | I)<label>(13)</label></formula><p>with N the batch size and ? a hyperparameter weighting the loss terms (set to 4 in our experiments).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Block modules</head><p>We introduce three new modules for block-based processing with convolutional neural networks: BlockSample, Block-Pad, and BlockCombine. <ref type="figure">Figure 5</ref> gives an overview of a simple block-processing pipeline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">BlockSample</head><p>The BlockSample module splits the image into blocks and downsamples low-complexity blocks consecutively. Average pooling is used for efficient downsampling. We use a downsampling factor of 2, reducing the computational cost of low-complexity regions by a factor 4. The CUDA implementation fuses the splitting and downsampling steps into a single operation. Images are split into blocks of a predefined block size, for instance 128?128 pixels. Downsampling operations, such as max pooling, further reduce the spatial dimensions of blocks throughout the network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">BlockPad</head><p>The BlockPad module replaces zero-padding and eliminates discontinuities at block borders by copying features from neighboring blocks into the padding. Evaluating the network with only high-resolution blocks is equivalent to normal processing without blocks. When two high-resolution or two low-resolution blocks are adjacent, pixels can be copied directly while preserving their spatial relationship. When copying features from low-resolution blocks into the padding of high-resolution blocks, features are nearest-neighbor upsampled in order to preserve the spatial relationship, as illustrated in <ref type="figure">Figure 5</ref>. When copying features from high-resolution blocks into the low-resolution block's padding, we considered two possibilities: strided subsampling and average sampling. <ref type="figure">Figure 6a</ref> illustrates that subsampling copies features in a strided pattern, whilst average sampling <ref type="figure">(Fig 6b)</ref> combines multiple pixels. Our experiments show that average sampling achieves better results with a small increase in overhead.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">BlockCombine</head><p>The BlockCombine module upsamples low-resolution blocks to the same resolution as the high-resolution ones, and then combines all blocks into a single tensor. Our CUDA implementation merges the nearest neighbour upsampling and block combining steps to reduce the overhead of this operation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>We integrate our dynamic processing method in Swift-Net <ref type="bibr" target="#b42">[43]</ref>, a state of the art network for real-time semantic segmentation of road scenes. We test semantic segmentation on the Cityscapes <ref type="bibr" target="#b0">[1]</ref>, Camvid <ref type="bibr" target="#b62">[62]</ref>, and Mapillary Vistas <ref type="bibr" target="#b31">[32]</ref> datasets. In addition, we provide an ablation study, analyzing the overhead of block-based processing. Our method is implemented in PyTorch 1.5 and CUDA 10.2, without TensorRT optimizations. We report two model complexity metrics: the number of computations and the frames per second.</p><p>The number of computations is reported in billions of multiply-accumulates (GMACs) per image, being half the number of floating-point operations (FLOPS). The FLOPS metric is often used to compare model complexity in a platform-agnostic manner, since the efficiency of the implementation has no impact. For our dynamic method, where the number of computations varies per image, we report the average GMAC count, over the validation or test set. Frames per second (FPS) is a more practical metric to measure inference speed, but depends largely on the implementation of building blocks. Therefore, both FPS and GMACs metrics should be taken into account for fair comparison. We measure the inference speed on both a high-end Nvidia GTX 1080 Ti 11GB GPU and low-end Nvidia GTX 1050 Ti 4 GB GPU, paired with an Intel i7 processor. The model is warmed up on the first half of the image set, and the speed is measured on the second half. To compare the inference speed to those reported by others, we normalize the FPS numbers (norm FPS) of different GPUs based on their relative performance, using the same scaling factors as Orsic et al. <ref type="bibr" target="#b42">[43]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Cityscapes semantic segmentation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Dataset and setup</head><p>The Cityscapes <ref type="bibr" target="#b0">[1]</ref> dataset for semantic segmentation consists of 2975 training, 500 validation and 1525 test images of 2048?1024 pixels. We use the standard 19 classes for semantic segmentation and do not use the additional coarsely labeled images.</p><p>We train SwiftNet <ref type="bibr" target="#b42">[43]</ref> with three different backbones: ResNet-18 (RN18), ResNet-50 (RN50) <ref type="bibr" target="#b43">[44]</ref>, and EfficientNet-Lite1 (EffL1), which corresponds to EfficientNet-B1 <ref type="bibr" target="#b46">[47]</ref>  without squeeze-and-excite and swish modules. Our dynamic processing method is integrated in these baseline networks and we trained models for different ? ? [0, 1], indicating the desired percentage of high-resolution blocks. The models are trained on 768?768 crops, augmented by image scaling between factor 0.5 and 2, slight color jitter and random horizontal flip. The optimizer is Adam, the learning rate is cosine annealed from 4e?4 to 1e?6 over 350 epochs, weight decay is set to 1e?4 and the batch size is 8. Standard cross entropy loss is used for models with a ResNet backbone, whereas the EfficientNet models use a bootstrapped cross entropy loss <ref type="bibr" target="#b65">[65]</ref>. The backbone is pretrained on ImageNet <ref type="bibr" target="#b44">[45]</ref>. Our method uses a block size of 128?128 pixels, resulting in a block grid of 16?8 blocks per image that can adapt the processing resolution to the content.   Simple images use fewer high-res blocks, down to 20%, and complex images have up to 55% high-res blocks (colored yellow).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Results and comparison</head><p>creasing the mIoU. In contrast, a static baseline network of similar complexity, trained on images of 1536?768 pixels, achieves 1.7% lower mIoU.  <ref type="table">Table 1</ref> compares the performance of our method with other dynamic methods and non-dynamic efficient segmentation architectures. Compared to other dynamic methods, our method achieves higher mIoU results at faster inference speeds. Patch Proposal Network <ref type="bibr" target="#b20">[21]</ref> is the most competitive method, and uses an architecture with a global branch and patch-based refinement branch. Our method achieves better accuracy and FPS due to our efficient block-based execution framework. This enables more fine-grained block-based execution, with in total 128 blocks (16?8 grid) compared to only 16 large patches, resulting in better adaptability and performance. Similarly, the method of Wu et al. <ref type="bibr" target="#b21">[22]</ref> refines regions with a high-resolution branch on large patches. Huang et al. <ref type="bibr" target="#b19">[20]</ref> use a cascade of two existing network architectures (e.g. lightweight ICNet and accurate PSPNet), where the second network is only applied on complex regions. However, this introduces additional latency and does not re-use any features of the lightweight network, resulting in low efficiency and only 1.8 FPS. The method of Marin et al. <ref type="bibr" target="#b50">[51]</ref> (Learning Downsampling) adaptively samples the input image, to more densely sample near segmentation boundaries. Only the theoretical computational cost (GMACs) is reported. HyperSeg <ref type="bibr" target="#b63">[63]</ref>, based on the EfficientNet backbone <ref type="bibr" target="#b46">[47]</ref>, adjusts the weights of the decoder dynamically per image. Stochastic sampling <ref type="bibr" target="#b55">[55]</ref> dynamically interpolates features spatially, but does not have an accelerated GPU implementation.</p><p>We are also competitive with state of the art architectures for fast semantic segmentation, such as BiSeNet <ref type="bibr" target="#b32">[33]</ref> or ERFNet <ref type="bibr" target="#b10">[11]</ref>. It is worthwhile to note that our proposed dynamic resolution method is complementary to further improvements in network architectures.</p><p>The table also demonstrates the inference speed improvement achieved using our efficient implementation. For instance, our method improves the inference speed of SwiftNet-RN50 from 16 FPS to 30 FPS on a GTX 1080 Ti, achieving real-time performance. This increase of 84 percent is obtained by reducing the theoretical complexity (GMACs) by 63%, while the mean IoU is decreased by 1.8%. Memory usage is reported in <ref type="table" target="#tab_4">Table 3</ref> as PyTorch's peak allocated memory during inference over the validation set, and indicates that our method also reduces memory consumption by storing low-complexity regions at lower resolution.</p><p>Our method adapts the operations to each individual image, and <ref type="figure">Fig. 8</ref> shows the distribution of operations over images. Qualitative results are shown in <ref type="figure">Fig. 9</ref>, visualizing the resolution decisions of the policy network and the respective segmentation output, for ? set to 0.4 and 0.2. The policy network, trained with reinforcement learning, properly selects complex regions for high-resolution processing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3">Per-class analysis</head><p>The IoU result per class is provided in <ref type="table">Table 2</ref>, as well as the percentage of pixels processed in high-resolution for each class. Our method mainly processes classes such as road, sky and vegetation in low resolution, whereas rider, motorcycle and bicycle are typically sampled in high resolution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">CamVid semantic segmentation</head><p>Another popular benchmark for real-time semantic segmentation is CamVid <ref type="bibr" target="#b62">[62]</ref>, having 701 densely annotated frames, divided in 367 training, 101 validation and 233 test images. Following the methodology of related works <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b42">[43]</ref>, <ref type="bibr" target="#b64">[64]</ref>, we train on the training and validation subsets and report test scores on 11 semantic classes. We evaluate on images of 960?704 pixels. We use the same hyperparameters and data augmentations as on Cityscapes, but with crops of 704 pixels. The backbone is pretrained on ImageNet, but we did not pretrain on Cityscapes. Dynamic methods use a block size of 64 pixels, resulting in a grid of 15?11 blocks. In order to adjust for the lower input resolution compared to Cityscapes, we remove the stride of the last residual block in  <ref type="bibr" target="#b64">[64]</ref> 72.4 125 BiSeNetV2-L <ref type="bibr" target="#b64">[64]</ref> 73.2 33 HyperSeg-S (ResNet-18) <ref type="bibr" target="#b63">[63]</ref> 77.0 32.5 HyperSeg-S (EfficientNet-B1) <ref type="bibr" target="#b63">[63]</ref> 78.4 6.3 ? 38.0 SFNet (DF2) <ref type="bibr" target="#b66">[66]</ref> 70.4 134 SFNet (ResNet-18) <ref type="bibr" target="#b66">[66]</ref> 73.8 41.2 ? 36 the backbone, leading to higher-resolution representations in the spatial pyramid pooling module. <ref type="table" target="#tab_5">Table 4</ref> shows that our results are competitive with other real-time semantic segmentation methods, and our dynamic execution (SegBlocks) improves inference speed over the static baseline network (SwiftNet). However, due to the smaller image dimensions and block size, the speedup is more modest compared to the high-res Cityscapes experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Mapillary Vistas semantic segmentation</head><p>Mapillary Vistas <ref type="bibr" target="#b31">[32]</ref> is a large dataset containing highresolution road scene images with labels for semantic and instance segmentation. We use the standard 15000/2000 train/val split, resize all images to 1536?1152 pixels and use the 65 classes for semantic segmentation. We report validation results, since the test server is not available. Seg-Blocks usees blocks of 128 pixels, resulting in an adaptive grid of 12?9 blocks. The hyperparameters are the same as the Cityscapes experiments, with the batchsize reduced to 4 as the number of classes requires more memory, and we train SegBlocks with an EfficientNet-Lite1 backbone for 100 epochs. <ref type="table" target="#tab_6">Table 5</ref> demonstrates that SegBlocks reduces the number of operations and improves inference speed for the segmentation model on this dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Ablation studies</head><p>Module execution time analysis: We profile the time characteristics of our block modules to analyze their overhead. <ref type="table" target="#tab_7">Table 6</ref> compares the execution time of the Policy Net, BlockSample, BlockPad and BlockCombine modules  <ref type="figure">Fig. 10</ref>. Comparison of block execution policies. The reinforcement learning policy is proposed in this work. The random policy randomly selects a percentage of blocks per frame for high-resolution processing. The heuristic policy is given in <ref type="bibr" target="#b27">[28]</ref>.</p><p>with the total runtime. The overhead of the Policy Net, BlockSample and BlockCombine modules is negligible, and the overhead of the BlockPad operation is reasonable with around 10 percent of the total execution time. The BlockPad module has less impact on the ResNet-50 backbone, as its 1?1 convolutions in the bottleneck function do not require padding. Note that the cost of the BlockPad operation scales with the total number of processed pixels, and thus with the number of high-resolution blocks.</p><p>Reinforcement learning policy: We compare the policy trained using reinforcement learning with other baselines in <ref type="figure">Fig. 10</ref>. The 'random' policy randomly selects blocks for high-resolution processing, with the number of selected blocks given by the target percentage. The heuristic policy was proposed in <ref type="bibr" target="#b27">[28]</ref> and selects the regions with the highest visual change, based on the average L2 distance between high-and low-resolution versions of a block.</p><p>Policy network architecture: The policy network, determining whether blocks should be executed with high-or low-resolution processing, should be lightweight but powerful enough for the selection task. The ablation <ref type="table" target="#tab_8">(Table 7)</ref> shows that the size of this network does not significantly impact the results. For our experiments, we use an input resolution of 512x256 with a 4 layer network having 64 features in each layer.</p><p>Training steps and progress: We compare the training speed of a standard model with a dynamic SegBlocks model in <ref type="figure">Fig. 11</ref>, which shows that the reinforcement learning of the policy does not significantly impact the required number of training steps.</p><p>Block size: <ref type="table" target="#tab_9">Table 8</ref> compares the impact of the method's block sizes on accuracy and performance. <ref type="figure" target="#fig_0">Figure 12</ref> demonstrates the granularity of each block size on  Cityscapes. Larger block sizes offer better inference speeds, by padding fewer pixels and having less overhead, whereas smaller block sizes offer finer control of adaptive processing. Sampling pattern: As described in Section 3.2.2, the BlockPad module pads individual blocks by sampling their neighbors. When low-resolution blocks are padded with features of high-resolution blocks, these features should be subsampled. <ref type="table" target="#tab_10">Table 9</ref> compares average-sampling with strided subsampling and shows that average-sampling achieves better accuracy (mIoU) with only a slight increase in overhead. In addition, we show that using zero-padding has a significant impact on accuracy, highlighting the importance of BlockPad when processing images in blocks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.1">Single residual block analysis</head><p>The overhead introduced by our block modules strongly depends on the block size: larger blocks have fewer pixels in the padding, and require fewer copy operations. Moreover, standard operations, implemented in PyTorch and cuDNN, are often optimized for larger spatial dimensions. We analyze a single residual block to measure the impact  of both block size and the percentage of high-resolution blocks. <ref type="figure" target="#fig_1">Figure 13a</ref> studies the impact of the block size, when evaluating half of the blocks at high and half of the blocks at low resolution. The number of floating-point operations is reduced by 56%, resulting in a theoretical 2.3 times speed increase. In practice, we measure 1.92 times faster inference of the residual block when the block size is larger than 32. Block sizes smaller than 4 pixels result in slower inference. Note that, even though block sizes are typically large at the network input (e.g. 128?128) downsampling in the network reduces the block size by the same factor. The SwiftNet network downsamples by a factor 32 throughout the network, resulting in block sizes of 4?4 for the deepest network layers. <ref type="figure" target="#fig_1">Figure 13a</ref> shows that, when evaluating using block size 8, the percentage of high-res blocks should be lower than 75% to achieve practical speedup. For lower percentages, the practical speedup of our implementation is around 70 percent of the theoretical one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>We proposed a method to reduce the computational cost of existing convolutional neural networks, by evaluating images in blocks and adjusting the processing resolution of each block dynamically. We introduced custom block operations, enabling efficient inference and training of these dynamic neural networks. Our BlockPad module is essential to enable feature propagation between individual blocks. A lightweight policy network, trained with reinforcement learning, selects complex regions for high-resolution processing. Our method is demonstrated on semantic segmentation of street scenes and we show that the computational complexity of the SwiftNet architecture can be reduced with only a small decrease in accuracy. Dynamically adjusting the processing resolution per block achieves better accuracy than simply downscaling the image to a lower resolution. The idea of block-based processing and dynamic resolution adaptation can be integrated in various network architectures and computer vision tasks. In particular, our method is suited for dense pixel-wise classification tasks such as depth estimation, instance segmentation or human pose estimation. Furthermore, our block modules such as BlockPad pave the way towards other variants, for instance by processing blocks with varying quantization levels. Dynamic processing can help to keep the number of operations and energy consumption under control.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">ACKNOWLEDGMENTS</head><p>This work was funded by FWO on the SBO project with agreement S004418N.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>Illustration of the zero-padding problem when processing blocks: convolutions pad individual blocks with zeros (grey), stopping the propagation of features between blocks and therefore resulting in a loss of global context. ever, features cannot propagate between individual blocks</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Comparison of feature maps when processing an image in blocks with either standard zero-padding or our custom BlockPad module.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>Training the policy network with reinforcement learning: The lightweight policy net predicts soft decisions s, which are sampled to actions a.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .Fig. 6 .</head><label>56</label><figDesc>Illustration of the BlockSample, BlockPad and BlockCombine modules. BlockSample splits the image and downsamples low-complexity blocks. BlockPad replaces zero-padding and enables feature propagation between blocks. The module copies features from neighboring blocks into the padding. When padding low-resolution blocks adjacent to high-resolution blocks, multiple pixel values of the high-resolution blocks are averaged to better preserve the spatial coherence of features.(a) Strided subsampling (b) Average sampling BlockPad aims to respect the spatial relationship between highand low-resolution blocks by sampling using the illustrated patterns. Low-resolution blocks (2?2 pixels size) are colored dark grey, while the 4?4 high-resolution blocks are colored light grey. The dotted blue and red grid shows the sampling pattern when padding high-resolution and low-resolution blocks respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 7 .</head><label>7</label><figDesc>Cityscapes validation results and comparison with static baselines of similar complexity. For a given computational complexity (GMACs), our adaptive resolution method SegBlocks consistently outperforms static baseline networks (SwiftNet) of similar complexity. Each backbone (RN50, RN18, Eff-L1) is trained and evaluated with ? ? 0.2, 0.4, 0.6 for dynamic models and resolutions 2048?1024, 1536?768, 1344?672 for static baselines.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7</head><label>7</label><figDesc>shows the mIoU accuracy of models at various computational costs (GMACs). Static baseline networks of different computational complexity are obtained by adjusting the training and evaluation resolution to 2048?1024, 1536?768 and 1344?672. The complexity of our SegBlocks methods is determined by the number of high-resolution</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>2 Fig. 9 .</head><label>29</label><figDesc>Examples of resolution decisions made by the policy network and corresponding segmentation outputs, for SegBlocks-RN18 with ? = 0.4 and ? = 0.2. High-resolution blocks are colored in yellow.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 13 .</head><label>13</label><figDesc>Comparison of theoretical versus practical speedup for a single residual block of ResNet-18, studying the impact of the block size and the percentage of high-resolution blocks on the performance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>and T. Tuytelaars are with the Center for Processing Speech and Images, Department Electrical Engineering, KU Leuven. E-mail: {thomas.verelst, tinne.tuytelaars}@esat.kuleuven.be</figDesc><table><row><cell>High-resolution image</cell></row><row><cell>2048 x 1024 pixels</cell></row><row><cell>Policy</cell></row><row><cell>high-resolution processing</cell></row><row><cell>low-res processing</cell></row><row><cell>Block Segmentation Network</cell></row><row><cell>segmentation result</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>High-resolution image: 2048 x 1024</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Expected gradient</cell></row><row><cell>4x</cell><cell>Conv</cell><cell>Conv</cell><cell>Conv</cell><cell></cell><cell></cell><cell>Sample</cell></row><row><cell>Downsample</cell><cell>BN</cell><cell>BN</cell><cell>BN</cell><cell>Conv</cell><cell>softmax</cell><cell>actions</cell></row><row><cell></cell><cell>ReLU</cell><cell>ReLU</cell><cell>ReLU</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Block</cell><cell>Block</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Sample</cell><cell>Combine</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE 1 TABLE 2 97.8 82.9 91.9 59.5 57.6 61.1 67.0 76.2 92.1 61.9 94.4 80.4 59.9 94.4 77.2 82.4 77.2 60.1 75</head><label>12</label><figDesc>Results on Cityscapes semantic segmentation. Our SegBlocks models are based on the respective SwiftNet baselines, and integrate block-based dynamic resolution processing in those networks . The symbol '-' indicates that the metric was not reported. For our method, only several test set results are reported due to submission limitations on Cityscapes test evaluation. IoU per class on the Cityscapes validation set: our method improves the IoU score for most classes compared to lower-resolution baselines with similar complexity. Classes such as person, rider, car, bus and train benefit more from high-resolution processing. The percentage of pixels processed in high-resolution, given per class, shows that our method mostly processes road and sky regions at low resolution.</figDesc><table><row><cell></cell><cell></cell><cell>Method</cell><cell></cell><cell>mIoU val</cell><cell>test</cell><cell>GMACs</cell><cell>FPS</cell><cell>norm FPS</cell></row><row><cell></cell><cell></cell><cell cols="2">SwiftNet-RN50 (baseline, our impl.)</cell><cell>78.0</cell><cell></cell><cell>206.3</cell><cell>16.3 @ 1080 Ti, 3.8 @ 1050 Ti</cell><cell>16.3</cell></row><row><cell></cell><cell></cell><cell cols="2">SegBlocks-RN50 (? =0.4)</cell><cell>77.5</cell><cell>76.1</cell><cell>127.2</cell><cell>23.3 @ 1080 Ti, 5.7 @ 1050 Ti</cell><cell>23.3</cell></row><row><cell cols="2">Our method</cell><cell cols="2">SegBlocks-RN50 (? =0.2) SwiftNet-RN18 (baseline, our impl.) SegBlocks-RN18 (? =0.4) SegBlocks-RN18 (? =0.2) SwiftNet-EffL1 (baseline, our impl.)</cell><cell>76.2 76.2 76.3 75.9 76.6</cell><cell>74.4 74.6</cell><cell>88.5 104.1 60.5 43.5 24.9</cell><cell>30.0 @ 1080 Ti, 7.3 @ 1050 Ti 38.4 @ 1080 Ti, 9.9 @ 1050 Ti 48.6 @ 1080 Ti, 13.5 @ 1050 Ti 57.8 @ 1080 Ti, 16.8 @ 1050 Ti 17.4 @ 1080 Ti, 7.1 @ 1050 Ti</cell><cell>30.0 38.4 48.6 57.8 17.4</cell></row><row><cell></cell><cell></cell><cell cols="2">SegBlocks-EffL1 (? =0.4)</cell><cell>76.3</cell><cell>74.1</cell><cell>15.6</cell><cell>30.4 @ 1080 Ti, 8.4 @ 1050 Ti</cell><cell>30.4</cell></row><row><cell></cell><cell></cell><cell cols="2">SegBlocks-EffL1 (? =0.2)</cell><cell>75.3</cell><cell></cell><cell>11.4</cell><cell>35.6 @ 1080 Ti, 10.6 @ 1050 Ti</cell><cell>35.6</cell></row><row><cell>Dynamic</cell><cell>networks</cell><cell cols="2">Patch Proposal Network (AAAI2020) [21] Huang et al. (MVA2019) [20] Wu et al. [22] Learning Downsampling (ICCV2019) [51] HyperSeg-M [63]</cell><cell>75.2 76.4 72.9 65.0 76.2</cell><cell>----75.8</cell><cell>---34 7.5</cell><cell>24 @ 1080 Ti 1.8 @ 1080 Ti 15 @ 980 Ti -36.9 @ 1080 Ti</cell><cell>24 1.8 33 -36.9</cell></row><row><cell></cell><cell></cell><cell cols="2">Stochastic Sampling [55]</cell><cell>80.6</cell><cell>-</cell><cell>373.2</cell><cell>no GPU implementation</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell cols="2">ShelfNet18-lw (CVPR2019) [35]</cell><cell>-</cell><cell>74.8</cell><cell>95</cell><cell>36.9 @ 1080 Ti</cell><cell>36.9</cell></row><row><cell>Efficient</cell><cell>static networks</cell><cell cols="2">SFNet (ResNet-18) (ECCV2020) [35] SwiftNet-RN18 (CVPR2019) [43] ESPNetV2 (CVPR2019) [37] DABNet (BMVC2019) [36] BiSeNet-RN18 (ECCV2018) [33] BiSeNetV2-RN18 (IJCV2021) [64]</cell><cell>-75.6 66.4 70.1 74.8 75.8</cell><cell>78.9 75.5 66.2 -74.7 75.3</cell><cell>247 104.0 2.7 -67 118.5</cell><cell>26 @ 1080 Ti 39.9 @ 1080 Ti -27.7 @ 1080 Ti 65.5 @ Titan Xp 47.3 @ 1080 Ti</cell><cell>26 39.9 -27.7 58.5 47.3</cell></row><row><cell></cell><cell></cell><cell cols="2">ICNet (ECCV2018) [10]</cell><cell>-</cell><cell>69.5</cell><cell>30</cell><cell>30.3 @ Titan X</cell><cell>49.7</cell></row><row><cell></cell><cell></cell><cell cols="2">GUNet (BMVC2018) [34]</cell><cell>-</cell><cell>70.4</cell><cell>-</cell><cell>33.3 @ Titan Xp</cell><cell>29.7</cell></row><row><cell></cell><cell></cell><cell cols="2">ERFNet (TITS2017) [11]</cell><cell>72.7</cell><cell>69.7</cell><cell>27.7</cell><cell>11.2 @ Titan X</cell><cell>18.4</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="6">road side-buil-wall fence pole traffic traffic vege-terrain sky per-rider car truck bus train motor-bicycle mIoU</cell></row><row><cell></cell><cell></cell><cell></cell><cell>walk ding</cell><cell cols="2">light sign tation</cell><cell>son</cell><cell>cycle</cell></row><row><cell cols="3">SwiftNet-RN18</cell><cell cols="3">97.8 82.9 91.8 50.9 56.2 63.0 69.0 78.1 92.2 61.4</cell><cell cols="2">94.7 80.5 60.0 94.8 78.3 84.3 75.0 6.5</cell><cell>76.3</cell><cell>76.2</cell></row><row><cell cols="3">(2048?1024)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">SwiftNet-RN18</cell><cell cols="3">97.8 83.0 91.5 54.1 52.8 60.5 65.5 76.4 91.9 62.1</cell><cell cols="2">94.6 79.2 58.2 94.2 75.5 80.5 69.6 57.6</cell><cell>74.9</cell><cell>74.7</cell></row><row><cell cols="3">(1536?768)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="9">SegBlocks-RN18 .7</cell><cell>76.3</cell></row><row><cell cols="3">(? =0.4)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="5">% high-res pixels 8.5 45.8 64.0 67.0 88.5 86.3 85.6 82.2 59.7 68.2</cell><cell cols="2">36.7 93.3 96.2 66.2 61.7 68.9 85.0 90.9</cell><cell>93.2</cell><cell>-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE 3</head><label>3</label><figDesc>Memory analysis for SegBlocks-RN18 running in 16-bit floating point, as reported by PyTorch. By storing some image areas at lower resolution, SegBlocks can reduce the memory usage, especially when using larger batch sizes.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>100</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>80</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>images</cell><cell>60</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>#</cell><cell>40</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>model</cell><cell>1</cell><cell>Batch size 2</cell><cell>4</cell><cell>20</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>SwiftNet-RN18 (static) SegBlocks-RN18 (? = 0.4) SegBlocks-RN18 (? = 0.2)</cell><cell>480 MB 571 MB 340 MB</cell><cell>880 MB 901 MB 721 MB</cell><cell>1681 MB 1586 MB 940 MB</cell><cell>0</cell><cell>0.20</cell><cell>0.25</cell><cell>0.30 Percentage of high-res blocks 0.35 0.40 0.45</cell><cell>0.50</cell><cell>0.55</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="7">Fig. 8. Percentage of high-resolution blocks per image, as a histogram</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="7">over 500 Cityscapes validation images, for SegBlocks-RN18 (? =0.4).</cell></row></table><note>blocks, changed by training for different values of hy- perparameter ? ? {0.2, 0.4, 0.6}. Our SegBlocks methods consistently outperform the static baseline networks, show- ing that dynamic resolution processing is more beneficial than sampling all regions at lower resolution. For instance, SegBlocks-RN18 with ? =0.4 reduces the computational complexity of the baseline network by 40%, without de-</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE 4</head><label>4</label><figDesc>Results on CamVid's test set for semantic segmentation<ref type="bibr" target="#b62">[62]</ref>. All methods are pretrained on ImageNet<ref type="bibr" target="#b44">[45]</ref>, without Cityscapes pretraining<ref type="bibr" target="#b0">[1]</ref>. Results annotated with ? are determined using open-source implementations. FPS measured on Nivida GTX 1080 Ti.</figDesc><table><row><cell>Method</cell><cell>mIoU</cell><cell>GMACs</cell><cell>FPS</cell></row><row><cell>SwiftNet-EffL1 [43] (our impl.)</cell><cell>75.1</cell><cell>13.4</cell><cell>48</cell></row><row><cell>SegBlocks-EffL1 (? = 0.4)</cell><cell>74.6 (-0.5%)</cell><cell>9.1 (-32%)</cell><cell>58 (+20%)</cell></row><row><cell>SegBlocks-EffL1 (? = 0.2)</cell><cell>73.4 (-1.7%)</cell><cell>6.6 (-50%)</cell><cell>64 (+33%)</cell></row><row><cell>BiSeNet [33]</cell><cell>68.7</cell><cell>32.4  ?</cell><cell>116</cell></row><row><cell>BiSeNetV2</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE 5</head><label>5</label><figDesc>Mapillary Vistas validation results. Results with ? are reported in<ref type="bibr" target="#b67">[67]</ref> and benchmark FPS with an Nvidia GTX2080 Ti GPU. Other results are benchmarked on an Nvidia GTX1080 Ti.</figDesc><table><row><cell>Network</cell><cell>Input size</cell><cell>FPS</cell><cell>GMACs</cell><cell>mIoU(%)</cell></row><row><cell>SwiftNet-EffL1 (our impl.)</cell><cell>1536?1152</cell><cell>20.7</cell><cell>21.5</cell><cell>41.7</cell></row><row><cell>SegBlocks-EffL1 (? = 0.4)</cell><cell>1536?1152</cell><cell>33.0</cell><cell>13.7</cell><cell>40.5</cell></row><row><cell>SegBlocks-EffL1 (? = 0.2)</cell><cell>1536?1152</cell><cell>40.1</cell><cell>11.5</cell><cell>39.8</cell></row><row><cell>AGLNet [68]</cell><cell>2048?1024</cell><cell>53.0</cell><cell>24.1G</cell><cell>30.7</cell></row><row><cell>WFDCNet [67]</cell><cell>2048?1024</cell><cell>53.1  ?</cell><cell>12.5G</cell><cell>30.5</cell></row><row><cell>FPENet [69]</cell><cell>2048?1024</cell><cell>92.5  ?</cell><cell>3.1G</cell><cell>28.3</cell></row><row><cell>DABNet [36]</cell><cell>2048?1024</cell><cell>78.3  ?</cell><cell>20.9G</cell><cell>29.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE 6</head><label>6</label><figDesc>Time profiling of block modules, as total time taken during the inference of 250 validation images on an Nvidia GTX 1080 Ti GPU.</figDesc><table><row><cell></cell><cell></cell><cell cols="3">SegBlocks-RN50</cell><cell>SegBlocks -RN18</cell></row><row><cell></cell><cell></cell><cell>? =0.4</cell><cell></cell><cell>? =0.2</cell><cell>? =0.4</cell><cell>? =0.2</cell></row><row><cell>mIoU</cell><cell></cell><cell>77.5%</cell><cell></cell><cell>76.2%</cell><cell>76.3%</cell><cell>75.9%</cell></row><row><cell>GMACs</cell><cell></cell><cell>127.2</cell><cell></cell><cell>88.6</cell><cell>60.5</cell><cell>43.5</cell></row><row><cell>policy GMACs</cell><cell></cell><cell>0.34</cell><cell></cell><cell>0.34</cell><cell>0.34</cell><cell>0.34</cell></row><row><cell>Runtime</cell><cell></cell><cell>11.6s</cell><cell></cell><cell>9.3s</cell><cell>5.8s</cell><cell>4.9s</cell></row><row><cell>Policy Net</cell><cell cols="2">0.04s (&lt;1%)</cell><cell cols="3">0.04s (&lt;1%)</cell><cell>0.04s (&lt;1%)</cell><cell>0.04s (&lt;1%)</cell></row><row><cell>BlockSample</cell><cell cols="2">0.12s (1%)</cell><cell cols="3">0.13s (1%)</cell><cell>0.11s (2%)</cell><cell>0.12s (2%)</cell></row><row><cell>BlockPad</cell><cell cols="2">0.75s (6%)</cell><cell cols="3">0.61s (7%)</cell><cell>0.68s (12%)</cell><cell>0.56s (11%)</cell></row><row><cell>BlockCombine</cell><cell cols="2">0.04s (&lt;1%)</cell><cell cols="3">0.03s (&lt;1%)</cell><cell>0.02s (&lt;1%)</cell><cell>0.02s (&lt;1%)</cell></row><row><cell></cell><cell></cell><cell>76.0</cell><cell></cell><cell></cell></row><row><cell></cell><cell>accuracy (mIoU)</cell><cell>74.0 74.5 75.0 75.5 73.5</cell><cell></cell><cell cols="2">reinforcement learning random</cell></row><row><cell></cell><cell></cell><cell>73.0</cell><cell></cell><cell cols="2">heuristic</cell></row><row><cell></cell><cell></cell><cell>50</cell><cell>60</cell><cell>70</cell><cell>80</cell><cell>90 100</cell></row><row><cell></cell><cell></cell><cell cols="4">Multiply-Accumulates ?10 9 (GMAC)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE 7</head><label>7</label><figDesc>Policy network ablation: adjusting the input resolution, number of layers and number of features for the policy network.</figDesc><table><row><cell>Policy input res.</cell><cell cols="2"># Layers</cell><cell># Features</cell><cell>mIoU</cell><cell>Policy GMACs</cell></row><row><cell>128?64</cell><cell>2</cell><cell></cell><cell>64</cell><cell>75.3</cell><cell>0.01</cell></row><row><cell>256?128</cell><cell>3</cell><cell></cell><cell>64</cell><cell>75.5</cell><cell>0.07</cell></row><row><cell>512?256</cell><cell>4</cell><cell></cell><cell>64</cell><cell>76.3</cell><cell>0.34</cell></row><row><cell>1024?512</cell><cell>5</cell><cell></cell><cell>64</cell><cell>76.1</cell><cell>1.51</cell></row><row><cell>512?256</cell><cell>4</cell><cell></cell><cell>32</cell><cell>76.2</cell><cell>0.11</cell></row><row><cell>512?256</cell><cell>4</cell><cell></cell><cell>64</cell><cell>76.3</cell><cell>0.34</cell></row><row><cell>512?256</cell><cell>4</cell><cell></cell><cell>128</cell><cell>76.1</cell><cell>1.21</cell></row><row><cell></cell><cell cols="5">0.8 Validation mIoU progress during training</cell></row><row><cell></cell><cell>0.6</cell><cell></cell><cell></cell><cell></cell></row><row><cell>mIoU</cell><cell>0.4</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.2</cell><cell></cell><cell cols="2">SegBlocks-RN18 (ours)</cell></row><row><cell></cell><cell>0.0</cell><cell></cell><cell cols="2">SwiftNet-RN18</cell></row><row><cell></cell><cell>0</cell><cell cols="3">25000 50000 75000 100000 training iteration</cell></row><row><cell cols="6">Fig. 11. Training progress given by validation mIoU for non-dynamic</cell></row><row><cell cols="6">baseline (SwiftNet-RN18) and our dynamic method (SegBlocks-RN18).</cell></row><row><cell cols="6">Learning the policy does not significantly impact the training progress.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE 8</head><label>8</label><figDesc>Block size comparison for SegBlocks-RN18 with ? =0.4 on the Cityscapes validation set .Fig. 12. Granularity of blocks of 64, 128, and 256 pixels on Cityscapes' images of 2048?1024 pixels.</figDesc><table><row><cell>Block size</cell><cell>mIoU</cell><cell>GMACs</cell><cell>FPS (1050 Ti)</cell></row><row><cell>64?64</cell><cell>76.3%</cell><cell>59.3</cell><cell>11.3</cell></row><row><cell>128?128</cell><cell>76.3%</cell><cell>60.5</cell><cell>13.5</cell></row><row><cell>256?256</cell><cell>75.2%</cell><cell>56.4</cell><cell>14.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>TABLE 9</head><label>9</label><figDesc>Comparison of BlockPad sampling patterns and zero-padding for SegBlocks-RN18 (? =0.4)</figDesc><table><row><cell cols="2">Method</cell><cell>mIoU</cell><cell>Runtime</cell><cell>BlockPad time</cell></row><row><cell cols="2">Avg-sampling</cell><cell>76.3%</cell><cell>5.8 s</cell><cell>0.68 s (12%)</cell></row><row><cell cols="2">Strided sampling</cell><cell>75.6%</cell><cell>5.7 s</cell><cell>0.63 s (11%)</cell></row><row><cell cols="2">Zero-padding</cell><cell>70.2%</cell><cell>5.2 s</cell><cell>N.A. (integrated in conv.)</cell></row><row><cell></cell><cell>2.25</cell><cell></cell><cell></cell></row><row><cell></cell><cell>2.00</cell><cell></cell><cell></cell></row><row><cell>Speedup</cell><cell>1.25 1.50 1.75</cell><cell cols="2">Practical Theoretical</cell></row><row><cell></cell><cell>1.00</cell><cell></cell><cell></cell></row><row><cell></cell><cell>4 8 16</cell><cell>32</cell><cell>64</cell></row><row><cell></cell><cell cols="2">Block size</cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The Cityscapes Dataset for Semantic Urban Scene Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting><address><addrLine>Las Vegas, NV, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016-06" />
			<biblScope unit="page" from="3213" to="3223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Skin lesion analysis toward melanoma detection: A challenge at the 2017 international symposium on niomedical imaging (isbi)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">C</forename><surname>Codella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gutman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Celebi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Helba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Marchetti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">W</forename><surname>Dusza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kalloo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Liopyris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kittler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">15th Int. Symp. on Biomedical Imaging</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="168" to="172" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">DeepGlobe 2018: A Challenge to Parse the Earth through Satellite Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Demir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Koperski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lindenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Basu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hughes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tuia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Raskar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Computer Vision and Pattern Recognition Workshops (CVPRW)</title>
		<meeting>Conf. Computer Vision and Pattern Recognition Workshops (CVPRW)<address><addrLine>Salt Lake City, UT</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018-06" />
			<biblScope unit="page" from="172" to="189" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Can semantic labeling methods generalize to any city? the inria aerial image labeling benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Maggiori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tarabalka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Charpiat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Alliez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Geoscience and Remote Sensing Symp. (IGARSS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017-07" />
			<biblScope unit="page" from="3226" to="3229" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="834" to="848" />
			<date type="published" when="2018-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">An Analysis of Deep Neural Network Models for Practical Applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Canziani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Culurciello</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.07678</idno>
		<imprint>
			<date type="published" when="2017-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Efficient processing of deep neural networks: A tutorial and survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Emer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE</title>
		<meeting>IEEE</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">105</biblScope>
			<biblScope unit="page" from="2295" to="2329" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">MobileNetV2: Inverted Residuals and Linear Bottlenecks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conf. on Computer Vision and Pattern Recognition (CVPR). Salt Lake City</title>
		<meeting><address><addrLine>UT</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018-06" />
			<biblScope unit="page" from="4510" to="4520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Shufflenet v2: Practical guidelines for efficient cnn architecture design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-T</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conf. on Computer Vision (ECCV)</title>
		<meeting>European Conf. on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="116" to="131" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">ICNet for Real-Time Semantic Segmentation on High-Resolution Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conf. Computer Vision (ECCV)</title>
		<editor>V. Ferrari, M. Hebert, C. Sminchisescu, and Y. Weiss</editor>
		<meeting>European Conf. Computer Vision (ECCV)</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">11207</biblScope>
			<biblScope unit="page" from="418" to="434" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">ERFNet: Efficient Residual Factorized ConvNet for Real-Time Semantic Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Romera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>?lvarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">M</forename><surname>Bergasa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Arroyo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="263" to="272" />
			<date type="published" when="2018-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Optimal Brain Damage</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Solla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>2, D. S. Touretzky</editor>
		<imprint>
			<publisher>Morgan-Kaufmann</publisher>
			<date type="published" when="1990" />
			<biblScope unit="page" from="598" to="605" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Deep Learning and Representation Learning Workshop</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Quantized Convolutional Neural Networks for Mobile Devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Leng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting><address><addrLine>Las Vegas, NV, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016-06" />
			<biblScope unit="page" from="4820" to="4828" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">SkipNet: Learning Dynamic Routing in Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-Y</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">E</forename><surname>Gonzalez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conf. on Computer Vision (ECCV)</title>
		<editor>V. Ferrari, M. Hebert, C. Sminchisescu, and Y. Weiss</editor>
		<meeting>European Conf. on Computer Vision (ECCV)</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">11217</biblScope>
			<biblScope unit="page" from="420" to="436" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Convolutional networks with adaptive inference graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Veit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conf. on Computer Vision (ECCV)</title>
		<meeting>European Conf. on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">BlockDrop: Dynamic Inference Paths in Residual Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Nagarajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rennie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Feris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conf. Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting><address><addrLine>Salt Lake City, UT</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018-06" />
			<biblScope unit="page" from="8817" to="8826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Batch-Shaping for Learning Conditional Channel Gated Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">E</forename><surname>Bejnordi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Blankevoort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.06627</idno>
		<imprint>
			<date type="published" when="2020-04" />
		</imprint>
	</monogr>
	<note>cs, stat</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Dynamic Convolutions: Exploiting Spatial Sparsity for Faster Inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Verelst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conf. Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting><address><addrLine>Seattle, WA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020-06" />
			<biblScope unit="page" from="2317" to="2326" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Uncertainty based model selection for fast semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Proesmans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Georgoulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">16th Int. Conf. on Machine Vision Applications (MVA)</title>
		<meeting><address><addrLine>Tokyo, Japan</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019-05" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Patch Proposal Network for Fast Semantic Segmentation of High-Resolution Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI Conf. on Artificial Intelligence</title>
		<meeting>AAAI Conf. on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020-04" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="12" to="402" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">V D</forename><surname>Hengel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.00213</idno>
		<title level="m">Real-time Semantic Image Segmentation via Spatial Sparsity</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Not All Pixels Are Equal: Difficulty-Aware Semantic Segmentation via Deep Layer Cascade</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting><address><addrLine>Honolulu, HI</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017-07" />
			<biblScope unit="page" from="6459" to="6468" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Spatially Adaptive Computation Time for Residual Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Figurnov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Vetrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting><address><addrLine>Honolulu, HI</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017-07" />
			<biblScope unit="page" from="1790" to="1799" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Dynamic Block Sparse Reparameterization of Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">T</forename><surname>Vooturi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Varma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kothapalli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. on Computer Vision Workshop (ICCVW)</title>
		<meeting>Int. Conf. on Computer Vision Workshop (ICCVW)<address><addrLine>Seoul, Korea (South</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019-10" />
			<biblScope unit="page" from="3046" to="3053" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">SBNet: Sparse Blocks Network for Fast Inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pokrovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. on Computer Vision and Pattern Recognition (CVPR). Salt Lake City</title>
		<meeting>Conf. on Computer Vision and Pattern Recognition (CVPR). Salt Lake City<address><addrLine>UT</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018-06" />
			<biblScope unit="page" from="8711" to="8720" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Computation on sparse neural networks: an inspiration for future hardware</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.11946</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Segblocks: Towards block-based adaptive resolution networks for fast segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Verelst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conf. Computer Vision Workshop</title>
		<meeting>European Conf. Computer Vision Workshop<address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="18" to="22" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Pyramid Scene Parsing Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting><address><addrLine>Honolulu, HI</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017-07" />
			<biblScope unit="page" from="6230" to="6239" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2481" to="2495" />
			<date type="published" when="2017-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Multi-scale context aggregation by dilated convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.07122</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">The Mapillary Vistas Dataset for Semantic Understanding of Street Scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Neuhold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ollmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Bulo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kontschieder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. on Computer Vision (ICCV)</title>
		<meeting><address><addrLine>Venice</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017-10" />
			<biblScope unit="page" from="5000" to="5009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">BiSeNet: Bilateral Segmentation Network for Real-Time Semantic Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conf. on Computer Vision (ECCV)</title>
		<editor>V. Ferrari, M. Hebert, C. Sminchisescu, and Y. Weiss</editor>
		<meeting>European Conf. on Computer Vision (ECCV)</meeting>
		<imprint>
			<publisher>Springer Int. Publishing</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">11217</biblScope>
			<biblScope unit="page" from="334" to="349" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Guided upsampling network for real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mazzini</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.07466</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Shelfnet for fast semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dvornek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. on Computer Vision Workshops (ICCVW)</title>
		<meeting>Int. Conf. on Computer Vision Workshops (ICCVW)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Dabnet: Depth-wise asymmetric bottleneck for real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11357</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Espnetv2: A light-weight, power efficient, and general purpose convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rastegari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shapiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hajishirzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>Conf. Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9190" to="9200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Ccnet: Criss-cross attention for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. on Computer Vision (ICCV)</title>
		<meeting>Int. Conf. on Computer Vision (ICCV)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="603" to="612" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Dynamic attention network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-Y</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">384</biblScope>
			<biblScope unit="page" from="182" to="191" />
			<date type="published" when="2020-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Asymmetric Non-Local Neural Networks for Semantic Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. on Computer Vision (ICCV). Seoul</title>
		<meeting>Int. Conf. on Computer Vision (ICCV). Seoul<address><addrLine>Korea (South</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019-10" />
			<biblScope unit="page" from="593" to="602" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Real-time semantic segmentation with fast attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">C</forename><surname>Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sclaroff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Robotics and Automation Letters</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="263" to="270" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.07971</idno>
		<title level="m">Non-local Neural Networks</title>
		<imprint>
			<date type="published" when="2018-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">In Defense of Pre-Trained ImageNet Architectures for Real-Time Semantic Segmentation of Road-Driving Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Orsic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kreso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bevandic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Segvic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019-06" />
			<biblScope unit="page" from="12" to="599" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Deep Residual Learning for Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting><address><addrLine>Las Vegas, NV, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">ImageNet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009-06" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2015-09" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="1904" to="1916" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Efficientnet: Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. on Machine Learning (ICML)</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6105" to="6114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Low-rank matrix factorization for Deep Neural Network training with high-dimensional output targets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kingsbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sindhwani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Arisoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ramabhadran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="6655" to="6659" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1405.3866</idno>
		<title level="m">Speeding up Convolutional Neural Networks with Low Rank Expansions</title>
		<imprint>
			<date type="published" when="2014-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Learning to zoom: a saliency-based sampling layer for neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Recasens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kellnhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Stent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Matusik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conf. on Computer Vision (ECCV)</title>
		<meeting>European Conf. on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="51" to="66" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Efficient Segmentation: Learning Downsampling Near Semantic Boundaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Marin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vajda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Chatterjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Boykov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. on Computer Vision (ICCV)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Korea</forename><surname>Seoul</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019-10" />
			<publisher>IEEE</publisher>
			<biblScope unit="page" from="2131" to="2141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-L</forename><surname>Bacon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pineau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Precup</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06297</idno>
		<title level="m">Conditional Computation in Neural Networks for faster models</title>
		<imprint>
			<date type="published" when="2016-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Estimating or Propagating Gradients Through Stochastic Neurons for Conditional Computation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>L?onard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1308.3432</idno>
		<imprint>
			<date type="published" when="2013-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1305.0445</idno>
		<title level="m">Deep Learning of Representations: Looking Forward</title>
		<imprint>
			<date type="published" when="2013-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Spatially adaptive inference with stochastic feature sampling and interpolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conf. on Computer Vision (ECCV)</title>
		<meeting>European Conf. on Computer Vision (ECCV)</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="531" to="548" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Perforatedcnns: Acceleration through elimination of redundant convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Figurnov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ibraimova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Vetrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="947" to="955" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Channel gating neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">M</forename><surname>De Sa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Suh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1886" to="1896" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Categorical Reparameterization with Gumbel-Softmax</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Poole</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Learning Representations (ICLR</title>
		<meeting>Int. Conf. Learning Representations (ICLR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">The Concrete Distribution: A Continuous Relaxation of Discrete Random Variables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Maddison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Learning Representations</title>
		<meeting>Int. Conf. Learning Representations</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Reinforcement learning: an introduction, ser. Adaptive computation and machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Barto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge, Mass</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Simple statistical gradient-following algorithms for connectionist reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="229" to="256" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Semantic object classes in video: A high-definition ground truth database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Brostow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fauqueur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="88" to="97" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Hyperseg: Patch-wise hypernetwork for real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Nirkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hassner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="4061" to="4070" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Bisenet v2: Bilateral network with guided aggregation for real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="page" from="1" to="18" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6596</idno>
		<title level="m">Training deep neural networks on noisy labels with bootstrapping</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Semantic flow for fast and accurate scene parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conf. on Computer Vision (ECCV)</title>
		<meeting>European Conf. on Computer Vision (ECCV)</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Real-time semantic segmentation with weighted factorized-depthwise convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">114</biblScope>
			<biblScope unit="page">104269</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Aglnet: Towards real-time semantic segmentation of selfdriving images via attention-guided lightweight network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Latecki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Soft Computing</title>
		<imprint>
			<biblScope unit="volume">96</biblScope>
			<biblScope unit="page">106682</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title level="m" type="main">Feature pyramid encoding network for real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.08599</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">He received a M.Sc. degree in Electrical Engineering at the same university in 2018. His research focuses on efficient and dynamic network architectures for classification, pose estimation and segmentation tasks</title>
	</analytic>
	<monogr>
		<title level="m">Thomas Verelst is a Ph.D. student at the Center for Processing Speech and Images (PSI) of KU Leuven university (Belgium)</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
