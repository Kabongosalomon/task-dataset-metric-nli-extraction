<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Single-Image HDR Reconstruction by Learning to Reverse the Camera Pipeline</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Lun</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">National Taiwan University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">MediaTek Inc</orgName>
								<address>
									<addrLine>3 Google 4 UC</addrLine>
									<settlement>Merced</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Sheng</forename><surname>Lai</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Sheng</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">National Taiwan University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Lung</forename><surname>Kao</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">National Taiwan University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yung-Yu</forename><surname>Chuang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">National Taiwan University</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Virginia Tech</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
						</author>
						<title level="a" type="main">Single-Image HDR Reconstruction by Learning to Reverse the Camera Pipeline</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>*Indicates equal contribution.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T16:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input LDR images</head><p>Our results <ref type="figure">Figure 1</ref>: HDR reconstruction from a single LDR image. Our method recovers missing details for both backlit and overexposed regions of real-world images by learning to reverse the camera pipeline. Note that the input LDR images are captured by different real cameras, and all reconstructed HDR images have been tone-mapped by [32] for display.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>Recovering a high dynamic range (HDR) image from a single low dynamic range (LDR) input image is challenging due to missing details in under-/over-exposed regions caused by quantization and saturation of camera sensors. In contrast to existing learning-based methods, our core idea is to incorporate the domain knowledge of the LDR image formation pipeline into our model. We model the HDRto-LDR image formation pipeline as the (1) dynamic range clipping, (2) non-linear mapping from a camera response function, and (3) quantization. We then propose to learn three specialized CNNs to reverse these steps. By decomposing the problem into specific sub-tasks, we impose effective physical constraints to facilitate the training of individual sub-networks. Finally, we jointly fine-tune the entire model end-to-end to reduce error accumulation. With extensive quantitative and qualitative experiments on diverse image datasets, we demonstrate that the proposed method performs favorably against state-of-the-art single-image HDR reconstruction algorithms.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>HDR images are capable of capturing rich real-world scene appearances including lighting, contrast, and details. Consumer-grade digital cameras, however, can only capture images within a limited dynamic range due to sensor constraints. The most common approach to generate HDR images is to merge multiple LDR images captured with different exposures <ref type="bibr" target="#b11">[12]</ref>. Such a technique performs well on static scenes but often suffers from ghosting artifacts on dynamic scenes or hand-held cameras. Furthermore, capturing multiple images of the same scene may not always be feasible (e.g., existing LDR images on the Internet).</p><p>Single-image HDR reconstruction aims to recover an HDR image from a single LDR input. The problem is challenging due to the missing information in under-/overexposed regions. Recently, several methods <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b55">56]</ref> have been developed to reconstruct an HDR image from a given LDR input using deep convolutional neural networks (CNNs). However, learning a direct LDR-to-HDR mapping is difficult as the variation of HDR pixels <ref type="bibr">(32-bit)</ref> is significantly higher than that of LDR pixels <ref type="bibr">(8-bit)</ref>. Recent methods address this challenge either by focusing on recovering the over-exposed regions <ref type="bibr" target="#b13">[14]</ref> or synthesizing several up-/down-exposed LDR images and fusing them to produce an HDR image <ref type="bibr" target="#b14">[15]</ref>. The artifacts induced by quantization and inaccurate camera response functions (CRFs) are, however, only implicitly addressed through learning.</p><p>In this work, we incorporate the domain knowledge of the LDR image formation pipeline to design our model. We model the image formation with the following steps <ref type="bibr" target="#b11">[12]</ref>: (1) dynamic range clipping, (2) non-linear mapping with a CRF, and (3) quantization. Instead of learning a direct LDR-to-HDR mapping using a generic network, our core idea is to decompose the single-image HDR reconstruction problem into three sub-tasks: i) dequantization, ii) linearization, and iii) hallucination, and develop three deep networks to specifically tackle each of the tasks. First, given an input LDR image, we apply a Dequantization-Net to restore the missing details caused by quantization and reduce the visual artifacts in the under-exposed regions (e.g., banding artifacts). Second, we estimate an inverse CRF with a Linearization-Net and convert the non-linear LDR image to a linear image (i.e., scene irradiance). Building upon the empirical model of CRFs <ref type="bibr" target="#b15">[16]</ref>, our Linearization-Net leverages the additional cues from edges, the intensity histogram and a monotonically increasing constraint to estimate more accurate CRFs. Third, we predict the missing content in the over-exposed regions with a Hallucination-Net. To handle other complicated operations (e.g., lens shading correction, sharpening) in modern camera pipelines that we do not model, we use a Refinement-Net and jointly fine-tune the whole model end-to-end to reduce error accumulation and improve the generalization ability to real input images.</p><p>By explicitly modeling the inverse functions of the LDR image formation pipeline, we significantly reduce the difficulty of training one single network for reconstructing HDR images. We evaluate the effectiveness of our method on four datasets and real-world LDR images. Extensive quantitative and qualitative evaluations, as well as the user study, demonstrate that our model performs favorably against the state-of-the-art single-image HDR reconstruction methods. <ref type="figure">Figure 1</ref> shows our method recovers visually pleasing results with faithful details. Our contributions are three-fold:</p><p>? We tackle the single-image HDR reconstruction problem by reversing image formation pipeline, including the dequantization, linearization, and hallucination. ? We introduce specific physical constraints, features, and loss functions for training each individual network. ? We collect two HDR image datasets, one with synthetic LDR images and the other with real LDR images, for training and evaluation. We show that our method performs favorably against the state-of-the-art methods in terms of the HDR-VDP-2 scores and visual quality on the collected and existing datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Multi-image HDR reconstruction. The most common technique for creating HDR images is to fuse a stack of bracketed exposure LDR images <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b37">38]</ref>. To handle dynamic scenes, image alignment and post-processing are required to minimize artifacts <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b49">50]</ref>. Recent methods apply CNNs to fuse multiple flow-aligned LDR images <ref type="bibr" target="#b22">[23]</ref> or unaligned LDR images <ref type="bibr" target="#b51">[52]</ref>. In contrast, we focus on reconstructing an HDR image from a single LDR image.</p><p>Single-image HDR reconstruction. Single-image HDR reconstruction does not suffer from ghosting artifacts but is significantly more challenging than the multi-exposure counterpart. Early approaches estimate the density of light sources to expand the dynamic range <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5]</ref> or apply the cross-bilateral filter to enhance the input LDR images <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b26">27]</ref>. With the advances of deep CNNs <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b47">48]</ref>, several methods have been developed to learn a direct LDRto-HDR mapping <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b55">56]</ref>. Eilertsen et al. <ref type="bibr" target="#b13">[14]</ref> propose the HDRCNN method that focuses on recovering missing details in the over-exposed regions while ignoring the quantization artifacts in the under-exposed areas. In addition, a fixed inverse CRF is applied, which may not be applicable to images captured from different cameras. Instead of learning a direct LDR-to-HDR mapping, some recent methods <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b29">30]</ref> learn to synthesize multiple LDR images with different exposures and reconstruct the HDR image using the conventional multi-image technique <ref type="bibr" target="#b11">[12]</ref>. However, predicting LDR images with different exposures from a single LDR input itself is challenging as it involves the non-linear CRF mapping, dequantization, and hallucination. Unlike <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b29">30]</ref>, our method directly reconstructs an HDR image by modeling the inverse process of the image formation pipeline. <ref type="figure">Figure 2</ref> illustrates the LDR image formation pipeline, state-of-the-art single-image HDR reconstruction approaches <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b39">40]</ref>, and the proposed method.</p><p>Dequantization and decontouring. When converting realvalued HDR images to 8-bit LDR images, quantization errors inevitably occurs. They often cause scattered noise or introduce false edges (known as contouring or banding artifacts) particularly in regions with smooth gradient changes. While these errors may not be visible in the non-linear LDR image, the tone mapping operation (for visualizing an HDR image) often aggravates them, resulting in noticeable artifacts. Existing decontouring methods smooth images by applying the adaptive spatial filter <ref type="bibr" target="#b8">[9]</ref> or selective average filter <ref type="bibr" target="#b48">[49]</ref>. However, these methods often involve meticulously tuned parameters and often produce undesirable artifacts in textured regions. CNN-based methods have also been proposed <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b57">58]</ref>. Their focus is on restoring an 8bit image from lower bit-depth input (e.g., 2-bit or 4-bit). In contrast, we aim at recovering a 32-bit floating-point image from an 8-bit LDR input image.</p><p>Radiometric calibration. As the goal of HDR reconstruction is to measure the full scene irradiance from an input LDR image, it is necessary to estimate the CRF. Recovering the CRF from a single image requires certain assumptions of statistical priors, e.g., color mixtures at edges <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b42">43]</ref> or noise distribution <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b50">51]</ref>. Nevertheless, these priors may not be applicable to a wide variety of images in the wild. A CRF can be empirically modeled by the basis vectors extracted from a set of real-world CRFs <ref type="bibr" target="#b15">[16]</ref> via the principal component analysis (PCA). Li and Peers <ref type="bibr" target="#b30">[31]</ref> train a CRF-Net to estimate the weights of the basis vectors from a single input image and then use the principal components to reconstruct the CRF. Our work improves upon <ref type="bibr" target="#b30">[31]</ref> by introducing new features and monotonically increasing constraint. We show that an accurate CRF is crucial to the quality of the reconstructed HDR image. After obtaining an accurate HDR image, users can adopt advanced tone-mapping methods (e.g., <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b45">46]</ref>) to render a more visually pleasing LDR image. Several other applications (e.g., image-based lighting <ref type="bibr" target="#b10">[11]</ref> and motion blur synthesis <ref type="bibr" target="#b11">[12]</ref>) also require linear HDR images for further editing or mapping.</p><p>Image completion. Recovering the missing contents in saturated regions can be posed as an image completion problem. Early image completion approaches synthesize the missing contents via patch-based synthesis <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b18">19</ref>]. Recently, several learning-based methods have been proposed to synthesize the missing pixels using CNNs <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b53">54]</ref>. Different from the generic image completion task, the missing pixels in the over-exposed regions always have equal or larger values than other pixels in an image. We incorporate this constraint in our Hallucination-Net to reflect the physical formation in over-exposed regions.</p><p>Camera pipeline. We follow the forward LDR image formation pipeline in HDR reconstruction <ref type="bibr" target="#b11">[12]</ref> and radiometric calibration <ref type="bibr" target="#b7">[8]</ref> algorithms. While the HDRCNN method <ref type="bibr" target="#b13">[14]</ref> also models a similar LDR image formation, this model does not learn to estimate accurate CRFs and reduce quantization artifacts. There exist more advanced and complex camera pipelines to model the demosaicing, white balancing, gamut mapping, noise reduction steps for image formation <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b25">26]</ref>. In this work, we focus on the components of great importance for HDR image reconstruction and model the rest of the pipeline by a refinement network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Learning to Reverse the Camera Pipeline</head><p>In this section, we first introduce the image formation pipeline that renders an LDR image from an HDR image (the scene irradiance) as shown in <ref type="figure">Figure 2</ref>(a). We then describe our design methodology and training procedures for single-image HDR reconstruction by reversing the image formation pipeline as shown in <ref type="figure">Figure 2</ref>  Blending Over-exposed mask !</p><formula xml:id="formula_0">" # ! LDR HDR $ $ Prediction Linear LDR Inverse CRF (fixed) % &amp;" ' ( ' ) CNN (d) HDRCNN [14]</formula><p>Linearization-Net</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dequantization-Net</head><p>Hallucination-Net Over-exposed mask</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>LDR HDR 32-bit LDR Linear LDR</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Refinement-Net</head><p>Inverse CRF (e) Proposed method <ref type="figure">Figure 2</ref>: The LDR Image formation pipeline and overview of single-image HDR reconstruction methods.</p><p>(a) We model the LDR image formation by (from right to left) dynamic range clipping, non-linear mapping, and quantization <ref type="bibr" target="#b11">[12]</ref>. (b) ExpandNet <ref type="bibr" target="#b39">[40]</ref> learns a direct mapping from LDR to HDR images. (c) DrTMO <ref type="bibr" target="#b14">[15]</ref> synthesizes multiple LDR images with different exposures and fuses them into an HDR image. (d) HDRCNN <ref type="bibr" target="#b13">[14]</ref> predicts details in over-exposed regions while ignoring the quantization errors in the under-exposed regions. (e) The proposed method explicitly learns to "reverse" each step of the LDR image formation pipeline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">LDR image formation</head><p>While the real scene irradiance has a high dynamic range, the digital sensor in cameras can only capture and store a limited extent, usually with 8 bits. Given the irradiance E and sensor exposure time t, an HDR image is recorded by H = E ? t. The process of converting one HDR image to one LDR image can be modeled by the following major steps: (1) Dynamic range clipping. The camera first clips the pixel values of an HDR image H to a limited range, which can be formulated by I c = C(H) = min (H, 1). Due to the clipping operation, there is information loss for pixels in the over-exposed regions.</p><p>(2) Non-linear mapping. To match the human perception of the scene, a camera typically applies a non-linear CRF mapping to adjust the contrast of the captured image: I n = F(I c ). A CRF is unique to the camera model and unknown in our problem setting.</p><p>(3) Quantization. After the non-linear mapping, the recorded pixel values are quantized to 8 bits by Q(I n ) = 255 ? I n + 0.5 /255. The quantization process leads to errors in the under-exposed and smooth gradient regions.</p><p>In summary, an LDR image L is formed by:</p><formula xml:id="formula_1">L = ?(H) = Q(F(C(H))) ,<label>(1)</label></formula><p>where ? denotes the pipeline of dynamic range clipping, non-linear mapping, and quantization steps.</p><p>To learn the inverse mapping ? ?1 , we propose to decompose the HDR reconstruction task into three sub-tasks: dequantization, linearization, and hallucination, which model the inverse functions of the quantization, non-linear mapping, and dynamic range clipping, respectively. We train three CNNs for the three sub-tasks using the corresponding supervisory signal and specific physical constraints. We then integrate these three networks into an end-to-end model and jointly fine-tune to further reduce error accumulation and improve the performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Dequantization</head><p>Quantization often results in scattered noise or contouring artifacts in smooth regions. Therefore, we propose to learn a Dequantization-Net to reduce the quantization artifacts in the input LDR image.</p><p>Architecture. Our Dequantization-Net adopts a 6-level U-Net architecture. Each level consists of two convolutional layers followed by a leaky ReLU (? = 0.1) layer. We use the Tanh layer to normalize the output of the last layer to [?1.0, 1.0]. Finally, we add the output of the Dequantization-Net to the input LDR image to generate the dequantized LDR image? deq .</p><p>Training. We minimize the 2 loss between the dequantized LDR image? deq and corresponding ground-truth image I n : L deq = ? deq ? I n 2 2 . Note that I n = F(C(H)) is constructed from the ground-truth HDR image with dynamic range clipping and non-linear mapping.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Linearization</head><p>The goal of linearization (i.e., radiometric calibration) is to estimate a CRF and convert a non-linear LDR image to a linear irradiance. Although the CRF (denoted by F) is distinct for each camera, all the CRFs must have the following properties. First, the function should be monotonically increasing. Second, the minimal and maximal input values should be respectively mapped to the minimal and maximal output values: F(0) = 0 and F(1) = 1 in our case. As the CRF is a one-to-one mapping function, the inverse CRF (denoted by G = F ?1 ) also has the above properties. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hard Histogram Voting Soft Histogram Voting</head><p>Bin range <ref type="figure">Figure 3</ref>: Spatial-aware soft histogram layer. We extract histogram features by soft counting on pixel intensities and preserving the spatial information.</p><p>To represent a CRF, we first discretize an inverse CRF by uniformly sampling 1024 points between [0, 1]. Therefore, an inverse CRF is represented as a 1024-dimensional vector g ? R 1024 . We then adopt the Empirical Model of Response (EMoR) model <ref type="bibr" target="#b15">[16]</ref>, which assumes that each inverse CRF g can be approximated by a linear combination of K PCA basis vectors. In this work, we set K = 11 as it has been shown to capture the variations well in the CRF dataset <ref type="bibr" target="#b30">[31]</ref>. To predict the inverse CRF, we train a Linearization-Net to estimate the weights from the input non-linear LDR image.</p><p>Input features. As the edge and color histogram have been shown effective to estimate an inverse CRF <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b33">34]</ref>, we first extract the edge and histogram features from the nonlinear LDR image. We adopt the Sobel filter to obtain the edge responses, resulting in 6 feature maps (two directions ? three color channels). To extract the histogram features, we propose a spatial-aware soft-histogram layer. Specifically, given the number of histogram bins B, we construct a soft counting of pixel intensities by:</p><formula xml:id="formula_2">h(i, j, c, b) = 1 ? d ? B , if d &lt; 1 B 0 , otherwise<label>(2)</label></formula><p>where i, j indicate horizontal and vertical pixel positions, c denotes the index of color channels, b ? {1, ? ? ? , B} is the index for the histogram bin, and d = |I(i, j, c) ? (2b ? 1)/(2B)| is the intensity distance to the center of the bth bin. Every pixel contributes to the two nearby bins according to the intensity distance to the center of each bin. <ref type="figure">Figure 3</ref> shows a 1D example of our soft-histogram layer. Our histogram layer preserves the spatial information and is fully differentiable.</p><p>Architecture. We use the ResNet-18 <ref type="bibr" target="#b16">[17]</ref> as the backbone of our Linearization-Net. To extract a global feature, we add a global average pooling layer after the last convolutional Fullyconnected layers <ref type="figure">Figure 4</ref>: Architecture of the Linearization-Net. Our Linearization-Net takes as input the non-linear LDR image, edge maps, and histogram maps, and predicts the PCA coefficients for reconstructing an inverse CRF, followed by enforcing the monotonically increasing constraint.</p><p>layer. We then use two fully-connected layers to generate K PCA weights and reconstruct an inverse CRF.</p><p>Monotonically increasing constraint. To satisfy the constraint that a CRF/inverse CRF should be monotonically increasing, we adjust the estimated inverse CRF by enforcing all the first-order derivatives to be non-negative. Specifically, we calculate the first-order derivatives by g 1 = 0 and g d = g d ? g d?1 for d ? [2, ? ? ? , 1024] and find the smallest negative derivative g m = min(min d (g d ), 0). We then shift the derivatives byg d = g d ? g m . The inverse CRF g = [g 1 , ? ? ? ,g 1024 ] is then reconstructed by integration and normalization:g</p><formula xml:id="formula_3">d = 1 1024 i=1g i d i=1g i .<label>(3)</label></formula><p>We normalizeg d to ensure the inverse CRF satisfies the constraint that G(0) = 0 and G(1) = 1. <ref type="figure">Figure 4</ref> depicts the pipeline of our Linearization-Net. With the normalized inverse CRFg, we then map the non-linear LDR image? deq to a linear LDR image? lin .</p><p>Training. We define the linear LDR image reconstruction loss by: L lin = ? lin ?I c 2 2 , where I c = C(H) is constructed from the ground-truth HDR image with the dynamic range clipping process. In addition, we formulate the inverse CRF reconstruction loss by: L crf = g ? g 2 2 , where g is the ground-truth inverse CRF. We train the Linearization-Net by optimizing L lin + ? crf L crf . We empirically set ? crf = 0.1 in all our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Hallucination</head><p>After dequantization and linearization, we aim to recover the missing contents due to dynamic range clipping. To this end, we train a Hallucination-Net (denoted by C ?1 (?)) to Reconstructed HDR Over-exposed mask Perceptual loss $ ( TV loss $ )* <ref type="figure">Figure 5</ref>: Architecture of the Hallucination-Net. We train the Hallucination-Net to predict positive residuals and recover missing content in the over-exposed regions.</p><p>predict the missing details within the over-exposed regions.</p><p>Architecture. We adopt an encoder-decoder architecture with skip connections <ref type="bibr" target="#b13">[14]</ref> as our Hallucination-Net. The reconstructed HDR image is modeled by? =? lin + ? ? C ?1 (? lin ), where? lin is the image generated from the Linearization-Net and ? = max(0,? lin ? ?)/(1 ? ?) is the over-exposed mask with ? = 0.95. Since the missing values in the over-exposed regions should always be greater than the existing pixel values, we constrain the Hallucination-Net to predict positive residuals by adding a ReLU layer at the end of the network. We note that our over-exposed mask is a soft mask where ? ? [0, 1]. The soft mask allows the network to smoothly blend the residuals with the existing pixel values around the over-exposed regions. <ref type="figure">Figure 5</ref> shows the design of our Hallucination-Net. We find that the architecture of <ref type="bibr" target="#b13">[14]</ref> may generate visible checkerboard artifacts in large over-exposed regions. In light of this, we replace the transposed convolutional layers in the decoder with the resize-convolution layers <ref type="bibr" target="#b43">[44]</ref>. Training. We train our Hallucination-Net by minimizing the log ? 2 loss: L hal = log(?) ? log(H) <ref type="bibr">2 2</ref> , where H is the ground-truth HDR image. We empirically find that training is more stable and achieves better performance when minimizing the loss in the log domain. As the highlight regions (e.g., sun and light sources) in an HDR image typically have values with orders of magnitude larger than those of other regions, the loss is easily dominated by the errors in the highlight regions when measured in the linear domain. Computing the loss in the log domain reduces the influence of these extremely large errors and encourages the network to restore more details in other regions.</p><p>To generate more realistic details, we further include the perceptual loss L p <ref type="bibr" target="#b21">[22]</ref>: As the VGG-Net (used in L p ) is trained on non-linear RGB images, directly feeding an linear HDR image to the VGG-Net may not obtain meaningful features. Therefore, we first apply a differentiable global tone-mapping operator <ref type="bibr" target="#b51">[52]</ref> to map the HDR images to a non-linear RGB space. We can then compute the perceptual loss on the tone-mapped HDR images. To improve the spatial smoothness of the predicted contents, we also minimize the total variation (TV) loss L tv on the recovered HDR image. Our total loss for training the Hallucination-Net is L hal + ? p L p + ? tv L tv . We empirically set ? p = 0.001 and ? tv = 0.1 in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Joint training</head><p>We first train the Dequantization-Net, Linearization-Net, and Hallucination-Net with the corresponding input and ground-truth data. After the three networks converge, we jointly fine-tune the entire pipeline by minimizing the combination of loss functions L total : <ref type="formula">(4)</ref> where we set the weights to ? deq = 1, ? lin = 10, ? crf = 1, ? hal = 1, ? p = 0.001, and ? tv = 0.1. The joint training reduces error accumulation between the sub-networks and further improves the reconstruction performance.</p><formula xml:id="formula_4">? deq L deq +? lin L lin +? crf L crf +? hal L hal +? p L p +? tv L tv</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.">Refinement</head><p>Modern camera pipeline contains significant amounts of spatially-varying operations, e.g. local tone-mapping, sharpening, chroma denoising, lens shading correction, and white balancing. To handle these effects that are not captured by our image formation pipeline, we introduce an optional Refinement-Net. Architecture. Our Refinement-Net adopts the same U-Net architecture as the Dequantization-Net, which learns to refine the output of the Hallucination-Net by a residual learning. The output of the Refinement-net is denoted by? ref .</p><p>Training. To model the effects of real camera pipelines, we train the Refinement-Net using HDR images reconstructed from exposure stacks captured by various cameras (more details in the supplementary material). We minimize the same L total for end-to-end fine-tuning (with ? deq , ? lin , ? crf , and ? hal set to 0 as there are no stage-wise supervisions), and replace the output of Hallucination-Net? with refined HDR image? ref .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Results</head><p>We first describe our experimental settings and evaluation metrics. Next, we present quantitative and qualitative comparisons with the state-of-the-art single-image HDR reconstruction algorithms. We then analyze the contributions of individual modules to justify our design choices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experiment setups</head><p>Datasets. For training and evaluating single-image HDR reconstruction algorithms, we construct two HDR image datasets: HDR-SYNTH and HDR-REAL. We also evaluate our method on two publicly available datasets: RAISE (RAW-jpeg pairs) <ref type="bibr" target="#b9">[10]</ref> and HDR-EYE <ref type="bibr" target="#b41">[42]</ref>. Evaluation metrics. We adopt the HDR-VDP-2 <ref type="bibr" target="#b38">[39]</ref> to evaluate the accuracy of HDR reconstruction. We normalize both the predicted HDR and reference ground-truth HDR images with the processing steps in <ref type="bibr" target="#b39">[40]</ref>. We also evaluate the PSNR, SSIM, and perceptual score with the LPIPS metric <ref type="bibr" target="#b56">[57]</ref> on the tone-mapped HDR images in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Comparisons with state-of-the-art methods</head><p>We compare the proposed method with five recent CNNbased approaches: HDRCNN <ref type="bibr" target="#b13">[14]</ref>, DrTMO <ref type="bibr" target="#b14">[15]</ref>, Expand-Net <ref type="bibr" target="#b39">[40]</ref>, Deep chain HDRI <ref type="bibr" target="#b28">[29]</ref>, and Deep recursive HDRI <ref type="bibr" target="#b29">[30]</ref>. As the ExpandNet does not provide the code for training, we only compare with their released pre-trained model. Both the Deep chain HDRI and Deep recursive HDRI methods do not provide their pre-trained models, so we compare with the results on the HDR-EYE dataset reported in their papers.</p><p>We first train our model on the training set of the HDR-SYNTH dataset (denoted by Ours) and the fine-tune on the training set of the HDR-REAL dataset (denoted by Ours+). For fair comparisons, we also re-train the HDRCNN and DrTMO models with both the HDR-SYNTH and HDR-REAL datasets (denoted by HDRCNN+ and DrTMO+). We provide more comparisons with the pre-trained models of HDRCNN and DrTMO and the our results from each training stage in the supplementary material.</p><p>Quantitative comparisons. <ref type="table" target="#tab_1">Table 1</ref> shows the average HDR-VDP-2 scores on the HDR-SYNTH, HDR-REAL, RAISE, and HDR-EYE datasets. The proposed method performs favorably against the state-of-the-art methods on all four datasets. After fine-tuning on the HDR-REAL training set, the performance of our model (Ours+) is further improved by 1.57 on HDR-REAL, 0.41 on the RAISE, and 0.5 on HDR-EYE datasets, respectively.</p><p>Visual comparisons. <ref type="figure">Figure 6</ref> compares the proposed model with existing methods on a real image captured using NIKON D90 provided by HDR-REAL and an example provided in <ref type="bibr" target="#b14">[15]</ref>. We note that both two examples in Figure 6 come from unknown camera pipeline, and there are no ground-truth HDRs. In general, the HDRCNN <ref type="bibr" target="#b13">[14]</ref> often generates overly-bright results and suffers from noise in the under-exposed regions as an aggressive and fixed inverse CRF x 2 is used. The results of the DrTMO <ref type="bibr" target="#b14">[15]</ref> often looks blurry or washed-out. The ExpandNet <ref type="bibr" target="#b39">[40]</ref> cannot restore the details well in the under-exposed regions and generates visual artifacts in the over-exposed regions, such as sky. Due to the space limit, we provide more visual comparisons in the supplementary material.</p><p>User study. We conduct a user study to evaluate the human preference on HDR images. We adopt the paired comparison <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b46">47]</ref>, where users are asked to select a preferred image from a pair of images in each comparison. We design the user study with the following two settings: (1) With-  <ref type="figure">Figure 6</ref>: Visual comparison on real input image. The example on the top is captured by NIKON D90 from HDR-REAL, and the bottom one is from DrTMO <ref type="bibr" target="#b14">[15]</ref>. The HDRCNN <ref type="bibr" target="#b13">[14]</ref> often suffers from noise, banding artifacts or over-saturated colors in the under-exposed regions. The DrTMO <ref type="bibr" target="#b14">[15]</ref> cannot handle over-exposed regions well and leads to blurry and low-contrast results. The ExpandNet <ref type="bibr" target="#b39">[40]</ref> generates artifacts in the over-exposed regions. In contrast, our method restores fine details in both the under-exposed and over-exposed regions and renders visually pleasing results. We evaluate all 70 HDR images in the HDR-REAL test set. We compare the proposed method with the HDR-CNN <ref type="bibr" target="#b13">[14]</ref>, DrTMO <ref type="bibr" target="#b14">[15]</ref>, and ExpandNet <ref type="bibr" target="#b39">[40]</ref>. We ask each participant to compare 30 pairs of images and collect the results from a total of 200 unique participants. <ref type="figure">Figure 7</ref> reports the percentages of the head-to-head comparisons in which users prefer our method over the HDRCNN, DrTMO, and ExpandNet. Overall, there are 70% and 69% of users prefer our results in the with-reference and no-reference tests, respectively. Both user studies show that the proposed method performs well to human subjective perception.  <ref type="figure">Figure 7</ref>: Results of user study. Our results are preferred by users in both with-reference and no-reference tests. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation studies</head><p>In this section, we evaluate the contributions of individual components using the HDR-SYNTH test set.</p><p>Dequantization. We consider the LDR images as the input and the image I n = F(C(H)) synthesized from the HDR images as the ground-truth of the dequantization procedure. We compare our Dequantization-Net with CNNbased models <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b34">35]</ref>. <ref type="table" target="#tab_4">Table 2</ref> shows the quantitative comparisons of dequantized images, where our method performs better than other approaches.</p><p>Linearization. Our Linearization-Net takes as input the non-linear LDR image, Sobel filter responses, and histogram features to estimate an inverse CRF. To validate the effectiveness of these factors, we train our Linearization-Net with different combinations of the edge and histogram features. <ref type="table" target="#tab_5">Table 3</ref> shows the reconstruction error of the inverse CRF and the PSNR between the output of our Linearization-Net and the corresponding ground-truth image I c = C(H). The edge and histogram features help predict more accurate inverse CRFs. The monotonically increasing constraint further boosts the reconstruction performance on both the inverse CRFs and the linear images.</p><p>Hallucination. We start with the architecture of Eilertsen et al. <ref type="bibr" target="#b13">[14]</ref>, which does not enforce the predicted residuals being positive. As shown in <ref type="table" target="#tab_6">Table 4</ref>, our model design (predicting positive residuals) can improve the performance by 1.19 HDR-VDP-2 scores. By replacing the transposed convolution with the resize convolution in the decoder, our  model effectively reduces the checkerboard artifacts. Furthermore, introducing the perceptual loss for training not only improves the HDR-VDP-2 scores but also helps the model to predict more realistic details. We provide visual comparisons in the supplementary material.</p><p>End-to-end training from scratch. To demonstrate the effectiveness of explicitly reversing the camera pipeline, we train our entire model (including all sub-networks) from scratch without any intermediate supervisions. Compared to the proposed model shown in <ref type="table" target="#tab_1">Table 1</ref>, the performance of such a model drops significantly (-4.43 and -3.48 HDR-VDP-2 scores in the HDR-SYNTH and HDR-REAL datasets, respectively). It shows that our stage-wise training is effective, and the performance improvement does not come from the increase of network capacity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>We have presented a novel method for single-image HDR reconstruction. Our key insight is to leverage the domain knowledge of the LDR image formation pipeline for designing network modules and learning to reverse the imaging process. Explicitly modeling the camera pipeline allows us to impose physical constraints for network training and therefore leads to improved generalization to unseen scenes. Extensive experiments and comparisons validate the effectiveness of our approach to restore visually pleasing details for a wide variety of challenging scenes.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>(e).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Quantitative comparison on HDR images with existing methods. * represents that the model is re-trained on our synthetic training data and + is fine-tuned on both synthetic and real training data. Red Red Red text indicates the best and blue text indicates the best performing state-of-the-art method. SYNTH + HDR-REAL 55.51 ? 6.64 51.38 ? 7.17 56.51 ? 4.33 51.08 ? 5.84 DrTMO+ [15] HDR-SYNTH + HDR-REAL 56.41 ? 7.20 50.77 ? 7.78 57.92 ? 3.69 51.26 ? 5.94</figDesc><table><row><cell>Method</cell><cell>Training dataset</cell><cell cols="2">HDR-SYNTH HDR-REAL</cell><cell cols="2">RAISE [10] HDR-EYE [42]</cell></row><row><cell cols="2">HDRCNN+ [14] HDR-ExpandNet [40] Pre-trained model of [40]</cell><cell cols="4">53.55 ? 4.98 48.67 ? 6.46 54.62 ? 1.99 50.43 ? 5.49</cell></row><row><cell>Deep chain HDRI [29]</cell><cell>Pre-trained model of [29]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>49.80 ? 5.97</cell></row><row><cell cols="2">Deep recursive HDRI [30] Pre-trained model of [30]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>48.85 ? 4.91</cell></row><row><cell>Ours*</cell><cell>HDR-SYNTH</cell><cell cols="4">60.11 ? 6.10 60.11 ? 6.10 60.11 ? 6.10 51.59 ? 7.42 58.80 ? 3.91 52.66 ? 5.64</cell></row><row><cell>Ours+</cell><cell cols="5">HDR-SYNTH + HDR-REAL 59.52 ? 6.02 53.16 ? 7.19 53.16 ? 7.19 53.16 ? 7.19 59.21 ? 3.68 59.21 ? 3.68 59.21 ? 3.68 53.16 ? 5.92 53.16 ? 5.92 53.16 ? 5.92</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>reference test: We show both the input LDR and the groundtruth HDR images as reference. This test evaluates the faithfulness of the reconstructed HDR image to the groundtruth.<ref type="bibr" target="#b1">(2)</ref> No-reference test: The input LDR and groundtruth HDR images are not provided. This test mainly compares the visual quality of two reconstructed HDR images.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Comparisons on Dequantization-Net. Our Dequantization-Net restores the missing details due to quantization and outperforms existing methods.</figDesc><table><row><cell>Method</cell><cell>PSNR (?)</cell><cell>SSIM (?)</cell></row><row><cell>w/o dequantization</cell><cell cols="2">33.86 ? 6.96 0.9946 ? 0.0109</cell></row><row><cell>Hou et al. [18]</cell><cell cols="2">33.79 ? 6.72 0.9936 ? 0.0110</cell></row><row><cell>Liu et al. [35]</cell><cell cols="2">34.83 ? 6.04 0.9954 ? 0.0073</cell></row><row><cell cols="3">Dequantization-Net (Ours) 35.87 ? 6.11 35.87 ? 6.11 35.87 ? 6.11 0.9955 ? 0.0070 0.9955 ? 0.0070 0.9955 ? 0.0070</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Analysis on alternatives of Linearization-Net. We demonstrate the edge and histogram features and monotonically increasing constraint are effective to improve the performance of our Linearization-Net.</figDesc><table><row><cell cols="2">Image Edge Histogram</cell><cell>Monotonically increasing</cell><cell cols="2">L2 error (?) of inverse CRF of linear image PSNR (?)</cell></row><row><cell>-</cell><cell>-</cell><cell>-</cell><cell>2.00 ? 3.15</cell><cell>33.43 ? 7.03</cell></row><row><cell></cell><cell>-</cell><cell>-</cell><cell>1.66 ? 2.93</cell><cell>34.31 ? 6.94</cell></row><row><cell>-</cell><cell></cell><cell>-</cell><cell>1.61 ? 3.03</cell><cell>34.51 ? 7.14</cell></row><row><cell></cell><cell></cell><cell>-</cell><cell>1.58 ? 2.73</cell><cell>34.53 ? 6.83</cell></row><row><cell></cell><cell></cell><cell></cell><cell>1.56 ? 2.52 1.56 ? 2.52 1.56 ? 2.52</cell><cell>34.64 ? 6.73 34.64 ? 6.73 34.64 ? 6.73</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Analysis on alternatives of Hallucination-Net. With the positive residual learning, the model predicts physically accurate values within the over-exposed regions. The resize convolution reduces the checkerboard artifacts, while the perceptual loss helps generate realistic details.</figDesc><table><row><cell>Positive residual</cell><cell>Resize convolution</cell><cell>Perceptual loss</cell><cell>HDR-VDP-2 (?)</cell></row><row><cell>-</cell><cell>-</cell><cell>-</cell><cell>63.60 ? 15.32</cell></row><row><cell></cell><cell>-</cell><cell>-</cell><cell>64.79 ? 15.89</cell></row><row><cell></cell><cell></cell><cell>-</cell><cell>64.52 ? 16.05</cell></row><row><cell></cell><cell></cell><cell></cell><cell>66.31 ? 15.82 66.31 ? 15.82 66.31 ? 15.82</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments. This work is supported in part by NSF CAREER (#1149783), NSF CRII (#1755785), MOST 109-2634-F-002-032, MediaTek Inc. and gifts from Adobe, Toyota, Panasonic, Samsung, NEC, Verisk, and Nvidia.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Do hdr displays support ldr content?: A psychophysical evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmet</forename><surname>Ouz Aky?z</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roland</forename><surname>Fleming</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><forename type="middle">E</forename><surname>Riecke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Reinhard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heinrich</forename><forename type="middle">H</forename><surname>B?lthoff</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<publisher>ACM TOG</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">High dynamic range imaging and low dynamic range expansion for generating HDR content. Computer Graphics Forum</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesco</forename><surname>Banterle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Debattista</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Artusi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumanta</forename><surname>Pattanaik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karol</forename><surname>Myszkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Ledda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Chalmers</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Inverse tone mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesco</forename><surname>Banterle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Ledda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Debattista</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Chalmers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on Computer graphics and interactive techniques in Australasia and Southeast Asia</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Expanding low dynamic range videos for high dynamic range applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesco</forename><surname>Banterle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Ledda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Debattista</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Chalmers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Spring Conference on Computer Graphics</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">A framework for inverse tone mapping. The Visual Computer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesco</forename><surname>Banterle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Ledda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Debattista</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Chalmers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marina</forename><surname>Bloj</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">PatchMatch: A randomized correspondence algorithm for structural image editing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Connelly</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Finkelstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><forename type="middle">B</forename><surname>Goldman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>ACM TOG</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Understanding the in-camera image processing pipeline for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ms Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kim</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Modeling radiometric uncertainty for vision with tonemapped color images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ayan</forename><surname>Chakrabarti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baochen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Scharstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Zickler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Decontouring: Prevention and removal of false contour artifacts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Scott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofan</forename><surname>Daly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Human Vision and Electronic Imaging IX</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Raise: A raw images dataset for digital image forensics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duc-Tien</forename><surname>Dang-Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cecilia</forename><surname>Pasquini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valentina</forename><surname>Conotter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giulia</forename><surname>Boato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Image-based lighting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Debevec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGGRAPH</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Recovering high dynamic range radiance maps from photographs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><forename type="middle">E</forename><surname>Debevec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997" />
			<publisher>ACM TOG</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Image quilting for texture synthesis and transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
			<publisher>ACM TOG</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">HDR image reconstruction from a single exposure using deep CNNs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Eilertsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Kronander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gyorgy</forename><surname>Denes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafa?</forename><forename type="middle">K</forename><surname>Mantiuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Unger</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>ACM TOG</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Deep reverse tone mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuki</forename><surname>Endo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshihiro</forename><surname>Kanamori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Mitani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>ACM TOG</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">What is the space of camera response functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shree</forename><forename type="middle">K</forename><surname>Grossberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nayar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Image companding and inverse halftoning using deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianxu</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoping</forename><surname>Qiu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Image completion using planar structure guidance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Sing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Narendra</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kopf</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>ACM TOG</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Physiological inverse tone mapping based on retina response. The Visual Computer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongqing</forename><surname>Huo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Brost</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Globally and locally consistent image completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satoshi</forename><surname>Iizuka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edgar</forename><surname>Simo-Serra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroshi</forename><surname>Ishikawa</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>ACM TOG</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Perceptual losses for real-time style transfer and super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Deep high dynamic range imaging of dynamic scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khademi</forename><surname>Nima</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ravi</forename><surname>Kalantari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ramamoorthi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>ACM TOG</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A software platform for manipulating the camera imaging pipeline</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Can</forename><surname>Hakki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael S</forename><surname>Karaimer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Ghost removal in high dynamic range images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmet</forename><surname>Erum Arif Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Oguz Akyuz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Reinhard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICIP</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">A new in-camera imaging model for color computer vision and its application</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seon Joo</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><forename type="middle">Ting</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sabine</forename><surname>S?sstrunk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael S</forename><surname>Brown</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">High-quality reverse tone mapping for a wide range of exposures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Rafael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manuel M</forename><surname>Kovaleski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Oliveira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">27th SIBGRAPI Conference on Graphics, Patterns and Images</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A comparative study for single image blind deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Sheng</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Narendra</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Deep chain hdri: Reconstructing a high dynamic range image from a single low dynamic range image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyeong</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suk-Ju</forename><surname>Gwon Hwan An</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Deep recursive hdri: Inverse tone mapping using generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyeong</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suk-Ju</forename><surname>Gwon Hwan An</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Crf-net: Single image radiometric calibration using CNNs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Peers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Visual Media Production</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A hybrid l1-l0 layer decomposition model for tone mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhetong</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zisheng</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Radiometric calibration from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinwei</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuntaro</forename><surname>Yamazaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heung-Yeung</forename><surname>Shum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Determining the radiometric response function from a single grayscale image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Learning-based dequantization for image restoration against extremely poor illumination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Shu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Image inpainting for irregular holes using partial convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guilin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fitsum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><forename type="middle">J</forename><surname>Reda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting-Chun</forename><surname>Shih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Catanzaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">High dynamic range video with ghost removal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Mangiat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jerry</forename><surname>Gibson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Applications of Digital Image Processing XXXIII</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">On being &apos;undigital&apos; with digital cameras: Extending dynamic range by combining differently exposed pictures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rosalind</forename><forename type="middle">W</forename><surname>Picard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IS&amp;T</title>
		<meeting>IS&amp;T</meeting>
		<imprint>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">HDR-VDP-2: A calibrated visual metric for visibility and quality predictions in all luminance conditions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafat</forename><surname>Mantiuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kil Joong</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Allan</forename><forename type="middle">G</forename><surname>Rempel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Heidrich</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<publisher>ACM TOG</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">ExpandNet: A deep convolutional neural network for high dynamic range expansion from low dynamic range content</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Demetris</forename><surname>Marnerides</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Bashford-Rogers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Hatchett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Debattista</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In EG</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Radiometric calibration from noise distributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasuyuki</forename><surname>Matsushita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Visual attention in ldr and hdr images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiromi</forename><surname>Nemoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Korshunov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philippe</forename><surname>Hanhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Touradj</forename><surname>Ebrahimi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">9th International Workshop on Video Processing and Quality Metrics for Consumer Electronics (VPQM)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Using geometry invariants for camera response function estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian-Tsong</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Fu</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mao-Pei</forename><surname>Tsui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Deconvolution and checkerboard artifacts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Augustus</forename><surname>Odena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Olah</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>Distill</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Context encoders: Feature learning by inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepak</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Krahenbuhl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Photomatix</surname></persName>
		</author>
		<ptr target="https://www.hdrsoft.com/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">A comparative study of image retargeting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Rubinstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diego</forename><surname>Gutierrez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Sorkine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ariel</forename><surname>Shamir</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>ACM TOG</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Hardware-efficient debanding and visual enhancement filter for inverse tone mapped high dynamic range images and videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guan-Ming</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pamela</forename><forename type="middle">C</forename><surname>Cosman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICIP</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Ghost detection and removal for high dynamic range images: Recent advances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhilash</forename><surname>Srikantha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D?sir?</forename><surname>Sidib?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Signal Processing: Image Communication</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Estimating radiometric response functions from image noise variance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Takamatsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasuyuki</forename><surname>Matsushita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katsushi</forename><surname>Ikeuchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Deep high dynamic range imaging with large foreground motions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shangzhe</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiarui</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Wing</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi-Keung</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Image correction via deep reciprocating hdr transformation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yibing</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaopeng</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lau</forename><surname>Rynson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Generative image inpainting with contextual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohui</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohui</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas S</forename><surname>Huang</surname></persName>
		</author>
		<title level="m">Free-form image inpainting with gated convolution. ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Learning high dynamic range from outdoor panoramas</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinsong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Francois</forename><surname>Lalonde</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">The unreasonable effectiveness of deep features as a perceptual metric</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Deep reconstruction of least significant bits for bit-depth expansion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronggang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wangmeng</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoping</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
