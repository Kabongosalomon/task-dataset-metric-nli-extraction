<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Beyond Max-Margin: Class Margin Equilibrium for Few-shot Object Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bohao</forename><surname>Li</surname></persName>
							<email>libohao20@mails.ucas.ac.cn</email>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Department of Artificial Intelligence</orgName>
								<orgName type="department" key="dep2">School of Informatics</orgName>
								<orgName type="laboratory">MAC</orgName>
								<orgName type="institution">Xiamen University</orgName>
								<address>
									<postCode>361005</postCode>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boyu</forename><surname>Yang</surname></persName>
							<email>yangboyu18@mails.ucas.ac.cn</email>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Department of Artificial Intelligence</orgName>
								<orgName type="department" key="dep2">School of Informatics</orgName>
								<orgName type="laboratory">MAC</orgName>
								<orgName type="institution">Xiamen University</orgName>
								<address>
									<postCode>361005</postCode>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Liu</surname></persName>
							<email>liuchang615@mails.ucas.ac.cn</email>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Department of Artificial Intelligence</orgName>
								<orgName type="department" key="dep2">School of Informatics</orgName>
								<orgName type="laboratory">MAC</orgName>
								<orgName type="institution">Xiamen University</orgName>
								<address>
									<postCode>361005</postCode>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Liu</surname></persName>
							<email>liufeng20@mails.ucas.ac.cn</email>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Department of Artificial Intelligence</orgName>
								<orgName type="department" key="dep2">School of Informatics</orgName>
								<orgName type="laboratory">MAC</orgName>
								<orgName type="institution">Xiamen University</orgName>
								<address>
									<postCode>361005</postCode>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rongrong</forename><surname>Ji</surname></persName>
							<email>rrji@xmu.edu.cnqxye@ucas.ac.cn</email>
							<affiliation key="aff2">
								<orgName type="department">Institute of Artificial Intelligence</orgName>
								<orgName type="institution">Xiamen University</orgName>
								<address>
									<postCode>361005</postCode>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">Peng Cheng Laboratory</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixiang</forename><surname>Ye</surname></persName>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Department of Artificial Intelligence</orgName>
								<orgName type="department" key="dep2">School of Informatics</orgName>
								<orgName type="laboratory">MAC</orgName>
								<orgName type="institution">Xiamen University</orgName>
								<address>
									<postCode>361005</postCode>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>EECE</roleName><forename type="first">?</forename><surname>Prisdl</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100049</postCode>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Beyond Max-Margin: Class Margin Equilibrium for Few-shot Object Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>Code is available at https://github.com/Bohao-Lee/CME.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T03:16+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Few-shot object detection has made substantial progress by representing novel class objects using the feature representation learned upon a set of base class objects. However, an implicit contradiction between novel class classification and representation is unfortunately ignored. On the one hand, to achieve accurate novel class classification, the distributions of either two base classes must be far away from each other (max-margin). On the other hand, to precisely represent novel classes, the distributions of base classes should be close to each other to reduce the intra-class distance of novel classes (min-margin). In this paper, we propose a class margin equilibrium (CME) approach, with the aim to optimize both feature space partition and novel class reconstruction in a systematic way. CME first converts the few-shot detection problem to the few-shot classification problem by using a fully connected layer to decouple localization features. CME then reserves adequate margin space for novel classes by introducing simple-yet-effective class margin loss during feature learning. Finally, CME pursues margin equilibrium by disturbing the features of novel class instances in an adversarial min-max fashion. Experiments on Pascal VOC and MS-COCO datasets show that CME significantly improves upon two baseline detectors (up to 3 ? 5% in average), achieving state-of-the-art performance. Code is available at https://github.com/Bohao-Lee/CME.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In the past few years, we witnessed the great progress of visual object detection <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b17">18]</ref>. This is attributed to the availability of large-scale datasets with precise anno-* Equal Contribution. ? Corresponding Author.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Base Class 1</head><p>Base Class 2  tations and convolutional neural networks (CNNs) capable of absorbing the annotation information. However, annotating a large amount of objects is expensive and laborious <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b33">34]</ref>. It is also not consistent with cognitive learning, which can build a precise model using few-shot supervisions <ref type="bibr" target="#b19">[20]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Min-Margin Max-Margin</head><p>Few-shot detection, which simulates the way that human learns, has attracted increasing attention. Given base classes of sufficient training data and novel classes of few supervisions, few-shot detection trains a model to simultaneously detect objects from both base and novel classes. To this end, a majority of works divided the training procedure to two stages: base class training (representation learning) and novel class reconstruction (meta training). In representation learning, the sufficient training data of base classes are used to train a network and constructs a representative feature space. In meta training, the network is finetuned so that the novel class objects can be represented within the feature space. Among the earliest work, Kang et al. <ref type="bibr" target="#b6">[7]</ref> pro-posed applying channel-attended feature re-weighting for semantic enforcement. In the two-stage framework, Wang et al. <ref type="bibr" target="#b26">[27]</ref> and Yan et al. <ref type="bibr" target="#b29">[30]</ref> contributed early few-shot detection methods. Wang et al. <ref type="bibr" target="#b23">[24]</ref> and Wu et al. <ref type="bibr" target="#b27">[28]</ref> proposed freezing the backbone network and reconstructing the novel classes using the classifier weights during detector finetuning.</p><p>Despite the substantial progress, the implicit contradiction between representation and classification is unfortunately ignored. To separate the classes, the distributions of two base classes requires to be far away from each other (max-margin), which however aggregates the diversity of novel classes, <ref type="figure" target="#fig_1">Fig. 1(a)</ref>. To precisely represent novel classes, the distributions of base classes should be close to each other (min-margin), which causes the difficult of classification, <ref type="figure" target="#fig_1">Fig. 1(b)</ref>. How to simultaneously optimize novel class representation and classification in the same feature space remains to be elaborated.</p><p>In this paper, we propose a class margin equilibrium (CME) approach, with the aim to optimize feature space partition for few-shot object detection with adversarial class margin regularization. For the object detection task, CME first introduces a fully connected layer to decouple localization features which could mislead class margins in the feature space. CME then pursues a margin equilibrium to comprise representation learning and feature reconstruction. Specifically, during base training, CME constructs a feature space where the margins between novel classes are maximized by introducing class margin loss. During network finetuning, CME introduces a feature disturbance module by truncating gradient maps. With multiple training iterations, class margins are regularized in an adversarial min-max fashion towards margin equilibrium, which facilities both feature reconstruction and object classification in the same feature space.</p><p>The contributions of this study include:</p><p>? We unveil the representation-classification constriction hidden in few-shot object detection, and propose a feasible way to alleviate the constriction from the perspective of class margin equilibrium (CME).</p><p>? We design the max-margin loss and feature disturbance module to implement class margin equilibrium in an adversarial min-max fashion.</p><p>? We convert the few-shot detection problem to a fewshot classification problem by filtering out localization features, We improve the state-of-the-art with significant margins upon both one-stage and two-stage baseline detectors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Object Detection</head><p>CNN-based methods have significantly improved the performance of object detection. While the one-stage methods, e.g., YOLO <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17]</ref> and SSD <ref type="bibr" target="#b13">[14]</ref>, have higher detection efficiency, the two-stage methods, e.g., Faster R-CNN <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b17">18]</ref> and FPN <ref type="bibr" target="#b9">[10]</ref> report higher performance, usually. Relevant CNN-based detectors provided fundamental techniques, e.g., RoI pooling <ref type="bibr" target="#b2">[3]</ref> and multi-scale feature aggregation <ref type="bibr" target="#b9">[10]</ref>, which benefit few-shot object detection. However these methods generally require large amounts of training data, which hinders their applications in practical scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Few-shot Learning</head><p>Existing few-shot learning methods can be broadly categorized as either: metric learning <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b8">9]</ref>, meta-learning <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b5">6]</ref>, or data augmentation <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b24">25]</ref>. Metric learning methods train networks to predict whether two images/regions belong to the same category. Meta-learning approaches specify optimization or loss functions which force faster adaptation of parameters to new categories with few examples. The data argumentation methods learn to generate additional examples for unseen categories. In the metric learning framework, prototypical models converted the spatial semantic information of objects to convolutional channels. In existing studies, it was observed that class margin has a great impact to classifiers when required to guarantee model discriminability under few supervisions. Li et al. <ref type="bibr" target="#b7">[8]</ref> proposed adaptive margin loss to improve model generalization ability. They further developed a class-relevant additive margin loss considering the semantic similarity between image pairs. However, solely pursuing max-margin could be infeasible because the novel classes required to be reconstructed with the base classes and large margin would improve the diversity of novel class samples. Liu et al. <ref type="bibr" target="#b10">[11]</ref> introduced negative class margin to benefit representation of novel classes. Existing studies inspire us to re-think the max-margin principle in few-shot settings, to comprise discriminability and representation capability of features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Few-shot Object Detection</head><p>Following the meta learning methods, Kang et al. <ref type="bibr" target="#b6">[7]</ref> contributed an early few-shot detection method, which fully exploited training data from base classes while quickly adapting the detection prediction network to predict novel classes. Yan et al. <ref type="bibr" target="#b29">[30]</ref> proposed meta-learning over RoIs, enabling Faster R-CNN be a meta-learner for few-shot detection. Wu et al. <ref type="bibr" target="#b27">[28]</ref> proposed positive sample refinement to enrich object scales for few-shot detection. Despite of the progress, the discriminability and representation equi- librium between novel and base classes remain unsolved. Furthermore, most existing methods treat few-shot detection as a few-shot classification problem, ignoring the role of features for object localization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">The Proposed Approach</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Few-shot Detection Framework</head><p>Problem Definition. Given base classes C base of sufficient training data and novel classes C novel of few supervisions, few-shot detection aims to train a model that can simultaneously detect objects from both base and novel classes. As shown in <ref type="figure" target="#fig_2">Fig. 2</ref>, a detection network is first trained with C base to construct feature representation. The network is then finetuned with both C base and C novel to represent the few-shot instances from novel classes. In what follows, we describe the proposed method by using meta YOLO <ref type="bibr" target="#b6">[7]</ref> as the baseline detector. Our approach can be applied to two-stage few-shot detectors <ref type="bibr" target="#b27">[28]</ref> in a plug-andplay fashion.</p><p>For both detector training and finetuning, the dataset D (either base classes or novel classes) is divided to a support set S and a query set</p><formula xml:id="formula_0">Q. D = S ? Q = {I S , M S } ? {I Q , M Q },</formula><p>where I S denotes support images with a mask annotations M S , which are generated according to object bounding-boxes. I Q denotes the query images with groundtruth bounding boxes M Q . Given N classes, each of which has K annotated instances, S can be further denoted as {{I S ik , M S ik }, i = 1, ..., N, k = 1, ..., K}, where i indexes the class and k indexes instance, and I S ik ? R W ?H?3 . The network consists of a support branch ( <ref type="figure" target="#fig_2">Fig. 2</ref> (upper)) a query branch ( <ref type="figure" target="#fig_2">Fig. 2 (lower)</ref>). On the support branch, the support images I S and their bounding-boxes M S are fed to the CNN to extract convolutional feature maps. With a global max pooling (GMP) operation, the feature maps are squeezed to prototype vectors</p><formula xml:id="formula_1">v ik = f ? S (I S ? M S ), where f ? S (?)</formula><p>denotes the network of the support branch with parameters ? S and ? the concatenate operation. The mean prototypes for the i-th class is calculated by ? i = 1 K K k=1 v ik , indicating the semantics of the object class. On the query branch, convolutional features F Q = f ? Q (I Q ) are extracted for the query images I Q , where ? Q denotes the query branch network parameters. The features are activated by multiplying with the prototype vectors {? i } through a pixel-wise multiple operation. The activated features are fed to a prediction (classification and box regression) module and output P ? P (F Q ? ? i ), where ? P denotes the prediction module parameters, ? means element-wise multiplication. For the general object detection task, the target of the prediction results are expected to match the ground-truth bounding box area M Q , through minimizing the following loss</p><formula xml:id="formula_2">arg min ? L det (P ? P (F Q ? u i ), M Q ),<label>(1)</label></formula><formula xml:id="formula_3">where ? = ? S ? ? Q ? ? P . L det is the object detection loss, defined as L det = L cls + L bbx + L obj , where L cls , L reg ,</formula><p>and L obj respectively denote the classification, regression and anchor confidence loss <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b16">17]</ref>. Feature Filtering. For object detection, the convolutional features incorporate both localization features and classification features. While the classification features are class dependent, the localization features are independent to object classes, and therefore tend to perturb class margins. To filtering out the localization features, a fully connected layer is used to decouple localization features, as v ik = FC(v ik ), to convert the few-shot detection problem to a pure few-shot classification problem, <ref type="figure" target="#fig_2">Fig. 2</ref>. Driven by the max-margin loss, the localization features are filtered out during detector training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Base Training: Class Max-margin</head><p>Max-Margin Loss. In the base training stage, the sufficient data of base classes are used to train the network and construct a representative feature space. As the object detection network is a discriminative model, the whole feature space will be divided into multiple sub-spaces each of which is occupied with a class. In the finetuning stage, novel classes will be embedded to the feature space, often to the margin space between base classes. To avoid aliasing, the margin space between base classes should be big enough to accommodate novel classes, <ref type="figure" target="#fig_1">Fig. 1(a)</ref>, i.e., class max-margin.</p><p>To pursue class max-margin, prototype vectors of the base classes require to be close to their mean prototypes (i.e., minimum intra-class variance) while those of different classes be far away from each other (i.e., maximum inter-class distance). Given the prototype vector v ik for the k-th instance, the mean prototype vector of the i-th class is calculated as ?</p><formula xml:id="formula_4">i = 1 K k?1 j=0 v ij , which repre- sents the semantics of the class. The intra-class distance is D Intra i = K?1 j=0 ||v ij ? ? i || 2 2 . The inter-class margin dis- tance is calculated as D Inter i = min j,j =i ||? j ? ? i || 2 2 .</formula><p>Generally speaking, margin is defined as the distance between the decision boundary and the sample of shortest distance to the boundary. For the feature space constructed by CNN, it is hard to directly calculate the margin M i,i between two classes. As an approximation, we first calculate the upper and lower bounds of M i,i and have</p><formula xml:id="formula_5">D Inter i ? D Intra i ? D Intra i ? M i,i ? D Inter i ,<label>(2)</label></formula><p>which indicates that the upper bound of margin is the interclass distance while the lower bound is the inter-class distance subtracting intra-class distance. According to Eq. 2, max-margin can be approximated by maximizing the upper and lower bounds of margins, as arg max</p><formula xml:id="formula_6">? M i,i arg max ? L mrg = N i D Intra i N i D Inter i ,<label>(3)</label></formula><p>where N denotes the class number.</p><p>Detector Training. In base training, a support set and a query set are constructed for base classes by randomly selecting training samples, S ? Q ? D C base . The detection network is trained by optimizing both the object detection loss and the max-margin loss, as arg min</p><formula xml:id="formula_7">? L trn = L det + ?L mrg ,<label>(4)</label></formula><p>where ? = 1.0 is an experimentally determined regularization factor to balance the two loss functions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Finetuning: Margin Equilibrium</head><p>Finetuning refers to a meta learning procedure, which uses few-shot novel class data to update network param- eters. However, without sufficient data, the novel classes cannot significantly change the feature representation, so that novel classes required to be represented by the features learned upon base classes <ref type="bibr" target="#b10">[11]</ref>. According to Eq. 3, the margins between base classes in the feature space are required to be large, which improves the D Intra and preserve sufficient margin space for novel classes. However, when the margin between base classes is large, the samples/features from novel classes can be of large diversity, <ref type="figure" target="#fig_1">Fig. 1(b)</ref>, which aggregates the difficulty to train the detection model for novel classes. To solve this problem, we propose the margin equilibrium strategy based on feature disturbance. Feature Disturbance. Feature disturbance defines an online data augmentation procedure according to the gradient maps of samples. During finetuning, images of base and novel classes are simultaneously fed to the network detector training driven by the margin loss and detection loss, <ref type="figure" target="#fig_3">Fig. 3</ref>. During the back-propagation procedure, the gradient map of a support image is calculated by G(x, y) = || ?L f tn ?I(x,y) S ||, where G(x, y) ? R W ?H and || ? || denotes the norm operation. L f tn denotes the finetuning loss defined on the detection loss and margin loss. (x, y) is the pixel location. According to the characteristics of CNN, the pixels of larger gradients correspond object parts of larger discrimination ability and contribute more to reduce the finetuning loss. During detector training, the disturbance procedure is carried out to truncate the pixels of large gradient and disturb the finetuned features. This is implemented by re-sampling the ground-truth mask according to the gradient map, as</p><formula xml:id="formula_8">T (G(x, y)) = 0 G(x, y) ? ? (G(x, y)) 1 otherwise ,<label>(5)</label></formula><p>where ? is a threshold which controls the ratio of feature disturbance. In experiments, ? is set to be a dynamic threshold so that top-15% pixels of large gradient are set to 0. For </p><formula xml:id="formula_9">M S (x, y) ? M S (x, y) ? T (G(x, y)).<label>(6)</label></formula><p>Margin Equilibrium. With the above defined feature disturbance strategy, the novel classes C novel are combined with the base classes C base for network parameter finetuning. Given a batch of support and query images, the network parameters are updated for a few iterations. The iteration number relies on the number (K) of training images in each novel class. In each finetuning iteration, the support mask is re-sampled, and the prototype vector is calculated by v ik = f ? S (I S ? M S ) guided by the re-sampled mask M S . Accordingly, detector finetuning is performed by minimizing the loss function</p><formula xml:id="formula_10">L f tn = L det + ?L mrg (M S (x, y)).<label>(7)</label></formula><p>Meanwhile, according to Eq. 6, the features are disturbed so that prototype vectors within the feature space are resampled to occupy the class margin. During back-propagation, network parameters are updated to maximize the class margins M i,i by optimizing Eq. 3. In the procedure, D Inter i increases while D Intra i decreases. During forward propagation, the support mask M S is updated according to Eq. 6 and the support image is re-sampled. In this way, the discriminative pixels on the image/features are erased so that the discrimination power of <ref type="table">Table 1</ref>. Ablation study of CME modules for few-shot object detection on Pascal VOC novel classes (split-1). "MM" denotes max-margin, "FF" feature filtering, "FD" feature disturbance and "avg. ?" average performance improvements. </p><formula xml:id="formula_11">M i,i , Forward Propagation ,<label>(8)</label></formula><p>which pursues class margin equilibrium for base classes and embedded novel classes, detailed in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experimental Setting</head><p>Datasets. The proposed CME approach for few-shot object detection is evaluated on Pascal VOC 2007, VOC 2012 and MS COCO, following the settings in Meta YOLO <ref type="bibr" target="#b6">[7]</ref>. The object categories in the datasets are divided into two groups: base classes with adequate annotations and novel classes with K-shot annotated instances. During base training process, the network is optimized upon using the training data of base classes. During finetuning, the network is optimized by K-shot instances of each novel and base classes. For Pascal VOC, the whole dataset is partitioned into 3 splits for cross validation. In each split, 5 classes are selected as novel classes and the rest of the 15 classes are base classes. The number of annotated instances K is set as 1, 2, 3, 5, and 10. For MS COCO, 20 classes are selected as novel ones and the remaining 60 classes are set as base ones.</p><p>Implementation Details. As a plug-and-play module, CME is fused with the one-stage detector (Meta YOLO <ref type="bibr" target="#b6">[7]</ref>) and the two-stage detector (MPSR <ref type="bibr" target="#b27">[28]</ref>) for evaluation. In what follows, the experimental analysis and ablation study are based on the Meta YOLO detector, which is implemented with PyTorch 1.0 and run on Nvidia Tesla V100 GPUs. During training, four data augmentation strategies are used, including size normalization, horizontal flipping, random cropping, and random resizing. The network is optimized by the SGD algorithm with initial learning rate of  <ref type="table">Table 1</ref> shows the efficacy of the main components of CME for few-shot object detection with different shot settings on Pascal VOC novel classes (split-1). With the maxmargin loss, the average performance gain is 2.6% compared with baseline method. By using feature filtering, the performance gain increases to 4.3%. With the feature disturbance for class margin equilibrium, the performance gain increases to 5.4%. It shows that CME achieves significant improvement over the baseline method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Ablation Study</head><p>Max-Margin. From <ref type="table">Table 1</ref>, we can find that maxmargin makes effect in 2,3,5 shots setting while being invalid in 1-shot setting. It validates that given limit training data, the increase of class margins is worthless for which is not conducive to the reconstruction of novel class.</p><p>Feature Filtering. In <ref type="table" target="#tab_2">Table 2</ref>, experiments are con-  ducted to determine the number of output feature channels in the feature filtering module. It reveals that 512 channels reports the best result. 1024 output channels is redundant that makes the margin loss invalid. 256 output channels is insufficiency because of the depression of the feature representation. That is to say that the FC layer requires a slightly smaller output channel number (compared with 1024 input channels) to filter out localization related features.</p><p>Feature Disturbance. In <ref type="table">Table 3</ref> and <ref type="table">Table 4</ref>, ablation studies are carried out to compare the feature disturbance strategies. <ref type="table">Table 3</ref> shows that it is better to disturb the prototypes of base class C Base without C N ovel . According to Eq. 8, the margin equilibrium is implemented by feature disturbance. The disturbance of base classes depresses margins between base classes which benefit the representation of novel class. Conversely, the disturbance of novel classes may degenerate the representation discrimination as the margin space turns limited. <ref type="table">Table 4</ref> validates that gradient truncation significantly outperforms feature truncation and feature crop strategies among those of feature disturbance method since it is an adversarial min-max margin manner against the gradient rather than a simple data augmentation strategy.   <ref type="figure">Figure 5</ref>. t-SNE visualization of the feature prototype evolution of two object classes during the finetuning stage. While the dashed curves denote feature disturbance routes (Forward Propagation), the solid line segments denote the routes driven by the fine-turning loss with max-margin regularization (Backward Propagation). In the finetuning stage, the instance features of novel classes form sub-spaces in the feature space learned on base classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Model Analysis</head><p>In <ref type="figure" target="#fig_4">Fig. 4</ref>, we compare the distributions of feature prototypes learned by the baseline method and our CME approach. One can see that CME optimizes novel class embeddings by reserving adequate margin space for novel classes when learning the feature representation. Furthermore, CME optimizes feature space partition by pursuing margin equilibrium in an adversarial min-max fashion when finetuning the network with the novel classes. While the baseline method is confused with the novel class "Cow" and the base classes "Cat", "Dog", "Sheep", and "Horse", CME clearly distinguishes them and reduces the overlap between classes. It proves that the CME can improve the representation capacity of the feature space for better object detection. <ref type="figure">Fig. 5</ref> visualizes the evolution of two prototypes in the feature space during the finetuning stage. With an adversarial min-max margin way (defined as Eq. 8), during forward propagation, the feature prototypes disturb to minimize the margin which is implemented by re-sampling the support mask. During backward propagation, the feature prototypes move to maximum the margin which is driven by the finetuning loss (Eq. 7). With multiple forward-backward propagation iterations, the samples span a feature sub-space for each object class. <ref type="figure" target="#fig_6">Fig. 6</ref> shows the detection result of the baseline method and the proposed CME approach. With the max-margin loss, the few-shot detector can reduce the false detection results because the margin between each class is increased which benefit the discrimination of the classifier. However, the max-margin is not conductive to the feature reconstruction with the raise of missing object. By marginequilibrium, our approach balanced the contradiction between classification and representation. It shows that CME can precisely detect more objects with fewer false positives.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Performance Comparison</head><p>Pascal VOC In <ref type="table" target="#tab_4">Table 5</ref>, we compare CME with the one-stage few-shot detectors including LSTD <ref type="bibr" target="#b0">[1]</ref>, Meta YOLO <ref type="bibr" target="#b6">[7]</ref>, and MetaDet <ref type="bibr" target="#b26">[27]</ref>, which are based on the YOLO detector. The proposed CME detector demonstrates great advantages over the compared detectors. Specifically, for Novel Set 1, CME respectively achieves 0.7%(17.8% vs. 17.1%) on 1-shot setting, 7.0%(26.1% vs. 19.1%) on 2-shot setting, 2.6%(31.5% vs. 28.9%) on 3-shot setting, 9.8%(44.8% vs. 35.0%) on 5-shot setting. The average improvement is 3.8%, which is s significant margin for the challenging task. The average performance improvements are respectively 0.7% for novel set 3.</p><p>We also compare the proposed approach with twostage detectors including MetaDet <ref type="bibr" target="#b26">[27]</ref>, Meta RCNN <ref type="bibr" target="#b29">[30]</ref>, TFA <ref type="bibr" target="#b23">[24]</ref> Viewpoint Estimation <ref type="bibr" target="#b28">[29]</ref>, and MPSR <ref type="bibr" target="#b27">[28]</ref>, which are based on the Faster-RCNN framework. One can see that in most settings CME outperforms the compared </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Shots</head><p>Method AP AP50 AP75 APS APM APL AR1 AR10 AR100 ARS ARM ARL  detectors. For novel set 1, CME respectively outperforms by 5%(47.5% vs. 42.5%) on 2-shot setting, 6%(58.2% vs. 52.2%) on 5-shot setting. The average improvement researches 1.2%. The average performance improvement for novel set 2 is 1.5% and 0.3% for novel set 3. MS COCO Compared with Pascal Voc, MS COCO has more object categories and images, which imply that the margin equilibrium may benefit for much richer feature rep-resentation. Thereby, our approach achieves more significant relative improvement on MS COCO as shown in Table 6. For the 10-shot setting, CME improves AP upon the baseline method MPSR by 5.3% and for the 30-shot setting, it improves 2.8%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We proposed a class margin equilibrium (CME) approach to optimize both feature space partition and novel class representation for few-shot object detection. During base training, CME preserves adequate margin space for novel classes by a simple-yet-effective class margin loss. During finetuning, CME pursues margin equilibrium by disturbing the instance features of novel classes in an adversarial min-max fashion. Extensive experiments validated the effectiveness of CME for alleviating the constriction of feature representation and classification in few-shot settings. As a plug-and-play module, CME improved both one-stage and two-stage few-shot detectors, in striking contrast to the state-of-the-arts. As a general method for feature representation learning and class margin optimization, CME provides a fresh insight for few-shot learning problems.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 .</head><label>1</label><figDesc>The contradiction of representation and classification in few-shot object detection. (a) To separate the classes with each other, either two base classes requires to be far away from each other (max-margin), which aggregates the intra-class distance of novel classes. (b) To precisely represent novel classes, the distributions of base classes should be close to those of novel classes (min-margin), which improves the difficulty of classification.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>Framework of the proposed few-shot detection which consists of a support branch and a query branch. This figure only illustrates base class training driven by detection loss and max-margin loss.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>Network finetuning with feature disturbance. Feature disturbance is implemented by truncating the gradient maps and re-sampling the training images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>t-SNE visualization of prototypes produced by the baseline method<ref type="bibr" target="#b6">[7]</ref> and our proposed CME approach. The novel classes are in bold font.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 .</head><label>6</label><figDesc>Comparison of detection results of the baseline method and the proposed CME approach. Red boxes indicate false detection results, green boxes indicate true detection results and blue boxes indicate missed objects.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Algorithm 1 Detector training and finetuning with CME Input:Support set S = {I S , M S }, query set Q = {I Q , M Q }; Output: Network parameters ? = ? S ? ? Q ? ? P ; Training: for (I S , M S , I Q , M Q in D C base )do Predict detections and calculate detection loss L det by Eq. 1; Calculate margin loss L mrg by Eq. 3 Update ? to minimize the training loss L trn by Eq. 7; end for Finetuning: for (each I S , M S, I Q , M Q in D C base ? D C novel ) dofor (iteration do Predict detections and calculate the detection loss L</figDesc><table><row><cell>end for</cell></row><row><cell>end for</cell></row><row><cell>feature disturbance, the support mask M S are updated ac-</cell></row><row><cell>cording to the gradients map, as</cell></row></table><note>det by Eq. 1; Calculate margin loss L mrg by Eq. 3 Update ? to minimize the finetuning loss L trn by Eq. 7 with back-propagation; Update support mask M S according to Eq. 6.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Ablation study of number of output channels in the feature filtering module on Pascal VOC Novel class (split-1).</figDesc><table><row><cell>Num.</cell><cell cols="2">Shots 1</cell><cell>2</cell><cell>3</cell><cell>5</cell><cell cols="2">10 avg. ?</cell></row><row><cell cols="2">W/O FF</cell><cell cols="5">13.5 21.9 28.5 40.2 47.0</cell></row><row><cell>1024</cell><cell></cell><cell cols="6">13.6 19.9 27.5 36.0 48.2 -1.2</cell></row><row><cell>512</cell><cell></cell><cell cols="6">13.2 23.5 29.9 43.1 49.8 +2.9</cell></row><row><cell>256</cell><cell></cell><cell cols="6">16.3 22.6 27.3 37.7 47.9 +1.3</cell></row><row><cell cols="8">Table 3. Ablation study of self-disturbance on Pascal VOC novel</cell></row><row><cell cols="2">classes (split-1).</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Method C N ovel C Base</cell><cell>1</cell><cell>2</cell><cell>Shots 3</cell><cell>5</cell><cell>10</cell><cell>avg. ?</cell></row><row><cell></cell><cell></cell><cell cols="5">13.2 23.4 29.9 43.1 49.8</cell></row><row><cell></cell><cell></cell><cell cols="6">13.0 22.7 29.5 43.5 49.5 -0.2</cell></row><row><cell></cell><cell></cell><cell cols="6">17.8 26.1 31.5 44.8 47.5 +1.7</cell></row><row><cell></cell><cell></cell><cell cols="6">16.0 24.9 31.0 43.9 49.5 +1.2</cell></row><row><cell cols="8">Table 4. Comparison of feature disturbance strategies on Pascal</cell></row><row><cell cols="7">VOC novel classes (split-1). "trun." denotes truncation.</cell></row><row><cell>Manner</cell><cell cols="2">Shots 1</cell><cell>2</cell><cell>3</cell><cell>5</cell><cell cols="2">10 avg. ?</cell></row><row><cell cols="7">w/o disturbance 13.2 23.4 29.9 43.1 49.8</cell></row><row><cell cols="8">Random sample 15.2 23.2 31.4 42.2 48.8 +0.3</cell></row><row><cell cols="2">Random crop</cell><cell cols="6">15.7 21.9 32.5 43.9 46.6 +0.2</cell></row><row><cell cols="2">Feature trun.</cell><cell cols="6">14.5 23.1 31.8 43.6 48.8 +0.5</cell></row><row><cell cols="2">Gradient trun.</cell><cell cols="6">17.8 26.1 31.5 44.8 47.5 +1.7</cell></row><row><cell cols="8">0.001, momentum of 0.9 for 80,200 iterations in base train-</cell></row><row><cell cols="8">ing and 2000 iterations in finetuning. There are 64 query</cell></row><row><cell cols="8">images per batch and 2 support images for each class.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 .</head><label>5</label><figDesc>Detection performance comparison on the Pascal VOC dataset.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Novel set 1</cell><cell></cell><cell></cell><cell cols="3">Novel set 2</cell><cell></cell><cell></cell><cell cols="3">Novel set 3</cell></row><row><cell cols="2">Framework Method</cell><cell>Shots</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>5</cell><cell>10</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>5</cell><cell>10</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>5</cell><cell>10</cell></row><row><cell></cell><cell cols="2">LSTD [1]</cell><cell cols="15">8.2 11.0 12.4 29.1 38.5 11.4 3.8 5.0 15.7 31.0 12.6 8.5 15.0 27.3 36.3</cell></row><row><cell>YOLO</cell><cell cols="17">Meta YOLO [7] 14.8 15.5 26.7 33.9 47.2 15.7 15.3 22.7 30.1 40.5 21.3 25.6 28.4 42.8 45.9 MetaDet [27] 17.1 19.1 28.9 35.0 48.8 18.2 20.6 25.9 30.6 41.5 20.1 22.3 27.9 41.9 42.9</cell></row><row><cell></cell><cell cols="2">CME (Ours)</cell><cell cols="15">17.8 26.1 31.5 44.8 47.5 12.7 17.4 27.1 33.7 40.0 15.7 27.4 30.7 44.9 48.8</cell></row><row><cell></cell><cell cols="2">MetaDet [27]</cell><cell cols="15">18.9 20.6 30.2 36.8 49.6 21.8 23.1 27.8 31.7 43.0 20.6 23.9 29.4 43.9 44.1</cell></row><row><cell></cell><cell cols="17">Meta R-CNN [30] 19.9 25.5 35.0 45.7 51.5 10.4 19.4 29.6 34.8 45.4 14.3 18.2 27.5 41.2 48.1</cell></row><row><cell>F-RCNN</cell><cell cols="17">Viewpoint [29] 24.2 35.3 42.2 49.1 57.4 21.6 24.6 31.9 37.0 45.7 21.2 30.0 37.2 43.8 49.6 TFA w/cos [24] 39.8 36.1 44.7 55.7 56.0 23.5 26.9 34.1 35.1 39.1 30.8 34.8 42.8 49.5 49.8</cell></row><row><cell></cell><cell cols="2">MPSR [28]</cell><cell cols="15">41.7 42.5 51.4 52.2 61.8 24.4 29.3 39.2 39.9 47.8 35.6 41.8 42.3 48.0 49.7</cell></row><row><cell></cell><cell cols="2">CME (Ours)</cell><cell cols="15">41.5 47.5 50.4 58.2 60.9 27.2 30.2 41.4 42.5 46.8 34.3 39.6 45.1 48.3 51.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 .</head><label>6</label><figDesc>Performance comparison on the MS COCO dataset.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>27.3 9.8 2.5 13.8 19.9 20.0 25.5 25.7 7.5 27.6 38.9 MPSR [28] 9.8 17.9 9.7 3.3 9.2 16.1 15.7 21.2 21.2 4.6 19.6 34.3 CME (Ours) 15.1 24.6 16.4 4.6 16.6 26.0 16.3 22.6 22.8 6.6 24.7 39.7</figDesc><table><row><cell></cell><cell>LSTD [1]</cell><cell cols="2">3.2 8.1</cell><cell cols="7">2.1 0.9 2.0 6.5 7.8 10.4 10.4</cell><cell cols="3">1.1 5.6 19.6</cell></row><row><cell></cell><cell>Meta YOLO [7]</cell><cell cols="9">5.6 12.3 4.6 0.9 3.5 10.5 10.1 14.3 14.4</cell><cell cols="3">1.5 8.4 28.2</cell></row><row><cell></cell><cell>MetaDet [27]</cell><cell cols="9">7.1 14.6 6.1 1.0 4.1 12.2 11.9 15.1 15.5</cell><cell cols="3">1.7 9.7 30.1</cell></row><row><cell>10</cell><cell cols="10">Meta R-CNN [30] 8.7 19.1 6.6 2.3 7.7 14.0 12.6 17.8 17.9</cell><cell cols="3">7.8 15.6 27.2</cell></row><row><cell></cell><cell>TFA w/cos [24]</cell><cell>10.0</cell><cell>-</cell><cell>9.3</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="11">Viewpoint [29] LSTD [1] Meta YOLO [7] MetaDet [27] 12.5 30 6.7 15.8 5.1 0.4 2.9 12.3 10.9 14.3 14.3 9.1 19.0 7.6 0.8 4.9 16.8 13.2 17.7 17.8 11.3 21.7 8.1 1.1 6.2 17.3 14.5 18.9 19.2 Meta R-CNN [30] 12.4 25.3 10.8 2.8 11.6 19.0 15.0 21.4 21.7</cell><cell cols="3">0.9 7.1 27.0 1.5 10.4 33.5 1.8 11.1 34.4 8.6 20.0 32.1</cell></row><row><cell></cell><cell>TFA w/cos [24]</cell><cell>13.7</cell><cell>-</cell><cell>13.4</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>Viewpoint [29]</cell><cell cols="9">14.7 30.6 12.2 3.2 15.2 23.8 22.0 28.2 28.4</cell><cell cols="3">8.3 30.3 42.1</cell></row><row><cell></cell><cell>MPSR [28]</cell><cell cols="9">14.1 25.4 14.2 4.0 12.9 23.0 17.7 24.2 24.3</cell><cell cols="3">5.5 21.0 39.3</cell></row><row><cell></cell><cell>CME (Ours)</cell><cell cols="9">16.9 28.0 17.8 4.6 18.0 29.2 17.5 23.8 24.0</cell><cell cols="3">6.0 24.6 42.5</cell></row><row><cell></cell><cell>Meta Yolo</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="3">Meta Yolo with Max-margin</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">Our Method</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">LSTD: A low-shot transfer detector for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yali</forename><surname>Hao Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoyou</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<editor>Sheila A. McIlraith and Kilian Q. Weinberger</editor>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="2836" to="2843" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Modelagnostic meta-learning for fast adaptation of deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1126" to="1135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Fast R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1440" to="1448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Collect and select: Semantic alignment metric learning for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fusheng</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fengxiang</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianzhong</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8460" to="8469" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Low-shot visual recognition by shrinking and hallucinating features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3037" to="3046" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Task agnostic meta-learning for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammad</forename><forename type="middle">Abdullah</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamal</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guo-Jun</forename><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="111719" to="111727" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Few-shot object detection via feature reweighting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingyi</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ICCV</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="8419" to="8428" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Boosting few-shot learning with adaptive margin loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aoxue</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiran</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenguo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="12573" to="12581" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">AMN: Attention Metric Network for One-Shot Remote Sensing Image Scene Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<idno>2020. 2</idno>
	</analytic>
	<monogr>
		<title level="j">Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">24</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="936" to="944" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Negative margin matters: Understanding margin in few-shot classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV, 2020</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Anti-aliasing semantic reconstruction for few-shot semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Binghao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbin</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Xiangyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixiang</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Harmonic feature activation for few-shot semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="3142" to="3153" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">SSD: single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><forename type="middle">E</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Yang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">9905</biblScope>
			<biblScope unit="page" from="21" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Optimization as a model for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sachin</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santosh</forename><surname>Kumar Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="779" to="788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">YOLO9000: better, faster, stronger</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Faster R-CNN: towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1137" to="1149" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning to compare: Relation network for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Flood</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongxin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">M</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1199" to="1208" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning compositional representations for few-shot recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Tokmakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Xiong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martial</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6372" to="6381" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Matching networks for one shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3630" to="3638" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">C-mil: Continuation multiple instance learning for weakly supervised object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fang</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyang</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbin</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixiang</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2199" to="2208" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Min-entropy latent model for weakly supervised object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fang</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengxu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenjun</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbin</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixiang</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Machine Intell</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2395" to="2409" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Frustratingly simple few-shot object detection. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">E</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><forename type="middle">E</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Low-shot learning from imaginary data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Xiong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hebert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hariharan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7278" to="7286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning to learn: Model regression networks for easy small sample learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Xiong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martial</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="616" to="634" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Metalearning to detect rare objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Xiong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martial</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ICCV</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="9924" to="9933" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Multi-scale positive sample refinement for few-shot object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songtao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<editor>Andrea Vedaldi, Horst Bischof, Thomas Brox, and Jan-Michael Frahm</editor>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="456" to="472" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Few-shot object detection and viewpoint estimation for objects in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renaud</forename><surname>Marlet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV, 2020</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Meta R-CNN: towards general solver for instance-level low-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaopeng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziliang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anni</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="9576" to="9585" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Prototype mixture models for few-shot semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boyu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bohao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbin</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixiang</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<biblScope unit="volume">12353</biblScope>
			<biblScope unit="page" from="763" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deepemd: Few-shot image classification with differentiable earth mover&apos;s distance and structured classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujun</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guosheng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="12200" to="12210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Freeanchor: Learning to match anchors for visual object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaosong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fang</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rongrong</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixiang</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="147" to="155" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Soft proposal networks for weakly supervised object localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanzhao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixiang</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbin</forename><surname>Jiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1859" to="1868" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
