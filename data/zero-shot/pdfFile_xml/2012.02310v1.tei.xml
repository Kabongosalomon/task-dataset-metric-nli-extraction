<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">BoxInst: High-Performance Instance Segmentation with Box Annotations</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Tian</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The University of Adelaide</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The University of Adelaide</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlong</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The University of Adelaide</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The University of Adelaide</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">BoxInst: High-Performance Instance Segmentation with Box Annotations</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>Code is available at: https://git.io/AdelaiDet</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T10:14+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="figure">Figure 1</ref> <p>-Some qualitative results of BoxInst with the ResNet-101 based model achieving 33.0% mask AP on COCO val2017. The model is trained without any mask annotations and can infer at 10 FPS on a 1080Ti GPU. Best viewed on screen.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>We present a high-performance method that can achieve mask-level instance segmentation with only bounding-box annotations for training. While this setting has been studied in the literature, here we show significantly stronger performance with a simple design (e.g., dramatically improving previous best reported mask AP of 21.1% [12] to 31.6% on the COCO dataset). Our core idea is to redesign the loss of learning masks in instance segmentation, with no modification to the segmentation network itself. The new loss functions can supervise the mask training without relying on mask annotations. This is made possible with two loss terms, namely, 1) a surrogate term that minimizes the discrepancy between the projections of the ground-truth box and the predicted mask; 2) a pairwise loss that can exploit the prior that proximal pixels with similar colors are very likely to have the same category label.</p><p>Experiments demonstrate that the redesigned mask loss can yield surprisingly high-quality instance masks with only box annotations. For example, without using any mask annotations, with a ResNet-101 backbone and 3? training * Corresponding author. schedule, we achieve 33.2% mask AP on COCO test-dev split (vs. 39.1% of the fully supervised counterpart). Our excellent experiment results on COCO and Pascal VOC indicate that our method dramatically narrows the performance gap between weakly and fully supervised instance segmentation.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Instance segmentation requires the algorithm to predict the pixel-wise masks and categories of instances of interest, and is one of the most fundamental tasks in computer vision. The performance of instance segmentation has been significantly advanced by a number of recent successful methods <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b27">27,</ref><ref type="bibr" target="#b29">29,</ref><ref type="bibr" target="#b30">30]</ref>. These methods have almost made the previously much more challenging instance segmentation task be as simple and fast as bounding-box object detection. For example, built on the detector FCOS <ref type="bibr" target="#b28">[28]</ref>, CondInst <ref type="bibr" target="#b27">[27]</ref> only adds very compact dynamic mask heads to predict instance masks, and thus only introduces less than 10% computation time, compared to FCOS. Instance segmentation is able to provide more accurate and finer mask- x 0 x 1 <ref type="figure">Figure 2</ref> -The two proposed loss terms. Top row: the projections onto x-axis and y-axis of the mask and the box, and the projections should be the same, where (x0, y0) and (x1, y1) are the two corners of the box. Bottom row: the pairwise term. For each pixel, we compute the pairwise label consistency between the pixel and its 8 neighbours (with dilation rate 2). Thus each pixel has 8 edges and we have 8 consistency maps in the right. The white locations in the right figure are the edges we have the supervision derived from the color similarity, and other edges are discarded in the loss computation.</p><p>level object location than detection. Thus, given that the extra computation cost is negligible, instance segmentation should be preferred over bounding box detection in many cases. For example, if a robot wants to grasp an object, an accurate mask will be much more helpful than a box. Now the main obstacle that impedes instance segmentation replacing box detection is the significantly heavier pixel-wise mask annotations. Compared to box-level annotations required by object detection, annotating pixel-level masks is notoriously time-consuming, as shown in <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b18">18]</ref>. Here we aim to eliminate this obstacle by training instance segmentation using box annotations only. A few works <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b18">18,</ref><ref type="bibr" target="#b21">21,</ref><ref type="bibr" target="#b23">23,</ref><ref type="bibr" target="#b25">25]</ref> attempted to obtain (semantic or instance-level) mask prediction with boxlevel annotations. Among them, most methods such as Box-Sup <ref type="bibr" target="#b5">[6]</ref> and Box2Seg <ref type="bibr" target="#b18">[18]</ref> rely on the region proposals that are generated by MCG <ref type="bibr" target="#b22">[22]</ref> or GrabCut <ref type="bibr" target="#b24">[24]</ref>. One drawback might be the slow training procedure since these algorithms are hard to be parallelized by modern GPUs. Moreover, in order to achieve good performance, some methods often require iterative training, resulting in a complicated training pipeline and more hyper-parameters. Most importantly, none of these methods is able to show strong weakly-supervised performance on large benchmarks such as COCO <ref type="bibr" target="#b20">[20]</ref>. Thus almost all of them are only evaluated on small datasets such as Pascal VOC <ref type="bibr" target="#b7">[8]</ref>.</p><p>In this work, we propose a simple, single-shot and high-performance box-supervised instance segmentation method, built upon the recent fully convolutional instance segmentation framework-CondInst <ref type="bibr" target="#b27">[27]</ref>. Our core idea is to replace the original pixel-wise mask losses in CondInst with a carefully designed mask loss consisting of two terms. The first term minimizes the discrepancy between the horizontal and vertical projections of the predicted mask and the ground-truth box (see <ref type="figure">Fig. 2 top)</ref>. This essentially ensures that the tightest box covering the predicted mask matches the ground-truth box. Since the ground-truth mask and ground-truth box have the same projections on the two axes 1 , this can be also viewed as a surrogate term that minimizes the discrepancy between the projections of the predicted mask and ground-truth mask. This loss term can be computed when we only have box annotations. Clearly, with this projection term, multiple masks can be projected to a same box. Therefore the projection loss alone would not suffice. Thus, we introduce the second loss term, encouraging the prediction and ground-truth masks have the same pairwise label similarity in proximal pixels ( <ref type="figure">Fig. 2  bottom)</ref>. At first glance, the pairwise similarity of the ground-truth masks cannot be computed if we do not have the mask annotations. With only box annotations available, in principle this pairwise supervision signal is inevitably noisy. However, an important observation is that the proximal pixels with similar colors are very likely to have the same label. Thus, we show that it is empirically plausible to determine a color similarity threshold such that only confident pairs of pixels having a same label are used in the loss computation (the white regions in the bottom right of <ref type="figure">Fig. 2</ref>), thus largely eliminating supervision noises. Using these two loss terms, we achieve stunning instance segmentation results without using any mask annotations. Some qualitative results are shown in <ref type="figure">Fig. 1</ref>. Even though ideas that are relevant to either of our two observations mentioned above were studied more or less in the literature, ranging from non-deep learning methods such as CRF <ref type="bibr" target="#b17">[17]</ref> and GrabCut <ref type="bibr" target="#b24">[24]</ref> to deep learning-based methods such as Box2Seg <ref type="bibr" target="#b18">[18]</ref> and BBTP <ref type="bibr" target="#b11">[12]</ref>, none of these works effectively incorporates them into a simple and appropriate framework. As a result, and more importantly, performance of existing methods on large challenging datasets (e.g., COCO) is far away from that of the full potential of box-supervised instance segmentation that is achievable, as we are going to reveal here. In summary, our method, termed BoxInst, enjoys the following advantages.</p><p>? The proposed method can achieve instance segmentation with box supervision by introducing two loss terms to the instance segmentation framework CondInst <ref type="bibr" target="#b27">[27]</ref>. Instance segmentation has long been believed to be much more challenging to solve than bounding box detection. Our strong performance of instance segmentation using only box supervision shows that it may not necessarily be the case.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.">Related Work</head><p>Box-supervised Semantic Segmentation. A few works attempted to obtain semantic masks using box annotations. For example, BoxSup <ref type="bibr" target="#b5">[6]</ref> uses the region proposals from MCG as the pseudo labels to train an FCN, and an iterative training algorithm is used to refine the estimated masks. The recent Box2Seg <ref type="bibr" target="#b18">[18]</ref> method employs the masks generated by GrabCut to supervise training of the mask prediction model. In addition, a per-class attention map is also predicted by the model to make the per-pixel cross entropy loss focus on foreground pixels and refine the segmentation boundaries. This method shows excellent performance on Pascal VOC <ref type="bibr" target="#b7">[8]</ref>. Authors of <ref type="bibr" target="#b25">[25]</ref> propose to use the unsupervised CRF <ref type="bibr" target="#b17">[17]</ref> to generate the segment proposals. Additionally, a class-wise filling rate loss to supervise the models for training, resulting in improved segmentation performance. One of the crucial steps in these methods is to employ the pseudo labels generated by unsupervised segmentation methods such as MCG <ref type="bibr" target="#b22">[22]</ref> or GrabCut <ref type="bibr" target="#b24">[24]</ref>. This is because these method all rely on pixel-wise mask loss functions, thus not being able to work without mask annotations. In this work, we remove the dependency on pixel-wise mask losses, as a result, eliminating the region proposals. Our new loss functions ensure that mask prediction can still be imperfectly supervised without using any mask annotations. Box-supervised Instance Segmentation. In the context of deep learning, instance segmentation with box annotations has not explored too much yet. SDI <ref type="bibr" target="#b15">[16]</ref> might be the first instance segmentation framework with box annotations. Similar to the methods for semantic segmentation, SDI also relies on the region proposals generated by MCG. Then they make use of an iterative training procedure to further refine the segmentation results. Recently, BBTP <ref type="bibr" target="#b11">[12]</ref> formulates the box-supervised instance segmentation into a multiple instance learning (MIL) problem. BBTP is built on Mask R-CNN and samples the positive and negative bags according to the ROIs on CNN feature maps. In contrast, our method is built on ROI-free CondInst <ref type="bibr" target="#b27">[27]</ref> and employs the proposed projection loss term to supervise the mask learning, eliminating the need for sampling. BBTP also makes use of the pairwise term. However, their pairwise term is defined on the set containing all neighboring pixel pairs with the oversimplified assumption of spatially neighboring pixel pairs being encouraged to have the same label, inevitably introducing heavily noisy supervision. The crucial prior derived from proximal pixels' colors was not exploited in BBTP. Our experiments in <ref type="table">Table 0a</ref> show that, the heavily noisy supervision can have a negative impact on accuracy.</p><p>As a result, we significantly outperform the mask AP of BBTP on COCO by an absolute 10%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Approach</head><p>Conditional Convolutions for Instance Segmentation (CondInst) Here, we briefly describe CondInst <ref type="bibr" target="#b27">[27]</ref>. The main goal of CondInst is to solve instance segmentation in an RoI-free fully convolutional way. In CondInst, they believe that the most challenging piece in instance segmentation is that the prediction of each pixel varies according to the instance to be predicted. For example, when the model is predicting the mask for instance A, the pixels of instance B should be predicted as background. However, when the target instance is B, the pixels of B turn to be foreground. This poses the main challenge for the FCNs' application on instance segmentation because the traditional FCNs can only make deterministic prediction for each pixel.</p><p>CondInst <ref type="bibr" target="#b27">[27]</ref> proposes to employ dynamic filters [15] to address the above issues. Instead of using a fixed mask head as in Mask R-CNN, according to the instance to be predicted, CondInst dynamically adapts the weights of the mask heads, as shown in <ref type="figure">Fig. 3</ref>, thus bypassing the above issue. With the instance-aware mask heads, CondInst can obtain the mask of each instance in the fashion of FCNs, eliminating the RoI operations. Notably, CondInst can generate the full-image instance masks, not only the masks within RoIs as in Mask R-CNN, as shown in <ref type="figure">Fig. 3</ref>  <ref type="figure">Figure 3</ref> -Illustration of CondInst <ref type="bibr" target="#b27">[27]</ref>. CondInst employs the instance-aware dynamically-generated mask heads to obtain the full-image instance masks. We refer readers to <ref type="bibr" target="#b27">[27]</ref> for more details.</p><p>played a crucial role in the box-supervised settings. Also, the mask head only predicts class-agnostic masks. The instance's category is determined by the detector's classification branch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Projection and Pairwise Affinity Mask Loss</head><p>Projection loss term. As mentioned before, the first term supervises the horizontal and vertical projections of the predicted mask using the ground-truth box annotation, which ensures that the tightest box covering the predicted mask matches the ground-truth box. Formally, let b b b ? {0, 1} H?W be the mask generated by assigning 1 to the locations in the ground-truth box and 0 otherwise, as shown in <ref type="figure">Fig. 2</ref> (top-right). Then we have</p><formula xml:id="formula_0">Proj x (b b b) = l l l x , Proj y (b b b) = l l l y ,<label>(1)</label></formula><p>where Proj x : R H?W ? R W and Proj y : R H?W ? R H indicate that projecting the mask onto x-axis and y-axis, respectively. l l l x ? {0, 1} W denotes the 1-D segmentation mask on x-axis and the same applies to l l l y . The process of projection is illustrated in <ref type="figure">Fig. 2 (top row)</ref>. The projection operation can be implemented by a max operation along with each axis. Formally, we define</p><formula xml:id="formula_1">Proj x (b b b) = max y (b b b) = l l l x , Proj y (b b b) = max x (b b b) = l l l y ,<label>(2)</label></formula><p>where max y and max x are the max operations along with y-axis and x-axis, respectively. Letm m m ? (0, 1) H?W be the network predictions for the instance mask, which can be viewed as the probabilities being foreground (i.e., the label is 1). We apply the same projection operations of Eq. (2) to the mask predictions and obtain the corresponding projectionsl l l x andl l l y . We then compute the loss between the projections of the ground-truth box and the predicted mask. The projection loss term is defined as:</p><formula xml:id="formula_2">L proj = L(Proj x (m m m), Proj x (b b b)) + L(Proj y (m m m), Proj y (b b b)) = L(max y (m m m), max y (b b b)) + L(max x (m m m), max x (b b b)) = L(l l l x , l l l x ) + L(l l l y , l l l y ),<label>(3)</label></formula><p>where L(?, ?) is the Dice loss as in CondInst 2 . Note that all the operations in the last equation are (sub-)differentiable. This loss function is applied to all the instances in a training image and the final loss is their average. As shown in our experiments, by using this projection loss term, we can already obtain decent instance segmentation results without using any mask annotations. Pairwise affinity loss term. In almost all instance segmentation frameworks such as Mask R-CNN and CondInst, they supervise the predicted masks in a per-pixel fashion. The pixelwise supervision becomes unavailable if we do not have the mask annotations. Here, we attempt to supervise the mask in a pairwise way, and we will show this supervision can be partially available even if we do not have any mask annotations.</p><p>Now, assume we have the ground-truth masks. Consider an undirected graph G = (V, E) built on an image, where V is the set of the pixels in the image, and E is the set of the edges. Each pixel is connected with its K ? K ? 1 neighbours (the dilation trick may be applied), as shown in <ref type="figure">Fig. 2</ref> (bottom left). Then we define y e ? {0, 1} be the label for the edge e, where y e = 1 means the two pixels linked by the edge have the same ground-truth label and y e = 0 means their labels are different. Let pixels (i, j) and (l, k) be the two endpoints of the edge e. The network predictio? m m m i,j ? (0, 1) can be viewed as the probability of pixel (i, j) being foreground. Then the probability of y e = 1 is P (y e = 1) =m m m i,j ?m m m k,l + (1 ?m m m i,j ) ? (1 ?m m m k,l ), <ref type="bibr" target="#b3">(4)</ref> and P (y e = 0) = 1 ? P (y e = 1). By convention, the probability distribution from the network prediction can be trained with the binary cross entropy (BCE) loss. Thus, the loss function is</p><formula xml:id="formula_3">L pairwise = ? 1 N e?Ein y e log P (y e = 1) +(1 ? y e ) log P (y e = 0),<label>(5)</label></formula><p>where E in is the set of the edges containing at least one pixel in the box. Using E in instead of E here can prevent the loss from being dominated by a large number of the pixels outside the box. N is the number of the edges in E in . If only the pairwise loss is used to supervise the mask learning (in the fully-supervised setting), ideally, two possible solutions may be obtained. The first one is the same as the ground-truth mask m m m, which is desirable. The second solution is the inverse 1 ? m m m. Fortunately, the second solution can be easily eliminated as long as we have a resolved label for any pixel. This can be achieved by the projection loss term because it ensures that the pixels outside the box is background. Note that the edges in E in still involve some pixels outside the box, which are of great importance to help the model get rid of the undesirable solutions. Overall, the total loss for mask learning can be formulated as</p><formula xml:id="formula_4">L mask = L proj + L pairwise .<label>(6)</label></formula><p>We will show in experiments that the redesigned mask loss can have similar performance to the original pixelwise one in the fully-supervised settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Learning without Mask Annotations</head><p>Thus far, we have shown that we can employ Eq. (6) to train the mask. In Eq. (6), the first term L proj is always valid no matter we have box or mask annotations. At first glance, the second term L pairwise still requires the mask annotations to compute the edge's label y e . However, an important observation is that if two pixels have similar colors, they are very likely to have the same labels as well (i.e., the corresponding edge's label is 1). Thus, we can determine a color similarity threshold ? such that the edge's label is 1 with a high probability if its color similarity is above ? . Formally, the color similarity is defined as</p><formula xml:id="formula_5">S e = S(c c c i,j , c c c l,k ) = exp ? ||c c c i,j ? c c c l,k || ? ,<label>(7)</label></formula><p>where S e be the color similarity of the edge e, and c c c i,j and c c c l,k are, respectively, the color vectors of the two pixels (i, j) and (l, k) linked by the edge. Here we use the LAB color space as it is closer to human perception. ? is a hyperparameter, being 2 in this work. Then we can compute the pairwise loss for these confident edges only and discard the edges with agnostic labels. As a result, the pairwise loss becomes  <ref type="formula" target="#formula_3">5)</ref> for positive edges because we can only infer that the label of e is 1 if S e ? ? ; but the label is agnostic if S e &lt; ? . We compute the proportion of the positive edges in the edges with S e ? ? with mask annotations on the COCO val2017 split. <ref type="figure" target="#fig_1">Fig. 4</ref> (blue curve) shows that the change of the proportion of the positive edges in the edges with S e ? ? as the threshold ? increases. As shown in <ref type="figure">figure,</ref> if the threshold is 0.1, more than 98% of the edges are positive. The proportion can be further improved if we continue to increase ? , but an overlarge threshold would reduce the number of the supervised edges (red curve in <ref type="figure" target="#fig_1">Fig. 4</ref>). We need to trade off between them.</p><p>As we only have positive labels in Eq. <ref type="formula">(8)</ref>, one may note this would result in two possible trivial solutions, i.e., the masks of all the pixels being 0 or 1. However, the masks with all pixels being 0 do not meet the projection term; and the masks of all pixels being 1 almost never appear since the pairwise term encourages the pixels near the box boundaries to be negative if their colors are similar to that of the negative pixels outside the box.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Experiments</head><p>We conduct experiments on COCO <ref type="bibr" target="#b20">[20]</ref> and Pascal VOC <ref type="bibr" target="#b7">[8]</ref>. For COCO, the models are trained with train2017 (115K images) and the ablation experiments are evaluated on val2017 (5K images). Unless specified, only box annotations are used during training. Our main results are reported on test-dev. For Pascal VOC, following previous works <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b15">16]</ref>, we train the models on the augmented Pascal VOC 2012 dataset <ref type="bibr" target="#b8">[9]</ref> with 10, 582 training images, and evaluate them on Pascal VOC 2012 val split with 1, 449 images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Implementation Details</head><p>Our proposed method only requires to change the mask loss in CondInst. Other training and testing details are kept as similar as possible to the original CondInst. On COCO, unless specified, we use the following training details. The models are trained for 90K iterations with batch size <ref type="bibr" target="#b15">16</ref>   <ref type="table">Table 1</ref> -Ablation study on the hyper-parameters in the proposed mask loss on the COCO val2017 split. "prop." is the proportion of the positive edges in the edges with Se ? ? . "fully-sup.": fully-supervised results. As shown in <ref type="table">Table 0a</ref>, by using ? = 0.1, BoxInst can achieve 30.7 mask AP with only box annotations, which is close the fully-supervised mask AP (35.4%) and significantly better localization precision than boxes (10.6% mask AP as shown in <ref type="table" target="#tab_4">Table 3</ref>).  <ref type="table">Table 2</ref> -The projection and pairwise affinity mask loss vs. the original pixelwise one in the fully-supervised settings. As we can see here, they attain very similar mask AP on the COCO split val2017. 8 V100 GPUs (2 images per GPU). The initial learning rate is set to 0.01 and reduced by a factor of 10 at step 60K and 80K, respectively. ResNet-50 <ref type="bibr" target="#b10">[11]</ref> is used as the backbone and is initialized with the ImageNet <ref type="bibr" target="#b6">[7]</ref> pre-training weights. The newly added layers are initialized as in <ref type="bibr" target="#b27">[27]</ref>. We use exactly the same data augmentation as in CondInst (e.g., left-right flipping and multi-scale training). For the dynamic mask heads, we use 3 conv. layers as in CondInst, but we increase the channels of the mask heads from 8 to 16, which can result in better performance with negligible extra computational overheads, and the compared baselines are adjusted accordingly. Following CondInst, the output mask is up-sampled to 1 /4 resolution of the input image. For the pairwise loss term, we compute the pairwise relationship within 3 ? 3 patches with the dilation rate being 2. On Pascal VOC, following <ref type="bibr" target="#b11">[12]</ref>, we use batch size 8 and the number of iterations is 20K. The learning rate is reduced by a factor of 10 at step 15K. Only left-right flipping is used as the data augmentation during training. Other settings are the same as on COCO. The inference is the same as the original CondInst on both benchmarks. The performance is evaluated with the COCO-style mask AP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Projection and Pairwise Affinity Loss for Mask Learning</head><p>We first demonstrate that the redesigned mask loss can have similar performance to the original pixelwise mask loss in the fully-supervised settings. The experiments are conducted on COCO. We replace the original Dice loss for mask training in CondInst with the proposed one, and keep other settings exactly the same. As shown in <ref type="table">Table 2</ref>, the proposed mask loss can have similar performance (35.4% vs. 35.6% mask AP), which suggests that using the proposed loss for mask learning is feasible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Box-supervised Instance Segmentation</head><p>The key advantage of the proposed mask loss is that it can still supervise the predicted masks with only box annotations. We confirm this here and conduct experiments to investigate the hyper-parameters in the proposed mask loss. Varying the threshold of color similarity. As mentioned before, we use a color similarity threshold ? to determine the edges that will be used to compute the pairwise loss. Here, we conduct experiments by varying ? . When ? = 0, all of the edges defined by the size of neighborhood are considered positive and used in the loss computation, as shown in Eq. <ref type="bibr" target="#b7">(8)</ref>. As shown in <ref type="table">Table 0a</ref>, in this case, 94.1% of the edges are truly positive, and ?6% of the edges are actually negative. Thus, the loss computation would introduce ?6% noisy labels and all of the truly negative edges are wrongly labelled positive. Unsurprisingly, this experiment yields a trivial solution with poor performance (9.4% mask AP) that almost all pixels in the box are predicted as foreground. If we increase ? to 0.1, the proportion of the truly positive edges are improved to 98.3%, and only less than ?2% of the edges are wrongly labelled. As a result, the model can yield high-quality instance masks, achieving 30.7% mask AP (vs. fully-supervised counterpart 35.4%). This result is even better than that of some fully-supervised methods method backbone aug. sched. AP AP50 AP75 APS APM APL fully supervised methods:</p><p>Mask R-CNN <ref type="bibr">[</ref>  <ref type="table">Table 4</ref> -Comparisons with state-of-the-art methods on the COCO test-dev split. " ?" means that the results are on the COCO val2017 split. BBTP only reported the results on the val2017 split. Our BoxInst outperforms the previous best reported mask AP by over absolute 10% mask AP. Ours even outperforms two recent fully supervised methods, YOLACT and PolarMask, and is close to state-of-the-art fully-supervised results. 'DCN': deformable convolutions <ref type="bibr" target="#b34">[34]</ref>. '1?' means 90K iterations. such as YOLACT and PolarMask. Some qualitative results are shown in <ref type="figure">Fig. 1</ref>. If we further increase the threshold ? to 0.2, the performance slightly drops to 30.6% mask AP. This might be because the number of the supervised positive edges decreases as we increase the threshold, as shown in <ref type="figure" target="#fig_1">Fig. 4</ref>. Varying the neighborhood of the pixels. We conduct experiments with the different neighbours for each pixel. The size (i.e., K) defines how many surrounding pixels of each pixel are used to compute the pairwise loss with the pixel. Additionally, we may use the dilation trick to enlarge the scope (as in dilated convolutions). As shown in <ref type="table" target="#tab_3">Table 0b</ref>, by increasing the size from 3 ? 3 to 5 ? 5, the performance is boosted from 29.7% to 30.5%. This suggests that a relatively long-distance pairwise relationship is important to the final performance. However, using 5 ? 5 requires more computational overheads in the training. Thus, we apply the dilation rate 2 to the 3 ? 3 patches. This can capture the long-distance relationship without increasing the computational overheads and achieves similar performance (30.7%). The performance cannot be further improved by applying the dilation trick to the 5 ? 5 patches because the assumption, two pixels with similar colors probably have the same label, might not hold if the two pixels are far from each other. The contribution of each loss term. <ref type="table" target="#tab_4">Table 3</ref> shows the contribution of each loss term. Even if only the first projection term is used, we can also achieve decent performance (21.2% mask AP), which can already provide much higher localization precision than boxes (10.6% mask AP). By further using the proposed pairwise term, high-quality instance masks can be obtained and the performance is largely im-proved to 30.7%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Comparisons with State-of-the-art</head><p>We compare BoxInst with state-of-the-art fully/box supervised instance segmentation methods on the COCO dataset. As shown in <ref type="table">Table 4</ref>, with the same backbone and training settings, BoxInst significantly surpasses the previous best reported result <ref type="bibr" target="#b11">[12]</ref> by absolute 10.5% mask AP (e.g., from 21.1% to 31.6%). BoxInst, without using any mask annotations, performs even better than some recent fully-supervised methods such as PolarMask <ref type="bibr" target="#b31">[31]</ref> and YOLACT <ref type="bibr" target="#b2">[3]</ref>, with the same backbones as well as similar training and testing settings (32.5% with R-101 1? vs. PolarMask 32.1% R-101 2? and YOLACT-700 31.2% R-101 4.5?). BoxInst also demonstrates competitive performance with top-performing fully-supervised instance segmentation methods. For example, with the same backbone ResNet-50-FPN and 3? training schedule, BoxInst achieves 32.1% mask AP (vs. 37.8% of the fully-supervised CondInst <ref type="bibr" target="#b27">[27]</ref>). With BiFPN <ref type="bibr" target="#b26">[26]</ref> and DCN <ref type="bibr" target="#b34">[34]</ref>, the performance can be further boosted to 35.0% mask AP. Some qualitative results are shown in <ref type="figure">Fig. 1</ref>. The excellent performance shows that BoxInst dramatically narrows the performance gap between the fully supervised and box-supervised instance segmentation, and for the first time, the great potential of box-supervised instance segmentation is revealed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Experiments on Pascal VOC</head><p>We also conduct experiments on Pascal VOC. As shown in <ref type="table" target="#tab_6">Table 5</ref>  <ref type="table">Table 6</ref> -BoxInst for semi-supervised instance segmentation. These models are trained with the 20 classes mask annotations and the other 60 classes (i.e., unseen classes) are only with box annotations.</p><p>in AP 50 and AP 75 by a large margin. Notably, the AP 75 is improved by more than relative 200% (17.1% vs. 37.0% mask AP), which suggests BoxInst can produce the masks of much higher quality. BoxInst is even much better than the BBTP with CRF. Additionally, BoxInst also performs much better than SDI <ref type="bibr" target="#b15">[16]</ref>. We also compare BoxInst with the traditional unsupervised segmentation method GrabCut <ref type="bibr" target="#b24">[24]</ref>. In the experiment, GrabCut takes as input the boundingboxes predicted by the ResNet-101 based FCOS in BoxInst. Thus the only difference between BoxInst and GrabCut is the way of obtaining instance masks. As shown in <ref type="table" target="#tab_6">Table 5</ref>, BoxInst is far better than GrabCut (19.0% vs. 36.5% mask AP). Moreover, BoxInst is fully convolutional and can benefit from the highly-efficient GPUs, thus inferring tens of times faster than GrabCut.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.">Extensions: Semi-supervised Instance Segmentation</head><p>In this section, we show that our method can also help the model generalize to unseen categories in the semisupervised setting where only partial classes have the mask annotations. Following previous works <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b19">19,</ref><ref type="bibr" target="#b33">33]</ref> in this setting, we conduct the experiments on the COCO dataset and split the 80 classes in COCO into two groups -20 classes present in Pascal VOC and 60 classes not in Pascal VOC. Then the models are trained with the mask an-Lproj Lpairwise all 80 classes 20 unseen classes AP AP50 AP75 AP AP50 AP75 32.1 51.6 33.9 25.5 45. <ref type="bibr" target="#b4">5</ref>   <ref type="table">Table 7</ref> -BoxInst for semi-supervised instance segmentation. The models are trained with the 60 classes mask annotations and other 20 classes (i.e., unseen classes) are only with box annotations. notations of the classes in one group, and another group of classes only have the box annotations. The generalization ability is evaluated with the mask AP averaged over the group of classes without mask annotations (i.e., unseen classes).</p><p>We first train the model with the 20 classes mask annotations. As shown in <ref type="table">Table 6</ref> (1st row), if our proposed loss terms are not used, where the mask loss is only computed for the instances with mask annotations and other instances are discarded during the mask learning, the model can only achieve 19.9% mask AP on the unseen categories. This low performance suggests that the model is difficult to generalize to unseen classes. If we use the L proj term for the 60 classes without the mask annotations during training, as shown in the table (2nd row), the performance can be dramatically improved to 29.7%. If we further apply the pairwise term L pairwise , the performance can be boosted to 30.9%. Moreover, compared to the setting only using the box annotations (last row in <ref type="table">Table 6</ref>), the performance on the unseen classes is also improved from 29.6% to 30.9%, which suggests that the box-supervised model can benefit from the partial mask annotations. Additionally, the experimental results with the 60 classes masks are shown in Table 7, and the same conclusions can be drawn.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.7.">Extensions: Box-supervised Character Segmentation</head><p>In order to demonstrate the generality of BoxInst, we conduct experiments to obtain the character masks with character box annotations. Our experiments are conducted on the ICDAR 2019 ReCTS dataset <ref type="bibr" target="#b32">[32]</ref>, which contains 20K training images and 5K testing images. These images are annotated with text-line and character-level boxes. We train our model with the character boxes. All the training settings are the same as that of COCO. Since we do not have mask annotations for the testing set, it is impossible to report the mask AP. We instead show some qualitative results in <ref type="figure" target="#fig_3">Fig. 5</ref>, demonstrating that BoxInst can obtain high-quality character masks. The text masks might provide useful cues for detecting and recognising text of arbitrary shapes. We believe that the ability of BoxInst generating character masks automatically may inspire new applications on this task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Conclusions</head><p>In this work, we have proposed BoxInst that can achieve high-quality instance segmentation with only box annotations. The core idea of BoxInst is to replace the original pixelwise mask loss with the proposed projection and pairwise affinity mask loss. With the proposed mask loss, we show excellent instance segmentation performance without using any mask annotations on COCO and Pascal VOC, significantly improving the state-of-the-art.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 -</head><label>4</label><figDesc>The relationship between the edges' labels and the color similarity thresholds. 'blue curve': the proportion of the positive edges in the edges with color similarity above the threshold. 'red curve': the proportion of the supervised positive edges in all the positive edges. The number of positive edges are computed with the ground-truth masks of the COCO val2017 split.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>L pairwise = ? 1 N e?Ein 1</head><label>11</label><figDesc>{Se?? } log P (y e = 1), (8) where 1 {Se?? } is the indicator function, being 1 if S e ? ? and 0 otherwise. Eq. (8) only involves the term in Eq. (</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 -</head><label>5</label><figDesc>Character masks predicted by BoxInst. No mask annotations are used for training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>BoxInst is simple as it does not modify the network model of CondInst at all, only using different loss terms. This means that the inference process of the proposed BoxInst is exactly the same as CondInst, thus naturally inheriting all desirable properties of CondInst.</figDesc><table><row><cell>? BoxInst attains excellent instance segmentation perfor-</cell></row><row><cell>mance on the large-scale benchmark COCO. With the</cell></row><row><cell>ResNet-101 backbone and 3? training schedule, our</cell></row><row><cell>BoxInst achieves 33.2% mask AP on COCO with no</cell></row><row><cell>mask annotations used in training, outperforming a few</cell></row><row><cell>recent fully supervised methods using the same back-</cell></row><row><cell>bone and trained with mask annotations, including</cell></row><row><cell>YOLACT [3] (31.2% AP) and PolarMask [31] (32.1%</cell></row><row><cell>AP). We empirically show that in the semi-supervised</cell></row><row><cell>setting, mask AP of BoxInst can be further improved,</cell></row><row><cell>as expected ( ?3.6).</cell></row><row><cell>? Since instance masks can provide much more precise</cell></row><row><cell>localization than boxes, we envision that BoxInst can</cell></row><row><cell>be used in many downstream tasks to boost their per-</cell></row><row><cell>formance without extra effort of annotating ground-</cell></row><row><cell>truth masks. For example, we can obtain text masks</cell></row><row><cell>using BoxInst (see  ?3.7), which often help text recog-</cell></row><row><cell>nition. BoxInst can also help annotate the mask-level</cell></row><row><cell>training data for the fully-supervised settings.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>.</head><label></label><figDesc>This has</figDesc><table><row><cell></cell><cell cols="3">mask head 1</cell></row><row><cell></cell><cell>conv</cell><cell>conv</cell><cell>conv</cell></row><row><cell></cell><cell></cell><cell>?</cell><cell>?</cell></row><row><cell></cell><cell cols="3">mask head k</cell></row><row><cell></cell><cell>conv</cell><cell>conv</cell><cell>conv</cell></row><row><cell>features</cell><cell cols="3">instance-aware</cell></row><row><cell>w/ rel. coord.</cell><cell cols="3">mask heads</cell></row></table><note>?</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>98.3% 30.7 52.2 31.1 13.8 33.1 45.7 ? = 0.2 98.4% 30.6 52.6 30.9 13.9 32.8 45.5 Varying the size and dilation of the local patches (with ? = 0.1).</figDesc><table><row><cell></cell><cell>prop.</cell><cell cols="5">AP AP50 AP75 APS APM APL</cell><cell cols="4">size dilation AP AP50 AP75 APS APM APL</cell></row><row><cell>fully-sup.</cell><cell>-</cell><cell cols="2">35.4 55.9</cell><cell cols="3">37.6 17.0 38.8 50.7</cell><cell>3</cell><cell>1</cell><cell>29.7 52.0</cell><cell>29.6 13.4 32.3 44.4</cell></row><row><cell>? = 0</cell><cell cols="2">94.1% 9.4</cell><cell>30.3</cell><cell>3.3</cell><cell>7.6</cell><cell>10.3 11.4</cell><cell>3</cell><cell>2</cell><cell>30.7 52.2</cell><cell>31.1 13.8 33.1 45.7</cell></row><row><cell>? = 0.1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>5</cell><cell>1</cell><cell>30.5 52.3</cell><cell>30.7 13.7 33.0 45.7</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>5</cell><cell>2</cell><cell>29.9 51.9</cell><cell>30.0 13.8 32.1 45.0</cell></row><row><cell></cell><cell cols="5">(a) Varying the color similarity threshold ? .</cell><cell></cell><cell>(b)</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>on</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 0b</head><label>0b</label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>shows the experiment results by varying the</cell></row><row><cell cols="3">neighbours of each pixel.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>mask loss</cell><cell>AP</cell><cell cols="5">AP50 AP75 APS APM APL</cell></row><row><cell>Dice loss</cell><cell>35.6</cell><cell>56.3</cell><cell>37.8</cell><cell>16.9</cell><cell>38.9</cell><cell>51.0</cell></row><row><cell>proposed</cell><cell>35.4</cell><cell>55.9</cell><cell>37.6</cell><cell>17.0</cell><cell>38.8</cell><cell>50.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 -</head><label>3</label><figDesc>The mask AP on COCO val2017 by applying the different loss terms. "box mask": using the masks generated by boxes. If both terms are not used, the model can only provide the box-level localization precision (10.6% mask AP).</figDesc><table><row><cell cols="4">Lproj Lpairwise AP AP50 AP75 APS APM APL</cell></row><row><cell>box mask</cell><cell>10.6 32.2</cell><cell>4.6</cell><cell>5.7 11.3 15.6</cell></row><row><cell></cell><cell cols="3">21.2 45.2 17.7 10.0 21.4 32.5</cell></row><row><cell></cell><cell cols="3">30.7 52.2 31.1 13.8 33.1 45.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 -</head><label>5</label><figDesc>, BoxInst achieves state-of-the-art instance segmentation with only box annotations. With the same backbone and training settings, BoxInst outperforms BBTP both Results on Pascal VOC val2012. BoxInst surpasses previous methods by a large margin. Here, the Grab-Cut obtains the instance masks by taking as input the boxes generated by BoxInst. Thus, the only difference between the GrabCut and BoxInst is the way to obtain the masks. Clearly, BoxInst achieves significantly improved mask AP, outperforming previous best by about 10%.</figDesc><table><row><cell>method</cell><cell>backbone</cell><cell>AP</cell><cell cols="2">AP50 AP75</cell></row><row><cell>GrabCut [24]</cell><cell cols="2">ResNet-101 19.0</cell><cell>38.8</cell><cell>17.0</cell></row><row><cell>SDI [16]</cell><cell>VGG-16</cell><cell>-</cell><cell>44.8</cell><cell>16.3</cell></row><row><cell>BBTP [12]</cell><cell cols="2">ResNet-101 23.1</cell><cell>54.1</cell><cell>17.1</cell></row><row><cell cols="3">BBTP w/ CRF ResNet-101 27.5</cell><cell>59.1</cell><cell>21.9</cell></row><row><cell>BoxInst</cell><cell>ResNet-50</cell><cell>34.3</cell><cell>59.1</cell><cell>34.2</cell></row><row><cell>BoxInst</cell><cell cols="2">ResNet-101 36.5</cell><cell>61.4</cell><cell>37.0</cell></row><row><cell>Lproj Lpairwise</cell><cell cols="4">all 80 classes AP AP50 AP75 AP AP50 AP75 60 unseen classes</cell></row><row><cell></cell><cell cols="4">24.7 44.6 24.2 19.9 38.3 18.5</cell></row><row><cell></cell><cell cols="4">31.8 52.5 33.2 29.7 49.3 31.0</cell></row><row><cell></cell><cell cols="4">32.5 53.0 34.0 30.9 50.1 32.4</cell></row><row><cell>box supervised</cell><cell cols="4">30.7 52.2 31.1 29.6 49.7 30.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>25.1 33.1 53.8 34.3 31.6 57.4 30.0 33.8 54.3 35.7 35.9 60.9 36.3 box supervised 30.7 52.2 31.1 29.6 49.7 30.4</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">This may not hold if the instance mask consists multiple disjointed regions.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">One can also use the cross-entropy loss here.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Weakly supervised instance segmentation by learning annotation consistent instances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Arun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">V</forename><surname>Jawahar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pawan</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comp. Vis</title>
		<meeting>Eur. Conf. Comp. Vis</meeting>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">What&apos;s the point: Semantic segmentation with point supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amy</forename><surname>Bearman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vittorio</forename><surname>Ferrari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comp. Vis</title>
		<meeting>Eur. Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="549" to="565" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">YOLACT: Real-time instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Bolya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fanyi</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><forename type="middle">Jae</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comp. Vis</title>
		<meeting>IEEE Int. Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Blendmask: Top-down meets bottom-up for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kunyang</forename><surname>Hao Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongming</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youliang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="8573" to="8581" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Boundary-preserving mask R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianheng</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lichao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyu</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comp. Vis</title>
		<meeting>Eur. Conf. Comp. Vis</meeting>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Boxsup: Exploiting bounding boxes to supervise convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comp. Vis</title>
		<meeting>IEEE Int. Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">ImageNet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The Pascal visual object classes (VOC) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">I</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vision</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Semantic contours from inverse detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Bharath Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubomir</forename><surname>Arbel?ez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhransu</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comp. Vis</title>
		<meeting>IEEE Int. Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="991" to="998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Piotr Doll?r, and Ross Girshick. Mask R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comp. Vis</title>
		<meeting>IEEE Int. Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Weakly supervised instance segmentation using the bounding box tightness prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Chun</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuang-Jui</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chung-Chi</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Yu</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yung-Yu</forename><surname>Chuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Advances in Neural Inf. Process. Syst</title>
		<meeting>Advances in Neural Inf. ess. Syst</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
	<note>Note that the mask AP results on COCO are in the supplementary. available at https://tinyurl.com/yyjovxn6. 1, 2, 3, 5, 6</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning to segment every thing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronghang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4233" to="4241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Mask scoring R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaojin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lichao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongchao</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6409" to="6418" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Dynamic filter networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bert</forename><forename type="middle">De</forename><surname>Brabandere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinne</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Advances in Neural Inf. Process. Syst</title>
		<meeting>Advances in Neural Inf. ess. Syst</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="667" to="675" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Simple does it: Weakly supervised instance and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Khoreva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Hosang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Hein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf</title>
		<meeting>IEEE Conf</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Comp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Patt. Recogn</title>
		<imprint>
			<biblScope unit="page" from="876" to="885" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Efficient inference in fully connected CRFs with Gaussian edge potentials</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Kr?henb?hl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Advances in Neural Inf. Process. Syst</title>
		<meeting>Advances in Neural Inf. ess. Syst</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Box2Seg: Attention weighted loss and discriminative feature learning for weakly supervised segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Viveka</forename><surname>Kulharia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siddhartha</forename><surname>Chandra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amit</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ambrish</forename><surname>Tyagi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comp. Vis</title>
		<meeting>Eur. Conf. Comp. Vis</meeting>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">ShapeMask: Learning to segment novel objects by refining shape priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weicheng</forename><surname>Kuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anelia</forename><surname>Angelova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comp. Vis</title>
		<meeting>IEEE Int. Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9207" to="9216" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Microsoft COCO: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comp. Vis</title>
		<meeting>Eur. Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Weakly-and semi-supervised learning of a deep convolutional network for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comp. Vis</title>
		<meeting>IEEE Int. Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1742" to="1750" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Multiscale combinatorial grouping for image segmentation and object proposal generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordi</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Arbelaez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Barron</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>Ferran Marques, and Jitendra Malik</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Object segmentation from bounding box annotations using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Rajchl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ozan</forename><surname>Oktay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantinos</forename><surname>Kamnitsas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Passerat-Palmbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjia</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mellisa</forename><surname>Damodaram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mary</forename><surname>Rutherford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Hajnal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Kainz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="674" to="683" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">GrabCut: interactive foreground extraction using iterated graph cuts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Kolmogorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Blake</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graphics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="309" to="314" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Box-driven class-wise region masking and filling rate guided loss for weakly supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunfeng</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">EfficientDet: Scalable and efficient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruoming</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10778" to="10787" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Conditional convolutions for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comp. Vis</title>
		<meeting>Eur. Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">FCOS: Fully convolutional one-stage object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comp. Vis</title>
		<meeting>IEEE Int. Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">SOLO: Segmenting objects by locations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuning</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comp. Vis</title>
		<meeting>Eur. Conf. Comp. Vis</meeting>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">SOLOv2: Dynamic and fast instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rufeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Advances in Neural Inf. Process. Syst</title>
		<meeting>Advances in Neural Inf. ess. Syst</meeting>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Polarmask: Single shot instance segmentation with polar representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enze</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peize</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoge</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuebo</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ding</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">ICDAR 2019 robust reading challenge on reading chinese text on signboard</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongsheng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qianyi</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghui</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingkun</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Document Analysis Recogn</title>
		<meeting>Int. Conf. Document Analysis Recogn</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1577" to="1581" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning saliency propagation for semisupervised instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanzhao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbin</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Deformable convnets v2: More deformable, better results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xizhou</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9308" to="9316" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
