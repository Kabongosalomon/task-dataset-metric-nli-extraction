<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Improving Distantly-Supervised Relation Extraction through BERT-based Label &amp; Instance Embeddings</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Despina</forename><surname>Christou</surname></persName>
							<email>christoud@csd.auth.gr</email>
							<affiliation key="aff0">
								<orgName type="department">School of Informatics</orgName>
								<orgName type="institution">Aristotle University of Thessaloniki</orgName>
								<address>
									<postCode>54124</postCode>
									<country key="GR">Greece</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grigorios</forename><surname>Tsoumakas</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Informatics</orgName>
								<orgName type="institution">Aristotle University of Thessaloniki</orgName>
								<address>
									<postCode>54124</postCode>
									<country key="GR">Greece</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Improving Distantly-Supervised Relation Extraction through BERT-based Label &amp; Instance Embeddings</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T18:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Distantly-supervised relation extraction (RE)</head><p>is an effective method to scale RE to large corpora but suffers from noisy labels. Existing approaches try to alleviate noise through multi-instance learning and by providing additional information, but manage to recognize mainly the top frequent relations, neglecting those in the long-tail. We propose RED-SandT (Relation Extraction with Distant Supervision and Transformers), a novel distantlysupervised transformer-based RE method, that manages to capture a wider set of relations through highly informative instance and label embeddings for RE, by exploiting BERT's pretrained model, and the relationship between labels and entities, respectively. We guide REDSandT to focus solely on relational tokens by fine-tuning BERT on a structured input, including the sub-tree connecting an entity pair and the entities' types. Using the extracted informative vectors, we shape label embeddings, which we also use as attention mechanism over instances to further reduce noise. Finally, we represent sentences by concatenating relation and instance embeddings. Experiments in the NYT-10 dataset show that REDSandT captures a broader set of relations with higher confidence, achieving state-of-theart AUC (0.424).</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Relation Extraction (RE) aims to detect semantic relationships between entity pairs in natural texts and has proven to be crucial in various natural language processing (NLP) applications, including question answering, and knowledge-base (KB) population.</p><p>Most RE methods follow a supervised approach, with the required number of labeled training data rendering the whole process time and laborintensive. To automatically construct datasets for RE, <ref type="bibr" target="#b14">(Mintz et al., 2009)</ref> proposed to use distant supervision (DS) from a KB, assuming that if two entities exhibit a relationship in a KB, then all sentences mentioning these entities express this relation. Inevitably, this assumption generates falsepositives and leads distantly-created datasets to contain erroneous labels. To alleviate the wrong labeling problem, <ref type="bibr" target="#b17">(Riedel et al., 2010)</ref> relaxed this assumption so that it does not hold for all instances and along with <ref type="bibr" target="#b5">(Hoffmann et al., 2011;</ref><ref type="bibr" target="#b21">Surdeanu et al., 2012)</ref> proposed multi-instance based learning. Under this setting, classification shifts from instance-level to bag-level, with a bag consisting of all instances that contain a specific entity pair.</p><p>Current state-of-the-art RE methods try to reduce the effect of noisy instances by: i) identifying valid instances through multi-instance learning and selective attention <ref type="bibr" target="#b10">(Lin et al., 2016)</ref>, ii) reducing inner-sentence noise by capturing long-range dependencies using syntactic information from dependency parses <ref type="bibr" target="#b14">(Mintz et al., 2009;</ref><ref type="bibr" target="#b4">He et al., 2018;</ref><ref type="bibr" target="#b11">Liu et al., 2018)</ref>, specialized models like piecewise CNN (PCNN) and graph CNN (GCNN), or wordlevel attention <ref type="bibr" target="#b4">(He et al., 2018)</ref>, and iii) enhancing model effectiveness using external knowledge (i.e. KB entity types <ref type="bibr" target="#b22">(Vashishth et al., 2018)</ref>, entity descriptions <ref type="bibr" target="#b8">(Ji, 2017;</ref><ref type="bibr" target="#b7">Hu et al., 2019)</ref>, relation phrases <ref type="bibr" target="#b22">(Vashishth et al., 2018)</ref>) or transfer knowledge from pre-trained models <ref type="bibr" target="#b0">(Alt et al., 2019)</ref>.</p><p>The study of the above approaches led us to the following core observations. First, among all models used in the literature, the use of a pretrained transformer-based language model (LM) can help in recognizing a broader set of relations, even though at the expense of time and computational resources, and second, the relationship between label and entities can entail valuable information but rarely used over external knowledge. Driven by these observations we inspired to develop a novel transformer-based model that can efficiently capture instance and label embeddings in less complexity so as to drive RE in recognizing a broader set of relations.</p><p>We propose REDSandT (Relation Extraction with Distant Supervision and Transformers), a novel transformer-based RE model for distant supervision. To handle the problem of noisy instances, we guide REDSandT to focus solely on relational tokens by fine-tuning BERT on a structured input, including the sub-tree connecting an entity pair (STP) and the entities' types. The input's RE-specific formation, along with BERT's knowledge from unsupervised pre-training, results in REDSandT generating informative vectors. Using these vectors, we shape relation embeddings representing the entities' distance in vector space. Relation embeddings are then used as relation-wise attention over instance representation to reduce the effect of less-informative tokens. Finally, RED-SandT encodes sentences by concatenating relation and weighted-instance embeddings, with relation classification to occur at bag-level as a weighted sum over its sentences' predictions.</p><p>We chose BERT over other transformer-based models because it considers bidirectionality while training. We assume that this characteristic is important to efficiently capture entities' interactions without requiring an additional task that importantly increases complexity (i.e. fine-tuning an auxiliary objective in GPT <ref type="bibr" target="#b0">(Alt et al., 2019)</ref>).</p><p>The main contributions of this paper can be summarized as follows:</p><p>? We extend BERT to handle multi-instance learning to directly fine-tune the model in a DS setting and reduce error accumulation. ? Relation embeddings captured through BERT fine-tuned on our RE-specific input help to recognize a wider set of relations, including relations in the long-tail. ? Suppressing the input sentence to its relational tokens through STP encoding allowed us to capture informative instance embeddings while preserving low complexity to train our model on modest hardware.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>? Experiments on the NYT-10 dataset show</head><p>REDSandT to surpass state-of-the-art models <ref type="bibr" target="#b22">(Vashishth et al., 2018;</ref><ref type="bibr" target="#b0">Alt et al., 2019)</ref> in AUC (1.0 &amp; 0.2 units respectively) and performance at higher recall values, while achieving a 7-10% improvement in P@{100,200,300} over <ref type="bibr" target="#b0">(Alt et al., 2019)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">REDSandT</head><p>Given a bag of sentences {s 1 , s 2 , ..., s n } that concern a specific entity pair, REDSandT generates a probability distribution on the set of possible relations. REDSandT utilizes BERT pre-trained LM to capture the semantic and syntactic features of sentences by transferring pre-trained commonsense knowledge. We extend BERT to handle multiinstance learning, and we fine-tune the model to classify the relation linking the entity pair given the associated sentences. During fine-tuning, we employ a structured, REspecific input to minimize architectural changes to the model <ref type="bibr" target="#b16">(Radford and Salimans, 2018)</ref>. Each sentence is adapted to a structured text, including the sentences' tokens connecting the entity pair (STP) along with the entities types. We transform the input into a (sub-)word-level distributed representation using BPE and positional embeddings from BERT fine-tuned on our corpora. Then, we form final sentence representation by concatenating relation embedding and sentence representation weighted with the relation embedding. Lastly, we use attention over the bag's sentences to shape bag representation, which is then fed to a softmax layer to get the bag 's relation distribution.</p><p>REDSandT can be summarized in three components, namely sentence encoder, bag encoder, and model training. Each component is described in detail in the following sections with the overall architecture shown in <ref type="figure" target="#fig_0">Figure 1</ref> and 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Sentence Encoder</head><p>Given a sentence x and an entity pair h, t , RED-SandT constructs a distributed representation of the sentence by concatenating relation and instance embeddings. Overall sentence encoding is represented in <ref type="figure" target="#fig_0">Figure 1</ref>, with following sections to examine the sentence encoder parts in a bottom-up way.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1">Input Representation</head><p>Relation extraction requires a structured input that can sufficiently capture the latent relation between an entity pair and its surrounding text. Our input representation encodes each sentence as a sequence of tokens, depicted in the very bottom of <ref type="figure" target="#fig_0">Figure 1</ref>.</p><p>It starts with the head entity type and token(s) followed by delimiter [H-SEP], continues with the tail entity type, and token(s) followed by delimiter [T-SEP] and ends with the token sequence of the sentence's STP path. The whole input starts Several other sentence encodings were attempted 1 with the presented one to perform the best. Moreover, the ablation studies in section 4.2, reveal the importance of encoding entities' types and compressing the original sentence to the belowpresented STP path. Below, we present in brief how we form the sub-tree parse of the input and the entity types. Sub-tree parse of input sentence: We utilize the sub-tree parse (STP) of the input sentence in order to reduce the noisy words within sentence and focus on the relational tokens. Precisely, STP preserves the path of the sentence that connects the two entities with their least common ancestor (LCA)'s parent. Compared to other implementations <ref type="bibr" target="#b11">(Liu et al., 2018)</ref>, who shape the final STP sequence by re-assigning the participating tokens into their original sequence order, we preserve the tokens' order within STP achieving a grammatical normalization of the original sentence.</p><p>Entity Type special tokens: In the extent that every relation puts some constraint on the type of participating entities <ref type="bibr" target="#b22">Vashishth et al., 2018)</ref>, we incorporate the entity type in the model's structured input (see bottom of <ref type="figure" target="#fig_0">Figure 1</ref>). Precisely, we incorporate 18 generic entity types, captured from recognizing NYT-10 sentence's entities with the spaCy model 2 . We assume these types KB-independent and easily accessible with our experiments in section 4.2 indicating their inclusion to improve performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.2">Input Embeddings</head><p>The input embedding h 0 to BERT is created by summing over the positional and byte pair embeddings for each token in the structured input. Byte-pair tokens encoding: To make use of subword information, we tokenize input using bytepair encoding (BPE) <ref type="bibr" target="#b18">(Sennrich et al., 2016)</ref>. We particularly use the tokenizer from the pre-trained model (30,000 tokens), which we extend with 20 task-specific tokens (e.g., [H-SEP], [T-SEP], and the 18 entity type tokens). Added tokens serve a special meaning in the input representation, thus are not split into sub-words by the tokenizer. Positional encoding: Positional encoding is an es-sential part of BERT's attention mechanism. Precisely, BERT learns a unique position embedding to represent each of the input (sub-word) token positions within the sequence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.3">Sentence Representation</head><p>Input sequence is transformed into feature vectors (h L ) using BERT's pre-trained language model, fine-tuned in our task. In spite of common practice to represent the sentence by the [CLS] vector in h L <ref type="bibr" target="#b0">(Alt et al., 2019)</ref>, we argue that not all words contribute equally to sentence representation.</p><p>By encoding the underlying relation as a function of the examining entities and by giving attention to vectors related to this underlying relation, we can further reduce sentence noise and improve precision. Core modules constitute the: relation embedding, entities-wise attention, and relation attention. We examine them below.</p><p>Relation Embedding: We formulate relation embeddings using the TransE model <ref type="bibr" target="#b1">(Bordes et al., 2013)</ref>. TransE model regards the embedding of the underlying relation l as the distance (difference) between h and t embeddings (l i = t i ? h i ), assuming that a relation r holds between an entity pair (h, t). Then, we shape relation embedding for each sentence i by applying a linear transformation on the head and tail entities vectors, activated through a Tanh layer to capture possible nonlinearities:</p><formula xml:id="formula_0">l i = T anh(w l (t i ? h i ) + b l )<label>(1)</label></formula><p>, where w l is the underlying relation weight matrix and b l ? dt is the bias vector. We mark relation embedding as l because it represents the possible underlying relation between the two entities and not the actual relationship r. Head h i and tail t i embeddings reflect only the entities' related tokens, which we capture through simple entities-wise attention, shown below.</p><p>Entities-wise Attention: Head and tail embeddings participating in the relation embedding are created by summing over respective token vectors from BERT's last layer h L . We capture these tokens through head-and tail-wise attention. Headwise attention assigns the weight ? h it to focus on head related tokens and tail-wise attention assigns the weight ? t it to focus on tail related tokens.</p><formula xml:id="formula_1">? h it = 1 if t = head in STP tokens 0 otherwise (2) ? t it = 1 if t = tail in STP tokens 0 otherwise (3)</formula><p>Head h i and tail t i embeddings are then shaped as follows:</p><formula xml:id="formula_2">h i = T t=1 ? h it ? h it (4) t i = T t=1 ? t it ? h it (5)</formula><p>Relation Attention: Even though REDSandT is trained on STP that naturally preserves only relational tokens, we wanted to further reduce possible left noise on sentence-level. For this reason, we use a relation attention to emphasize on sentence tokens that are mostly related to the underlying relation l i . We calculate relation attention ? r by comparing each sentence representation against the learned representation l i for each sentence i:</p><formula xml:id="formula_3">? r = exp(s i l i ) n j=1 exp(s j l i )<label>(6)</label></formula><p>Then, we weight BERT' s last hidden layer h L ? d h with relation embedding:</p><formula xml:id="formula_4">h L = T t=1 ? r ? h it<label>(7)</label></formula><p>Finally, sentence representation s i ? d h * 2 is computed as the concatenation of the relation embedding l i and the sentence's weighted hidden representation h L :</p><formula xml:id="formula_5">s i = l i ; h L<label>(8)</label></formula><p>Several other representation techniques were tested, with the presented method to outperform.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">BAG Encoder</head><p>Bag encoding, i.e., aggregation of sentence representations in a bag, comes to reduce noise generated by the erroneously annotated relations accompanying DS. Assuming that not all sentences contribute equally to the bag representation, we use selective attention <ref type="bibr" target="#b10">(Lin et al., 2016)</ref> to emphasize on sentences that better express the underlying relation.</p><formula xml:id="formula_6">B = i ? i s i ,<label>(9)</label></formula><p>As seen, selective attention represents bag as a weighted sum of the individual sentences. Attention ? i is calculated by comparing each sentence representation against a learned representation r:</p><formula xml:id="formula_7">? i = exp(s i r) n j=1 exp(s j r)<label>(10)</label></formula><p>Finally, bag representation B is fed to a softmax classifier to obtain the probability distribution over the relations.</p><formula xml:id="formula_8">p(r) = Sof tmax(W r ? B + b r ),<label>(11)</label></formula><p>where W r is the relation weight matrix and b r ? dr is the bias vector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Training</head><p>REDSandT utilizes a transformer model, precisely BERT, which fine-tunes on our specific setup to capture the semantic features of relational sentences. Below, we present the overall process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.1">Model Pre-training</head><p>For our experiments, we use the pre-trained bertbase-cased language model <ref type="bibr" target="#b3">(Devlin et al., 2018)</ref>, which consists of 12 layers, 12 attention heads, and 110M parameters, with each layer being a bidirectional Transformer encoder <ref type="bibr" target="#b23">(Vaswani et al., 2017)</ref>. The model is trained on cased English text of BooksCorpus and Wikipedia with a total of 800M and 2.5K words respectively. BERT is pre-trained using two unsupervised tasks: masked LM and next sentence prediction, with masked LM being its core novelty as it allows the previously impossible bidirectional training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.2">Model Fine-tuning</head><p>We initialize REDSandT model' s weights with the pre-trained BERT model, and we fine-tune its 4-last layers under the multi-instance learning setting presented in <ref type="figure" target="#fig_1">Figure 2</ref>, given the specific input shown in <ref type="figure" target="#fig_0">Figure 1</ref>. We end up fine-tuning only the last four layers after experimentation. During fine-tuning, we optimize the following objective:</p><formula xml:id="formula_9">L(D) = |B| i=1 logP (l i |B i ; ?)<label>(12)</label></formula><p>, where for all entity pair bags |B| in the dataset, we want to maximize the probability of correctly predicting the bag's relation given its sentences' representation and parameters.     <ref type="figure" target="#fig_2">Figure 3</ref> compares the precision-recall curves of REDSandT against state-of-the-art models. We observe that: (1) The NN-based approaches outperform the probabilistic method <ref type="bibr">(Mintz)</ref>, showing human-designed features limitation against neural networks' automatically extracted features.</p><p>(2) RE-SIDE, DISTRE, and REDSandT achieve better performance than PCNN+ATT, which even exhibiting the highest precision in the beginning soon follows an abrupt decline. This reveals the importance of both side-information (i.e., entity types and relation alias), and transfer knowledge.</p><p>(3) RESIDE performs the best in low recalls and generally performs well, which we attribute to the multitude of sideinformation given. steady, downward trend, acting similar to RESIDE at the low and medium recalls and surpassing all baselines in the very high recall values. We believe the reason is that we use potential label information as an additional feature and as attention over the instance tokens. The learned label embeddings are of high quality since they carry common-knowledge from the pre-trained model fine-tuned on the specific dataset and task. Moreover, the chosen pretrained model, BERT, considers bidirectionality while training, being thus able to efficiently capture head and tail interaction. <ref type="table">Table 1</ref>, which presents AUC and precision at various points in the P-R curve, reveals our model's precision performance to be between that of RE-SIDE and DISTRE while preserving the stateof-the-art AUC. Precisely, REDSandT' s precision does not exceed RESIDE', even though it is close enough, which suggests that additional side-information would improve our model. Meanwhile, REDSandT surpasses DISTRE' s precision, which we attribute to our selected pre-trained model that efficiently captures label embeddings. Consequently, our model is more consistent to the various points of the P-R curve. <ref type="table" target="#tab_4">Table 2</ref> shows the distribution over relation types for the top 300 predictions of REDSandT and baseline models. REDSandT encompasses 10 distinct relation types, two of which (place founded, /geographic distribution) are not recognized by none of rest models. PCNN+ATT predictions are highly biased towards a set of only four relation types, while RESIDE captures three additional types. DISTRE and REDSandT manage to recognize more types than all models, emphasizing the contribution of transfer knowledge. Moreover, REDSandT correctly not recognizes /location/country/capital relation that DISTRE does, as their authors found most errors to arise from the specific predicted relation in manual evaluation. Meanwhile, we highlight REDSandT' s effectiveness in recognizing rela-    tions in the long-tail. Particularly, our model captures, founders (1.47%), neighborhood of (1.06%), person/children (0.47%), and sports team/location (0.16%) relations. Relations are listed in descending order regarding population in test set with respective percentage referenced in parentheses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Ablation Studies</head><p>To assess the effectiveness of the different modules of REDSandT, we create four ablation models:  <ref type="bibr" target="#b26">(Xu et al., 2015)</ref> in sentence encoding. As shown in <ref type="table" target="#tab_5">Table 3</ref>, all modules contribute to final model' s effectiveness. Greatest impact comes from relation embeddings with their removal result-ing in the highest AUC (2 units) and P@300 (5.3%) drop. Meanwhile, P@100 goes up to 80% with inspection of top 300 predictions revealing a focus on 5 relation types only, with /location/contains to make up the 79% of these. Simple integration of entity types in input representation is the next most important feature that boosts our model. Next, "REDSandT w. SDP", shows STP's superiority, while a manual inspection in the model's top 300 predictions prove SDP's weakness to recognize relations in the long tail, with focus given on /person/nationality relation. Finally, removing the relation attention over instance tokens exhibits the least effect in AUC (0.002) and precision (?2%). Meanwhile, we notice that model focuses solely on 8 relation types in the top 300 predictions.  <ref type="figure" target="#fig_4">Figure 4</ref> shows a visualization of the relation attention weights, highlighting the different parts of the sentence that drive relation extraction, for two longtail relations. In both cases, we see that the special tokens preserve important information, while also the entity type is given more weight than the entity itself. Moreover, we see which tokens affect more the relation. Tokens "girlfriend", "son", and the repetition of name "James" are predictive of the "children" relation, while tokens "neighborhood", "was", "in", along with a GPE entity type show a probable "neighborhood of " relation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Case study: Effect of relation attention</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>Our work is related to distant supervision, neural relation extraction (mainly pre-trained LMs), subtree parse of input, label embedding, and entity type side information. Distant Supervision: DS plays a key role in RE, as it satisfies its need for extensive training data, easily and inexpensively. The use of DS <ref type="bibr" target="#b2">(Craven and Kumlien, 1999;</ref><ref type="bibr" target="#b20">Snow et al., 2005)</ref> to generate large training data for RE was proposed by <ref type="bibr" target="#b14">(Mintz et al., 2009)</ref>, who assumed that all sentences that include an entity pair, which exhibits a relationship in a KB, express the same relation. However, this assumption comes with noisy labels, especially when the KB is not directly related to the domain at hand. Multi-instance learning methods were proposed to alleviate the issue, by conducting relation classification at the bag level, with a bag including instances that mention the same entity pair <ref type="bibr" target="#b17">(Riedel et al., 2010;</ref><ref type="bibr" target="#b5">Hoffmann et al., 2011)</ref>. Neural Relation Extraction: While the performance of the above approaches heavily relies on handcrafted features (POS tags, named entity tags, morphological features, etc.), the advent of neural networks in RE set the focus on model architecture. <ref type="bibr" target="#b29">Zeng et al. (2014)</ref> propose a CNN-based method to automatically capture the semantics of sentences, while PCNN <ref type="bibr" target="#b28">(Zeng et al., 2015)</ref> became the common architecture to embed sentences. PCNN is used in several approaches that handle DS noisy patterns, such as intra-bag attention <ref type="bibr" target="#b10">(Lin et al., 2016)</ref>, inter-bag attention (Ye and Ling, 2019), soft labeling  and adversarial training <ref type="bibr" target="#b25">(Wu et al., 2018;</ref><ref type="bibr" target="#b15">Qin et al., 2018)</ref>. Moreover, Graph-CNNs proved an effective way to encode syntactic information from text <ref type="bibr" target="#b22">(Vashishth et al., 2018)</ref>.</p><p>The latest development of pre-trained LMs relying on transformer architecture <ref type="bibr" target="#b23">(Vaswani et al., 2017)</ref> has shown to capture semantic and syntactic features better <ref type="bibr" target="#b16">(Radford and Salimans, 2018)</ref>. <ref type="bibr" target="#b6">Howard and Ruder (2018)</ref> found that they significantly improve text classification performance, prevent overfitting, and increase sample efficiency. <ref type="bibr" target="#b19">Shi and Lin (2019)</ref> fine-tuned BERT <ref type="bibr" target="#b3">(Devlin et al., 2018)</ref> on the TACRED dataset showing that simple NNs built on top of BERT improve performance. Meanwhile, <ref type="bibr" target="#b0">Alt et al. (2019)</ref> extended GPT (Radford and Salimans, 2018) to the DS setting by incorporating a multi-instance training mechanism, proving that pre-trained LMs provide a stronger signal for DS than specific linguistic and side-information features <ref type="bibr" target="#b22">(Vashishth et al., 2018)</ref>. Side information: Apart from model architecture, several methods propose additional information to further reduce noise. <ref type="bibr" target="#b22">Vashishth et al. (2018)</ref> use relation phrases and incorporate Freebase entity types achieving state-of-the-art precision at higher recall values, while <ref type="bibr" target="#b8">(Ji, 2017;</ref><ref type="bibr" target="#b7">Hu et al., 2019)</ref> use entity descriptors to enhance entity and label embeddings, respectively. Sub-Parses of Input: <ref type="bibr" target="#b26">Xu et al. (2015)</ref> showed the importance of the shortest-dependency path (SDP) in reducing irrelevant to RE words. <ref type="bibr" target="#b11">Liu et al. (2018)</ref> further reduce the noise within sentences by preserving the sub-path of the sentence that connects the two entities with their least common ancestor's parent (STP). In contrast with <ref type="bibr" target="#b11">(Liu et al., 2018)</ref>, who shape the final STP sequence by re-assigning the participating tokens into their original sequence order, we preserve the tokens' order within the STP to maintain the emerged grammar information. Label Embedding: Label embeddings aim to embed labels in the same space with word vectors. The idea comes from computer vision, with  to introduce them in text classification and <ref type="bibr" target="#b7">(Hu et al., 2019)</ref> to use them as attentionmechanism over relational tokens in distantlysupervised RE. We make use of the TransE <ref type="bibr" target="#b1">(Bordes et al., 2013)</ref> model to shape label embeddings as the entities' distance in BERT's vector space, and we show that their use both as a feature and as attention over sentences significantly improves RE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We presented a novel transformer-based relation extraction model for distant supervision. REDSandT manages to acquire high-informative instance and label embeddings and is efficient at handling the noisy labeling problem of DS. REDSandT captures high-informative embeddings for RE by fine-tuning BERT on a RE-specific structured input that focuses solely on relational arguments, including the sub-tree connecting the entities along with entities' types. Then, it utilizes these vectors to encode label embeddings, which are also used as attention mechanism over instances to reduce the effect of less-informative tokens. Finally, relation extraction occurs at bag-level by concatenating label and weighted instance embeddings. Extensive experiments on the NYT-10 dataset illustrate RED-SandT's effectiveness over existing baselines in current literature. Precisely, REDSandT manages to recognize relations that other methods fail to detect, including relations in the long-tail. Future work includes an investigation of whether additional information, such as entity descriptors, influence REDSandT's performance and to what extent, while also whether the special token embeddings can act as global embeddings for RE.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Sentence Representation in REDSandT. The input embedding h 0 to BERT is created by summing over the positional and byte pair embeddings for each token in the structured input. States h t are obtained by selfattending over the states of the previous layer h t?1 . Final sentence representation is obtained by concatenating the relation embedding r ht , and the final fine-tuned BERT layer h L weighted with relation attention ? r . Head and tail tokens participating in the relation embedding formation are marked with bold and dashed lines respectively. and ends with special delimiters [CLS] and [SEP], respectively. In BERT, [CLS] typically acts as a pooling token representing the whole sequence for downstream tasks, such as RE.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Transformer architecture (left) and training framework (right). Sentence representation s i is formed as shown inFigure 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Precision-Recall curves.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Relation attention weights for children (top) and neighborhood of (bottom) long-tail relations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell cols="3">: Relation Distribution over the top 300 pre-</cell></row><row><cell cols="3">dictions for PCNN+ATT(pcnn), RESIDE(res), DIS-</cell></row><row><cell cols="3">TRE(dis) and REDSandT(red) models</cell></row><row><cell>Metrics</cell><cell>AUC</cell><cell>P@N(%)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Evaluation results AUC and P@N of variant models on NYT-10 dataset.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Trials included encoding overall sentence tokens, STP tokens only, SDP (<ref type="bibr" target="#b26">(Xu et al., 2015)</ref>) tokens only, using common h, t delimiter, using single delimiter between entities and STP, removing entity type information.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://spacy.io/models/en</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Fine-tuning Pre-Trained Transformer Language Models to Distantly Supervised Relation Extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Alt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>H?bner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonhard</forename><surname>Hennig</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/p19-1134</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1388" to="1398" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Translating embeddings for modeling multirelational data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>Garcia-Dur?n</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oksana</forename><surname>Yakhnenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2787" to="2795" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Constructing biological knowledge bases by extracting information from text sources</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Craven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johan</forename><surname>Kumlien</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings ofthe Seventh International Conference on Intelligent Systems for Molecular Biology</title>
		<meeting>the Seventh International Conference on Intelligent Systems for Molecular Biology</meeting>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="page" from="77" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><forename type="middle">Toutanova</forename><surname>Google</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A I</forename><surname>Language</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">SEE: Syntax-aware entity embedding for neural relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengqiu</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenliang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenghua</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meishan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">32nd AAAI Conference on Artificial Intelligence, AAAI 2018</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5795" to="5802" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Knowledgebased weak supervision for information extraction of overlapping relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raphael</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Congle</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL-HLT 2011 -Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="541" to="550" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Universal Language Model Fine-tuning for Text Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeremy</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL 2018 -56th Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference (Long Papers)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="328" to="339" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Improving Distantly-Supervised Relation Extraction with Joint Label Embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linmei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuan</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liqiang</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weili</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Yang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/d19-1395</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3821" to="3829" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Distant Supervision for Relation Extraction with Hierarchical Attention and Entity Descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ji</surname></persName>
		</author>
		<idno type="DOI">10.1109/IJCNN.2018.8489631</idno>
	</analytic>
	<monogr>
		<title level="m">Thirty-First AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><forename type="middle">Lei</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<title level="m">ADAM: A METHOD FOR STOCHASTIC OPTIMIZA-TION. In ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Neural Relation Extraction with Selective Attention over Instances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiqi</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huanbo</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/p16-1200</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2124" to="2133" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Neural Relation Extraction via Inner-Sentence Noise Reduction and Transfer Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinsong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanhao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijia</forename><surname>Jia</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/d18-1243</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2195" to="2204" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A Soft-label Method for Noisetolerant Distantly Supervised Relation Extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kexiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baobao</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifang</forename><surname>Sui</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/d17-1189</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1790" to="1795" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Exploring fine-grained entity type constraints for distantly supervised relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING 2014 -25th International Conference on Computational Linguistics, Proceedings of COLING 2014: Technical Papers</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2107" to="2116" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Distant supervision for relation extraction without labeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Mintz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Bills</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rion</forename><surname>Snow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP</title>
		<meeting>the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1003" to="1011" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">DSGAN: Generative Adversarial Training for Distant Supervision Relation Extraction. Association for Computational Linguistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengda</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiran</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">Yang</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="496" to="505" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Improving Language Understanding by Generative Pre-Training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<ptr target="https://s3-us-west-2.amazon-aws.com/openai-assets/research-covers/language-unsupervised/languageunderstandingpaper.pdf" />
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Modeling relations and their mentions without labeled text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-642-15939-8{_}10</idno>
	</analytic>
	<monogr>
		<title level="m">Joint European Conference on Machine Learning and Knowledge Discovery in Databases</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">6323</biblScope>
			<biblScope unit="page" from="148" to="163" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Neural machine translation of rare words with subword units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/p16-1162</idno>
	</analytic>
	<monogr>
		<title level="m">54th Annual Meeting of the Association for Computational Linguistics, ACL 2016 -Long Papers</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1715" to="1725" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Simple BERT Models for Relation Extraction and Semantic Role Labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.05255</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning syntactic patterns for automatic hypernym discovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rion</forename><surname>Snow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="1297" to="1304" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Multi-instance multi-label learning for relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julie</forename><surname>Tibshirani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramesh</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP-CoNLL 2012 -2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, Proceedings of the Conference</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="455" to="465" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">RESIDE: Improving Distantly-Supervised Neural Relation Extraction using Side Information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shikhar</forename><surname>Vashishth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rishabh</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chiranjib</forename><surname>Sai Suman Prayaga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Partha</forename><surname>Bhattacharyya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Talukdar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1257" to="1266" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Attention Is All You Need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Google</forename><surname>Brain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Joint embedding of words and labels for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoyin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenlin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhe</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dinghan</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ricardo</forename><surname>Henao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Carin</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/p18-1216</idno>
	</analytic>
	<monogr>
		<title level="m">ACL 2018 -56th Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference (Long Papers)</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2321" to="2331" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Adversarial Training for Relation Extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Bamman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stuart</forename><surname>Russell</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/d17-1187</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1778" to="1783" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Classifying relations via long short term memory networks along shortest dependency paths</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lili</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ge</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchuan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Jin</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/d15-1206</idno>
	</analytic>
	<monogr>
		<title level="m">Conference Proceedings -EMNLP 2015: Conference on Empirical Methods in Natural Language Processing</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1785" to="1794" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Distant Supervision Relation Extraction with Intra-Bag and Inter-Bag Attentions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen-Hua</forename><surname>Zhi-Xiu Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ling</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/n19-1288</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2810" to="2819" />
		</imprint>
		<respStmt>
			<orgName>Long and Short Papers</orgName>
		</respStmt>
	</monogr>
	<note>Association for Computational Linguistics (ACL)</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Distant supervision for relation extraction via Piecewise Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daojian</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yubo</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference Proceedings -EMNLP 2015: Conference on Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1753" to="1762" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Relation classification via convolutional deep neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daojian</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siwei</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangyou</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING 2014 -25th International Conference on Computational Linguistics, Proceedings of COLING 2014: Technical Papers</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2335" to="2344" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
