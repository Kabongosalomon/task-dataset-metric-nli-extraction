<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SpinalNet: Deep Neural Network with Gradual Input</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">M</forename><surname>Dipu Kabir</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moloud</forename><surname>Abdar</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seyed</forename><surname>Mohammad</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jafar</forename><surname>Jalali</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Senior Member, IEEE</roleName><forename type="first">Abbas</forename><surname>Khosravi</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><surname>Amir</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Senior Member, IEEE;</roleName><forename type="first">F</forename><surname>Atiya</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Fellow, IEEE;</roleName><forename type="first">Saeid</forename><surname>Nahavandi</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Fellow, IEEE.</roleName><forename type="first">Dipti</forename><surname>Srinivasan</surname></persName>
						</author>
						<title level="a" type="main">SpinalNet: Deep Neural Network with Gradual Input</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T08:34+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-DNN</term>
					<term>CNN</term>
					<term>AdaNet</term>
					<term>ResNet</term>
					<term>VGG</term>
					<term>Transfer Learning</term>
					<term>Transferred Initialization</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Deep neural networks (DNNs) have achieved the state of the art performance in numerous fields. However, DNNs need high computation times, and people always expect better performance in a lower computation. Therefore, we study the human somatosensory system and design a neural network (SpinalNet) to achieve higher accuracy with fewer computations. Hidden layers in traditional NNs receive inputs in the previous layer, apply activation function, and then transfer the outcomes to the next layer. In the proposed SpinalNet, each layer is split into three splits: 1) input split, 2) intermediate split, and 3) output split. Input split of each layer receives a part of the inputs. The intermediate split of each layer receives outputs of the intermediate split of the previous layer and outputs of the input split of the current layer. The number of incoming weights becomes significantly lower than traditional DNNs. The SpinalNet can also be used as the fully connected or classification layer of DNN and supports both traditional learning and transfer learning. We observe significant error reductions with lower computational costs in most of the DNNs. Traditional learning on the VGG-5 network with SpinalNet classification layers provided the stateof-the-art (SOTA) performance on QMNIST, Kuzushiji-MNIST, EMNIST (Letters, Digits, and Balanced) datasets. Traditional learning with ImageNet pre-trained initial weights and SpinalNet classification layers provided the SOTA performance on STL-10, Fruits 360, Bird225, and Caltech-101 datasets. The scripts of the proposed SpinalNet are available at the following link: https://github.com/dipuk0506/SpinalNet</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Deep Neural Networks (DNNs) have brought the state of the art performance in various scientific and engineering fields <ref type="bibr" target="#b0">[1]</ref>- <ref type="bibr" target="#b3">[4]</ref>. DNNs usually have a large number of input features, as the consideration of more parameters usually improves the accuracy of the prediction. The size of the first hidden layer is critical. A small first hidden layer fails to propagate all input features properly, and a large first hidden layer increases the number of weights drastically. Another limitation of the traditional DNNs is the vanishing gradient. When the number of layers is large, the gradient is high at parameters near the H M Dipu <ref type="bibr">Kabir</ref>  output, and the gradient becomes negligible at parameters near inputs. DNN training becomes difficult due to the vanishing gradient problem.</p><p>The human brain receives a lot of information from our skin. Numerous tactile sensory neurons send the sense of touch, heat, vibration, etc. from different parts of our body. They can sense pressure, heat, vibrations, complex textures, hardness, state of matter, etc. <ref type="bibr" target="#b4">[5]</ref>. Humans can have different touch sensitivity over time. Although the exact mechanism is unknown to humans, the current knowledge base states a tremendous function of our spinal cord neurons. The human spinal cord receives senses of touch from different locations in different parts of it. Multiple vertebrae can be connected to one internal organ too. <ref type="figure" target="#fig_0">Fig. 1(a)</ref> presents simplified rough connections between human touch-sensors and the spinal cord. Researchers have developed convolutional neural networks arXiv:2007.03347v3 [cs.CV] 7 Jan 2022 (CNN) by mimicking the functionality of the cats' visual cortex, and that brings a significant improvement in the accuracy of NNs <ref type="bibr" target="#b5">[6]</ref>. The miraculous spinal architecture of humans and the recent success of CNNs motivate us to develop a neural network with gradual inputs.</p><p>A well-known approach to reducing computation is pooling <ref type="bibr" target="#b6">[7]</ref>. However, pooling causes loss of information. Popular solutions to the vanishing gradient problem are ResNet and DenseNet. They allow shortcut connections over different layers. Therefore, the gradient remains high at neurons near the input <ref type="bibr" target="#b7">[8]</ref>. ResNets usually provide better performance with increasing depth and can be as deep as thousands of layers. However, there is a slight marginal improvement in ResNet with increased depth. Moreover, very deep ResNets have a problem of diminishing feature reuse. Therefore, Sergey et al. proposed wide residual networks <ref type="bibr" target="#b8">[9]</ref> and achieved superior performance. Zifeng et al. also received superior performance with shallow and wide NNs <ref type="bibr" target="#b9">[10]</ref>. Gao et al.</p><p>proposed DenseNet, where all layers are connected <ref type="bibr" target="#b10">[11]</ref>. DenseNet training is faster and provides better performance in most situations due to two reasons i) all layers of DenseNet are connected, ii) they made DenseNet narrower than the ResNet. When all layers are connected, the gradient and feature-reuse do not vanish over layers. However, as all layers are connected, an increment of the network size by one layer needs connections to that layer from all existing layers. Therefore, deep DenseNets are computationally expensive. Adaptive Structural Learning of Neural Networks(AdaNet) performs both connecting neurons and optimizing weights during the training. Consideration of all possible connections of a DNN is computationally intensive. The inauguration of a new neuron requires the consideration of connecting the neuron to other neurons. Therefore, AdaNet is suitable for shallow NNs <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>.</p><p>Although DNNs have achieved SOTA performances in numerous fields, DNNs still suffer from large computational overheads during training and execution <ref type="bibr" target="#b13">[14]</ref>. This paper proposes the SpinalNet, presented in <ref type="figure" target="#fig_0">Fig. 1</ref>(b) to improve performance with smaller computational overhead. The proposed structure with gradual and repetitive input capabilities enables NNs to achieve promising results with fewer parameters. We investigate the proposed SpinalNet as the fully connected layer of the VGG-5 network and receive SOTA performance in four MNIST datasets. We apply the transferred initialization with SpinalNet fully connected layers and receive SOTA performance in STL-10, Fruits 360, and Caltech-101 datasets. Subsection IID explains the term: transferred initialization. We investigate SpinalNet and its variations in more than seventeen different datasets. We also receive promising results in other datasets with at least one variant of SpinalNet.</p><p>We organize the rest of the paper as follows: Section II presents the theoretical background of SpinalNet. The section discusses the similarity between the human spinal cord and the proposed SpinalNet, proves the universal approximation of the SpinalNet, and discusses transferred initialization. Section III reports the experimental results of SpinalNet with other competitors for solving regression and classification problems, Section IV presents the prospects of SpinalNet, and Section The information of any touch or pain reaches the brain through the nerve plexus and the spinal cord. The nerve plexus is a network of intersecting nerves. Our spinal cord receives information gradually. Here, C5-C8 and T1 are vertebrae <ref type="bibr" target="#b15">[16]</ref> in the human skeleton.</p><p>V is the concluding section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. THEORETICAL BACKGROUND</head><p>Deep NNs can be of convolutional or non-convolutional type. The structure of non-convolutional NNs consists of fully connected input, hidden, and output layers. Deep convolutional NNs contain convolutional layers and fully connected layers, except the SqueezeNet <ref type="bibr" target="#b14">[15]</ref>. SqueezeNet uses global average pooling instead of a fully connected layer. However, SqueezeNet is not one of the best models in terms of accuracy at this moment. DNNs usually have a large number of parameters in the fully connected or classifier layers. Especially DNNs of VGG series has millions of parameters at the fully connected layer. DNNs of ResNet or DenseNet series lack neurons in their fully connected layer. Therefore, there is a linear relationship between features and outputs. However, there can be non-linear relationships between features and outputs. Consideration of linear relationships may result in inferior performances. Therefore we demonstrate Spinal layers instead of the traditional hidden-layers. We demonstrate Spinal layers for both shallow NNs and as the fully-connected part of several popular Deep CNNs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Human Somatosensory System and the Spinal Cord</head><p>Although the exact mechanism of the human somatosensory system is not well understood, we find several similarities between the human spinal cord and the proposed neural network <ref type="bibr" target="#b16">[17]</ref>. We aim to mimic the following characteristics of human spinal cord: 1) Gradual input and nerve plexus.</p><p>2) Voluntary and involuntary actions.</p><p>3) Attention to pain intensity. Sensory neurons reach the spinal cord through a complex network, known as the nerve plexus. <ref type="figure" target="#fig_1">Fig. 2</ref> represents a portion of the nerve plexus. A single vertebra does not receive all of the information. The tactile sensory network covers millions of sensors. Furthermore, the human tactile system is more stable compared to the vision or the auditory systems, as there are fewer touch-blind patients, compared to the number of blind patients. The nerve plexus network sends all tactile signals to the spinal cord gradually. Different locations of a spinal cord receive the pain of the leg and the pain of the hand <ref type="bibr" target="#b16">[17]</ref>. The neurons existing in the vertebral column are responsible for transferring the sense of touch to the brain and may take some actions. Our brain can control the spinal neurons to increase or decrease the pain intensity <ref type="bibr" target="#b17">[18]</ref>. Sensory neurons may also convey information to the lower motor before getting instruction from the brain. This miraculous procedure is called involuntary or reflex movements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Proposed SpinalNet</head><p>The proposed SpinalNet has the following similarities with the above-mentioned features of the human spinal cord. 1) Gradual input 2) Local output and probable global influence 3) Weights reconfigured during training Similar to our spinal cord, the proposed SpinalNet takes inputs gradually and repetitively. Each layer of the SpinalNet contributes towards the local output (reflex). The SpinalNet also sends a modulated version of inputs towards the global output (brain). The NN training process configures weights based on the training data. Spinal cord neurons are also get configured for tuning the pain sensitivity of different sensories of our body. <ref type="figure" target="#fig_0">Fig. 1</ref> demonstrates the structure of the proposed SpinalNet. The network structure consists of input sub-layers, intermediate sub-layers, and an output layer. The input is split and sent to the intermediate sub-layers of multiple hidden layers. In <ref type="figure" target="#fig_0">Fig. 1</ref>, the intermediate sub-layers contain two neurons per hidden layer. The number of intermediate neurons can be changed according to the user. However, both the number of intermediate neurons and the number of inputs per layer are regularly kept small to reduce the number of multiplication. As typically the number of inputs and the intermediate hidden neurons per layer allocates a small amount, the network may have an under-fit shape. As a consequence, each layer receives inputs from the previous layer. Since the input is repeated, if one important feature of input does not impact the output in one hidden layer, the feature may impact the output in another hidden layer. The intermediate sub-layers contain a nonlinear activation function and the output layer contains the linear activation function. In <ref type="figure" target="#fig_0">Fig. 1(b)</ref>, the input values are split into three rows. These rows are assigned to the different hidden layers in a repeated manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Universal Approximation of the Proposed SpinalNet</head><p>Proposing a new NN architecture raises a question about its universal approximability <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>. Therefore, we prove the universal approximation. The traditional mathematical proof of the universal approximation theorem contains scholarly and esoteric equations. However, we aim to make the paper equation-free to attract general audiences. Therefore, we prove the universal approximation with the following approach. 1) Single hidden layer of a NN with large width is a universal approximator <ref type="bibr" target="#b20">[21]</ref>. 2) If we can prove that SpinalNet of a large number of layers is equivalent to the single hidden layer of a large width NN, the universal approximation is proved. <ref type="figure" target="#fig_2">Fig. 3</ref> presents how a simpler version of SpinalNet can be equivalent to a single hidden layer NN. In <ref type="figure" target="#fig_2">Fig. 3</ref>(a), a SpinalNet with two hidden layers (HLs), each layer containing two neurons is simplified. The neurons of the first layer contain the purely linear function. Therefore, the first layer gives only the weighted sum of x 1 to x 5 inputs. Outputs of each hidden neuron for the first hidden layer only go to a similar neuron of the second hidden layer. Cross connections and connections from the first hidden layer to output are disconnected by assigning zero weights. The second hidden layer receives the weighted summation from x 6 to x 10 . It also receives the weighted summation from x 1 to x 5 of the previous layer. Then, the neurons of this layer apply an activation function to the weighted sum of x 1 to x 10 . Therefore, these two layers are equivalent to a neural network of single HL, containing two hidden neurons, shown in <ref type="figure" target="#fig_2">Fig. 3</ref></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(b). A simplified version of</head><p>SpinalNet with four HLs, containing two neurons in each layer is shown in <ref type="figure" target="#fig_2">Fig. 3(d)</ref>. Similarly, the SpinalNet in <ref type="figure" target="#fig_2">Fig. 3(d)</ref> is also equivalent to a NN with one HL, containing four neurons in <ref type="figure" target="#fig_2">Fig. 3</ref>(c). In the supplementary material section, we have proved that any activation function in the first hidden layer can be approximated as the 'purelin' function for a specific combination of weights and biases.</p><p>Similarly, a deep SpinalNet can be equivalent to a NN of a single hidden layer, containing a large number of neurons. A NN with a single hidden layer and a large number of neurons achieves the universal approximation. Therefore, a deep SpinalNet also has universal approximability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Transferred Initialization</head><p>Transfer of learning is an efficient technique of using previously acquired knowledge and skills in novel problems. It is also similar to educating humans with a much broader syllabus to achieve competencies for an unpredictable future. One of the most efficient ideas of machine learning is transfer learning(TL), which is similar to the transfer of learning in humans <ref type="bibr" target="#b21">[22]</ref>. DNNs require adequate training samples for proper training. Insufficient training samples may result in poor performance. Transfer learning is an efficient DNN training technique where initial layers of DNN are pre-trained with a large dataset. The corresponding train dataset trains only a few final layers. As a result, the user can get a welltrained NN for the specific dataset of a small sample number, with lower computational overhead <ref type="bibr" target="#b22">[23]</ref>. The TL is gaining huge popularity these days due to exceptional performance and usability. Many researchers expect TL as the next driver of the commercial success of machine learning <ref type="bibr" target="#b23">[24]</ref>. Several standard datasets contain insufficient training samples. CIFAR-100, Caltech-101, STL-10, etc. datasets are examples of such datasets. The traditional DNN training trains the entire DNN. According to the reported results, current traditional DNN training techniques haven't achieved more than 90% accuracy on the CIFAR-100 dataset but TL has achieved more than 93% accuracy.</p><p>We apply the TL method without freezing weights of convolutional layers. Therefore, the initial weights are transferred, and the training is traditional. When initial weights are frozen, the optimization happens at a local minimum. While training NNs multiple times without freezing weights, one optimization can potentially happen in the global minima. Moreover, pretrained initial layers can potentially fail to propagate several important features of the target dataset. That propagation error may result in inferior performance in transfer learning than the transferred initialization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. RESULTS</head><p>In this section, we aim to verify the effectiveness of Spinal-Net for regression and classification problems. We apply different variants of the SpinalNet and receive SOTA performance in several datasets. The training procedure follows stochastic gradient descent (SGD) or Adam training technique. We upload training scripts to GitHub to help future researchers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Regression Dataset</head><p>Regression is a less popular topic among the researchers of NN compared to classification. There exist a large number of datasets and organized competition among various algorithms for the classification problem. Regregression lacks popular standard datasets and competitions. Therefore, we compare our SpinalNet with the PyTorch regression example, developed by Ben Phillips <ref type="bibr" target="#b24">[25]</ref>. The example considers a single input and a single output. Following the example, we apply the Adam algorithm <ref type="bibr" target="#b25">[26]</ref> for the optimization purpose. The loss function used in the experiments is the mean square error (MSE), the learning rate is equal to 0.01, and the number of the epoch is equal to 200. We changed the problem to eight variables and tried different combinations of variables with the same level of noise. Combinations are 1) summation of variables ( x), 2) sine of summation of variables (sin( x)), and 3) product of variables ( x), and 4) sine of product of variables (sin( x)). We record the MSE at 100 and 200 epochs. The default code <ref type="bibr" target="#b24">[25]</ref> shows the MSE of the last epoch, but our code shows the minimum MSE among the current and previous epochs. We segment inputs into two groups, each containing four inputs. The number of hidden neurons in traditional NN is 300 <ref type="bibr" target="#b24">[25]</ref>. The number of hidden neurons in SpinalNet is also 300. The number of multiplication in traditional NN is 21.7k, and the number of multiplications in SpinalNet is 14k. The SpinalNet achieves a 35.5% reduction in the number of multiplications. Memory requirements and execution times are two important determinants of the quality of the neural network besides the accuracy or error value. Recent papers on the proposal of novel NN are presenting the number of parameters as an indication of memory and time requirement <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b27">[28]</ref>. The number of parameters in the traditional NN is 22k, and the number of parameters in SpinalNet is 14.3k. There are eight combinations for MSE comparisons, as shown in <ref type="table" target="#tab_1">Table I</ref>. Superior performances are highlighted as bold characters. The SpinalNet performs better in six out of eight combinations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Classification: Learning from Random Initialization</head><p>We train several existing networks and different variations of SpinalNet with random initialization on MNIST, Fashion-MNIST, KMNIST, QMNIST, EMNIST, CIFAR-10, and CIFAR-100 image classification datasets. We train each model ten times and present average and best results. 1) MNIST: The MNIST dataset is one of the most popular datasets for investigating image classification algorithms due to its simplicity and small size. We compare our SpinalNet with PyTorch CNN <ref type="bibr" target="#b28">[29]</ref>. A hidden layer of fifty neurons joins them with the output. The default CNN code provides 98.17% accuracy. We investigate the same NN with a SpinalNet fully connected (FC) layer. The FC layer consists of six sub-hidden layers, each layer containing eight neurons. CNN with Spinal FC provides 98.44% accuracy. That structure brings more than a 48.5% reduction in multiplication and a 4% reduction in the activation functions on the fully connected layer. The first segment in <ref type="table" target="#tab_1">Table II</ref> presents results on the MNIST dataset. The SpinalNet reduces the overall number of parameters and increases performance significantly.</p><p>As VGG models perform very well with the MNIST datasets, we incorporate the SpinalNet with the VGG-5 network <ref type="bibr" target="#b29">[30]</ref>. VGG-5 with the Spinal fully connected layer provides a near state-of-the-art performance. We receive 99.72% accuracy with VGG-5 (Spinal FC). According to our literature search, it is one of the top twenty reported accuracies. We perform the random rotation of 10 degrees and the random perspective PyTorch library functions to enhance data.</p><p>2) Fashion-MNIST: The Fashion-MNIST data is quite similar to MNIST. It also contains 28 ? 28 grayscale images and the output contains 10 classes. Therefore, MNIST codes are executable to the Fashion-MNIST data without any modification. The same NN is applied to compute CNN and CNN (Spinal FC).</p><p>We receive 94.68% accuracy with VGG-5 (Spinal FC). The default VGG-5 provides 94.63% accuracy. Therefore, the Spinal FC provides better performance with a lower number of multiplications. We apply random rotation and random resized crops to enhance the data. According to our literature search, it is one of the top five reported results.</p><p>3) Kuzushiji-MNIST: Kuzushiji-MNIST or shortly, KM-NIST is a Japanese character recognition dataset. The data format of KMNIST is also the same as the MNIST data, and the same codes can be applied. <ref type="table" target="#tab_1">Table II</ref> presents the performance of codes. We receive superior performance by using the Spinal FC layer with CNN.</p><p>We receive 99.15% accuracy with VGG-5 (Spinal FC). It is a new SOTA for the Kuzushiji-MNIST dataset <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b31">[32]</ref>. We apply the random perspective and the random rotation to enhance the data. The data augmentation for VGG-5 and VGG-5 (Spinal FC) are the same. The default VGG-5 provides 98.94% accuracy. Therefore, the Spinal FC provides better performance with a lower number of multiplications. 4) QMNIST: QMNIST is a recently published English digit recognition dataset. The QMNIST dataset contains fifty thousand test images. The dimensions of inputs and outputs are the same as the dimension of inputs and outputs of the MNIST dataset. Therefore, the same code is executable for the QMNIST dataset. The fourth segment of table II presents QMNIST results. The default PyTorch CNN provides 97.82% accuracy on the QMNIST data. CNN(Spinal FC) provides 97.97% and 98.07% accuracy for the spinal-layer size of eight and ten respectively.</p><p>The VGG-5 receives 99.66% accuracy. The VGG-5 (Spinal FC) receives 99.68% accuracy. According to our literature search, we have received SOTA performance for the QMNIST dataset. We apply the random perspective and random rotation functions to achieve these results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5) EMNIST:</head><p>The EMNIST dataset contains several handwritten character datasets. These datasets are derived from the NIST database and converted to a 28?28 pixel greyscale image format. The EMNIST(digits) dataset also has ten classes. The accuracy of NNs on the EMNIST (digit) data are available in the fifth segment of table II. The default PyTorch CNN provides 98.89% accuracy. The VGG-5 provides 99.81% accuracy and VGG-5 (Spinal FC) provides 99.82% accuracy. According to our literature search, we have received SOTA performance for the EMNIST (digits) dataset.</p><p>The default PyTorch CNN provides 87.57% accuracy for EMNIST (letters) dataset. The VGG-5 provides 95.86% accuracy and VGG-5 (Spinal FC) provides 95.88% accuracy. According to our literature search, we have received SOTA performance for the EMNIST (letters) dataset.</p><p>The default PyTorch CNN provides 79.61% accuracy for EMNIST (balanced) dataset. The VGG-5 provides 91.04% accuracy. VGG-5 (Spinal FC) provides 91.05% accuracy. According to our literature search, we have received SOTA performance for the EMNIST (balanced) dataset. We apply the random perspective and the random rotation functions to enhance all EMNIST datasets. 6) CIFAR-10 Dataset: The CIFAR-10 dataset is one of the most popular datasets in computer vision <ref type="bibr" target="#b32">[33]</ref>. The CIFAR-10 dataset contains 32 ? 32 sized color images. There are ten output classes. The dataset contains fifty thousand images for training and ten thousand images for testing.</p><p>We train and report results with ResNet-18 <ref type="bibr" target="#b7">[8]</ref> and VGG-19 <ref type="bibr" target="#b33">[34]</ref> models. Although results do not improve with the ResNet-18, we observe superior results with the VGG-19 model. Higher accuracy is achieved with a lower number of parameters with the VGG-19 model. 8 th segment of <ref type="table" target="#tab_1">Table II</ref> presents results of traditional training on CIFAR-10 dataset.</p><p>7) CIFAR-100 Dataset: The CIFAR-100 is another popular dataset in computer vision <ref type="bibr" target="#b32">[33]</ref>. The CIFAR-100 dataset contains 32 ? 32 sized color images. There are hundred output classes. The dataset contains five hundred images for training and one hundred images for testing for each class.</p><p>We train and report results with ResNet-18 <ref type="bibr" target="#b7">[8]</ref> and VGG-19 <ref type="bibr" target="#b33">[34]</ref> models. We observe superior results with the VGG-19 model. Higher accuracy is achieved with a lower number of parameters with the VGG-19 model. The number of neurons in the fully connected layer is reduced to half with the Spinal FC layer. Moreover, the number of multiplication in the FC layer is reduced to 7%. The VGG-19 has two hidden layers of size 4096. Replacing them with four spinal layers of 512 size reduces the number of multiplications significantly. The last segment of <ref type="table" target="#tab_1">Table II</ref> presents results of traditional training with random initialization on CIFAR-100 dataset.</p><p>However, the Spinal fully connected layer does not improve the performance of ResNet. Although the number of hidden neurons in the fully connected layer is increasing for ResNet, we are getting a lower performance. The probable reason can be a decrease in the gradient in the initial layers of ResNet due to additional layers in the SpinalNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Classification: Learning from Transferred Initialization</head><p>While learning from scratch, we observe that, although the number of hidden neurons in the fully connected layer is increasing, the ResNet with Spinal FC is getting a lower performance compared to the similar ResNet. The probable reason can be a decrease in the gradient in the initial layers of ResNet due to additional layers. Therefore, we apply the transferred initialization technique to the pre-trained ResNet and VGG networks. We download pre-trained VGG-19 bn and Wide-Resnet-101 models from Torchvision. These models are pretrained on the Imagenet dataset. We apply these two models to the following datasets. According to our simulations, an FC layer of 2-layer narrow NN provides lower accuracy than the default FC of ResNet and SpinalNet 1 . Therefore, we compare SpinalNet with the default FC of ResNet. 1) CIFAR-10: We train pre-trained VGG-19 bn and Wide-Resnet-101 models to the CIFAR-10 data. We investigate both the traditional FC layers and Spinal FC layers. The SpinalNet achieves a significant improvement in the VGG network in terms of accuracy and the number of parameters. However, there is a slightly lower performance in the Wide-ResNet with the Spinal FC, compared to the original Wide-ResNet.</p><p>2) CIFAR-100: As usual, the SpinalNet performs well with the VGG network on the CIFAR-100 dataset. We receive a superior accuracy of 88.34% with a wide Spinal FC of 512 neurons with dropout on the Wide-ResNet-101. It is one of The SpinalNet achieves a significant improvement in the VGG network in terms of accuracy and the number of parameters. There is a slightly lower performance in the wide ResNet. Our results with the Wide ResNet networks are among the top 5 accuracies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>10) Fruits 360:</head><p>Fruits 360 dataset is one of the easiest machine learning problems. Several programmers have released codes with more than 99.9% efficiency in Kaggle for that dataset. We achieve promising performance with the SpinalNet on the Fruit 360 dataset with both the VGG and the Wide-ResNet networks. The Wide-ResNet-101 with Spinal FC provides 100% accuracy. However, another paper has reported the same efficiency this year.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>11) Other Investigated Datasets:</head><p>We also investigate the Wide-ResNet101 with SpinalNet fully connected layers in 'Intel Image Classification' and '10 Monkey Species' datasets in Kaggle and receive accuracies of 93.77% and 99.26% respectively. www.paperswithcode.com/task/image-classification * * After the online appearance of the current paper, the performance of SpinalNet will be SOTA performance; unless someone reports a better performance earlier. <ref type="table" target="#tab_1">Table IV</ref> presents SOTA performances for our investigated datasets. Combining SpinalNet with VGG-5 provides near SOTA or SOTA performance in MNIST datasets. MNIST SOTA models are usually combined models and have the best performance for a specific dataset. We also obtain SOTA performance in five MNIST datasets. The prime reason for obtaining SOTA is not the proposed network. As these datasets are new, very few researchers investigated these data, and our result is the best among the reported results. We also apply transferred initialization with a spinal fully connected layer and achieve SOTA performance in several color image datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. SOTA Performances</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. PROSPECTS OF SPINALNET</head><p>There are many rooms for further investigation and improvement of SpinalNet. The rest of this section presents several prospects of SpinalNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Auto Dimension Reduction</head><p>Dimension reduction is a popular technique of reducing the number of inputs to a neural network without facing noticeable performance degradation <ref type="bibr" target="#b59">[60]</ref>. The input combination of the NN network may contain a large number of inter-related or irrelevant data. As the proposed SpinalNet takes input in every layer and there are fewer neurons per hidden layer than the total number of inputs, the SpinalNet may automatically discard irrelevant data. Moreover, the necessity for dimension reduction may decrease, as a large number of input features do not increase the computation greatly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Very Deep NN</head><p>The computation performed inside the proposed SpinalNet increases linearly with the increase in depth. The SpinalNet has outputs at every layer. Moreover, gradual training may enable us to increase SpinalNet depth gradually. <ref type="figure">Fig. 4</ref>. Any traditional hidden layer can be converted to a spinal hidden layer. The traditional hidden layer in (a) is converted to a spinal hidden layer in (b). A spinal hidden layer has the structure of the proposed SpinalNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Spinal Hidden Layer</head><p>This paper presents the SpinalNet as an independent network and as the fully connected layer of a CNN. The Spinal-Net can also replace a wide hidden layer of a traditional NN. <ref type="figure">Fig. 4</ref> presents how a SpinalNet can replace a traditional hidden layer. The figure shows two inputs and one neuron per sub-layer. The number of inputs can be large and the input can be segmented into more than two segments. Similarly, one sub-layer may hold more than one neuron. The number of sub-layer can be more than or less than four.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Better Accuracy and New Datasets</head><p>The SpinalNet may achieve higher accuracy with augmented datasets and its different structural variants. We apply Spinal-Net as fully connected layers on several other networks to achieve higher accuracies. Researchers may apply SpinalNet for different datasets, new applications [61]- <ref type="bibr" target="#b62">[63]</ref>, and combining with other networks in the future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. NN Ensemble and Voting</head><p>Recently Mo Kweon et al. perform ensemble and voting from two different VGG networks and ResNet to achieve better performance <ref type="bibr" target="#b29">[30]</ref>. Researchers may use different NNs within SpinalNet to get better performance.</p><p>After the online appearance of the paper, a researcher applied SpinalNet to predict DNA N6-Methyladenine Sites in Genomes <ref type="bibr" target="#b64">[64]</ref>. One researcher is also thinking of applying it for zero-shot learning <ref type="bibr" target="#b65">[65]</ref>. Another recent ICLR submission applied SpinalNet along with other networks to explore supervised representation learning <ref type="bibr" target="#b66">[66]</ref>. Future research work may also deal with more scholarly analysis, such as sensitivity, adversarial learning, dropout uncertainty, etc <ref type="bibr" target="#b67">[67]</ref>- <ref type="bibr" target="#b69">[69]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>This paper presented a novel DNN model named Spinal-Net. SpinalNet is built by mimicking the chordate nervous system, which has a unique way of connecting a large number of sensing information and taking local decisions. One major drawback of feed-forward NN models is their computational intensiveness for large inputs. Therefore, taking inputs gradually and considering local decisions similar to our spinal cord decreases computations. We also present the effectiveness of SpinalNet on several well-known benchmark datasets leading to the improvement of the classification accuracy and regression error. Moreover, the SpinalNet is usually less computation-intensive than its counterpart. Combining with VGG-5, SpinalNet has achieved SOTA performance in several handwritten character recognition datasets. With the transferred initialization, the SpinalNet has achieved SOTA performance in several color image datasets. Researchers may try to improve the accuracy of the proposed SpinalNet and apply the improved SpinalNet to a wide range of real-world scenarios.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>We develop SpinalNet by mimicking several characteristics of the human somatosensory system to receive large input efficiently and to achieve better performance. (a) How our spinal cord is connected to our body for receiving and sending sensory signals from our body. (b) Structure of the proposed SpinalNet. Each layer of the proposed NN is split into input split, intermediate split, and output split. Each intermediate split receives a portion of the input. All intermediate splits except the intermediate split of the first layer also receive outputs of the previous intermediate split. The output spit adds the weighted outputs of all intermediate splits. The user can also construct and train a SpinalNet for any arbitrary number of inputs, intermediate neurons, and outputs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>A portion of the human nerve plexus; known as the brachial plexus.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>The visual proof of the universal approximation of the SpinalNet. A simplified version of SpinalNet in (a) can act as a NN of a single hidden layer, drawn in (b). Similarly, a 4 layer SpinalNet in (d) can be equivalent to a NN of one hidden layer (HL), containing four neurons, shown in (c).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>, Moloud Abdar, Seyed Mohammad Jafar Jalali, Abbas Khosravi, and Saeid Nahavandi are with Institute for Intelligent Systems Research and Innovation (IISRI), Deakin University, Australia. (Email: {hussain.kabir, mabdar, sjalali, abbas.khosravi, saeid.nahavandi}@deakin.edu.au) Amir F Atiya in with Cairo University. Email: amir@alumni.caltech.edu Dipti Srinivasan is with National University of Singapore. Email: dipti@nus.edu.sg Manuscript received -, 2020; accepted---.</figDesc><table><row><cell></cell><cell>Brain</cell></row><row><cell></cell><cell>Y[1:N]</cell></row><row><cell></cell><cell>Brachial Plexus</cell></row><row><cell>Cord</cell><cell>Intercostal Nerve</cell></row><row><cell>Spinal</cell><cell></cell><cell>X[2k:end]</cell></row><row><cell></cell><cell>Lumbar Plexus</cell></row><row><cell></cell><cell>Sacral Plexus</cell><cell>X[1:k]</cell></row><row><cell></cell><cell>+</cell></row><row><cell></cell><cell>Sciatic Nerve</cell><cell>X[2k:end]</cell></row><row><cell></cell><cell>Common Personal</cell><cell>X[k:2k]</cell></row><row><cell></cell><cell>Nerve</cell></row><row><cell></cell><cell>Superficial Personal</cell><cell>X[1:k]</cell></row><row><cell></cell><cell>Nerve</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I COMPARISON</head><label>I</label><figDesc>BETWEEN TRADITIONAL FEED-FORWARD NN AND SPINALNET FOR REGRESSION DATASETS.</figDesc><table><row><cell>Neural Network</cell><cell>Data</cell><cell></cell><cell cols="2">MSE (10 ?3 Unit)</cell></row><row><cell></cell><cell></cell><cell></cell><cell>100 Epoch</cell><cell>200 Epoch</cell></row><row><cell>Feed-forward NN</cell><cell>8 Var.</cell><cell>x</cell><cell>1.178</cell><cell>0.887</cell></row><row><cell>Two Hidden Layers</cell><cell cols="2">8 Var. sin( x)</cell><cell>1.918</cell><cell>1.086</cell></row><row><cell>200, 100 Neurons</cell><cell>8 Var.</cell><cell>x</cell><cell>3.875</cell><cell>3.875</cell></row><row><cell>[25]</cell><cell cols="2">8 Var. sin( x)</cell><cell>3.403</cell><cell>1.554</cell></row><row><cell>SpinalNet</cell><cell>8 Var.</cell><cell>x</cell><cell>1.007</cell><cell>0.855</cell></row><row><cell>6 Hidden Layers</cell><cell cols="2">8 Var. sin( x)</cell><cell>1.912</cell><cell>1.219</cell></row><row><cell>50 Neurons Each Layer</cell><cell>8 Var.</cell><cell>x</cell><cell>3.966</cell><cell>2.217</cell></row><row><cell>Half Input Each Layer</cell><cell cols="2">8 Var. sin( x)</cell><cell>0.910</cell><cell>0.910</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE II PERFORMANCE</head><label>II</label><figDesc>OF THE SPINALNET AND SEVERAL POPULAR NETS ON DIFFERENT CLASSIFICATION DATASETS</figDesc><table><row><cell>Data</cell><cell>Model</cell><cell cols="2">Size of Fully Connected Layer Epoch</cell><cell cols="2">Test Accuracy</cell><cell cols="2">Error Reduction Parameters</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Average</cell><cell>Best</cell><cell>(Best)</cell><cell></cell></row><row><cell></cell><cell>CNN [29]</cell><cell>1HL, 50 Neurons</cell><cell>8</cell><cell>97.98%</cell><cell>98.17%</cell><cell>-</cell><cell>21.84k</cell></row><row><cell>MNIST</cell><cell>CNN (Spinal FC)</cell><cell>6HL, 8 Neurons Per Layer</cell><cell>8</cell><cell>98.32%</cell><cell>98.44%</cell><cell>14.8%</cell><cell>13.82k</cell></row><row><cell>[35]</cell><cell>CNN(Spinal FC)</cell><cell>6HL, 10 Neurons Per Layer</cell><cell>8</cell><cell>98.37%</cell><cell>98.48%</cell><cell>16.9%</cell><cell>16.05k</cell></row><row><cell></cell><cell>VGG-5 [30]</cell><cell>1HL, 512 Neurons</cell><cell>100</cell><cell>99.69%</cell><cell>99.72%</cell><cell>-</cell><cell>3.646M</cell></row><row><cell></cell><cell>VGG-5 (Spinal FC)</cell><cell>4HL, 128 Neurons Per Layer</cell><cell>100</cell><cell>99.69%</cell><cell>99.72%</cell><cell>0.0%</cell><cell>3.630M</cell></row><row><cell></cell><cell>CNN [29]</cell><cell>1HL, 50 Neurons</cell><cell>8</cell><cell>83.59%</cell><cell>84.10%</cell><cell>-</cell><cell>21.84k</cell></row><row><cell>Fashion-MNIST</cell><cell>CNN (Spinal FC)</cell><cell>6HL, 8 Neurons Per Layer</cell><cell>8</cell><cell>85.31%</cell><cell>85.98%</cell><cell>11.8%</cell><cell>13.82k</cell></row><row><cell>[36]</cell><cell>CNN(Spinal FC)</cell><cell>6HL, 10 Neurons Per Layer</cell><cell>8</cell><cell>85.71%</cell><cell>86.61%</cell><cell>15.8%</cell><cell>16.05k</cell></row><row><cell></cell><cell>VGG-5 [30]</cell><cell>1HL, 512 Neurons</cell><cell>100</cell><cell>94.02%</cell><cell>94.63%</cell><cell>-</cell><cell>3.646M</cell></row><row><cell></cell><cell>VGG-5 (Spinal FC)</cell><cell>4HL, 128 Neurons Per Layer</cell><cell>100</cell><cell>94.19%</cell><cell>94.68%</cell><cell>0.9%</cell><cell>3.630M</cell></row><row><cell></cell><cell>CNN [29]</cell><cell>1HL, 50 Neurons</cell><cell>8</cell><cell>83.24%</cell><cell>84.48%</cell><cell>-</cell><cell>21.84k</cell></row><row><cell>Kuzushiji-MNIST</cell><cell>CNN (Spinal FC)</cell><cell>6HL, 8 Neurons Per Layer</cell><cell>8</cell><cell>85.98%</cell><cell>87.94%</cell><cell>22.3%</cell><cell>13.82k</cell></row><row><cell>(KMNIST) [37]</cell><cell>CNN (Spinal FC)</cell><cell>6HL, 10 Neurons Per Layer</cell><cell>8</cell><cell>86.48%</cell><cell>88.25%</cell><cell>24.3%</cell><cell>16.05k</cell></row><row><cell></cell><cell>VGG-5 [30]</cell><cell>1HL, 512 Neurons</cell><cell>200</cell><cell>98.76%</cell><cell>98.94%</cell><cell>-</cell><cell>3.646M</cell></row><row><cell></cell><cell>VGG-5 (Spinal FC)</cell><cell>4HL, 128 Neurons Per Layer</cell><cell>200</cell><cell>99.05%</cell><cell>99.15%</cell><cell>19.8%</cell><cell>3.630M</cell></row><row><cell></cell><cell>CNN [29]</cell><cell>1HL, 50 Neurons</cell><cell>8</cell><cell>97.66%</cell><cell>97.82%</cell><cell>-</cell><cell>21.84k</cell></row><row><cell>QMNIST</cell><cell>CNN (Spinal FC)</cell><cell>6HL, 8 Neurons Per Layer</cell><cell>8</cell><cell>97.81%</cell><cell>97.97%</cell><cell>6.9%</cell><cell>13.82k</cell></row><row><cell>[38]</cell><cell>CNN(Spinal FC)</cell><cell>6HL, 10 Neurons Per Layer</cell><cell>8</cell><cell>98.00%</cell><cell>98.07%</cell><cell>11.5%</cell><cell>16.05k</cell></row><row><cell></cell><cell>VGG-5 [30]</cell><cell>1HL, 512 Neurons</cell><cell>100</cell><cell>99.63%</cell><cell>99.66%</cell><cell>-</cell><cell>3.646M</cell></row><row><cell></cell><cell>VGG-5 (Spinal FC)</cell><cell>4HL, 128 Neurons Per Layer</cell><cell>100</cell><cell>99.64%</cell><cell>99.68%</cell><cell>5.9%</cell><cell>3.630M</cell></row><row><cell></cell><cell>CNN [29]</cell><cell>1HL, 50 Neurons</cell><cell>8</cell><cell>98.80%</cell><cell>98.89%</cell><cell>-</cell><cell>21.84k</cell></row><row><cell>EMNIST</cell><cell>CNN (Spinal FC)</cell><cell>6HL, 8 Neurons Per Layer</cell><cell>8</cell><cell>99.03%</cell><cell>99.12%</cell><cell>20.7%</cell><cell>13.82k</cell></row><row><cell>(Digits) [39]</cell><cell>CNN(Spinal FC)</cell><cell>6HL, 10 Neurons Per Layer</cell><cell>8</cell><cell>99.07%</cell><cell>99.16%</cell><cell>24.3%</cell><cell>16.05k</cell></row><row><cell></cell><cell>VGG-5 [30]</cell><cell>1HL, 512 Neurons</cell><cell>50</cell><cell>99.75%</cell><cell>99.81%</cell><cell>-</cell><cell>3.646M</cell></row><row><cell></cell><cell>VGG-5 (Spinal FC)</cell><cell>4HL, 128 Neurons Per Layer</cell><cell>50</cell><cell>99.75%</cell><cell>99.82%</cell><cell>5.3%</cell><cell>3.630M</cell></row><row><cell></cell><cell>CNN [29]</cell><cell>1HL, 50 Neurons</cell><cell>8</cell><cell>87.29%</cell><cell>87.57%</cell><cell>-</cell><cell>21.84k</cell></row><row><cell>EMNIST</cell><cell>CNN (Spinal FC)</cell><cell>6HL, 8 Neurons Per Layer</cell><cell>8</cell><cell>89.88%</cell><cell>90.07%</cell><cell>20.11%</cell><cell>13.82k</cell></row><row><cell>(Letters) [39]</cell><cell>CNN(Spinal FC)</cell><cell>6HL, 10 Neurons Per Layer</cell><cell>8</cell><cell>90.02%</cell><cell>90.23%</cell><cell>21.4%</cell><cell>16.05k</cell></row><row><cell></cell><cell>VGG-5 [30]</cell><cell>1HL, 512 Neurons</cell><cell>200</cell><cell>95.71%</cell><cell>95.86%</cell><cell>-</cell><cell>3.646M</cell></row><row><cell></cell><cell>VGG-5 (Spinal FC)</cell><cell>4HL, 128 Neurons Per Layer</cell><cell>200</cell><cell>95.79%</cell><cell>95.88%</cell><cell>0.5%</cell><cell>3.630M</cell></row><row><cell></cell><cell>CNN [29]</cell><cell>1HL, 50 Neurons</cell><cell>8</cell><cell>78.99%</cell><cell>79.61%</cell><cell>-</cell><cell>21.84k</cell></row><row><cell>EMNIST</cell><cell>CNN (Spinal FC)</cell><cell>6HL, 8 Neurons Per Layer</cell><cell>8</cell><cell>82.13%</cell><cell>82.77%</cell><cell>15.50%</cell><cell>13.82k</cell></row><row><cell>(Balanced) [39]</cell><cell>CNN(Spinal FC)</cell><cell>6HL, 10 Neurons Per Layer</cell><cell>8</cell><cell>82.57%</cell><cell>83.21%</cell><cell>17.66%</cell><cell>16.05k</cell></row><row><cell></cell><cell>VGG-5 [30]</cell><cell>1HL, 512 Neurons</cell><cell>200</cell><cell>90.71%</cell><cell>91.04%</cell><cell>-</cell><cell>3.646M</cell></row><row><cell></cell><cell>VGG-5 (Spinal FC)</cell><cell>4HL, 128 Neurons Per Layer</cell><cell>200</cell><cell>90.73%</cell><cell>91.05%</cell><cell>0.1%</cell><cell>3.630M</cell></row><row><cell></cell><cell>ResNet-18 [8]</cell><cell>Only Output Layer</cell><cell>150</cell><cell>90.65%</cell><cell>91.98%</cell><cell>-</cell><cell>3.14M</cell></row><row><cell>CIFAR-10</cell><cell>ResNet-18 (Spinal FC)</cell><cell>4HL, 20 Neurons Per Layer</cell><cell>150</cell><cell>90.39%</cell><cell>91.42%</cell><cell>-7.0%</cell><cell>3.22M</cell></row><row><cell>[40]</cell><cell>VGG-19 [34]</cell><cell>2HL, 4096 Neurons Each</cell><cell>200</cell><cell>90.48%</cell><cell>90.75%</cell><cell>-</cell><cell>38.96M</cell></row><row><cell></cell><cell>VGG-19 (Spinal FC)</cell><cell>4HL, 512 Neurons Each</cell><cell>200</cell><cell>90.97%</cell><cell>91.40%</cell><cell>7.0%</cell><cell>20.16M</cell></row><row><cell></cell><cell>ResNet-18 [8]</cell><cell>Only Output Layer</cell><cell>30</cell><cell>64.41%</cell><cell>65.04%</cell><cell>-</cell><cell>3.16M</cell></row><row><cell>CIFAR-100</cell><cell>ResNet-18 (Spinal FC)</cell><cell>4HL, 128 Neurons Per Layer</cell><cell>30</cell><cell>62.93%</cell><cell>63.60%</cell><cell>-4.1%</cell><cell>4.66M</cell></row><row><cell>[40]</cell><cell>VGG-19 [34]</cell><cell>2HL, 4096 Neurons Each</cell><cell>150</cell><cell>61.69%</cell><cell>62.05%</cell><cell>-</cell><cell>39.33M</cell></row><row><cell></cell><cell>VGG-19 (Spinal FC)</cell><cell>4HL, 512 Neurons Each</cell><cell>150</cell><cell>64.12%</cell><cell>64.77%</cell><cell>7.2%</cell><cell>20.20M</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE III TRANSFERRED</head><label>III</label><figDesc>INITIALIZATION (TI) PERFORMANCE OF THE SPINALNET AND SEVERAL POPULAR NETS ON DIFFERENT CLASSIFICATION DATASETS Although we have received a slightly better result with the Spinal FC for both networks, our performance is not one of the top twenty performances. The probable reason for getting such poor performance is the training with a different kind of data. Most classes in the Imagenet dataset are not digits. Pre-training with different types of data may limit the performance. 7) CINIC-10: We receive significant improvements in terms of accuracy for both VGG and Wide-ResNet neural networks. Our result with Wide-ResNet Spinal FC is one of the top five reported results. Detailed procedures of data augmentation and simulations are available in the GitHub code.8) STL-10: The SpinalNet performs well with both the VGG and the Wide-ResNet network on the STL-10 dataset. We receive SOTA performance for the STL-10 dataset.</figDesc><table><row><cell>Data</cell><cell>Model</cell><cell>Size of Fully Connected Layer</cell><cell>Epoch</cell><cell cols="2">Test Accuracy</cell><cell>Error Reduction</cell><cell>Parameters</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Average</cell><cell>Best</cell><cell>(Best)</cell><cell></cell></row><row><cell></cell><cell>VGG-19 bn</cell><cell>2 HL, 4096 Neurons Per Layer</cell><cell>25</cell><cell>94.88%</cell><cell>95.91%</cell><cell>-</cell><cell>263.27M</cell></row><row><cell>CIFAR-10</cell><cell>VGG-19 bn (Spinal FC)</cell><cell>4HL, 1024 Neurons Per Layer</cell><cell>25</cell><cell>95.18%</cell><cell>96.00%</cell><cell>2.2%</cell><cell>198.26M</cell></row><row><cell>[40]</cell><cell>Wide ResNet-101 2</cell><cell>0 Neurons</cell><cell>50</cell><cell>97.43%</cell><cell>98.22%</cell><cell>-</cell><cell>124.86M</cell></row><row><cell cols="2">Wide ResNet-101 2 (Spinal FC)</cell><cell>4HL, 20 Neurons Per Layer</cell><cell>50</cell><cell>97.61%</cell><cell>98.12%</cell><cell>-5.6%</cell><cell>124.92M</cell></row><row><cell></cell><cell>VGG-19 bn</cell><cell>2 HL, 4096 Neurons Per Layer</cell><cell>25</cell><cell>77.82%</cell><cell>79.22%</cell><cell>-</cell><cell>263.63M</cell></row><row><cell>CIFAR-100</cell><cell>VGG-19 bn (Spinal FC)</cell><cell>4HL, 1024 Neurons Per Layer</cell><cell>25</cell><cell>77.93%</cell><cell>79.56%</cell><cell>1.6%</cell><cell>198.63M</cell></row><row><cell>[40]</cell><cell>Wide ResNet-101 2</cell><cell>0 Neurons</cell><cell>50</cell><cell>86.79%</cell><cell>87.15%</cell><cell>-</cell><cell>125.04M</cell></row><row><cell cols="2">Wide ResNet-101 2 (Spinal FC)</cell><cell>4HL, 512 Neurons Per Layer</cell><cell>50</cell><cell>88.00%</cell><cell>88.34%</cell><cell>9.26%</cell><cell>132.59M</cell></row><row><cell></cell><cell>VGG-19 bn</cell><cell>2 HL, 4096 Neurons Per Layer</cell><cell>10</cell><cell>91.31%</cell><cell>92.98%</cell><cell>-</cell><cell>263.64M</cell></row><row><cell>Caltech-101</cell><cell>VGG-19 bn (Spinal FC)</cell><cell>4HL, 1024 Neurons Per Layer</cell><cell>10</cell><cell>91.92%</cell><cell>93.16%</cell><cell>2.6%</cell><cell>198.63M</cell></row><row><cell>[41]</cell><cell>Wide ResNet-101 2</cell><cell>0 Neurons</cell><cell>10</cell><cell>96.68%</cell><cell>97.11%</cell><cell>-</cell><cell>125.05M</cell></row><row><cell cols="2">Wide ResNet-101 2 (Spinal FC)</cell><cell>4HL, 101 Neurons Per Layer</cell><cell>10</cell><cell>97.04%</cell><cell>97.32%</cell><cell>7.27%</cell><cell>132.60M</cell></row><row><cell></cell><cell>VGG-19 bn</cell><cell>2 HL, 4096 Neurons Per Layer</cell><cell>25</cell><cell>98.04%</cell><cell>98.67%</cell><cell>-</cell><cell>264.15M</cell></row><row><cell>Bird225</cell><cell>VGG-19 bn (Spinal FC)</cell><cell>4HL, 1024 Neurons Per Layer</cell><cell>25</cell><cell>98.62%</cell><cell>99.02%</cell><cell>26.3%</cell><cell>199.14M</cell></row><row><cell>[42]</cell><cell>Wide ResNet-101 2</cell><cell>0 Neurons</cell><cell>25</cell><cell>98.95%</cell><cell>99.38%</cell><cell>-</cell><cell>125.30M</cell></row><row><cell cols="2">Wide ResNet-101 2 (Spinal FC)</cell><cell>4HL, 225 Neurons Per Layer</cell><cell>25</cell><cell>99.11%</cell><cell>99.56%</cell><cell>11.1%</cell><cell>126.11M</cell></row><row><cell></cell><cell>VGG-19 bn</cell><cell>2 HL, 4096 Neurons Per Layer</cell><cell>25</cell><cell>86.42%</cell><cell>87.21%</cell><cell>-</cell><cell>264.03M</cell></row><row><cell>Stanford Cars</cell><cell>VGG-19 bn (Spinal FC)</cell><cell>4HL, 1024 Neurons Per Layer</cell><cell>25</cell><cell>88.06%</cell><cell>88.72%</cell><cell>13.4%</cell><cell>153.78M</cell></row><row><cell>[43]</cell><cell>Wide ResNet-101 2</cell><cell>0 Neurons</cell><cell>25</cell><cell>92.89%</cell><cell>93.35%</cell><cell>-</cell><cell>125.24M</cell></row><row><cell cols="2">Wide ResNet-101 2 (Spinal FC)</cell><cell>4HL, 196 Neurons Per Layer</cell><cell>25</cell><cell>92.86%</cell><cell>93.35%</cell><cell>0%</cell><cell>132.98M</cell></row><row><cell></cell><cell>VGG-19 bn</cell><cell>2 HL, 4096 Neurons Per Layer</cell><cell>25</cell><cell>93.96%</cell><cell>94.80%</cell><cell>-</cell><cell>263.27M</cell></row><row><cell>SVHN</cell><cell>VGG-19 bn (Spinal FC)</cell><cell>4HL, 1024 Neurons Per Layer</cell><cell>25</cell><cell>94.61%</cell><cell>95.26%</cell><cell>18.28%</cell><cell>198.26M</cell></row><row><cell>[44]</cell><cell>Wide ResNet-101 2</cell><cell>0 Neurons</cell><cell>25</cell><cell>97.01%</cell><cell>97.80%</cell><cell>-</cell><cell>124.86M</cell></row><row><cell cols="2">Wide ResNet-101 2 (Spinal FC)</cell><cell>4HL, 20 Neurons Per Layer</cell><cell>25</cell><cell>97.00%</cell><cell>97.87%</cell><cell>3.18%</cell><cell>124.92M</cell></row><row><cell></cell><cell>VGG-19 bn</cell><cell>2 HL, 4096 Neurons Per Layer</cell><cell>25</cell><cell>89.88%</cell><cell>90.28%</cell><cell>-</cell><cell>263.27M</cell></row><row><cell>CINIC-10</cell><cell>VGG-19 bn (Spinal FC)</cell><cell>4HL, 1024 Neurons Per Layer</cell><cell>25</cell><cell>90.69%</cell><cell>91.00%</cell><cell>8.00%</cell><cell>198.26M</cell></row><row><cell>[45]</cell><cell>Wide ResNet-101 2</cell><cell>0 Neurons</cell><cell>50</cell><cell>91.72%</cell><cell>92.15%</cell><cell>-</cell><cell>124.86M</cell></row><row><cell cols="2">Wide ResNet-101 2 (Spinal FC)</cell><cell>4HL, 20 Neurons Per Layer</cell><cell>50</cell><cell>93.02%</cell><cell>93.60%</cell><cell>18.47%</cell><cell>124.92M</cell></row><row><cell></cell><cell>VGG-19 bn</cell><cell>2 HL, 4096 Neurons Per Layer</cell><cell>10</cell><cell>94.97%</cell><cell>95.44%</cell><cell>-</cell><cell>263.27M</cell></row><row><cell>STL-10</cell><cell>VGG-19 bn (Spinal FC)</cell><cell>4HL, 1024 Neurons Per Layer</cell><cell>10</cell><cell>95.03%</cell><cell>95.57%</cell><cell>2.9%</cell><cell>198.26M</cell></row><row><cell>[46]</cell><cell>Wide ResNet-101 2</cell><cell>0 Neurons</cell><cell>10</cell><cell>97.83%</cell><cell>98.40%</cell><cell>-</cell><cell>124.86M</cell></row><row><cell cols="2">Wide ResNet-101 2 (Spinal FC)</cell><cell>4HL, 20 Neurons Per Layer</cell><cell>10</cell><cell>98.23%</cell><cell>98.66%</cell><cell>16.3%</cell><cell>124.92M</cell></row><row><cell></cell><cell>VGG-19 bn</cell><cell>2 HL, 4096 Neurons Per Layer</cell><cell>25</cell><cell>94.65%</cell><cell>95.20%</cell><cell>-</cell><cell>263.64M</cell></row><row><cell>Oxford 102</cell><cell>VGG-19 bn (Spinal FC)</cell><cell>4HL, 1024 Neurons Per Layer</cell><cell>25</cell><cell>94.98%</cell><cell>95.46%</cell><cell>32.5%</cell><cell>198.63M</cell></row><row><cell>Flower [47]</cell><cell>Wide ResNet-101 2</cell><cell>0 Neurons</cell><cell>50</cell><cell>98.91%</cell><cell>99.39%</cell><cell>-</cell><cell>125.05M</cell></row><row><cell cols="2">Wide ResNet-101 2 (Spinal FC)</cell><cell>4HL, 101 Neurons Per Layer</cell><cell>50</cell><cell>99.07%</cell><cell>99.30%</cell><cell>-14.7%</cell><cell>125.32M</cell></row><row><cell></cell><cell>VGG-19 bn</cell><cell>2 HL, 4096 Neurons Per Layer</cell><cell>10</cell><cell>99.90%</cell><cell>99.92%</cell><cell>-</cell><cell>263.76M</cell></row><row><cell>Fruits 360</cell><cell>VGG-19 bn (Spinal FC)</cell><cell>4HL, 1024 Neurons Per Layer</cell><cell>10</cell><cell>99.95%</cell><cell>99.96%</cell><cell>60%</cell><cell>198.75M</cell></row><row><cell>[48]</cell><cell>Wide ResNet-101 2</cell><cell>0 Neurons</cell><cell>10</cell><cell>99.94%</cell><cell>99.96%</cell><cell>-</cell><cell>125.11M</cell></row><row><cell cols="2">Wide ResNet-101 2 (Spinal FC)</cell><cell>4HL, 131 Neurons Per Layer</cell><cell>10</cell><cell>99.98%</cell><cell>100%</cell><cell>100%</cell><cell>125.50M</cell></row><row><cell>get high accuracy.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>6) SVHN:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>9) Oxford 102 Flower: We trained pre-trained VGG-19 bn and Wide-Resnet-101 to the Oxford 102 Flower data. We investigate both the traditional FC layers and Spinal FC layers.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE IV SOTA</head><label>IV</label><figDesc>PERFORMANCES OF INVESTIGATED DATASETS IN JUNE 2020*</figDesc><table><row><cell>Data</cell><cell>Model Name</cell><cell>Accuracy</cell></row><row><cell>MNIST</cell><cell>Branching/Merging CNN [1]</cell><cell>99.84%</cell></row><row><cell>Fashion-MNIST</cell><cell>PreAct-ResNet18 + FMix [49]</cell><cell>96.36%</cell></row><row><cell>Kuzushiji-MNIST**</cell><cell>CAMNet3 [32]</cell><cell>99.05%</cell></row><row><cell>QMNIST**</cell><cell>Deep Regularization [50]</cell><cell>99.67%</cell></row><row><cell>EMNIST (Digits)**</cell><cell>DWT-DCT with KNN [51]</cell><cell>97.74%</cell></row><row><cell>EMNIST (Letters)**</cell><cell>TextCaps [52]</cell><cell>95.39%</cell></row><row><cell cols="2">EMNIST (Balanced)** TextCaps [52]</cell><cell>90.46%</cell></row><row><cell>CIFAR-10</cell><cell>BiT-L (ResNet) [53]</cell><cell>99.00%</cell></row><row><cell>CIFAR-100</cell><cell>BiT-L (ResNet) [53]</cell><cell>93.51%</cell></row><row><cell>Caltech-101**</cell><cell>UL-Hopfield [54]</cell><cell>91.00%</cell></row><row><cell>Stanford Cars</cell><cell>DAT [55]</cell><cell>96.20%</cell></row><row><cell>Oxford 102 Flowers</cell><cell>BiT-L (ResNet) [53]</cell><cell>99.63%</cell></row><row><cell>STL-10**</cell><cell>NAT-M4 [56]</cell><cell>97.90%</cell></row><row><cell>CINIC-10</cell><cell>NAT-M4 [56]</cell><cell>94.80%</cell></row><row><cell>SVHN</cell><cell>WideResNet-28-10 [57]</cell><cell>99.00%</cell></row><row><cell>225 Bird Species**</cell><cell>Vgg-16 [58]</cell><cell>99.10%</cell></row><row><cell>Fruits 360**</cell><cell>EfficientNet-B1 [59]</cell><cell>100%</cell></row><row><cell cols="3">*  According to the following website and our literature search:</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/dipuk0506/SpinalNet/tree/master/Jupyter%20Notebooks /SpinalNet vs 2L FC the top ten reported results on the CIFAR-100 dataset. Results are presented in the second segment of table III. 3) Caltech-101: The SpinalNet performs well with both the VGG and the Wide-ResNet network on the Caltech-101 dataset compared to traditional fully connected layers. We receive SOTA performance for the Caltech-101 dataset. 4) Bird225: The SpinalNet performs well with both the VGG and the Wide-ResNet network on the Bird225 dataset. We receive SOTA performance for the Bird225 dataset. Results are available as the fourth segment of table III. However, the provider of the dataset has added more images to the dataset with new classes. 5) Stanford Cars: Although SpinalNet performs well with the VGG model, the accuracy with the Wide-ResNet model is equal. Moreover, our results are not one of the top 20 results in the Stanford Car leaderboard. The probable reason for the poor performance is the high resolution of the image. Several other researchers are concentrating on certain parts of the image to</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">A branching and merging convolutional network with homogeneous filter capsules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Byerly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kalganova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Dear</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.09136</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Multi-column deep neural networks for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ciregan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Meier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="3642" to="3649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Representation learning: A review and new perspectives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="1798" to="1828" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Rmdl: Random multimodel deep learning for classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kowsari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Heidarysafa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">E</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">J</forename><surname>Meimandi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">E</forename><surname>Barnes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd International Conference on Information System and Data Mining</title>
		<meeting>the 2nd International Conference on Information System and Data Mining</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="19" to="28" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Psychophysical dimensions of tactile perception of textures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Okamoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Nagano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yamada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Haptics</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="81" to="93" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Receptive fields, binocular interaction and functional architecture in the cat&apos;s visual cortex</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">H</forename><surname>Hubel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Wiesel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of physiology</title>
		<imprint>
			<biblScope unit="volume">160</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="106" to="154" />
			<date type="published" when="1962" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Face recognition: A convolutional neural-network approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Giles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Tsoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">D</forename><surname>Back</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on neural networks</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="98" to="113" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Wide residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.07146</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Wider or deeper: Revisiting the resnet model for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="page" from="119" to="133" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE confer</title>
		<meeting>the IEEE confer</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4700" to="4708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Adanet: Adaptive structural learning of artificial neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gonzalvo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kuznetsov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mohri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="874" to="883" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Partial adversarial training for neural network-based uncertainty quantification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">D</forename><surname>Kabir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nahavandi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Emerging Topics in Computational Intelligence</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Optimal autonomous driving through deep imitation learning and neuroevolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M J</forename><surname>Jalali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">M</forename><surname>Kebria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saleh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Nahavandi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nahavandi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE International Conference on Systems, Man and Cybernetics (SMC)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1215" to="1220" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Squeezenet: Alexnet-level accuracy with 50x fewer parameters and? 0.5 mb model size</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">N</forename><surname>Iandola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">W</forename><surname>Moskewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ashraf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">J</forename><surname>Dally</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Keutzer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.07360</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Vascularized brachial plexus allotransplantation-an experimental study in brown norway and lewis rats</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gorden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">W</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hernon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shafarenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D. C.-C</forename><surname>Chuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transplantation</title>
		<imprint>
			<biblScope unit="volume">103</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="149" to="159" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Spinal cord mechanisms of pain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dickenson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">British journal of anaesthesia</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="8" to="16" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Attention modulates spinal cord responses to pain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sprenger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Eippert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Finsterbusch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Bingel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>B?chel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Current Biology</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1019" to="1022" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Resnet with one-neuron hidden layers is a universal approximator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6169" to="6178" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Universal approximation with quadratic deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">124</biblScope>
			<biblScope unit="page" from="383" to="392" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Approximation with artificial neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">C</forename><surname>Cs?ji</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page">7</biblScope>
			<pubPlace>Hungary</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Faculty of Sciences, Etvs Lornd University</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Transfertransfo: A transfer learning approach for neural network based conversational agents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Delangue</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.08149</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Starcraft micromanagement with reinforcement learning and curriculum transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Emerging Topics in Computational Intelligence</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="73" to="84" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Nuts and bolts of building ai applications using deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Keynote Talk</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Regression with neural networks in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Phillips</surname></persName>
		</author>
		<ptr target="https://medium.com/@benjamin.phillips22/simple-regression-with-neural-networks-in-pytorch-313f06910379" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Fixing the train-test resolution discrepancy: Fixefficientnet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>J?gou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.08237</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Spinenet: Learning scale-permuted backbone for recognition and localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ghiasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="11" to="592" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Mnist handwritten digit recognition in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Koehler</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Mnist competition tensorflow kr group</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kweon</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Training neural networks with local error signals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>N?kland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">H</forename><surname>Eidnes</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.06656</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Context-aware multipath networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tissera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kahatapitiya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wijesinghe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Rodrigo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11519</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Gpipe: Efficient training of giant neural networks using pipeline parallelism</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bapna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ngiam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="103" to="112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">The mnist database of handwritten digit images for machine learning research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="141" to="142" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note>best of the web</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Rasul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vollgraf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.07747</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Deep learning for classical japanese literature</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Clanuwat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bober-Irizar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kitamoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lamb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yamamoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ha</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.01718</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Cold case: The lost mnist digits</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yadav</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="443" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Emnist: Extending mnist to handwritten letters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Afshar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tapson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Schaik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 International Joint Conference on Neural Networks (IJCNN)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2921" to="2926" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>Citeseer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Caltech 101 dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">225 bird species</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerry</forename></persName>
		</author>
		<ptr target="https://www.kaggle.com/gpiosenka/100-bird-species" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Vehicle-detected stanford cars data classes folder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename></persName>
		</author>
		<ptr target="https://www.kaggle.com/sungtheillest/vehicledetected-stanford-cars-data-classes-folder" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">The street view house numbers (svhn) dataset</title>
		<ptr target="http://ufldl.stanford.edu/housenumbers/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">N</forename><surname>Darlow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">J</forename><surname>Crowley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Antoniou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Storkey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.03505</idno>
		<title level="m">Cinic-10 is not imagenet or cifar-10</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">An analysis of single-layer networks in unsupervised feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the fourteenth international conference on artificial intelligence and statistics</title>
		<meeting>the fourteenth international conference on artificial intelligence and statistics</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="215" to="223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Automated flower classification over a large number of classes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-E</forename><surname>Nilsback</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2008 Sixth Indian Conference on Computer Vision, Graphics &amp; Image Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="722" to="729" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Fruit recognition from images using deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Mure?an</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Oltean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Acta Universitatis Sapientiae, Informatica</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="26" to="42" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Understanding and enhancing mixed sample data augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Harris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Marcu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Painter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Niranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pr?gel-Bennett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hare</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.12047</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Deep regularization and direct training of the inner layers of neural networks with kernel flows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">R</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Owhadi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.08335</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Handwritten digit and letter recognition using hybrid dwt-dct with knn and svm classifier</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ghadekar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ingole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sonone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 Fourth International Conference on Computing Communication Control and Automation (ICCUBEA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Textcaps: Handwritten character recognition with very small datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Jayasundara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jayasekara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jayasekara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rajasegaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Seneviratne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Rodrigo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE winter conference on applications of computer vision (WACV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="254" to="262" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Large scale learning of general visual representations for transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Puigcerver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Houlsby</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.11370</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Unsupervised learning using pretrained cnn and associative memory bank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mukhopadhyay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 International Joint Conference on Neural Networks (IJCNN)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Domain adaptive transfer learning with specialist models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ngiam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.07056</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sreekumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Banzhaf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Deb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">N</forename><surname>Boddeti</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.05859</idno>
		<title level="m">Neural architecture transfer</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Randaugment: Practical automated data augmentation with a reduced search space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.13719</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">bird cnn vgg16 final</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Purohit</surname></persName>
		</author>
		<ptr target="https://www.kaggle.com/aditya276/bird-cnn-vgg16-99-accuracy-on-test-set" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Automated fruit recognition using efficientnet and mixnet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">T</forename><surname>Duong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">T</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Di</forename><surname>Sipio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Di Ruscio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers and Electronics in Agriculture</title>
		<imprint>
			<biblScope unit="volume">171</biblScope>
			<biblScope unit="page">105326</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Active subspace-based dimension reduction for chemical kinetics applications with epistemic uncertainty</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vohra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alexanderian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Guy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mahadevan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Combustion and Flame</title>
		<imprint>
			<biblScope unit="volume">204</biblScope>
			<biblScope unit="page" from="152" to="161" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Neural network-based uncertainty quantification: A survey of methodologies and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">D</forename><surname>Kabir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Hosen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nahavandi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">A unified approach for conventional zero-shot, generalized zero-shot, and few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rahman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Porikli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="5652" to="5667" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Towards end-to-end lane detection: an instance segmentation approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Neven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>De Brabandere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Georgoulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Proesmans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE intelligent vehicles symposium (IV)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<imprint>
			<biblScope unit="page" from="286" to="291" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Spinenet-6ma: A novel deep learning tool for predicting dna n6-methyladenine sites in genomes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Abbas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tayara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">A review of generalized zero-shot learning methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pourpanah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abdar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">P</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-Z</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.08641</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Toward understanding supervised representation learning with rkhs and gan</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Anonymous</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
	<note>submitted</note>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">Neuralsens: Sensitivity analysis of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pizarroso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Portela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mu?oz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.11423</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Alzheimer&apos;s patient analysis using image and gene expression data and explainable-ai to present associated genes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Kamal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Northcote</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chowdhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">G</forename><surname>Crespo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Herrera-Viedma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Instrumentation and Measurement</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="1" to="7" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title level="m" type="main">Evaluating state-of-the-art classification models against bayes optimality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Theisen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">R</forename><surname>Varshney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.03357</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
