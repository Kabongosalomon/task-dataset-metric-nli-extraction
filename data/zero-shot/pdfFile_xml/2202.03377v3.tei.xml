<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Benchmarking and Analyzing Point Cloud Classification under Corruptions</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Ren</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Pan</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
						</author>
						<title level="a" type="main">Benchmarking and Analyzing Point Cloud Classification under Corruptions</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T08:31+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>3D perception, especially point cloud classification, has achieved substantial progress. However, in real-world deployment, point cloud corruptions are inevitable due to the scene complexity, sensor inaccuracy, and processing imprecision. In this work, we aim to rigorously benchmark and analyze point cloud classification under corruptions. To conduct a systematic investigation, we first provide a taxonomy of common 3D corruptions and identify the atomic corruptions. Then, we perform a comprehensive evaluation on a wide range of representative point cloud models to understand their robustness and generalizability. Our benchmark results show that although point cloud classification performance improves over time, the state-of-the-art methods are on the verge of being less robust. Based on the obtained observations, we propose several effective techniques to enhance point cloud classifier robustness. We hope our comprehensive benchmark, in-depth analysis, and proposed techniques could spark future research in robust 3D perception. Code is available at https://github. com/jiawei-ren/modelnetc.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Robustness to common corruptions is crucial to point cloud classification. Compared to 2D images, point cloud data suffer more severe corruptions in real-world deployment due to the inaccuracy in 3D sensors and complexity in real-world 3D scenes <ref type="bibr" target="#b33">(Wu et al., 2019;</ref><ref type="bibr" target="#b39">Yan et al., 2020)</ref>. Furthermore, point cloud is widely employed in safety-critical applications such as autonomous driving. Therefore, robustness to out-of-distribution (OOD) point cloud data caused by corruptions becomes an important part of the test suite since the beginning of learning-based point cloud classification <ref type="bibr">(Qi 1</ref>   OA gradually saturates but mCE is at the risk of increasing due to the lack of a standard test suite. Lower. Point cloud classifer's robustness to various corruptions in a radar chart. Proposed ModelNet-C allows fine-grained corruption analysis. Different architectures have diverse strengths and weaknesses to corruptions. "-G": -Global. "-L": -Local. <ref type="bibr" target="#b23">et al., 2017b;</ref><ref type="bibr" target="#b27">Simonovsky &amp; Komodakis, 2017)</ref>.</p><p>Ideally, robustness should be measured in a standard way like how classification accuracy and computational cost are measured. However, prior research evaluates point cloud classifier robustness in many different protocols: arXiv:2202.03377v3 [cs.CV] 14 Jun 2022 <ref type="table">Table 1</ref>. Corruptions studied in existing robustness analysis. Prior works evaluate point cloud classification robustness on different sets of corruptions, and hence their evaluations can be partial and unfair. To standardize the corruption evaluation, our test suite ModelNet-C includes all previously studied corruptions, including "Jitter", "Drop Global/Local", "Add Global/Local", "Scale" and "Rotate". Jitter Drop Global Drop Local Add Global Add Local Scale Rotate</p><p>Protocol-1. Evaluate the robustness to a selected set of corruptions <ref type="bibr" target="#b23">(Qi et al., 2017b;</ref><ref type="bibr">a;</ref><ref type="bibr" target="#b32">Wang et al., 2019;</ref><ref type="bibr" target="#b4">Chen et al., 2020;</ref><ref type="bibr" target="#b14">Kim et al., 2021)</ref>, e.g., random point dropping and random jittering. This evaluation method is popular in point cloud research, as summarized in <ref type="table">Table 1</ref>. However, the freedom to select corruptions brings both positive and negative effects to the evaluation. On the upside, customized selection allows the evaluation to focus on the most characteristic corruptions. On the downside, a selected set of corruptions cannot provide a comprehensive evaluation of a model's robustness. In addition, different corruption selections and training protocols in implementation also make it difficult to compare across methods.</p><p>Protocol-2. Evaluate the robustness to the sim-to-real gap <ref type="bibr" target="#b25">(Reizenstein et al., 2021;</ref><ref type="bibr" target="#b0">Ahmadyan et al., 2021)</ref>, e.g., train on ModelNet40 <ref type="bibr" target="#b34">(Wu et al., 2015)</ref> and test on ScanOb-jectNN <ref type="bibr" target="#b30">(Uy et al., 2019)</ref>. To exploit the naturally occurred corruptions in real-world point cloud object datasets, robustness is formulated as the generalizability from a synthetic training set to a real test set. However, real-world corruptions always come in a composite way, e.g., self-occlusion and scanner noise, making it hard to analyze each corruption independently. Besides, the sim-to-real performance gap couples with the domain gap within each category, e.g., a chair in ModelNet40 and ScanObjectNN may have different styles, which obfuscates the evaluation results.</p><p>Protocol-3. Evaluate the robustness to adversarial attack <ref type="bibr" target="#b33">(Zhou et al., 2019;</ref><ref type="bibr" target="#b7">Dong et al., 2020;</ref><ref type="bibr" target="#b28">Sun et al., 2021)</ref>, e.g., adversarial point shifting and dropping. Different from real-world scenarios where corruptions are drawn from natural distributions, adversarial attacks corrupt point clouds for the purpose to deceive a classifier while keeping the attacked point cloud similar to the input. Therefore, adversarial robustness is a good measure of a model's worst-case performance but can not reflect a point cloud classifier's robustness to common corruptions in the natural world.</p><p>Despite various ways to evaluate a point cloud classifier's robustness, there lacks a standard, comprehensive benchmark for point cloud classification under corruptions. In this work, we present a full corruption test suite to close this gap. First, we break down real-world corruptions in Protocol-2 into 7 fundamental atomic corruptions ( <ref type="figure">Figure 2)</ref>, which also forms a superset of the ad-hoc corruption selections in Protocol-1. As we aim to measure real-world robustness, adversarial attacks in Protocol-3 are excluded. Then, we apply the atomic corruptions to the validation set of Mod-elNet40 as our corruption test suite dubbed ModelNet-C. Inspired by the 2D image classification robustness benchmark <ref type="bibr" target="#b11">(Hendrycks &amp; Dietterich, 2019)</ref>, we further create 5 severity levels for each atomic corruption and use the mean Corruption Error (mCE) metric for evaluation. Finally, based on the test suite, we benchmark 14 point cloud classification methods, including 9 architectures, 3 augmentations, and 2 pretrains. As shown in <ref type="figure" target="#fig_0">Figure 1</ref>, our benchmark results show that although point cloud classification performance on the clean ModelNet40 improves by time, state-of-the-art (SoTA) methods are on the verge of being less robust.</p><p>To remedy the issue, we conduct an in-depth analysis of the benchmark results and summarize two effective techniques to enhance point cloud classifier robustness. Strictly following the best design choice summarized from the benchmark results, we present Robust Point cloud Classifier (RPC), a robust network architecture for point cloud classification, which achieves the least mCE on ModelNet-C benchmark, and comparable overall accuracy on the clean ModelNet40 with the SoTAs. In particular, we present WOLFMix, a strong augmentation baseline that exploits both deformation-  <ref type="figure">Figure 2</ref>. Corruption taxonomy. We break down common corruptions into detailed corruption sources on object-, senor-and processing levels, which are further simplified into a combination of seven atomic corruptions for a more controllable empirical analysis. based augmentation and mix-based augmentation to provide a stronger regularization. Empirically, WOLFMix achieves the best robustness results compared to existing augmentation techniques. According to our experiments, the performance gain by augmentations does not equally transfer to all model architectures. We identify the best combination from existing methods, and call for model design that fully exploits the augmentation power.</p><p>Our contributions are summarized as: 1) We present the first systematically-designed test-suite ModelNet-C for point cloud classifier under corruptions. 2) We comprehensively benchmark existing methods on their robustness to corruptions. 3) We summarize several effective techniques, such as RPC and WOLFMix, to enhance point cloud classifier's robustness and identify that the synergy between architecture and augmentation should be considered in future research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head><p>Point Cloud Classification. Point cloud classification serves as an fundamental task for 3D understanding from raw hardware inputs. Point cloud classifier has diverse architectural designs. There are MLP-based models <ref type="bibr" target="#b23">(Qi et al., 2017b;</ref><ref type="bibr">a)</ref>, convolution-based models <ref type="bibr" target="#b37">Xu et al., 2021a)</ref>, graph-based models <ref type="bibr" target="#b27">(Simonovsky &amp; Komodakis, 2017;</ref><ref type="bibr" target="#b32">Wang et al., 2019)</ref> and recently proposed transformer-based models <ref type="bibr" target="#b10">(Guo et al., 2020;</ref><ref type="bibr" target="#b45">Zhao et al., 2021;</ref><ref type="bibr">Mazur &amp; Lempitsky, 2021)</ref>. Besides, there is a rising discussion on point cloud augmentation, including mix-based augmentations <ref type="bibr" target="#b14">Lee et al., 2021)</ref>, deformation-based augmentations <ref type="bibr" target="#b14">(Kim et al., 2021)</ref> and auto-augmentations <ref type="bibr" target="#b39">(Li et al., 2020)</ref>. Moreover, selfsupervised pre-train has drawn much research attention recently. Pre-trains obtained from pre-text tasks like occlusion reconstruction <ref type="bibr">(Wang et al., 2021)</ref> and mask inpainting  provide better classification performance than random initialization.</p><p>Robustness in Point Cloud. Several attempts are made to improve point cloud classifier's robustness. Triangle-Net <ref type="bibr" target="#b36">(Xiao &amp; Wachs, 2021)</ref> designs feature extraction that is invariant to positional, rotational, and scaling disturbances. Although Triangle-Net achieves exceptional robustness un-der extreme corruptions, its performance on clean data is not on par with SoTA. PointASNL <ref type="bibr" target="#b39">(Yan et al., 2020)</ref> introduces adaptive sampling and local-nonlocal modules to improve robustness. However, PointASNL takes a fixed number of points in implementation. Other works improve a model's adversarial robustness by denoising and upsampling <ref type="bibr" target="#b33">(Zhou et al., 2019)</ref>, voting on subsampled point clouds , exploiting local feature's relative position <ref type="bibr" target="#b7">(Dong et al., 2020)</ref> and self-supervision <ref type="bibr" target="#b28">(Sun et al., 2021)</ref>. Robust-PointSet <ref type="bibr" target="#b29">(Taghanaki et al., 2020)</ref> evaluates the robustness of point cloud classifiers under different corruptions, and shows that basic data augmentations poorly generalize to "unseen" corruptions. However, our work shows that more advanced augmentation techniques, e.g., mixing and local deformation, can substantially improve the robustness.</p><p>Robustness Benchmarks in Image Classification. Comprehensive robustness benchmark has been built for 2D image classification recently. ImageNet-C <ref type="bibr" target="#b11">(Hendrycks &amp; Dietterich, 2019)</ref> corrupts the ImageNet <ref type="bibr" target="#b6">(Deng et al., 2009</ref>)'s test set with simulated corruptions like compression loss and motion blur. ObjectNet <ref type="bibr" target="#b1">(Barbu et al., 2019)</ref> collects a test set with rich variations in rotation, background and viewpoint. ImageNetV2 <ref type="bibr" target="#b24">(Recht et al., 2019)</ref> re-collects a test set following ImageNet's protocal and evaluates the performance gap due to the natural distribution shift. Moreover, ImageNet-A <ref type="bibr" target="#b13">(Hendrycks et al., 2021b)</ref> and ImageNet-R <ref type="bibr" target="#b12">(Hendrycks et al., 2021a</ref>) benchmark classifier's robustness to natural adversarial examples and abstract visual renditions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Corruptions Taxonomy and Test Suite</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Corruptions Taxonomy</head><p>Real-world corruptions come from a wide range of sources, based on which we provide a taxonomy of the corruptions in <ref type="figure">Figure 2</ref>. Common corruptions are categorized into three levels: object-level, sensor-level, and processing-level corruptions. Object-level corruptions come inherently in complex 3D scenes, where an object can be occluded by other objects or parts of itself. Different viewpoints also introduce variations to the point cloud data in terms of rotation. Note that viewpoint variation also leads to a change in self-occlusion. Sensor-level corruptions happen when  perceiving with 3D sensors like LiDAR. As discussed in prior works <ref type="bibr" target="#b33">(Wu et al., 2019;</ref><ref type="bibr" target="#b2">Berger et al., 2014)</ref>, sensorlevel corruptions can be summarized as 1) dropout noise, where points are missing due to sensor limitations; 2) spatial inaccuracy, where point positions, object scale, and angle can be wrongly measured; 3) outliers, which are caused by the structural artifacts in the acquisition process. More corruptions could be introduced during postprocessing. For example, inaccurate point cloud registration leads to misalignment. Background remain and imperfect bounding box are two common corruptions during 3D object scanning.</p><p>However, it is challenging to directly simulate real-world corruptions for the following reasons. 1) Real-world corruptions have a rich variation, e.g., different hardware may have different sensor-level corruptions.</p><p>2) The combination of inter-object occlusion or background remains can be inexhaustive. 3) Moreover, a few corruptions lead to the same kind of operations to point clouds, e.g., self-occlusion, interobject occlusion, and cropping error all lead to the missing of a local part of the object. To this end, we simplify the corruption taxonomy into seven fundamental atomic corruptions: "Add Global", "Add Local", "Drop Global", "Drop Local", "Rotate", "Scale" and "Jitter". Consequently, each real-world corruption is broken down into a combination of the atomic corruptions, e.g., background remain can be viewed as a combination of "Add Local" and "Add Global".</p><p>Although the atomic corruptions cannot seamlessly simulate real-world corruptions, they provide a practical solution to achieve controllable empirical study on fundamentally analyzing point cloud classification robustness. Note that noisy translation and random permutation are not considered in this work, because point cloud normalization and permutation-invariance are two basic properties of recent point cloud classification approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">ModelNet-C: A Robustness Test Suite</head><p>ModelNet40 is one of the most commonly used benchmarks in point cloud classification, and it collects 12,311 CAD models in 40 categories (9,843 for training and 2,468 for testing). Most recent point cloud classification methods follow the settings of PointNet <ref type="bibr" target="#b23">(Qi et al., 2017b)</ref>, which samples 1024 points from each aligned CAD model and then normalizes them into a unit sphere. Based on ModelNet40 and the settings by <ref type="bibr" target="#b23">(Qi et al., 2017b)</ref>, we further corrupt the ModelNet40 test set with the aforementioned seven atomic corruptions to establish a comprehensive test-suite ModelNet-C. To achieve fair comparisons and meanwhile following the OOD evaluation principle, we use the same training set with ModelNet40. Similar corruption operations are strictly not allowed during training.</p><p>The seven atomic corruptions are implemented as follows: "Scale" applies a random anisotropic scaling to the point cloud; "Rotate" rotates the point cloud by a small angle; "Jitter" adds a Gaussian noise to point coordinates; "Drop Global" randomly drops points from the point cloud; "Drop Local" randomly drops several k-NN clusters from the point cloud; "Add Global" adds random points sampled inside a unit sphere; "Add Local" expand random points on the point cloud into normally distributed clusters. The example corrupted point clouds from ModelNet-C are shown in <ref type="figure" target="#fig_1">Figure 3</ref>. In addition, we set different five severity levels for each corruption, based on which we randomly sample from the atomic operations to form a composite corruption test set. The detailed description and implementation can be found in the appendix. Note that we restrict the rotation to small angle variations, as in real-world applications we mostly observe objects from common viewpoints with small variations. Robustness to arbitrary SO(3) rotations is a specific challenging research topic <ref type="bibr" target="#b44">(Zhang et al., 2019;</ref><ref type="bibr" target="#b3">Chen et al., 2019)</ref>, which is out of the scope of this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Evaluation Metrics</head><p>To normalize the severity of different corruptions, we choose DGCNN, a classic point cloud classification method, as the baseline. Inspired by the 2D robustness evaluation metrics <ref type="bibr" target="#b11">(Hendrycks &amp; Dietterich, 2019)</ref>, we use mean CE (mCE), as the primary metric. To compute mCE, we first compute CE:</p><formula xml:id="formula_0">CE i = 5 l=1 (1 ? OA i,l ) 5 l=1 (1 ? OA DGCNN i,l ) ,<label>(1)</label></formula><p>where OA i,l is the overall accuracy on a corrupted test set i at corruption level l, OA DGCNN i,l is baseline's overall accuracy mCE is the average of CE over all seven corruptions: where N = 7 is the number of corruptions. We also compute Relative mCE (RmCE), which measures performance drop compared to a clean test set as:</p><formula xml:id="formula_1">mCE = 1 N N i=1 CE i ,<label>(2)</label></formula><formula xml:id="formula_2">RCE i = 5 l=1 (OA Clean ? OA i,l ) 5 l=1 (OA DGCNN Clean ? OA DGCNN i,l ) ,<label>(3)</label></formula><formula xml:id="formula_3">RmCE = 1 N N i=1 RCE i ,<label>(4)</label></formula><p>where OA Clean is the overall accuracy on the clean test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Evaluation Protocol</head><p>Because most SoTA methods adopt the DGCNN protocol <ref type="bibr" target="#b9">(Goyal et al., 2021)</ref>, we also use it as the consistent protocol for the benchmark. Two conventional augmentations are used during training: 1) random anisotropic scaling in the range [2/3, 3/2]; 2) random translation in the range [-0.2, +0.2]. Note that the random scaling ranges for training and testing are not overlapped. Point cloud sampling is fixed during training, and no voting is used in the inference stage. For each method, we select the model that performs the best on the clean ModelNet40 test set during evaluation. We highlight that the same corruptions are not allowed during training to reflect model OOD generalizability. Following works are recommended to specify augmentations in training when reporting results on ModelNet-C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Systematic Benchmarking</head><p>Implementation Details. We benchmark 14 methods in total, covering three key components for robust point cloud classification as shown in <ref type="figure" target="#fig_2">Figure 4</ref>.  <ref type="bibr" target="#b9">Goyal et al. (2021)</ref>. For CurveNet, GDANet, and PAConv, we use their official pretrained models. The rest of the models are trained using their official codes.  <ref type="figure">Figure 5</ref>. Key components in the architecture design. Point cloud data (PCD) repeatedly goes through local operations, advanced grouping, and featurization before being classified. Alternatively, PCD may be projected into multi-view images and processed by traditional CNN-based backbones. This figure means to show how the key components are usually connected, but not to faithfully show every detailed architecture design.</p><p>Main Results. Benchmark results (mCE) are reported in <ref type="table" target="#tab_5">Table 3</ref>, <ref type="table" target="#tab_6">Table 4 and Table 5</ref> for architechtures, pretrains and augmentations, respectively. RmCE and Overall Accuracy are reported in the appendix. In <ref type="figure" target="#fig_0">Figure 1</ref>, we sort benchmarked architectures in chronological order and visualize a second-order polynomial fitting results with 50% confidence interval. We observe that although new architecture's performance are constantly progressing and saturates around 0.94, their mCE performance shows a large variance. We also observe that self-supervised pretraining is able to transfer the pretrain signal to the downstream model, but has a mixed effect on the overall performance. Moreover, recent point cloud augmentations can substantially improvement robustness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Comprehensive Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Architecture Design</head><p>We analyze four key components of point cloud classifier architectures: local operations, advanced grouping, featurizer, and representation dimension, as illustrated in <ref type="figure">Figure 5</ref>. The design choices of recent classifier architectures are summarized in <ref type="table" target="#tab_4">Table 2</ref>. When analyzing a specific component, we group all methods that utilize the component. Since design choices are not rigorously controlled variables in the analysis, we visualize the 95% confidence interval together with the mean value in the bar charts, and only low variance results are considered in our conclusion. Further-  more, to empirically verify our conclusion, we build a new architecture, RPC, strictly following the conclusions.</p><p>Local Operations. We compare the robustness of different local aggregations, including no local operations, k-NN, and ball-query. As shown in <ref type="figure">Figure 6a</ref>, the exploitation of the point cloud locality is a key component to robustness.</p><p>Without local aggregations, PointNet (shown as "No Local Ops.") has the highest mCE. Considering each corruption individually, PointNet is on the two extremes: it shows the best robustness to "Jitter" and "Drop-G", meanwhile being one of the worst methods for the rest corruptions. Local operations target to encode informative representations by exploiting local geometric features. Ball-query randomly samples neighboring points in a predefined radius, while k-NN focuses on nearest neighboring points. Generally, k-NN performs better than ball-query in the benchmark, especially for "Drop-L". The reason is that points surrounding the dropped local part will lose its neighbors in ball-query due to its fixed searching radius, but k-NN will choose neighbors from the remaining points. However, ball-query shows the advantage over k-NN in "Add-G", since, for a point on the object, outliers are less likely to fall in the query ball than to be its nearest neighbors.</p><p>Advanced Grouping. Recent methods design advanced grouping techniques, such as Frequency Grouping <ref type="bibr" target="#b38">(Xu et al., 2021b)</ref> and Curve Grouping <ref type="bibr" target="#b35">(Xiang et al., 2021)</ref>, to introduce structural prior into architecture design. Frequency grouping uses a graph high-pass filter <ref type="bibr" target="#b26">(Sandryhaila &amp; Moura, 2014;</ref><ref type="bibr" target="#b20">Ortega et al., 2018)</ref> to group point features in the frequency domain. Curve grouping forms a curvelike point set {P 1 , P 2 , ...P N } by walking from P i to P i+1 following a learnable policy ?. As shown in <ref type="figure">Figure 6b</ref>, we observe that both grouping techniques improve model robustness by a clear margin. The idea of frequency group-ing aligns with the observations in <ref type="bibr" target="#b40">(Yin et al., 2019)</ref>: there is a trade-off between model robustness to low-frequency corruptions and high-frequency corruptions. By viewing local-grouped features as low-frequency features and curvegrouped feature as high-frequency features, the robustness gain can be again interpreted from a frequency perspective. Nonetheless, it is noteworthy that advanced grouping is more time-consuming during both training and testing.</p><p>Featurizer. We refer conventional operators to shared MLPs and convolutional layers, which are common building blocks for point cloud models. Recent works explore various advanced feature processing methods, such as adaptive kernels and self-attention operations. RSCNN  and PAConv <ref type="bibr" target="#b37">(Xu et al., 2021a)</ref> design adaptive kernels whose weights change with low-level features like spatial coordinates and surface normals. Based on self-attention, PCT <ref type="bibr" target="#b10">(Guo et al., 2020)</ref> proposes the offset-attention operation, which achieves impressive performance for point cloud analysis. Despite the success of RSCNN and PAConv on clean point cloud classifications, they tend to be more sensitive to corruptions than conventional operators in our experiments shown in <ref type="figure">Figure 6c</ref>. Data corruption exacerbates through data-dependent kernels. Compared to conventional operators, self-attention operations improve classifier robustness in several aspects, particularly in "Drop-G". We speculate that its robustness gains to "Drop-G" come from its ability to understand non-local relations from the global perspective. Note that Point-BERT  also introduces an self-attention-based architecture. However, it includes a fixed tokenizer that is trained on pretext tasks, which could be the bottleneck for its robustness performance. Therefore, we do not include the randomly initialized Point-BERT result in the architecture analysis.  <ref type="figure">Figure 6</ref>. Analysis on different architecture designs, pretrain strategies and augmentation strategies' effect to classifier's performance under different corruptions. "-G": Global. "-L": Local. <ref type="bibr" target="#b9">Goyal et al., 2021)</ref> first project 3D shapes to 2D frames from different viewpoints, and then use 2D classifiers for recognizing 3D points. The recently proposed projection-based method, SimpleView <ref type="bibr" target="#b9">(Goyal et al., 2021)</ref> performs surprisingly well on clean 3D point clouds. In our experiments shown in <ref type="figure">Figure 6d</ref>, projecting 3D points to 2D images brought mixed effects to classification. The projection significantly reduces the effect of "Jitter" and "Add-L", but suffers a lot from point scarcity, particularly "Drop-G". This is consistent with human visual perception, as it is challenging for human vision to recognize the shape from point projections, especially for sparse and noisy points without texture information. Adding more observations from different perspectives might improve 2D perception accuracy, while extra efforts are required. In a nutshell, we think 3D cues are more straight-forward and preferable for building a robust point cloud classifier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Self-supervised Pretraining</head><p>Recently, various self-supervised pretrain methods have been proposed for point cloud classification models, such as Point-BERT  and OcCo <ref type="bibr">(Wang et al., 2021)</ref>. We study their robustness against corruptions in <ref type="figure">Figure 6e</ref>, which reveals that pretrain signals can be transferred, and hence benefiting classification under specific corruptions. During self-supervised pretrain, Point-BERT first drops points using the block-wise masking strategy and then reconstructs the missing points based on the rest points. Interestingly, models finetuned on Point-BERT pre-train show better classification robustness when local part is missing. OcCo employs a similar reconstruction pretrain task, but with a different masking strategy. By observing from different camera viewpoints, OcCo masks the points that are self-occluded. Meanwhile, point clouds are also rotated with different camera angles. Consequently, the OcCo pretrained models are significantly more robust to rotation perturbations. Moreover, OcCo also improves the robustness to "Jitter" and "Add-L".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Augmentation Method</head><p>Following the principle of OOD evaluation, the corruptions should not be used as augmentations during training, and therefore we choose mixing and deformation augmentations. As shown in <ref type="figure">Figure 6f</ref>, mixing and deformation augmentations can bring significant improvements to model robustness. PointMixUp  and RSMix <ref type="bibr" target="#b14">(Lee et al., 2021)</ref> are two mix strategies. Similar to MixUp <ref type="bibr" target="#b43">(Zhang et al., 2018)</ref> in 2D augmentation, PointMixup mixes two point clouds using shortest-path interpolation. Similar to CutMix <ref type="bibr" target="#b42">(Yun et al., 2019)</ref> in 2D augmentation, RSMix mixes two point clouds using rigid transformation. Both mix strategies substantially reduce CE on corruptions including "Add-G", "Add-L", "Rotate" and "Jitter". However, an unexpected side effect of the mix strategies is that classifiers become more vulnerable to scaling effects. By non-rigidly deforming local parts of an object, PointWOLF <ref type="bibr" target="#b14">(Kim et al., 2021)</ref> enrich the data variation, which constantly improves classifier robustness on all evaluated corruptions.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Boosting Corruption Robustness</head><p>Based on the above observations, we propose to improve point cloud classifier robustness in the following ways.</p><p>RPC: A Robust Point Cloud Classifier. Following the conclusions in the architecture analysis, we construct RPC using 3D representation, k-NN, frequency grouping and selfattention. The detailed architecture is shown in the appendix. As reported in <ref type="table" target="#tab_5">Table 3</ref>, RPC achieves the best mCE compared to all SoTA methods. The success of RPC empirically verifies our conclusions on the architecture design choices, and it could serve as a strong baseline for future robustness research. The implementation details are provided in the appendix.</p><p>WOLFMix: A Strong Augmentation Strategy. We design WOLFMix upon two powerful augmentation strategies, PointWOLF and RSMix. During training, WOLFMix first deforms the object, and then rigidly mixes the two deformed objects together. Ground-truth labels are mixed accordingly. We show an illustration of WOLFMix in <ref type="figure" target="#fig_4">Figure 7</ref>. By taking advantage of both rigid and non-rigid transformations, WOLFMix brings substantial robustness gain over standalone PointWOLF and RSMix in <ref type="table" target="#tab_7">Table 5</ref>. Implementation details can be found in the appendix.</p><p>Synergy between Architecture and Augmentation. We observe that augmentation techniques do not equally transfer to different architectures. <ref type="table" target="#tab_8">Table 6</ref> shows that the improvement by WOLFMix on corruption robustness varies with different models. Although RPC achieves the lowest standalone mCE, its improvements by WOLFMix are less than desk chair RSMix desk+PointWOLF chair+PointWOLF WOLFMix WOLFMix for DGCNN, PCT and GDANet. PointNet enjoys limited robustness gain as well. Hence, we speculate that there is a capacity upper bound to corruptions for each architecture. Future classification robustness research is suggested to study: 1) standalone robustness for architecture and augmentations independently; and 2) their synergy in between. Furthermore, we identify that training GDANet with WOLFMix achieves the best robustness in all existing methods, with an impressive 0.571 mCE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>In this work, we establish a comprehensive test suite ModelNet-C for robust point cloud classification under corruptions. We systematically benchmarked and analyzed representative point cloud classification methods. By analyzing benchmark results, we propose two effective strategies, RPC and WOLFMix, for improving robustness. As the SoTA methods for point cloud classification on clean data are becoming less robust to random real-world corruptions, we highly encourage future research to focus on classification robustness so as to benefit real applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Acknowledgment</head><p>This work is supported by the National Research Foundation, Singapore under its AI Singapore Programme (AISG Award No: AISG2-PhD-2021-08-018), NTU NAP, MOE AcRF Tier 2 (T2EP20221-0033), and under the RIE2020 Industry Alignment Fund -Industry Collaboration Projects (IAF-ICP) Funding Initiative, as well as cash and in-kind contribution from the industry partner(s).</p><p>Zhou, H., <ref type="bibr">Chen, K., Zhang, W., Fang, H., Zhou, W., and Yu, N. Dup-net:</ref> Denoiser and upsampler network for 3d adversarial point clouds defense. In Proceedings of the IEEE/CVF International Conference on Computer <ref type="bibr">Vision, pp. 1961</ref><ref type="bibr">Vision, pp. -1970</ref><ref type="bibr">Vision, pp. , 2019</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Corruptions and Severity Level Settings</head><p>We elaborate on the implementation of corruptions and severity level settings in this section. A visualization is shown in <ref type="figure" target="#fig_5">Figure 8</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1. Jitter</head><p>We add a Gaussian noise ? N (0, ? 2 ) to each of a point's X, Y, and Z coordinates, where ? ? {0.01, 0.02, 0.03, 0.04, 0.05} for the five levels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Scale</head><p>We apply random scaling to the X, Y, and Z axis respectively. The scaling coefficient for each axis are independently sampled as s ? U(1/S, S), where S ? {1.6, 1.7, 1.8, 1.9, 2.0} for the five levels. Point clouds are re-normalized to a unit sphere after scaling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3. Rotate</head><p>We randomly apply a rotation described by an X-Y-Z Euler angle (?, ?, ?), where ?, ?, ? ? U(??, ?) and ? ? {?/30, ?/15, ?/10, ?/7.5, ?/6} for the five levels. Note that the sampling method does not guarantee a uniform SO(3) rotation sampling, but sufficient to cover a range of rotation variations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4. Drop Global</head><p>We randomly shuffle all points and drop the last N * ? points, where N = 1024 is the number of points in the point cloud and ? ? {0.25, 0.375, 0.5, 0.675, 0.75} for all five levels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5. Drop Local</head><p>We drop K points in total, where K ? {100, 200, 300, 400, 500} for the five levels. We randomly choose C, the number of local parts to drop, by C ? U{1, 8}. We further randomly assign i-th local part a cluster size N i so that K = C i=1 N i . Then we repeat the following steps for C times: we randomly select a point as the i-th local center, and drop its N i -nearest neighbour points (including itself) from the point cloud.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.6. Add Global</head><p>We uniformly sample K points inside a unit sphere and add them to the point cloud, where K ? {10, 20, 30, 40, 50} for the five levels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.7. Add Local</head><p>We add K points in total, where K ? {100, 200, 300, 400, 500} for the five levels. We randomly shuffle the points, and select the first C ? U{1, 8} points as the local centers. We further randomly assign i-th local part a cluster size N i so that K = C i=1 N i . Neighbouring point's X-Y-Z coordinates are generated from a Normal distribution N (? i , ? 2 i I), where ? i is the i-th local center's X-Y-Z coordinate and ? i ? U(0.075, 0.125). We then add each local part to the point cloud one by one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Implementation Details</head><p>We elaborate on implementation details for RPC and WOLFMix in this section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1. RPC</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1.1. DETAILED ARCHITECTURE</head><p>We show a detailed architecture of RPC in <ref type="figure">Figure 9</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Local Operations</head><p>Advanced Grouping Featurizer <ref type="figure">Figure 9</ref>. Detailed architecture of RPC. We design RPC following the conclusions we draw from the benchmark. It optimizes the use of existing building blocks in point cloud classifiers and serves as a strong baseline for corruption robustness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1.2. HYPER-PARAMETERS</head><p>For local operation, we use k=30 for the number of neighbors in k-NN. For, frequency grouping, we follow the default hyper-parameters in GDANet <ref type="bibr" target="#b38">(Xu et al., 2021b)</ref>. The number of points in each frequency component is set to 256.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1.3. TRAINING</head><p>We train the model for 250 epochs with a batch size of 32. We use SGD with momentum 0.9 for optimization. We use a cosine annealing scheduler to gradually decay the learning rate from 1e-2 to 1e-4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2. WOLFMix</head><p>For the deformation step, we use the default hyper-parameters in PointWOLF <ref type="bibr" target="#b14">(Kim et al., 2021)</ref>. We set the number of anchors to 4, sampling method to farthest point sampling, kernel bandwidth to 0.5, maximum local rotation range to 10 degrees, maximum local scaling to 3, and maximum local translation to 0.25. AugTune proposed along with PointWOLF is not used in training. For the mixing step, we use the default hyper-parameters in RSMix <ref type="bibr" target="#b14">(Lee et al., 2021)</ref>. We set RSMix probability to 0.5, ? to 1.0, and the maximum number of point modifications to 512. For training, the number of neighbors in k-NN is reduced to 20 and the number of epochs is increased to 500 for all methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Additional Discussions</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1. Correlation between ModelNet-C mCE and ScanObjectNN OA</head><p>We additionally evaluate all models on ScanObjectNN <ref type="bibr" target="#b30">(Uy et al., 2019)</ref>, and we use the OBJ BG PB T25 variant to include both background remains and bounding box inaccuracy. The results are shown in <ref type="figure" target="#fig_0">Figure 10</ref>, and ModelNet-C mCE strongly correlates to ScanObjectNN OA, while ModelNet40 OA has nearly no correlations. Note that the results we report are lower than the results originally reported in <ref type="bibr" target="#b30">Uy et al. (2019)</ref>   ModelNet40 OA <ref type="figure" target="#fig_0">Figure 10</ref>. Correlation between ModelNet-C mCE and ScanObjectNN OA.  <ref type="table" target="#tab_11">Table 8</ref>. PoinASNL shows outstanding performance to noisy jittering and achieves a competitive overall mCE result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.4. Evaluation on works designed for rotation robustness</head><p>Robustness to arbitrary SO(3) rotations is out of the scope of our benchmark where we examine common corruptions like small view angle variation. Nevertheless, we evaluate Vector Neurons , a rotation-invariant model, and show the results in <ref type="table" target="#tab_11">Table 8</ref>. Vector Neuron achieves impressive robustness to rotational corruption, but under-performs to other types of corruption.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Full Results</head><p>We show full results for the OA metric and the RmCE metric in <ref type="table" target="#tab_12">Table 9 and Table 10</ref>.  <ref type="bibr" target="#b23">(Qi et al., 2017b)</ref> 0.907 0.658 0.881 0.797 0.876 0.778 0.121 0.562 0.591 PointNet++ <ref type="bibr" target="#b21">(Qi et al., 2017a)</ref> 0.930 0.751 0.918 0.628 0.841 0.627 0.819 0.727 0.698 RSCNN  0.923 0.739 0.899 0.630 0.800 0.686 0.790 0.683 0.682 SimpleView <ref type="bibr" target="#b9">(Goyal et al., 2021)</ref> 0.939 0.757 0.918 0.774 0.692 0.719 0.710 0.768 0.717 GDANet <ref type="bibr" target="#b38">(Xu et al., 2021b)</ref> 0.934 0.789 0.922 0.735 0.803 0.815 0.743 0.715 0.789 CurveNet <ref type="bibr" target="#b35">(Xiang et al., 2021)</ref> 0 </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>P o i n t N e t P o i n t N e t + + D G C N N R S C N N P C T G D A N e t P A C o n v S i m p l e V i e w C u r v e N e Upper. Blue curve shows overall accuracy (OA) on Mod-elNet40. Red curve shows mean Corruption Error (mCE) on proposed ModelNet-C. Methods are sorted in chronological order.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Examples of our proposed ModelNet-C. We corrupt the clean test set of ModelNet-C using seven types of corruptions with five levels of severity to provide a comprehensive robustness evaluation. Listed examples are from severity level 2. More visualizations on different severity levels can be found in the supplementary material.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Robust point cloud classification paradigm. Point cloud classification robustness to various corruptions largely depends on three main components, including architecture design, selfsupervised pretraining and augmentation methods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>2D vs. 3D Representation. A few methods<ref type="bibr" target="#b22">(Qi et al., 2016</ref>;Scale Jitter Rotate Drop-G Drop-L Add-G Add-2D v.s. 3D Scale Jitter Rotate Drop-G Drop-L Add-G Add-</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 .</head><label>7</label><figDesc>Illustration of the proposed WOLFMix augmentation. Point clouds are first locally deformed and then rigidly mixed. Ground truth labels are mixed accordingly.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 8 .</head><label>8</label><figDesc>Corruptions on all levels. The severity of corruptions increases with the level. We average model's error on all levels for a comprehensive evaluation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>due to different training protocols. Uy et al. (2019) uses random rotation and per-point jitter in training while we follow the DGCNN protocol (Goyal et al., 2021).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>S-Lab, Nanyang Technological University. Correspondence to: Ziwei Liu &lt;ziwei.liu@ntu.edu.sg&gt;. Proceedings of the 39 th International Conference on Machine Learning, Baltimore, Maryland, USA, PMLR 162, 2022. Copyright 2022 by the author(s).</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 .</head><label>2</label><figDesc>Systematic study for architecture design.</figDesc><table><row><cell></cell><cell>Representation</cell><cell>Local Operations</cell><cell>Advanced Grouping</cell><cell>Featurizer</cell><cell>mCE(?)</cell></row><row><cell>PointNet</cell><cell>3D</cell><cell>No</cell><cell>No</cell><cell cols="2">Conventional 1.422</cell></row><row><cell>PointNet++</cell><cell>3D</cell><cell>Ball-query</cell><cell>No</cell><cell cols="2">Conventional 1.072</cell></row><row><cell>DGCNN</cell><cell>3D</cell><cell>k-NN</cell><cell>No</cell><cell cols="2">Conventional 1.000</cell></row><row><cell>RSCNN</cell><cell>3D</cell><cell>Ball-query</cell><cell>No</cell><cell>Adaptive</cell><cell>1.130</cell></row><row><cell>PAConv</cell><cell>3D</cell><cell>k-NN</cell><cell>No</cell><cell>Adaptive</cell><cell>1.104</cell></row><row><cell>CurveNet</cell><cell>3D</cell><cell>k-NN</cell><cell>Curve</cell><cell cols="2">Conventional 0.927</cell></row><row><cell>GDANet</cell><cell>3D</cell><cell>k-NN</cell><cell cols="3">Frequency Conventional 0.892</cell></row><row><cell>PCT</cell><cell>3D</cell><cell>k-NN</cell><cell>No</cell><cell cols="2">Self-attention 0.925</cell></row><row><cell>SimpleView</cell><cell>2D</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>1.047</cell></row><row><cell>RPC (Ours)</cell><cell>3D</cell><cell>k-NN</cell><cell cols="3">Frequency Self-attention 0.863</cell></row><row><cell></cell><cell cols="2">2D Projection</cell><cell>CNN</cell><cell></cell><cell></cell></row><row><cell>PCD</cell><cell cols="2">Local Ops</cell><cell>Featurizer</cell><cell></cell><cell>Prediction</cell></row><row><cell></cell><cell></cell><cell>Advanced</cell><cell></cell><cell>Repeat</cell><cell></cell></row><row><cell></cell><cell></cell><cell>Grouping</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 .</head><label>3</label><figDesc>Architectures. Bold: best in column. Underline: second best in column. Blue: best in row. Red: worst in row.</figDesc><table><row><cell></cell><cell>OA ?</cell><cell>mCE ?</cell><cell>Scale</cell><cell>Jitter</cell><cell>Drop-G</cell><cell>Drop-L</cell><cell>Add-G</cell><cell>Add-L</cell><cell>Rotate</cell></row><row><cell>DGCNN (Wang et al., 2019)</cell><cell>0.926</cell><cell>1.000</cell><cell>1.000</cell><cell>1.000</cell><cell>1.000</cell><cell>1.000</cell><cell>1.000</cell><cell>1.000</cell><cell>1.000</cell></row><row><cell>PointNet (Qi et al., 2017b)</cell><cell>0.907</cell><cell>1.422</cell><cell>1.266</cell><cell>0.642</cell><cell>0.500</cell><cell>1.072</cell><cell>2.980</cell><cell>1.593</cell><cell>1.902</cell></row><row><cell>PointNet++ (Qi et al., 2017a)</cell><cell>0.930</cell><cell>1.072</cell><cell>0.872</cell><cell>1.177</cell><cell>0.641</cell><cell>1.802</cell><cell>0.614</cell><cell>0.993</cell><cell>1.405</cell></row><row><cell>RSCNN (Liu et al., 2019)</cell><cell>0.923</cell><cell>1.130</cell><cell>1.074</cell><cell>1.171</cell><cell>0.806</cell><cell>1.517</cell><cell>0.712</cell><cell>1.153</cell><cell>1.479</cell></row><row><cell>SimpleView (Goyal et al., 2021)</cell><cell>0.939</cell><cell>1.047</cell><cell>0.872</cell><cell>0.715</cell><cell>1.242</cell><cell>1.357</cell><cell>0.983</cell><cell>0.844</cell><cell>1.316</cell></row><row><cell>GDANet (Xu et al., 2021b)</cell><cell>0.934</cell><cell>0.892</cell><cell>0.830</cell><cell>0.839</cell><cell>0.794</cell><cell>0.894</cell><cell>0.871</cell><cell>1.036</cell><cell>0.981</cell></row><row><cell>CurveNet (Xiang et al., 2021)</cell><cell>0.938</cell><cell>0.927</cell><cell>0.872</cell><cell>0.725</cell><cell>0.710</cell><cell>1.024</cell><cell>1.346</cell><cell>1.000</cell><cell>0.809</cell></row><row><cell>PAConv (Xu et al., 2021a)</cell><cell>0.936</cell><cell>1.104</cell><cell>0.904</cell><cell>1.465</cell><cell>1.000</cell><cell>1.005</cell><cell>1.085</cell><cell>1.298</cell><cell>0.967</cell></row><row><cell>PCT (Guo et al., 2020)</cell><cell>0.930</cell><cell>0.925</cell><cell>0.872</cell><cell>0.870</cell><cell>0.528</cell><cell>1.000</cell><cell>0.780</cell><cell>1.385</cell><cell>1.042</cell></row><row><cell>RPC (Ours)</cell><cell>0.930</cell><cell>0.863</cell><cell>0.840</cell><cell>0.892</cell><cell>0.492</cell><cell>0.797</cell><cell>0.929</cell><cell>1.011</cell><cell>1.079</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 .</head><label>4</label><figDesc>Pretrain. ?: randomly initialized. Bold: best in column. Underline: second best in column. Blue: best in row. Red: worst in row.</figDesc><table><row><cell></cell><cell>OA ?</cell><cell>mCE ?</cell><cell>Scale</cell><cell>Jitter</cell><cell>Drop-G</cell><cell>Drop-L</cell><cell>Add-G</cell><cell>Add-L</cell><cell>Rotate</cell></row><row><cell>DGCNN (Wang et al., 2019)</cell><cell>0.926</cell><cell>1.000</cell><cell>1.000</cell><cell>1.000</cell><cell>1.000</cell><cell>1.000</cell><cell>1.000</cell><cell>1.000</cell><cell>1.000</cell></row><row><cell>+OcCo (Wang et al., 2021)</cell><cell>0.922</cell><cell>1.047</cell><cell>1.606</cell><cell>0.652</cell><cell>0.903</cell><cell>1.039</cell><cell>1.444</cell><cell>0.847</cell><cell>0.837</cell></row><row><cell>Point-BERT  ?</cell><cell>0.919</cell><cell>1.317</cell><cell>0.936</cell><cell>0.987</cell><cell>0.899</cell><cell>1.295</cell><cell>2.336</cell><cell>1.360</cell><cell>1.409</cell></row><row><cell>+Point-BERT (Yu et al., 2021)</cell><cell>0.922</cell><cell>1.248</cell><cell>0.936</cell><cell>1.259</cell><cell>0.690</cell><cell>1.150</cell><cell>1.932</cell><cell>1.440</cell><cell>1.326</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 .</head><label>5</label><figDesc>Augmentation. Bold: best in column. Underline: second best in column. Blue: best in row. Red: worst in row.</figDesc><table><row><cell></cell><cell>OA ?</cell><cell>mCE ?</cell><cell>Scale</cell><cell>Jitter</cell><cell>Drop-G</cell><cell>Drop-L</cell><cell>Add-G</cell><cell>Add-L</cell><cell>Rotate</cell></row><row><cell>DGCNN (Wang et al., 2019)</cell><cell>0.926</cell><cell>1.000</cell><cell>1.000</cell><cell>1.000</cell><cell>1.000</cell><cell>1.000</cell><cell>1.000</cell><cell>1.000</cell><cell>1.000</cell></row><row><cell>+PointWOLF (Kim et al., 2021)</cell><cell>0.926</cell><cell>0.814</cell><cell>0.926</cell><cell>0.864</cell><cell>0.988</cell><cell>0.874</cell><cell>0.807</cell><cell>0.764</cell><cell>0.479</cell></row><row><cell>+RSMix (Lee et al., 2021)</cell><cell>0.930</cell><cell>0.745</cell><cell>1.319</cell><cell>0.873</cell><cell>0.653</cell><cell>0.589</cell><cell>0.281</cell><cell>0.629</cell><cell>0.870</cell></row><row><cell>+WOLFMix (Ours)</cell><cell>0.932</cell><cell>0.590</cell><cell>0.989</cell><cell>0.715</cell><cell>0.698</cell><cell>0.575</cell><cell>0.285</cell><cell>0.415</cell><cell>0.451</cell></row><row><cell>PointNet++ (Qi et al., 2017a)</cell><cell>0.930</cell><cell>1.072</cell><cell>0.872</cell><cell>1.177</cell><cell>0.641</cell><cell>1.802</cell><cell>0.614</cell><cell>0.993</cell><cell>1.405</cell></row><row><cell>+PointMixUp (Chen et al., 2020)</cell><cell>0.915</cell><cell>1.028</cell><cell>1.670</cell><cell>0.712</cell><cell>0.802</cell><cell>1.812</cell><cell>0.458</cell><cell>0.615</cell><cell>1.130</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 .</head><label>6</label><figDesc>Results of combining WOLFMix with different architectures. Bold: best in column. Underline: second best in column. Blue: best in row. Red: worst in row.</figDesc><table><row><cell></cell><cell>OA ?</cell><cell>mCE ?</cell><cell>Scale</cell><cell>Jitter</cell><cell>Drop-G</cell><cell>Drop-L</cell><cell>Add-G</cell><cell>Add-L</cell><cell>Rotate</cell></row><row><cell>DGCNN (Wang et al., 2019)</cell><cell>0.926</cell><cell>1.000</cell><cell>1.000</cell><cell>1.000</cell><cell>1.000</cell><cell>1.000</cell><cell>1.000</cell><cell>1.000</cell><cell>1.000</cell></row><row><cell>+WOLFMix</cell><cell>0.932</cell><cell>0.590</cell><cell>0.989</cell><cell>0.715</cell><cell>0.698</cell><cell>0.575</cell><cell>0.285</cell><cell>0.415</cell><cell>0.451</cell></row><row><cell>PointNet (Qi et al., 2017b)</cell><cell>0.907</cell><cell>1.422</cell><cell>1.266</cell><cell>0.642</cell><cell>0.500</cell><cell>1.072</cell><cell>2.980</cell><cell>1.593</cell><cell>1.902</cell></row><row><cell>+WOLFMix</cell><cell>0.884</cell><cell>1.180</cell><cell>2.117</cell><cell>0.475</cell><cell>0.577</cell><cell>1.082</cell><cell>2.227</cell><cell>0.702</cell><cell>1.079</cell></row><row><cell>PCT (Guo et al., 2020)</cell><cell>0.930</cell><cell>0.925</cell><cell>0.872</cell><cell>0.870</cell><cell>0.528</cell><cell>1.000</cell><cell>0.780</cell><cell>1.385</cell><cell>1.042</cell></row><row><cell>+WOLFMix</cell><cell>0.934</cell><cell>0.574</cell><cell>1.000</cell><cell>0.854</cell><cell>0.379</cell><cell>0.493</cell><cell>0.298</cell><cell>0.505</cell><cell>0.488</cell></row><row><cell>GDANet (Xu et al., 2021b)</cell><cell>0.934</cell><cell>0.892</cell><cell>0.830</cell><cell>0.839</cell><cell>0.794</cell><cell>0.894</cell><cell>0.871</cell><cell>1.036</cell><cell>0.981</cell></row><row><cell>+WOLFMix</cell><cell>0.934</cell><cell>0.571</cell><cell>0.904</cell><cell>0.883</cell><cell>0.532</cell><cell>0.551</cell><cell>0.305</cell><cell>0.415</cell><cell>0.409</cell></row><row><cell>RPC</cell><cell>0.930</cell><cell>0.863</cell><cell>0.840</cell><cell>0.892</cell><cell>0.492</cell><cell>0.797</cell><cell>0.929</cell><cell>1.011</cell><cell>1.079</cell></row><row><cell>+WOLFMix</cell><cell>0.933</cell><cell>0.601</cell><cell>1.011</cell><cell>0.968</cell><cell>0.423</cell><cell>0.512</cell><cell>0.332</cell><cell>0.480</cell><cell>0.479</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 7 .</head><label>7</label><figDesc>Additional results of WolfMix.We report additional results on RSCNN, SimpleView and PointNet++ with WolfMix inTable 7. The unequal benefits from augmentations motivate future research to explore the synergy between architecture and augmentation.</figDesc><table><row><cell></cell><cell>OA</cell><cell>mCE</cell></row><row><cell>PointNet++ (Qi et al., 2017a)</cell><cell cols="2">0.931 0.641</cell></row><row><cell>RSCNN (Liu et al., 2019)</cell><cell cols="2">0.918 0.601</cell></row><row><cell cols="3">SimpleView (Goyal et al., 2021) 0.922 0.676</cell></row><row><cell>C.2. More results of Wolfmix</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 8 .</head><label>8</label><figDesc>More techniques. Evaluation on specific techniques proposed for enhancing robustnessThere are a few methods for robustness enhancement. TriangleNet<ref type="bibr" target="#b36">(Xiao &amp; Wachs, 2021)</ref>'s clean performance is not comparable to SoTA and PointASNL<ref type="bibr" target="#b39">(Yan et al., 2020)</ref> requires a fixed number of points. Nonetheless, we manage to evaluate PointASNL with additional manual efforts and show the results in</figDesc><table><row><cell></cell><cell>OA</cell><cell cols="6">mCE Scale Jitter Drop-G Drop-L Add-G Add-L Rotate</cell></row><row><cell>PointASNL (Yan et al., 2020)</cell><cell cols="2">0.918 0.959 1.191 0.687</cell><cell>0.944</cell><cell>0.826</cell><cell>0.959</cell><cell>0.953</cell><cell>1.153</cell></row><row><cell cols="3">Vector Neuron (Deng et al., 2021) 0.908 1.345 1.287 1.601</cell><cell>1.875</cell><cell>1.754</cell><cell>0.902</cell><cell>1.567</cell><cell>0.428</cell></row><row><cell>C.3.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 9 .</head><label>9</label><figDesc>Full results for Overall Accuracy (OA). ?: randomly initialized. Bold: best in column. Underline: second best in column. Blue: best in row. Red: worst in row. mOA: average OA over all corruptions.Clean ? mOA? Scale Jitter Drop-G Drop-L Add-G Add-L Rotate</figDesc><table><row><cell>DGCNN (Wang et al., 2019)</cell><cell>0.926</cell><cell>0.764</cell><cell>0.906 0.684</cell><cell>0.752</cell><cell>0.793</cell><cell>0.705</cell><cell>0.725</cell><cell>0.785</cell></row><row><cell>PointNet</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 10 .</head><label>10</label><figDesc>Full results for Relative mCE. ?: random initialized. Bold: best in column. Underline: second best in column. Blue: best in row.</figDesc><table><row><cell></cell><cell>.938</cell><cell>0.779</cell><cell>0.918 0.771</cell><cell>0.824</cell><cell>0.788</cell><cell>0.603</cell><cell>0.725</cell><cell>0.826</cell></row><row><cell>PAConv (Xu et al., 2021a)</cell><cell>0.936</cell><cell>0.730</cell><cell>0.915 0.537</cell><cell>0.752</cell><cell>0.792</cell><cell>0.680</cell><cell>0.643</cell><cell>0.792</cell></row><row><cell>PCT (Guo et al., 2020)</cell><cell>0.930</cell><cell>0.781</cell><cell>0.918 0.725</cell><cell>0.869</cell><cell>0.793</cell><cell>0.770</cell><cell>0.619</cell><cell>0.776</cell></row><row><cell>RPC (Ours)</cell><cell>0.930</cell><cell>0.795</cell><cell>0.921 0.718</cell><cell>0.878</cell><cell>0.835</cell><cell>0.726</cell><cell>0.722</cell><cell>0.768</cell></row><row><cell>DGCNN+OcCo (Wang et al., 2021)</cell><cell>0.922</cell><cell>0.766</cell><cell>0.849 0.794</cell><cell>0.776</cell><cell>0.785</cell><cell>0.574</cell><cell>0.767</cell><cell>0.820</cell></row><row><cell>Point-BERT  ?</cell><cell>0.919</cell><cell>0.678</cell><cell>0.912 0.688</cell><cell>0.777</cell><cell>0.732</cell><cell>0.311</cell><cell>0.626</cell><cell>0.697</cell></row><row><cell>Point-BERT (Yu et al., 2021)</cell><cell>0.922</cell><cell>0.693</cell><cell>0.912 0.602</cell><cell>0.829</cell><cell>0.762</cell><cell>0.430</cell><cell>0.604</cell><cell>0.715</cell></row><row><cell>PN2+PointMixUp (Chen et al., 2020)</cell><cell>0.915</cell><cell>0.785</cell><cell>0.843 0.775</cell><cell>0.801</cell><cell>0.625</cell><cell>0.865</cell><cell>0.831</cell><cell>0.757</cell></row><row><cell>DGCNN+PW (Kim et al., 2021)</cell><cell>0.926</cell><cell>0.809</cell><cell>0.913 0.727</cell><cell>0.755</cell><cell>0.819</cell><cell>0.762</cell><cell>0.790</cell><cell>0.897</cell></row><row><cell>DGCNN+RSMix (Lee et al., 2021)</cell><cell>0.930</cell><cell>0.839</cell><cell>0.876 0.724</cell><cell>0.838</cell><cell>0.878</cell><cell>0.917</cell><cell>0.827</cell><cell>0.813</cell></row><row><cell>DGCNN+WOLFMix (Ours)</cell><cell>0.932</cell><cell>0.871</cell><cell>0.907 0.774</cell><cell>0.827</cell><cell>0.881</cell><cell>0.916</cell><cell>0.886</cell><cell>0.903</cell></row><row><cell>PointNet+WOLFMix</cell><cell>0.884</cell><cell>0.743</cell><cell>0.801 0.850</cell><cell>0.857</cell><cell>0.776</cell><cell>0.343</cell><cell>0.807</cell><cell>0.768</cell></row><row><cell>PCT+WOLFMix</cell><cell>0.934</cell><cell>0.873</cell><cell>0.906 0.730</cell><cell>0.906</cell><cell>0.898</cell><cell>0.912</cell><cell>0.861</cell><cell>0.895</cell></row><row><cell>GDANet+WOLFMix</cell><cell>0.934</cell><cell>0.871</cell><cell>0.915 0.721</cell><cell>0.868</cell><cell>0.886</cell><cell>0.910</cell><cell>0.886</cell><cell>0.912</cell></row><row><cell>RPC+WOLFMix</cell><cell>0.933</cell><cell>0.865</cell><cell>0.905 0.694</cell><cell>0.895</cell><cell>0.894</cell><cell>0.902</cell><cell>0.868</cell><cell>0.897</cell></row><row><cell>Red: worst in row.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="8">RmCE ? Scale Jitter Drop-G Drop-L Add-G Add-L Rotate</cell></row><row><cell>DGCNN (Wang et al., 2019)</cell><cell></cell><cell>1.000</cell><cell>1.000 1.000</cell><cell>1.000</cell><cell>1.000</cell><cell>1.000</cell><cell>1.000</cell><cell>1.000</cell></row><row><cell>PointNet (Qi et al., 2017b)</cell><cell></cell><cell>1.488</cell><cell>1.300 0.455</cell><cell>0.178</cell><cell>0.970</cell><cell>3.557</cell><cell>1.716</cell><cell>2.241</cell></row><row><cell>PointNet++ (Qi et al., 2017a)</cell><cell></cell><cell>1.114</cell><cell>0.600 1.248</cell><cell>0.511</cell><cell>2.278</cell><cell>0.502</cell><cell>1.010</cell><cell>1.645</cell></row><row><cell>RSCNN (Liu et al., 2019)</cell><cell></cell><cell>1.201</cell><cell>1.200 1.211</cell><cell>0.707</cell><cell>1.782</cell><cell>0.602</cell><cell>1.194</cell><cell>1.709</cell></row><row><cell>SimpleView (Goyal et al., 2021)</cell><cell></cell><cell>1.181</cell><cell>1.050 0.682</cell><cell>1.420</cell><cell>1.654</cell><cell>1.036</cell><cell>0.851</cell><cell>1.574</cell></row><row><cell>GDANet (Xu et al., 2021b)</cell><cell></cell><cell>0.865</cell><cell>0.600 0.822</cell><cell>0.753</cell><cell>0.895</cell><cell>0.864</cell><cell>1.090</cell><cell>1.028</cell></row><row><cell>CurveNet (Xiang et al., 2021)</cell><cell></cell><cell>0.978</cell><cell>1.000 0.690</cell><cell>0.655</cell><cell>1.128</cell><cell>1.516</cell><cell>1.060</cell><cell>0.794</cell></row><row><cell>PAConv (Xu et al., 2021a)</cell><cell></cell><cell>1.211</cell><cell>1.050 1.649</cell><cell>1.057</cell><cell>1.083</cell><cell>1.158</cell><cell>1.458</cell><cell>1.021</cell></row><row><cell>PCT (Guo et al., 2020)</cell><cell></cell><cell>0.884</cell><cell>0.600 0.847</cell><cell>0.351</cell><cell>1.030</cell><cell>0.724</cell><cell>1.547</cell><cell>1.092</cell></row><row><cell>RPC (Ours)</cell><cell></cell><cell>0.778</cell><cell>0.450 0.876</cell><cell>0.299</cell><cell>0.714</cell><cell>0.923</cell><cell>1.035</cell><cell>1.149</cell></row><row><cell>DGCNN+OcCo (Wang et al., 2021)</cell><cell></cell><cell>1.302</cell><cell>3.650 0.529</cell><cell>0.839</cell><cell>1.030</cell><cell>1.575</cell><cell>0.771</cell><cell>0.723</cell></row><row><cell>Point-BERT  ?</cell><cell></cell><cell>1.330</cell><cell>0.350 0.955</cell><cell>0.816</cell><cell>1.406</cell><cell>2.751</cell><cell>1.458</cell><cell>1.574</cell></row><row><cell>Point-BERT (Yu et al., 2021)</cell><cell></cell><cell>1.262</cell><cell>0.500 1.322</cell><cell>0.534</cell><cell>1.203</cell><cell>2.226</cell><cell>1.582</cell><cell>1.468</cell></row><row><cell>PN2+PointMixUp (Chen et al., 2020)</cell><cell></cell><cell>1.254</cell><cell>3.600 0.579</cell><cell>0.655</cell><cell>2.180</cell><cell>0.226</cell><cell>0.418</cell><cell>1.121</cell></row><row><cell cols="2">DGCNN+PointWOLF (Kim et al., 2021)</cell><cell>0.698</cell><cell>0.650 0.822</cell><cell>0.983</cell><cell>0.805</cell><cell>0.742</cell><cell>0.677</cell><cell>0.206</cell></row><row><cell>DGCNN+RSMix (Lee et al., 2021)</cell><cell></cell><cell>0.839</cell><cell>2.700 0.851</cell><cell>0.529</cell><cell>0.391</cell><cell>0.059</cell><cell>0.512</cell><cell>0.830</cell></row><row><cell>DGCNN+WOLFMix (Ours)</cell><cell></cell><cell>0.485</cell><cell>1.250 0.653</cell><cell>0.603</cell><cell>0.383</cell><cell>0.072</cell><cell>0.229</cell><cell>0.206</cell></row><row><cell>PCT+WOLFMix</cell><cell></cell><cell>0.488</cell><cell>1.400 0.843</cell><cell>0.161</cell><cell>0.271</cell><cell>0.100</cell><cell>0.363</cell><cell>0.277</cell></row><row><cell>GDANet+WOLFMix</cell><cell></cell><cell>0.439</cell><cell>0.950 0.880</cell><cell>0.379</cell><cell>0.361</cell><cell>0.109</cell><cell>0.239</cell><cell>0.156</cell></row><row><cell>RPC+WOLFMix</cell><cell></cell><cell>0.517</cell><cell>1.400 0.988</cell><cell>0.218</cell><cell>0.293</cell><cell>0.140</cell><cell>0.323</cell><cell>0.255</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Objectron: A large scale dataset of object-centric videos in the wild with pose annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ahmadyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ablavatski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Grundmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="7822" to="7831" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Objectnet: A large-scale bias-controlled dataset for pushing the limits of object recognition models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Barbu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mayo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Alverio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gutfreund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Katz</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9448" to="9458" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">State of the art in surface reconstruction from point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Berger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tagliasacchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Seversky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Alliez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sharf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Silva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eurographics 2014-State of the Art Reports</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="161" to="185" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deep hierarchical cluster network with rigorously rotation-invariant representation for point cloud analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Clusternet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4989" to="4997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Pointmixup: Augmentation for point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">T</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mensink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mettes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">G M</forename><surname>Snoek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">12348</biblScope>
			<biblScope unit="page" from="330" to="345" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Vector neurons: A general framework for so (3)-equivariant networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Litany</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Poulenard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tagliasacchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="12200" to="12209" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">ImageNet: A Large-Scale Hierarchical Image Database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">09</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Self-robust 3d point recognition via gather-vector guidance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="11513" to="11521" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Orderly disorder in point cloud domain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ghahremani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Tiddeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Behera</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV (28)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">12373</biblScope>
			<biblScope unit="page" from="494" to="509" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Revisiting point cloud shape classification with a simple and effective baseline</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-X</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-N</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-J</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-M</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pct</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>Point cloud transformer</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Benchmarking neural network robustness to common corruptions and perturbations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">G</forename><surname>Dietterich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR (Poster). OpenReview.net</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">The many faces of robustness: A critical analysis of out-of-distribution generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Basart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kadavath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Dorundo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Desai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Parajuli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Steinhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gilmer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Basart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Steinhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Song</surname></persName>
		</author>
		<title level="m">Natural adversarial examples. CVPR</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Point cloud augmentation with weighted local transformations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2021-10" />
			<biblScope unit="page" from="548" to="557" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Regularization strategy for point cloud via rigidly mixed sample</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Woo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>Computer Vision Foundation / IEEE</publisher>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Pointaugment: An autoaugmentation framework for point cloud classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Heng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>Computer Vision Foundation / IEEE</publisher>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Provably robust 3d point cloud classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">Z</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pointguard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="6186" to="6195" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Relation-shape convolutional neural network for point cloud analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8895" to="8904" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Cloud transformers: A universal approach to point cloud processing tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mazur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Graph signal processing: Overview, challenges, and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ortega</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Frossard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kova?evi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Moura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE</title>
		<meeting>the IEEE</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">106</biblScope>
			<biblScope unit="page" from="808" to="828" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deep hierarchical feature learning on point sets in a metric space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pointnet++</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Volumetric and multi-view cnns for object classification on 3d data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nie?ner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5648" to="5656" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deep learning on point sets for 3d classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pointnet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="652" to="660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Do imagenet classifiers generalize to imagenet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Roelofs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Shankar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML, volume 97 of Proceedings of Machine Learning Research</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5389" to="5400" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Common objects in 3d: Large-scale learning and evaluation of real-life 3d category reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Reizenstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Shapovalov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Henzler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sbordone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Labatut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Novotny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Discrete signal processing on graphs: Frequency analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sandryhaila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Moura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="3042" to="3054" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Dynamic edgeconditioned filters in convolutional neural networks on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Simonovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Adversarially robust 3d point cloud recognition using self-supervisions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Anandkumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">M</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Fifth Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Taghanaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">K</forename><surname>Jayaraman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Jatavallabhula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Robustpointset</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.11572</idno>
		<title level="m">A dataset for benchmarking robustness of point cloud classifiers</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Revisiting point cloud classification: A new benchmark dataset and classification model on real-world data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Uy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">T</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yeung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1588" to="1597" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Unsupervised point cloud pre-training via occlusion completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lasenby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Kusner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision, ICCV</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Dynamic graph cnn for learning on point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Sarma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Solomon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Improved model structure and unsupervised domain adaptation for road-object segmentation from a lidar point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Keutzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Squeezesegv2</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4376" to="4382" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">3d shapenets: A deep representation for volumetric shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1912" to="1920" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Walk in the cloud: Learning curves for point clouds shape analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2021-10" />
			<biblScope unit="page" from="915" to="924" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Triangle-net: Towards robustness in point cloud learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Wachs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WACV</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="826" to="835" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Position adaptive convolution with dynamic kernel assembling on point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Paconv</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Learning geometry-disentangled representation for complementary understanding of 3d object point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="3056" to="3064" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Robust point clouds processing using nonlocal neural networks with adaptive sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pointasnl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5589" to="5598" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">A fourier perspective on model robustness in computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">G</forename><surname>Lopes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gilmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="13255" to="13265" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Point-bert: Pre-training 3d point cloud transformers with masked point modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Cutmix: Regularization strategy to train strong classifiers with localizable features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6023" to="6032" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">mixup: Beyond empirical risk minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ciss?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lopez-Paz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR (Poster). OpenReview.net</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Rotation invariant convolutions for 3d point clouds deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B.-S</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">W</forename><surname>Rosen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-K</forename><surname>Yeung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on 3D Vision (3DV)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="204" to="213" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Point</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Transformer</surname></persName>
		</author>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="16259" to="16268" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
