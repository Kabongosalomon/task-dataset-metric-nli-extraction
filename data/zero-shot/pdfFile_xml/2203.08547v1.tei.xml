<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Non-isotropy Regularization for Proxy-based Deep Metric Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karsten</forename><surname>Roth</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of T?bingen</orgName>
								<address>
									<addrLine>2 DeepMind</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeynep</forename><surname>Akata</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of T?bingen</orgName>
								<address>
									<addrLine>2 DeepMind</addrLine>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">MPI for Intelligent Systems</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Non-isotropy Regularization for Proxy-based Deep Metric Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T15:26+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Deep Metric Learning (DML) aims to learn representation spaces on which semantic relations can simply be expressed through predefined distance metrics. Best performing approaches commonly leverage class proxies as sample stand-ins for better convergence and generalization. However, these proxy-methods solely optimize for sampleproxy distances. Given the inherent non-bijectiveness of used distance functions, this can induce locally isotropic sample distributions, leading to crucial semantic context being missed due to difficulties resolving local structures and intraclass relations between samples. To alleviate this problem, we propose non-isotropy regularization (NIR) for proxy-based Deep Metric Learning. By leveraging Normalizing Flows, we enforce unique translatability of samples from their respective class proxies. This allows us to explicitly induce a non-isotropic distribution of samples around a proxy to optimize for. In doing so, we equip proxybased objectives to better learn local structures. Extensive experiments highlight consistent generalization benefits of NIR while achieving competitive and state-of-theart performance on the standard benchmarks CUB200-2011, Cars196 and Stanford Online Products. In addition, we find the superior convergence properties of proxybased methods to still be retained or even improved, making NIR very attractive for practical usage. Code available at github.com/ExplainableML/NonIsotropicProxyDML.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Visual similarity plays a crucial role for applications in image &amp; video retrieval and clustering <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b53">52,</ref><ref type="bibr" target="#b64">63]</ref>, face reidentification <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b32">32]</ref> or general supervised <ref type="bibr" target="#b21">[22]</ref> and unsupervised <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b17">18]</ref> contrastive representation learning. A majority of approaches used in these fields employ or can be derived from Deep Metric Learning (DML). DML aims to learn highly nonlinear distance metrics parametrized by deep networks. These networks span a representation space in which semantic relations between images are expressed as distances between respective representations. In the field of DML, methods utilizing proxies have shown to provide <ref type="bibr">Figure 1</ref>. Proxy-based Deep Metric Learning methods optimize for non-bijective similarity measures between proxies ( ) and sample representation (?), which can introduce local isotropy around proxies, impeding local structures and non-discriminative features to be learned. We propose NIR to explicitly resolve this. among the most consistent and highest performances in addition to fast convergence <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b39">38,</ref><ref type="bibr" target="#b57">56]</ref>. While other methods introduce ranking tasks over samples for the network to solve, proxy-based methods require the network to contrast samples against a proxy representation, commonly approximating generic class prototypes. Their utilization addresses sampling complexity issues <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b48">47,</ref><ref type="bibr" target="#b53">52,</ref><ref type="bibr" target="#b64">63]</ref> inherent to purely sample-based approaches, resulting in improved convergence and benchmark performance.</p><p>However, there is no free lunch. Relying on sampleproxy relations, relations between samples within a class can not be explicitly captured. This is exacerbated by proxy-based objectives optimizing for distances between samples and proxies using non-bijective distance functions. This means, for a particular proxy, that alignment to a sample is non-unique -as long as the angle between sample and proxy is retained, i.e. samples being aligned isotropically around a proxy (see <ref type="figure">Fig. 1</ref>), their distances and respective loss remain the same. This means that samples lie on a hypersphere centered around a proxy with same distance and thus incurring the same training loss. This incorporates an undesired prior over sample-proxy distributions which doesn't allow local structures to be resolved well. By incorporating multiple classes and proxies (which is automatically done when applying proxy-based losses such as <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b39">38,</ref><ref type="bibr" target="#b44">43,</ref><ref type="bibr" target="#b57">56]</ref> to training data with multiple classes), this is extended to a mixture of sample distributions around proxies. While this offers an implicit workaround to ad-dress isotropy around modes by incorporating relations of samples to proxies from different classes, relying only on other unrelated proxies potentially far away makes finegrained resolution of local structures difficult. Furthermore, as training progresses and proxies move further apart. As a consequence, the distribution of samples around proxies, which proxy-based objectives optimize for, comprises modes with high affinity towards local isotropy. This introduces semantic ambiguity, as semantic relations between samples within a class are not resolved well. However, a lot of recent work has shown that understanding and incorporating these non-discriminative relations drives generalization performance <ref type="bibr" target="#b31">[31,</ref><ref type="bibr" target="#b34">34,</ref><ref type="bibr" target="#b47">46,</ref><ref type="bibr" target="#b50">49,</ref><ref type="bibr" target="#b67">66]</ref>.</p><p>To tackle this issue without resorting to sample-based objectives that impede the superior convergence and generalization of proxy-based approaches, this work proposes non-isotropy regularisation (NIR) for proxy-based DML. NIR extends proxy-based objectives to encourage explicitly learning unique sample-proxy relations and eliminating semantic ambiguity. In detail, we introduce a novel uniqueness constraint, in which samples within a class must be uniquely and sufficiently described by a (non-linear) translation from the respective class proxy. This explicitly induces a distribution for proxy-based objectives to match in which isotropy and ambiguity is heavily penalized. We achieve uniqueness by leveraging a bijective and thus invertible family of translations. As the proxy-sample translations need to adapt to the specific domain at hand, we require both trainability and non-linearity of our translation models. These functional constraints are naturally expressed through Normalizing Flows and Invertible Networks <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b42">41]</ref>. Using conditional variants, we then formalize NIR where sample relations are (uniquely) mapped by a Normalizing Flow given some residual conditioned on the respective class proxy.</p><p>Extensive experiments show that NIR indeed introduces higher feature diversity, reduces overclustering, increases uniformity in learned representation spaces and learns more diverse class-distributions than non-regularized counterparts. Evaluating our approach on the standard DML benchmarks CUB200-2011 <ref type="bibr" target="#b58">[57]</ref>, CARS196 <ref type="bibr" target="#b30">[30]</ref> and Stanford Online Products <ref type="bibr" target="#b41">[40]</ref> showcases improved generalization capabilities of NIR-equipped proxy DML, achieving competitive or state-of-the-art performance while retaining or even improving convergence speeds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head><p>Deep Metric Learning (DML) has driven research in image and video retrieval &amp; zero-shot clustering applications <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b53">52,</ref><ref type="bibr" target="#b55">54,</ref><ref type="bibr" target="#b64">63]</ref>, with particular applications for example in person re-identification <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b32">32,</ref><ref type="bibr" target="#b60">59]</ref> and as an auxiliary tool for improved supervised <ref type="bibr" target="#b21">[22]</ref> and unsupervised representation learning <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b37">37]</ref>. Commonly proposed DML methods introduce ranking tasks for networks to solve as training surrogates. These can involve ranking constituents in tuples (such as pairs <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b40">39]</ref>, triplets <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b53">52,</ref><ref type="bibr" target="#b64">63]</ref> or higher-order tuples <ref type="bibr" target="#b41">[40,</ref><ref type="bibr" target="#b55">54,</ref><ref type="bibr" target="#b62">61]</ref>) to contrasting between sample and prototypical representations <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b39">38,</ref><ref type="bibr" target="#b44">43,</ref><ref type="bibr" target="#b57">56,</ref><ref type="bibr" target="#b70">69]</ref>. These prototypical-or proxy-based approaches are commonly introduced in order to address sampling complexity issues when sampling tuples for a network to solve, which are otherwise addressed through various sampling heuristics <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b48">47,</ref><ref type="bibr" target="#b53">52,</ref><ref type="bibr" target="#b64">63]</ref>. We propose a natural extension to proxy-based approaches by addressing a major shortcoming introduced when only contrasting between samples and proxies while retaining beneficial properties of these methods. Finally, recent work has focused on generic extensions to DML to improve the quality of learned representation spaces through divide-and-conquer <ref type="bibr" target="#b52">[51]</ref>, synthetic data <ref type="bibr" target="#b31">[31,</ref><ref type="bibr" target="#b67">66]</ref>, adversarial and graph-based training <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b54">53,</ref><ref type="bibr" target="#b70">69]</ref>, bypassing representation bottlenecks <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b49">48]</ref>, attention <ref type="bibr" target="#b24">[25]</ref> and auxiliary or few-shot feature learning <ref type="bibr">[12, 34-36, 46, 49]</ref>. These works offer distinct, orthogonal benefits.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Non-isotropic Deep Metric Learning</head><p>A DML model defines a distance metric d ? (x i , x j ) over images x i ? X parametrized by a feature extraction backbone ? and a projection f onto the final metric space ? ? R d , such that ? := f ? ?(X ). ? is commonly normalized to the unit hypersphere <ref type="bibr" target="#b61">[60,</ref><ref type="bibr" target="#b64">63]</ref> such that ? = S d?1 ? . This metric space is commonly equipped with a predefined distance metric such as the euclidean distance d(?, ?) or cosine similarity s(?, ?), which are equivalent on the hypersphere <ref type="bibr" target="#b59">[58,</ref><ref type="bibr" target="#b66">65]</ref>. During training, DML learns ? = f ? ? to connect d ? (x i , x j ) := d(? i , ? j ) to the semantic similarity of images x i and x j . Training methods commonly involve the definition of ranking tasks for the network to solve -given e.g. a triplet of anchor x a , positive x p and negative x n with y a = y p = y n where y ? Y denotes the respective class and triplets (x a , x p , x n ) ? T B sampled from a minibatch B. However, tuple sampling is difficult, as the tuple space complexity increases with tuple dimensionality; incurring a lot of redundancy <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b53">52,</ref><ref type="bibr" target="#b64">63]</ref>. While sampling heuristics have been introduced to address this <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b48">47,</ref><ref type="bibr" target="#b53">52,</ref><ref type="bibr" target="#b64">63]</ref>, recent work <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b39">38,</ref><ref type="bibr" target="#b57">56]</ref> has heuristically supported the promise of proxy-based ranking objectives.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">On the shortcoming of proxy-based DML</head><p>Proxy-based objectives use contrastive operations not between samples (e.g. via the cosine similarity s ? (x i , x j ) <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b62">61]</ref>), but between class-prototypical (class proxies) representations ? j ? P s(? i , ? j ) for classes y i and y j . This removes the need for complex sampling operations in methods reliant on sample-based tuples, which allows proxybased objectives to benefit from fast convergence and good <ref type="figure">Figure 2</ref>. NIR -Non-isotropy Regularisation. We refine the distribution of samples ? around class proxies ?, p(?|?), learned via proxybased DML by leveraging Normalizing Flows (? , ). These allow us to define a bijective translation ? which uses a simple density q = N (0, 1) of residuals ? to induce a distribution over unique sample-proxy relations, p(? (?|?)|?). This allows for better resolution of local structures and non-discriminative features to be learned, improving generalisation while retaining fast convergence. generalization performance.</p><p>However, this property also incurs the strongest shortcoming, because relying on sample-proxy pairs and the non-bijective similarity measure s(?, ?) := s(? i , ? y ? i ) can induce features to locally follow an isotropic distribution around the proxy. This can be seen more explicitly when looking at the sample-proxy distributions various proxyobjectives optimize for. Take for example the foundational ProxyNCA objective <ref type="bibr" target="#b39">[38]</ref>. ProxyNCA is heavily connected to various recent, state-of-the-art objectives (such as Prox-yAnchor <ref type="bibr" target="#b23">[24]</ref> or SoftTriple <ref type="bibr" target="#b44">[43]</ref>) and has the form </p><formula xml:id="formula_0">L PNCA = ?E x?Xy</formula><p>with the complete set of class proxies P and with class y removed P ?y 1 that are trained jointly during training. Minimizing the distance of samples to their respective class proxies while maximizing it for non-related proxies, this objective can be regarded as implicitly maximizing the loglikelihood of samples ? belonging to proxy ? (such that y ? = y ? ) under a von-Mises-Fisher (vMF 2 ) mixture model <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b16">17]</ref> around directions ?</p><formula xml:id="formula_2">p vMFmm (?|?) = ? ? C d (? ? )e ??s(?,?) ? * ?P ? ? * C d (? ? * )e ? ? * s(?,? * ) (2) C d (?) = ? d/2?1 ? (2?) d/2 I d/2?1 (?) ?1<label>(3)</label></formula><p>assuming a class-independent concentration parameter ? ? = ? and mixture ? ? = ? such that C d (? ? ) = C d (?) = const 3 . Even more, <ref type="bibr" target="#b57">[56]</ref> show that performance of L PNCA <ref type="bibr" target="#b0">1</ref> We use cosine similarity instead of the euclidean distance as done in <ref type="bibr" target="#b39">[38,</ref><ref type="bibr" target="#b57">56]</ref>, as both are equivalent on the hypersphere. <ref type="bibr" target="#b1">2</ref> An assumption found also e.g. in self-supervised learning <ref type="bibr" target="#b71">[70]</ref>. <ref type="bibr" target="#b2">3</ref> C d incorporates the modified Bessel function Ip of the first kind and order p, which can be neglected here as C d cancels out improves when actually optimize the proxy-assignment probability (by replacing P ?y with P in the denominator, giving L PNCA++ ) directly, which equals an explicit negative log-likelihood minimization of p vMFmm :</p><formula xml:id="formula_3">L PNCA++ = ?E x?Xy,y?Y [log p vMFmm (?(x)|? y )]<label>(4)</label></formula><p>Recent and state-of-the-art proxy objectives that extend upon ProxyNCA, such as the ProxyAnchor <ref type="bibr" target="#b22">[23]</ref> objective</p><formula xml:id="formula_4">L PA = 1 |P + | ??P + log ? ? 1 + x?B,yx=y? e ???[s(x,?)??] ? ? + 1 |P| ??P log ? ? 1 + x?B,yx =y? e ??[s(x,?)+?] ? ?<label>(5)</label></formula><p>operate under similar assumptions, suggesting slight, more hyperparameter-heavy variations to the loss terms. While ProxyAnchor specifically suggests pulling samples towards proxies instead of proxies towards samples as done in L PNCA , it similarly relies on the same sample-proxy contrastive operations to learn a respective metric space. This means that these methods can only learn sample distributions around proxies that are closely related to p vMFmm -like distributions learned via L PNCA . Indeed, our experiments (see ?4.2 and Tab. 1) show that when adapting the Prox-yNCA objective to the ProxyAnchor formulation</p><formula xml:id="formula_5">L * PNCA = 1 |B + | x?B log 1 + e ???[s(x,?y x )??] + 1 |B| x?X log ? ? 1 + ??P,yx =y? e ??[s(x,?)+?] ? ?<label>(6)</label></formula><p>performance becomes much more similar than indicated in <ref type="bibr" target="#b22">[23]</ref>. And while ProxyAnchor may not optimize for the exact p(?|?) formulation, the results support a very strong distributional relation between these proxy-objectives. However, mixture distributions such as p vMFmm suffer from several issues. Firstly, each mode on its own is isotropic as s(?, ?) returns the same value as long as the angle ?(?, ?) is retained. This means that class-specific structures can only be resolved implicitly through relations with proxies of different classes. Secondly, this intraclass resolution becomes worse as training progress, since proxies from different classes continue to contrast further and sample-proxy relations for same-class pairs are overshadowed (see e.g. Eq. 1, 5, 6). Similarly, for samples closer towards each respective class proxy, resolving local structures becomes harder. Effectively, this results in learned sample distributions to have a strong affinity towards local isotropy.</p><p>As such, proxy-based objectives are inherently handicapped in resolving local intraclass clusters and structures. Consequently, semantic relations between samples within a class are not well encoded in the representational structure of the learned deep metric spaces. However, the ability to explain and represent intraclass sample relations has been consistently shown to be a key driver for downstream generalization performance in DML <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b31">31,</ref><ref type="bibr" target="#b34">34,</ref><ref type="bibr" target="#b35">35,</ref><ref type="bibr" target="#b47">46,</ref><ref type="bibr" target="#b50">49]</ref>.</p><p>While sample-based objectives similarly suffer from the non-bijectivity of s(?, ?), the usage of sample-to-sample operations explicitly introduces intersample relational context <ref type="bibr" target="#b50">[49]</ref> that allows for better structuring of samples within a class. However, just incorporating a sample-based contrastive operation into the training process is not a sufficient remedy as it re-introduces the sampling complexity issue; whose removal was what made proxy-based methods and their fast convergence attractive in the first place.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Non-isotropy Regularization</head><p>Motivation. To address the non-learnability of intraclass context in proxy-based DML, we therefore have to address the inherent issue of local isotropy in the learned sampleproxy distribution p(?|?). However, to retain the convergence (and generalization) benefits of proxy-based methods, this has to be achieved without resorting to additional augmentations that move the overall objective away from a purely proxy-based one. As such, we aim to find p(?|?) whose optimization better resolves the distribution of sample representations ? around our proxy ?. This can be achieved by breaking down the fundamental issue of nonbijectivity in the used similarity measure s(?, ?), which (on its own) introduces non-unique sample-proxy relations. To do so, we look for some regularization that specifically encourages unique sample-proxy relations to exists. For such unique sample-proxy relations to exist, we must have access to some bijective and thus invertible (deterministic) translation ? = ? (?|?) which, given some residual ? from some prior distribution q(?), allows to uniquely translate from the respective proxy ? to ?. Given such a unique translation of samples and proxies within a class, the local alignment of samples would then no longer rely on relations to proxies and samples from different classes which, as noted, do not scale well locally and as training progresses. Assuming proxies and sample representation to have the same dimensionality, this can be achieved through some affine transformation. However, to capture non-linear relations and proxy-to-sample translations, it is much more beneficial for ? to be non-linear.</p><p>Normalizing Flows. Such invertible, non-linear functions are naturally expressed through Normalizing Flows (NF) or more generally invertible neural networks <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b45">44,</ref><ref type="bibr" target="#b46">45,</ref><ref type="bibr" target="#b51">50]</ref>. A Normalizing Flow can be generally seen as a transformation between two probability distributions, most commonly between simple, well-defined ones and complex multimodal ones <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b42">41]</ref>. More specifically, we leverage flows similar to the one proposed in <ref type="bibr" target="#b8">[9]</ref> and <ref type="bibr" target="#b26">[27]</ref> (and as used e.g. in <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b46">45,</ref><ref type="bibr" target="#b51">50]</ref>), which introduces a sequence of non-linear, but still invertible coupling operations as showcased in <ref type="figure">Fig. 2 ( )</ref>. Given some input representation ?, a coupling block splits ? into ? 1 and ? 2 , which are scaled and translated in succession with nonlinear scaling and translation networks ? 1 and ? 2 , respectively. Note that following <ref type="bibr" target="#b26">[27]</ref>, each network ? i provides both scaling ? s i and translation values ? t i , such that</p><formula xml:id="formula_6">? * 2 = ? 2 exp (? s 1 (? 1 )) + ? t 1 (? 1 ) ? * 1 = ? 1 exp (? s 2 (? * 2 )) + ? t 2 (? * 2 ) ? * = [? * 1 , ? * 2 ]<label>(7)</label></formula><p>where ? * denotes ? after passing through a respective coupling block. Successive application of different ? i then gives our non-linear invertible transformation ? from some prior distribution over residuals q(?) with explicit density and CDF (for sampling) to our target distribution.</p><p>Enforcing non-isotropy. Consequently, our bijective ? (conditioned on proxies ?) induces a new sample representation distribution p(? (?|?)|?) as pushforward from our prior distribution of residuals q(?) which accounts for unique sample-to-proxy relations, and which we wish to impose over our learned sample distribution p(?|?). This introduces our Non-Isotropy Regularization (NIR). NIR can be naturally approached through maximization of the expected log-likelihood E x,?y x [log p (?(x)|? yx )] over sample-proxy pairs (x, ? yx ) similar to Eq. 4, but under the constraint that each distribution of samples around a respective proxy, p(?|?), is a pushforward of ? from our residual distribution q(?). This gives (see e.g. <ref type="bibr" target="#b29">[29]</ref>)</p><formula xml:id="formula_7">L NIR = ?E x,?y x [log q ? ?1 (?(x)|? yx ) + log | det J ? ?1 (? ?1 (?(x)|? yx )|? yx )|]<label>(8)</label></formula><p>with Jacobian J for translation ? ?1 and proxies ? yx , where y x denotes the class of sample x. To arrive at above equation, we simply leveraged the change of variables formula</p><formula xml:id="formula_8">p(?|?) = q(? ?1 (?|?))| det J ? ?1 (? ?1 (?|?)|?)|.<label>(9)</label></formula><p>In practice, by setting our prior q(?) to be a standard zeromean unit-variance normal distribution N (0, 1), we get</p><formula xml:id="formula_9">L NIR = 1 |B| (x,?y x )?B ? ?1 (?(x)|? yx ) 2 2 ? log | det J ? ?1 (? ?1 (?(x)|? yx )|? yx )|<label>(10)</label></formula><p>i.e. given sample representations ?(x), we project them onto our residual space ? via ? ?1 and compute Eq. 10. By selecting suitable normalizing flows such as GLOW <ref type="bibr" target="#b26">[27]</ref>, we make sure that the Jacobian is cheap to compute. NIR for proxy-based DML. As NIR targets the alignment of samples around proxies, we still need to learn the global alignment of proxies through proxy-based DML L PDML (?, P). This gives the full training objective</p><formula xml:id="formula_10">L = f (L NIR ) + ? ? L PDML (?, P)<label>(11)</label></formula><p>where f (?) is a monotonous function of L NIR to match the scaling and training dynamics of L PDML without changing the invertibility constraint. As most proxy-based L PDML utilize exponential components, we simply use f (?) = exp(?). Full optimization over L then learns proxies while uniquely resolving sample placement around them. More specifically, backpropagating through L NIR optimizes for sample alignment around proxies ?, the translation ? and provides updates to the proxies, though we found the latter to not be a necessity, as proxies primarily serve to resolve global alignment of sample clusters. NIR-proxy-DML has several advantages. Firstly, the final sample-proxy distribution optimized for directly addresses issues of local isotropy to better resolve local intraclass structure, as the retention of a unique sample distribution around each proxies requires implicit knowledge about the intraclass alignment of each class sample around their respective proxies. Secondly, unlike ProxyNCA-like objectives (see the previous section), we do not assume the same concentration of samples per class as assumed in e.g. Eq. 1. Instead, the non-linear translation conditioned on the class proxy can introduce class-dependent concentrations when needed. Finally, being able to directly resolve local structures can potentially benefit convergence of these methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>This section lists experimental details ( ?4.1), showcases significant benefits of NIR for proxy-based DML ( ?4.2) and highlights the impact on convergence and training times in ?4.3. We also study quantitative impacts on learned representation spaces ( ?4.4), provide method ablations in ?4.5 and investigate self-regulatory properties of NIR ( ?4.6).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experimental Details</head><p>Implementations use PyTorch <ref type="bibr" target="#b43">[42]</ref>. ImageNet <ref type="bibr" target="#b5">[6]</ref>pretrainings are taken from torchvision <ref type="bibr" target="#b33">[33]</ref> and timm <ref type="bibr" target="#b63">[62]</ref>. Our experiments were run on compute servers with NVIDIA 2080Ti. Our Normalizing Flow utilizes 8 coupling blocks and subnets ? comprising linear layers with 128 nodes. Optimization is done using Adam <ref type="bibr" target="#b25">[26]</ref> (learning rate 10 ?5 , weight decay 4 ? 10 ?3 , <ref type="bibr" target="#b50">[49]</ref>). We set ? ? [0.001, 0.01] depending on the choice for L PDML . In general, we found consistent improvements in this interval. Following <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b39">38,</ref><ref type="bibr" target="#b57">56]</ref> we utilize a high learning rate multiplication for the proxies (4000). We also saw this helping the Normalizing Flow and use 50 for all experiments. Finally, we found a warmup epoch to help; adapting the translation ? over pretrained features first before joint training.</p><p>Datasets. We use the standard benchmarks CUB200-2011 <ref type="bibr" target="#b58">[57]</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Effectiveness of Non-isotropy Regularisation</head><p>To evaluate the relative benefits of NIR, we follow protocols proposed in <ref type="bibr" target="#b50">[49]</ref> to encourage exact comparability with no learning rate scheduling. Initial tuning of newly introduced hyperparameters is done on a random 15% validation split (see e.g. <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b50">49]</ref>). Note that we don't perform joint hyperparameter tuning of auxiliary proxy objectives L DML and NIR to see how well NIR can be applied to blackbox proxy approaches. For L DML , we select ProxyAnchor <ref type="bibr" target="#b22">[23]</ref>, SoftTriplet <ref type="bibr" target="#b44">[43]</ref>(10 centroids for CUB200-2011/CARS196 and 2 for SOP) and ProxyNCA <ref type="bibr" target="#b39">[38]</ref> following Eq. 6. Results over multiple seeds are provided in <ref type="table" target="#tab_1">Table 1</ref>, showing that generalization significantly improves for all proxy objectives across metrics and benchmarks, even for objectives with more than one proxy per class s.a. SoftTriple. For the latter, we find NIR to also improve convergence properties as shown in ?4.3. Especially for datasets where a reasonable amount of samples per class is available (s.a. CUB200-2011 &amp; CARS196) to learn meaningful class distributions we see major improvements, e.g. for state-of-the-art Prox-yAnchor from 82.4% to 85.2 &amp; 64.4 to 66.0% on CARS196 &amp; CUB200-2011, respectively. However, even for datasets such as SOP with a small number of samples per class a consistent performance improvement can be seen, highlighting the general benefit of NIR for proxy-based DML.</p><p>To compare to the overall corpus of DML methods, we also provide a literature comparison in <ref type="table">Table 2</ref>, with approaches divided based on backbone architecture and embedding dimensionality; both of which drive generalization performance independent of DML objectives <ref type="bibr" target="#b40">[39,</ref><ref type="bibr" target="#b50">49]</ref>. Results reported here use stepwise learning rate scheduling at most twice, with parameters determined by performance on a random, 15% validation subset <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b34">34,</ref><ref type="bibr" target="#b50">49]</ref>. As can  <ref type="table">Table 2</ref>. Literature comparison using ProxyAnchor (PA) + NIR. Across backbones/dim. and benchmarks, we find competitive and even state-of-the-art performance against much more complex methods. X : Combination of pooling operations in backbone as done in <ref type="bibr" target="#b22">[23]</ref>.</p><p>Bold denotes best in a given Architecture/Dimensionality setting. Bluebold denotes best overall. be seen, NIR-equipped ProxyAnchor achieves competitive performance across settings and benchmarks with a new highest overall score. In addition, PA+NIR beats much more complex methods such as DiVA [34] using joint multitask and self-supervised training or MIC <ref type="bibr" target="#b47">[46]</ref> using external feature mining. This supports the benefit of learning global alignments with proxies while jointly refining local sample alignment. Taking into account that NIR retains the superior convergence of proxy-based approaches (see ?4.3), this makes NIR very attractive for practical usage, and provides a strong proof-of-concept on the benefits of intraclass context for proxy-DML.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Convergence Properties</head><p>A primary motivation for NIR, besides generalization improvements, is to retain fast convergence speeds. We investigate this following the same setup used for Tab. 1 ( ?4.2). Results visualized in <ref type="figure">Fig. 3</ref> show mean test generalisation performance after every epoch <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b39">38,</ref><ref type="bibr" target="#b52">51]</ref> (un-like Tab. 1 showing overall mean test performance) for every auxiliary proxy objective. Besides significant improvements in generalisation performance, convergence speeds and behaviour are either retained or even improved e.g. for SoftTriple, presumably due to better resolved class structures allowing for better alignment of multiple learned class centroids. Furthermore, the addition of the Normalizing Flow adds only limited additional computational overhead since we operate in feature space. Especially with respect to the large backbone, we find changes in walltime and required additional GPU memory to be negligible (&lt; 1%).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Qualitative differences in alignment</head><p>In this section, we investigate how NIR changes the structural properties of the representation spaces learned by proxy-based objectives. For our experiments, we select ProxyAnchor as stand-in for L PDML . We then compare the structure of representation space learned with and without NIR by looking at different structural metrics: (1) <ref type="figure">Figure 3</ref>. Impact of NIR on convergence. We find that NIR increases generalisation performance while retaining or even improving the fast convergence behaviour. <ref type="table">Table 3</ref>. Change in Structural Properties. Applying NIR increase feature diversity ? and uniformity of learned representations G2, reduces overclustering ?density and encourages a higher degree in class concentration difference between classes (? 2 ? ).  Feature richness measured by spectral decay <ref type="bibr" target="#b50">[49]</ref> ?(?) = KL(U||S(?)) with singular value decomposition S(?) of feature space ?. ?(?) measures the number of significant directions of variance in the learned feature spacelower scores indicate a higher feature variety, linked to improved generalisation in <ref type="bibr" target="#b34">[34,</ref><ref type="bibr" target="#b50">49]</ref>. (2) Representational Uniformity/Density <ref type="bibr" target="#b50">[49]</ref> ? density = ?intra /?inter, which measures the ratio of mean intraclass and interclass distance ? intra and ? inter , respectively. ? density relates class concentration against overall alignment across the hypersphere. Higher values indicate lower class concentrations and overclustering, a potential link to better generalisation <ref type="bibr" target="#b50">[49]</ref>.</p><formula xml:id="formula_11">Dataset Setup ? ? ? density ? ? 2 ? ? G 2 ?</formula><formula xml:id="formula_12">(3) Em- bedding space uniformity [60] G t=2 (u, v) = e ?t u?v 2 2</formula><p>evaluating uniformity of embedding spaces measured by a Radial Basis Function kernel <ref type="bibr" target="#b2">[3]</ref>. Lower values have been linked in <ref type="bibr" target="#b61">[60]</ref> to improved downstream performance in contrastive self-supervised learning. (4) Variance of classconcentrations ? 2 ? approximated by the mean distances to the class centers of mass (relative to the mean interclass distance ? inter to account for the overall scale of representations). As NIR allows for different class-conditional distributions to be learned, we assume a higher ? 2 ? . Results in Tab. 3 indeed show higher feature diversity ? (over 30%, cf. <ref type="figure" target="#fig_2">Fig. 4</ref> for the sorted singular value spectra), reduced overclustering (&gt; 15%) as measured by ? density and increased uniformity in learned representation spaces (up to 9% as evaluated via G 2 ) -all linked to better generalization as noted above. This links well with our initial motivation for NIR to allow for better explicit resolution of local structures and clusters, which requires local separability of representations and the introduction of auxiliary features <ref type="bibr" target="#b34">[34,</ref><ref type="bibr" target="#b50">49]</ref>. We also find that the variance of classconcentrations increases, supporting that NIR helps proxybased objectives learn class-dependent sample distributions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Ablations</head><p>We now ablate NIR. Results are provided in Tab. 4. We use ProxyAnchor as stand-in for L PDML (a) Training Normalizing Flows. We first ablate the scaling f (?) ( ?3.2, Eq. 10). As can be seen, the exact choice of exponential function does not matter, with changes in temperature (f (?, t)) or a Softplus (f (x) = log(1 + exp x)) performing similarly. We also experiment with gradient clipping ("Grad. Clipping"), but found no benefits. Furthermore, we investigate joint negative log-likelihood (NLL) maximization for sample-proxy-pairs with different classes ("w/ Negative Pairs)" following Eq. 10, but found no benefits over just minimizing the NLL for same-class pairs. This supports our hypothesis that the benefits of NIR indeed lie in improved resolution of class-local structures. This is also supported through the minimal impact of proxy updates through L NIR ("No Proxy Backprop"), as proxies primarily help with global cluster alignment while NIR is developed for local refinement. Completely removing L PDML ("? = 0") gives similar insights -while we see decent performance solely through non-isotropy regularisation, the absence of a global alignment objective for proxies incorporated through L PDML ("? = 0") results in a notable performance drop and reduction in convergence speeds.</p><p>(b) Normalizing Flows Architecture. We ablate the number of coupling blocks (D) and subnets widths (W) used in NIR and find dataset-dependent optima with slight improvements over the default ( ?4.1). For consistency, we report all other results using the default setup. We also examine the conditioning of the Normalizing Flow -inserting proxies at the start, mid or end ("Condition start/mid/end", as opposed to every coupling block by default). As can be seen, the exact choice of conditioning influences generalization performance somewhat, but with significant improvements regardless of the exact conditioning. Overall, these results show that in an applied setting, further improvements can be found by more aggressive, but not necessarily principled hyperparameter tuning of NIR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Self-regularization</head><p>Finally, we study a natural extension to NIR by leveraging the generative process defined through our Normalizing Flow, which provides a translation from a probability density we can sample from (in this case just N (0, 1)) to the representation space for a respective class y (conditioned on ? y ). Similar to <ref type="bibr" target="#b31">[31]</ref> or <ref type="bibr" target="#b67">[66]</ref>, which saw benefits in generalisation through synthetic samples in samplebased DML, we investigate in Tab. 5 whether sampling from the residual prior ? ? q(?) and traversing the Normalizing Flow in reverse can generate synthetic samples that offer additional self-regularisation. More specifically, we investigate whether synthetic samples ? s = ? (?|?) can be used in L PDML (? sX , P) to learn more generic proxies ("Generate"), detach the synthetic samples ( X ). Similarly, we investigate whether the generated samples can be used to refine the quality of the Normalizing Flow by evaluating the matching quality with the learned proxies via L PDML (? s , P X ) ("Match"), as well as doing both steps jointly (L PDML (? s , P), "Generate &amp; Match). In its current setting, generalization and convergence suffer from artificial samples. Especially for the former, we see drops in performance from 66.0% back down to 64.8% when introducing artificial samples and even 63.2% when applying reverse distribution matching. We hypothesize that this is due to the introduction of noisy samples especially in earlier training stages and the interdependence of NIR on the quality of the learned proxies. We believe a better adapted incorporation following e.g. a hardness-aware heuristic <ref type="bibr" target="#b67">[66]</ref> could better leverage the benefits of such selfregularization. We leave this to future work to address.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>This work proposes NIR -non-isotropy regularisation for proxy-based Deep Metric Learning (DML). NIR tackles the inherent problem of proxy-based objectives to resolve local structures and clusters to learn non-discriminative features that facilitate generalization, without influencing the superior convergence of proxy-based DML. NIR achieves this by refining the sample-distributional prior optimized in standard proxy-based DML through unique sample-proxy relation constraints. Extensive experiments support the idea of NIR and, besides the retention of fast convergence speeds, show significant improvements in the generalisation performance of proxy-based objectives, achieving competitive and state-of-the-art performance on all benchmarks.</p><p>Limitations. NIR relies on learning meaningful translations from (class-)proxies to respective samples. With high proxy count, the quality of these translations is impacted, evident in the performance on SOP. In addition, our current setting can not yet leverage the sample-generative process induced by the Normalizing Flows for additional regularization (which is also bottlenecked by high proxy-counts/low number of samples per class).</p><p>Broader Impact. Our work significantly benefits proxybased DML. With fast convergence, this makes application in DML-driven domains s.a image &amp; video retrieval, but also face re-identification, very attractive. Especially for the latter more controversial application however, a potential for misuse given. However, while notable, improvements through NIR are not sufficient to drive a significant change in the societal usage in these domains.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>y?Y log e s(?(x),?y) ? * ?P ?y e s(?(x),? * )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>? 0.01 0.68 ? 0.04 0.37 ? 0.02 0.078 ? 0.001 + NIR 0.13 ? 0.02 0.79 ? 0.03 0.44 ? 0.02 0.072 ? 0.002CARS196PA 0.17 ? 0.01 0.59 ? 0.01 0.32 ? 0.01 0.079 ? 0.001 + NIR 0.13 ? 0.01 0.68 ? 0.02 0.38 ? 0.02 0.072 ? 0.001</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Singular Value Distribution used to estimate feature diversity ?, showing NIR to introduce more directions of variance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Relative comparison. We follow protocols proposed in<ref type="bibr" target="#b50">[49]</ref> <ref type="bibr" target="#b3">4</ref> , with no learning rate scheduling, to ensure exact comparability. The results show significant improvements over very strong proxy objectives on all benchmarks, but especially on CUB200 and CARS196 where a more significant number of samples per class is available. ? 0.<ref type="bibr" target="#b1">2</ref> 67.8 ? 0.4 31.1 ? 0.3 81.6 ? 0.3 69.6 ? 0.5 31.7 ? 0.1 76.0 ? 0.1 89.4 ? 0.1 43.3 ? 0.1 ProxyAnchor [23] 64.4 ? 0.3 68.4 ? 0.2 33.2 ? 0.3 82.4 ? 0.4 69.0 ? 0.3 34.2 ? 0.3 78.0 ? 0.1 90.1 ? 0.1 45.5 ? 0.1 + NIR 66.0 ? 0.3 69.6 ? 0.1 34.2 ? 0.2 85.2 ? 0.3 71.6 ? 0.3 36.4 ? 0.2 78.9 ? 0.1 90.4 ? 0.1 46.5 ? 0.1 ProxyNCA [38] 64.2 ? 0.2 68.6 ? 0.3 33.1 ? 0.2 82.1 ? 0.4 68.2 ? 0.2 32.4 ? 0.5 78.3 ? 0.1 90.0 ? 0.1 45.5 ? 0.1 + NIR 66.1 ? 0.2 69.8 ? 0.2 34.3 ? 0.1 84.3 ? 0.3 70.6 ? 0.6 34.5 ? 0.3 79.1 ? 0.1 90.2 ? 0.1 46.2 ? 0.1</figDesc><table><row><cell>BENCHMARKS?</cell><cell></cell><cell cols="2">CUB200-2011</cell><cell></cell><cell cols="2">CARS196</cell><cell></cell><cell>SOP</cell><cell></cell></row><row><cell>APPROACHES ?</cell><cell>R@1</cell><cell>NMI</cell><cell>mAP@1000</cell><cell>R@1</cell><cell>NMI</cell><cell>mAP@1000</cell><cell>R@1</cell><cell>NMI</cell><cell>mAP@1000</cell></row><row><cell cols="10">Multisimilarity 62.8 SoftTriplet [43] 62.3 ? 0.3 68.2 ? 0.2 31.6 ? 0.2 80.7 ? 0.2 66.4 ? 0.3 30.4 ? 0.2 76.9 ? 0.2 89.6 ? 0.1 43.5 ? 0.1</cell></row><row><cell>+ NIR</cell><cell cols="9">63.8 ? 0.4 68.5 ? 0.2 34.0 ? 0.4 83.4 ? 0.4 68.8 ? 0.5 35.5 ? 0.2 77.6 ? 0.1 90.0 ? 0.1 44.9 ? 0.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>? 0.2 77.7 ? 0.3 69.8 ? 0.2 85.3 ? 0.2 91.1 ? 0.2 72.1 ? 0.2 79.6 ? 0.1 90.7 ? 0.1 90.5 ? 0.1 -R50/128 X 67.9 ? 0.2 78.3 ? 0.2 71.4 ? 0.4 86.5 ? 0.3 92.0 ? 0.2 72.7 ? 0.2 79.4 ? 0.1 90.7 ? 0.1 90.6 ? 0.1 ProxyAnchor (PA) [23] CVPR '20 IBN/512 X</figDesc><table><row><cell>BENCHMARKS ?</cell><cell></cell><cell></cell><cell></cell><cell>CUB200 [57]</cell><cell></cell><cell></cell><cell>CARS196 [30]</cell><cell></cell><cell></cell><cell>SOP [40]</cell><cell></cell></row><row><cell>METHODS ?</cell><cell>Venue</cell><cell>Arch/Dim.</cell><cell>R@1</cell><cell>R@2</cell><cell>NMI</cell><cell>R@1</cell><cell>R@2</cell><cell>NMI</cell><cell>R@1</cell><cell>R@10</cell><cell>NMI</cell></row><row><cell>Div&amp;Conq [51]</cell><cell cols="2">CVPR '19 R50/128</cell><cell>65.9</cell><cell>76.6</cell><cell>69.6</cell><cell>84.6</cell><cell>90.7</cell><cell>70.3</cell><cell>75.9</cell><cell>88.4</cell><cell>90.2</cell></row><row><cell>MIC [46]</cell><cell>ICCV '19</cell><cell>R50/128</cell><cell>66.1</cell><cell>76.8</cell><cell>69.7</cell><cell>82.6</cell><cell>89.1</cell><cell>68.4</cell><cell>77.2</cell><cell>89.4</cell><cell>90.0</cell></row><row><cell>PADS [47]</cell><cell cols="2">CVPR '20 R50/128</cell><cell>67.3</cell><cell>78.0</cell><cell>69.9</cell><cell>83.5</cell><cell>89.7</cell><cell>68.8</cell><cell>76.5</cell><cell>89.0</cell><cell>89.9</cell></row><row><cell>RankMI [21]</cell><cell cols="2">CVPR '20 R50/128</cell><cell>66.7</cell><cell>77.2</cell><cell>71.3</cell><cell>83.3</cell><cell>89.8</cell><cell>69.4</cell><cell>74.3</cell><cell>87.9</cell><cell>90.5</cell></row><row><cell>PA+NIR</cell><cell>-</cell><cell>R50/128</cell><cell>66.9 68.4</cell><cell>79.2</cell><cell>-</cell><cell>86.8</cell><cell>91.6</cell><cell>-</cell><cell>79.1</cell><cell>90.8</cell><cell>-</cell></row><row><cell>ProxyGML [69]</cell><cell cols="2">NeurIPS '20 IBN/512</cell><cell>66.6</cell><cell>77.6</cell><cell>69.8</cell><cell>85.5</cell><cell>91.8</cell><cell>72.4</cell><cell>78.0</cell><cell>90.6</cell><cell>90.2</cell></row><row><cell>DRML [68]</cell><cell>ICCV '21</cell><cell>IBN/512</cell><cell>68.7</cell><cell>78.6</cell><cell>69.3</cell><cell>86.9</cell><cell>92.1</cell><cell>72.1</cell><cell>71.5</cell><cell>85.2</cell><cell>88.1</cell></row><row><cell>PA + MemVir [28]</cell><cell>ICCV '21</cell><cell>IBN/512</cell><cell>69.0</cell><cell>79.2</cell><cell>-</cell><cell>86.7</cell><cell>92.0</cell><cell>-</cell><cell>79.7</cell><cell>91.0</cell><cell>-</cell></row><row><cell>PA+NIR</cell><cell>--</cell><cell cols="10">IBN/512 IBN/512 X 70.1 ? 0.1 80.1 ? 0.2 71.0 ? 0.2 87.9 ? 0.2 92.8 ? 0.1 73.7 ? 0.2 79.3 ? 0.1 90.4 ? 0.1 90.2 ? 0.2 69.4 ? 0.2 79.7 ? 0.2 71.1 ? 0.1 87.1 ? 0.2 92.5 ? 0.1 73.1 ? 0.2 79.4 ? 0.1 90.5 ? 0.1 90.3 ? 0.2</cell></row><row><cell>EPSHN [64]</cell><cell cols="2">WACV '20 R50/512</cell><cell>64.9</cell><cell>75.3</cell><cell>-</cell><cell>82.7</cell><cell>89.3</cell><cell>-</cell><cell>78.3</cell><cell>90.7</cell><cell>-</cell></row><row><cell>Circle [55]</cell><cell cols="2">CVPR '20 R50/512</cell><cell>66.7</cell><cell>77.2</cell><cell>-</cell><cell>83.4</cell><cell>89.7</cell><cell>-</cell><cell>78.3</cell><cell>90.5</cell><cell>-</cell></row><row><cell>DiVA [34]</cell><cell cols="2">ECCV '20 R50/512</cell><cell>69.2</cell><cell>79.3</cell><cell>71.4</cell><cell>87.6</cell><cell>92.9</cell><cell>72.2</cell><cell>79.6</cell><cell>91.2</cell><cell>90.6</cell></row><row><cell>DCML-MDW [67]</cell><cell cols="2">CVPR '21 R50/512</cell><cell>68.4</cell><cell>77.9</cell><cell>71.8</cell><cell>85.2</cell><cell>91.8</cell><cell>73.9</cell><cell>79.8</cell><cell>90.8</cell><cell>90.8</cell></row><row><cell>PA+NIR</cell><cell>--</cell><cell cols="10">R50/512 R50/512 X 70.5 ? 0.1 80.6 ? 0.2 72.5 ? 0.3 89.1 ? 0.2 93.4 ? 0.2 75.0 ? 0.3 80.4 ? 0.1 91.4 ? 0.2 90.6 ? 0.1 69.1 ? 0.2 79.6 ? 0.2 72.0 ? 0.2 87.7 ? 0.2 92.5 ? 0.1 74.2 ? 0.2 80.7 ? 0.1 91.5 ? 0.1 90.9 ? 0.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Structural Ablations.</figDesc><table><row><cell>BENCHMARKS?</cell><cell cols="2">CUB200-2011</cell><cell cols="2">CARS196</cell></row><row><cell>SETUPS ?</cell><cell>R@1</cell><cell>mAP @1000</cell><cell>R@1</cell><cell>mAP @1000</cell></row><row><cell>PA + NIR</cell><cell cols="4">66.0 ? 0.3 34.2 ? 0.2 85.2 ? 0.2 36.4 ? 0.2</cell></row><row><cell cols="2">(a) Normalizing Flows Training</cell><cell></cell><cell></cell><cell></cell></row><row><cell>f (?) = SoftPlus</cell><cell cols="4">66.3 ? 0.2 34.0 ? 0.2 85.1 ? 0.2 36.3 ? 0.3</cell></row><row><cell cols="5">f (?, t = 0.3) = Exp 66.1 ? 0.2 34.1 ? 0.1 85.0 ? 0.2 36.2 ? 0.2</cell></row><row><cell>f (?, t = 3) = Exp</cell><cell cols="4">65.8 ? 0.3 33.8 ? 0.2 84.8 ? 0.3 36.1 ? 0.2</cell></row><row><cell>Grad. Clipping</cell><cell cols="4">65.9 ? 0.1 34.2 ? 0.1 85.3 ? 0.1 36.5 ? 0.1</cell></row><row><cell cols="5">No Proxy Backprop 66.2 ? 0.4 34.1 ? 0.3 85.0 ? 0.2 36.3 ? 0.1</cell></row><row><cell>w/ Negative Pairs</cell><cell cols="4">64.9 ? 0.3 33.8 ? 0.3 83.5 ? 0.4 34.9 ? 0.3</cell></row><row><cell>? = 0</cell><cell cols="4">60.0 ? 0.4 30.1 ? 0.3 73.5 ? 0.6 27.2 ? 0.5</cell></row><row><cell cols="3">(b) Normalizing Flows Architecture</cell><cell></cell><cell></cell></row><row><cell>D15 -W64</cell><cell cols="4">66.5 ? 0.5 34.0 ? 0.2 85.1 ? 0.4 36.6 ? 0.3</cell></row><row><cell>D5 -W512</cell><cell cols="4">66.1 ? 0.3 34.1 ? 0.1 84.9 ? 0.2 36.1 ? 0.2</cell></row><row><cell>D3 -W1024</cell><cell cols="4">65.8 ? 0.4 34.1 ? 0.2 85.3 ? 0.1 36.3 ? 0.2</cell></row><row><cell>Condition -Start</cell><cell cols="4">66.4 ? 0.4 34.1 ? 0.2 85.1 ? 0.2 36.2 ? 0.3</cell></row><row><cell>Condition -Mid</cell><cell cols="4">66.1 ? 0.2 34.1 ? 0.2 84.9 ? 0.2 36.0 ? 0.2</cell></row><row><cell>Condition -End</cell><cell cols="4">65.7 ? 0.2 33.8 ? 0.3 84.8 ? 0.3 36.0 ? 0.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 .</head><label>5</label><figDesc>Self-Regularisation by reversing ? to generate synthetic samples does not benefit generalisation. ? 0.1 31.0 ? 0.1 83.4 ? 0.1 32.8 ? 0.1 Generate &amp; Match 63.5 ? 0.2 31.6 ? 0.2 83.9 ? 0.4 35.7 ? 0.3</figDesc><table><row><cell>BENCHMARKS?</cell><cell cols="2">CUB200-2011</cell><cell cols="2">CARS196</cell></row><row><cell>APPROACH ?</cell><cell>R@1</cell><cell>mAP @1000</cell><cell>R@1</cell><cell>mAP @1000</cell></row><row><cell>PA + NIR</cell><cell cols="4">66.0 ? 0.3 34.2 ? 0.2 85.2 ? 0.2 36.4 ? 0.2</cell></row><row><cell>Generate</cell><cell cols="4">64.8 ? 0.4 33.0 ? 0.3 84.6 ? 0.6 35.9 ? 0.3</cell></row><row><cell>Reverse Match</cell><cell>63.2</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work has been partially funded by the ERC (853489 -DEXIM) and by the DFG (2064/1 -Project number 390727645). We thank the International Max Planck Research School for Intelligent Systems (IMPRS-IS) for supporting Karsten Roth. Karsten Roth further acknowledges his membership in the European Laboratory for Learning and Intelligent Systems (ELLIS) PhD program.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Conditional invertible neural networks for diverse image-to-image translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Lynton Ardizzone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Kruse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niels</forename><surname>L?th</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Bracher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ullrich</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>K?the</surname></persName>
		</author>
		<idno>abs/2105.02104, 2021. 4</idno>
		<imprint>
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Conditional invertible neural networks for guided image generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lynton</forename><surname>Ardizzone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>L?th</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Kruse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ullrich</forename><surname>K?the</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Unsupervised learning by predicting noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="517" to="526" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Rethinking zero-shot video classification: End-to-end training for realistic applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Biagio</forename><surname>Brattoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Tighe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fedor</forename><surname>Zhdanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krzysztof</forename><surname>Chalupka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">Everest</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">ImageNet: A Large-Scale Hierarchical Image Database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Arcface: Additive angular margin loss for deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4685" to="4694" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">NICE: non-linear independent components estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations</title>
		<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-05-07" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note>Workshop Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Density estimation using real NVP</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jascha</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">5th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Toulon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-04-24" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings. OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Stochastic image-to-video synthesis using cinns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Dorkenwald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Milbich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Blattmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robin</forename><surname>Rombach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantinos</forename><forename type="middle">G</forename><surname>Derpanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bjorn</forename><surname>Ommer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="3742" to="3753" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep adversarial metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yueqi</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenzhao</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xudong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Is fairness only metric deep? evaluating and addressing subgroup gaps in deep metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalie</forename><surname>Dullerud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karsten</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kimia</forename><surname>Hamidieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marzyeh</forename><surname>Ghassemi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<biblScope unit="volume">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep metric learning with hierarchical triplet loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weifeng</forename><surname>Ge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="269" to="285" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Von mises-fisher clustering models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siddharth</forename><surname>Gopal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<idno>PMLR. 3</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Machine Learning</title>
		<editor>Eric P. Xing and Tony Jebara</editor>
		<meeting>the 31st International Conference on Machine Learning<address><addrLine>Bejing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-06" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="22" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Dimensionality reduction by learning an invariant mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raia</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Smart mining for deep metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Harwood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gustavo</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Carneiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Drummond</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2821" to="2829" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">St?phane Gentric, and Liming Chen. von mises-fisher mixture model-based deep learning: Application to face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Md</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Hasnat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Bohn?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Milgram</surname></persName>
		</author>
		<idno>abs/1706.04264</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Discriminative deep metric learning for face verification in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2014 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Metric learning with horde: High-order regularizer for deep embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Jacob</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Picard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aymeric</forename><surname>Histace</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Rankmi: A mutual information maximizing ranking loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mete</forename><surname>Kemertas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leila</forename><surname>Pishdad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantinos</forename><forename type="middle">G</forename><surname>Derpanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Afsaneh</forename><surname>Fazly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prannay</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Teterwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Sarna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Maschinot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ce</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note>Supervised contrastive learning</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Proxy anchor loss for deep metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungyeon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongwon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minsu</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suha</forename><surname>Kwak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Embedding transfer with label relaxation for improved metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungyeon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongwon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minsu</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suha</forename><surname>Kwak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2021-06" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Attention-based ensemble for deep metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wonsik</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhavya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kunal</forename><surname>Chawla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungmin</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keunjoo</forename><surname>Kwon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations</title>
		<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-05-07" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Glow: Generative flow with invertible 1x1 convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Durk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dhariwal</surname></persName>
		</author>
		<editor>S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R</editor>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Garnett</surname></persName>
		</author>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning with memory-based virtual classes for deep metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byungsoo</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geonmo</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han-Gyu</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2021-10" />
			<biblScope unit="page" from="11792" to="11801" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Normalizing flows: An introduction and review of current methods. 2019. cite arxiv:1908.09257Comment: Updated version, currently under review in a journal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Kobyzev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Prince</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><forename type="middle">A</forename><surname>Brubaker</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">3d object representations for fine-grained categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision Workshops</title>
		<meeting>the IEEE International Conference on Computer Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deep variational metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xudong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yueqi</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiyuan</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Sphereface: Deep hypersphere embedding for face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiyang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yandong</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiding</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhiksha</forename><surname>Raj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Torchvision the machine-vision package of torch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S?bastien</forename><surname>Marcel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Rodriguez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th ACM International Conference on Multimedia, MM &apos;10</title>
		<meeting>the 18th ACM International Conference on Multimedia, MM &apos;10<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1485" to="1488" />
		</imprint>
	</monogr>
	<note>Association for Computing Machinery</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Diva: Diverse visual feature aggregation for deep metric learning. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Milbich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karsten</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Homanga</forename><surname>Bharadhwaj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samarth</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bj?rn</forename><surname>Ommer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><forename type="middle">Paul</forename><surname>Cohen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Sharing matters for generalization in deep metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Milbich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Brattoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ommer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Characterizing generalization under out-of-distribution shifts in deep metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Milbich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karsten</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samarth</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ludwig</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marzyeh</forename><surname>Ghassemi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bj?rn</forename><surname>Ommer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>A. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman Vaughan</editor>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Self-supervised learning of pretext-invariant representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
			</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Seattle, WA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="6706" to="6716" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">No fuss distance metric learning using proxies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yair</forename><surname>Movshovitz-Attias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="360" to="368" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">A metric learning reality check. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Musgrave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ser-Nam</forename><surname>Lim</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Deep metric learning via lifted structured feature embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyun Oh</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Masked autoregressive flow for density estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papamakarios</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theo</forename><surname>Pavlakou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iain</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><forename type="middle">I</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><forename type="middle">V</forename><surname>Luxburg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vishwanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Garnett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Softtriple loss: Deep metric learning without triplet sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baigui</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juhua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rong</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Variational inference with normalizing flows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><surname>Jimenez Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shakir</forename><surname>Mohamed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on International Conference on Machine Learning</title>
		<meeting>the 32nd International Conference on International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="1530" to="1538" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Networkto-network translation with conditional invertible neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robin</forename><surname>Rombach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Esser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bjorn</forename><surname>Ommer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Mic: Mining interclass characteristics for improved metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karsten</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Biagio</forename><surname>Brattoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bjorn</forename><surname>Ommer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Pads: Policy-adapted sampling for visual similarity learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karsten</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Milbich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bjorn</forename><surname>Ommer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Simultaneous similarity-based self-distillation for deep metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karsten</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Milbich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bjorn</forename><surname>Ommer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><forename type="middle">Paul</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marzyeh</forename><surname>Ghassemi</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th International Conference on Machine Learning</title>
		<editor>Marina Meila and Tong Zhang</editor>
		<meeting>the 38th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2021-07" />
			<biblScope unit="volume">139</biblScope>
			<biblScope unit="page" from="18" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Revisiting training strategies and generalization performance in deep metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karsten</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Milbich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samarth</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prateek</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bjorn</forename><surname>Ommer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><forename type="middle">Paul</forename><surname>Cohen</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th International Conference on Machine Learning</title>
		<editor>Hal Daum? III and Aarti Singh</editor>
		<meeting>the 37th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">119</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Same same but differnet: Semi-supervised defect detection with normalizing flows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Rudolph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Wandt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bodo</forename><surname>Rosenhahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="1907" to="1916" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Divide and conquer the embedding space for metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Artsiom</forename><surname>Sanakoyeu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vadim</forename><surname>Tschernezki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uta</forename><surname>Buchler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bjorn</forename><surname>Ommer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Facenet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="815" to="823" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Learning intra-batch connections for deep metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenny</forename><forename type="middle">Denise</forename><surname>Seidenschwarz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ismail</forename><surname>Elezi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Leal-Taix?</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th International Conference on Machine Learning</title>
		<editor>Marina Meila and Tong Zhang</editor>
		<meeting>the 38th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2021-07" />
			<biblScope unit="volume">139</biblScope>
			<biblScope unit="page" from="18" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Improved deep metric learning with multiclass n-pair loss objective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihyuk</forename><surname>Sohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1857" to="1865" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Circle loss: A unified perspective of pair similarity optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changmao</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongdao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Proxynca++: Revisiting and revitalizing proxy neighborhood component analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terrance</forename><surname>Eu Wern Teh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><forename type="middle">W</forename><surname>Devries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2020 -16th European Conference</title>
		<editor>Andrea Vedaldi, Horst Bischof, Thomas Brox, and Jan-Michael Frahm</editor>
		<meeting><address><addrLine>Glasgow, UK</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005" />
			<biblScope unit="volume">12369</biblScope>
			<biblScope unit="page" from="448" to="464" />
		</imprint>
	</monogr>
	<note>Proceedings, Part XXIV</note>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">The caltech-ucsd birds-200-2011 dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<idno>CNS-TR-2011-001</idno>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">6</biblScope>
		</imprint>
		<respStmt>
			<orgName>California Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Normface: L?sub?2?/sub? hypersphere embedding for face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">Loddon</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM International Conference on Multimedia, MM &apos;17</title>
		<meeting>the 25th ACM International Conference on Multimedia, MM &apos;17<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1041" to="1049" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Cosface: Large margin cosine loss for deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yitong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dihong</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingchao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Understanding contrastive representation learning through alignment and uniformity on the hypersphere</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tongzhou</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.10242</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Multi-similarity loss with general pair weighting for deep metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xintong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weilin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dengke</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">R</forename><surname>Scott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Pytorch image models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Wightman</surname></persName>
		</author>
		<ptr target="https://github.com/rwightman/pytorch-image-models" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Sampling matters in deep embedding learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chao-Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">J</forename><surname>Manmatha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Krahenbuhl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2840" to="2848" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Improved embeddings with easy positive triplet mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Xuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abby</forename><surname>Stylianou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Pless</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)</meeting>
		<imprint>
			<date type="published" when="2020-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Deep metric learning with spherical embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dingyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongfei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS, 2020</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Hardness-aware deep metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenzhao</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaodong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Deep compositional metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenzhao</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengkun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2021-06" />
			<biblScope unit="page" from="9320" to="9329" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Deep relational metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenzhao</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Borui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2021-10" />
			<biblScope unit="page" from="12065" to="12074" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Fewer is more: A deep graph metric learning perspective using fewer proxies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuehua</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muli</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Contrastive learning inverts the data generating process</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roland</forename><forename type="middle">S</forename><surname>Zimmermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yash</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steffen</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Bethge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wieland</forename><surname>Brendel</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th International Conference on Machine Learning</title>
		<editor>Marina Meila and Tong Zhang</editor>
		<meeting>the 38th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2021-07" />
			<biblScope unit="volume">139</biblScope>
			<biblScope unit="page" from="18" to="24" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
