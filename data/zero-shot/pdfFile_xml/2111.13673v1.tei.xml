<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Mask Transfiner for High-Quality Instance Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Ke</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Kuaishou Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Danelljan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Kuaishou Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xia</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Kuaishou Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Wing</forename><surname>Tai</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Kuaishou Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi-Keung</forename><surname>Tang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Kuaishou Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Kuaishou Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eth</forename><surname>Z?rich</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Kuaishou Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hkust</forename></persName>
							<affiliation key="aff0">
								<orgName type="department">Kuaishou Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Mask Transfiner for High-Quality Instance Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T03:59+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Two-stage and query-based instance segmentation methods have achieved remarkable results. However, their segmented masks are still very coarse. In this paper, we present Mask Transfiner for high-quality and efficient instance segmentation. Instead of operating on regular dense tensors, our Mask Transfiner decomposes and represents the image regions as a quadtree. Our transformer-based approach only processes detected error-prone tree nodes and selfcorrects their errors in parallel. While these sparse pixels only constitute a small proportion of the total number, they are critical to the final mask quality. This allows Mask Transfiner to predict highly accurate instance masks, at a low computational cost. Extensive experiments demonstrate that Mask Transfiner outperforms current instance segmentation methods on three popular benchmarks, significantly improving both two-stage and query-based frameworks by a large margin of +3.0 mask AP on COCO and BDD100K, and +6.6 boundary AP on Cityscapes. Our code and trained models will be available at</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Advancements in image instance segmentation has largely been driven by the developments of powerful object detection paradigms. Approaches based on Mask R-CNN <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b31">32]</ref> and more recently DETR <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b20">21]</ref> have achieved ever increasing performance on, for instance, the COCO challenge <ref type="bibr" target="#b30">[31]</ref>. While these methods excel in detection and localization of objects, the problem of efficiently predicting highly accurate segmentation masks has so far remained elusive.</p><p>As shown in <ref type="figure">Figure 3</ref>, there is still a significant gap between the bounding box and segmentation performance of the recent state-of-the-art methods, especially for the recent query-based methods. This strongly indicates that improvements in mask quality has not kept pace with the advancements detection capability. In <ref type="figure">Figure 2</ref>, the predicted masks of previous methods are very coarse, most often oversmoothing object boundaries. In fact, efficient and accurate mask prediction is highly challenging, due to the need for  <ref type="figure">Figure 1</ref>. We propose Mask Transfiner for high-quality instance segmentation. It first builds a quadtree based on the sparse incoherent regions on the RoI pyramid and then jointly refines all tree nodes using the refinement transformer with quadtree attention.</p><p>high-resolution deep features, which demands large computational and memory costs <ref type="bibr" target="#b37">[38]</ref>.</p><p>To address these issues, we propose Mask Transfiner, an efficient transformer-based approach for high-quality instance segmentation. In <ref type="figure">Figure 1</ref>, our approach first identifies error-prone regions, which are mostly strewn along object boundaries or in high-frequency regions. To this end, our network learns to detect incoherent regions, defined by the loss of information when downsampling mask itself. These incoherent pixels are sparsely located, consisting only of a small portion of the total pixels. However, as they are shown to be critical to the final segmentation performance, it allows us to only process small parts of the high-resolution feature maps in the refinement process. Thus, we build a hierarchical quadtree <ref type="bibr" target="#b15">[16]</ref> to represent and process the incoherent image pixels at multiple scales.</p><p>To refine the mask labels of the incoherent quadtree nodes, we design an refinement network based on the transformer instead of standard convolutional networks because they require operating on uniform grids. Our transformer has three modules: node encoder, sequence encoder and pixel decoder. The node encoder first enriches the feature embedding for each incoherent point. The sequence encoder then takes these encoded feature vectors across multiple quadtree levels as input queries. Finally, the pixel decoder predicts their corresponding mask labels. Comparing Mask R-CNN BMask R-CNN SOLQ Mask Transfiner (Ours) PointRend <ref type="figure">Figure 2</ref>. Instance Segmentation on COCO <ref type="bibr" target="#b30">[31]</ref> validation set by a) Mask R-CNN <ref type="bibr" target="#b18">[19]</ref>, b) BMask R-CNN <ref type="bibr" target="#b11">[12]</ref>, c) SOLQ <ref type="bibr" target="#b13">[14]</ref>, d) PointRend <ref type="bibr" target="#b24">[25]</ref>, g) Mask Transfiner (Ours) using R50-FPN as backbone, where Mask Transfiner produces significantly more detailed results at high-frequency image regions by replacing Mask R-CNN's default mask head. Zoom in for better view.   <ref type="figure">Figure 3</ref>. The performance gap between object detection and segmentation for instance segmentation models on COCO test-dev set using R50-FPN as backbone. Detailed comparisons are in <ref type="table">Table 9</ref>.</p><p>to MLP <ref type="bibr" target="#b24">[25]</ref>, the sequential representation and multi-head attention enables Mask Transfiner to flexibly takes as input sparse feature points across levels in parallel, models their pixel-wise relations, and then propagates information among them even in a long distance range. We extensively analyze our approach on COCO, Cityscapes and BDD100K benchmarks, where quantitative and qualitative results show that Mask Transfiner not only outperforms existing two-stage and query-based methods, but also is efficient in computation and memory cost compared to standard transformer usages. We establish a new state-of-the-art result on COCO test-dev of 41.6 AP Mask using ResNet-50, outperforming most recent SOLQ <ref type="bibr" target="#b13">[14]</ref> and QueryInst <ref type="bibr" target="#b14">[15]</ref> by a significant margin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Instance Segmentation Two-stage instance segmentation methods <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b28">29]</ref> first detects bounding boxes and then performing segmentation in each RoI region. Mask R-CNN <ref type="bibr" target="#b18">[19]</ref> extends Faster R-CNN <ref type="bibr" target="#b33">[34]</ref> with an FCN branch. The follow-up works <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b32">33]</ref> also contribute to the family of Mask R-CNN models. One-stage methods <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b26">27]</ref> and kernel-based method <ref type="bibr" target="#b47">[48]</ref>, such as PolarMask <ref type="bibr" target="#b43">[44]</ref>, YOLOACT <ref type="bibr" target="#b0">[1]</ref>, and SOLO <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b40">41]</ref> remove the proposal generation and feature re-pooling steps, achieving comparable results with higher efficiency.</p><p>Query-based instance segmentation methods <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b41">42]</ref>, which are inspired by DETR <ref type="bibr" target="#b3">[4]</ref>, have emerged very recently by treating segmentation as a set prediction problem. These methods use queries to represent the interested objects and jointly perform classification, detection and mask regression on them. In <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b20">21]</ref>, the object masks are compressed as encoding vectors using DCT or PCA algorithms, while QueryInst <ref type="bibr" target="#b14">[15]</ref> adopts dynamic mask heads with mask information flow. However, the large gaps between the detection and segmentation performance in <ref type="figure">Figure</ref> 3 reveals that the mask quality produced by these querybased methods are still unsatisfactory. In contrast to the above methods, Mask Transfiner is targeted for high-quality instance segmentation. In our efficient transformer the input queries are incoherent pixels nodes, instead of representing the objects. Our method is applicable to and effective in both the two-stage and query-based frameworks. Refinement for Instance Segmentation Most existing works on instance segmentation refinement rely on specially designed convolutional networks <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b46">47]</ref> or MLPs <ref type="bibr" target="#b24">[25]</ref>. PointRend <ref type="bibr" target="#b24">[25]</ref> samples feature points with low-confidence scores and refines their labels with a shared MLP, where the selected points are determined by the coarse predictions of the Mask R-CNN. RefineMask <ref type="bibr" target="#b46">[47]</ref> incorporates fine-grained features with an additional semantic head as the guidance. The post-processing method BPR <ref type="bibr" target="#b35">[36]</ref> crops boundary patches of images and initial masks as input and use <ref type="bibr" target="#b37">[38]</ref> for segmentation. Notably some methods <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b45">46]</ref> focus on refining semantic segmentation details. However, it is much more challenging for instance segmentation due to the more complex segmentation setting, with varying number of objects per image and the requirement of delineating similar and overlapping objects.</p><p>Compared to these refinement methods, Mask Transfiner is an end-to-end instance segmentation method, using a transformer for correcting errors. The regions to be refined are predicted using a lightweight FCN, instead of non-deterministic sampling based on mask scores <ref type="bibr" target="#b24">[25]</ref>. Different from the MLP in <ref type="bibr" target="#b24">[25]</ref>, the sequential and hierarchical input representation enables Mask Transfiner to efficiently take non-local sparse feature points as input queries, where the strong global processing of transformers is a natural fit for our quadtree structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Mask Transfiner</head><p>We propose an approach to efficiently tackle high-quality instance segmentation. The overall architecture of Mask Transfiner is depicted in <ref type="figure">Figure 5</ref>. From the base object detection network, e.g. Mask R-CNN <ref type="bibr" target="#b18">[19]</ref>, we employ a multiscale deep feature pyramid. The object detection head then  predicts bounding boxes as instance proposals. This component also generates a coarse initial mask prediction at low resolution. Given this input data, our aim is to predict highly accurate instance segmentation masks.</p><p>Since much of the segmentation errors are attributed to the loss of spatial resolution, we first define such incoherent regions and analyze their properties in Section 3.1. To identify and refine incoherent regions in multiple scales, we employ a quadtree, discussed in Section 3.2. The lightweight incoherent region detector takes as input the coarse initial mask alongside the multi-scale features, and predicts the incoherent regions for each scale in a cascaded manner. This allows ours Mask Transfiner to save huge computational and memory burdens, because only a small part of the highresolution image features are processed by the refinement network itself. Our refinement transformer, detailed in Section 3.3, operates in the detected incoherent regions. Since it operates on feature points on the constructed quadtree, and not in a uniform grid, we design a transformer architecture which jointly processes all incoherent nodes in all levels of the quadtree. Finally, we present the training strategy of Mask Transfiner along with the implementation details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Incoherent Regions</head><p>Much of the segmentation errors produced by existing instance segmentation methods <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b18">19]</ref> are due to the loss of spatial resolution, such as the mask downsampling operations, small RoI pooling size, and coefficients compression <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b20">21]</ref>, where mask prediction itself is performed at a coarse feature scale. Despite its efficiency, low spatial resolution makes it challenging to predict accurate object boundaries, due to the loss of high-frequency details. In this section, we first define incoherent regions, where mask information is lost due to reduced spatial resolution. Then, by analyzing their properties, we observe that a large portion of the errors are indeed located in these regions. Definition of Incoherent Regions To identify incoherent regions, we simulate the loss of information due to downsampling in the network by also downsampling the mask itself. Specifically, information is lost in regions where the mask cannot be correctly reconstructed by a subsequent upsampling step, as illustrated in <ref type="figure" target="#fig_1">Figure 4</ref>. Formally, let M l be a binary ground-truth instance mask of an object at scale level l. The resolution at each scale level differs by a factor of 2, where l = 0 is the finest and l = L is the coarsest scale. We denote 2? nearest neighbor down and upsampling by S ? and S ? respectively. The incoherent region at scale l is then the binary mask achieved as,</p><formula xml:id="formula_0">D l = O ? (M l?1 ? S ? (S ? (M l?1 ))) .<label>(1)</label></formula><p>Here, ? denotes the logical 'exclusive or' operation and O ? is 2? downsampling by performing the logical 'or' operation in each 2 ? 2 neighborhood. A pixel (x, y) is thus incoherent D l (x, y) = 1 if the original mask value M l?1 differs from its reconstruction in at least one pixel in the finer scale level. Intuitively, incoherent regions are mostly strewn along object instance boundaries or high-frequency regions, consisting of points with missing or extra predicted wrong labels by coarse masks. We provide the visualizations of them in <ref type="figure">Figure 5</ref> and Supp. file, which are sparsely and non-contiguously distributed on a typical image. Properties of Incoherent Regions In <ref type="table" target="#tab_3">Table 1</ref>, we provide an analysis of the incoherent regions defined above.</p><p>It shows that a large portion of prediction errors are concentrated in these incoherent regions, occupying 43% of all wrongly predicted pixels, while only taking 14% to the corresponding bounding box areas. The accuracy of the coarse mask prediction in incoherent regions is 56%. By fixing the bounding boxes detector, we conduct an oracle study to fill all these incoherent regions for each object with ground truth labels, while leaving the remaining parts as initial mask predictions. Compared to using initial mask predictions in the incoherent regions, the performance surges from 35.5 AP to 51.0 AP, indeed justifying they are critical for improving final performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Quadtree for Mask Refinement</head><p>In this section, we describe our approach for detecting and refining incoherent regions in the image. Our approach is based on the idea of iteratively detecting and dividing the incoherent regions in each feature scale. By only splitting the identified incoherent pixels for further refinement, our approach efficiently processes high-resolution features by only focusing on the important regions. To formalize our approach, we employ a quadtree structure to first identify incoherent regions across scales. We then predict the refined segmentation labels for all incoherent nodes in the quadtree, using our network detailed in Section 3.3. Finally, our quadtree is employed to fuse the new predictions from multiple scales by propagating the corrected mask probabilities from coarse to finer scales. Detection of Incoherent Regions The right part of <ref type="figure">Figure 5</ref> depicts the design of our lightweight module to efficiently detect incoherent regions on a multi-scale feature pyramid. Following a cascaded design, we first concatenate the smallest features and coarse object mask predictions as input, and use a simple fully convolutional network (four 3?3 Convs) followed by a binary classifier to predict the coarsest incoherence masks. Then, the detected lower-resolution masks are upsampled and fused with the larger-resolution feature in neighboring level to guide the finer incoherence predictions, where only single 1?1 convolution layer is employed. During training, we enforce the groundtruth incoherent points in lower-level generated by Eq. 1 within the coverage of their parent points in higher-level feature map. Quadtree Definition and Construction We define a point quadtree for decomposing the detected incoherent regions. Our structure is illustrated in <ref type="figure">Figure 5</ref>, where one yellow point in higher-level of FPN feature (such as feature resolution 28 ?28) has four quadrant points in its neighboring lower-level FPN feature map (such as resolution 56?56). These are all feature points but with different granularities because they are on different pyramid levels. In contrast to the conventional quadtree 'cells' used in computer graphics, where a quadtree 'cell' can have multiple points, the subdivision unit for our point quadtree is always on a single point, with the division of points decided by the detected incoherent values and the threshold for the binary classifier.</p><p>Based on the detected incoherent points, we construct a multi-level hierarchical quadtree, beginning from using the detected points in the highest-level feature map as root nodes. These root nodes are selected for subdividing to their four quadrants on the lower-level feature map, with larger resolution and more local details. Note that at the fine level, only the quadrant points detected as incoherent could make a further break down and the expansion of incoherent tree nodes is restricted in regions corresponding to the incoherent predictions at the previous coarse level. Quadtree Refinement We refine the mask predictions of the incoherent nodes of the quadtree using a transformerbased architecture. Our design is described in Sec. 3.3. It directly operates on the nodes of the quadtree, jointly providing refined mask probabilities at each incoherent node. Quadtree Propagation Given the refined mask predictions, we design a hierarchical mask propagation scheme that exploits our quadtree structure. Given the initial coarse masks predictions in low-resolution, Mask Transfiner first corrects the points labels belong to the root level of the quadtree, and then propagates these corrected point labels to their corresponding four quadrants in neighboring finer level by nearest neighbor interpolation. The process of labels correction is efficiently conducted on the incoherent nodes in a levelwise manner until reaching the finest quadtree level. Comparing to only correcting the labels of finest leaf nodes on the quadtree, it enlarges the refinement areas with negligible cost by propagating refinement labeled to leaf nodes of the intermediate tree levels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Mask Transfiner Architecture</head><p>In this section, we describe the architecture of the refinement network, which takes as input the incoherent points on the built quadtree (Section 3.2) for final segmentation refinement. These points are sparsely distributed along the high-frequency regions across levels and not spatially contiguous. Thus, standard convolutional networks operating on uniform grids are not suitable. Instead, we design a refinement transformer, Mask Transfiner, that corrects the predictions of all incoherent quadtree nodes in parallel.</p><p>Accurately segmenting ambiguous points requires both fine-grained deep features and coarse semantic information. The network therefore needs strong modeling power to sufficiently relate points and their surrounding context, including both spatial and cross-level neighboring points. Thus, a transformer, which can take sequential input and perform powerful local and non-local reasoning through the multi-head attention layers, is a natural choice for our Mask Transfiner design. Compared to the MLP in <ref type="bibr" target="#b24">[25]</ref>, the strong global processing of transformers is a natural fit for our quadtree structure. It benefits the effective fusion of the multi-level feature points information with different granularities and the explicit modeling of pairwise point relations. <ref type="figure">Figure 5</ref> shows the overall architecture of our Mask Transfiner. Based on the hierarchical FPN <ref type="bibr" target="#b29">[30]</ref>, instance segmentation is tackled in a multi-level and coarse-to-fine manner. Instead of using single-level FPN feature for each object <ref type="bibr" target="#b18">[19]</ref>, Mask Transfiner takes as input sequence the sparsely detected feature points in incoherent image regions across the RoI feature pyramid levels, and outputs the corresponding segmentation labels. RoI Feature Pyramid Given an input image, the CNN backbone network equipped with FPN first extracts hierarchical feature maps for downstream processing, where we utilize feature levels from P 2 to P 5 . The base object detector <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b18">19]</ref>   <ref type="figure">Figure 5</ref>. The framework of Mask Transfiner. On the point quadtree, yellow point grids denote detected incoherent regions requiring further subdivision to four quadrants. The incoherent query sequence is composed of points across three levels of the quadtree for joint refinement. The encoder of Transfiner consists of node encoder and sequence encoder, while the pixel decoder is on top of each self-attended query pixel and output their final labels. The incoherence detector is detailed in the right part of the figure with detections on multi-level incoherent regions (Yellow). The higher-resolution detection is under the guidance of the predicted incoherent mask up-sampled from lower level. level features contain more contextual and semantic information, while the finer levels resolve more local details. Input Node Sequence Given the quadtree discussed in Section 3.2 along with the associated FPN features for each node, we construct the input sequence for our transformerbased architecture. The sequence consists of all incoherent nodes from all three levels of the quadtree. The resulting sequence thus has a size of C ? N , where N is the total number of nodes and C is the feature channel dimension. Notably, N HW due to the high degree of sparsity. Moreover, the ordering of the sequence does not matter due to the permutation invariance of transformer. In contrast to standard transformer encoder, the encoder of Transfiner has two parts: the node encoder and the sequence encoder. Node Encoder To enrich the incoherent points feature, the node encoder of Mask Transfiner encodes each quadtree node using the following four different information cues: 1) The fined-grained features extracted from corresponding location and level of the FPN pyramid.</p><p>2) The initial coarse mask prediction from the base detector provides region-specific and semantic information.</p><p>3) The relative positional encoding in each RoI encapsulates spatial distances and relations between nodes, capturing important local dependence and correlations. 4) The surrounding context for each node captures local details to enrich the information. For each node, we use features extracted from the 3?3 neighborhood, compressed by a fully connected layer. Intuitively, this helps in localizing edges and boundaries, as well as capturing the local shape of the object. As illustrated in <ref type="figure">Figure 5</ref>, the fine-grained features, coarse segmentation cues and context features are first concatenated and fused by a FC layer to original feature dimension. The positional embedding is then added to the resulting feature vector. Sequence Encoder and Pixel Decoder Then, the sequence transformer encoder of Transfiner jointly processes the encoded nodes from all levels in the quadtree. The transformer thus performs both global spatial and inter-scale reasoning. Each sequence encoder layer has a standard transformer structure, formed by a multi-head self-attention module and a fully connected feed forward network (FFN). To equip the incoherent points sequence with adequate positive and negative references, we also use all feature points from the coarsest FPN level with small size 14?14. Different from the standard transformer decoder <ref type="bibr" target="#b3">[4]</ref> with deep attention layers, the pixel decoder in Mask Transfiner is a small twolayer MLP, which decodes the output query for each node in the tree, in order to predict the final mask labels. Training and inference Based on the constructed quadtree, we develop flexible and adaptive training and inference schemes for Mask Transfiner, where all detected incoherent nodes across quadtree levels are formed into a sequence for parallel prediction. During inference, to obtain final object masks, Mask Transfiner follows the quadtree propagation scheme (Section 3.2) after obtaining the refined labels for incoherent nodes. During training, the whole Mask Transfiner framework can be trained in an end-to-end manner. We employ a multi-task loss,</p><formula xml:id="formula_1">L = ? 1 L Detect + ? 2 L Coarse + ? 3 L Refine + ? 4 L Inc . (2)</formula><p>Here, L Refine denotes the refinement with L1 loss between the predicted labels for incoherent nodes and their groundtruth labels. A Binary Cross Entropy loss L Inc is for detecting incoherent regions. The detection loss L Detect in-cludes the localization and classification losses from the base detector, e.g. Faster R-CNN <ref type="bibr" target="#b33">[34]</ref> or DETR detector. Finally, L Coarse represents the loss for the initial coarse segmentation prediction used by <ref type="bibr" target="#b18">[19]</ref>. ? {1,2,3,4} are hyperparameter weights {1.0, 1.0, 1.0, 0.5}. Implementation Details Mask Transfiner is implemented on both the two-stage detector Faster R-CNN <ref type="bibr" target="#b33">[34]</ref> and query-based detector <ref type="bibr" target="#b3">[4]</ref>. We design a 3-level quadtree and use the hyper-parameters and training schedules of Mask R-CNN implemented in Detectron2 <ref type="bibr" target="#b42">[43]</ref> for the backbone and coarse mask head. The Mask Transfiner encoder consists of three standard transformer layers. Each layer has four attention heads with feature dimension at 256. In our ablation study, R-50-FPN <ref type="bibr" target="#b19">[20]</ref> and Faster R-CNN with 1? learning schedule are adopted. For COCO leaderboard comparison, we adopt the scale-jitter with shorter image side randomly sampled from [640, 800], following training schedules in <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b26">27]</ref>. More details are in the Supp. file.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experimental Setup</head><p>COCO We perform experiments on COCO dataset <ref type="bibr" target="#b30">[31]</ref>, where we train our networks on 2017train and evaluate our results on both the 2017val and 2017test-dev. We employ the standard AP metrics and the recently proposed boundary IoU metrics <ref type="bibr" target="#b9">[10]</ref>. Notably, AP B for boundary IoU is a measure focusing on boundary quality. Following <ref type="bibr" target="#b24">[25]</ref>, we also report AP , which evaluates the val set of COCO with significantly higher-quality LVIS annotations <ref type="bibr" target="#b17">[18]</ref> that can better reveal improvements in mask quality.</p><p>Cityscapes We report the results on Cityscapes <ref type="bibr" target="#b12">[13]</ref>, a highquality instance segmentation dataset containing 2975, 500, 1525 images with resolution of 2048?1024 for training, validation and test respectively. Cityscapes focus on selfdriving scenes with 8 categories (e.g., car, person, bicycle). BDD100K We further train and evaluate Mask Transfiner on the BDD100K <ref type="bibr" target="#b44">[45]</ref> instance segmentation dataset, which has 8 categories with 120K high-quality instance mask annotations. We follow the standard practice, using 7k, 1k, 2k images for training, validation and testing respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Ablation Experiments</head><p>We conduct detailed ablation studies on the COCO validation set, analyzing the impact of the proposed incoherent regions and individual components of Mask Transfiner. <ref type="table" target="#tab_3">Table 1</ref> presents an analysis on the properties of incoherent regions described in Section 3.1. It reveals they are critical to the final segmentation performance. <ref type="table">Table 2</ref> presents analyzes the effectiveness of the detected incoherent regions by replacing the refinement regions with full RoIs or detected object boundary regions. Due to memory limitation, the full RoIs only uses output size 28?28. The comparison shows the advantage of incoherent regions, with 1.8 AP and 0.7 AP gain over the use of full RoIs and detected boundary regions respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effect of the Incoherent Regions</head><p>To study the influence of incoherent regions on different pyramid levels, in <ref type="table">Table 2</ref>, we also perform ablation experiments by removing the refinement regions of the Mask Transfiner in a level-wise order. We find that all three levels are beneficial to the final performance, while L 1 contributes most with 0.8 AP increase, where L 1 denotes the root level of Mask Transfiner with the smallest feature size. Ablation on the Incoherent Regions Detector We evaluate the performance of the light-weight incoherent region detector by computing its recall and accuracy rates. In <ref type="table">Table 3</ref>, with the guidance of the predicted incoherent mask up-sampled from lower level ( <ref type="figure">Figure 5</ref>), the recall rate of detected incoherent regions has an obvious improvement from 74% to 86%, and the accuracy rate also increases from 79% to 84%. Note that recall rate is more important here to cover all the error-prone regions for further refinements. Effect of Incoherent Points Encoding We analyze the effect of the four information cues in the incoherent points encoding. In <ref type="table">Table 4</ref>, comparing to only using the fine-grained feature, the coarse segmentation features with semantic information brings a gain of 0.4 point AP. The positional encoding feature has a large influence on model performance by significantly improving 2.6 points on AP and 3.5 points on AP B respectively. The positional encoding for incoherent points are crucial, because transformer architecture is permutation-invariant and the segmentation task is positionsensitive. The surrounding context feature further promotes the segmentation results from 36.8 AP to 37.3 AP by aggregating local neighboring details. Influence of Quadtree Depths In <ref type="table" target="#tab_6">Table 5</ref>, we study the influence on hierarchical refinement stages by constructing the quadtree in our Mask Transfiner with different depths.   Quadtree Attention for Node R 4 <ref type="figure">Figure 6</ref>. Qualitative results comparison between the coarse mask predictions by our baseline <ref type="bibr" target="#b19">[20]</ref> and the refinement results with various depths of the quadtree built on detected incoherent regions. The bottom row visualizes the quadtree attention weights distribution in the sparse incoherent regions for four sampled red nodes.</p><p>Depth 0 denotes the baseline using coarse head mask prediction w/o refinement steps. The output size grows twice larger than its preceding stage. By varying the output sizes from 28?28 to 224?224, the mask AP increases from 38.4 to 40.7 with increased tree depth. This reveals that models with more levels and larger output sizes for an object indeed brings more gain to segmentation performance. The large objects benefit most from the increasing sizes with an improvement of 2.8 point in AP L . We further find that the performance saturates when the output size is larger than 112?112, while the 3-stage Transfiner also has a lower computational cost and runs at 7.1 fps. <ref type="figure">Figure 6</ref> visualizes results with increasing quadtree depths, where masks become substantially finer detail around object boundaries. Mask Transfiner vs. MLP and CNN We compare different popular choices of the refinement networks, including the MLP and CNN structures. MLP is implemented with three hidden layers of 256 channels <ref type="bibr" target="#b24">[25]</ref>, while CNN is a FCN with four convolution layers with 3?3 kernels <ref type="bibr" target="#b18">[19]</ref>. Note that for full refinement regions, CNN and MLP are limited to the RoI size 56 ? 56 due to memory limitations, and CNN is not suitable for incoherent regions because uniform grids are required. In <ref type="table" target="#tab_7">Table 6</ref>, our Mask Transfiner outperforms the MLP by 0.9 AP, benefiting from the non-local pixel-wise relation modeling, where we use the same incoherent regions on all three quadtree levels for fair comparison. Moreover, we investigate the influence of layer depth D and width W of Mask Transfiner and find that deeper and wider attention layers only lead to minor performance change. In <ref type="figure">Figure 6</ref>, we visualize the sparse quadtree attention maps of the last sequence encoder layer of the Transfiner, focusing on a few incoherent points. The encoder already seems to distinguish between foreground instances and background, where the neighboring attended regions of point R 1 are separated by the object boundary. Efficacy of Quadtree Structure <ref type="table" target="#tab_8">Table 7</ref> compares Mask Transfiner with different attention mechanisms. Compared to pixels relation modeling using 3-layer non-local attention <ref type="bibr" target="#b38">[39]</ref> or standard transformer <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b36">37]</ref>, Mask Transfiner not only obtains higher accuracy but also is very efficient in computation and memory consumption. For example, Mask Transfiner with multi-head attention uses 3 times less memory than the non-local attention given same output size, due to the small number of incoherent pixels. Compared to standard transformer operating on full RoI regions of much smaller size 56?56, the quadtree subdivision and inference allows Mask Transfiner to produce a high-resolution 224?224 prediction using only half of the FLOPs computation. Note that the standard transformer with output size 112?112 runs out of memory in our experiments. Effect of Multi-level Joint Refinement Given incoherent nodes from the 3-level quadtree, Transfiner forms all of them into a sequence for joint refinement in single forward pass. In <ref type="table" target="#tab_9">Table 8</ref>, we compare it with separately refining the quadtree nodes on each level with multiple sequences. The performance boost of 0.6 AP shows the benefit of multiscale feature fusion and richer context in global reasoning. Effect of Quadtree Mask Propagation During inference, after Mask Transfiner has refined all incoherent points, we utilize a hierarchical coarse-to-fine mask propagation scheme along the quadtree levels to obtain the final predictions. Comparing to only correcting the labels of finest leaf nodes on the quadtree in <ref type="table" target="#tab_9">Table 8</ref>, the propagation enlarges the refinement areas and improves the performance from 36.5 AP to 37.0 AP. The propagation brings negligible computation because the new labels for the quadrant leaf (coherent) nodes in intermediate tree levels are obtained via duplicating the refined label values of their parents. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Comparison with State-of-the-art</head><p>We compare our approach with the state-of-the-art methods on the benchmarks COCO, Cityscapes and BDD100K,  <ref type="figure">Figure 7</ref>. Qualitative comparisons with instance segmentation methods Mask R-CNN <ref type="bibr" target="#b18">[19]</ref>, BMask R-CNN <ref type="bibr" target="#b11">[12]</ref>, PointRend <ref type="bibr" target="#b24">[25]</ref> and our Mask Transfiner on Cityscapes val set. Mask Transfiner produces more natural boundaries while revealing details for small parts, such as the rear mirrors of the car and the high-heeled shoes. Zoom in for better view. Refer to the supplemental file for more visual comparisons. where Mask Transfiner outperforms all existing methods without bells and whistles, demonstrating efficacy on both two-stage and query-based segmentation frameworks.</p><p>Codes and models will be released upon publication.</p><p>COCO <ref type="table">Table 9</ref> compares Mask Transfiner with state-ofthe-art instance segmentation methods on COCO dataset. Transfiner achieves consistent improvement on different backbones and object detectors, demonstrating its effectiveness by outperforming RefineMask <ref type="bibr" target="#b46">[47]</ref> and BCNet [24] by 1.3 AP and 0.9 AP using R101-FPN and Faster R-CNN, and exceeding QueryInst [15] by 1.7 AP using query-based detector <ref type="bibr" target="#b3">[4]</ref>. Note QueryInst consists of six-stage refinement in parallel with far more parameters to optimize. Besides, we find that Transfiner using Faster R-CNN and R50-FPN with much lower object detection performance still achieves comparable segmentation results with query-based methods <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b20">21]</ref> on mask AP, and over 2 points gain in boundary AP B , further validating the higher AP achieved by Transfiner is indeed contributed by the fine-grained masks.</p><p>Cityscapes The results of Cityscapes benchmark is tabulated in <ref type="table" target="#tab_3">Table 10</ref>, where Mask Transfiner achieves the best mask AP 37.6 and boundary AP B 18.0. Our approach significantly surpasses existing SOTA methods, including PointRend <ref type="bibr" target="#b24">[25]</ref> and BMask R-CNN <ref type="bibr" target="#b11">[12]</ref> by a margin of 1.3 AP B and 2.3 AP B using the same Faster R-CNN detector. Compared to our baseline Mask R-CNN <ref type="bibr" target="#b18">[19]</ref>, Transfiner greatly improves the boundary AP from 11.4 to 18.0, which shows the effectiveness of the quadtree refinement.</p><p>BDD100K <ref type="table" target="#tab_3">Table 11</ref> shows results on BDD100K dataset, where Mask Transfiner obtains the highest AP mask of 23.5 and outperforms the baseline <ref type="bibr" target="#b19">[20]</ref> by 3 points under the comparable AP Box . The significant advancements reveals the high accuracy of the predicted masks by Transfiner. Qualitative Results <ref type="figure">Figure 7</ref> shows qualitative comparisons on Cityscapes, where our Mask Transfiner produces masks with substantially higher precision and quality than previous methods <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b24">25]</ref>, especially for the hard regions, such as the small rear mirrors and high-heeled shoes. Refer to supplementary file for more visual comparisons.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We present Mask Transfiner, a new high-quality and efficient instance segmentation method. Transfiner first detects and decomposes the image regions to build a hierarchical quadtree. Then, all points on the quadtree are transformed into to a query sequence for our transformer to predict final labels. In contrast to previous segmentation methods using convolutions limited by uniform image grids, Mask Transfiner produces high-quality masks with low computation and memory cost. We validate the efficacy of Transfiner on both the two-stage and query-based segmentation frameworks, and show that Transfiner achieves large performance advantages on COCO, Cityscapes and BDD100K. A current limitation is the fully supervised training required by our Mask Transfiner as well as the competing methods. Future work will strive towards relaxing this assumption.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Appendix</head><p>We first provide more implementation and training/inference details of Mask Transfiner on three instance segmentation benchmarks (Section 6.1). Then we conduct more experimental analysis and discussion of comparison between Mask Transfiner and other methods (Section 6.2). We further present more qualitative results comparisons on COCO <ref type="bibr" target="#b30">[31]</ref>, BDD100K <ref type="bibr" target="#b44">[45]</ref> and Cityscapes <ref type="bibr" target="#b12">[13]</ref> datasets in various scenes (Section 6.3). Finally, we visualize quadtree attention weights, detected incoherent regions and segmentation results with various quadtree depths (Section 6.4), including failure cases analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">More Implementation Details</head><p>Implementation and Training/Inference Details We implement Mask Transfiner based on Detectron2 <ref type="bibr" target="#b42">[43]</ref>, where SGD is used with 0.9 momentum and 1K constant warm-up iterations. The weight decay is set to 0.0001. On the two-stage and query-based frameworks, we employ Mask Transfiner using Faster R-CNN <ref type="bibr" target="#b33">[34]</ref> and DETR <ref type="bibr" target="#b3">[4]</ref> detectors respectively while leaving the RoI pyramid construction and refinement transformer unchanged.</p><p>To make the detection on incoherent regions more robust, we adopt jittering operations along the boundaries of the ground truth incoherent regions because in our case, the recall rate of the detection to cover all the incoherent regions play a more critical role in influencing final performance. We use 0.5 as the threshold for the binary incoherence classifier. For the experiment of <ref type="table">Table 2</ref> in the paper, the boundary regions are pixels within two-pixel Euclidean distance to the detected object mask contours on all three levels of the object feature pyramid, where the object boundary detector <ref type="bibr" target="#b23">[24]</ref> is used. The coarse mask head is composed of a FCN network with four 3?3 Convs attached on the ROI feature of size 28?28.</p><p>During training, we randomly permute the order of the incoherent points for each object and select 300 of them (100 per quadtree level), so as to maintain the same sequence length for each object for batch efficiency. We adopt the horizontal flipping and scale data augmentation during training following <ref type="bibr" target="#b24">[25]</ref>.</p><p>During inference, no test-time augmentation is used. We employ a hierarchical propagation scheme based on the quadtree structure from coarse to finer scales (detailed in Section 3.1 of the paper) and the refined incoherent nodes predictions. In <ref type="figure">Figure 8</ref>, we further illustrate the mask propagation process with a simplified 3-level quadtree. The incoherent nodes number N with their refined predictions value V n are formatted in N : V n , where {1 : v1, 2 : v2, 5 : v5, 7 : v7, 8 : v8, 10 : v10, 13 : v13} are incoherent nodes numbers and prediction values pairs in our given example. We break down the mask correction and propagation into 3 steps corresponding to 3 levels of the quadtree with visualizations. Comparing to only correcting the labels of finest leaf nodes on the quadtree, it enlarges the refinement areas with negligible cost by propagating refinement labeled to leaf nodes {3, 4, 6, 9, 11, 12}. We validate the effect of quadtree mask propagation in <ref type="table" target="#tab_9">Table 8</ref> of the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>COCO:</head><p>We set 16 images per mini-batch. Following <ref type="bibr" target="#b24">[25]</ref>, our training schedule is 60k / 20k / 10k with updating learning rates 0.02 / 0.002 / 0.0002 respectively. For ablation study, our method is trained on four GPUs using ResNet-50, where we use SGD for optimization and set initial learning rate to 0.01 with total batch size 8. We train Mask Transfiner for 12 epochs (taking about 8 hours with NVIDIA RTX 2080 Ti), and decrease the learning rate by 0.1 after 8 and 11 epochs.</p><p>Cityscapes: We adopt 8 images per mini-batch and the training schedule is 18k / 6k updates at learning rates of 0.01  <ref type="figure">Figure 8</ref>. The simplified illustration of mask propagation on a 3-level quadtree during inference. Given the detected incoherent nodes and their refined predictions, in Step 1, Mask Transfiner corrects the nodes labels belonging to L1 level of the quadtree, and then propagates these corrected labels to their corresponding four quadrants in L2 level. In Step 2, the process of labels correction is efficiently conducted on the incoherent nodes in L2 and further propagating to L3. This process is recursive until reaching the finest quadtree level. / 0.001 respectively. During training, the images are resized randomly to a shorter edge from [800, 1024] pixels with a step of 32 pixels. The inference images are resized to a shorter edge size of 1024 pixels. For Cityscapes evaluation, we train the models on the fine annotations of the train set with 64 epochs following <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b24">25]</ref>. BDD100K: We use 16 images per mini-batch and and the training schedule is 22k / 4k/ 4k updates at learning rates of 0.02 / 0.002/ 0.0002 respectively. During training, the images are resized randomly to a shorter edge from [600, 720] pixels with a step of 24 pixels. During inference, the images are resized to a shorter edge size of 720 pixels. Note all compared methods are trained with the same schedules and image size settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">More Experimental Analysis</head><p>Accuracy Comparison In <ref type="table">Table 9</ref> of the main text, we compare the accuracy of Mask Transfiner with previous methods and find that Mask Transfiner achieves consistently large improvements on different backbones and object detectors. We further observe that the usage of DCN <ref type="bibr" target="#b48">[49]</ref> with Mask Transfiner can bring a surge in performance. We compare Transfiner with Mask Scoring R-CNN <ref type="bibr" target="#b21">[22]</ref>   <ref type="table">Table 9</ref> of the paper. For more comprehensive comparisons on two-stage instance segmentation methods, in <ref type="table" target="#tab_3">Table 12</ref>, we also train Mask R-CNN <ref type="bibr" target="#b18">[19]</ref>, PointRend <ref type="bibr" target="#b24">[25]</ref>, BCNet <ref type="bibr" target="#b23">[24]</ref>, Cascade Mask R-CNN <ref type="bibr" target="#b2">[3]</ref> and HTC <ref type="bibr" target="#b5">[6]</ref> with the multi-scale 3? training schedule with DCN, and submit their predictions to the evaluation server for obtaining their accuracies on the test-dev split. The performance advantages of Mask Transfiner are consistently significant, improving the baseline Mask R-CNN ? for 2.8 mask AP and outperforming PointRend by 0.9 AP.   <ref type="figure">Figure 10</ref>. Qualitative comparisons with baseline method Mask R-CNN <ref type="bibr" target="#b18">[19]</ref> and our Mask Transfiner on BDD100K <ref type="bibr" target="#b44">[45]</ref> val set. Mask Transfiner produces more correct and natural segmentation results by revealing details for high-frequency regions.  <ref type="figure">Figure 11</ref>. Qualitative comparisons with instance segmentation methods Mask R-CNN <ref type="bibr" target="#b18">[19]</ref>, BMask R-CNN <ref type="bibr" target="#b11">[12]</ref>, PointRend <ref type="bibr" target="#b24">[25]</ref> and our Mask Transfiner on Cityscapes <ref type="bibr" target="#b12">[13]</ref> val set. Mask Transfiner produces more precise and natural segmentation results, where even the small triangle-shaped traffic sign occluding the bus (3rd row) and the gap between the hand and leg (5th row) could be correctly separated. size 28?28), our Transfiner using 3-level quadtree is much faster and more accurate with higher-resolution predictions (112?112). Comparing to the baseline Mask R-CNN, although there is a drop on inference speed for about 35% due to multi-head attention modeling between hierarchical incoherent regions, the significant performance boost of 2.8 mask AP and 4 times larger output height/width are good compensation trade-offs. Note that standard transformer (3 layers and 4 attention heads in each layer) operating on uniform grids with output size 56?56 only runs at 1.4 FPS, which is much slower than our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">More Qualitative Comparisons</head><p>We provide more qualitative results comparisons on three evaluation benchmarks COCO <ref type="figure" target="#fig_3">(Figure 9</ref>), B100K ( <ref type="figure">Figure 10</ref>) and Cityscapes <ref type="figure">(Figure 11)</ref>, where our Mask Transfiner consistently produces masks with substantially higher precision and quality than previous methods <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b24">25]</ref>. Take the third case in <ref type="figure" target="#fig_3">Figure 9</ref> as an example, SOLQ and the baseline Mask R-CNN only provides very coarse mask predictions in the high-frequency regions, such as the giraffe's head and feet regions, due to their low-resolution output sizes 28?28. Although PointRend employs large output size 224?224, it still fails to delineate the thin gap between the left legs of giraffe. Note that the mask output size of Mask Transfiner only is 112?112. These segmentation errors on ambiguous regions reveal the limitation of segmenting each pixel separately only by a share MLP <ref type="bibr" target="#b24">[25]</ref> without global reasoning and hierarchical pixel-wise relations modeling.  <ref type="figure">Figure 12</ref>. Qualitative results comparison between the coarse mask predictions by our baseline <ref type="bibr" target="#b19">[20]</ref> and the refinement results of Mask Transfiner on COCO with various depths of the quadtree built on detected incoherent regions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4.">Visual Analysis</head><p>Visualization Multi-level Refinement In <ref type="figure">Figure 12</ref>, we analyze how the mask predictions evolve with increasing quadtree depths. The predicted masks become substantially finer in detail around object boundaries, which reveals that the quadtree nodes with more levels at larger output sizes for an object preserves more low-level details for finegrained segmentation.</p><p>Failure Cases We also analyze the failure cases and find one typical failure mode shown in the last row of <ref type="figure">Figure 12</ref>, where a small portion of the bird's paw is wrongly predicted as background wood due to their highly similar appearance and texture. Visualization on Quadtree Attention and Incoherent Regions In <ref type="figure">Figure 13</ref> and <ref type="figure" target="#fig_1">Figure 14</ref>, we further provide more quadtree attention visualization examples and their detected incoherent regions on RoI pyramid, where the outline of objects can be observed and the sparsity of quadtree attention is clearly shown. The quadtree nodes with higher appearance or positional similarity has larger attention weights attending between them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mask Prediction</head><p>Quadtree  <ref type="figure">Figure 13</ref>. Visualization on the quadtree attention weights distribution in the sparse incoherent regions for four sampled red nodes.</p><p>Input Image Incoherent Regions on Level 1 Incoherent Regions on Level 2 Incoherent Regions on Level 3 <ref type="figure" target="#fig_1">Figure 14</ref>. Visualization of detected incoherent regions on different levels of the constructed quadtree based on the RoI pyramid, where the incoherent nodes regions in deeper quadtree levels with larger resolution size are distributed more sparsely.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Mask R-CNN BMask R-CNN PointRend Mask Transfiner(Ours)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 .</head><label>4</label><figDesc>Illustration on incoherent regions definition by simulating mask information loss.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 9 .</head><label>9</label><figDesc>Instance Segmentation on COCO<ref type="bibr" target="#b30">[31]</ref> validation set by a) Mask R-CNN<ref type="bibr" target="#b18">[19]</ref>, b) SOLQ<ref type="bibr" target="#b13">[14]</ref>, c) PointRend<ref type="bibr" target="#b24">[25]</ref>, d) Mask Transfiner (Ours) using R50-FPN as backbone, where Mask Transfiner produces significantly more detailed results at high-frequency image regions by replacing Mask R-CNN's default mask head. Zoom in for better view.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>BMask R-CNNMask R-CNN + PointRendMask R-CNN + Mask Transfiner (Ours) Mask R-CNN</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 .</head><label>1</label><figDesc>Experimental analysis of the incoherent regions on COCO val set. Percent denotes the area ratio of incoherent regions in the object bounding boxes. RecallErr is the ratio for all wrongly predicted pixels per object. Acc is the accuracy rate for coarse mask predictions inside incoherent regions. APCoarse is measured by using coarse mask predictions for whole object regions while APGT only fills the incoherent regions with the ground truth labels.</figDesc><table><row><cell cols="4">Percent Recall Err Acc AP GT AP Coarse</cell></row><row><cell>14%</cell><cell>43%</cell><cell>56% 51.0</cell><cell>35.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>predicts bounding boxes as instance proposals. Then the RoI feature pyramid is built by extracting RoI features across three different levels {P i , P i?1 , P i?2 } of FPN with increasing square sizes {28, 56, 112}. The starting level i is computed as i = i 0 + log 2 ( ? W H/224) , where i 0 = 4, W and H are the RoI width and height. The coarsest</figDesc><table><row><cell>Box Head</cell><cell>Box Proposals</cell><cell>Coarse Mask Head</cell><cell></cell><cell></cell><cell>28?28</cell><cell>Detector Incoherence</cell><cell>Coarse Masks</cell><cell>28?28 ROI Feature</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Coarse Masks</cell><cell></cell></row><row><cell>Backbone</cell><cell></cell><cell cols="3">ROI Align</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>28?28</cell><cell></cell><cell></cell><cell cols="2">Node Encoder</cell></row><row><cell></cell><cell></cell><cell cols="3">ROI Align</cell><cell></cell><cell cols="2">Fine Coarse</cell><cell>Context</cell><cell>Up-sample</cell><cell>FCN</cell></row><row><cell></cell><cell></cell><cell></cell><cell>56?56</cell><cell></cell><cell></cell><cell>FC</cell><cell>56?56</cell></row><row><cell></cell><cell></cell><cell cols="3">ROI Align 112?112</cell><cell cols="2">Point Quadtree</cell><cell>Positional Fused Embedding</cell></row><row><cell>Input Image</cell><cell cols="2">FPN</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Embedding</cell><cell>Conv</cell></row><row><cell></cell><cell cols="2">Pixel Decoder</cell><cell cols="3">Sequence Encoder</cell><cell></cell><cell>Up-sample</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>112?112</cell></row><row><cell></cell><cell></cell><cell cols="2">FFN</cell><cell>Add &amp; Norm</cell><cell>Multi-head Attention</cell><cell></cell></row><row><cell></cell><cell>MLP</cell><cell></cell><cell></cell><cell>3?</cell><cell></cell><cell></cell></row><row><cell>Prediction</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Conv</cell></row><row><cell cols="2">Sequential Prediction</cell><cell>Encoded Sequence</cell><cell></cell><cell></cell><cell cols="3">Incoherent Query Sequence</cell><cell>Detected Incoherent Masks</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2 .Table 3 .Table 4 .</head><label>234</label><figDesc>Effect of the incoherent regions on COCO val set. AP B is evaluated Boundary IoU [10] while AP uses LVIS annotations. Full RoIs (28 ? 28) 35.5 21.4 38.3 59.5 Boundary regions 36.6 23.8 40.1 60.2 Incoherent regions 37.3 24.2 40.5 60.7 Effect of lower-level masks guidance in detecting incoherent regions on COCO val. AP and AP B are final performance. Analysis of node encoding cues on COCO val set.</figDesc><table><row><cell>Region Type</cell><cell cols="3">AP AP B AP</cell><cell>AP 50</cell></row><row><cell cols="4">Incoherent regions (w/o L 1 ) 36.5 23.5 39.8 59.7</cell></row><row><cell cols="4">Incoherent regions (w/o L 2 ) 36.8 23.8 40.2 60.1</cell></row><row><cell cols="4">Incoherent regions (w/o L 3 ) 36.7 23.6 40.0 59.9</cell></row><row><cell cols="4">Lower-level Guidance Acc Recall AP AP B</cell></row><row><cell></cell><cell>79%</cell><cell>73%</cell><cell>36.6 23.7</cell></row><row><cell></cell><cell>84%</cell><cell>86%</cell><cell>37.3 24.2</cell></row><row><cell cols="4">Fine Coarse Pos. Context AP AP B AP</cell><cell>AP 50</cell></row><row><cell></cell><cell></cell><cell cols="2">33.8 20.1 37.0 53.8</cell></row><row><cell></cell><cell></cell><cell cols="2">34.2 20.4 37.3 54.3</cell></row><row><cell></cell><cell></cell><cell cols="2">36.8 23.9 40.1 60.1</cell></row><row><cell></cell><cell></cell><cell cols="2">37.3 24.2 40.5 60.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 .</head><label>5</label><figDesc>Analysis of the quadtree depth on the COCO val using R50-FPN as backbone.</figDesc><table><row><cell cols="3">Depth Output size AP AP</cell><cell cols="2">APL APM APS FPS</cell></row><row><cell>0</cell><cell>28?28</cell><cell cols="3">35.2 37.6 50.3 37.7 17.2 12.3</cell></row><row><cell>1</cell><cell>28?28</cell><cell cols="3">35.5 38.4 50.9 38.1 17.2 10.6</cell></row><row><cell>2</cell><cell>56?56</cell><cell cols="2">36.2 39.1 51.9 38.7 17.3</cell><cell>8.9</cell></row><row><cell>3</cell><cell>112?112</cell><cell cols="2">37.3 40.5 52.9 39.5 17.5</cell><cell>7.1</cell></row><row><cell>4</cell><cell>224?224</cell><cell cols="2">37.1 40.7 53.1 39.3 17.4</cell><cell>5.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 .</head><label>6</label><figDesc>Mask Transfiner vs. MLP and CNN on COCO val set using ResNet-50-FPN.</figDesc><table><row><cell>Model</cell><cell>AP AP B AP</cell><cell>AP 50</cell></row><row><cell>CNN (full regions, 56 ? 56)</cell><cell cols="2">35.7 21.8 38.7 58.8</cell></row><row><cell>MLP (full regions, 56 ? 56)</cell><cell cols="2">36.1 23.4 39.2 59.2</cell></row><row><cell cols="3">MLP (PointRend [25], 112 ? 112) 36.2 23.1 39.1 59.0</cell></row><row><cell>MLP (incoherent regions)</cell><cell cols="2">36.4 23.7 39.7 59.8</cell></row><row><cell>Mask Transfiner (D = 3, H = 4)</cell><cell cols="2">37.3 24.2 40.5 60.7</cell></row><row><cell>Mask Transfiner (D = 3, H = 8)</cell><cell cols="2">37.1 24.1 40.2 60.8</cell></row><row><cell>Mask Transfiner (D = 6, H = 4)</cell><cell cols="2">37.4 24.4 40.6 60.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 .</head><label>7</label><figDesc>Efficacy of Transfiner compared to standard attention models on COCO val. NLA denotes non-local attention<ref type="bibr" target="#b38">[39]</ref>.</figDesc><table><row><cell>Model</cell><cell cols="4">AP FLOPs (G) Memory (M) FPS</cell></row><row><cell>NLA [39] (112?112)</cell><cell>36.3</cell><cell>24.6</cell><cell>8347</cell><cell>4.6</cell></row><row><cell>NLA [39] (224?224)</cell><cell>36.6</cell><cell>80.2</cell><cell>18091</cell><cell>2.4</cell></row><row><cell>Transformer [4] (28?28)</cell><cell>36.1</cell><cell>37.2</cell><cell>4368</cell><cell>6.9</cell></row><row><cell>Transformer [4] (56?56)</cell><cell>36.5</cell><cell>68.3</cell><cell>17359</cell><cell>2.1</cell></row><row><cell cols="2">Mask Transfiner (112?112) 37.3</cell><cell>16.8</cell><cell>2316</cell><cell>7.1</cell></row><row><cell cols="2">Mask Transfiner (224?224) 37.1</cell><cell>38.1</cell><cell>4871</cell><cell>5.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 8 .</head><label>8</label><figDesc>Effect of the multi-level joint refinement (MJR) and quadtree mask propagation (QMP) on COCO val set.</figDesc><table><row><cell>MJR QMP AP AP B AP</cell><cell>AP 50</cell></row><row><cell cols="2">36.5 23.7 39.6 59.7</cell></row><row><cell cols="2">36.9 23.9 40.2 60.2</cell></row><row><cell cols="2">37.0 24.0 40.1 60.2</cell></row><row><cell cols="2">37.3 24.2 40.5 60.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 9 .Table 11 .</head><label>911</label><figDesc>Comparison with SOTA methods on COCO test-dev and val set. All methods are trained on COCO train2017. ?: trained with DCN<ref type="bibr" target="#b48">[49]</ref>. AP denotes evaluation using LVIS<ref type="bibr" target="#b17">[18]</ref> annotation and AP B denotes using Boundary IoU<ref type="bibr" target="#b9">[10]</ref>. Performance comparison between instance segmentation methods on BDD100K val set.</figDesc><table><row><cell>Method</cell><cell></cell><cell cols="2">Backbone</cell><cell>Type</cell><cell>AP</cell><cell cols="2">AP val AP B val</cell><cell cols="3">AP Box APS APM APL</cell></row><row><cell cols="2">Mask R-CNN [19]</cell><cell cols="2">R50-FPN</cell><cell>Two-stage</cell><cell>37.5</cell><cell>38.2</cell><cell>21.2</cell><cell>41.3</cell><cell>21.1</cell><cell>39.6</cell><cell>48.3</cell></row><row><cell>PointRend [25]</cell><cell></cell><cell cols="2">R50-FPN</cell><cell>Two-stage</cell><cell>38.1</cell><cell>39.7</cell><cell>23.5</cell><cell>41.5</cell><cell>18.8</cell><cell>40.2</cell><cell>49.4</cell></row><row><cell>B-MRCNN [12]</cell><cell></cell><cell cols="2">R50-FPN</cell><cell>Two-stage</cell><cell>37.8</cell><cell>39.8</cell><cell>23.5</cell><cell>41.6</cell><cell>19.7</cell><cell>40.3</cell><cell>49.6</cell></row><row><cell>BPR [36]</cell><cell></cell><cell cols="2">R50-FPN</cell><cell>Two-stage</cell><cell>38.4</cell><cell>40.2</cell><cell>24.3</cell><cell>41.3</cell><cell>20.2</cell><cell>40.5</cell><cell>49.7</cell></row><row><cell>Mask Transfiner</cell><cell></cell><cell cols="2">R50-FPN</cell><cell>Two-stage</cell><cell>39.4</cell><cell>42.3</cell><cell>26.0</cell><cell>41.8</cell><cell>22.3</cell><cell>41.2</cell><cell>50.2</cell></row><row><cell cols="2">Mask Transfiner  ?</cell><cell cols="2">R50-FPN</cell><cell>Two-stage</cell><cell>40.5</cell><cell>43.1</cell><cell>26.8</cell><cell>43.2</cell><cell>22.8</cell><cell>42.3</cell><cell>52.5</cell></row><row><cell cols="4">Mask R-CNN [19] R101-FPN</cell><cell>Two-stage</cell><cell>38.8</cell><cell>39.3</cell><cell>23.1</cell><cell>43.1</cell><cell>21.8</cell><cell>41.4</cell><cell>50.5</cell></row><row><cell>PointRend [25]</cell><cell></cell><cell cols="2">R101-FPN</cell><cell>Two-stage</cell><cell>39.6</cell><cell>41.4</cell><cell>25.3</cell><cell>43.3</cell><cell>19.8</cell><cell>42.6</cell><cell>53.7</cell></row><row><cell cols="2">MS R-CNN  ? [22]</cell><cell cols="2">R101-FPN</cell><cell>Two-stage</cell><cell>39.6</cell><cell>41.1</cell><cell>25.0</cell><cell>44.1</cell><cell>18.9</cell><cell>42.7</cell><cell>55.1</cell></row><row><cell>HTC [6]</cell><cell></cell><cell cols="2">R101-FPN</cell><cell>Two-stage</cell><cell>39.7</cell><cell>42.5</cell><cell>25.4</cell><cell>45.9</cell><cell>21.0</cell><cell>42.2</cell><cell>53.5</cell></row><row><cell>RefineMask [47]</cell><cell></cell><cell cols="2">R101-FPN</cell><cell>Two-stage</cell><cell>39.4</cell><cell>42.3</cell><cell>26.8</cell><cell>43.8</cell><cell>21.6</cell><cell>42.0</cell><cell>53.1</cell></row><row><cell>BCNet [24]</cell><cell></cell><cell cols="2">R101-FPN</cell><cell>Two-stage</cell><cell>39.8</cell><cell>41.9</cell><cell>26.1</cell><cell>43.5</cell><cell>22.7</cell><cell>42.4</cell><cell>51.1</cell></row><row><cell>Mask Transfiner</cell><cell></cell><cell cols="2">R101-FPN</cell><cell>Two-stage</cell><cell>40.7</cell><cell>43.6</cell><cell>27.3</cell><cell>43.9</cell><cell>23.1</cell><cell>42.8</cell><cell>53.8</cell></row><row><cell cols="4">Mask Transfiner  ? R101-FPN</cell><cell>Two-stage</cell><cell>42.2</cell><cell>45.0</cell><cell>28.6</cell><cell>45.8</cell><cell>24.1</cell><cell>44.8</cell><cell>55.4</cell></row><row><cell>ISTR [21]</cell><cell></cell><cell cols="2">R50-FPN</cell><cell cols="2">Query-based 38.6</cell><cell>39.5</cell><cell>23.0</cell><cell>46.8</cell><cell>22.1</cell><cell>40.4</cell><cell>50.6</cell></row><row><cell>QueryInst [15]</cell><cell></cell><cell cols="2">R50-FPN</cell><cell cols="2">Query-based 39.9</cell><cell>42.1</cell><cell>25.1</cell><cell>44.5</cell><cell>22.9</cell><cell>41.7</cell><cell>51.9</cell></row><row><cell>SOLQ [14]</cell><cell></cell><cell cols="2">R50-FPN</cell><cell cols="2">Query-based 39.7</cell><cell>39.8</cell><cell>23.3</cell><cell>47.8</cell><cell>21.5</cell><cell>42.5</cell><cell>53.1</cell></row><row><cell cols="2">Mask Transfiner</cell><cell cols="2">R50-FPN</cell><cell cols="2">Query-based 41.6</cell><cell>45.4</cell><cell>28.2</cell><cell>46.5</cell><cell>24.2</cell><cell>44.6</cell><cell>55.2</cell></row><row><cell cols="5">Table 10. Performance comparison between two-stage instance</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">segmentation methods on Cityscapes val set using R50-FPN.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell cols="2">AP B AP B 50</cell><cell cols="2">AP AP 50</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">Mask R-CNN (Baseline) [19] 11.4 37.4 33.8 61.5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>PointRend [25]</cell><cell></cell><cell cols="3">16.7 47.2 35.9 61.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>BMask R-CNN [12]</cell><cell></cell><cell cols="3">15.7 46.2 36.2 62.6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Panoptic-DeepLab [9]</cell><cell></cell><cell cols="3">16.5 47.7 35.3 57.9</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>RefineMask [47]</cell><cell></cell><cell cols="3">17.4 49.2 37.6 63.3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Mask Transfiner (Ours)</cell><cell></cell><cell cols="3">18.0 49.8 37.9 64.1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell cols="2">Backbone</cell><cell cols="2">AP mask AP box</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">Mask R-CNN (Baseline) [19] R101-FPN</cell><cell>20.5</cell><cell>26.1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Cascade Mask R-CNN [3]</cell><cell cols="2">R101-FPN</cell><cell>19.8</cell><cell>24.7</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Mask R-CNN + DCNv2 [49]</cell><cell cols="2">R101-FPN</cell><cell>20.9</cell><cell>26.0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>HRNet [38]</cell><cell cols="2">HRNet-w32</cell><cell>22.5</cell><cell>28.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Mask Transfiner (Ours)</cell><cell cols="2">R101-FPN</cell><cell>23.6</cell><cell>26.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head></head><label></label><figDesc>trained with DCN under the same setting and training schedules. Using ResNet-101 and Faster R-CNN [34] detector, the mask AP of Mask Transfiner on COCO test-dev is 42.2, while Mask Scoring R-CNN is 39.6 in</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 12 .</head><label>12</label><figDesc>Performance comparison between two-stage instance segmentation methods on COCO test-dev set using R101-FPN. The dagger ? denotes training with DCN [49] and 3? training schedule in our implementation. HTC and Cascade Mask R-CNN use 3-stage cascade refinement with multiple object detectors and mask heads. The standard transformer with output size 112?112 runs out of memory in our experiments.Inference Speed We adopt frames per second (FPS) to evaluate the inference speed of the models. InTable 12, we benchmark all the compared two-stage methods using a Titan RTX GPU. The reported FPS is the average obtained in five runs, where each run measures the FPS of a model through 200 iterations. Compared to the Cascade Mask R-CNN and HTC with three-stage cascade refinement and multiple object detectors/mask heads (output</figDesc><table><row><cell>Method</cell><cell cols="2">Output Size AP APS APM APL FPS</cell></row><row><cell>Mask R-CNN  ? [19] (Baseline) [19]</cell><cell>28?28</cell><cell>39.4 18.6 42.8 54.5 9.6</cell></row><row><cell>Mask Scoring R-CNN  ? [22]</cell><cell>28?28</cell><cell>39.6 18.9 42.7 55.1 9.2</cell></row><row><cell>BCNet  ? [24]</cell><cell>28?28</cell><cell>41.2 23.6 43.9 52.8 8.9</cell></row><row><cell>PointRend  ? [25]</cell><cell>224?224</cell><cell>41.3 20.6 44.0 55.3 7.2</cell></row><row><cell>Cascade Mask R-CNN  ? [3]</cell><cell>28?28</cell><cell>41.5 22.1 42.6 54.2 4.8</cell></row><row><cell>HTC  ? [6]</cell><cell>28?28</cell><cell>41.7 23.3 44.2 53.8 2.1</cell></row><row><cell>Standard Transformer  ?</cell><cell>56?56</cell><cell>41.3 23.4 43.5 53.2 1.4</cell></row><row><cell>Mask Transfiner  ? (Ours: Quadtree Transformer)</cell><cell>112?112</cell><cell>42.2 24.1 44.8 55.4 6.1</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Yolact: real-time instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Bolya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fanyi</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><forename type="middle">Jae</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Cascade r-cnn: Delving into high quality object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaowei</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nuno</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Cascade r-cnn: High quality object detection and instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaowei</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nuno</forename><surname>Vasconcelos</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">End-toend object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">BlendMask: Top-down meets bottom-up for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kunyang</forename><surname>Hao Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongming</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youliang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Hybrid task cascade for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangmiao</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxiao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuyang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wansen</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Masklab: Instance segmentation by refining object detection with semantic and direction features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Hermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Tensormask: A foundation for dense object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Panoptic-deeplab: A simple, strong, and fast baseline for bottom-up panoptic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Maxwell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Boundary iou: Improving object-centric image segmentation evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Alexander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kirillov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Cascadepsp: toward class-agnostic and very highresolution segmentation via global and local refinement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jihoon</forename><surname>Ho Kei Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Wing</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi-Keung</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Boundary-preserving mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianheng</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lichao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyu</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uwe</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Solq: Segmenting objects by learning queries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangao</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiancai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shusheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Shan</surname></persName>
		</author>
		<title level="m">Bin Feng, and Wenyu Liu. Instances as queries</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note>ICCV</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Quad trees a data structure for retrieval on composite keys</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Raphael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><forename type="middle">Louis</forename><surname>Finkel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bentley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Acta informatica</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="9" />
			<date type="published" when="1974" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Sotr: Segmenting objects with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruohao</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dantong</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liao</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenbo</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Lvis: A dataset for large vocabulary instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agrim</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Piotr Doll?r, and Ross Girshick. Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Istr: End-toend instance segmentation via transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liujuan</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengchuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feiyue</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rongrong</forename><surname>Ji</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.00637</idno>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Mask scoring r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaojin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lichao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongchao</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Prototypical cross-attention networks for multiple object tracking and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Wing</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi-Keung</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deep occlusionaware instance segmentation with overlapping bilayers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Wing</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi-Keung</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Kaiming He, and Ross Girshick. Pointrend: Image segmentation as rendering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Shapemask: Learning to segment novel objects by refining shape priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weicheng</forename><surname>Kuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anelia</forename><surname>Angelova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Centermask: Real-time anchor-free instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngwan</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongyoul</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR, 2020</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Fully convolutional instance-aware semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haozhi</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyang</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Polytransform: Deep polygon transformer for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Namdar</forename><surname>Homayounfar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Chiu</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuwen</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Kaiming He, Bharath Hariharan, and Serge Belongie. Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Path aggregation network for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifang</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Rank sort loss for object detection and instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kemal</forename><surname>Oksuz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emre</forename><surname>Baris Can Cam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sinan</forename><surname>Akbas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kalkan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV, 2021</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Gated-scnn: Gated shape cnns for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Towaki</forename><surname>Takikawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Acuna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Look closer to segment better: Boundary patch refinement for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chufeng</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoxiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolin</forename><surname>Hu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Deep high-resolution representation learning for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianheng</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Borui</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaorui</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yadong</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingkui</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Abhinav Gupta, and Kaiming He. Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<biblScope unit="issue">7</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuning</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Solo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.04488</idno>
		<title level="m">Segmenting objects by locations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Solov2: Dynamic and fast instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rufeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">End-to-end video instance segmentation with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuqing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoliang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoshan</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaxia</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR, 2021</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wan-Yen</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Detectron2</surname></persName>
		</author>
		<ptr target="https://github.com/facebookresearch/detectron2" />
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Polarmask: Single shot instance segmentation with polar representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enze</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peize</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoge</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuebo</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ding</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Vashisht Madhavan, and Trevor Darrell. Bdd100k: A diverse driving dataset for heterogeneous multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haofeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenqi</forename><surname>Xian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingying</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangchen</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Segfix: Model-agnostic boundary refinement for segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhui</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingyi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xilin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Refinemask: Towards high-quality instance segmentation with fine-grained features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingru</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoxiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanquan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolin</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR, 2021</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">K-net: Towards unified image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangmiao</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen Change</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS, 2021</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Deformable convnets v2: More deformable, better results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xizhou</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
