<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">End-to-End Environmental Sound Classification using a 1D Convolutional Neural Network</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sajjad</forename><surname>Abdoli</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Software and IT Engineering</orgName>
								<orgName type="institution" key="instit1">?cole de Technologie Sup?rieure</orgName>
								<orgName type="institution" key="instit2">Universit? du Qu?bec</orgName>
								<address>
									<postCode>H3C 1K</postCode>
									<settlement>Montreal</settlement>
									<region>QC</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Cardinal</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Software and IT Engineering</orgName>
								<orgName type="institution" key="instit1">?cole de Technologie Sup?rieure</orgName>
								<orgName type="institution" key="instit2">Universit? du Qu?bec</orgName>
								<address>
									<postCode>H3C 1K</postCode>
									<settlement>Montreal</settlement>
									<region>QC</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><forename type="middle">Lameiras</forename><surname>Koerich</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Software and IT Engineering</orgName>
								<orgName type="institution" key="instit1">?cole de Technologie Sup?rieure</orgName>
								<orgName type="institution" key="instit2">Universit? du Qu?bec</orgName>
								<address>
									<postCode>H3C 1K</postCode>
									<settlement>Montreal</settlement>
									<region>QC</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">End-to-End Environmental Sound Classification using a 1D Convolutional Neural Network</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T10:44+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Convolutional neural network</term>
					<term>Environmental sound classification</term>
					<term>Deep learning</term>
					<term>Gammatone filterbank</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we present an end-to-end approach for environmental sound classification based on a 1D Convolution Neural Network (CNN) that learns a representation directly from the audio signal. Several convolutional layers are used to capture the signal's fine time structure and learn diverse filters that are relevant to the classification task. The proposed approach can deal with audio signals of any length as it splits the signal into overlapped frames using a sliding window. Different architectures considering several input sizes are evaluated, including the initialization of the first convolutional layer with a Gammatone filterbank that models the human auditory filter response in the cochlea. The performance of the proposed end-to-end approach in classifying environmental sounds was assessed on the UrbanSound8k dataset and the experimental results have shown that it achieves 89% of mean accuracy. Therefore, the propose approach outperforms most of the state-of-the-art approaches that use handcrafted features or 2D representations as input. Furthermore, the proposed approach has a small number of parameters compared to other architectures found in the literature, which reduces the amount of data required for training.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In the last years, Convolutional Neural Networks (CNNs) have had significant impact on several audio and music processing tasks such as automatic music tagging <ref type="bibr" target="#b8">(Dieleman &amp; Schrauwen, 2014)</ref>, large-scale video clip classification based on audio information <ref type="bibr" target="#b15">(Hershey et al., 2017)</ref>, music genre classification <ref type="bibr" target="#b6">(Costa et al., 2017)</ref>, speaker identification <ref type="bibr" target="#b31">(Ravanelli &amp; Bengio, 2018)</ref>, environmental sound classification <ref type="bibr" target="#b27">(Piczak, 2015a;</ref><ref type="bibr" target="#b29">Pons &amp; Serra, 2018;</ref><ref type="bibr" target="#b38">Simonyan &amp; Zisserman, 2014;</ref> , among others. Environmental sound classification is an interesting problem <ref type="bibr" target="#b37">(Sigtia et al., 2016;</ref><ref type="bibr" target="#b40">Stowell et al., 2015)</ref> which has different applications ranging from crime detection <ref type="bibr" target="#b30">(Radhakrishnan et al., 2005)</ref> to environmental context aware processing <ref type="bibr" target="#b4">(Chu et al., 2009)</ref>. Moreover, with the increasing interest in smart cities, IOT devices embedding automatic audio classification can be very useful for urban acoustic monitoring <ref type="bibr" target="#b26">(Mydlarz et al., 2017)</ref> like intelligent audio-based surveillance system in public transportation <ref type="bibr" target="#b21">(Laffitte et al., 2019)</ref>.</p><p>Like typical automatic classification systems, most of the approaches for environmental sound classification rely on handcrafted features or learn representations from mid-level representations such as spectro-temporal features <ref type="bibr" target="#b22">(Lude?a-Choez &amp; Gallardo-Antol?n, 2016;</ref><ref type="bibr" target="#b5">Costa et al., 2012)</ref>. Spectral representations have been used as features in several approaches based on matrix factorization <ref type="bibr" target="#b24">(Mesaros et al., 2015;</ref><ref type="bibr" target="#b1">Benetos et al., 2016;</ref><ref type="bibr" target="#b2">Bisot et al., 2016;</ref><ref type="bibr" target="#b34">Salamon &amp; Bello, 2015;</ref><ref type="bibr" target="#b12">Geiger &amp; Helwani, 2015)</ref>. <ref type="bibr" target="#b24">Mesaros et al. (2015)</ref> presented an approach for overlapping sound event detection based on learning non-negative dictionaries through joint use of spectrum and class activity annotation. <ref type="bibr" target="#b1">Benetos et al. (2016)</ref> presented an approach for overlapping acoustic event detection based on probabilistic latent component analysis where each exemplar in a sound event dictionary consists of a succession of spectral templates. <ref type="bibr" target="#b2">Bisot et al. (2016)</ref> learn features from time-frequency images in an unsupervised manner. The images are decomposed using matrix factorization methods to build a dictionary and the projection coefficients are used as features for classification. <ref type="bibr" target="#b34">Salamon &amp; Bello (2015)</ref> proposed a dictionary learning method based on the Spherical K-Means (SKM) algorithm which used log-Mel spectrograms as input. <ref type="bibr" target="#b12">Geiger &amp; Helwani (2015)</ref> used Gabor filterbank features and Gaussian Mixture Models for event detection. <ref type="bibr" target="#b25">Mulimani &amp; Koolagudi (2019)</ref> used a singular value decomposition method for extracting acoustic event specific features from spectrogram. Theses features are used as inputs to a Support Vector Machine (SVM) classifier. Recently, <ref type="bibr" target="#b44">Xie &amp; Zhu (2019)</ref> proposed a method for aggregation of acoustic and visual features for acoustic scene classification. Several acoustic features like spectral centroid, spectral entropy as well as several visual features like local binary pattern, histogram of gradients are proposed. A suitable feature selection algorithm like principle component analysis is also used. The selected feature set is used as input to an SVM classifier.</p><p>Recent works explore CNN-based approaches given the significant improvements over hand-crafted feature-based methods <ref type="bibr" target="#b27">(Piczak, 2015a;</ref><ref type="bibr" target="#b29">Pons &amp; Serra, 2018;</ref><ref type="bibr" target="#b38">Simonyan &amp; Zisserman, 2014;</ref>. However, most of these approaches first convert the audio signal into a 2D representation (spectrogram) and use 2D CNN architectures that were originally designed for object recognition such as AlexNet and VGG <ref type="bibr" target="#b38">(Simonyan &amp; Zisserman, 2014)</ref>. One of the main advantages of us-ing 2D representations is that spectrograms can summarize high dimensional waveforms into a compact representation. Furthermore, 1D representations are noisier compared to 2D representations <ref type="bibr" target="#b41">(Stowell &amp; Plumbley, 2014)</ref>. Piczak <ref type="bibr" target="#b27">(Piczak, 2015a)</ref> presented a CNN with two layers followed by three dense layers. The network operates on two input channels: log-Mel spectra and their deltas. However, one of the challenges in using 2D CNNs for environmental sound classification is that the modelling capacity of such networks depends on the availability of a large amount of training data to learn kernel parameters without over-fitting. The scarcity of labeled data of environmental sounds is also a problem.  presented a method based on a 2D CNN with five layers (SB-CNN) where new training samples are generated using data augmentation methods such as time stretching, pitch shifting, dynamic range compression or adding background noise <ref type="bibr" target="#b23">(McFee et al., 2015)</ref>. The 2D CNN was trained on the augmented dataset and evaluated on the original samples. They reported the classification accuracy of 79% on a dataset of environmental sounds <ref type="bibr" target="#b36">(Salamon et al., 2014)</ref>. <ref type="bibr" target="#b29">Pons &amp; Serra (2018)</ref> used randomly weighted 2D CNNs (non-trained) for extracting features from audio spectrograms and raw audio samples for sound classification. Several experiments have been conducted to find the best architectures for this method. In the case of environmental sound classification, the best results have been obtained by using a VGG 2D CNN <ref type="bibr" target="#b38">(Simonyan &amp; Zisserman, 2014)</ref> as a feature extractor and SVMs as classifiers. They reported mean accuracy of 7% for this problem.  proposed a new method called Between-Class (BC) learning for training neural networks. The network, for which the input is a mixture of two audio samples, is trained to predict the mixing ratio of the samples. According to their experiments, the BC learning has shown performance improvement for various architectures used for sound identification tasks. They also proposed an end-to-end 1D CNN (EnvNet-v2) that performs well on various environmental sound datasets when trained with the BC learning approach, compared to conventional learning techniques. The best error rate of 8.6% is reported on ESC-10 dataset <ref type="bibr" target="#b28">(Piczak, 2015b)</ref>.</p><p>1D CNNs that learn acoustic models directly from audio waveforms are becoming a popular method in audio processing due to the ability of these networks to take advantage of the signal's fine time structure <ref type="bibr" target="#b16">(Hoshen et al., 2015)</ref>. <ref type="bibr" target="#b19">Kim et al. (2018)</ref> proposed a 1D CNN architecture for music auto-tagging inspired by the building blocks of Resnets <ref type="bibr" target="#b14">(He et al., 2016)</ref> and SENets <ref type="bibr" target="#b17">(Hu et al., 2018)</ref>. <ref type="bibr" target="#b47">Zhu et al. (2016)</ref> proposed an end-to-end learning approach for speech recognition based on multiscale convolutions that learns the representation directly from audio waveforms. Three 1D convolutional layers with different kernel sizes are used for feature extraction and the features are concatenated by a pooling layer for ensuring a consistent sampling frequency for the rest of the network. They reported 23.28% of word error rate on a dataset drawn from a collection of sources including read, conversational, accented, and noisy speech. <ref type="bibr" target="#b31">Ravanelli and Bengio (Ravanelli &amp; Bengio, 2018)</ref> proposed the SincNet, an end-to-end approach for speaker identification and verification. The first layer of such a model is based on parametric sinc functions, which are band-pass filters. Only low and high cutoff frequencies of the filters are learned from data. This model learns meaningful filters for the first layer and decreases the number of parameters of the model. This model achieves a sentence error rate of 0.85% on TIMIT dataset <ref type="bibr" target="#b10">(Garofolo et al., 1993)</ref>. <ref type="bibr" target="#b45">Zeghidour et al. (2018)</ref> also proposed an end-to-end 1D CNN architecture for speech recognition by learning a filter bank which is considered as a replacement of Mel-filterbanks. <ref type="bibr" target="#b16">Hoshen et al. (2015)</ref> proposed an end-to-end multichannel 1D CNN for speech recognition. They also found that the timing difference between channels is an indicator of the location of the input in space. They reported 27.1% of single channel word error rate on a large vocabulary voice search dataset. <ref type="bibr" target="#b33">Sainath et al. (2015)</ref> used a similar architecture for speech recognition. They showed that features learned directly from the audio waveform match the performance of log-Mel filterbank energies. <ref type="bibr" target="#b7">Dai et al. (2017)</ref> proposed several very deep convolutional models for environmental sound classification that achieved 72% of accuracy on UrbanSound8k dataset. The proposed models consist of batch normalization, residual learning, and down-sampling in the initial layers of the CNN.</p><p>In this paper, we propose an end-to-end 1D CNN for environmental sound classification that learns the representation directly from the audio waveforms instead of from 2D representations <ref type="bibr" target="#b27">(Piczak, 2015a;</ref>. The proposed end-to-end approach provides a compact architecture that reduces the computation cost and the amount of data required for training. With the aim of extracting relevant information directly from audio waveforms, several convolutional layers are used to learn low-level and high-level representations. The highest level of representation is then used for classifying the input signal by means of three fully connected layers. Experimental results on UrbanSound8k dataset, which contains 8,732 environmental sounds from 10 classes, have shown that the proposed approach outperforms other approaches based on 2D representations such as spectrograms <ref type="bibr" target="#b27">(Piczak, 2015a;</ref><ref type="bibr" target="#b29">Pons &amp; Serra, 2018;</ref><ref type="bibr" target="#b34">Salamon &amp; Bello, 2015)</ref> by between 11.24% (SB-CNN) and 27.14% (VGG) in terms of mean accuracy. Furthermore, the proposed approach does not require data augmentation or any signal pre-processing for extracting features.</p><p>Our contribution in this paper is twofold. We present an end-to-end 1D CNN initialized with Gammatone filterbanks that has few parameters and which does not require a large amount of data for training compared to dense 2D CNNs which have millions of trainable parameters. Besides, it achieves state-of-the-art performance. Secondly, the proposed approach can handle audio signals of any length by using a sliding window of appropriate width that breaks up the audio signal into short frames of dimension compatible with the input layer of the end-to-end 1D CNN. This paper is organized as follows. Section 2 presents the ideas behind the proposed end-to-end 1D CNN architecture and the proposed approach to deal with variable audio lengths. We also present the variations in the architecture that may arise from different input dimensions as well the process of aggregating the predictions on audio frames. Section 3 presents the benchmarking dataset, the experimental protocol, the procedure used to fine-tune the proposed 1D CNN to the data, the evaluation of different input sizes, the enhancements in the proposed 1D CNN to improve its performance and an analysis of the frequency response of the filters learned at the different convolutional layers. In Section 4, we compare the performance of the proposed approach with the state-of-the-art in environmental sound classification and we analyze the magnitude responses of the filters learned at the first convolutional layer to gain some insight on the behaviour of the proposed 1D CNN. Finally, the conclusions and perspectives of future work are presented in the last section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Proposed End-to-End Architecture</head><p>The aim of the proposed end-to-end architecture is to handle audio signals of variable lengths, learning directly from the audio signal, a discriminative representation that achieves a good classification performance on different environmental sounds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Variable Audio Length</head><p>One of the challenges of using 1D CNNs in audio processing is that the length of the input sample must be fixed but the sound captured from the environment may have various duration. Therefore, it is necessary to adapt a CNN to be used with audio signals of different lengths. Moreover, a CNN must be used for continuous prediction of input audio signals of environmental sounds.</p><p>One way to circumvent this constraint imposed by the CNN input layer is to split the audio signal into several frames of fixed length using a sliding window of appropriate width. Therefore, in our approach we use a window of variable width to conditionate the audio signal to the input layer of the proposed 1D CNN. The window width depends mainly on the signal sampling rate. Furthermore, successive audio frames may also have a certain percentage of overlapping, which aim is to maximize the use of information. This naturally increases the number of samples as some parts of the audio signal are reused and that can be viewed as some sort of data augmentation. The process of framing the audio signal into appropriate frames is illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>. Moreover, the sampling rate of the audio signals has a direct impact on the dimensionality of the input sample and eventually on the computational cost of model. For environmental sounds, a sampling rate of 16 kHz may be considered a good trade-off between the quality of the input sample and the computational cost of the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">1D CNN Topology</head><p>A 1D CNN is analogous to a regular neural network but but it has generally raw data as input instead of handcrafted features. Such an input data is processed through several trainable convolutional layers for learning an appropriate representation of the input. According to the "local connectivity" theorem, the neurons in a layer are connected only to a small region of the previous layer. This small region of connectivity is called a receptive field. The input to out 1D CNN is an array representing the audio waveform, which is denoted as X. The network is designed to learn a set of parameters ? to map the input to the prediction T according to a hierarchical feature extraction given by Equation 1:</p><formula xml:id="formula_0">T = F (X | ?) = f L (...f 2 (f 1 (X | ? 1 ) | ? 2 ) | ? L )<label>(1)</label></formula><p>where L is the number of hidden layers in the network. For the convolutional layers, the operation of the l-th layer can be expressed as:</p><formula xml:id="formula_1">T l = f l (X l | ? l ) = h(W ? X l + b), ? l = [W, b]<label>(2)</label></formula><p>where ? denotes the convolution operation, X l is a two-dimensional input matrix of N feature maps, W is a set of N one dimensional kernels (receptive field) used for extracting a new set of features from the input array, b is the bias vector, and h(?) is the activation function. The shapes of X l , W and T l are (N, d), (N, m) and (N, d ? m + 1), respectively. Several pooling layers are also applied between the convolutional layers for increasing the area covered by the next receptive fields. The output of the final convolutional layer is then flattened and used as input of several stacked fully connected layers, which can be described as:</p><formula xml:id="formula_2">T l = f l (X l | ? l ) = h(W X l + b), ? l = [W, b]<label>(3)</label></formula><p>In the case of multiclass classification, the number of neurons of the output layer is the number of classes. Using softmax as the activation function for the output layer, each output neuron indicates the membership degree of the input samples for each class. During the training process, the parameters of the network are adjusted according to the back-propagated classification error and the parameters of the network are optimized to minimize an appropriate loss function <ref type="bibr" target="#b13">(Goodfellow et al., 2016)</ref>.</p><p>The proposed topology aims a compact 1D CNN architecture with a reduced number of parameters. The number of parameters of a CNN is directly related to the computational effort to train such a network as well as to the need of a large amount of data for training. Therefore, the proposed architecture shown in <ref type="figure" target="#fig_1">Figure 2</ref> is made of four convolutional layers, possibly interlaced with max pooling layers, followed by two fully connected layers and an output layer. The baseline model shown in <ref type="figure" target="#fig_1">Figure 2</ref> has as input an array of 16,000 dimensions, which represents 1-second of audio sampled at 16 kHz. However, this is not a constraint since we can adapt the model for different audio lengths and sampling rates in two ways: (i) change the model architecture to adapt it to the characteristics of the audio inputs; (ii) padding or segmenting the audio piece to adapt it to the input dimensions of the network.  The dimension, number of filters and filter size are given for the input size of 16,000. For other input sizes, the values are presented in <ref type="table" target="#tab_1">Table 1</ref>.</p><p>Several other configurations can also be derived from subtle modifications of the base model (shown in <ref type="figure" target="#fig_1">Figure 2</ref>) to adapt it to shorter or longer audio inputs, as shown in <ref type="table" target="#tab_1">Table 1</ref>. This implies modifying the number of convolutional layers as well as the number and the dimension of filters and the stride. However, for long contiguous audio recordings, instead of increasing the input dimension of the network, which also implies increasing the number of parameters, and consequently its complexity, it is preferable to split the audio waveform into shorter frames by changing the window width as explained in Section 2.1. In this way, we keep the network compact and it can process audio waveforms of any length. In spite of that, in Section 3.2 we evaluate different audio lengths as input, keeping a fixed sampling rate of 16 kHz.</p><p>The proposed 1D CNN has large receptive fields in the first convolutional layers since it is assumed that the first layers should have a more global view of the audio signal. Moreover, the environmental sound signal is non-stationary i.e. the frequency or spectral contents of the signal changes with respect to time. Therefore, shorter filters do not provide a general view on the spectral contents of the signal. The output of the last pooling layer for all feature maps is flattened and used as input to a fully connected layer. In order to reduce the over-fitting, batch normalization is applied after the activation function of each convolution layer <ref type="bibr" target="#b18">(Ioffe &amp; Szegedy, 2015)</ref>. The last fully connected layer has ten neurons. Mean squared logarithmic error, defined in Equation 4 is used as loss function (L):</p><formula xml:id="formula_3">L = 1 N N i log( p i + 1 a i + 1 ) 2 (4)</formula><p>where p i , a i and N are the predicted class, the actual class, and the number of samples respectively. For all input sizes shown in <ref type="table" target="#tab_1">Table 1</ref>, after the last pooling layer, there are two fully connected layers with 128 and 64 neurons respectively on which a drop-out is applied with a probability of 0.25 for both layers <ref type="bibr" target="#b39">(Srivastava et al., 2014)</ref>. The ReLU activation function (h(x) = max(x, 0)) is used for all layers, except for the output layer where a softmax activation function is used. Since the amount of data for training is limited, it is not feasible to use deeper architectures without significant over-fitting. By the use of the architecture shown in <ref type="figure" target="#fig_1">Figure 2</ref>, it is possible to omit a signal processing module because the network is powerful enough to extract relevant low-level and high-level information from the audio waveform.</p><p>The convolutional layers of the proposed architecture are inspired in <ref type="bibr" target="#b0">Aytar et al. (2016)</ref> who proposed a CNN architecture (SoundNet) for learning sound representations from unlabeled videos. The SoundNet <ref type="bibr" target="#b0">(Aytar et al., 2016)</ref> learns multimodal features from audio and video using two concurrent CNNs which are further used with a SVM classifier. On the other hand, the proposed 1D CNN architecture learns the representation directly from the audio waveform, and it uses such a learned representation as input to a fully connected neural network for classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Gammatone Filterbanks</head><p>Another interesting characteristic of such a 1D CNN is that its first layer can be initialized as a Gammatone filter bank. A Gammatone filter is a linear filter described by an impulse response of a gamma distribution and a sinusoidal tone. This initialization can be viewed as a trade-off between handcrafted features and representation learning. In this configuration, the kernels of the first layer are initialized by 64 band-pass Gammatone filters with central frequency ranging from 100 Hz to 8 kHz. Such a filterbank decomposes the input signal into 64 frequency bands.</p><p>Gammatone filters have been used in models of the human auditory system and are physiologically motivated to simulate the structure of peripheral auditory processing stage. For this reason, Gammatone filters have also been used to initialize the first layer of 1D CNNs for automatic speech recognition <ref type="bibr" target="#b16">(Hoshen et al., 2015;</ref><ref type="bibr" target="#b45">Zeghidour et al., 2018;</ref><ref type="bibr" target="#b33">Sainath et al., 2015)</ref>. <ref type="figure" target="#fig_2">Figure 3</ref> illustrates the frequency response of the Gammatone filterbank, generated by the Gammatone-like spectrograms toolbox developed by Ellis <ref type="bibr" target="#b9">(Ellis, 2009</ref><ref type="bibr">, accessed: February 2019</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Aggregation of Audio Frames</head><p>In the case where the input audio waveform X is split into S frames denoted as X 1 , X 2 , . . . , X S , during the classification we need to aggregate the CNN predictions to come up to a decision on X, as illustrated in <ref type="figure">Figure 4</ref>. For such an aim, different fusion rules can be used to reach a final decision, such as the majority vote or the sum rule, which are denoted in Equations 5 and 6 respectively.</p><formula xml:id="formula_4">y i = S j=1 o ji<label>(5)</label></formula><p>where o is the CNN prediction for the j = 1, . . . , S segment of the audio waveform X and i = 1, . . . , K is the predicted class. S is the number of frames and K is the number of classes.</p><formula xml:id="formula_5">y i = 1 S S j=1 o ji<label>(6)</label></formula><p>When there are K classes, we generate K values and them for an audio input, we choose the class with the maximum y i value:</p><formula xml:id="formula_6">Choose C i if y i = K max k=1 y k<label>(7)</label></formula><p>X 2 X 1 ? X s 1D CNN Aggregation ? Decision <ref type="figure">Figure 4</ref>: Aggregation of the predictions on the audio frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Experimental Results</head><p>The proposed end-to-end 1D CNN for environmental sound classification was evaluated on the UrbanSound8k dataset <ref type="bibr" target="#b36">(Salamon et al., 2014)</ref>. This dataset consists of 8,732 audio clips summing up to 7.3 hours of audio recordings. The maximum duration of audio clips is four seconds. The classes and the number of samples in each class are: "Air conditioner (AI): 1000", "Car horn (CA): 429", "Children playing (CH): 1000", "Dog bark (DO): 1000", "Drilling (DR): 1000", "Engine (EN) idling: 1000", "Gun shot (GU): 374", "Jackhammer (JA): 1000", "Siren (SI): 929", "Street music (ST): 1000". The original audio clips are recorded at different sample rates. For the experiments presented in this paper, they have been downsampled to 16 kHz in order to unify the shape of the input signal for the 1D CNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Fine-Tuning the 1D CNN Architecture</head><p>The number of convolutional layers plays a key role in detecting high-level concepts. The number of convolutional layers for the base model shown in <ref type="figure" target="#fig_1">Figure 2</ref> was determined in an exploratory experiment using the audio files of the UrbanSound8k dataset. The audio files were segmented into 16,000 samples and successive frames have 50% of overlapping. Ten percent of the dataset was used as validation set and 10% percent of the dataset was also used as test set. Each network was trained with 80% of the dataset up to 100 epochs with batch sizes of 100 samples. The accuracy achieved by the 1D CNN with one to four convolutional layers on test set was 69%, 75%, 79% and 80%, respectively. Four convolutional layers is the upper limit since the minimal dimension of the feature map has been reached at this layer. The same procedure was also adopted to find the best number of convolution layers as well as their parameters for the other configurations derived from the base model which are described in <ref type="table" target="#tab_1">Table 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Evaluation on Different Audio Lengths</head><p>All experiments reported in this subsection used a 10-fold cross-validation procedure to produce a fair comparison with the results reported by <ref type="bibr" target="#b36">Salamon et al. (2014)</ref>. One of the nine training folds is used as validation set for optimizing the parameters of the network to achieve the best accuracy. A batch size of 100 samples was used for training the CNNs and they were trained up to 100 epochs with early stopping. The Adadelta <ref type="bibr" target="#b46">(Zeiler, 2012)</ref> optimizer with the default learning rate of 1.0 was used. Adadelta has been chosen because this method dynamically adapts the learning rate during the optimization process.</p><p>First, the proposed end-to-end 1D CNN is evaluated on different audio lengths to assess the impact of the input length on the classification performance. Next, the full audio recordings of UrbanSound8k dataset, which have 59,999 frames (? three seconds), were also segmented into shorter frames using a sliding window and considering different overlapping percentages (0%, 25%, 50%, and 75%). The architecture shown in <ref type="figure" target="#fig_1">Figure 2</ref> was adapted according to the parameters described in <ref type="table" target="#tab_1">Table 1</ref>, leading to audio frames of 1,600 (? 100 msec), 8,000 (? 500 msec), 16,000 (? 1 second) and 32,000 samples (? 2 seconds).</p><p>The process of segmenting the audio signal into frames and aggregating the predictions of the classifier for all frames, resembles the process of aggregating the prediction of ensemble of classifiers. In this process, the most important parts of the audio signal contribute more to the final decision while the noisy or outlier frames have their importance averaged during the aggregation process. <ref type="table" target="#tab_2">Table 2</ref> shows the best results achieved with different frame sizes, window overlapping and combination rules on the UrbanSound8k dataset in terms of mean accuracy. For the classification of each test sample of the original dataset, the predictions for each audio segment are combined using either the majority voting or the sum rule <ref type="bibr" target="#b20">(Kittler et al., 1998)</ref>. <ref type="table" target="#tab_2">Table 2</ref> shows that the 16,000-input architecture achieved the highest accuracy which is the same accuracy achieved by 1D CNN with 59,999 inputs, even if it has almost twice less parameters than that network. Furthermore, the 8,000-input architecture achieved a mean accuracy close to that, even if it has almost three times less parameters. On the other hand, for the 1,600-input architecture, the mean accuracy is about 7% lower than the best architectures. This is an indication that short audio frames do not contain enough information to train properly the 1D CNN. However, this behaviour may be particular for the UrbanSound8k dataset and it cannot be generalized to other audio classification tasks or datasets. The box-plot of <ref type="figure" target="#fig_3">Figure 5</ref> also shows that the 16,000-input 1D CNN is the best choice since it provides the highest median; the interquartile range is the smallest one; and there is no outlier. Furthermore, such an architecture has the same mean accuracy, but almost half of the number of parameters than the second-best choice, the 50,999-input 1D CNN. Therefore, the 16,000-input 1D CNN is preferable over other architectures, as it presents the best trade-off between the number of parameters and accuracy.</p><p>In order to have a better insight about the behavior of the convolutional filters learned by the proposed 1D CNN, the Fourier transform of some filters was computed and their frequency responses are shown in <ref type="figure" target="#fig_4">Figure 6</ref>. These filters were randomly initialized and trained for the specific task and all of their parameters, such as central frequency, bandwidth, gain/attenuation, were learned directly from the data with the aim of minimizing a loss function. The learned filters are a combination of different (mainly band-pass and band-reject) filters with selective attenuation levels for different frequency levels. The filters of the first layers (CL1 and CL2) do not exhibit dominant frequencies and are quite noisy. On the other hand, the filters learned at the deeper layers (CL3 and CL4) are more regular filters, i.e., they have a well-defined frequency response which is closer to ideal filters. However, the resolution of the Fourier transform of the deeper layers is lower than in the initial layers because they are smaller than the initial ones. This analysis lead us to propose some enhancements to the proposed approach as an attempt to improve the response of the filters learned by the network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Architecture Enhancement</head><p>Three enhancements to the proposed approach are evaluated: (i) replacing the Hamming sliding window by a rectangular window because the Hamming window smooths the signal and reduces the energy of the beginning and end of the audio frame and this may cause a loss of information; (ii) augmenting slightly the amount of training data by increasing the window overlapping during the audio segmentation; (iii) initializing the first convolutional layer as a Gammatone filterbank as described in Section 2, and make this layer non-trainable. <ref type="table" target="#tab_3">Table 3</ref> summarizes the three proposed enhancements and their impact on the mean accuracy. The rectangular window leads to a slight improvement of 2% in the mean accuracy. Increasing the overlapping from 50% to 75% led to another 2% of improvement in the mean accuracy. Finally, initializing the first layer of such a 1D CNN with a Gammatone filterbank, also contributed to improve the mean accuracy in 2%, even if the number of parameters doubles due to increase of the number of filters in such a layer. An important remark is that all these enhancements have also improved the performance of most of the other 1D CNN architectures presented in <ref type="table" target="#tab_1">Table 1</ref>. In spite of that, the 16,000-input 1D CNN remains the one with the highest mean accuracy.   <ref type="table" target="#tab_4">Table 4</ref> shows the mean classification accuracy achieved by the proposed 1D CNN as well as the results achieved by other state-of-the-art approaches described in the literature. The proposed 1D CNN achieved a mean accuracy of 89% with a standard deviation of only 0.9% across the 10 folds. Note that the VGG architecture <ref type="bibr" target="#b29">(Pons &amp; Serra, 2018)</ref> is implemented based on non-trained (randomly weighted) method for feature extraction. The proposed 1D CNN, the EnvNet-v2  and the M18 CNN <ref type="bibr" target="#b7">(Dai et al., 2017)</ref> are end-to-end architectures, which learn the representation directly from the audio waveform while all other approaches in <ref type="table" target="#tab_4">Table  4</ref> use 2D representations of the audio signal as input. Therefore, besides the reduced number of parameters of the proposed end-to-end 1D CNN, which indicates that the proposed approach can be trained with fewer number of training samples relative to most of the other approaches, it outperforms all other approaches. <ref type="figure" target="#fig_7">Figure 8</ref> compares the proposed 1D CNN with other approaches reported in <ref type="bibr" target="#b34">(Salamon &amp; Bello, 2015)</ref> using a boxplot generated from the accuracy scores of 10 folds. We can see that the proposed 1D CNN has the highest mean and median, the interquartile range is one of the smallest one and there is no outlier. <ref type="figure" target="#fig_8">Figure 9</ref> shows the confusion matrix of the proposed end-to-end 1D CNN on the Ur-banSound8k dataset. Values along the diagonal indicate the number of samples classified    2D 79% 241 k EnvNet-v2  1D 78% 101 M SKM (DA) <ref type="bibr" target="#b34">(Salamon &amp; Bello, 2015)</ref> 2D 76% NA SKM <ref type="bibr" target="#b34">(Salamon &amp; Bello, 2015)</ref> 2D 74% NA PiczakCNN <ref type="bibr" target="#b27">(Piczak, 2015a)</ref> 2D 73% 26 M M18 CNN <ref type="bibr" target="#b7">(Dai et al., 2017)</ref> 1D 72% 3.7 M SB-CNN  1D 73% 241 k VGG <ref type="bibr" target="#b29">(Pons &amp; Serra, 2018)</ref> 2D <ref type="formula" target="#formula_6">70%</ref>  True label 940 21 9 22 10 2 13 7 5 12 2 277 4 6 1 4 1 1 1 0 7 28 859 63 10 7 17 2 7 69 5 17 30 776 9 17 12 3 17 17 5 16 10 19 898 1 15 19 1 16 5 26 4 26 12 951 10 5 2 7 3 0 1 5 2 3 267 0 2 1 0 14 1 12 28 2 16 956 1 6 5 11 31 31 13 9 9 1 885 15 28 19 51 40 17 4 14 6 8 857 correctly for each specific class. It shows that the ST and CH classes are the hardest classes for the CNN. However, EN and GU classes are well separated by the proposed CNN. For a better insight into the performance of the proposed end-to-end CNN, <ref type="figure" target="#fig_0">Figure  10</ref> shows the per-class accuracy for each specific class in the UrbanSound8k dataset in comparison with the SB-CNN with data augmentation which uses a 2D representation . <ref type="figure" target="#fig_0">Figure 10</ref> shows that for class GU and ST the proposed CNN achieves relatively similar result that the SB-CNN. For classes AI, DR, EN, SI, CA and JA, the proposed CNN works better than the SB-CNN while for classes CH and DO the SB-CNN approach is slightly better than the proposed end-to-end CNN. Since the input of the proposed CNN is the audio waveform, it seems that the network tends to extract different features from the audio waveform which might be missing in the spectrogram representation of the signal. However, the SB-CNN, which uses spectrograms as input , needs 20 times more samples than the proposed end-to-end 1D CNN to achieve the accuracy of 79%. It is interesting to note that both classifiers could be fused in order to improve the overall accuracy since it can be seen that these approaches have different per-class accuracy for some classes such as AI and JA. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Discussion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Filter Response</head><p>The magnitude responses of the convolutional filters of the first layer of the proposed 1D CNN are shown in <ref type="figure" target="#fig_0">Figure 11</ref>. To obtain a better image representation of the frequency response, the number of kernels in the first layer has been increased to 64 (compared to 16 in the one used in the experiments). Note that this configuration led to a slight decrease in the classification accuracy. <ref type="figure" target="#fig_0">Figures 11(a) and 11(b)</ref> show the response of the filters after convergence and the response of the kernels sorted based on their central frequencies, respectively. The central frequency of each kernel is computed by computing the Fast Fourier transform of the filter and by selecting the frequency bin with the highest peak. Each row in the image is created by feeding the network with a sinusoidal wave with a specific frequency. For such an aim, sinusoidal waves in the range of 1 Hz to 8 kHz, with a step of 100 Hz, have been used. The feature map of the first convolutional layer is first obtained and then, it is computed the average of the feature map along the time axis. <ref type="figure" target="#fig_0">Figure 11(c)</ref> shows the output of 64 Gammatone filters used as band-pass filters.  <ref type="bibr" target="#b3">(Brown, 1991)</ref>.</p><p>From <ref type="figure" target="#fig_0">Figure 11(b)</ref>, it can be seen that the learned filters have a logarithmic response similar to the band pass filters created using cardinal sinusoidal functions. In addition, this behavior is also similar to how humans perceive sounds, which is also logarithmic <ref type="bibr" target="#b32">(Roederer, 2008)</ref>. A similar behavior has also been observed in other end-to-end systems for audio processing tasks <ref type="bibr" target="#b16">(Hoshen et al., 2015;</ref><ref type="bibr" target="#b33">Sainath et al., 2015;</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, an end-to-end 1D CNN for environmental sound classification has been proposed. The architecture of the network consists of three to five convolutional layers, depending on the length of the audio signal. Instead of using handcrafted static filterbanks such as those used to extract MFCC features, the proposed 1D CNN learns the filters directly from the audio waveform. The proposed approach was evaluated on a dataset of 8,732 audio samples and the experimental results have shown that the proposed end-to-end approach learns several relevant filter representations which allows it to outperform other state-of-the-art approaches based on 2D representations and 2D CNNs. Furthermore, the proposed end-to-end 1D architecture has fewer parameters than most of the other CNN architectures for environmental sound classification, while achieving mean accuracy that is between 11.24% and 27.14% higher than such 2D architectures.</p><p>However, even if we have achieved the best results using 1D representation of the audio signal, it may have a complementarity between the learned 1D filters and the filters learned from 2D representations (spectrograms), at least for some classes, as highlighted in <ref type="figure" target="#fig_0">Figure 10</ref>. This is an indication that the overall performance may be improved by combining the approaches that use 1D and 2D representations. As a future work, we will investigate if such a combination is feasible and if it can lead to a better performance in classifying environmental sounds. Furthermore, the filters learned in the intermediate convolutional layers of the proposed 1D CNN do not exhibit dominant frequencies and seems to be noisy. A further investigation is necessary to find out how to circumvent such a problem and possibly improve further the performance of the proposed 1D CNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Availability of Data and Material</head><p>UrbanSound8k dataset <ref type="bibr" target="#b36">(Salamon et al., 2014)</ref> is used for training and testing the method. The dataset is available online at: https://urbansounddataset.weebly.com/urbansound8k.html.</p><p>The source code of the proposed end-to-end CNN will also be made available in the final version of the paper.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Framing the input audio signal into several frames (s, s + 1) with appropriate overlapping percentage (50%).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>The architecture of the proposed end-to-end 1D CNN for environmental sound classification.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Frequency response of 64 filters of Gammatone filterbank.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>The box plot for the five different input sizes on UrbanSound8k dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Fourier transform of randomly selected filters from the four convolutional layers (CLs) of the proposed 16,000-input 1D CNN shown inFigure 2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7</head><label>7</label><figDesc>shows the Fourier transform of some of the filters of the enhanced model with non-trainable Gammatone filterbank. Similar to the filters of the original model (Figure 6), the filters of the deepest layers (CL3 and CL4) have a well-defined frequency response. Filters of the intermediate layer (CL2) still do not exhibit dominant frequency levels. Even thought, the minor changes in the responses of the intermediate and deeper filters, the Gammatone filters of the first layer were useful to improve the mean accuracy of the proposed 1D CNN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Fourier transform of randomly selected filters from the four convolutional layers (CLs) of the 16,000-input 1D CNN with Gammatone filterbank in the first convolutional layer of the network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 :</head><label>8</label><figDesc>77 M NA: Not available. DA: With data augmentation. Classification accuracy of the proposed 1D CNN as well as the results obtained by other state-of-the-art approaches. Figure adapted from (Salamon &amp; Bello, 2017). AI CA CH DO DR EN GU JA SI</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 9 :</head><label>9</label><figDesc>Confusion matrix for the proposed end-to-end 1D CNN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 10 :</head><label>10</label><figDesc>Per-class weighted accuracy for the proposed 1D CNN and the SB-CNN with data augmentation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 11 :</head><label>11</label><figDesc>Magnitude response of the convolutional filters of the first layer of the end-to-end 1D CNN: (a) response of the filters after convergence; (b) response of the kernels after sorting based on their central frequency; (c) frequency response of band pass filters. Center frequency of filters are selected according to constant Q transform rules</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>The configuration of the convolutional layers (CL) and pooling layers (PL) for the end-to-end CNN considering different input sizes (audio lengths).</figDesc><table><row><cell>Input Size</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Layer</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>CL1</cell><cell>PL1</cell><cell>CL2</cell><cell cols="5">PL2 CL3 CL4 CL5 PL3</cell></row><row><cell>50,999</cell><cell>Dim</cell><cell cols="4">25,468 3,183 1,576 197</cell><cell>91</cell><cell>42</cell><cell>20</cell><cell>5</cell></row><row><cell></cell><cell># Filters</cell><cell>16</cell><cell>16</cell><cell>32</cell><cell>32</cell><cell>64</cell><cell>128</cell><cell>256</cell><cell>256</cell></row><row><cell></cell><cell cols="2">Filter Size 64</cell><cell>8</cell><cell>32</cell><cell>8</cell><cell>16</cell><cell>8</cell><cell>4</cell><cell>4</cell></row><row><cell></cell><cell>Stride</cell><cell>2</cell><cell>8</cell><cell>2</cell><cell>8</cell><cell>2</cell><cell>2</cell><cell>2</cell><cell>4</cell></row><row><cell>32,000</cell><cell>Dim</cell><cell cols="3">15,969 1,996 983</cell><cell>122</cell><cell>54</cell><cell>24</cell><cell>11</cell><cell>2</cell></row><row><cell></cell><cell># Filters</cell><cell>16</cell><cell>16</cell><cell>32</cell><cell>32</cell><cell>64</cell><cell>128</cell><cell>256</cell><cell>256</cell></row><row><cell></cell><cell cols="2">Filter Size 64</cell><cell>8</cell><cell>32</cell><cell>8</cell><cell>16</cell><cell>8</cell><cell>4</cell><cell>4</cell></row><row><cell></cell><cell>Stride</cell><cell>2</cell><cell>8</cell><cell>2</cell><cell>8</cell><cell>2</cell><cell>2</cell><cell>2</cell><cell>4</cell></row><row><cell>16,000</cell><cell>Dim</cell><cell>7,969</cell><cell>996</cell><cell>483</cell><cell>60</cell><cell>23</cell><cell>8</cell><cell>NA</cell><cell>NA</cell></row><row><cell></cell><cell># Filters</cell><cell>16</cell><cell>16</cell><cell>32</cell><cell>32</cell><cell>64</cell><cell>128</cell><cell>NA</cell><cell>NA</cell></row><row><cell></cell><cell cols="2">Filter Size 64</cell><cell>8</cell><cell>32</cell><cell>8</cell><cell>16</cell><cell>8</cell><cell>NA</cell><cell>NA</cell></row><row><cell></cell><cell>Stride</cell><cell>2</cell><cell>8</cell><cell>2</cell><cell>8</cell><cell>2</cell><cell>2</cell><cell>NA</cell><cell>NA</cell></row><row><cell>16,000G</cell><cell>Dim</cell><cell cols="3">15,489 19,36 953</cell><cell>119</cell><cell>52</cell><cell>23</cell><cell>NA</cell><cell>NA</cell></row><row><cell></cell><cell># Filters</cell><cell>64</cell><cell>64</cell><cell>32</cell><cell>32</cell><cell>64</cell><cell>128</cell><cell>NA</cell><cell>NA</cell></row><row><cell></cell><cell cols="2">Filter Size 512</cell><cell>8</cell><cell>32</cell><cell>8</cell><cell>16</cell><cell>8</cell><cell>NA</cell><cell>NA</cell></row><row><cell></cell><cell>Stride</cell><cell>1</cell><cell>8</cell><cell>2</cell><cell>8</cell><cell>2</cell><cell>2</cell><cell>NA</cell><cell>NA</cell></row><row><cell>8,000</cell><cell>Dim</cell><cell>3,969</cell><cell>496</cell><cell>233</cell><cell>29</cell><cell>7</cell><cell>NA</cell><cell>NA</cell><cell>NA</cell></row><row><cell></cell><cell># Filters</cell><cell>16</cell><cell>16</cell><cell>32</cell><cell>32</cell><cell>64</cell><cell>NA</cell><cell>NA</cell><cell>NA</cell></row><row><cell></cell><cell cols="2">Filter Size 64</cell><cell>8</cell><cell>32</cell><cell>8</cell><cell>16</cell><cell>NA</cell><cell>NA</cell><cell>NA</cell></row><row><cell></cell><cell>Stride</cell><cell>2</cell><cell>8</cell><cell>2</cell><cell>8</cell><cell>2</cell><cell>NA</cell><cell>NA</cell><cell>NA</cell></row><row><cell>1,600</cell><cell>Dim</cell><cell>785</cell><cell>392</cell><cell>189</cell><cell>94</cell><cell>44</cell><cell>NA</cell><cell>NA</cell><cell>NA</cell></row><row><cell></cell><cell># Filters</cell><cell>16</cell><cell>16</cell><cell>32</cell><cell>32</cell><cell>64</cell><cell>NA</cell><cell>NA</cell><cell>NA</cell></row><row><cell></cell><cell cols="2">Filter Size 32</cell><cell>2</cell><cell>16</cell><cell>2</cell><cell>8</cell><cell>NA</cell><cell>NA</cell><cell>NA</cell></row><row><cell></cell><cell>Stride</cell><cell>2</cell><cell>2</cell><cell>2</cell><cell>2</cell><cell>2</cell><cell>NA</cell><cell>NA</cell><cell>NA</cell></row></table><note>NA: Not Applicable. G: With Gammatone filterbank for the first layer of CNN.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Mean accuracy and standard deviation on the UrbanSound8k dataset over the 10 folds for the different architectures having as input the full audio (59,999) or segmented audio with different window widths and 50% overlapping.</figDesc><table><row><cell>Input</cell><cell cols="2">Combination Mean?SD # of</cell></row><row><cell cols="2">Dimension Rule</cell><cell>Accuracy Parameters</cell></row><row><cell cols="2">59,999 NA</cell><cell>83%?1.3% 421,146</cell></row><row><cell cols="2">32,000 Maj Voting</cell><cell>82%?0.9% 322,842</cell></row><row><cell cols="2">16,000 Sum Rule</cell><cell>83%?1.3% 256,538</cell></row><row><cell></cell><cell>8,000 Sum Rule</cell><cell>80%?1.9% 116,890</cell></row><row><cell></cell><cell>1,600 Sum Rule</cell><cell>77%?3.0% 394,906</cell></row></table><note>NA: Not applicable</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Improvements in the mean accuracy for the 16,000-input 1D CNN on the UrbanSound8k dataset.</figDesc><table><row><cell>CL1</cell><cell>Window</cell><cell cols="2">Overlapping Combination</cell><cell>Mean</cell><cell># of</cell></row><row><cell>Initialization</cell><cell></cell><cell></cell><cell>Rule</cell><cell cols="2">Accuracy Parameters</cell></row><row><cell>Randomly</cell><cell>Hamming</cell><cell>50%</cell><cell>Sum Rule</cell><cell>83%</cell><cell>256,538</cell></row><row><cell>Randomly</cell><cell>Rectangular</cell><cell>50%</cell><cell>Sum Rule</cell><cell>85%</cell><cell>256,538</cell></row><row><cell>Randomly</cell><cell>Rectangular</cell><cell>75%</cell><cell>Sum Rule</cell><cell>87%</cell><cell>256,538</cell></row><row><cell>Gammatone</cell><cell>Rectangular</cell><cell>50%</cell><cell>Sum Rule</cell><cell>89%</cell><cell>550,506</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Mean accuracy of different approaches on the UrbanSound8k dataset.</figDesc><table><row><cell>Approach</cell><cell>Representation</cell><cell>Mean</cell><cell># of</cell></row><row><cell></cell><cell></cell><cell cols="2">Accuracy Parameters</cell></row><row><cell>Proposed 1D CNN Gamma</cell><cell>1D</cell><cell>89%</cell><cell>550 k</cell></row><row><cell>Proposed 1D CNN Rand</cell><cell>1D</cell><cell>87%</cell><cell>256 k</cell></row><row><cell>SB-CNN (DA)</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work was funded by the Natural Sciences and Engineering Research Council of Canada (NSERC). This work was also supported by the NVIDIA GPU Grant Program.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Competing Interests</p><p>The authors declare that they have no competing interests. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Credit Authorship Contribution Statement</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Soundnet: Learning sound representations from unlabeled video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Aytar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="892" to="900" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Detection of overlapping acoustic events using a temporally-constrained probabilistic model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Benetos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lafay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lagrange</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Plumbley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="6450" to="6454" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Acoustic scene classification with matrix factorization for unsupervised feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Bisot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Serizel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Essid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Richard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="6445" to="6449" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Calculation of a constant Q spectral transform. The Journal of the</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Brown</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1991" />
			<publisher>Acoustical Society of America</publisher>
			<biblScope unit="volume">89</biblScope>
			<biblScope unit="page" from="425" to="434" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Environmental sound recognition with time-frequency audio features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kuo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="1142" to="1158" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Music genre classification using LBP textural features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Costa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Oliveira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Koerich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Gouyon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Martins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Signal Processing</title>
		<imprint>
			<biblScope unit="volume">92</biblScope>
			<biblScope unit="page" from="2723" to="2737" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">An evaluation of Convolutional Neural Networks for music classification using spectrograms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">M</forename><surname>Costa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Oliveira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">N</forename><surname>Silla</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.asoc.2016.12.024</idno>
	</analytic>
	<monogr>
		<title level="j">Applied Soft Computing</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="page" from="28" to="38" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Very Deep Convolutional Neural Networks for Raw Waveforms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Das</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="421" to="425" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">End-to-end learning for music audio</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schrauwen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="6964" to="6968" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Gammatone-like spectrograms, web resource</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P W</forename><surname>Ellis</surname></persName>
		</author>
		<ptr target="http://www.ee.columbia.edu/~dpwe/resources/matlab/gammatonegram/" />
		<imprint>
			<date type="published" when="2009-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Garofolo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">F</forename><surname>Lamel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">M</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">G</forename><surname>Fiscus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Pallett</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Darpa timit acoustic-phonetic continous speech corpus cd-rom. nist speech disc 1-1.1. NASA STI/Recon technical report n</title>
		<imprint>
			<biblScope unit="page">93</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Improving event detection for audio surveillance using Gabor filterbank features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Helwani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">23rd European Signal Processing Conference</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="714" to="718" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>MIT Press Cambridge</publisher>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Cnn architectures for large-scale audio classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hershey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chaudhuri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gemmeke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Plakal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Platt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Saurous</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Seybold</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="131" to="135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Speech acoustic modeling from raw multichannel waveforms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hoshen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">W</forename><surname>Wilson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4624" to="4628" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7132" to="7141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Sample-level cnn architectures for music autotagging using raw waveforms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="366" to="370" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kittler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hatef</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Duin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<title level="m">On combining classifiers. IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="226" to="239" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Assessing the performances of different neural network architectures for the detection of screams and shouts in public transportation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Laffitte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sodoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Girin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert Systems with Applications</title>
		<imprint>
			<biblScope unit="volume">117</biblScope>
			<biblScope unit="page" from="29" to="41" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Acoustic event classification using spectral band selection and non-negative matrix factorization-based features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lude?a-Choez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gallardo-Antol?n</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert Systems with Applications</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="page" from="77" to="86" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A software framework for musical data augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mcfee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Humphrey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Society for Music Information Retrieval Conference</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="248" to="254" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Sound event detection in real life recordings using coupled matrix factorization of spectral representations and class activity annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mesaros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Heittola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Dikmen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Virtanen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="151" to="155" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Segmentation and characterization of acoustic event spectrograms using singular value decomposition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mulimani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">G</forename><surname>Koolagudi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert Systems with Applications</title>
		<imprint>
			<biblScope unit="volume">120</biblScope>
			<biblScope unit="page" from="413" to="425" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">The implementation of low-cost urban acoustic monitoring devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Mydlarz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Salamon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Acoustics</title>
		<imprint>
			<biblScope unit="volume">117</biblScope>
			<biblScope unit="page" from="207" to="218" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Environmental sound classification with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Piczak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">25th International Workshop on Machine Learning for Signal Processing</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Esc: Dataset for environmental sound classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">J</forename><surname>Piczak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd ACM international conference on Multimedia</title>
		<meeting>the 23rd ACM international conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1015" to="1018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Randomly weighted cnns for (music) audio classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pons</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Serra</surname></persName>
		</author>
		<ptr target="https://arxiv.org/pdf/1805.00237.pdf" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Audio analysis for surveillance applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Radhakrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Divakaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Smaragdis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Workshop on Applications of Signal Processing to Audio and Acoustics</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="158" to="161" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Speaker recognition from raw waveform with sincnet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ravanelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE Spoken Language Technology Workshop (SLT)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1021" to="1028" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">The physics and psychophysics of music: an introduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">G</forename><surname>Roederer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>Springer Science &amp; Business Media</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning the speech front-end with raw waveform cldnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">W</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Sixteenth Annual Conference of the International Speech Communication Association</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Unsupervised feature learning for urban sound classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Salamon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="171" to="175" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Deep convolutional neural networks and data augmentation for environmental sound classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Salamon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="279" to="283" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A dataset and taxonomy for urban sound research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Salamon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Jacoby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">22nd ACM International Conference on Multimedia</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1041" to="1044" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Automatic environmental sound recognition: Performance versus computational cost</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sigtia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Krstulovi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Plumbley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="2096" to="2107" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<ptr target="https://arxiv.org/pdf/1409.1556.pdf" />
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Detection and classification of acoustic scenes and events</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Stowell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Giannoulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Benetos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lagrange</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Plumbley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="1733" to="1746" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Automatic large-scale classification of bird sounds is strongly improved by unsupervised feature learning. PeerJ , 2 , e488</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Stowell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Plumbley</surname></persName>
		</author>
		<idno type="DOI">10.7717/peerj.488</idno>
		<ptr target="https://doi.org/10.7717/peerj.488" />
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Learning environmental sounds with end-to-end convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tokozume</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Harada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2721" to="2725" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Learning from between-class examples for deep sound recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tokozume</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ushiku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Harada</surname></persName>
		</author>
		<ptr target="https://arxiv.org/pdf/1711.10282.pdf" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Investigation of acoustic and visual features for acoustic scene classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert Systems with Applications</title>
		<imprint>
			<biblScope unit="volume">126</biblScope>
			<biblScope unit="page" from="20" to="29" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Endto-end speech recognition from the raw waveform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zeghidour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Dupoux</surname></persName>
		</author>
		<ptr target="https://arxiv.org/pdf/1806.07098.pdf" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Adadelta: an adaptive learning rate method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zeiler</surname></persName>
		</author>
		<ptr target="https://arxiv.org/pdf/1212.5701.pdf" />
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Learning multiscale features directly from waveforms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Enge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hannun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Conference of the International Speech Communication Association (INTERSPEECH)</title>
		<meeting>the Annual Conference of the International Speech Communication Association (INTERSPEECH)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1305" to="1309" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
