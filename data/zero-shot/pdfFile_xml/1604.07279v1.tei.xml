<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Actionness Estimation Using Hybrid Fully Convolutional Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institutes of Advanced Technology</orgName>
								<orgName type="institution">Shenzhen key lab of Comp. Vis. &amp; Pat. Rec</orgName>
								<address>
									<settlement>Shenzhen</settlement>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">CAS</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Computer Vision Lab</orgName>
								<orgName type="institution">ETH Zurich</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institutes of Advanced Technology</orgName>
								<orgName type="institution">Shenzhen key lab of Comp. Vis. &amp; Pat. Rec</orgName>
								<address>
									<settlement>Shenzhen</settlement>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">CAS</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institutes of Advanced Technology</orgName>
								<orgName type="institution">Shenzhen key lab of Comp. Vis. &amp; Pat. Rec</orgName>
								<address>
									<settlement>Shenzhen</settlement>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">CAS</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Department of Information Engineering</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
								<address>
									<settlement>Hong Kong</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><forename type="middle">Van</forename><surname>Gool</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">Computer Vision Lab</orgName>
								<orgName type="institution">ETH Zurich</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Actionness Estimation Using Hybrid Fully Convolutional Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T11:20+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Actionness <ref type="bibr" target="#b2">[3]</ref> was introduced to quantify the likelihood of containing a generic action instance at a specific location. Accurate and efficient estimation of actionness is important in video analysis and may benefit other relevant tasks such as action recognition and action detection. This paper presents a new deep architecture for actionness estimation, called hybrid fully convolutional network (H-FCN), which is composed of appearance FCN (A-FCN) and motion FCN (M-FCN). These two FCNs leverage the strong capacity of deep models to estimate actionness maps from the perspectives of static appearance and dynamic motion, respectively. In addition, the fully convolutional nature of H-FCN allows it to efficiently process videos with arbitrary sizes. Experiments are conducted on the challenging datasets of Stanford40, UCF Sports, and JHMDB to verify the effectiveness of H-FCN on actionness estimation, which demonstrate that our method achieves superior performance to previous ones. Moreover, we apply the estimated actionness maps on action proposal generation and action detection. Our actionness maps advance the current state-of-the-art performance of these tasks substantially.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Action understanding in videos is an important problem in computer vision and has received extensive research attention in this community rencently. Most of the research works focused on the problem of action classification <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b40">41]</ref>, which aims at predicting an action label given a video. State-of-the-art classification methods <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b41">42]</ref> have achieved relatively good performance on several challenging datasets, such as HMDB51 <ref type="bibr" target="#b19">[20]</ref> and UCF101 <ref type="bibr" target="#b31">[32]</ref>. However, these classification methods are only able to answer "is there an action of certain type present in the video", but fail to provide the information about "where is it if there is an action in the video". To overcome this limitation, the problem of action detection has been studied by several recent works <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b38">39]</ref>, but these methods still perform relatively poorly on the realistic datasets, such as UCF Sports <ref type="bibr" target="#b25">[26]</ref> and JHMDB <ref type="bibr" target="#b13">[14]</ref>. For action detection in videos, we need to estimate bounding boxes of the action of interest at each frame, which together form a spatio-temporal tube in the input video. Sliding window becomes computationally prohibitive due to the huge numbers of candidate windows in the video space. For example, give a video of size W ? H ? T , the number of possible boxes for each frame is around O((W H) 2 ) and the number of possible tubes for the video is as large as O((W H) 2T ). Motivated by fast object detection using proposals <ref type="bibr" target="#b9">[10]</ref>, the idea of "action proposal" <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b45">46]</ref> has been introduced for efficient action detection <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b12">13]</ref>. Like object proposal algorithms, most of these methods depend on low-level visual cues, such as spatial edges and motion boundaries, and generate action candidates by hierarchically merging super-voxels <ref type="bibr" target="#b43">[44]</ref>. Therefore, these methods usually require heuristic designs and sophisticated merging algorithms, which are difficult to be optimized for action detection and may be sensitive to input noise. Besides, a large amount of candidate regions (around 0.1K-1K) are usually generated by these methods for each frame, which still leads to large computational cost in the subsequent processing.</p><p>In this paper we focus on a more general problem regarding action understanding and try to estimate the interestingness maps of generic action given the raw frames, called as actionness estimation <ref type="bibr" target="#b2">[3]</ref>. Each value of the actionness maps describes the confidence of containing an action instance at this place, where higher value indicates larger probability. According to the recent work <ref type="bibr" target="#b2">[3]</ref>, from the perspective of computer vision, action is defined as intentional bodily movement of biological agents (such as people, animals). Therefore, there are two important visual cues for actionness estimation: appearance and motion. Appearance information is helpful to locate the biological agents, while motion information contributes to detect bodily movements. In addition, the visual cues of appearance and motion are complementary to each other and fusing them may lead to more accurate actionness estimation.</p><p>To accomplish the goal of actionness estimation, we propose a two-stream fully convolutional architecture to transform the raw videos into the map of actionness, called as hybrid fully convolutional network (H-FCN). Our H-FCN is composed of two separate neural networks: (i) appearance fully-convolutional network (A-FCN), taking RGB image as input, which captures the spatial and static visual cues, (ii) motion fully-convolutional neural network (M-FCN), using optical flow fields as input, that extracts the temporal and motion information. The actionness maps from these two different FCNs are complementary to each other as shown in <ref type="figure" target="#fig_0">Figure 1</ref>. Each FCN is essentially a discriminative network trained in an end-to-end and pixel-to-pixel manner. By using fully-convolutional architecture, our H-FCN allows for input with arbitrary size and produces the actionness map of corresponding size.</p><p>Specifically, we adopt the contemporary classification networks (ClarifaiNet <ref type="bibr" target="#b48">[49]</ref>) into fully-convolutional architecture and transfer the pre-trained model parameters from the large dataset (e.g. ImageNet <ref type="bibr" target="#b4">[5]</ref>, UCF101 <ref type="bibr" target="#b31">[32]</ref>) to the task of actionness estimation by fine tuning. We verify the performance of H-FCN for actionness estimation on both images and videos. For image data, there is no motion information available and we only use the A-FCN to produce the actionness map on the dataset of Stanford40 <ref type="bibr" target="#b44">[45]</ref>. For video data with human movement, we use the H-FCN to estimate the actionness on the datasets of UCF Sports <ref type="bibr" target="#b25">[26]</ref> and JHMDB <ref type="bibr" target="#b13">[14]</ref>. The experimental results on these two datasets demonstrate that our proposed actionness estimation method outperforms previous methods.</p><p>Moreover, actionness map can be viewed as a new kind of feature and could be exploited to assist many video based tasks such as action classification, action detection, and ac-tor tracking. In this paper we incorporate our estimated actionness maps into the successful RCNN-alike <ref type="bibr" target="#b9">[10]</ref> detection framework to perform action detection in videos. We first design a NMS score sampling method to produce action proposals based on actionness maps for each frame. Then, we choose the two-stream convolutional networks <ref type="bibr" target="#b29">[30]</ref> as classifiers to perform action detection. We extensively evaluate the effectiveness of our proposed method on two tasks: action proposal generation on the datasets of Stanford 40 <ref type="bibr" target="#b44">[45]</ref> and JHMDB <ref type="bibr" target="#b13">[14]</ref>, and action detection on the dataset of JHMDB <ref type="bibr" target="#b13">[14]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head><p>Actionness and action proposals. Chen et al. <ref type="bibr" target="#b2">[3]</ref> first studied the problem of actionness from the philosophical and visual perspective of action. They proposed Lattice Conditional Ordinal Random Fields to rank actionness. Our definition of actionness is consistent with theirs but we introduce a new method called hybrid fully convolutional networks to estimate actionness. Besides, we further apply our actionness map for the task of action detection. Motivated by object proposals in images <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b34">35]</ref>, several methods have been developed to generate action proposals in video domain <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b12">13]</ref>. Most of these methods generated action proposals based on low-level segmentation and hierarchically merge super-voxels <ref type="bibr" target="#b43">[44]</ref> in spatio-temporal domain. However, video segmentation itself is a difficult problem and still under research. Yu et al. <ref type="bibr" target="#b45">[46]</ref> exploited human and motion detection algorithms to generate candidate bounding boxes as action proposals. Our method does not rely on any pre-processing technique and directly transform raw images into actionness map with fully convolutional networks.</p><p>Action detection. Action detection has been comprehensively studied in previous works <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b23">24]</ref>. Methods in <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b20">21]</ref> used Bag of Visual Words (BoVWs) representation to describe action and utilized sliding window scheme for detection. Ke et al. <ref type="bibr" target="#b17">[18]</ref> utilized global template matching with the volume features for event detection. Lan et al. <ref type="bibr" target="#b20">[21]</ref> resorted to latent learning to locate action automatically. Tian et al. <ref type="bibr" target="#b33">[34]</ref> extended the 2D deformable part model to 3D cases for localizing actions and Wang et al. <ref type="bibr" target="#b38">[39]</ref> proposed a unified approach to perform action detection and pose estimation by using dynamic-poselets and modeling their relations. Lu et al. <ref type="bibr" target="#b23">[24]</ref> proposed a MRF framework for human action segmentation with hierarchical super-voxel consistency. Jain et al. <ref type="bibr" target="#b12">[13]</ref> produced action proposals using super-voxels and utilized hand-crafted features. Gkioxari et al. <ref type="bibr" target="#b10">[11]</ref> proposed a similar proposal-based action detection method, but replaced hand-crafted features with deep-learned representations. Our method focuses on actionness estimation and is complementary to these proposal-based action detection  <ref type="figure">Figure 2</ref>. Pipeline of our approach. We propose a new architecture, called hybrid full convolutional network (H-FCN), for the task of actionness estimation. H-FCN contains two parts, namely appearance fully convolutional network (A-FCN) and motion fully convolutional network (F-FCN), which captures the visual cues from the perspectives of static appearance and dynamic motion, respectively. Based the estimated actionness maps, we design a RCNN-alike <ref type="bibr" target="#b9">[10]</ref> action detection system, by first using actionness to generate action proposals and then applying two-stream convolutional networks to classify these proposals. methods in sense that our actionness map can be used to generate proposals.</p><p>Fully convolutional networks. Convolutional neural networks (CNNs) have achieved remarkable successes for various tasks, such as object recognition <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b48">49]</ref>, event recognition <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b7">8]</ref>, crowd analysis <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28]</ref> and so on. Recently, several attempts have been made in applying CNN for action recognition from videos <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b49">50]</ref>. Fully convolutional networks (FCN) were originally introduced for the task of semantic segmentation in images <ref type="bibr" target="#b22">[23]</ref>. One advantage of FCN is that it can take input of arbitrary size and produce semantic maps of corresponding size with efficient inference and learning. To our best knowledge, we are the first to apply FCN into video domain for actionness estimation. In particular, we propose an effective hybrid fully convolutional network which leverages both appearance and motion cues for detecting actionness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Our Approach</head><p>In this section, we introduce our approach for actionness estimation and show how to apply actionness maps for action proposal generation and action detection. In particular, a brief introduction of fully convolutional networks is firstly described as preparation. Then, we propose hybrid fully convolutional networks to estimate the actionness map from raw frames and optical flow fields. Finally, based on the estimated actionness maps, we develop a RCNN-alike <ref type="bibr" target="#b9">[10]</ref> framework for action detection in videos.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Fully convolutional networks</head><p>The feature map processed in each convolutional layer of CNN can be seen as a three-dimensional volume of size h ? w ? c, where h and w are the height and width of the map respectively, and c is the number of map channels (filters). The input of CNN is a raw image, with h ? w pixels and c colors. The basic components in CNN contain convolutional operation, pooling operation, and activation function. These basic operations are performed at specific local regions and their parameters are shared across the whole spatial domain of input image or feature map. Hence, this structure allows CNN to have the desired property of translation invariance.</p><p>Let f t i,j ? R ct denote the feature vector at location (i, j) in a particular layer t, and f t+1 i,j be the feature vector of following layer t + 1 at location (i, j). Then, we obtain the following formula for the basic calculation:</p><formula xml:id="formula_0">f t+1 i,j = h k,s ({f t si+?i,sj+?j } 0??i,?j?k ),<label>(1)</label></formula><p>where k is the kernel size, s is the stride, and h k,s determines the layer type: matrix multiplication for convolutional layer, average or max operation for pooling layer, an element-wise nonlinear operation for activation function. When deep convolutional networks are constructed by stacking these basic components layer by layer, a network that only contains the nonlinear filter in Equation <ref type="formula" target="#formula_0">(1)</ref> is called fully convolutional network (FCN) <ref type="bibr" target="#b22">[23]</ref>. A FCN can be viewed as performing convolutional operation with a deep filter built by a series of local filters, whose receptive field is determined by the network connections. We can convert these successful classification convolutional architectures, for example AlexNet <ref type="bibr" target="#b18">[19]</ref>, ClarifaiNet <ref type="bibr" target="#b48">[49]</ref>, GoogLeNet <ref type="bibr" target="#b32">[33]</ref>, and VGGNet <ref type="bibr" target="#b30">[31]</ref>, into fully convolutional networks by replacing the top fully connected layers with convolutional layers. This replacement leads to two advantages: (i) It allows for input of arbitrary sizes and outputs the corresponding-sized semantic map. (ii) It is very efficient for processing images of large sizes compared with applying sliding window with these classical CNNs. Thanks to these advantages, we choose the architecture of fully convolutional networks for actionness estimation, with a loss function defined as follows:</p><formula xml:id="formula_1">(x, m; ?) = i,j (x, m ij ; ?),<label>(2)</label></formula><p>where x is the input, m is dense map we need to estimate, and ? is model parameter. The loss is a sum of each individual loss (x, m ij ; ?) at a specific location (i, j) over the spatial domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Actionness estimation</head><p>After the introduction of fully convolutional networks (FCNs), we now describe how to exploit this architecture for the problem of actionness estimation. Actionness essentially describes the likelihood of having an action instance at a certain location. The sizes of action instance vary for different videos and there may be multiple action instances in a single input video. Therefore, it is reasonable to treat the detection of actionness as a dense estimation problem, where the value at each location represents the confidence of containing an action instance there.</p><p>Action is defined as intentional bodily movement of biological agents. This definition contains two key elements: (i) "movement" and (ii) "agent". Bodily movement addresses motion procedure contained in action, while agent refers to the actor performing the action. According to this definition, two visual cues are crucial for estimating actionness, namely appearance and motion. The motion cues describe the visual patterns of bodily movement and the appearance cues capture the static image information about actors. Following the two-stream convolutional networks <ref type="bibr" target="#b29">[30]</ref> for action recognition, we propose a hybrid fully convolutional networks (H-FCN) for the task of actionness estimation, as illustrated in <ref type="figure">Figure 2</ref>. H-FCN is composed of two networks: Appearance fully convolutional network (A-FCN) and Motion fully convolutional network (M-FCN).</p><p>The appearance fully convolutional network (A-FCN) uses a single frame as input, which is a W ? H ? 3 volume. A-FCN aims to learn useful features from the appearance cues for actionness estimation. The input of motion fully convolutional network (M-FCN) is a stack of the optical flow fields from two consecutive frames, thus its size is W ? H ? 4. The goal of M-FCN is to extract effective representation from motion patterns. Hopefully, A-FCN and M-FCN capture visual cues from two different perspectives and the combination of them is expected to be more powerful due to their complementarity. The final estimated actionness map is an average of the two maps from A-FCN and M-FCN. Most of action datasets provide only the bounding boxes of action instances instead of the actor segmentation masks. The bounding boxes can be viewed as a kind of weak supervision for actionness map. We convert the annotations of bounding boxes into binary actionness maps, simply by setting the actionness value of pixels inside the bounding box as 1 and otherwise as 0. Although this weak supervision is not accurate, we observe that it is sufficient to perform H-FCN training from the experimental results in Section 4.</p><p>Specifically, the architectures of A-FCN and M-FCN are similar to each other except for the input layers, and the network details are shown in <ref type="figure">Figure 2</ref>. Basically, we adapt the successful ClarifaiNet <ref type="bibr" target="#b48">[49]</ref> to build our H-FCN. But we make three important modifications to make the network structure more suitable for the task of actionness estimation. First, we replace the fully connected layers (fc6, fc7, and fc8) with the convolutional layers (conv6, conv7, and conv8), where the kernel size is 1 ? 1 and convolutional stride is 1 ? 1. Second, we change the pooling stride from 2 ? 2 to 1 ? 1 after the 5 th convolutional layer. As our goal is to estimate the dense map of actionness, we need to reduce the down sampling ratio caused by the pooling operation. Third, the H-FCN output at each position is twodimensional, since we only need estimate the presence likelihood of an action instance. We choose cross-entropy loss in Equation <ref type="formula" target="#formula_1">(2)</ref> to train our H-FCN and the implementation details about network training can be found in Section 4.</p><p>Extension to multi-scale actionness estimation. The above description on actionness estimation is based on a single scale. However, in realist videos, action instances may vary in scales and we propose an effective and efficient method to handle the issue of scale variance. The fully convolutional nature of H-FCN allows for handling input images of different sizes and producing the actionness maps of corresponding sizes. As shown in <ref type="figure">Figure 2</ref>, we construct multi-scale pyramid representations of video frames and optical flow fields. We then feed these pyramid representations into H-FCN to obtain multi-scale actionness maps. Finally, these multi-scale actionness maps are resized to the same size and averaged to produce the final estimated maps. In practice, we use 4 scales for pyramid representations with scale set to 1/ ? 2, 1, ? 2, 2. It is worth noting that we just adopt this multi-scale actionness estimation during test phase of H-FCN and we only train H-FCN from a single scale determined by the ground truth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Application on action detection</head><p>In this subsection we will show how to use the estimated actionness maps for action detection in videos. Generally speaking, our estimated actionness maps can be viewed as new kind of features and can also benefit other relevant problems, such as action classification, actor tracking and so on. More specifically, we adopt an RCNN-alike <ref type="bibr" target="#b10">[11]</ref> action detection framework to verify the effectiveness of actionness maps. RCNN-alike action detection framework con- sists of two steps: generating action proposals and classifying the obtained proposals. Here we aim to design a more unified framework for action detection, where we produce action proposals based the outputs of our H-FCNs rather than using traditional proposal generation method like selective search <ref type="bibr" target="#b34">[35]</ref>. Action proposals. Based on actionness maps, we design a simple yet effective method to generate action proposals for each frame. Specifically, in our current implementation, we propose a non-maximum suppression (NMS) sampling method to produce boxes based on actionness map. As shown in <ref type="figure" target="#fig_2">Figure 3</ref>, we first resize the actionness map into scale 32 ? 32. Then, we use integral image representation to speed up the calculation of average actionness score in the bounding boxes of any sizes. Finally, we sample boxes according to their scores and the spatial overlaps between them. This NMS sampling method has two benefits: sampling boxes with high actionness scores and covering diverse locations.</p><p>Action classifiers. Regarding action classifiers, we choose the two-stream convolutional networks <ref type="bibr" target="#b29">[30]</ref> and adapt the pre-trained models to the specific classification task for the target dataset. For positive examples, we crop the frame regions or optical flow fields using the ground truth bounding boxes. For negative examples, we choose these action proposals that overlap less than 0.25 with ground truth regions. The last layer of two-stream convolutional networks has |A| + 1 outputs, classifying the action proposals into a pre-defined action category or a background class. At test time, we directly use the k th output of two-stream convolutional networks as the score of k th action detector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, we first introduce the evaluation datasets and their experimental settings. Then, we describe the implementation details of training H-FCNs. Finally, we evaluate our proposed method and perform comparison with other approaches on three tasks, namely actionness estimation, action proposal generation, and action detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>In order to evaluate our proposed method, we conduct experiments on both images and videos. Specifically, we choose three datasets, namely Stanford40 Actions dataset <ref type="bibr" target="#b44">[45]</ref>, UCF Sports dataset <ref type="bibr" target="#b25">[26]</ref>, and JHMDB dataset <ref type="bibr" target="#b13">[14]</ref>. The Stanford40 Action dataset contains 9, 532 images of human performing 40 diverse daily actions, such as ridingbike, playing guitar, calling, and so on. In each image, a bounding box is provided to annotate the actor. The whole dataset is divided into 4, 000 training images and 5, 532 testing images. We use these bounding boxes in training images to learn our A-FCN and the bounding boxes of testing images to evaluate the performance of trained model.</p><p>The UCF Sports dataset <ref type="bibr" target="#b25">[26]</ref> is composed of broadcast videos. It has 150 video clips and contains 10 action classes, such as diving, golfing, swinging, and so on. It provides the bounding boxes of actors for all the frames. The whole dataset is split into 103 samples for training and 47 samples for testing. We follow the standard split of training and testing to learn and evaluate our H-FCN.</p><p>The JHMDB dataset <ref type="bibr" target="#b25">[26]</ref> is a much larger dataset with full annotations of human joints and body masks, containing 928 videos and 21 action classes. The dataset provides three different splits of training and testing, and we report the average performance over these three splits. It should be noted that, like other datasets, we simply use the bounding boxes generated from the body masks as weak supervision to train and evaluate our H-FCN.</p><p>The UCF Sports and JHMDB are two large public datasets with bounding box annotations and actionness ground truth. Although these datasets contain temporally trimmed videos, they exhibit complex background and large intra-class variations. Therefore, estimating actionness on these realistic videos are still very challenging.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation details</head><p>In this subsection, we describe the training details of the H-FCN introduced in Section 3.2 and the two-stream action classifiers introduced in Section 3.3. Training deep convolutional networks is extremely challenging for these action datasets, as the their sizes are much smaller compared with that of the ImageNet dataset <ref type="bibr" target="#b4">[5]</ref>. Therefore, we choose the strategy of "supervised pre-training and careful fine-tuning" to relieve the over-fitting risk caused by small training data.</p><p>For appearance fully convolutional network (A-FCN), we choose the model pre-trained on the ImageNet dataset, which is released by paper <ref type="bibr" target="#b1">[2]</ref>. Then, we transfer the model parameters of convolutional layers to A-FCN and fine tune the network weights on the target dataset for actionness estimation. To reduce the risk of over-fitting, we fix the parameters of the first three convolutional layers and set the learning rate of fourth and fifth convolutional layer as 0.1 times of network learning rate. The learning rate for the whole network is set as 10 ?2 initially, decreased to 10 ?3 after 1K iterations, and to 10 ?4 after 2K iterations, and training is stopped at 3K iterations. The network weights are learned using the mini-batch (set to 100) stochastic gradient descent with momentum (set to 0.9). During training phase, we resize the training images as 224 ? 224 and their corresponding actionness map as 14 ? 14. For testing, we use the multi-scale pyramid representations of images to produce the multi-scale actionness maps as described in Section 3.2. These actionness maps from different scales are first up-sampled to that of original image and then averaged.</p><p>For motion fully convolutional network (M-FCN), the input is 3D volume of stacking two-frame optical flow fields. We choose the TVL1 optical flow algorithm <ref type="bibr" target="#b47">[48]</ref> and use OpenCV implementation, due to its balance between accuracy and efficiency. For fast computation, we discretize the values of optical flow fields into integers and set their range as 0-255 just like images. We choose to pre-train the M-FCN on the UCF101 dataset <ref type="bibr" target="#b31">[32]</ref>, which contains 13, 320 videos and 101 action classes. We first train the ClarifaiNet on UCF101 from scratch for the task of action recognition. As the dataset is relatively small, we use high dropout ratios to improve the generalization capacity of learned model (0.9 for fc6 layer and 0.8 for fc7 layer). The training procedure of ClarifaiNet on the UCF101 dataset is similar to that of two-stream convolutional networks <ref type="bibr" target="#b29">[30]</ref>. After the training on the UCF101 dataset, we transfer the weights of convolutional layers of ClarifaiNet to M-FCN, and fine tune the whole network on the target dataset for actionness estimation. The fine tuning procedure is the same with that of A-FCN.</p><p>The architecture of two-stream action classifier in Section 3.3 is the same with that of its original version <ref type="bibr" target="#b29">[30]</ref> except the final output layer. Specifically, we follow our previous works on action recognition with deep learning <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b41">42]</ref>, and the training procedure of two-stream action classifier on target dataset is the same with theirs. The training code with multi-GPU extension is publicly available 1 .  Method Stanford 40 UCF Sports JHMDB L-CORF <ref type="bibr" target="#b2">[3]</ref> 72.5% 60.8% 69.1% DPM <ref type="bibr" target="#b5">[6]</ref> 85.6% 54.9% 58.2% RankSVM <ref type="bibr" target="#b15">[16]</ref> 55  <ref type="table">Table 1</ref>. Evaluation of actionness estimation. We report mAP values on three datasets and compare with the previous methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Evaluation on actionness estimation</head><p>Evaluation protocol. We first evaluate the performance of our method on actionness estimation. Following <ref type="bibr" target="#b2">[3]</ref>, we select the mean average precision (mAP) to evaluate our approach. First, we plot 16?16 grids for images or 16?16?4 grids for videos. Then, we score the patch or cuboid of each grid using the average of actionness confidence in this patch or cuboid. The patch or cuboid is treated as positive sample if its intersection over union (IoU) with respect to the ground truth bounding box is larger than 0.5 threshold. Finally, based on the scores and labels of patches or cuboids, we plot precision-recall (PR) curve and report average precision (AP) as the area under this curve for each test sample. mAP is obtained by taking average over all the test samples.</p><p>Results. We conduct experiments on images (Stan-ford40) and videos (UCF Sports and JHMDB). We first study the effect of multi-scale pyramid representation of image on actionness estimation and the results are reported in <ref type="figure" target="#fig_4">Figure 4</ref>. From these results, we see that the actionness maps of different scales are complementary to each other and the combination of them is useful for improving performance. We also report the computational time of different scales on the right of <ref type="figure" target="#fig_4">Figure 4</ref>. Thanks to the CUDA implementation of Caffe toolbox <ref type="bibr" target="#b14">[15]</ref>, it is efficient and only requires about 30ms to process an image with multi-scale pyramid representations using Tesla K40 GPU. <ref type="table">Table 1</ref> shows the quantitative results of our method and the comparison with other approaches on three datasets. We only use A-FCN on the Stanford40 dataset as there is no motion information available in images. We separately investigate both M-FCN and A-FCN in videos, which  . We compare our method with previous actionness estimation approach (L-CORF) <ref type="bibr" target="#b2">[3]</ref>, Spatio-temporal object detection proposal (STODP) <ref type="bibr" target="#b24">[25]</ref>, objectness <ref type="bibr" target="#b0">[1]</ref>, DPM <ref type="bibr" target="#b5">[6]</ref> and random sampling. Best viewed in color.</p><p>are found complementary to each other. We first compare our method with previous actionness estimation method (L-CORF) <ref type="bibr" target="#b2">[3]</ref>. Our H-FCN outperforms L-CORF by around 7% to 20% on all these datasets, which indicates the effectiveness of fully convolutional networks. DPM <ref type="bibr" target="#b5">[6]</ref> is another important baseline for both images and videos. It obtains the best performance on the dataset of Stanford40, which implies agent detection is important for actionness estimation. However, the performance of DPM on video datasets is much lower than that of H-FCN. This result may be ascribed to the fact that the human pose variations in image dataset is much smaller than in video datasets. Besides, the DPM lacks considering motion information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Evaluation on action proposal generation</head><p>Evaluation protocol. Having evaluated the performance of H-FCNs on actionness estimation, we now apply action-ness maps to produce action proposals. In the current implementation, we generate action proposals for each frame independently and therefore we conduct evaluation in frame level. There have been several works on action proposal generation <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b45">46]</ref>, but there is no standard evaluation protocol to evaluate these different proposal generation algorithms. We follow a recent comprehensive study on object proposals <ref type="bibr" target="#b11">[12]</ref> and use proposal recall to measure the performance of action proposal methods. There are two kinds of measurements: (i) recall-number of proposal curve, which measures the detection rate versus the number of windows, with fixed IoU overlap threshold; (ii) recall-IoU overlap curve, which reports the detection rate versus IoU overlap, with fixed number of proposals. brush-hair catch clap climb golf jump kick-ball pick pour pullup push run shoot-ball shoot-bow shoot-gun sit stand swing-baseball throw walk wave mAP spatial-CNN <ref type="bibr" target="#b10">[11]</ref> 55.8 25.  <ref type="table">Table 2</ref>. Action detection results on the JHMDB dataset. We report frame-AP and video-AP for the spatial net (our s-net) and temporal net (our t-net), and their combination (our full net). We compare our method with the state-of-the-art performance <ref type="bibr" target="#b10">[11]</ref> on this dataset. mated actionness maps are very effective for producing action proposals. We only need to generate 10 boxes for each image on the dataset of Stanford40, and 4 boxes for each frame on the dataset of JHMDB, to obtain 0.9 recall at IoU above 0.5. For higher IoU threshold (0.7), our method still achieves 0.5-0.6 detection rate when producing 10 boxes for each image. We also separately report the performance of producing action proposals with the estimated maps by A-FCN and M-FCN on the dataset of JHMDB. We notice that A-FCN is better than M-FCN and the combination of them can further boost the performance. Next, we compare our method on action proposal generation with actionness estimation algorithm (L-CORF) <ref type="bibr" target="#b2">[3]</ref>, DPM <ref type="bibr" target="#b5">[6]</ref>, and objectness method <ref type="bibr" target="#b0">[1]</ref>. These three methods use the same NMS score sampling to produce bounding boxes and only differ in how to generate the confidence maps for sampling. From the results in <ref type="figure" target="#fig_6">Figure 6</ref>, we see that our method achieves comparable performance on images but much better performance on videos. Finally, we also compare our method with a recent action proposal method, namely STODP <ref type="bibr" target="#b24">[25]</ref>, on videos and our method outperforms this approach by a large margin. We also show several examples of actionness maps and action proposals in <ref type="figure" target="#fig_5">Figure 5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results. We conduct experiments on the datasets of</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Evaluation on action detection</head><p>Evaluation protocol. Finally, we evaluate the performance of action detection using our generated action proposals. Following a recent work on action detection <ref type="bibr" target="#b10">[11]</ref>, we choose two evaluation criteria: frame-AP and video-AP. Frame-AP measures the area under the precision-recall curve of the detection for each frame. A detection is correct if the IoU with ground truth at that frame is greater than 0.5 and the predicted label is correct. Video-AP measures the area under the precision-recall of the action tubes detection. A tube is correct if the mean of per-frame IoU value across the whole video is larger than 0.5 and the action label is correctly predicted.</p><p>Results. We use the generated action proposals of each frame in previous subsections and perform action classification on these proposals. We choose the two-stream convolutional networks <ref type="bibr" target="#b29">[30]</ref> as action classifiers due to their good performance on action recognition. As we generate action proposals for each frame independently, we first report the performance using frame-AP measurement and results are shown in <ref type="table">Table 2</ref>. We notice that temporal nets (t-net) outperform the spatial nets (s-net) on action detection, which is consistent with fact that temporal nets are better than spatial nets for action recognition <ref type="bibr" target="#b29">[30]</ref>. Next, we generate action tubes for the whole video and report the performance evaluated by video-AP. To generate action tubes, we resort to the same temporal linking method in <ref type="bibr" target="#b10">[11]</ref>. The linking algorithm jointly considers the overlaps between detected regions of consecutive frames and their detection scores, and seeks a maximum temporal path over the video. The performance regarding action tubes are shown in <ref type="table">Table 2</ref> and there is a significant improvement (around 15%) over frame based detection, which implies that the temporal structure is of great importance for action detection in videos. Finally, we compare our method with the state-of-the-art approach <ref type="bibr" target="#b10">[11]</ref> and our performance is better than theirs by about 3% for both frame-AP and video-AP evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>In this paper we have proposed a new deep architecture for efficient actionness estimation, called hybrid fully convolutional networks (H-FCN). H-FCN is composed of appearance FCN (A-FCN) and motion FCN (M-FCN), which incorporates the static and dynamic visual cues for estimating actionness, respectively. Our method obtained the stateof-the-art performance for actionness estimation on three challenging datasets. In addition, we applied our estimated actionness maps on action proposal generation and action detection, which further demonstrates the effectiveness of estimated actionness maps on relevant video analysis tasks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>An example of actionness maps. Our Hybrid-FCN (H-FCN) is composed of Appearance-FCN (A-FCN) and Motion-FCN (M-FCN). A-FCN captures appearance information from static RGB image, while M-FCN extract motion cues from optical flow fields. The two FCNs are complementary to each other for the task of actionness estimation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Procedure of generating action proposals. We design an NMS sampling method to generate action proposals based on actionness maps. We resize each map into 32 ? 32 and compute actionness score of any bounding boxes using integral image representation. Totally, there are 32 4 /2 possible boxes and sample proposal boxes according to their scores and the overlaps between them.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>1</head><label></label><figDesc>https://github.com/yjxiong/caffe</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>Exploration of multi-scale image representation for actionness estimation on the Standard40 dataset. Left: Performance of different scales and their combination. Right: Computational costs of different scales.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 .</head><label>5</label><figDesc>Examples of actionness maps and action proposals. We generate 5 action proposals for each image in this illustration. The first 4 images are from the dataset of Stanford 40 and the last 4 images are from the dataset of JHMDB. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 .</head><label>6</label><figDesc>Evaluation of action proposals on the dataset of Stanford 40 (top row) and JHMDB (bottom row)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>Stanford40 and JHMDB, and the results are shown in Figure 6 and Figure 5. From these results, we see that our esti-frame-AP(%)</figDesc></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>Yu Qiao is the corresponding author and partially supported by Guangdong Innovative Research Program (2015B010129013, 2014B050505017) and Shenzhen Research Program (KQCX2015033117354153, JSGG20150925164740726, CXZZ20150930104115529). In addition, we gratefully acknowledge support through the ERC Advanced Grant VarCity.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Measuring the objectness of image windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Alexe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Deselaers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Return of the devil in the details: Delving deep into convolutional nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chatfield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Actionness ranking with lattice conditional ordinal random fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Corso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Online video SEEDS for temporal window objectness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">V</forename><surname>Bergh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Roig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Boix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Manen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J V</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Ima-geNet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Object detection with discriminatively trained part-based models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Exploring semantic inter-class relationships (sir) for zeroshot action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Devnet: A deep event network for multimedia event detection and evidence recounting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-Y</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Recognizing an action using its name: A knowledge-based approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhuang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>IJCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Finding action tubes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">How good are detection proposals, really</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Hosang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Action localization with tubelets from motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Van Gemert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>J?gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bouthemy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">G M</forename><surname>Snoek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Towards understanding action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zuffi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Caffe: Convolutional architecture for fast feature embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno>abs/1408.5093</idno>
		<imprint>
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Making large scale svm learning practical</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Kernel Methods -Suport Vector Learning</title>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Large-scale video classification with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Event detection in crowded videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">ImageNet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">HMDB: A large video database for human motion recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Garrote</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Serre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Discriminative figure-centric models for joint action localization and recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<title level="m">On space-time interest points. IJCV</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Human action segmentation with hierarchical supervoxel consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Corso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Spatiotemporal object detection proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Oneata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Revaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Verbeek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Action MACH: a spatio-temporal maximum average correlation height filter for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Rodriguez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deeply learned attributes for crowded scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Slicing convolutional neural network for crowd video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Background subtraction for freely moving cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Javed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">UCF101: A dataset of 101 human actions classes from videos in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<idno>abs/1212.0402</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Spatiotemporal deformable part models for action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Selective search for object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R R</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">E A</forename><surname>Van De Sande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W M</forename><surname>Smeulders</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>IJCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Action recognition with improved trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Motionlets: Mid-level 3D parts for human motion recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Latent hierarchical model of temporal structure for complex activity classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Video action detection with relational dynamic-poselets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Action recognition with trajectory-pooled deep-convolutional descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">MoFAP: A multi-level representation for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>IJCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Towards good practices for very deep two-stream convnets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<idno>abs/1507.02159</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Recognize complex events from static images by fusing deep channels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Evaluation of super-voxel methods for early video processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Corso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Human action recognition by learning bases of action attributes and parts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Fast action proposals for human action detection and search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Discriminative subvolume search for efficient action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">A duality based approach for realtime tv-L 1 optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">29th DAGM Symposium on Pattern Recognition</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Visualizing and understanding convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Realtime action recognition with enhanced motion vector CNNs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
