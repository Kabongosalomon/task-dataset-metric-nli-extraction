<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Bi-Dimensional Feature Alignment for Cross-Domain Object Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Zhao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">DiDi Chuxing</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhong</forename><surname>Guo</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">DiDi Chuxing</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Carleton University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jieping</forename><surname>Ye</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">DiDi Chuxing</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Bi-Dimensional Feature Alignment for Cross-Domain Object Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T15:39+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>domain adaptation</term>
					<term>object detection</term>
					<term>style</term>
					<term>attention</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recently the problem of cross-domain object detection has started drawing attention in the computer vision community. In this paper, we propose a novel unsupervised cross-domain detection model that exploits the annotated data in a source domain to train an object detector for a different target domain. The proposed model mitigates the cross-domain representation divergence for object detection by performing cross-domain feature alignment in two dimensions, the depth dimension and the spatial dimension. In the depth dimension of channel layers, it uses inter-channel information to bridge the domain divergence with respect to image style alignment. In the dimension of spatial layers, it deploys spatial attention modules to enhance detection relevant regions and suppress irrelevant regions with respect to cross-domain feature alignment. Experiments are conducted on a number of benchmark cross-domain detection datasets. The empirical results show the proposed method outperforms the state-of-the-art comparison methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The deployment of supervised deep learning models has led to great advance in many computer vision tasks such as image classification <ref type="bibr" target="#b28">[29]</ref>, object detection <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b21">22]</ref>, and image segmentation <ref type="bibr" target="#b34">[35]</ref>. However, their success relies on the assumptions of standard supervised learning; that is, the deep models need to be trained with a sufficient amount of i.i.d. labeled samples that come from the same distribution as the test data. In practice, due to factors such as the collection means or weather conditions, the operational test dataset can be different from the training dataset, which can significantly degrade the performance of image analysis systems. For example, <ref type="figure" target="#fig_0">Fig. 1</ref> presents the direct deployment result of an object detector trained in one domain, Cityscapes, and applied in another domain, Foggy Cityscapes. It shows the detection model trained with images collected in normal weather fails to detect many objects on images collected in foggy weather. Although one can solve this problem by collecting labeled data from the same test dataset, the data annotation/labeling process is typically time-consuming and expensive. To avoid the expensive needs of repeatedly collecting labeled images, many unsupervised domain adaptation methods have been developed for image segmentation and classification tasks to overcome arXiv:2011.07205v1 [cs.CV] 14 Nov 2020 the cross-domain performance degradation <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b30">31]</ref>. However, much less effort has been devoted to the more complex cross-domain object detection task.</p><p>As a detector needs to identify both the objects and their precise locations in an image, it is more challenging to design an effective cross-domain detector than a cross-domain classifier. One early work for adaptive object detection <ref type="bibr" target="#b0">[1]</ref> adopts a domain adversarial feature alignment strategy at both the global image level and the proposal instance level. However, global image feature alignment is more effective to domain shifts over image appearances and textures, while unsuitable for handling cross-domain spatial distribution divergences. A more recent work <ref type="bibr" target="#b25">[26]</ref> improves adaptive detection by deploying strong cross-domain alignment at low-level features such as local textures/colors and weak alignment at high-level global image features. Nevertheless, this work still fails to explore the spatial properties of features which are essential for object detection.</p><p>In this paper, we propose a novel end-to-end deep learning model for crossdomain object detection by aligning features from the source and target domains in both the depth and spatial dimensions. Our assumption is that the image representation can be captured from the perspectives of both the semantic contents (e.g., the objects contained in the image) and the style of the image, and hence cross-domain feature alignment should be addressed from both aspects. Following previous work <ref type="bibr" target="#b8">[9]</ref>, we represent the style of an image using the inter-channel Gram matrix computed over features in the depth dimension of the feature map, which captures the correlations between the different filter responses along the spatial dimension, and can be adversarially aligned across domains. For crossdomain content feature alignment, we propose to use an attention module along the spatial dimension to enhance features in important regions (e.g., regions with objects) and suppress features in irrelevant background areas. This attention module not only will guide the domain adaption model to form a regionsensitive domain adversarial feature alignment, but also will be added into the feature representations of the backbone network to facilitate the consequent region proposal and local object classification steps of the detector. Overall the contribution of this work can be summarized as follows: (1) This is the first domain adaptation work that performs cross domain semantic content and style feature alignments separately and simultaneously in the spatial and depth dimensions. <ref type="bibr" target="#b1">(2)</ref> We deploy a novel spatial attention module to achieve target region sensitive cross-domain feature alignment. (3) We conduct extensive experiments on benchmark cross-domain detection datasets and the proposed model achieves the state-of-the-art performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Object Detection. The development of convolutional neural networks (CNN) has led to great advance in object detection. Traditional object detection methods use sliding windows and manual feature classification designs <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b6">7]</ref>. In recent years, a two-stage detection strategy based on region of interest (ROI) has gained wide applicability <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b24">25]</ref>. The early RCNN model <ref type="bibr" target="#b11">[12]</ref> uses selective search to generates a set of region proposals for object detection. Fast-RCNN <ref type="bibr" target="#b10">[11]</ref> improves RCNN by identifying region proposals and deploying ROI pooling on the convolutional feature map of CNN. Faster-RCNN <ref type="bibr" target="#b24">[25]</ref> combines Region Proposal Network (RPN) and Fast-RCNN to replace the previous selective search and further improve the detection performance. As a landmark detection model, Faster-RCNN provides the basis for many subsequent research studies <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b13">14]</ref>. This paper and many related unsupervised domain adaptive object detection methods also use Faster-RCNN as the backbone detection model.</p><p>Unsupervised Domain Adaptation. Unsupervised domain adaptation, which aims to train a model in a label-rich source domain for using in an unlabeled target domain, has attracted a lot of attention in the computer vision community. Many works have tried to learn cross-domain aligned feature representations <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b1">2]</ref>. The work in <ref type="bibr" target="#b7">[8]</ref> used a gradient reversal layer (GRL) to achieve the adversarial feature alignment operation. The authors of <ref type="bibr" target="#b22">[23]</ref> proposed a conditional adversarial domain adaptive method by using category predictions as an additional input for the domain discriminator. The work in <ref type="bibr" target="#b1">[2]</ref> used an image generation mechanism to achieve cross-domain transformation through image pixel-level alignment. There are also some studies that perform domain adaptation by minimizing various feature distribution distances between different domains, such as the maximum mean discrepancy(MMD) <ref type="bibr" target="#b5">[6]</ref> and the Wasserstein distance <ref type="bibr" target="#b27">[28]</ref>. However, most of these studies focus on image classification and segmentation tasks.</p><p>Domain Adaptation for Object Detection. Although there are many works on cross-domain image classification and segmentation, domain adaption for object detection has just begun to receive attention. One relatively early work <ref type="bibr" target="#b0">[1]</ref> proposed to align image-level features and instance-level features with adversarial domain adaptation strategy for adaptive object detection. The work in <ref type="bibr" target="#b15">[16]</ref> used image pixel-level transitions and pseudo-labeling to achieve cross-domain weakly supervised target detection. Another work <ref type="bibr" target="#b18">[19]</ref> used Cycle-GAN <ref type="bibr" target="#b36">[37]</ref> to generate multiple intermediate domain images between the source domain and the target domain to learn domain invariant representations. The work <ref type="bibr" target="#b25">[26]</ref> proposed a multi-level adversarial feature alignment strategy, global weak alignment and local strong alignment, to improve cross-domain detection performance.</p><p>Multi-level alignment is also adopted in <ref type="bibr" target="#b32">[33]</ref>, which aligns the distributions of local features and global features simultaneously. while another work <ref type="bibr" target="#b37">[38]</ref> focused on selective alignment of related areas. The work <ref type="bibr" target="#b35">[36]</ref> performs conditional adversarial global feature alignment with dual multi-label prediction. The authors of <ref type="bibr" target="#b14">[15]</ref> adopted the idea of layered alignment by adding proportional reduction and weighted gradient inversion layers to achieve domain invariance. Different from these existing methods, our proposed approach induce domain invariant features by enforcing not only multi-level alignments, but also multi-dimensional alignments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head><p>In this section, we consider the unsupervised cross-domain object detection problem, where the source domain contains fully labeled data and the data in the target domain is entirely unannotated. Let X s denote the fully annotated source domain data, such that</p><formula xml:id="formula_0">X s = {(x s i , b s i , c s i )} ns i=1 , where x s i denotes the i-th image,</formula><p>b s i and c s i represent the bounding box coordinates and the corresponding labels respectively for the objects contained in the i-th image. Let X t = {x t i } ns i=1 represent the unannotated images from the target domain. We aim to develop a good cross-domain detection method that trains an object detector on these available image resources to perform well in the target domain.</p><p>In this work we propose a bi-dimensional feature alignment method, a Style and Spatial Attention enhanced feature alignment method for Domain Adaptive detection (SSA-DA). The main idea of SSA-DA is to model the cross-domain style representation divergence and semantic content representation divergence separately by aligning image styles across domains in the depth dimension with style domain adaptive modules and aligning detection effective features in the spatial dimension with spatial attention enhanced domain alignment modules. SSA-DA adopts the widely used Faster-RCNN as the backbone detection network, while the feature alignments can be conducted in multiple layers of the feature extraction subnetwork. The overall structure of the proposed SSA-DA model is illustrated in <ref type="figure" target="#fig_1">Fig. 2</ref>. As shown in <ref type="figure" target="#fig_1">Fig. 2</ref>, a style domain adaptive module is added after the 3rd, 4th, and 5th convolution blocks separately, and a spatial attention enhanced domain alignment module is added after the 4th and 5th convolution blocks separately. The details of the two types of adaptive alignment modules and the overall learning problem will be introduced below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Depthwise Style Domain Adaptive Module</head><p>Image style is an important aspect of the image representation. Variations in image styles can hinder the cross-domain object detection performance. Following a previous work <ref type="bibr" target="#b8">[9]</ref>, we build the style representation of an image in a feature space that captures texture information, by calculating inter-channel feature correlations, i.e., correlations between different filter responses, in the feature map produced in any layer of the feature extraction network. Let the feature map obtained for a given image x after the l-th convolution block in the backbone of Faster-RCNN be expressed as Z l = F l (x) ? R C l ?H l ?W l , where C l denotes the number of channels (i.e., filters), H l and W l denote the spatial dimensions. Each channel contains the responses of the corresponding convolution filter of the current layer. To facilitate calculation, we transform Z l into a two-dimensional matrix f l ? R C l ?M l with M l = H l ? W l , such that the rows and columns of the matrix represent the channel and spatial dimensions respectively. Then the style features can be calculated as a Gram matrix G l ? R C?C , such that</p><formula xml:id="formula_1">G l ij = k f l ik f l jk<label>(1)</label></formula><p>Each entry G l ij captures the inter-channel correlations between the i-th and jth channels. For simplicity, we can further reshape G l into a vector form g l ? R C l2 ?1 , which contains the style features produced. Such style features capture the texture information but not the location arrangement.</p><p>To overcome the cross-domain style variation, we propose to align the style features across domains at a given layer with an adversarial domain adaptation mechanism based on the generative adversarial network (GAN), which can effectively align two distributions <ref type="bibr" target="#b12">[13]</ref>. As shown in <ref type="figure" target="#fig_1">Fig. 2</ref>, a style domain adaptive module is used to induce style-invariant feature representations at multiple convolution blocks. At the l-th block, a domain discriminator D l style is introduced to predict the domain of an input image style feature vector g l , with D l style (g l ) denoting the predicted probability of g l coming from the source domain. The feature alignment can be achieved through a min-max game between the feature extractor F l and the domain discriminator D l style :</p><formula xml:id="formula_2">min F l max D l style L l style = 1 2 (L ls style + L lt style ) (2) L ls style = E xs?Xs (1 ? D l style (g l s )) ? log(D l style (g l s )) (3) L lt style = E xt?Xt (D l style (g l t )) ? log(1 ? D l style (g l t ))<label>(4)</label></formula><p>Here we adopted the focal loss <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b25">26]</ref> in this adversarial training objective to give more weights to examples that are hard to classify by D l style , and ? is a modulation factor that controls the contribution degree of the hard examples. g l s represents the style feature vector generated from the convolutional feature map F l (x s ) from the source domain and g l t represents the style feature vector generated from F l (x t ) from the target domain. In the adversarial min-max game, the discriminator D l style tries to maximally discriminate the source domain features from the target domain features, where the feature extractor F l tries to induces style features such that the discriminator can be maximally confused.</p><p>The style representation is a multi-scale representation involving multiple layers of the deep network. The style features obtained from lower level layers reflect more pixel level information, while the style features from higher level layers reflect more image structural information <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b16">17]</ref>. Therefore, we apply adversarial style feature alignment on multiple convolution blocks to obtain stable and multi-scale style domain adaptation of image features. In particular, in the proposed model, one style domain adaptive module is added to each of the convolution block 3, block 4 and block 5 of the backbone network of the Faster-RCNN before the region proposal network (RPN). The overall multi-level style adversarial training loss L style can be summarized as follows:</p><formula xml:id="formula_3">L style = 5 l=3 min F l max D l style L l style (5)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Spatial Attention Domain Alignment Module</head><p>Image features that reflect the semantic contents of images are essential for object detection and it is important to bridge domain divergence over the image content features. In general, an object is usually localized into some local region in an image and the detector only needs to recognize the relevant regions. Therefore directly aligning the global image features across domains might not be the most suitable strategy as these features can be dominated by irrelevant regions, especially when the objects are small. To address this problem, we propose a spatial attention domain alignment module, as shown in <ref type="figure" target="#fig_1">Fig. 2</ref>, which learns spatial attention to enhance features from the semantically relevant regions. Specifically, given the feature map produced from the l-th convolution block, Z l = F l (x), we follow the method of <ref type="bibr" target="#b31">[32]</ref> to generate a spatial attention map ? l ? R 1?H l ?W l using an attention network A l , such that ? l = A l (Z l ), where A l is a convolution network with filters of 7 ? 7. Then the spatial attention enhanced feature map can be produced as Z l ? =? l ? Z l , where ? denotes a replicated element-wise product operator that multiplies ? l to each channel of Z l . It is expected that the learned spatial attention can enhance features in relevant regions of Z l and diminish features in irrelevant regions.</p><p>Given the spatial attention enhanced feature map Z l ? , we introduce a domain discriminator D l att to perform adversarial cross-domain feature alignment. Similar to the adversarial alignment on style features, we use focal loss in the adversarial objective, such that the min-max adversarial optimization problem is formulated as:</p><formula xml:id="formula_4">min F l max D l att L l att = 1 2 (L ls att + L lt att )<label>(6)</label></formula><formula xml:id="formula_5">L ls att = E xs?Xs (1 ? D l att (Z l ? s )) ? log(D l att (Z l ? s ))<label>(7)</label></formula><formula xml:id="formula_6">L lt att = E xt?Xt (D l att (Z l ? t )) ? log(1 ? D l att (Z l ? t ))<label>(8)</label></formula><p>where ? is a modulation factor hyperparameter for the focal loss; Z l ? s and Z l ? t are the spatial attention enhanced feature maps from the l-th block for the source domain image x s and the target domain image x t respectively. Moreover, as previous work <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b16">17]</ref> has shown effective semantic information that can characterize image content is often available in the deep layers of a convolutional network instead of the shallow layers, we propose to perform multi-level spatial attention enhanced domain alignment on the last two convolution blocks of the Faster-RCNN, i.e., block 4 and block 5 in <ref type="figure" target="#fig_1">Fig. 2</ref>. The overall spatial attention enhanced adversarial training loss L att can be written as follows:</p><formula xml:id="formula_7">L att = 5 l=4 min F l max D l att L l att<label>(9)</label></formula><p>In this min-max adversarial training, the feature extraction network {F 4 , F 5 } will tries to maximally confuse the domain discriminators {D 4 att , D 5 att } and align the spatial attention enhanced features across domains.</p><p>Meanwhile, the spatial attention enhanced feature map Z l ? at each block will be used as the input for the next block along the backbone of the detection network. The attention enhanced feature map, Z 5 ? , at the last convolution block, block 5, will be provided to the region proposal network (RPN) to produce region proposals and perform object classification and bounding box regression. The object detection loss L det in the source domain can be expressed as:</p><formula xml:id="formula_8">L det = 1 n s ns i=1 L cr (R(Z 5 ?i ), (b s i , c s i ))<label>(10)</label></formula><p>where R denotes the combined function for the RPN, region classification and regression modules of the Faster-RCNN, and L cr represents all the supervised classification loss and regression loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Overall Adversarial Learning</head><p>We combine object detection loss L det , style adversarial training loss L style , and the spatial attention enhanced adversarial training loss L att together to form the following overall adversarial learning objective:</p><formula xml:id="formula_9">L all = L det + ?L style + ?L att<label>(11)</label></formula><p>where ? and ? are the trade-off parameters to balance different loss terms. This overall learning problem minimizes the detection loss on the labeled source domain data, while bridging the representation gap between the source and target domains from both the style and content perspectives. SGD optimization algorithm is used to perform training, while GRL [8] is adopted to implement the gradient sign flip for the domain discriminator update.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>To evaluate the proposed SSA-DA model, we conducted experiments on benchmark cross-domain detection tasks in three cross-domain variation scenarios: (1) Normal to foggy weather variation. In this scenario, we used the cross-domain detection task of adapting from Cityscapes <ref type="bibr" target="#b3">[4]</ref> to Foggy Cityscapes <ref type="bibr" target="#b26">[27]</ref>. <ref type="formula">(2)</ref> Virtual to real scene variation. The adaptive detection task from SIM-10K <ref type="bibr" target="#b17">[18]</ref> to Cityscapes <ref type="bibr" target="#b3">[4]</ref> is used in this scenario. (3) Cross-camera situation. We used data collected with two different cameras to form the cross-domain detection task from KITTI <ref type="bibr" target="#b9">[10]</ref> and Cityscapes <ref type="bibr" target="#b3">[4]</ref>. We compared our proposed model with the state-of-the-art cross-domain detection methods. In this section, we present our experimental results and discussions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Setup</head><p>We followed the same experimental setup as in <ref type="bibr" target="#b0">[1]</ref>. The VGG16 <ref type="bibr" target="#b28">[29]</ref> model is used as the backbone of the Faster-RCNN detection model and pre-trained on ImageNet. We set the momentum as 0.9, the weight decay as 0.0005, and the total training epoch number as 20. The domain discriminator D l style has three fully connected layers, while the discriminator D l att has one convolutional layer and two fully connected layers. For the hyperparameters involved in the proposed method, we set ? = 1, set ? to 5 on all blocks, and set ? to 5 on block 5, and 4 on block 4. For all the experiments, we used the mean average precision (mAP) with a threshold of 0.5 to evaluate the results. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Normal to Foggy Weather Adaptation</head><p>For object detection in real road scenarios, weather condition is a common factor that affects the detection performance. The change of weather conditions can lead to large visual variations in images and videos, which presents an obvious domain shift situation for the deployment of object detectors. To test the proposed adaptive detection model in this scenario, we used the cross-domain detection task from Cityscapes <ref type="bibr" target="#b3">[4]</ref> to Foggy Cityscapes <ref type="bibr" target="#b26">[27]</ref>. These two datasets have eight object categories: person, rider, car, truck, bus, train, motorcycle and bicycle. Foggy Cityscapes is a fog dataset synthesized from Cityscapes, which can simulate the fog weather condition in real scenes. The Cityscapes are used as the source domain, and the training set of Foggy Cityscapes is used as the target domain. We set the hyperparameter ? = 0.5 in the experiment, and report detection results for all categories on the Foggy Cityscapes validation set.</p><p>We compared the proposed SSA-DA method with seven state-of-the-art crossdomain detection methods and one baseline source-only training method. The comparison results are reported in <ref type="table" target="#tab_0">Table 1</ref>. As the baseline Source-only is only trained in the source domain without handling the domain shift problem, we can see that all the other domain adaptive detection methods outperform Sourceonly. By having both style and spatial attention enhanced feature alignments, the proposed SSA-DA greatly improves the cross-domain detection performance, far exceeding all other comparison methods. It outperforms the Source-only baseline by 19.1% in terms of the average mAP. As the domain shift is caused by the fog in this task, there is a significant stylistic difference across domains. We can see that by using the style domain adaptive component (SD) alone in SSA-DA, it has already outperformed the best comparison method by 2.9% in terms of average mAP. Meanwhile by using only the spatial attention enhanced feature <ref type="table">Table 2</ref>. Detection results on adaptation from SIM-10k to Cityscapes. SD and SA denote the two major components of the proposed SSA-DA: SD denotes style domain adaptive component, and SA denotes spatial attention enhanced feature alignment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>SD SA AP of Car Source-only 34.3 BDC-Faster <ref type="bibr" target="#b25">[26]</ref> 31.8 DA-Faster <ref type="bibr" target="#b0">[1]</ref> 39.0 MAF <ref type="bibr" target="#b14">[15]</ref> 41.1 SW-DA <ref type="bibr" target="#b25">[26]</ref> 40.1 SW-DA(?=3) <ref type="bibr" target="#b25">[26]</ref> 42.3 SC-DA(Type3) <ref type="bibr" target="#b37">[38]</ref> 43.0 Dense-DA(n=6) <ref type="bibr" target="#b32">[33]</ref> 42 alignment component (SA), SSA-DA works very well in the 'truck' category, where all the other comparison methods have poor performance. This suggests that the 'truck' object under the condition of fog is very difficult to capture, while the spatial attention mechanism can help mitigate the problem. However, excessive attention alone may have a negative impact on the category of 'train', while the style feature alignment component can help to mitigate this drawback. Overall, by integrating both the SD and SA components, the proposed SSA-DA approach demonstrates great performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Virtual to Real Scene Adaptation</head><p>As it is difficult to collect annotated data in many application tasks, it is a good option to use computer-generated labeled virtual image data for training models. However due to the visual difference between the virtual data and real data, the performance of the detection model trained on the virtual data can severely degrade when applying to the real data, hence cross-domain detection techniques are important. In this experiment, we tested the proposed SSA-DA method on the domain adaptive detection task from virtual scenes to real scenes. In particular, we adopted the virtual scene dataset SIM-10K <ref type="bibr" target="#b17">[18]</ref> as the source domain, and took the real scene dataset Cityscapes <ref type="bibr" target="#b3">[4]</ref> as the target domain, while using the car category detection as the domain adaptive detection task. All training images from both domains were used during training, test evaluation was conducted on the validation set of Cityscapes. Following <ref type="bibr" target="#b0">[1]</ref>, we set the tradeoff hyperparameter ? = 0.1. Same as above, we compared the proposed SSA-DA with both the Sourceonly baseline and a number of state-of-the-art methods which were tested on this cross-domain detection task. The comparison results are reported in Ta- <ref type="table">Table 3</ref>. Detection results of cross camera adaptation from KITTI and Cityscapes. SD and SA denote the two major components of SSA-DA: SD denotes style domain adaptive component, and SA denotes spatial attention enhanced feature alignment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>SD SA AP of Car Source-only 30.2 DA-Faster <ref type="bibr" target="#b0">[1]</ref> 38.5 SC-DA(Type3) <ref type="bibr" target="#b37">[38]</ref> 42.5 MAF <ref type="bibr" target="#b14">[15]</ref> 41.0 SSA-DA 42.6 42.2 43.3 ble 2. We can see that our proposed SSA-DA improves the performance of the Source-only baseline model by 9.5%, and exceeds the best results of all the other cross-domain detection models. It can also be observed that even with only one of the two components, SD and SA, SSA-DA can still reach a good performance level and outperform some of the latest methods. In addition, we can also observe that the SC-DA(Type3) method produces the best result among the other comparison method and it outperforms the latest Dense-DA method. Meanwhile, SC-DA(Type3) also demonstrates an obvious advantage over other comparison methods in the category 'car' in the experiment of Section 4.2. This validates that SC-DA(Type3) is more suitable for small vehicle detection. Nevertheless, our proposed SSA-DA outperforms SC-DA(Type3). These results suggest that the proposed SSA-DA is very effective for cross-domain detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Cross-Camera Adaptation</head><p>Due to the variations in camera equipments and collection scenes, real road condition data acquired under similar weather conditions can also have a domain shift problem. In this experiment, we used two real datasets, KITTI <ref type="bibr" target="#b9">[10]</ref> and Cityscapes <ref type="bibr" target="#b3">[4]</ref>, to study cross-domain object detection under cross-camera variations. Following <ref type="bibr" target="#b0">[1]</ref>, we used the KITTI dataset as the source domain, used the Cityscapes training set as the target domain, and evaluated the performance of adaptive detection models on the validation set of Cityscapes with the category 'car'. The experimental results are reported in <ref type="table">Table 3</ref>. We can see that the proposed SSA-DA method produced the best result, which is 13.1% higher than the baseline, and outperforms even the more complex SC-DA(Type3) models that are suitable for automotive inspection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Qualitative Result Visualization</head><p>In addition to the quantitative results reported above, we present an example of the qualitative adaptive detection results in <ref type="figure" target="#fig_2">Fig. 3</ref>. We can see that the Sourceonly baseline can only detect objects within close range and missed most objects far away. DA-Faster was able to detect cars that were a little further away, but it mistakenly classified the motorcycle, and missed the rider, person, as well as many other objects. The proposed SSA-DA model correctly detected motorcycle and rider, and detected more person and car objects in the dense fog.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Analysis of the SSA-DA Model</head><p>The proposed SSA-DA model has two major components, the style domain adaptive module (SD) and the spatial attention enhanced feature alignment module (SA). In this section, we will analyze the impact of these two components at multi-levels on the performance of the SSA-DA model using the adaptive detection task from Cityscapes to Foggy Cityscapes, as well as investigate the impact of the hyperparameters on the components and the full model.</p><p>Impact of the Component Modules. We use SD-DA to denote the variant of SSA-DA that drops the SA modules and keeps only the SD modules. Similarly, we use SA-DA to denote the variant of SSA-DA that drops the SD modules and keeps only the SA modules. Both modules are applied on multiple blocks (block 3, 4 and 5) of the backbone detection network, we hence further conducted ablation study to investigate their impacts on lower levels of convolution layers and upper levels of convolution layers. As the style features obtained at the lower levels of the network reflect more detailed pixel level information and the style features at the higher level reflect smooth structural information, we performed ablation study on SD-DA by adding the SD module from the lower level blocks to the higher level blocks. Semantic content information however is more prevalent at higher levels of the extraction network. Hence we performed ablation study on SA-DA by adding the SA module from the higher level blocks to the lower level blocks. The experimental results are reported in <ref type="table" target="#tab_2">Table 4</ref>. We can see that the overall performance of the SD-DA gradually increase by adding the SD module into higher level blocks. This proves that simultaneous style feature alignments at multiple blocks from the low level to the high level of the network can benefit the domain adaptation performance. Meanwhile, for SA-DA, adding the SA module to the low level block 3 actually degrades its overall performance. It has a significant negative impact on the 'train' category. This suggests that with spatial attention, it is more suitable to conduct semantic content feature alignment at higher levels of the network.</p><p>Parameter Sensitivity on ? and ?. These two parameters are modulation factors for the focal loss used in the adversarial alignment of style features (SD module) and spatial content features respectively (SA module). As they are separately involved in the two modules, we conducted sensitivity experiments for ? and ? using SD-DA and SA-DA respectively. As the style feature alignment has been shown to be useful at both low and high level blocks, we set ? to the same value for the SD modules added to block 3, 4 and 5. We tested the SD-DA variant by varying the ? within the range of values <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6]</ref>, and the results are reported in <ref type="figure">Fig. 4(a)</ref>. We can see SD-DA outperforms Dense-DA for different ? values, especially when 3 ? ? ? 5, while the best result is achieved with ? = 5.</p><p>From previous experiments, we can see that the SA module is more suitable for higher level blocks. Hence here we separately investigated the best ? value for the SA module used in block 4 and block 5. First we use a variant, SA-DA(5), which only adds SA module to block 5, to conduct sensitivity experiments by varying ? within the range of values <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6]</ref>. The results are reported in <ref type="figure">Fig. 4(b)</ref>. We can see adding SA module only to block 5 leads to degraded results comparing to the full SA-DA. The performance varies with different ? values while the best result is gained with ? = 5. Then we fixed ? = 5 for the SA module in block 5, and tested the full SA-DA method by varying ? in block 4 within the range of values <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5]</ref>. The results are reported in <ref type="figure">Fig. 4(c)</ref>. We can see that when ? = {2, 3, 4} on block4, the performance remains at a high level, but will drop when ? continues to increase. Overall, these results suggest ? and ? should not be set to big values as they may overfit the hard examples. A value no larger than 5 would be more suitable.</p><p>Parameter Sensitivity on ? and ?. In addition, we conducted experiments on the complete model SSA-DA by adjusting the trade-off parameters ? and ? in Eq.(11). These two parameters control the weight of style domain adaption (SD) and spatial attention enhanced feature alignment (SA) in the detection model. To vary the ? value, we fixed ? = 0.5; to vary the ? value, we fixed ? = 1. The sensitivity results are reported in <ref type="figure">Fig. 5</ref>. We can see that although the performance varies with different ? and ? values, the performance in general is superior to Dense-DA and DA-Faster for most of range of values, while the best results are obtained when ?=1 and ?=0.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we proposed a novel style and spatial attention enhanced bidimensional feature alignment method for domain adaptive detection (SSA-DA). The proposed method deploys two important modules, the style domain adaptive module and the spatial attention enhanced domain alignment module, at multilevels to align features in both the depth and spatial dimensions across domains. With both the style and spatial attention enhanced content feature alignments, the detector trained in the source domain can be more adaptive to the target domain. We conducted experiments on benchmark datasets in three different cross-domain variation scenarios. The experimental results demonstrated the proposed model achieved the state-of-the-art adaptive detection performance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Example of deployment of supervised object detector with different training and test datasets, Cityscapes and Foggy Cityscapes. (a) Labeled image example from the training set, Cityscapes. (b) The detection result on an image in Foggy Cityscapes using the detector trained on Cityscapes. (c) The ground-truth annotation of the Foggy Cityscapes example. This example shows that weather-induced domain gaps can lead to performance degradation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>The model structure of the proposed multi-dimensional domain adaptive object detection method, the Style and Spatial Attention enhanced feature alignment method for Domain Adaptive detection (SSA-DA). The proposed model uses Faster-RCNN as its backbone network and has two major adaptive components: the style domain adaptive module and the spatial attention enhanced domain alignment module.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Qualitative results of adaptation from Cityscapes to Foggy Cityscapes. (a) Annotated image in Cityscapes. (b) Detection results of Source-only in the target domain. (c) Detection results of DA-Faster. (d) Detection results of SSA-DA. The blue boxes show ground-truth and the green boxes show the detection results.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .Fig. 5 .</head><label>45</label><figDesc>Parameter sensitivity on ? and ? with adaptation from Cityscapes to Foggy Cityscapes. (a): Sensitivity results over ? with SD-DA. (b): Sensitivity results over ? with SA-DA(5). (c): Sensitivity results over ? with SA-DA. Parameter sensitivity analysis on ? and ? with the adaptation from Cityscapes to Foggy Cityscapes. (a): Sensitivity results over ?. (b): Sensitivity results over ?.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Detection results on the validation set of the Foggy Cityscapes. SD and SA denote the two major components of the proposed SSA-DA: SD denotes style domain adaptive component, and SA denotes spatial attention enhanced feature alignment.</figDesc><table><row><cell>Method</cell><cell cols="2">SD SA person rider car truck bus train mcycle bicycle mAP</cell></row><row><cell>Source-only</cell><cell>25.1 32.7 31.0 12.5 23.9 9.1 23.7</cell><cell>29.1 23.4</cell></row><row><cell>BDC-Faster [26]</cell><cell>26.4 37.2 42.4 21.2 29.2 12.3 22.6</cell><cell>28.9 27.5</cell></row><row><cell>DA-Faster [1]</cell><cell>25.0 31.0 40.5 22.1 35.3 20.2 20.0</cell><cell>27.1 27.6</cell></row><row><cell>SC-DA(Type3) [38]</cell><cell>33.5 38.0 48.5 26.5 39.0 23.3 28.0</cell><cell>33.6 33.8</cell></row><row><cell>MAF [15]</cell><cell>28.2 39.5 43.9 23.8 39.9 33.3 29.2</cell><cell>33.9 34.0</cell></row><row><cell>SW-DA [26]</cell><cell>29.9 42.3 43.5 24.5 36.2 32.6 30.0</cell><cell>35.3 34.3</cell></row><row><cell>DD-MRL [19]</cell><cell>30.8 40.5 44.3 27.2 38.4 34.5 28.4</cell><cell>32.2 34.6</cell></row><row><cell>Dense-DA [33]</cell><cell>33.2 44.2 44.8 28.2 41.8 28.7 30.5</cell><cell>36.5 36.0</cell></row><row><cell></cell><cell>33.3 46.2 44.0 31.1 47.7 36.4 36.1</cell><cell>36.4 38.9</cell></row><row><cell>SSA-DA</cell><cell>32.7 47.5 44.9 36.2 43.7 23.4 38.0</cell><cell>36.5 37.8</cell></row><row><cell></cell><cell cols="2">33.9 48.3 47.7 35.7 52.0 44.7 39.6 37.9 42.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 .</head><label>4</label><figDesc>Detection results of SD-DA and SA-DA with deployment on different blocks.</figDesc><table><row><cell cols="3">Method block5 block4 block3 person rider car truck bus train mcycle bicycle mAP</cell></row><row><cell></cell><cell>32.3 44.4 43.7 28.8 42.6 22.7 33.3</cell><cell>38.2 35.8</cell></row><row><cell>SD-DA</cell><cell>32.5 45.7 43.8 26.8 49.6 30.0 33.5</cell><cell>37.5 37.4</cell></row><row><cell></cell><cell>33.3 46.2 44.0 31.1 47.7 36.4 36.1</cell><cell>36.4 38.9</cell></row><row><cell></cell><cell>30.6 40.0 40.5 26.4 37.4 27.3 30.0</cell><cell>33.4 33.2</cell></row><row><cell>SA-DA</cell><cell>32.7 47.5 44.9 36.2 43.7 23.4 38.0</cell><cell>36.5 37.8</cell></row><row><cell></cell><cell>33.9 47.2 44.9 35.5 41.2 11.9 40.0</cell><cell>35.6 36.3</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Domain adaptive faster R-CNN for object detection in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sakaridis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Self-ensembling with gan-based data augmentation for domain adaptation in semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kim</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Unsupervised domain adaptation via regularized conditional alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cicek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Soatto</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.10885</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Histograms of oriented gradients for human detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Training generative neural networks via maximum mean discrepancy optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">K</forename><surname>Dziugaite</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>UAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Object detection with discriminatively trained part-based models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1627" to="1645" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ustinova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ajakan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Germain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Laviolette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Marchand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
		<title level="m">Domain-adversarial training of neural networks</title>
		<imprint>
			<publisher>JMLR</publisher>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Image style transfer using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Gatys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Ecker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bethge</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Are we ready for autonomous driving? the kitti vision benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<title level="m">Fast R-CNN. In: ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<title level="m">Generative adversarial nets</title>
		<imprint>
			<publisher>NIPS</publisher>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Multi-adversarial faster-rcnn for unrestricted object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Cross-domain weakly-supervised object detection through progressive domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Inoue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Furuta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yamasaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Aizawa</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Perceptual losses for real-time style transfer and super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Driving in the matrix: Can virtual worlds replace human-generated annotations for real world tasks?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Johnson-Roberson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Barto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">N</forename><surname>Sridhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Rosaen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vasudevan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>ICRA</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Diversify and match: A domain adaptive representation learning paradigm for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kim</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Ssd: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Conditional adversarial domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>NIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Faster R-CNN: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>NIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Strong-weak distribution alignment for adaptive object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ushiku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Harada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Semantic foggy scene understanding with synthetic data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sakaridis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>IJCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Wasserstein distance guided representation learning for domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Learning to adapt structured output space for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">H</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">C</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schulter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chandraker</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Domain adaptation for structured output via discriminative representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">H</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schulter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chandraker</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Cbam: Convolutional block attention module</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Woo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">S</forename><surname>Kweon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Multi-level domain adaptive learning for cross-domain detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Curriculum domain adaptation for semantic segmentation of urban scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gong</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Adaptive object detection with dual multi-label prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ye</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Unpaired image-to-image translation using cycle-consistent adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Adapting object detectors via selective cross-domain alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
