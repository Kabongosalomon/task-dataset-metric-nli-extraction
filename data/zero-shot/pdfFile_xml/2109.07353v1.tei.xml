<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">IEEE TRANSACTIONS ON IMAGE PROCESSING 1 Learning Dynamical Human-Joint Affinity for 3D Pose Estimation in Videos</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junhao</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yali</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhipeng</forename><surname>Zhou</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyu</forename><surname>Luan</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Senior Member, IEEE</roleName><forename type="first">Yu</forename><forename type="middle">Qiao</forename></persName>
						</author>
						<title level="a" type="main">IEEE TRANSACTIONS ON IMAGE PROCESSING 1 Learning Dynamical Human-Joint Affinity for 3D Pose Estimation in Videos</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T11:15+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-3D Human Pose Estimation</term>
					<term>Graph Convolu- tion</term>
					<term>Deep Learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Graph Convolution Network (GCN) has been successfully used for 3D human pose estimation in videos. However, it is often built on the fixed human-joint affinity, according to human skeleton. This may reduce adaptation capacity of GCN to tackle complex spatio-temporal pose variations in videos. To alleviate this problem, we propose a novel Dynamical Graph Network (DG-Net), which can dynamically identify human-joint affinity, and estimate 3D pose by adaptively learning spatial/temporal joint relations from videos. Different from traditional graph convolution, we introduce Dynamical Spatial/Temporal Graph convolution (DSG/DTG) to discover spatial/temporal humanjoint affinity for each video exemplar, depending on spatial distance/temporal movement similarity between human joints in this video. Hence, they can effectively understand which joints are spatially closer and/or have consistent motion, for reducing depth ambiguity and/or motion uncertainty when lifting 2D pose to 3D pose. We conduct extensive experiments on three popular benchmarks, e.g., Human3.6M, HumanEva-I, and MPI-INF-3DHP, where DG-Net outperforms a number of recent SOTA approaches with fewer input frames and model size.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>T HIS paper focuses on the problem of 3D pose estimation in videos. It is an important computer vision task, due to its wide applications in video surveillance, virtual reality, etc. With rapid development of deep learning, this task has achieved remarkable progresses <ref type="bibr" target="#b0">[1]</ref>- <ref type="bibr" target="#b2">[3]</ref>. In particular, recent studies <ref type="bibr" target="#b3">[4]</ref>- <ref type="bibr" target="#b5">[6]</ref> have shown that, graph convolution networks can effectively boost 3D pose estimation in videos, by learning spatio-temporal relations from 2D pose. However, spatial/temporal graph convolutions in these models are built upon the fixed human-joint affinity that is defined by human skeleton. As a result, it is often limited to tackle depth ambiguity and/or motion uncertainty of human joints in videos, without taking dynamical pose variations into account. We use two examples in <ref type="figure">Fig.1</ref> to illustrate this problem.</p><p>First, we discuss spatial graph convolution by a SittingDown case at Frame t. As shown in <ref type="figure">Fig.1(a)</ref>, wrist is connected with elbow in the human skeleton. Based on such fixed spatial affinity, spatial graph convolution leverages elbow as the main context to update wrist. But, elbow stays at a higher position that is relatively far from wrist in this SittingDown case.</p><p>Consequently, when integrating context from elbow, spatial graph convolution would lift wrist to an unsatisfactory position that is higher than its ground truth. In fact, wrist is spatially closer to hip and pelvis in this SittingDown case. Hence, it is necessary to further exploit such spatial affinity and learn context from these joints for estimating 3D position of wrist.</p><p>Second, we discuss temporal graph convolution by a Walking case from Frame t to t + 1. According to the fixed temporal affinity in <ref type="figure">Fig.1(b)</ref>, ankle at Frame t is connected with the same joint ankle at Frame t+1. In this case, temporal graph convolution leverages ankle at Frame t + 1 as temporal context for estimating ankle at Frame t. But, ankle at Frame t + 1 is moving forward. Only integrating such context would mislead temporal graph convolution to estimate ankle at Frame t to be forward. To capture how ankle moves correctly, it would be better to use other joints with similar motion as contextual guidance. In fact, in this Walking case, ankle keeps the consistent movement with wrist and shoulder from Frame t to Frame t+1. To reduce motion uncertainty, it is necessary to further enhance such temporal affinity, and integrate contexts from wrist and shoulder at Frame t + 1 for estimating 3D position of ankle at t.</p><p>Both cases show that, the fixed human-joint affinity often lacks adaptation capacity to describe the personalized pose dynamics in videos. To alleviate this difficulty, we propose a novel Dynamical Graph Network (DG-Net), which can dynamically adjust spatial/temporal human-joint affinity in videos, and adaptively integrate joint-dependent context for accurate 3D pose estimation. More specifically, we first design two dynamical graph convolution operations, i.e., Dynamical Spatial Graph (DSG) and Dynamical Temporal Graph (DTG) convolution. DSG/DTG can dynamically construct human-joint affinity, according to similarity of spatial distance/temporal movement between joints. Hence, they can capture robust human-joint relations to tackle pose variations in different videos. Second, we embed DSG and DTG into a dynamical graph convolution block, and cascade a number of such blocks progressively to build up DG-Net. Via adding 3D pose supervision on individual blocks and fusion of different blocks, our DG-Net can regularize the cooperative power of spatial and temporal joint relations in each block, and integrate complementary features in different blocks to boost 3D pose estimation in videos. Finally, we perform extensive experiments on widely-used human pose benchmarks, i.e., Human3.6M, HumanEva-I, and MPI-INF-3DHP. Our DG-Net outperforms a number of recent SOTA approaches. arXiv:2109.07353v1 [cs.CV] 15 Sep 2021 <ref type="figure">Fig. 1</ref>: Our motivations. The fixed human-joint affinity often limits adaptation capacity of spatial/temporal graph convolution to tackle complex pose variations in videos. To alleviate such difficulty, the graph affinity should depend on the specific human pose in the video. To achieve this goal, we propose to learn spatial/temporal human-joint affinity dynamically, according to spatial/temporal similarity between joints. For example, wrist is spatially closer to hip and pelvis in the SittingDown case of <ref type="figure">Fig.1(a)</ref>. Hence, we should further enhance such spatial affinity to estimate wrist at Frame t. Similarly, ankle keeps the consistent movement with wrist and shoulder in the Walking case. Hence, we should further enhance such temporal affinity to estimate ankle at Frame t. More explanations can be found in the introduction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>2D Pose Estimation. 2D human pose estimation has been well studied in the decades. Traditionally, it is addressed by hand-craft features and pictorial structures <ref type="bibr" target="#b6">[7]</ref>. With fast development of deep learning, 2D pose estimation have achieved remarkable successes by using Convolutional Neural Networks (CNNs) <ref type="bibr" target="#b7">[8]</ref>- <ref type="bibr" target="#b10">[11]</ref>. For example, <ref type="bibr" target="#b8">[9]</ref> firstly introduces an end-toend CNN to solve this problem by regressing human pose. <ref type="bibr" target="#b7">[8]</ref> designs a multi-stage architecture which estimates human pose in a coarse-to-fine manner. <ref type="bibr" target="#b10">[11]</ref> proposes a hourglass network to utilize multi-scale features to capture human pose in the images. <ref type="bibr" target="#b9">[10]</ref> develops a cascaded pyramid network to address pose estimation from simple to hard keypoints. Lately, <ref type="bibr" target="#b11">[12]</ref> utilizes dynamical graph to boost 2D pose estimation in the single image. However, affinity matrix of <ref type="bibr" target="#b11">[12]</ref> is actually fixed and same for all the persons in each training iteration. It is called as a dynamical matrix, since it is changed over training iterations according to Bernoulli distribution of a precomputed soft matrix. On the contrary, our affinity matrix is dynamical, since it adaptively finds contextual joints to describe spatiotemporal pose relation of each human exemplar in the video. Additionally, we work on 3D pose estimation in videos, while <ref type="bibr" target="#b11">[12]</ref> focuses on 2D pose estimation in still images.</p><p>3D Pose Estimation. 3D pose estimation has been mainly driven by deep neural networks. Currently, most approaches can be categorized into two frameworks. One framework is to directly estimate 3D joint locations from images <ref type="bibr" target="#b12">[13]</ref>- <ref type="bibr" target="#b15">[16]</ref>. A popular solution in this framework is based on the volumetric representation of human pose <ref type="bibr" target="#b13">[14]</ref>. However, this method requires the non-differentiable post-processing step and introduces the quantization error <ref type="bibr" target="#b14">[15]</ref>. To alleviate such difficulty, several extensions have been proposed by integral regression <ref type="bibr" target="#b14">[15]</ref>, ordinal depth supervision <ref type="bibr" target="#b15">[16]</ref>, etc. Another framework is to first estimate 2D pose and then lift 2D pose to 3D pose <ref type="bibr" target="#b3">[4]</ref>- <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b16">[17]</ref>- <ref type="bibr" target="#b18">[19]</ref>, due to the fast development of 2D pose estimation <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b19">[20]</ref>. For instance, <ref type="bibr" target="#b16">[17]</ref> introduces a simple but effective residual fully connected network to regress 3D pose from 2D pose. <ref type="bibr" target="#b20">[21]</ref> formulates the problem as a 2D-to-3D distance matrix regression, which is often robust to missing observations. <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b21">[22]</ref> encode the temporal information in their lifting network, in order to tackle video 3D pose estimation with temporal smoothness. However, these approaches are often limited to learn distinct spatio-temporal relations between human joints in different actions, i.e., an important clue of pose ambiguity reduction. Alternatively, our DG-Net can capture rich relations by learning humanjoint affinity dynamically, which effectively boosts 3D pose estimation in videos.</p><p>Graph Convolution Networks. Graph Convolution Network (GCN) is widely used to model graph-structured data <ref type="bibr" target="#b22">[23]</ref>- <ref type="bibr" target="#b26">[27]</ref>. It has been adopted for skeleton-based action recognition <ref type="bibr" target="#b27">[28]</ref> and group activity recognition <ref type="bibr" target="#b28">[29]</ref>. Different from these works, we tackle 3D pose estimation with depth ambiguity and/or motion uncertainty. Recently, some GCN approaches has been introduced to lift 2D input pose for 3D pose estimation <ref type="bibr" target="#b3">[4]</ref>- <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b29">[30]</ref>. For instance, <ref type="bibr" target="#b3">[4]</ref> introduces a semantic GCN, which learns channel-wise edge weights for enhancing relations between joints. <ref type="bibr" target="#b5">[6]</ref> proposes a localto-global GCN network, in order to learn multi-scale pose relations for 3D estimation. However, all these approaches are limited for learning joint relations, based on the following reasons. First, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b27">[28]</ref> are built upon a fixed spatial/temporal graph affinity, i.e., the connections between joints are based on the fixed human skeleton. Hence, they are limited to capture . Graph affinity A is constructed according to fixed spatial structure of human skeleton. (b) Dynamical Spatial Graph (DSG). Graph affinity B t is constructed by finding connections of spatial neighbors D t and weighting importance of these neighbors R t . (c) Fixed Temporal Graph (FTG). Forward and backward graph affinity P t+1 , P t?1 are constructed according to fixed temporal structure of human skeleton. (d) Dynamical Temporal Graph (DTG). Forward and backward graph affinity Q t+1 , Q t?1 are constructed by finding connections of temporal neighbors L t+1 , L t?1 and weighting importance of these neighbors H t+1 , H t?1 . The black line denotes the connection between joints. The size of circle in DSG and DTG refers to the importance of connected neighbors. More details can be found in Section III. complex pose variations in videos. Second, <ref type="bibr" target="#b28">[29]</ref> utilizes graph attention to learn the relation among all the actors, where all the actors are fully connected in the graph affinity. However, this design may not be suitable for human joints, i.e., every unit only contains the fully-connected graph in the global manner, which often brings the noisy joint relations when learning local affinity in the pose estimation. Third, <ref type="bibr" target="#b30">[31]</ref> is the combination of two cases above. It utilizes a graph affinity that contains three terms, i.e. skeleton term, data-driven term, and attention term. The skeleton and data-driven terms refer to the first case of the fixed affinity, while the attention term refers to the second case of the fully-connected affinity. Both cases are limited to adjust human-joint affinity for capturing the personalized poses of different actors in the video. On the contrary, our graph convolution with KNN can adaptively exploit most relevant joints to estimate a certain joint in 3D space, which effectively reduces connection redundancy to boost performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. METHOD</head><p>In this section, we first analyze spatial/temporal graph convolution with fixed human-joint affinity, and explain how to design our dynamical human-joint affinity. Then, we integrate Dynamical Spatial/Temporal Graph convolution (DSG/DTG) to build up our Dynamical Graph Network (DG-Net) for 3D human pose estimation in videos.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Dynamical Spatial Graph Convolution</head><p>For 3D pose estimation in videos, spatial graph is to describe spatial relations between human joints at each frame. Without loss of generality, we denote it as G S = (V S , E S ), where V S and E S are respectively the node and edge sets. Each node in V S refers to a human joint at a frame, while each edge in E S refers to the connection between two joints.</p><p>Fixed Spatial Graph (FSG) Convolution. For frame t, the node set V S corresponds to a feature matrix of human joints in this frame, i.e.,</p><formula xml:id="formula_0">X t = [x 1 t , x 2 t , ...., x N t ] ? R N ?Cx , where x i t ? R Cx</formula><p>is the feature vector of the i-th human joint and N is the number of joints. Furthermore, the edge set E S is represented by an affinity matrix A ? R N ?N . Traditionally, the edge between joint i and j is defined by human skeleton. Hence, the affinity matrix A is fixed, i.e.,</p><formula xml:id="formula_1">A(i, j) = 1, i and j are connected in skeleton 0, otherwise<label>(1)</label></formula><p>Based on such fixed affinity, spatial graph convolution performs message passing to update the node feature X t as Y t ? R N ?Cy ,</p><formula xml:id="formula_2">Y t = ?(AX t ?),<label>(2)</label></formula><p>where ? ? R Cx?Cy is the parameter matrix, and ? is a nonlinear activation function, e.g., ReLU. Note that, since A is based on the fixed human skeleton, Eq.(2) updates the feature of each human joint by integrating spatial context only from its physically-connected joints. However, a joint is not necessarily close to its physically-connected joints in different video frames, e.g., for the SittingDown case in the introduction, wrist is physically connected with elbow, while its location is closer to and influenced more by hip and pelvis.</p><p>Only integrating context from elbow may lead to unsatisfactory estimation of wrist. Dynamical Spatial Graph (DSG) Convolution. To tackle the problem caused by the fixed affinity, we propose to adaptively construct edges between joints for each frame. Specifically, for a human joint i at frame t, we find a set of its K nearest joints ? i t , according to the feature matrix of human joints X t in this frame,</p><formula xml:id="formula_3">? i t = KNN(x i t , X t , K),<label>(3)</label></formula><p>where KNN refers to the K-Nearest-Neighbor algorithm, and we compute Euclidean distance between x i t and X t to identify ? i t . When joint j belongs to ? i t , there is an edge between joint i and j. Formally, we can obtain a dynamical affinity matrix D t ,</p><formula xml:id="formula_4">D t (i, j) = 1, j ? ? i t 0, otherwise<label>(4)</label></formula><p>Moreover, for joint i, the importance of its neighbor joints in ? i t can be different. To take it into account, we introduce a concise weighting mechanism to represent the importance score of joint j ? ? i t ,</p><formula xml:id="formula_5">R t (i, j) = ?([x i t , x j t ]),<label>(5)</label></formula><p>where ?(?) is a nonlinear mapping with the concatenated input of [</p><formula xml:id="formula_6">x i t , x j t ].</formula><p>In our experiment, a fully-connected layer works well for ?(?). Finally, we obtain a weighted affinity via element-wise multiplication ,</p><formula xml:id="formula_7">B t = D t R t .<label>(6)</label></formula><p>This leads to our dynamical spatial graph convolution,</p><formula xml:id="formula_8">Y t = ?(B t X t ?),<label>(7)</label></formula><p>where ? ? R Cx?Cy is a parameter matrix. Compared to the fixed and physical A in FSG (Eq. 2), B t in DSG is adaptively generated from human joint feature X t . Hence, it makes DSG robust to pose variations in different frames, by dynamically finding and weighting the important spatial neighbors for each joint.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Dynamical Temporal Graph Convolution</head><p>Spatial graph convolution mainly focuses on learning spatial relations between human joints in each frame, while it ignores temporal relations between human joints in different frames. To bridge this gap, it is necessary to introduce temporal graph convolution. Specifically, we denote temporal graph as</p><formula xml:id="formula_9">G T = (V T , E T ),</formula><p>where V T and E T are respectively the node and edge sets. Without loss of generality, we describe temporal graph at frame t in the following. First, the node set V T consists of human joints in three adjacent frames, t ? 1, t and t + 1. This is mainly because we would like to leverage both forward and backward temporal context for frame t. In this case, the node set involves three matrices of human joint features, i.e., X t?1 , X t and X t+1 . Second, the edge set E T consists of human-joint connections between two adjacent frames. Hence, it refers to two affinity matrices, i.e., a forward affinity matrix that describes connections between frame t and t+1, and a backward affinity matrix that describes connections between frame t and t ? 1.</p><p>Fixed Temporal Graph (FTG) Convolution. In the traditional temporal graph, each human joint at frame t is assumed to be connected with the same joint at frame t + 1 and t ? 1.</p><p>As a result, the forward and backward affinity matrices P t+1 , P t?1 ? R N ?N are fixed and identical,</p><formula xml:id="formula_10">P t+1 = P t?1 = I,<label>(8)</label></formula><p>i.e., when joint i at t and joint j at t + 1 are corresponding to the same joint, P t+1 (i, j) = 1. Otherwise, P t+1 (i, j) = 0. It is the same case for P t?1 (i, j). Based on such affinity matrices, temporal graph convolution is actually reduced as the traditional temporal convolution,</p><formula xml:id="formula_11">Y t = ?(X t W x + P t+1 X t+1 W f + P t?1 X t?1 W b ) = ?(X t W x + X t+1 W f + X t?1 W b ),<label>(9)</label></formula><p>where W x , W f , W b ? R Cx?Cy are the parameter matrices.</p><p>One can see that, Eq. (9) updates the feature of each human joint at t, only by the features of the same joint at t?1 and t+1. However, such temporal context is often insufficient to reflect how this joint moves, e.g., for Walking in the introduction, ankle actually keeps the consistent movement with wrist and shoulder from t to t + 1. If we integrate context only from ankle at t + 1, temporal graph convolution tends to mistakenly estimate ankle at t to be forward. Dynamical Temporal Graph (DTG) Convolution. To deal with such problem in the temporal graph, we propose to dynamically discover joint edges between two adjacent frames. As mentioned in the introduction, for a human joint at t, other joints with similar movement (forward and/or backward) are the important temporal context to reduce motion uncertainty of this joint. Hence, we leverage the difference of joint features as guidance, and find these contextual joints from frame t + 1 and t ? 1. Specifically, for joint i at frame t, we use KNN to find a set of K related joints at frame t + 1 with similar forward motion,</p><formula xml:id="formula_12">i t+1 = KNN(x i t+1 ? x i t , X t+1 ? X t , K),<label>(10)</label></formula><p>where we compute Euclidean distance of feature difference between this joint x i t+1 ? x i t and all the joints in X t+1 ? X t , in order to identify the forward set i t+1 . Similarly, we can find a set of K joints at frame t ? 1 with similar backward motion,</p><formula xml:id="formula_13">i t?1 = KNN(x i t ? x i t?1 , X t ? X t?1 , K)</formula><p>. Based on i t+1 and i t?1 , one can construct the associated affinity matrices that change over time, e.g., when joint j at frame t + 1 belongs to the forward set i t+1 , there is an edge between joint i at frame t and joint j at frame t + 1,</p><formula xml:id="formula_14">L t+1 (i, j) = 1, j ? i t+1 0, otherwise<label>(11)</label></formula><p>The backward case is similar, i.e.,</p><formula xml:id="formula_15">L t?1 (i, j) = 1 when j ? i t?1 . Otherwise, L t?1 (i, j) = 0.</formula><p>Moreover, for joint i at t, the importance of joints in the forward set (or the backward set) can be different. Hence, like our dynamical spatial graph convolution before, we introduce a weighting mechanism, e.g., for joint i at t, we compute the importance score of joint j in the forward set i t+1 ,</p><formula xml:id="formula_16">H t+1 (i, j) = ?([x i t , x j t+1 ]).<label>(12)</label></formula><p>Similarly, we introduce a weighting mechanism for joint j in the backward set i t?1 , i.e., H t?1 (i, j) = ?([x i t , x j t?1 ]). In our experiment, a fully-connected layer works well for ?(?) and ?(?) respectively. Subsequently, we obtain the weighted affinity matrices for both forward and backward cases,</p><formula xml:id="formula_17">Q t+1 = L t+1 H t+1 , Q t?1 = L t?1 H t?1 .<label>(13)</label></formula><p>This leads to our dynamical temporal graph convolution, where U x , U f , U b ? R Cx?Cy are the parameter matrices. Compared to FTG in Eq. (9), our DTG contains two timevarying affinity matrices Q t+1 and Q t?1 . For each joint at t, these matrices can effectively discover and weight its important temporal neighbors, according to movement trends. It allows our DTG to reduce motion uncertainty when estimating this joint.</p><formula xml:id="formula_18">Y t = ?(X t U x + Q t+1 X t+1 U f + Q t?1 X t?1 U b ),<label>(14)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Dynamical Graph Network</head><p>In this section, we integrate dynamical spatial and temporal graph convolution to build a Dynamical Graph Network (DG-Net) for 3D pose estimation in videos. Specifically, recent studies <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b16">[17]</ref> have demonstrated that 3D pose can be lifted from 2D pose. Hence, we follow this concise style to design DG-Net. As shown in <ref type="figure" target="#fig_1">Fig. 3</ref>, we first use 2D pose estimator to predict 2D pose in each sampled frame, and then design a dynamical graph convolution block to estimate 3D pose by learning spatio-temporal relations of human joints.</p><p>Dynamical Graph Convolution (DG-Conv) Block. Since spatio-temporal factorization has been widely used for video learning <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b31">[32]</ref>, we build a DG-Conv block with two dynamical units. First, dynamical spatial unit learns spatial pose relations in each frame. Then, dynamical temporal unit learns temporal pose relations between different frames. Moreover, as shown in <ref type="figure" target="#fig_1">Fig. 3</ref>, each dynamical unit is mixed with both fixed and dynamical graph convolution operations, in order to take advantage of their complementary characteristics for pose feature enhancement. For example, dynamical spatial unit consists of FSG and DSG in Section III-A. In this case, this unit can first capture physical pose relations from human skeleton, and then adjust relations dynamically according to pose variations in the video. Dynamical temporal unit is a similar case. Finally, we apply the good practices of <ref type="bibr" target="#b3">[4]</ref> in our DG-Net, e.g., we use FSG to map the input 2D pose of each frame into the 128-dim pose feature at the beginning. In this case, we can effectively exploit spatial and temporal neighbors for DSG/DTG, by encoding joint location information in a flexible high-dimension space. Additionally, for each type of graph convolution in a dynamical unit, we use the residualstyle module built by two layers with 128 feature channels. At the end of a DG-Conv block, we map the final pose feature into the output 3D pose. As suggested in <ref type="bibr" target="#b3">[4]</ref>, we also add a spatial non-local block after each dynamical unit to enhance holistic pose relations. We repeat our DG-Conv block five times to increase network capacity.</p><p>Multi-Level 3D Pose Supervision. We propose to supervise DG-Net in a multi-level manner. First, we use 3D pose supervision in the block level, i.e., for each DG-Conv block, we supervise the predicted 3D pose by ground truth,</p><formula xml:id="formula_19">L block 3d = T t=1 N i=1 (? block,i t ? ? i t ) 2 ,<label>(15)</label></formula><p>where? block,i t ? R 3 is the predicted 3D position of joint i at frame t, and ? i t ? R 3 is the corresponding ground truth. In this case, each block is regularized to be discriminative for predicting effective 3D pose. Second, we use 3D pose supervision in the network level, i.e., we concatenate the output features of all DG-Conv blocks together. Subsequently, we map the concatenated feature into the predicted 3D pose, and use ground truth to supervise it,</p><formula xml:id="formula_20">L network 3d = T t=1 N i=1 (? network,i t ? ? i t ) 2 .<label>(16)</label></formula><p>In this case, DG-Net can fuse semantic representations of all DG-Conv blocks together to boost 3D pose estimation. Finally, we train our DG-Net with both losses,</p><formula xml:id="formula_21">L total 3d = L network 3d + ? block=5 block=1 L block 3d ,<label>(17)</label></formula><p>where ? is a weight coefficient. This allows to train our DG-Net effectively, by leveraging the cooperation power of both block and network levels of 3D pose supervision. In the testing stage, one can simply obtain the predicted 3D pose from the network-level output.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head><p>Datasets. We perform DG-Net on three widely-used benchmarks in 3D pose estimation, including Human3.6M <ref type="bibr" target="#b41">[41]</ref>, HumanEva-I <ref type="bibr" target="#b42">[42]</ref>, and MPI-INF-3DHP <ref type="bibr" target="#b12">[13]</ref>. Human3.6M consists of 3.6 million images from 4 different cameras where 7 actors perform 15 activities in the indoor environment. Following the previous works <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, we use subjects 1, 5, 6, 7, 8 for training and subjects 9, 11 for testing. The evaluation metric is the mean per joint position error (MPJPE) between ground truth and estimated 3D pose, which refers to Protocol1. We also report the results of Protocol2, where estimated 3D pose is aligned to ground truth via a rigid        3DHP is captured with a multi-view setup, where subjects do not wear visible markers. Following <ref type="bibr" target="#b39">[40]</ref>, the evaluation metrics of MPI-INF-3DHP are MPJPE, PCK (percentage of correct keypoints), and AUC (area under curve). Implementation Details. Unless stated otherwise, we implement DG-Net as follows. The number of neighbors K in DSG and DSG are respectively 3 and 4. The weighting function in Eq. (5) and <ref type="bibr" target="#b11">(12)</ref> refer to a fully-connected layer. DG-Net contains 5 DG-Conv blocks. The number of input frames is 4. In the loss function of Eq. (17), ? is set as 0.1. We <ref type="figure">Fig. 4</ref>: MPJPE vs. Model Size (Human3.6M). Comparing with Pavllo et al. <ref type="bibr" target="#b2">[3]</ref>, our DG-Net achieves a smaller MPJPE but with 84.1% parameter reduction.</p><formula xml:id="formula_22">[3] ? 243 -- -- ---- -- ---- -- -- -- -- -- -- -- --</formula><formula xml:id="formula_23">1 -- -- ---- -- ---- -- -- -- -- -- -- -- --</formula><formula xml:id="formula_24">? 1 -- -- ---- -- ---- -- -- -- -- -- -- -- --</formula><p>implement DG-Net in PyTorch, where we use Adam optimizer with an initial learning rate of 0.001 and the exponential decay. We train our model for 200 epochs using mini-batches of size 64 on a single GTX 1080 GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. SOTA Comparison</head><p>Human3.6M. We compare our approach to the state-of-thearts on the Human3.6M dataset. The results of protocol 1 and 2 are shown in <ref type="table" target="#tab_2">Table I</ref> and II. One can see that, our DG-Net outperforms the SOTA method <ref type="bibr" target="#b2">[3]</ref>, with only 4/243 = 1.6% input frames. Moreover, when using ground truth 2D pose, our DG-Net achieves 3.5 mm (protocol 1) and 3.8 mm (protocol 2) estimation improvement, compared to the SOTA approaches <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b37">[38]</ref>. Finally, our DG-Net obtains the smaller estimation error on complex actions (e.g., Sit Down, Photo) that contain high depth ambiguity and motion uncertainty of human joints. It indicates that, our method adaptively learns spatio-temporal joint relation to capture human pose in different actions.</p><p>HumanEva-I. Following the previous methods <ref type="bibr" target="#b0">[1]</ref>- <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b38">[39]</ref>, we report results on Walking and Jogging in three subjects of HumanEva-I by using protocol 2. As shown in <ref type="table" target="#tab_2">Table III</ref>, we achieve a better overall performance. Note that, the high error on S3 (Walk) of HumanEva-I is due to corrupted mocap data, as indicated by <ref type="bibr" target="#b2">[3]</ref>.</p><p>MPI-INF-3DHP. As suggested in <ref type="bibr" target="#b17">[18]</ref>, we mainly compare the previous 2D-to-3D pose lifters for MPI-INF-3DHP, in order to show generalization capacity of our model. In this case, the ground truth 2D pose is given as input for all the methods. Clearly, as shown in <ref type="table" target="#tab_2">Table IV</ref>, our DG-Net outperforms other lifters on all the evaluation metrics.</p><p>Model Size and Inference Time. As suggested in <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b37">[38]</ref>, we show tradeoff between model size and estimation error in <ref type="figure">Fig. 4</ref>. It is worth mentioning that, we use multilevel 3D pose supervision in the training phase, while we simply obtain the predicted 3D pose from the network-level output in the testing phase. Hence, in the testing phase, we do not need the final FSG module (that is just above each block-level loss in <ref type="figure" target="#fig_1">Fig. 3</ref>) to generate the block-level output. As a result, the size of model parameters (2.39M) in the testing phase is smaller than that (2.69M) in the training phase. However, to keep consistency with other approaches,     we still report the model size of training phase in <ref type="figure">Fig. 4</ref>, where our DG-Net achieves the best balance among the related approaches. Additionally, we further show the inference time, by comparing our method with the SOTA approach <ref type="bibr" target="#b2">[3]</ref>. As shown in <ref type="table" target="#tab_9">Table V</ref>, the inference time of [3] is 2.3 ms/frame, while that of our method is 0.5 ms/frame, by using the same GTX 1080 GPU. It proves that our method is not only effective but also efficient. Moreover, it is worth mentioning that, KNN would bring little computation cost in our model, and K is not to be large to achieve good performance (e.g., K = 4 for each joint in our experiment). To validate it, we further show the inference time of our method without KNN. One can see that, DG-Net without KNN spends 0.41ms/frame on inference with MPJPE 52.1 mm, while the proposed DG-Net spends 0.52ms/frame on inference with MPJPE 45.5 mm. It clearly demonstrates that, KNN in our design brings little more computation but leads to much better estimation. <ref type="figure">Fig. 5</ref>: Cross Actions Generalization Ability. We train baseline, 2D-to-3D pose lifter <ref type="bibr" target="#b5">[6]</ref>, and our DG-Net on each of the 15 actions in Human3.6M. Then, we test these models on all the actions.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Ablation Study</head><p>Number of joint neighbors K in DSG and DTG. Without loss of generality, we investigate ablation studies on Human3.6M. When we change K in DSG (or DTG), we fix K in DTG (or DSG) as 4 (or 3). As expected, when we increase K in DSG (or DTG) in <ref type="table" target="#tab_2">Table VI</ref>, the estimation error of DG-Net first decreases and then increases. The main reason is that, when a joint is connected with too few (or many) neighbors, its pose context tends to be insufficient (or noisy). Hence, we choose the moderate K (3/4 for DSG/DTG) in our experiments. As the layer is going deeper, the receptive field of spatial/temporal affinity is getting bigger, which can boost pose estimation by learning discriminative clues from local to global context of human joints.</p><p>Graph Connection Styles &amp; Weighting Functions. For graph connection style in DSG and DTG, we investigate six settings. The random setting is to randomly select K neighbors for each joint. The full setting is to connect each joint with all other joints. The fixed setting is to connect joints according to human skeleton. The symmetry setting refers to the symmetrical connection of joints in <ref type="bibr" target="#b32">[33]</ref>. The precomputed setting is to find K neighbors for each joint by the estimated 2D pose, and then fix these connections in DSG and DTG. The dynamical setting is our design. For the weighting function, we investigate three settings. Except the without setting, we use a FC layer or the kernel in <ref type="bibr" target="#b44">[44]</ref> to obtain the importance score. We denote them as the fully connected or embedded gaussian settings. As shown in <ref type="table" target="#tab_2">Table VII</ref>, the dynamical setting achieves the best among all the graph styles, showing the effectiveness of our design. Moreover, the fully connected setting of weighting mechanism is preferable. Hence, we choose this setting in our experiment. Additionally, the full setting of graph connection with various weighting functions is actually the case of graph convolution with self-attention. As shown in TableVI, our dynamical setting with KNN clearly outperforms this setting. The main reason is that graph convolution with soft affinity matrix learned by self-attention is often redundant and noisy, since all the joints are leveraged to estimate a certain joint. Alternatively, KNN can adaptively exploit most relevant joints to estimate a certain joint in 3D space, which effectively reduces connection redundancy to boost performance.</p><p>Dynamical Spatial/Temporal Units. We gradually introduce our DSG and DTG operations in DG-Conv Block. As shown in <ref type="table" target="#tab_2">Table VIII</ref>, the performance achieves the best with (dynamical spatial unit: FSG+DSG)+(dynamical temporal unit: FTG+DTG+FTG). Hence, we choose this setting in our experiment. Note that, FSG+DSG+DSG and FSG+DSG+FSG settings are used to illustrate that FSG+DSG is empirically the best. Adding more spatial graph convolution (either fixed or dynamical) on FSG+DSG would increase unnecessary model complexity to reduce performance. Hence, both cases are the poor settings. FSG+DSG+DSG is relatively worse than FSG+DSG+FSG, since it may introduce more complexity due to extra dynamical spatial affinity.</p><p>Blocks &amp; Frames. In <ref type="table" target="#tab_2">Table IX</ref>, the performance is the best with 5 DG-Conv blocks. Additionally, the 4-frame setting is comparable with the 5-frame setting. To reduce computation cost, we choose the 4-frame setting.</p><p>Robustness-to-Noise. We evaluate the robustness of our approach (T=4) to corrupted 2D joint locations. In this experiment, all approaches are trained using the ground truth 2D and 3D pairs in Human3.6M dataset. Then we test on inputs corrupted by different levels of Gaussian noise. As shown in <ref type="table" target="#tab_15">Table X</ref>, our method obtains the best result and maintains robustness to all levels of Gaussian noise, due to our dynamical affinity of joint connections.</p><p>Cross Actions Generalization Ability. We train our model on one of the 15 actions in the Human3.6M <ref type="bibr" target="#b41">[41]</ref> dataset and test on all actions. As shown in <ref type="figure">Figure 5</ref>, our DG-Net significantly improves the performance compared with baseline and outperforms the SOTA video 2D-pose lifter <ref type="bibr" target="#b5">[6]</ref> by a large margin. It demonstrates that our DG-Net is capable of adjusting human-joint affinity dynamically to enhance 3D pose estimation, depending on the personalized poses of different actions in the video.</p><p>Other Detailed Designs. We further investigate other de-tailed designs in  3D joint coordinates in the camera coordinate system. We denote it as P c = (X c , Y c , Z c ). The second refers to the concatenation of the pixel UV coordinates extracted from 2D pose detector and the predicted depth of each joint. We denote it as P p = (U, V, D). The non-balanced setting in <ref type="table" target="#tab_2">Table XI refers</ref>  , Z c ). As expected, such balance takes advantage of both systems to achieve a better 3D estimation. Fourth, we evaluate the impact of 2D pose estimators. With a more accurate 2D estimator <ref type="bibr" target="#b9">[10]</ref>, the result is better. Finally, we perform both the unified form and factorized form of our DG-Conv block, i.e., the unified form refers to the spatio-temporal convolution, while the factorized form (i.e., our design) refers to the spatial convolution and temporal convolution in our DG-Conv. We can see that, the MPJPE of unified/factorized forms are 48.9/45.3 mm, which shows the effectiveness of our proposed architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Visualization</head><p>We visualize DG-Net via the estimated 3D pose in a W alkingDog video and a W alking video. Moreover, we show spatial affinity (per frame) in DSG, and temporal affinity (between two frames) in DTG. All these affinity matrices are extracted from the 1st DG-Conv block. As shown in <ref type="figure" target="#fig_2">Fig.  6</ref>, spatial/temporal human-joint affinity matrices change over time. This allows DG-Net to dynamically exploit pose context for each joint to boost 3D estimation, e.g., in <ref type="figure" target="#fig_2">Fig. 6 (a)</ref>, for Joint12 (left wrist) at t, spatial affinity discovers Joint0 (hip) and Joint7 (spine) as extra spatial context at t, while temporal affinity discovers Joint4 (left hip), Joint7 (spine), Joint11 (left shoulder) as extra temporal context at t+1, and Joint8 (thorax), Joint9 (nose), Joint16 (right wrist) as extra temporal context at t?1. Those joints can effectively tell that, Joint12 is straighten and moving similarly with the upper body. As a result, our DG-Net correctly estimate Joint12 at t. On the contrary, baseline distorts this joint with a wrong depth, due to the fixed spatial and temporal graph convolutions. Additional qualitative results on Human3.6M <ref type="bibr" target="#b41">[41]</ref>, MPI-INF-3DHP <ref type="bibr" target="#b12">[13]</ref> and Upenn Action <ref type="bibr" target="#b45">[45]</ref> are shown in <ref type="figure">Figure 7</ref>. Our model also achieves good results in outdoor scenes and in the wild, which proves the generalization of our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>In this work, we propose a novel DG-Net for video 3D pose estimation. By learning human-joint affinity dynamically, it can build up effective joint relations to reduce spatial and temporal ambiguity caused by complex pose variations in videos. Extensive experiments demonstrate high accuracy and effectiveness of our DG-Net.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 :</head><label>2</label><figDesc>Graph Illustration. (a) Fixed Spatial Graph (FSG)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 :</head><label>3</label><figDesc>Our DG-Net Architecture. We design a dynamical graph convolution (DG-Conv) block to lift 2D pose to 3D pose. Additionally, we supervise DG-Net in a multi-level manner, by leveraging the cooperation power of different DG-Conv blocks. More details can be found in Section III-C.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 6 :</head><label>6</label><figDesc>Visualization. We show the estimated 3D pose in a WalkingDog video (a) and a Walking video (b). As expected, spatial and temporal affinity matrices are dynamically adjusted, which allows DG-Net to reduce ambiguity by learning richer pose relations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>2 ,</head><label>2</label><figDesc>to the prediction of P c . For the balanced setting, we use the camera intrinsic parameters (c x , c y , f ) to project UV coordinates into camera coordinate system, where X c = Z c U ?cx f and Y c = Z c V ?cy f . Then, we compute the final prediction of 3D pose via average balancing, where P = ( Xc+Xc Yc+Yc2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE I :</head><label>I</label><figDesc></figDesc><table /><note>SOTA Comparison on Human3.6M under Protocol 1 (mm). The mark ? refers to using GT 2D pose as input of methods. T is the input sequence length. Note that, our 4-frame DG-Net outperforms 243-frame SOTA method [3].Protocol2 (mm) T Direct. Discuss Eat Greet Phone Photo Pose Purch. Sitting SitD. Smoke Wait WalkD Walk. WalkT. Avg Pavlakos et al. [14]</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE II :</head><label>II</label><figDesc>SOTA Comparison on Human3.6M under Protocol2 (mm). The mark ? refers to using GT 2D pose as input of methods. T is the input sequence length. Note that, our 4-frame DG-Net outperforms 243-frame SOTA method<ref type="bibr" target="#b2">[3]</ref>.</figDesc><table><row><cell>Protocol2 (mm)</cell><cell>T</cell><cell>Walking S1 S2 S3 S1 S2 S3 Joging</cell><cell>Avg</cell></row><row><cell>Yasin et al. [39]</cell><cell cols="3">1 35.8 32.4 41.6 46.6 41.4 35.4 38.9</cell></row><row><cell>Lin et al. [1]</cell><cell cols="3">1 26.5 20.7 38.0 41.0 29.7 29.1 30.8</cell></row><row><cell cols="4">Moreno-Noguer et al. [21] 1 19.7 13.0 24.9 39.7 20.0 21.0 26.9</cell></row><row><cell>Pavlakos et al. [14]</cell><cell cols="3">1 22.1 21.9 29.0 29.8 23.6 26.0 25.5</cell></row><row><cell>Martinez et al. [17]</cell><cell cols="3">1 19.7 17.4 46.8 26.9 18.2 18.6 24.6</cell></row><row><cell>Fang et al. [33]</cell><cell cols="3">1 19.4 16.8 37.4 30.4 17.6 16.3 22.9</cell></row><row><cell>Marcard et al. [2]</cell><cell cols="3">5 19.1 13.6 43.9 23.2 16.9 15.5 22.0</cell></row><row><cell>Pallvo et al. [3]</cell><cell cols="3">27 14.5 10.5 47.3 21.9 13.4 13.9 20.2</cell></row><row><cell>Our DG-Net</cell><cell cols="3">4 13.7 9.5 47.1 21.0 12.6 13.4 19.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE III</head><label>III</label><figDesc></figDesc><table><row><cell>2D-to-3D Pose Lifters</cell><cell>T</cell><cell>PCK?</cell><cell>AUC?</cell><cell>MPJPE (mm)?</cell></row><row><cell>Mono et al. [13]</cell><cell>1</cell><cell>75.7</cell><cell>39.3</cell><cell>-</cell></row><row><cell>Mehta et al. [40]</cell><cell>1</cell><cell>79.4</cell><cell>41.6</cell><cell>-</cell></row><row><cell>Lin et al. [18]</cell><cell>25</cell><cell>83.6</cell><cell>51.4</cell><cell>79.8</cell></row><row><cell>Lin et al. [18]</cell><cell>50</cell><cell>82.4</cell><cell>49.6</cell><cell>81.9</cell></row><row><cell>Our DG-Net</cell><cell>4</cell><cell>87.5</cell><cell>53.8</cell><cell>76.0</cell></row><row><cell>: SOTA Comparison on HumanEva-I under Proto-</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>col2 (mm).</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>transformation. Additionally, following [3], we extract the input 2D pose of Human3.6M from cascaded pyramid network [10]. HumanEva-I contains video sequences of 3 subjects with 6 actions which are recorded by 3 cameras. Following [3], [17], we train DG-Net and test Protocol2 results for each action. Moreover, as suggested in [3], we extract the input 2D pose of HumanEva-I from Mask R-CNN [43]. MPI-INF-</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE IV :</head><label>IV</label><figDesc>SOTA Comparison on MPI-INF-3DHP under PCK, AUC and MPJPE (mm).</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE V :</head><label>V</label><figDesc>Inference Time.</figDesc><table><row><cell>K in DSG</cell><cell>1</cell><cell>3</cell><cell>5</cell><cell>7</cell></row><row><cell>Our DG-Net</cell><cell cols="4">47.3 mm 45.3 mm 48.2 mm 50.8 mm</cell></row><row><cell>K in DTG</cell><cell>1</cell><cell>3</cell><cell>5</cell><cell>7</cell></row><row><cell>Our DG-Net</cell><cell cols="4">48.0 mm 46.5 mm 47.3 mm 49.2 mm</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>TABLE VI :</head><label>VI</label><figDesc>No. of joint neighbors K used in DSG and DTG.</figDesc><table><row><cell>Graph</cell><cell>Weighting</cell><cell>W/O</cell><cell>FC</cell><cell>EG</cell></row><row><cell>Random</cell><cell></cell><cell cols="3">83.4 mm 80.5 mm 80.9 mm</cell></row><row><cell>Fixed</cell><cell></cell><cell cols="3">52.7 mm 50.3 mm 51.4 mm</cell></row><row><cell>Full</cell><cell></cell><cell cols="3">58.8 mm 56.3 mm 57.3 mm</cell></row><row><cell>Symmetry</cell><cell></cell><cell cols="3">53.2 mm 53.0 mm 53.0 mm</cell></row><row><cell>Precomputed</cell><cell></cell><cell cols="3">50.4 mm 48.1 mm 48.8 mm</cell></row><row><cell cols="2">Our Dynamical</cell><cell cols="3">48.1 mm 45.3 mm 46.9 mm</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>TABLE VII</head><label>VII</label><figDesc></figDesc><table><row><cell cols="2">: Graph Connection Styles &amp; Weighting Functions</cell></row><row><cell cols="2">in DSG and DTG. W/O: Without Weighting. FC: Fully-</cell></row><row><cell cols="2">Connected Layer. EG: Embedded Gaussian Kernel.</cell></row><row><cell>Dynamical Spatial Unit</cell><cell>MPJPE (mm)</cell></row><row><cell>FSG</cell><cell>57.9</cell></row><row><cell>FSG+DSG</cell><cell>55.8</cell></row><row><cell>FSG+FSG</cell><cell>57.4</cell></row><row><cell>FSG+DSG+DSG</cell><cell>57.4</cell></row><row><cell>FSG+DSG+FSG</cell><cell>56.5</cell></row><row><cell>Dynamical Temporal Unit</cell><cell>MPJPE (mm)</cell></row><row><cell>FSG+DSG+FTG</cell><cell>52.0</cell></row><row><cell>FSG+DSG+FTG+DTG</cell><cell>47.0</cell></row><row><cell>FSG+DSG+FTG+FTG</cell><cell>51.3</cell></row><row><cell>FSG+DSG+FTG+DTG+DTG</cell><cell>47.6</cell></row><row><cell>FSG+DSG+FTG+DTG+FTG</cell><cell>45.3</cell></row><row><cell>FSG+DSG+FTG+DTG+FTG+DTG</cell><cell>46.1</cell></row><row><cell>FSG+DSG+FTG+DTG+FTG+FTG</cell><cell>45.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>TABLE VIII :</head><label>VIII</label><figDesc></figDesc><table /><note>DG-Conv Block.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>TABLE IX :</head><label>IX</label><figDesc>No. of DG-Conv Blocks &amp; Input Frames.</figDesc><table><row><cell>Train/Test (Protocol2 (mm))</cell><cell>[21]</cell><cell>[17]</cell><cell>[2]</cell><cell>Our</cell></row><row><cell>GT / GT</cell><cell>62.17</cell><cell>37.10</cell><cell>31.6</cell><cell>24.17</cell></row><row><cell>GT / GT+N (0, 5)</cell><cell>67.11</cell><cell cols="2">46.65 37.46</cell><cell>31.86</cell></row><row><cell>GT / GT+N (0, 10)</cell><cell>79.12</cell><cell cols="2">52.84 49.41</cell><cell>38.63</cell></row><row><cell>GT / GT+N (0, 15)</cell><cell>96.08</cell><cell cols="2">59.97 61.80</cell><cell>46.92</cell></row><row><cell>GT / GT+N (0, 20)</cell><cell cols="3">115.55 70.24 73.65</cell><cell>56.33</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>TABLE X :</head><label>X</label><figDesc></figDesc><table /><note>Robustness-to-Noise.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table XI .</head><label>XI</label><figDesc>First, we evaluate our multi-level supervision. As expected, the total loss in Eq. (17) leads to a better performance, compared to the network loss in Eq.<ref type="bibr" target="#b15">(16)</ref>. It illustrates that, adding 3D supervision in each block can effectively regularize our DG-Net with more discriminative pose representations. Second, we follow the good practice in<ref type="bibr" target="#b3">[4]</ref> via embedding spatial nonlocal operations in our DG-Conv block. As expected, it achieves a better performance by learning global pose relations. Third, we evaluate the impact of coordinate operations. In general, there are two coordinate systems for 3D human pose. The first uses the root-relativeFig. 7: Visualization. (a) shows the results in Human3.6M<ref type="bibr" target="#b45">[45]</ref> dataset; (b) shows the results in MPI-INF-3DHP<ref type="bibr" target="#b12">[13]</ref> and Upenn Action<ref type="bibr" target="#b45">[45]</ref> datasets.</figDesc><table><row><cell>Supervision</cell><cell>Network Loss</cell><cell>Total Loss</cell></row><row><cell>Our DG-Net</cell><cell>48.7 mm</cell><cell>45.3 mm</cell></row><row><cell>Nonlocal</cell><cell>Without</cell><cell>With</cell></row><row><cell>Our DG-Net</cell><cell>50.1 mm</cell><cell>45.3 mm</cell></row><row><cell>Coordinate Operation</cell><cell>Non-Balanced</cell><cell>Balanced</cell></row><row><cell>Our DG-Net</cell><cell>46.1 mm</cell><cell>45.3 mm</cell></row><row><cell>Choice of 2D Estimator</cell><cell>SH [20]</cell><cell>CPN [10]</cell></row><row><cell>Pavllo et al. [3]</cell><cell>53.4 mm</cell><cell>46.8 mm</cell></row><row><cell>Our DG-Net</cell><cell>49.9 mm</cell><cell>45.3 mm</cell></row><row><cell>DG Form</cell><cell>Unified</cell><cell>Factorized</cell></row><row><cell>Our DG-Net</cell><cell>48.9 mm</cell><cell>45.3 mm</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>TABLE XI :</head><label>XI</label><figDesc>Detailed Designs (Supervision, Nonlocal, Coordinate Operation, Choice of 2D Pose Estimator, DG Form).</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Recurrent 3d pose sequence machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Exploiting temporal information for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R I</forename><surname>Hossain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Little</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note type="report_type">ECCV</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">3d human pose estimation in video with temporal convolutions and semi-supervised training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pavllo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Semantic graph convolutional networks for 3d human pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kapadia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Optimizing network structure for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Exploiting spatial-temporal relationships for 3d pose estimation via graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-J</forename><surname>Cham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">M</forename><surname>Thalmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Human pose estimation using body parts dependent joint regressors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dantone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Leistner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3041" to="3048" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Convolutional pose machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramakrishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4724" to="4732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deeppose: Human pose estimation via deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1653" to="1660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Cascaded pyramid network for multi-person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Dgcn: Dynamic graph convolutional network for efficient multi-person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI, 2020</title>
		<imprint>
			<biblScope unit="volume">931</biblScope>
			<biblScope unit="page" from="11" to="924" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Monocular 3d human pose estimation in the wild using improved cnn supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3DV</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Coarse-tofine volumetric prediction for single-image 3D human pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">G</forename><surname>Derpanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Integral human pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Ordinal depth supervision for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A simple yet effective baseline for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hossain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Little</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Trajectory space factorization for deep videobased 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.08289</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Motion guided 3d pose estimation from videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.13985</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">3d human pose estimation from a single image via distance matrix regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Moreno-Noguer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Propagating lstm: 3d pose estimation based on joint interdependency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ECCV</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2018-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Deep convolutional networks on graph-structured data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Henaff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>in arXiv</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Gated graph sequence neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tarlow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brockschmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Geometric deep learning: going beyond euclidean data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Graph attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Veli?kovi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Spatial temporal graph convolutional networks for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AAAI</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning actor relation graphs for group activity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Construct dynamic graphs for hand gesture recognition via spatial-temporal attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMVC</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Two-stream adaptive graph convolutional networks for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A closer look at spatiotemporal convolutions for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6450" to="6459" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning pose grammar to encode human body configuration for 3d pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-S</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-C</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>3d human pose estimation in the wild by adversarial learning,&quot; in CVPR</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Not all parts are created equal: 3d pose estimation by modeling bi-directional dependencies of body parts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Deep kinematics analysis for monocular 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Gast-net: Graph attention spatiotemporal convolutional networks for 3d human pose estimation in video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rojas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.14179</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Attention mechanism exploits temporal contexts: Real-time 3d human pose reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Asari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020-06" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A dual-source approach for 3d pose estimation from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yasin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kruger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016-06" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sridhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shafiei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-P</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Vnect: Real-time 3d human pose estimation with a single rgb camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Seidel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Theobalt</surname></persName>
		</author>
		<ptr target="http://gvv.mpi-inf.mpg.de/projects/VNect/6" />
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">36</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Human3.6m: Large scale datasets and predictive methods for 3d human sensing in natural environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Papava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Olaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">10</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Humaneva: Synchronizedvideo and motion capture dataset and baseline algorithm forevaluation of articulated human motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sigal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">O</forename><surname>Balan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">From actemes to action: A strongly-supervised representation for detailed action understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">G</forename><surname>Derpanis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2248" to="2255" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
