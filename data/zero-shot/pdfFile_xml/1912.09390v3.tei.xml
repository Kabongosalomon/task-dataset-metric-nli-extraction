<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Tangent Images for Mitigating Spherical Distortion</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><forename type="middle">Eder</forename></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of North Carolina at Chapel Hill Chapel Hill</orgName>
								<address>
									<region>NC</region>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhailo</forename><surname>Shvets</surname></persName>
							<email>mshvets@cs.unc.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of North Carolina at Chapel Hill Chapel Hill</orgName>
								<address>
									<region>NC</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Lim</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of North Carolina at Chapel Hill Chapel Hill</orgName>
								<address>
									<region>NC</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan-Michael</forename><surname>Frahm</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of North Carolina at Chapel Hill Chapel Hill</orgName>
								<address>
									<region>NC</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Tangent Images for Mitigating Spherical Distortion</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T20:22+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this work, we propose "tangent images," a spherical image representation that facilitates transferable and scalable 360 ? computer vision. Inspired by techniques in cartography and computer graphics, we render a spherical image to a set of distortion-mitigated, locally-planar image grids tangent to a subdivided icosahedron. By varying the resolution of these grids independently of the subdivision level, we can effectively represent high resolution spherical images while still benefiting from the low-distortion icosahedral spherical approximation. We show that training standard convolutional neural networks on tangent images compares favorably to the many specialized spherical convolutional kernels that have been developed, while also scaling efficiently to handle significantly higher spherical resolutions. Furthermore, because our approach does not require specialized kernels, we show that we can transfer networks trained on perspective images to spherical data without fine-tuning and with limited performance drop-off. Finally, we demonstrate that tangent images can be used to improve the quality of sparse feature detection on spherical images, illustrating its usefulness for traditional computer vision tasks like structure-from-motion and SLAM.</p><p>Authors Note: This version of this paper has been updated with new network transfer results not present in the version published at CVPR 2020. These important new results demonstrate that network transfer to spherical images using our representation provides equivalent performance to perspective image networks after only a single epoch of fine-tuning and actually improves performance after 10 epochs of fine-tuning. The accompanying code has also been updated to coincide with this version of the paper. A full log of the changes is provided in Section 6.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>A number of methods have been proposed to address convolutions on spherical images. These techniques vary in design, encompassing learnable transformations <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28]</ref>, <ref type="bibr">Level 1 Icosahedron</ref> Tangent Images</p><p>Interior View <ref type="figure" target="#fig_5">Figure 1</ref>: Using tangent images to represent a 4k Earth image <ref type="bibr" target="#b13">[14]</ref>. TL: A base level 1 icosahedron. TR: Selection of tangent images rendered from the Earth image. B: Interior view of the tangent image spherical approximation.</p><p>generalizations and modifications of the convolution operation <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b28">29]</ref>, and specialized kernels for spherical representations <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b30">31]</ref>. In general, these spherical convolutions fall into two classes: those that operate on equirectangular projections and those that operate on a subdivided icosahedral representation of the sphere. The latter has been shown to significantly mitigate spherical distortion, which leads to significant improvements for dense prediction tasks <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b19">20]</ref>. It also has the useful property that icosahedron's faces and vertices scale roughly by a factor of 4 at each subdivision, permitting a simple analogy to 2? upsampling and downsampling operations in standard convolutional neural networks (CNNs). Because of the perfor-mance improvements provided by the subdivided icosahedron representation, we focus expressly on it in this paper. Despite a growing body of work on these icosahedral convolutions, there are two significant impediments to further development: <ref type="bibr" target="#b0">(1)</ref> the transferability of standard CNNs to spherical data on the icosahedron, and (2) the difficulty in scaling the proposed spherical convolution operations to high resolution spherical images. Prior work has implied <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b10">11]</ref> or demonstrated <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b30">31]</ref> the transferability of networks trained on perspective images to different spherical representations. However, those who report results see a noticeable decrease in accuracy compared to CNN performance on perspective images and specialized networks that are trained natively on spherical data, leaving this important and desired behavior an unresolved question. Additionally, the proposed specialized convolutional kernels either require subsequent network tuning <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b30">31]</ref> or are incompatible with the standard convolution <ref type="bibr" target="#b16">[17]</ref>.</p><p>Nearly all prior work on icosahedral convolutions has been built on the analogy between pixels and faces <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b19">20]</ref> or pixels and vertices <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b30">31]</ref>. While elegant on the surface, this parallel has led to difficulties in scaling to higher resolution spherical images. <ref type="figure">Figure 2</ref> depicts spherical image resolutions evaluated in the prior work. Notice that the highest resolution obtained so far is a level 8 subdivision, which is comparable to a 512 ? 1024 equirectangular image. Superficially, this pixel resolution seems reasonably high, but the angular resolution per pixel is still quite low. A 512?1024 equirectangular image has an angular resolution of 0.352 ? . For comparison, a VGA resolution (480 ? 640) perspective image with 45 ? ? 60 ? field of view (FOV) has an angular resolution of 0.094 ? . This is most similar to a 2048 ? 4096 equirectangular image, which has an angular resolution of 0.088 ? and corresponds to a level 10 subdivided icosahedron. As this is a significantly higher resolution than prior work has been capable of demonstrating, this is the resolution on which we test our proposed approach.</p><p>In this work, we aim to address both transferability and scalability while leveraging efficient implementations of existing network architectures and operations. To this end, we propose a solution that decouples resolution from subdivision level using oriented, distortion-mitigated images that can be filtered with the standard grid convolution operation. Using these tangent images, standard CNN performance is competitive with specialized networks, yet they efficiently scale to high resolution spherical data and open the door to performance-preserving network transfer between perspective and spherical data. Furthermore, use of the standard convolution operation allows us to leverage highlyoptimized convolution implementations, such as those from the cuDNN library <ref type="bibr" target="#b4">[5]</ref>, to train our networks. Additionally, the benefits of tangent images are not restricted to deep learning, as they address distortion through the data repre-sentation rather than the data processing tools. This means that our approach can be used for traditional vision applications like structure-from-motion and SLAM as well.</p><p>We summarize our contributions as follows:</p><p>We propose the tangent image spherical representation: a set of oriented, low-distortion images rendered tangent to faces of the icosahedron. We show that standard CNNs trained on tangent images perform competitively with specialized spherical convolutional kernels while also scaling effectively to high resolution spherical images. We demonstrate that tangent images facilitate network transfer between perspective and spherical images with no fine tuning and minimal performance drop-off. We illustrate the utility of tangent images for traditional computer vision tasks by using them to improve sparse keypoint matching on spherical images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Recently, there have been a number of efforts to close the gap between CNN performance on perspective images and spherical images. These efforts can be naturally divided based on the spherical image representation used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Equirectangular images</head><p>Equirectangular images are a popular spherical image representation thanks to their simple relation between rectangular and spherical coordinates. However, they demonstrate severe image distortion as a result. A number of methods have been proposed to address this issue. Su and Grauman <ref type="bibr" target="#b26">[27]</ref> develop a learnable, adaptive kernel to train a CNN to transfer models trained on perspective images to the equirectangular domain. Su et al. <ref type="bibr" target="#b27">[28]</ref> extend this idea by developing a kernel that learns to transform a feature map according to local distortion properties. Cohen et al. <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b5">6]</ref> develop spherical convolutions, which provides the rotational equivariance necessary for convolutions on the sphere. This method requires a specialized kernel, however, making it difficult to transfer the insights developed from years of research into traditional CNNs. Works from Coors et al. <ref type="bibr" target="#b8">[9]</ref> and Tateno et al. <ref type="bibr" target="#b28">[29]</ref> address equirectangular image distortion by warping the planar convolution kernel in a location-dependent manner. Because the equirectangular representation is so highly distorted, most recent work on this topic, has looked to leverage the distorted-reducing properties of the icosahedral spherical approximation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Icosahedral representations</head><p>Representing the spherical image as a subdivided icosahedron mitigates spherical distortion, thus improving CNN accuracy compared to techniques that operate on equirectangular images. Eder and Frahm <ref type="bibr" target="#b9">[10]</ref>  ther research on this representation has primarily focused on the development of novel kernel designs to handle discretization and orientation challenges on the icosahedral manifold. Lee et al. <ref type="bibr" target="#b19">[20]</ref> convolve on this representation by defining new, orientation-dependent, kernels to sample from triangular faces of the icosahedron. Jiang et al. <ref type="bibr" target="#b16">[17]</ref> reparameterize the convolutional kernel as a linear combination of differential operators on the surface of an icosahedral mesh. Zhang et al. <ref type="bibr" target="#b30">[31]</ref> present a method that applies a special hexagonal convolution on the icosahedral net. Cohen et al. <ref type="bibr" target="#b6">[7]</ref> precompute an atlas of charts at different orientations that cover the icosahedral grid and use masked kernels along with an feature-orienting transform to convolve on these planar representations. Eder et al. <ref type="bibr" target="#b10">[11]</ref> define the "mapped convolution" that allows the custom specification of convolution sampling patterns through a type of graph convolution. In this way, they specify the filters' orientation and sample from the icosahedral surface. Our tangent image representation addresses data orientation by ensuring all tangent images are consistently oriented when rendering and circumvents the discretization issue by rendering to image pixel grids.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Mitigating Spherical Distortion</head><p>Image distortion is the reason that we cannot simply apply many state-of-the-art CNNs to spherical data. Distortion changes the representation of the image, resulting in local content deformation that violates translational equivariance, the key property of a signal required for convolution functionality. The graph in <ref type="figure" target="#fig_0">Figure 3</ref> shows just how little distortion is required to produce a significant dropoff in CNN performance. Distortion in the most popular spherical image representations, equirectangular images and cube maps, is quite significant <ref type="bibr" target="#b9">[10]</ref>, and hence results in even worse performance. Although we can typically remove most lens distortion in perspective images using tools like the Brown-Conrady distortion model <ref type="bibr" target="#b1">[2]</ref>, spherical dis- distortion is added to test images by varying the K1 parameter of the Brown-Conrady radial distortion model <ref type="bibr" target="#b1">[2]</ref>. An example digit is shown at different distortion levels.</p><formula xml:id="formula_0">K1 = 0.0 K1 = 0.1 K1 = 0.2 K1 = 0.3 K1 = 0.4 K1 = 0.</formula><p>tortion is inescapable and is actually a function of the planar representation. This follows from Gauss's Theorema Egregium, a consequence of which is that a spherical surface is not isometric to a plane. As such, any effort to represent a spherical image as a planar one will result in some degree of distortion. Thus, our objective, and one shared by cartographers for thousands of years, is limited to finding the optimal planar representation of the sphere for our use case.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">The icosahedral sphere</head><p>Consider the classical method of exhaustion of approximating a circle with inscribed regular polygons. It follows that, in three dimensions, we can approximate a sphere in the same way. Thus, the choice of planar spherical approximation ought to be the convex Platonic solid with the most faces: the icosahedron. The icosahedron has been used by cartographers to represent Earth at least as early as Buckminster Fuller's Dymaxion map <ref type="bibr" target="#b2">[3]</ref>, which projects the globe onto the icosahedral net. Recent work in computer to the surface area of a sphere of the same radius at each subdivision level. This global metric demonstrates how closely the subdivision surface approximates a sphere and is drawn from established cartographic metrics <ref type="bibr" target="#b17">[18]</ref>. Note the leveling off after the third subdivision level. vision <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b30">31]</ref> has demonstrated the shape's utility for resolving the distortion problem for CNNs on spherical images as well.</p><p>While an improvement over single-plane image projections and its Platonic solid cousin, the cube, the 20-face icosahedron on its own is still limited in its distortionmitigating properties. It can be improved by repeatedly applying Loop subdivision <ref type="bibr" target="#b22">[23]</ref> to subdivide the faces and interpolate the vertices, producing increasingly close spherical approximations with decreasing amounts of local distortion on each face. <ref type="figure" target="#fig_1">Figure 4</ref> demonstrates how distortion decreases at each subdivision level. Not all prior work takes advantage of this extra distortion reduction, though. There has largely been a trade-off between efficiency and representation. The charts used by Cohen et al. <ref type="bibr" target="#b6">[7]</ref> and the net used by Zhang et al. <ref type="bibr" target="#b30">[31]</ref> are efficient thanks to their planar image representations, but they are limited to the distortion properties of a level 0 icosahedron. On the other hand, the mapped convolution proposed by Eder et al. <ref type="bibr" target="#b10">[11]</ref> operates on the mesh itself and thus can benefit from higher level subdivision, but it does not scale well to higher level meshes due to cache coherence problems when computing intermediate features on the mesh. Jiang et al. <ref type="bibr" target="#b16">[17]</ref> provide efficient performance on the mesh, but do so by approximating convolution with a differential operator, which means existing networks can not be transferred. It is also interesting to note that the current top-performing method for many deep learning tasks, <ref type="bibr" target="#b30">[31]</ref>, uses the net of the level 0 icosahedron. This suggests that extensive subdivisions may not be necessary for all use cases.</p><p>Practical methods for processing spherical images must address the efficient scalability problem, but also should permit the transfer of well-researched, high-performance methods designed for perspective images. They should also provide the opportunity to modulate the level of acceptable distortion depending on the application. To address these constraints, we propose to break the coupling of subdivi- </p><formula xml:id="formula_1">Base Level +2 Base Level +3 Base Level +4 Base Level +5</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Tangent images</head><p>Subdividing the icosahedron provides diminishing returns rather quickly from a distortion-reduction perspective, as indicated by the red vertical line in <ref type="figure" target="#fig_1">Figure 4</ref>. Nonetheless, existing methods must continue to subdivide in order to match the spherical image resolution to the number of mesh elements. We untether these considerations by fixing a base level of subdivision, b, to define an acceptable degree of distortion, and then rendering the spherical image to square, oriented, planar pixel grids tangent to each face at that base level. The resolution of these tangent images is subsequently determined by the resolution of the spherical input. Given a subdivision level, s, corresponding to the spherical input resolution, the dimension of the tangent image, d, is given by the relation:</p><formula xml:id="formula_2">d = 2 s?b<label>(1)</label></formula><p>This design preserves the same resolution scaling that would occur through further subdivisions by instead increasing the resolution of the tangent image. This relationship is illustrated in <ref type="figure" target="#fig_2">Figure 5</ref>.</p><p>Our tangent images are motivated by existing techniques in related fields. The approximation of sections of the sphere by low-distortion planar regions is similar to the Universal Transverse Mercator (UTM) geodetic coordinate system, which divides the Earth into a number of nearly-Euclidean zones. Additionally, as tangent images can be thought of as rendering a spherical mesh to a set of quad textures, the high resolution benefits are similar to Ptex <ref type="bibr" target="#b3">[4]</ref>, a computer graphics technique that enables efficient highresolution texturing by providing every quad of a 3D mesh with its own texture map. A visualization of the tangent image concept is provided in <ref type="figure" target="#fig_5">Figure 1</ref>.</p><p>Computing tangent images Tangent images are the gnomonic projection of the spherical data onto oriented, square planes centered at each face of a level b subdivided icosahedron. The number of tangent images, N , is determined by the faces of the base level icosahedron: N = 20(4 b ), while their spatial extent is a function of the vertex resolution, R v (b ? 1), of the level b ? 1 icosahedron and the resolution of the image grid, given by Equation <ref type="bibr" target="#b0">(1)</ref>. Let (? f , ? f ) be the barycenter of a triangular face of the icosahedron in spherical coordinates. We then compute the bounds of the plane in spherical coordinates as the inverse gnomonic projection at central latitude and longitude (? f , ? f ) of the points:</p><formula xml:id="formula_3">? f ? d ? 1 2d R v (b ? 1) ? ? f ? d ? 1 2d R v (b ? 1) (2) The vertex resolution, R v , of a level b icosahedron, S(b),</formula><p>is computed as the mean angle between all vertices, v, and their neighbors, adj(v):</p><formula xml:id="formula_4">R v (b) = 1 |S(b)| v?S(b) w?adj(v) ?(v, w) |adj(v)|<label>(3)</label></formula><p>Using R v (b?1) ensures that the tangent images completely cover their associated triangular faces. Because vertex resolution roughly halves at each subsequent subdivision level, we define R v (?1) = 2R v (0). Using tangent images Tangent images require rendering from and to the sphere only once each. First, we create the tangent image set by rendering to the planes defined by Equation <ref type="formula">(2)</ref>. Then, we apply the desired perspective image algorithm (e.g. a CNN or keypoint detector). Finally, we compute the regions on each plane visible to a spherical camera at the center of the icosahedron and render the algorithm output back to the sphere.</p><p>We have released our tangent image rendering code and associated experiments as a PyTorch extension 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>Prior research has established a common suite of experiments that have become the test bed for new research on spherical convolutions. This set typically includes some combination of spherical MNIST classification  <ref type="table">Table 1</ref>: Classification results on the ModelNet40 dataset <ref type="bibr" target="#b29">[30]</ref>.</p><p>Without any specialized convolution operations, our approach is competitive with the state of the art spherical convolution methods. <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b30">31]</ref>, shape classification <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b16">17]</ref>, climate pattern segmentation <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b30">31]</ref>, and semantic segmentation <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b30">31]</ref>. In order to benchmark against these prior works, we evaluate our method on the shape classification and semantic segmentation tasks. Additionally, we demonstrate our method's fairly seamless transfer of CNNs trained on perspective images to spherical data. Finally, to show the versatility of the tangent image representation, we introduce a new benchmark, sparse keypoint detection on spherical images, and compare our representation to an equirectangular image baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Classification</head><p>We first evaluate our proposed method on the shape classification task. As with prior work, we use the ModelNet40 dataset <ref type="bibr" target="#b29">[30]</ref> rendered using the method described by Cohen et al. <ref type="bibr" target="#b7">[8]</ref>. Because the data densely encompasses the entire sphere, unlike spherical MNIST, which is sparse and projected only on one hemisphere, we believe this task is more indicative of general classification performance.</p><p>Experimental setup We use the network architecture from Jiang et al. <ref type="bibr" target="#b16">[17]</ref>, but we replace the specialized kernels with simple 3?3 2D convolutions. A forward pass involves running the convolutional blocks on each patch separately and subsequently aggregating the patch features with average pooling. We train and test on level 5 resolution data as with the prior work.</p><p>Results and analysis Results of our experiments are shown in <ref type="table">Table 1</ref>. Without any specialized convolutional kernels, we outperform most of the prior work on this task. The best performing method from Jiang et al. <ref type="bibr" target="#b16">[17]</ref> leverages a specialized convolution approximation on the mesh, which inhibits the ability to fine-tune existing CNN models for the task. Our method can be thought of as using a traditional CNN in a multi-view approach to spherical images. This means that, for global inference tasks like classification, we could select our favorite pre-trained network and transfer it to spherical data. In this case, it is likely that some fine-tuning may be necessary to address the final patch aggregation step in our network design.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Semantic segmentation</head><p>We next consider the task of semantic segmentation in order to demonstrate dense prediction capabilities. To compare to prior work, we perform a baseline evaluation of our  <ref type="table">Table 2</ref>: Semantic segmentation results. s is the input resolution in terms of equivalent icosahedron level, b is the base subdivison level (ERP denotes equirectangular inputs), mIoU is the mean intersection-over-union metric, and mAcc is the weighted per-class mean prediction accuracy.</p><p>method at low icosahedron resolutions (5 and 7), but we also evaluate the performance of our method at a level 10 input resolution in order to demonstrate the usefulness of the tangent image representation for processing high resolution spherical data. No prior work has operated at this resolution. We hope that our work can serve as a benchmark for further research on high resolution spherical images. Experimental setup We train and test our method on the Stanford 2D3DS dataset <ref type="bibr" target="#b0">[1]</ref>, as with prior work <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b30">31]</ref>. We evaluate RGB-D inputs at levels 5, 7, and 10, the maximum resolution provided by the dataset. At level 10 we also evaluate using only RGB inputs to demonstrate the benefit of high resolution capabilities. For the level 5 and 7 experiments, we use the residual UNet-style architecture as in <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b30">31]</ref>, but we again replace the specialized kernels with 3?3 convolutions. The higher resolution of the level 10 inputs requires the larger receptive field of a deeper network, so we use a FCN-ResNet 101 <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b21">22]</ref> model pretrained on COCO <ref type="bibr" target="#b20">[21]</ref> for those experiments. For level 5 data, we train on the entire set of tangent images, while for the higher resolution experiments, we randomly sample a subset of tangent images from each spherical input to expedite training. We found this sampling method to be useful without loss of accuracy. We liken it to training on multiple perspective views of a scene. Forward passes are run on all sampled tangent images before each backward pass. In this way, the computed gradients at every iteration come from the entire span of the spherical image's field of view.</p><p>Results and analysis We report the results of our experiments in <ref type="table">Table 2</ref>. Results on the Stanford2D3DS dataset are averaged over the 3 folds. Individual class results can be found in the supplementary material (Section 9). As expected, our method does not perform as well as prior work at the level 5 resolution. Recall that a level 5 resolution spherical image is equivalent to a 16?16 perspective image with 45 ? FOV. Our method takes that already low angular resolution image and separates it into a set of low pixel resolution images. Although it had limited impact on classification, these dual low resolutions are problematic for dense prediction tasks. We expound on the low-resolution limitation further in the supplementary material (Section 7).</p><p>Where our tangent image representation excels is when scaling to high resolution images. What we sacrifice in lowresolution performance, we make up for by efficiently scaling to high resolution inputs. By scaling to the full resolution of the dataset, we are able to report the highest performing results ever on this spherical dataset by a wide margin using only RGB inputs. Adding the extra depth channel, we are able to increase the performance further (+4.9 mAcc, +6.9 mIOU). At input level 10, we find that base level 1 delivers the best trade-off between the lower FOV at higher base levels and the increased distortion present in lower ones. We elaborate on this trade-off in the supplementary material (Section 7).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Network transfer</head><p>Our contribution aims to address equivalent network performance regardless of the input data format. That is, for a given network, we strive to achieve equal performance on both perspective and spherical data. This objective is motivated by the limited number of spherical image datasets and the difficulty of collecting large scale spherical training data. If we can achieve high transferability of perspective image networks, we reduce the need for large amounts of spherical training data. Because generating tangent images inherently converts a spherical image into a collection of perspective ones, this representation facilitates the desired network transferability without requiring fine-tuning on the spherical data and with limited performance drop-off.</p><p>Experimental setup We evaluate the transferability of the tangent image representation in three experiments.</p><p>In the first experiment, we evaluate semantic segmentation performance on a spherical image test set using a network trained on the corresponding perspective image training set. We fine-tune the pre-trained, FCN-ResNet101 model <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b21">22]</ref> provided by the PyTorch model zoo on the Stanford2D3DS dataset's <ref type="bibr" target="#b0">[1]</ref> perspective image training set. We then evaluate semantic segmentation performance on the spherical image test set at a level 8 resolution. As with the other semantic segmentation experiments, this experiment uses RGB-D inputs. During the dataset fine-tuning, we make sure to consider the desired angular resolution of the spherical test images. A network trained on perspective images with an angular resolution of 1 ? has learned filters accordingly. Should we apply those filters to an image captured at the identical position, at the same image resolution, but with a narrower FOV, the difference in angular resolution is effectively scale distortion. To match the angular resolution of our spherical evaluation set, we normalize the camera matrices for all perspective images during training such they have the same angular resolution as the test images. Because this is effectively a center-crop of the data, we also randomly shift our new camera center in order capture all parts of the image. Details of this pre-processing are given in the supplementary material (Section 8). We evaluate performance without fine-tuning on spherical data, after 1 epoch of spherical fine-tuning, and again after 10 epochs. To control for the extra training, we also evaluate the perspective network after an additional 10 epochs of training.</p><p>The second experiment compares the transferability provided by tangent images to prior work that addresses this topic <ref type="bibr" target="#b30">[31]</ref>. Using the network architecture from Zhang et al. <ref type="bibr" target="#b30">[31]</ref>, we train a model on the perspective images from the SYNTHIA dataset <ref type="bibr" target="#b25">[26]</ref> that correspond to the OmniSYN-THIA dataset's <ref type="bibr" target="#b30">[31]</ref> training set. We again utilize the camera normalization procedure mentioned above. We evaluate performance on the OmniSYNTHIA test set at base level 1.</p><p>Finally, the third experiment studies the impact of matching angular resolution between training and testing. For this, we apply our FCN-ResNet 101 semantic segmentation model from the first experiment to the spherical test set at various resolutions.</p><p>Results and analysis Results for the first two experiments are given in <ref type="table" target="#tab_5">Tables 3 and 4</ref>, respectively.</p><p>In the first experiment, note that both results are attained using a network trained only on perspective data. Without fine-tuning, we preserve about 97% of perspective network accuracy and 93% of the mean IOU. With only a single epoch of fine-tuning on spherical data, we see effective parity in performance, if not slightly improved performance on the spherical inputs. Finally, after 10 epochs of fine-tuning on the spherical format, the transferred network actually noticeably outperforms the original perspective network performance, even when compared to applying the same finetuning to the original network on perspective images. We surmise that this improvement comes from the greater FOV provided by the 360 ? image. These results suggest that tangent images sufficiently mitigate distortion, and, as a result, a network can begin to benefit from the ultra-wide field of view of 360 ? images. Although the tangent image representation breaks up the spherical FOV, the gradients are still computed from the full 360 ? image in our training routine, so the network still benefits from this extra information.</p><p>Additionally, we provide the results broken down by individual folds of the dataset because Fold 2 highlights the benefit of even 1 epoch of fine-tuning. Fold 2 is the hardest of the three at baseline, which might explain why tangent images provide less of a benefit than for the other two folds. However, after 10 epochs of fine-tuning on spherical data, we see Fold 2 performance increase significantly. We hypothesize that the ability to engage the wider FOV helps address particularly difficult scenes in the test set.  <ref type="table">Table 3</ref>: Transfer learning using RGB-D data from the Stan-ford2D3DS dataset. "P" means the original network trained and evaluated on perspective images only, while "S" is that network evaluated on spherical data using tangent images, without any finetuning. "FT-#" denotes epochs of fine-tuning on a given format.</p><p>The percentage next to the spherical results denotes how much of the original perspective network performance is maintained across input formats. Notice that, with fine-tuning, tangent images can actually lead to better performance than the corresponding centralperspective network.</p><p>The results of the second experiment demonstrate that the tangent image approach significantly outperforms the prior state-of-the-art without any specialized kernels or subsequent fine-tuning. Note that Zhang et al. <ref type="bibr" target="#b30">[31]</ref> only report results after 10 epochs of fine-tuning on spherical images. Using tangent images actually provides noticeably better results without any such fine-tuning, although when we add 10 epochs of fine-tuning, we see an extra performance boost. It is also worth observing that our transfer results actually outperform the Zhang et al. <ref type="bibr" target="#b30">[31]</ref> results trained natively on spherical data. Our experiments have been limited by the maximum resolution of available spherical image datasets, but this outcome suggests that network transfer with tangent images may permit even higher resolution spherical image inference.</p><p>Finally, the results of the third experiment are plotted in <ref type="figure" target="#fig_3">Figure 6</ref>. Recall that this model was trained on perspective images normalized to have a per-pixel angular resolution most similar to that of a level 8 icosahedron. This chart  "no-FT" denotes no fine-tuning on spherical data, "FT-10" mean after 10 epochs of fine-tuning, and "native" means both trained and evaluated on spherical data. Even without fine-tuning tangent images significantly improve over the previous state-of-the-art. Fine-tuning provides a small, but noticeable, additional further improvement. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Sparse keypoint correspondences</head><p>Recent research on spherical images has focused on deep learning tasks, primarily because many of those works have focused on the convolution operation. As our contribution relates to the representation of spherical data, not specifically convolution, we aim to show that our approach has applications beyond deep learning. To this end, we evaluate the use of tangent images for sparse keypoint detection, a critical step of structure-from-motion, SLAM, and a variety of other traditional computer vision applications. Data As there is no existing benchmark for this task, we create a dataset using a subset of the spherical images provided by the Stanford2D3DS dataset <ref type="bibr" target="#b0">[1]</ref>. To create this dataset, we first cluster the dataset's Area 1 images according to the provided room information. Then, for each location, we compute SIFT features <ref type="bibr" target="#b23">[24]</ref> in the equirectangular images and identify which image pairs have FOV overlap using the spherical structure-from-motion pipeline provided by the OpenMVG library <ref type="bibr" target="#b24">[25]</ref>. Next, we compute the average volumetric FOV overlap for each overlapping image pair. Because we are dealing with 360 ? images, there are no image bounds to constrain "visible" regions. Instead, we use the ground truth depth maps and pose information to back-project each image pair into a canonical pose. We then compute the percentage of right image points visible to the left camera using the the left image depth map to remove occluded points, and vice versa. We average the two values to provide an FOV overlap score for the image pair. This overlap is visualized in <ref type="figure" target="#fig_4">Figure 7</ref>. We define our keypoints dataset as the top 60 image pairs according to this overlap metric. Finally, we split the resulting dataset into an "Easy" set and "Hard" set, again based on FOV overlap. The resulting dataset statistics are shown in <ref type="table" target="#tab_7">Table 5</ref>. All images are evaluated at their full, level 10 resolution. We provide the dataset details in the supplementary material (Section 10) to enable further research.</p><p>Experimental setup To evaluate our proposed representation, we detect and describe keypoints on the tangent image grids and then render those keypoints back to the spherical image. This rendering step ensures only keypoints visible to a spherical camera at the center of the icosahedron are rendered, as the tangent images have overlapping content. We then use OpenMVG <ref type="bibr" target="#b24">[25]</ref> to compute putative correspondences and geometrically-consistent inlier matches.</p><p>Results and analysis We evaluate the quality of cor-  respondence matching at 3 different base levels using the equirectangular image format as a baseline. We compute the putative matching ratio (PMR), matching score (MS), and precision (P) metrics defined by Heinly et al. <ref type="bibr" target="#b15">[16]</ref>. For an image set S of image pairs, (L, R), with p putative correspondences, f inlier matches, and n {L,R} detected keypoints visible to both images, the metrics over the image pairs as defined as follows:</p><formula xml:id="formula_5">PMR = 1 2|S| (L,R)?S p n L + p n R MS = 1 2|S| (L,R)?S f n L + f n R P = 1 |S| (L,R)?S f p<label>(4)</label></formula><p>In the same way that we compute the FOV overlap, we use the ground truth pose and depth information provided by the dataset to determine which keypoints in the left image should be visible to the right image (n L ) and vice versa (n R ), accounting for occlusion. Results are given in <ref type="table" target="#tab_9">Table 6</ref>. Our use of tangent images has a strong impact on the resulting correspondences, particularly on the hard split. Recall that this split has a lower FOV overlap and fewer inlier matches at the baseline equirectangular representation. Improved performance in this case is thus especially useful. We observe a significant improvement in PMR in both splits. We attribute this improvement to the computation of the SIFT feature vector on our less distorted representation. Like the convolution operation, SIFT descriptors also require translational equivariance in the detection domain. Tangent images restore this property with their low-distortion representation, which enables repeatable descriptors. The better localization of the keypoints affects the inlier matches as well, resulting in a better MS score. We attribute the leveling off in performance beyond level 1 to the reduced FOV of higher level subdivisions, which impedes the detector's ability to find keypoints at larger scales.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We have presented tangent images, a spherical image representation that renders the image onto a oriented pixel grids tangent to a subdivided icosahedron. We have shown that these tangent images do not require specialized con-  volutional kernels for training CNNs and efficiently scale to represent high resolution data. We have also shown that they facilitate the transfer of networks trained on perspective images to spherical data with limited performance loss.</p><p>These results further suggest that network transfer using tangent images can open the door to processing even higher resolution spherical images. Lastly, we have demonstrated the utility of tangent images for traditional computer vision tasks in addition to deep learning. Our results indicate that tangent images can be a very useful spherical representation for a wide variety of computer vision applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Differences from CVPR 2020 Publication</head><p>In order to present the improved performance of our experiments, ensure that our results match our publicly released code, and correct some minor errors in the CVPR 2020 version of this work, we have updated this version.</p><p>An additional note about training with tangent images is added to Section 4.2. <ref type="table">Table 2</ref> in the CVPR version misreports results from Lee et al. <ref type="bibr" target="#b19">[20]</ref>. This version corrects that mistake, but we note that it has no bearing on the analysis. We clarify that transfer learning experiments are performed on RGB-D images in Section 4.3. The CVPR version has mixed notes on this, saying in one place that they are performed on RGB images only and in another that they are performed on RGB-D inputs. We provide further exploration of network transfer by fine-tuning the network on spherical data. This additional work was done to be able to better compare to prior work <ref type="bibr" target="#b30">[31]</ref> who report results only after finetuning. This extra fine-tuning is done for both network transfer experiments, and demonstrates the ability of tangent images to enable improved performance on spherical images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Material: Tangent Images for Mitigating Spherical Distortion</head><p>In this supplementary material we provide the following additional information:</p><p>Expanded discussion of some of the current limitations of tangent images (Section 7)</p><p>The details of the camera normalization process and class-level results of our transfer learning experiments (Section 8) Class-level and qualitative results for the semantic segmentation experiments at different input resolutions (Section 9) Details of our 2D3DS Keypoints dataset along with individual image pair results and a qualitative comparison of select image pairs (Section 10) Training and architecture details for all CNN experiments detailed in this paper (Section 11) An example of a spherical image represented as tangent images <ref type="figure">(Figure 8)</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Limitations</head><p>We have demonstrated the usefulness of our proposed tangent images, but we have also exposed some limitations of the formulation and opportunities for further research.  <ref type="table">Table 7</ref>: Tangent image field of view at different base levels (b) for a level 10 input. There is a slight variation depending on the input resolution due to faces near the 12 icosahedral singular points, but the values stay mostly consistent.</p><p>Resolution When using tangent images, low angular resolution spherical data is processed on potentially low pixel resolution images. This can severely limit the receptive field of the networks, to which we attribute our poor performance on the level 5 semantic segmentation task, for example. However, this limitation is only notable in the context of the existing literature, because prior work has been restricted to low resolution spherical data, as shown in <ref type="figure">Figure  2</ref> in the main paper. One viable solution is to incorporate the rendering into the convolution operation. In this way, we could regenerate the tangent images at every operation and effectively resolve the receptive field issue. However, as this is an issue for low resolution images, and our work is focused on addressing high resolution spherical data, we leave this modification for future study.</p><p>FOV The base subdivision level provides a constraint on the FOV of the tangent images. <ref type="table">Table 7</ref> shows the FOV of the tangent images at different base subdivision levels. As the FOV decreases, algorithms that rely on some sense of context or neighborhood break down. We observe this effect for both the CNN and keypoint experiments. While this is certainly a trade-off with tangent images, we have demonstrated that base levels and resolutions can be modulated to fit the required application. Another important point to observe regarding tangent image FOV is that the relationship between FOV and subdivision level does not hold perfectly at lower subdivision levels due the outsize influence of faces near the 12 singular points on the icosahedron. This effect largely disappears after base level 2, but when normalizing camera matrices to match spherical angular resolution at base levels 0 and 1, it is necessary to choose the right base level for the data. We use a base level of 1 in our transfer learning experiments on the OmniSYNTHIA dataset for this reason.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Network Transfer</head><p>In this section, we detail the camera normalization process used when training the network for transferring to spherical data. We also provide class-level results for our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.1.">Camera normalization</head><p>In order to ensure angular resolution conformity between perspective training data and spherical test data, we normalize the intrinsic camera matrices of our training images to match the angular resolution of the spherical inputs, ? s . To do this, we resample all perspective image inputs to a common camera with the desired angular resolution. The angular resolution in radians-per-pixel of an image, ? x and ? y , is defined as:</p><formula xml:id="formula_6">? x = ? x W ? y = ? y H<label>(5)</label></formula><p>where ? x and ? y are the fields of view of the image as a function of the image dimensions, W and H, and the focal lengths, f x and f y :</p><formula xml:id="formula_7">? x = 2 arctan W 2f x ? y = 2 arctan H 2f y<label>(6)</label></formula><p>Because spherical inputs have uniform angular resolution in every direction, we resample our perspective inputs likewise: ? x = ? y = ? s . Choosing camera properties For our cameranormalized perspective images, we want to choose fields of view, ? x and ? y , and image dimensions, W and H that satisfy:</p><formula xml:id="formula_8">? x W = ? y H = ? s<label>(7)</label></formula><p>While there are a variety of options that we could use, we choose to set ? x = ? y = ? 4 because ? 4 radians (45 ? ) is a reasonable field of view for a perspective image. We select W and H accordingly. For a level 8 input, this results in W = H = 128.</p><p>Normalizing intrinsics Recall the definition of the intrinsic matrix, K, given focal lengths f x and f y and principal point (c x , c y ):</p><formula xml:id="formula_9">K = ? ? f x 0 c x 0 f y c y 0 0 1 ? ?<label>(8)</label></formula><p>Given our choices for fields of view and image dimensions explained above, we compute a new, common intrinsic matrix. The new focal lengths, f x and f y , are computed as:</p><formula xml:id="formula_10">f x = W 2 tan ? x 2 f y = H 2 tan ? y 2<label>(9)</label></formula><p>and, for simplicity, the new principal point is chosen to be:</p><formula xml:id="formula_11">c x = W 2 c y = H 2<label>(10)</label></formula><p>Defining:</p><formula xml:id="formula_12">K = ? ? f x 0 c x 0 f y c y 0 0 1 ? ?<label>(11)</label></formula><p>the camera intrinsics can be normalized using the relation:</p><formula xml:id="formula_13">x = K K ?1 x<label>(12)</label></formula><p>where x and x are homogeneous pixel coordinates in the original and resampled images, respectively, and K ?1 is the inverse of the intrinsic matrix associated with the original image.</p><p>Random shifts If we were to simply resample the image using Equation <ref type="formula" target="#formula_2">(12)</ref>, we would end up with center crops of the original perspective images. In order to ensure that we do not discard useful information, we randomly shift the principle point of the original camera by some (? x , ? y ) before normalizing. This produces variations in where we crop the original image. Including this shift, we arrive at the formula we use for resampling the perspective training data:</p><formula xml:id="formula_14">x = K (K + ?) ?1 x<label>(13)</label></formula><p>where:</p><formula xml:id="formula_15">? = ? ? 0 0 ? x 0 0 ? y 0 0 0 ? ?<label>(14)</label></formula><p>To ensure our crops stay within the bounds of the original image, we want:</p><formula xml:id="formula_16">? x + P (0, 0) x ? 0 ? y + P (0, 0) y ? 0 ? x + P (W , H ) x ? W ? y + P (W , H ) y ? H (15)</formula><p>where P (x , y ) {x,y} denotes the xand y-dimensions of the new camera's coordinates projected into the original camera's coordinate system:</p><formula xml:id="formula_17">P (x , y ) = KK ?1 ? ? x y 1 ? ?<label>(16)</label></formula><p>Using this constraint, we sample crops over the entire image by randomly choosing ? x and ? y from the ranges: <ref type="table">Table 8</ref> gives the per-class results for our semantic segmentation transfer experiment. While perspective image performance should be considered an upper bound on spherical performance, note that in some classes, we appear to actually perform better on the spherical image. This is because the spherical evaluation is done on equirectangular images in order to be commensurate across representations. This means that certain labels are duplicated and others are reduced due to distortion, which can skew the per-class results.</p><formula xml:id="formula_18">f x f x c x ? c x ? ? x ? W ? c x ? f x f x (W ? c x ) f y f y c y ? c y ? ? y ? H ? c y ? f y f y (H ? c y )<label>(17)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.2.">Per-class results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.">Semantic Segmentation</head><p>We provide the per-class quantitative results for our semantic segmentation experiments from Section 4.2 in the main paper. Additionally, we qualitatively analyze the benefits of training higher resolution networks, made possible by the tangent image representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.1.">Class results</head><p>Per-class results are given for semantic segmentation in <ref type="table" target="#tab_12">Table 9</ref>. Nearly every class benefits from the highresolution inputs facilitated by tangent images. This is especially noticeable for classes with fine-grained detail and smaller objects, like chair, window, and bookcase. The table class is an interesting example of the benefit of our method. While prior work has higher accuracy, our high resolution classification has significantly better IOU. In other words, our high resolution inputs may not result in correct classifications of every table pixel, but the classifications that are correct are much more precise. This increased precision is reflected almost across the board by mean IOU performance. <ref type="figure">Figure 9</ref> gives 3 examples of semantic segmentation results at each resolution. The most obvious benefits of the higher resolution results are visible in the granularity of the output segmentation. Notice the fine detail preserved in the chairs in the level 10 output in the bottom row and even the doorway and whiteboard in the middle row. However, recall that our level 10 network uses a base level of 1. The effects of the smaller FOV of the tangent images are visible in the misclassifications of wall on the right of the level 10 output in the middle row. The level 5 network has no such problems classifying that surface, having been trained at a lower input resolution and using base level 0. Nevertheless, it is worth noting that large, homogeneous regions are going to be problematic for high resolution images, regardless of method, due to receptive field limitations of the network. If the region in question is larger than the receptive field of the network, there is no way for the network to infer context. As such, we are less concerned by errors on these large regions. <ref type="table" target="#tab_13">Tables 10 and 11</ref> give the details of the image pairs in our keypoints dataset. <ref type="table" target="#tab_15">Tables 12 and 13</ref> provide the individual metrics computed for each image pair.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.2.">Qualitative results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10.">Stanford 2D3DS Keypoints Dataset</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10.1.">Details</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10.2.">Qualitative examples</head><p>We provide a qualitative comparison of keypoint detections in <ref type="figure" target="#fig_5">Figure 10</ref>. These images illustrate two interesting effects, in particular. First, in highly distorted regions of the image that have repeatable texture, like the floor in both images, detecting on the equirectangular format produces a number of redundant keypoints distributed over the distorted region. With tangent images, we see fewer redundant points as the base level increases and the ones that are detected are more accurate and robust, as indicated by the higher MS score. Additionally, the equirectangular representation results in more keypoint detections at larger scales. These outsize scales are an effect of distortion. Rotating the camera so that the corresponding keypoints are detected at different pixel locations with different distortion characteristics will produce a different scale, and consequently a difference descriptor. This demonstrates the need for translational equivariance in keypoint detection, which requires the lower distortion provided by our tangent images. This is reflected quantitatively by the higher PMR scores. <ref type="figure" target="#fig_5">Figure 11</ref> shows an example of inlier correspondences computed on the equirectangular images and at different base levels for an image pair from the hard split. Even though we detect fewer keypoints using tangent planes, we still have the same quality or better inlier correspondences. Distortion in the equirectangular format results in keypoint over-detection, which can potentially strain the subsequent inlier model fitting. Using tangent images, we detect fewer, but higher quality, samples. This results in more efficient and reliable RANSAC <ref type="bibr" target="#b12">[13]</ref> model fitting. This is why tangent images perform noticeably better on the hard set, where there are fewer correspondences to be found.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="11.">Network Training Details</head><p>We detail the training parameters and network architectures used in our experiments to encourage reproducible research. All deep learning experiments were performed using the PyTorch library.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="11.1.">Shape classification</head><p>For the shape classification experiment, we use the network architecture from <ref type="bibr" target="#b16">[17]</ref>, replacing their MeshConv layers with 3 ? 3 2D convolutions with unit padding. For downsampling operations, we bilinearly interpolate the tangent images. We first render the ModelNet40 <ref type="bibr" target="#b29">[30]</ref> shapes to equirectangular images to be compatible with our tangent image generation implementation. The equirectangular image dimensions are 64?128, which is equivalent to the level 5 icosahedron. We train the network with a batch size of 16 and learning rate of 5 * 10 ?3 using the Adam optimizer <ref type="bibr" target="#b18">[19]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="11.2.">Semantic segmentation</head><p>We use the residual U-Net-style architecture from <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b30">31]</ref> for our semantic segmentation results at levels 5 and 7. Instead of MeshConv <ref type="bibr" target="#b16">[17]</ref> and HexConv <ref type="bibr" target="#b30">[31]</ref>, we swap in 3 ? 3 2D convolutions with unit padding. For the level 10 results, we use the fully-convolutional ResNet101 <ref type="bibr" target="#b14">[15]</ref> pretrained on COCO <ref type="bibr" target="#b20">[21]</ref> provided in the PyTorch model zoo. We train and test on each of the standard folds of the Stanford 2D3DS dataset <ref type="bibr" target="#b0">[1]</ref>. For all spherical data, evaluation metrics are computed on the re-rendered spherical image, not on the tangent images. As mentioned in the main paper, when training with tangent images, forward passes are run on all tangent images in a batch before a backward pass is run. This ensures that the whole 360 ? image is incorporated into the gradient computation.</p><p>Level 5, 7 parameters For the level 5 and 7 experiments, our tangent images were base level 0 RGB-D images, and we use all 20 tangent images from each image in a batch of 8 spherical images, resulting in an effective batch size of 160 tangent images. We use Adam optimization <ref type="bibr" target="#b18">[19]</ref> with an initial learning rate of 10 ?2 and decay by 0.9 every 20 epochs, as in <ref type="bibr" target="#b16">[17]</ref>.</p><p>Level 10 parameters For the level 10 experiments, we use RGB-D images at base level 1 and randomly sample 4 tangent images from each image in a batch of 4 spherical inputs, resulting in an effective batch size of 16 tangent images. Because the pre-trained network does not have a depth channel, we initialize the depth channel filter with zero weights. This has the effect of slowly adding the depth information to the model. Similarly, the last layer is randomly initialized, as Stanford 2D3DS has a different number of classes than COCO. We use Adam optimization <ref type="bibr" target="#b18">[19]</ref> with a learning rate of 10 ?4 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="11.3.">Transfer learning</head><p>We again use the fully-convolutional ResNet101 <ref type="bibr" target="#b14">[15]</ref> architecture pre-trained on COCO <ref type="bibr" target="#b20">[21]</ref>. We fine tune for 10 epochs on the perspective images of the Standford2D3DS dataset <ref type="bibr" target="#b0">[1]</ref>. We use a batch size of 16 and a learning rate of 10 ?4 . When fine-tuning, we again use a batch size of 16, but we reduce the learning rate to 10 ?5 .  <ref type="table">Table 8</ref>: Per-class results for the semantic segmentation transfer learning experiment on the Stanford 2D3DS dataset <ref type="bibr" target="#b0">[1]</ref>. "Perf. %" denotes how much better (+) or worse (-) the transferred network performance is as a percentage of the corresponding perspective image network. "P" and "S" refer to evaluation on perspective images and spherical images, respectively. "FT-10" refers to the network fine-tuned on the associated format for 10 epochs. These metrics are averaged over all folds.      </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 :</head><label>3</label><figDesc>MNIST classification accuracy decreases as pincushion</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>Ratio of the surface area of the subdivided icosahedron</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :</head><label>5</label><figDesc>Illustrating how the tangent image resolution increases without changing the underlying subdivision level. The field-ofview of the tangent pixel grid remains unchanged, but its resolution increases by a factor of 2 in each dimension, demonstrated by the blue dots representing pixel samples on the sphere. This scaling maintains the angular pixel resolution of higher level icosahedrons without the need for additional subdivisions. sion level and spherical image resolution by representing a spherical image as a collection of images with tunable resolution and distortion characteristics.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 :</head><label>6</label><figDesc>Results are shown for spherical semantic segmentation using a network trained on perspective images that are normalized to have a angular resolution equivalent to a level 8 spherical input. Performance drops off considerably as the angular resolution of the spherical inputs becomes more dissimilar to the training data. Level 8 results are darkened. highlights the importance of camera normalization when training on perspective images with the purpose of transferring the network. Observe how performance deteriorates as the angular resolution of the spherical input moves further from the angular resolution of the training data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 :</head><label>7</label><figDesc>FOV overlap visualized between an image pair from our keypoints benchmark derived from the Stanford 2D3DS dataset<ref type="bibr" target="#b0">[1]</ref>. The red regions in the left image represent areas visible to the right camera, and the green regions in the right image represent areas visible to the left camera.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>1 ?</head><label>1</label><figDesc>51.6 ? 31.5 ? 16.7 ? 8.4 ?</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 10 :Figure 11 :</head><label>1011</label><figDesc>Comparison of SIFT keypoint detections. Each column, top to bottom: equirectangular, L0, L1, L2 Comparison of SIFT matches on image pair 15. From top to bottom: equirectangular, L0, L1, L2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>motivate this representation using analysis from the field of cartography. Fur-</figDesc><table><row><cell>Level 0</cell><cell>Level 1</cell><cell>Level 5</cell><cell>Level 7</cell><cell>Level 8</cell><cell>Level 10</cell></row><row><cell>[12 vertices,</cell><cell>[42 vertices,</cell><cell>[10,242 vertices,</cell><cell>[163,842 vertices,</cell><cell>[655,362 vertices,</cell><cell>[10,485,762 vertices,</cell></row><row><cell>20 faces]</cell><cell>80 faces]</cell><cell>20,480 faces]</cell><cell>327,680 faces]</cell><cell>1,310,720 faces]</cell><cell>20,971,510 faces]</cell></row><row><cell>90 ? / pixel</cell><cell>45 ? / pixel</cell><cell>2.812 ? / pixel</cell><cell>0.703 ? / pixel</cell><cell>0.352 ? / pixel</cell><cell>0.088 ? / pixel</cell></row><row><cell>[2 ? 4 pixels]</cell><cell>[4 ? 8 pixels]</cell><cell>[64 ? 128 pixels]</cell><cell>[256 ? 512 pixels]</cell><cell>[512 ? 1024 pixels]</cell><cell>[2048 ? 4096 pixels]</cell></row><row><cell></cell><cell></cell><cell>[7, 17]</cell><cell>[10, 11, 20]</cell><cell>[31]</cell><cell>[Ours]</cell></row><row><cell cols="6">Figure 2: Demonstrating the number of elements, corresponding equirectangular image dimensions, and angular pixel resolution at various</cell></row></table><note>icosahedral subdivision levels. The citations beneath each denote the maximum resolution examined in those respective papers. Except for ours, they have all been limited by the pixel-to-face or pixel-to-vertex analogy.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Comparing our transfer learning results to the prior work from Zhang et al. [31] on the OmniSYNTHIA dataset at different input resolutions, s.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Statistics of our keypoints benchmark. # Corr. is the number of inlier matches detected on the equirectangular images in that split. Statistics are averaged over the splits.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6 :</head><label>6</label><figDesc>Keypoint evaluation metrics. We report the each metric's average over all image pairs per split. L{0,1,2} are the subdivision levels at which we compute the keypoints.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 9 :</head><label>9</label><figDesc>Per-class results for RGB-D inputs on the Stanford 2D3DS dataset<ref type="bibr" target="#b0">[1]</ref> Qualitative results of semantic segmentation on the Stanford 2D3DS dataset<ref type="bibr" target="#b0">[1]</ref> at different input resolutions. These results illustrate the performance gains we access by being able to scale to high resolution spherical inputs.</figDesc><table><row><cell>(a) Spherical image (equirectangular format)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 10 :</head><label>10</label><figDesc>Easy split of Stanford2D3DS keypoints dataset image pairs.</figDesc><table><row><cell></cell><cell></cell><cell>Hard</cell><cell></cell></row><row><cell cols="2">Pair ID Left Image</cell><cell>Right Image</cell><cell>FOV Overlap</cell></row><row><cell>0</cell><cell>c14611b hallway 7</cell><cell>37a4f42 hallway 7</cell><cell>0.83</cell></row><row><cell>6</cell><cell>1b253d2 hallway 7</cell><cell>6e945c8 hallway 7</cell><cell>0.84</cell></row><row><cell>7</cell><cell>5d3a59a hallway 7</cell><cell>ec0b9b3 hallway 7</cell><cell>0.81</cell></row><row><cell>8</cell><cell>ac01e35 hallway 7</cell><cell>649838b hallway 7</cell><cell>0.85</cell></row><row><cell>9</cell><cell>f6c6ce3 hallway 7</cell><cell>5221e31 hallway 7</cell><cell>0.85</cell></row><row><cell>11</cell><cell>438c5fb hallway 7</cell><cell>ec0b9b3 hallway 7</cell><cell>0.82</cell></row><row><cell>12</cell><cell>ec0b9b3 hallway 7</cell><cell>531efee hallway 7</cell><cell>0.85</cell></row><row><cell>13</cell><cell>724bbea hallway 7</cell><cell>c8c806b hallway 7</cell><cell>0.85</cell></row><row><cell>14</cell><cell>724bbea hallway 7</cell><cell>55db392 hallway 7</cell><cell>0.82</cell></row><row><cell>15</cell><cell>32d9e73 hallway 7</cell><cell>55db392 hallway 7</cell><cell>0.85</cell></row><row><cell>16</cell><cell>fcd2380 office 22</cell><cell>2d842ce office 22</cell><cell>0.85</cell></row><row><cell>17</cell><cell>2d842ce office 22</cell><cell>ffd2cca office 22</cell><cell>0.86</cell></row><row><cell>18</cell><cell>89d9828 hallway 6</cell><cell>87e6e35 hallway 6</cell><cell>0.81</cell></row><row><cell>19</cell><cell>89d9828 hallway 6</cell><cell>7d58331 hallway 6</cell><cell>0.84</cell></row><row><cell>21</cell><cell>75acaa8 hallway 6</cell><cell>87e6e35 hallway 6</cell><cell>0.84</cell></row><row><cell>22</cell><cell>92b146f hallway 6</cell><cell>8c78856 hallway 6</cell><cell>0.86</cell></row><row><cell>26</cell><cell>b640b47 hallway 6</cell><cell>87e6e35 hallway 6</cell><cell>0.80</cell></row><row><cell>27</cell><cell>bed890d hallway 6</cell><cell>97ab30c hallway 6</cell><cell>0.85</cell></row><row><cell>32</cell><cell>af50002 WC 1</cell><cell>36dd48f WC 1</cell><cell>0.84</cell></row><row><cell>33</cell><cell>1edba7e WC 1</cell><cell>e0c041d WC 1</cell><cell>0.84</cell></row><row><cell>36</cell><cell>c40ca55 office 31</cell><cell>a77fba5 office 31</cell><cell>0.85</cell></row><row><cell>38</cell><cell>4a41b27 office 31</cell><cell>da4629d office 31</cell><cell>0.82</cell></row><row><cell>40</cell><cell>da4629d office 31</cell><cell>9084f21 office 31</cell><cell>0.84</cell></row><row><cell>41</cell><cell>75361af office 31</cell><cell>ecf7fb4 office 31</cell><cell>0.82</cell></row><row><cell>42</cell><cell>2100dd9 office 4</cell><cell>26c24c7 office 4</cell><cell>0.83</cell></row><row><cell>46</cell><cell cols="2">84cdc9a conferenceRoom 1 0d600f9 conferenceRoom 1</cell><cell>0.83</cell></row><row><cell>48</cell><cell>dcab252 hallway 3</cell><cell>a9cda4d hallway 3</cell><cell>0.82</cell></row><row><cell>53</cell><cell>6549526 office 21</cell><cell>08aa476 office 21</cell><cell>0.83</cell></row><row><cell>57</cell><cell>dbcdb33 office 20</cell><cell>f02c98c office 20</cell><cell>0.83</cell></row><row><cell>59</cell><cell>24f42d6 hallway 5</cell><cell>684b940 hallway 5</cell><cell>0.84</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 11 :</head><label>11</label><figDesc>Hard split of Stanford2D3DS keypoints dataset image pairs.</figDesc><table><row><cell>Easy</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 12 :</head><label>12</label><figDesc>Keypoint matching results on individual image pairs in the easy split.</figDesc><table><row><cell>Hard</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We would like to thank David Luebke, Pierre Moulon, Li Guan, and Jared Heinly for their consultation in support of this work. This research was funded in part by Zillow.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Joint 2d-3d-semantic data for indoor scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iro</forename><surname>Armeni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sasha</forename><surname>Sax</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Amir R Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Savarese</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.01105</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Decentering distortion of lenses. Photogrammetric Engineering and Remote Sensing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Duane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Brown</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1966" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Fuller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Buckminster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cartography</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">US Patent</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="1946" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Ptex: Per-face texture mapping for production rendering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brent</forename><surname>Burley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dylan</forename><surname>Lacewell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics Forum</title>
		<imprint>
			<publisher>Wiley Online Library</publisher>
			<date type="published" when="2008" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="1155" to="1164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharan</forename><surname>Chetlur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cliff</forename><surname>Woolley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philippe</forename><surname>Vandermersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1410.0759</idno>
		<title level="m">Efficient primitives for deep learning</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taco</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>K?hler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.04893</idno>
		<title level="m">Convolutional networks for spherical signals</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Gauge equivariant convolutional networks and the icosahedral cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taco</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maurice</forename><surname>Weiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Berkay</forename><surname>Kicanaoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="1321" to="1330" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Spherical CNNs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Taco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Khler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Spherenet: Learning spherical representations for detection and classification in omnidirectional images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Coors</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandru</forename><surname>Paul Condurache</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="525" to="541" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Convolutions on spherical images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Eder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan-Michael</forename><surname>Frahm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Eder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">True</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thanh</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akash</forename><surname>Bapat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan-Michael</forename><surname>Frahm</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.11096</idno>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
	<note type="report_type">Mapped convolutions. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning so (3) equivariant representations with spherical cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Esteves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christine</forename><surname>Allen-Blanchette</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="52" to="68" />
		</imprint>
	</monogr>
	<note>Ameesh Makadia, and Kostas Daniilidis</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Random sample consensus: a paradigm for model fitting with applications to image analysis and automated cartography</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert C</forename><surname>Fischler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bolles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="381" to="395" />
			<date type="published" when="1981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">James Hastings-Trew Planet texture maps</title>
		<imprint>
			<biblScope unit="page" from="2019" to="2023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Comparative evaluation of binary features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jared</forename><surname>Heinly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enrique</forename><surname>Dunn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan-Michael</forename><surname>Frahm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="759" to="773" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Spherical CNNs on unstructured grids</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Chiyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingwei</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karthik</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kashinath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Prabhat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Niessner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Comparing geometrical properties of global grids. Cartography and Geographic Information Science</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><forename type="middle">A</forename><surname>Kimerling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Sahr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denis</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lian</forename><surname>Song</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="271" to="288" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Spherephd: Applying cnns on a spherical polyhedron representation of 360deg images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yeonkun</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaeseok</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongseob</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wonjune</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuk-Jin</forename><surname>Yoon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Smooth subdivision surfaces based on triangles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Loop</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1987" />
		</imprint>
		<respStmt>
			<orgName>University of Utah</orgName>
		</respStmt>
	</monogr>
	<note>Master&apos;s thesis. Department of Mathematics</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Object recognition from local scaleinvariant features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="volume">99</biblScope>
			<biblScope unit="page" from="1150" to="1157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Openmvg: Open multiple view geometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Moulon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Monasse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Romuald</forename><surname>Perrot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renaud</forename><surname>Marlet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Workshop on Reproducible Research in Pattern Recognition</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="60" to="74" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">The synthia dataset: A large collection of synthetic images for semantic segmentation of urban scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">German</forename><surname>Ros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Sellart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joanna</forename><surname>Materzynska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Vazquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><forename type="middle">M</forename><surname>Lopez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning spherical convolution for fast features from 360 imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Chuan</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristen</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="529" to="539" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Kernel transformer networks for compact spherical convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Chuan</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristen</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Distortion-aware convolutional filters for dense prediction in panoramic images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keisuke</forename><surname>Tateno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nassir</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Tombari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="732" to="750" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">3d shapenets: A deep representation for volumetric shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhirong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuran</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linguang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxiong</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Orientation-aware semantic segmentation on icosahedron spheres</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Liwicki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
