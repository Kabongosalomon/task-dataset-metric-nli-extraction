<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">YOSUKE SHINYA: USB: UNIVERSAL-SCALE OBJECT DETECTION BENCHMARK USB: Universal-Scale Object Detection Benchmark</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yosuke</forename><surname>Shinya</surname></persName>
						</author>
						<title level="a" type="main">YOSUKE SHINYA: USB: UNIVERSAL-SCALE OBJECT DETECTION BENCHMARK USB: Universal-Scale Object Detection Benchmark</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T16:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>to humans. For example, they will help impaired people and the elderly, save lives by autonomous driving, and provide safe entertainment during pandemics by automatic translation.</p><p>Researchers have pushed the limits of object detection systems by establishing datasets and benchmarks <ref type="bibr" target="#b41">[43]</ref>. One of the most important milestones is PASCAL VOC <ref type="bibr" target="#b18">[20]</ref>. It has enabled considerable research on object detection, leading to the success of deep learningbased methods and successor datasets such as ImageNet <ref type="bibr" target="#b54">[56]</ref> and COCO <ref type="bibr" target="#b38">[40]</ref>. Currently, COCO serves as the standard dataset and benchmark for object detection because it has several advantages over PASCAL VOC <ref type="bibr" target="#b18">[20]</ref>. COCO contains more images, categories, and objects (especially small objects) in their natural context <ref type="bibr" target="#b38">[40]</ref>. Using COCO, researchers can develop and evaluate methods for multi-scale object detection. However, the current object detection benchmarks, especially COCO, have the following two problems.</p><p>Problem 1: Variations in object scales and image domains remain limited. To realize human-level perception, computers must handle various object scales and image domains as humans can. Among various domains <ref type="bibr" target="#b74">[75]</ref>, the traffic and artificial domains have extensive scale variations (see Sec. <ref type="bibr">3.3)</ref>. COCO is far from covering them. Nevertheless, the current computer vision community is overconfident in COCO results. For example, most studies on state-of-the-art methods in 2020 only report COCO results <ref type="bibr" target="#b11">[13,</ref><ref type="bibr" target="#b12">14,</ref><ref type="bibr" target="#b34">36,</ref><ref type="bibr" target="#b36">38,</ref><ref type="bibr" target="#b73">74,</ref><ref type="bibr">81]</ref> or those for bounding box object detection <ref type="bibr" target="#b3">[5,</ref><ref type="bibr" target="#b17">19,</ref><ref type="bibr" target="#b51">53,</ref><ref type="bibr" target="#b68">69]</ref>. Readers cannot assess whether these methods are specialized for COCO or generalizable to other datasets and domains.</p><p>Problem 2: Protocols for training and evaluation are not well established. There are standard experimental settings for the COCO benchmark <ref type="bibr" target="#b9">[11,</ref><ref type="bibr" target="#b24">26,</ref><ref type="bibr" target="#b36">38,</ref><ref type="bibr" target="#b39">41,</ref><ref type="bibr" target="#b40">42,</ref><ref type="bibr" target="#b70">71,</ref><ref type="bibr">81]</ref>. Many studies train detectors within 24 epochs using a learning rate of 0.01 or 0.02 and evaluate them on images within 1333?800. These settings are not obligations but nonbinding agreements for fair comparison. Some studies do not follow the settings for accurate and fast detectors 1 . Their abnormal and scattered settings hinder the assessment of the most suitable method. Furthermore, by "buying stronger results" <ref type="bibr" target="#b56">[57]</ref>, they build a barrier for those without considerable funds to develop and train detectors.</p><p>This study makes the following two contributions to resolve the problems. Contribution 1: We introduce the Universal-Scale object detection Benchmark (USB) that consists of three datasets. In addition to COCO, we selected the Waymo Open Dataset <ref type="bibr" target="#b61">[62]</ref> and Manga109-s <ref type="bibr" target="#b1">[3,</ref><ref type="bibr" target="#b46">48]</ref> to cover various object scales and image domains. They are the largest public datasets in their domains and enable reliable comparisons. To the best of our knowledge, USB is the first benchmark beyond COCO that evaluates finer scale-wise metrics across multiple domains. We conducted extensive experiments using 15 methods and found weaknesses of existing COCO-biased methods.</p><p>Contribution 2: We established the USB protocols for fair training and evaluation, inspired by weight classes in sports and the backward compatibility of the Universal Serial Bus. Specifically, USB protocols enable fair and easy comparisons by defining multiple divisions for training epochs and evaluation image resolutions. Furthermore, we introduce compatibility across training protocols by requesting participants to report results with not only higher protocols (longer training) but also lower protocols (shorter training). To the best of our knowledge, our training protocols are the first ones that allow for both fair comparisons with shorter training and strong results with longer training. Our protocols promote inclusive, healthy, and sustainable object detection research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Multi-scale object detection. Detecting multi-scale objects is a fundamental challenge in object detection <ref type="bibr" target="#b6">[8,</ref><ref type="bibr" target="#b41">43]</ref>. Various components have been improved, including backbones and modules <ref type="bibr" target="#b13">[15,</ref><ref type="bibr" target="#b20">22,</ref><ref type="bibr" target="#b26">28,</ref><ref type="bibr" target="#b28">30,</ref><ref type="bibr" target="#b63">64]</ref>, necks <ref type="bibr" target="#b14">[16,</ref><ref type="bibr" target="#b39">41,</ref><ref type="bibr" target="#b68">69,</ref><ref type="bibr" target="#b73">74]</ref>, heads and training sample selection <ref type="bibr" target="#b42">[44,</ref><ref type="bibr" target="#b52">54,</ref><ref type="bibr">81]</ref>, and multi-scale training and testing <ref type="bibr" target="#b53">[55,</ref><ref type="bibr" target="#b59">60,</ref><ref type="bibr">81</ref>] (see Supp. B for details). Unlike most prior studies, we analyzed their methods across various object scales and image domains through the proposed benchmark.</p><p>Single-domain benchmarks. There are numerous object detection benchmarks that specialize in a specific domain or consider natural images as a single generic domain. For specific (category) object detection, recent benchmarks such as WIDER FACE <ref type="bibr">[78]</ref> and TinyPerson [79] contain tiny objects. For autonomous driving, KITTI <ref type="bibr" target="#b22">[24]</ref> and Waymo Open Dataset <ref type="bibr" target="#b61">[62]</ref> mainly evaluate three categories (car, pedestrian, and cyclist) in their leaderboards. For generic object detection, PASCAL VOC <ref type="bibr" target="#b18">[20]</ref> and COCO <ref type="bibr" target="#b38">[40]</ref> include 20 and 80 categories, respectively. The number of categories has been further expanded by recent benchmarks, such as Open Images <ref type="bibr" target="#b35">[37]</ref>, Objects365 <ref type="bibr" target="#b57">[58]</ref>, and LVIS <ref type="bibr" target="#b25">[27]</ref>. The above datasets comprise photographs, whereas Clipart1k, Watercolor2k, Comic2k <ref type="bibr" target="#b32">[34]</ref>, and Manga109-s <ref type="bibr" target="#b1">[3,</ref><ref type="bibr" target="#b46">48]</ref> comprise artificial images. Although Waymo Open Dataset <ref type="bibr" target="#b61">[62]</ref> and Manga109-s <ref type="bibr" target="#b1">[3,</ref><ref type="bibr" target="#b46">48]</ref> have extensive scale variations (see Sec. 3.3), scale-wise metrics have not been evaluated <ref type="bibr" target="#b48">[50,</ref><ref type="bibr" target="#b61">62]</ref>. Unlike the above benchmarks, our USB consists of multiple domains and contains many instances in both photographs and artificial images, and we can evaluate the generalization ability of methods.</p><p>Cross-domain benchmarks. To avoid performance drops in target domains without laborintensive annotations, many studies have tackled domain adaptation of object detection <ref type="bibr" target="#b49">[51]</ref>. Some datasets have been proposed for this setting <ref type="bibr" target="#b32">[34,</ref><ref type="bibr" target="#b33">35]</ref>. Typically, there is a strong constraint to share a label space. Otherwise, special techniques are needed for training, architectures, unified label spaces <ref type="bibr">[82,</ref><ref type="bibr">83]</ref>, and partial or open-set domain adaptation <ref type="bibr" target="#b49">[51]</ref>. In contrast, we focus on fully supervised object detection, which allows us to analyze many standard detectors.</p><p>Multi/universal-domain benchmarks. Even if target datasets have annotations for training, detectors trained and evaluated on a specific dataset may perform worse on other datasets or domains. To address this issue, some benchmarks consist of multiple datasets. In the Robust Vision Challenge (RVC) 2020 [1], detectors were evaluated on three datasets in the natural and traffic image domains. A few studies have explored the two domains by enriching RVC <ref type="bibr">[83]</ref> or making a unique combination [82], although they focus on methods for unified detectors. For universal-domain object detection, the Universal Object Detection Benchmark (UODB) <ref type="bibr" target="#b74">[75]</ref> comprises 11 datasets in the natural, traffic, aerial, medical, and artificial image domains. Although it is suitable for evaluating detectors in various domains, variations in object scales are limited. Unlike UODB, our USB focuses on universal-scale object detection. The datasets in USB contain more instances, including tiny objects, than the datasets used in UODB.</p><p>Criticism of experimental settings. For fair, inclusive, and efficient research, many studies have criticized experimental settings (e.g., <ref type="bibr" target="#b47">[49,</ref><ref type="bibr" target="#b56">57]</ref>). These previous studies do not propose fair and practical protocols for object detection benchmarks. As discussed in Sec. 1, the current object detection benchmarks allow extremely unfair settings (e.g., 25? epochs). We resolved this problem by establishing protocols for fair training and evaluation.  <ref type="bibr" target="#b31">[33,</ref><ref type="bibr" target="#b59">60]</ref>. USB covers diverse scale variations. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Benchmark Protocols of USB</head><p>Here, we present the principle, datasets, protocols, and metrics of USB. See Supp. C for additional information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Principle</head><p>We focus on the Universal-Scale Object Detection (USOD) task that aims to detect various objects in terms of object scales and image domains. Unlike separate discussions for multiscale object detection (Sec. 2) and universal (-domain) object detection <ref type="bibr" target="#b74">[75]</ref>, USOD does not ignore the relation between scales and domains (Sec. 3.3).</p><p>For various applications and users, benchmark protocols should cover from short to long training and from small to large test scales. On the other hand, they should not be scattered for meaningful benchmarks. To satisfy the conflicting requirements, we define multiple divisions for training epochs and evaluation image resolutions. Furthermore, we urge participants who have access to extensive computational resources to report results with standard training settings. This request enables fair comparison and allows many people to develop and compare object detectors. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Definitions of Object Scales</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Datasets</head><p>To establish USB, we selected the COCO <ref type="bibr" target="#b38">[40]</ref>, Waymo Open Dataset (WOD) <ref type="bibr" target="#b61">[62]</ref>, and Manga109-s (M109s) <ref type="bibr" target="#b1">[3,</ref><ref type="bibr" target="#b46">48]</ref>. WOD and M109s are the largest public datasets with many small objects in the traffic and artificial domains, respectively. Object scales in these domains vary significantly with distance and viewpoints, unlike those in the medical and aerial domains 2 . USB covers diverse scale variations qualitatively ( <ref type="figure">Figure 1</ref>) and quantitatively ( <ref type="figure" target="#fig_0">Figure 2</ref>). As shown in <ref type="table">Table 1</ref>, these datasets contain more instances and larger scale variations <ref type="bibr" target="#b59">[60]</ref> than their counterpart datasets in UODB <ref type="bibr" target="#b74">[75]</ref>. USOD needs to evaluate detectors  on datasets with many instances because more instances enable more reliable comparisons of scale-wise metrics. For the first dataset, we adopted the COCO dataset <ref type="bibr" target="#b38">[40]</ref>. COCO contains natural images of everyday scenes collected from the Internet. Annotations for 80 categories are used in the benchmark. As shown in <ref type="figure">Figure 1</ref> (left), object scales mainly depend on categories and distance. Although COCO contains objects smaller than those of PASCAL VOC <ref type="bibr" target="#b18">[20]</ref>, objects in everyday scenes (especially indoor scenes) are relatively large. Since COCO is the current standard dataset for multi-scale object detection, we adopted the same training split train2017 as the COCO benchmark to eliminate the need for retraining across benchmarks. We adopted the val2017 split (also known as minival) as the test set.</p><p>For the second dataset, we adopted the WOD, which is a large-scale, diverse dataset for autonomous driving <ref type="bibr" target="#b61">[62]</ref> with many annotations for tiny objects ( <ref type="figure" target="#fig_0">Figure 2</ref>). The images were recorded using five high-resolution cameras mounted on vehicles. As shown in <ref type="figure">Figure 1</ref> (middle), object scales vary mainly with distance. The full data splits of WOD are too large for benchmarking methods. Thus, we extracted 10% size subsets from the predefined training split (798 sequences) and validation split (202 sequences) <ref type="bibr" target="#b61">[62]</ref>. Specifically, we extracted splits based on the ones place of the frame index (frames 0, 10, ..., 190) in each sequence. We call the subsets f0train and f0val splits. Each sequence in the splits contains ?20 frames (20 s, 1 Hz), and each frame contains five images for five cameras. We used three categories (vehicle, pedestrian, and cyclist) following the official ALL_NS setting <ref type="bibr" target="#b0">[2]</ref> used in WOD competitions.</p><p>For the third dataset, we adopted the M109s <ref type="bibr" target="#b1">[3,</ref><ref type="bibr" target="#b46">48]</ref>. M109s contains artificial images of manga (Japanese comics) and annotations for four categories (body, face, frame, and text). Many characteristics differ from those of natural images. Most images are grayscale. The objects are highly overlapped <ref type="bibr" target="#b48">[50]</ref>. As shown in <ref type="figure">Figure 1</ref> (right), object scales vary unrestrictedly with viewpoints and page layouts. Small objects differ greatly from downsampled versions of large objects because small objects are drawn with simple lines and points. For example, small faces look like a sign (?). This characteristic may ruin techniques developed mainly for natural images. We carefully selected 68, 4, and 15 volumes for training, validation, and testing splits, and we call them the 68train, 4val, and 15test, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Motivation of Training Protocols</head><p>We describe the motivation of our training protocols with <ref type="table" target="#tab_3">Table 2</ref>, which compares existing protocols (A and B) and novel protocols (C and D). Protocol A is the current standard training protocol within 24 epochs, popularized by successive detectors, Detectron <ref type="bibr" target="#b24">[26]</ref>, and MMDetection <ref type="bibr" target="#b9">[11]</ref>. This protocol is fair but not suitable for slowly convergent models (e.g., DETR <ref type="bibr" target="#b8">[10]</ref>). Protocol B is lawless without any regulations. Participants can train their models with arbitrary settings suitable for them, even if they are unfair settings (e.g., standard training for existing methods and longer training for proposed ones). Since object   detectors can achieve high accuracy with long training schedules and strong data augmentation <ref type="bibr" target="#b17">[19,</ref><ref type="bibr" target="#b23">25,</ref><ref type="bibr" target="#b69">70]</ref>, participants can buy stronger results <ref type="bibr" target="#b56">[57]</ref>.</p><p>Since both existing protocols A and B have advantages and disadvantages, we considered novel protocols to bridge them. We first defined multiple divisions for training epochs, inspired by weight classes in sports. This Protocol C enables fair comparison in each division. Participants can select divisions according to their purposes and resources. However, we cannot compare models across divisions. To resolve this, we propose Protocol D by introducing backward compatibility like the Universal Serial Bus. As described above, our protocols introduce a completely different paradigm from existing limited or unfair protocols.</p><p>The training protocols mainly target resource-intensive factors that can increase the required resources 10 times or more. This decision improves fairness without obstructing novel methods and practical settings that researchers can adopt without many resources. We do not adopt factors that have large overlaps with inference efficiency, which has been considered in many previous studies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Training Protocols</head><p>For fair training, we propose the USB training protocols shown in <ref type="table" target="#tab_5">Table 3</ref>. By analogy with the backward compatibility of the Universal Serial Bus 3 , USB training protocols emphasize compatibility between protocols. Importantly, participants should report results with not only higher protocols but also lower protocols. For example, when a participant trains a model for 150 epochs with standard hyperparameters, it corresponds to USB 3.0. The participant should also report the results of models trained for 24 and 73 epochs in a paper. This reveals the effectiveness of the method by ablating the effect of long training. The readers of the paper can judge whether the method is useful for standard epochs. Since many people do not have access to extensive computational resources, such information is important.</p><p>The number of maximum epochs for USB 1.0 is 24, following a popular setting in COCO <ref type="bibr" target="#b9">[11,</ref><ref type="bibr" target="#b24">26]</ref>. We adopted 73 epochs for USB 2.0, where models trained from scratch can catch up with those trained from ImageNet pre-trained models <ref type="bibr" target="#b27">[29]</ref>. This serves as a guideline for comparison between models with and without pre-training, although perfectly fair comparisons are impossible considering the large differences caused by pre-training <ref type="bibr" target="#b58">[59]</ref>. We adopted 300 epochs for USB 3.x such that YOLOv4 <ref type="bibr" target="#b3">[5]</ref> and most EfficientDet models <ref type="bibr" target="#b69">[70]</ref> correspond to this protocol. Models trained for more than 300 epochs are regarded as Freestyle. They are not suitable for benchmarking methods, although they may push the empirical limits of detectors <ref type="bibr" target="#b8">[10,</ref><ref type="bibr" target="#b69">70]</ref>. The correspondences between Tables 2 and 3 are as follows: Protocol A corresponds to only USB 1.0; Protocol B corresponds to only Freestyle; Protocol C corresponds to all protocols (divisions) in <ref type="table" target="#tab_5">Table 3</ref> without compatibility; and Protocol D corresponds to all protocols (divisions) in <ref type="table" target="#tab_5">Table 3</ref> with compatibility.</p><p>In addition to long training schedules, hyperparameter optimization is resource-intensive. If authors of a paper fine-tune hyperparameters for their architecture, other people without sufficient computational resources cannot compare methods fairly. For hyperparameters that need to be tuned exponentially, such as learning rates and 1 ? m where m denotes momentum, the minimum ratio of hyperparameter choices should be greater than or equal to 2 (e.g., choices {0.1, 0.2, 0.4, 0.8, ...}, {0.1, 0.2, 0.5, 1.0, ...}, and {0.1, 0.3, 1.0, ...}). For hyperparameters that need to be tuned linearly, the number of choices should be less than or equal to 11 (e.g., choices {0.0, 0.1, 0.2, ..., 1.0}). When participants perform aggressive hyperparameter optimization (AHPO) by manual fine-tuning or automatic algorithms, 0.1 is added to their number of protocols. They should report both results with and without AHPO. To further improve fairness without sacrificing the protocols' simplicity, we consider it a kind of AHPO to use data augmentation techniques that more than double the time per epoch.</p><p>For models trained with annotations other than 2D bounding boxes (e.g., segmentation, keypoint, caption, and point cloud), 0.5 is added to their number of protocols. Participants should also report results without such annotations if possible for their algorithms.</p><p>For ease of comparison, we limit the pre-training datasets to the three datasets and ImageNet-1k (ILSVRC 1,000-class classification) <ref type="bibr" target="#b54">[56]</ref>. Other datasets are welcome only when the results with and without additional datasets are reported. Participants should describe how to use the datasets (e.g., fine-tuning models on WOD and M109s from COCO pre-trained models, or training a single model jointly <ref type="bibr" target="#b74">[75,</ref><ref type="bibr">83]</ref> on the three datasets).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Evaluation Protocols</head><p>For fair evaluation, we propose the USB evaluation protocols shown in <ref type="table" target="#tab_6">Table 4</ref>. By analogy with the size variations of the Universal Serial Bus connectors for various devices, USB evaluation protocols have variations in test image scales for various devices and applications.</p><p>The maximum resolution for Standard USB follows the popular test scale of 1333?800 in the COCO benchmark <ref type="bibr" target="#b9">[11,</ref><ref type="bibr" target="#b24">26]</ref>. For Mini USB, we limit the resolution based on 512?512. This resolution is popular in the PASCAL VOC benchmark <ref type="bibr" target="#b18">[20,</ref><ref type="bibr" target="#b42">44]</ref>, which contains small images and large objects. It is also popular in real-time detectors <ref type="bibr" target="#b3">[5,</ref><ref type="bibr" target="#b68">69]</ref>. We adopted a further small-scale 224?224 for Micro USB. This resolution is popular in ImageNet classification <ref type="bibr" target="#b26">[28,</ref><ref type="bibr" target="#b54">56]</ref>. Although small object detection is extremely difficult, it is suitable for low-power devices. Additionally, this protocol enables people to manage object detection tasks using one or few GPUs. To cover larger test scales than Standard USB, we define Large USB and Huge USB based on WOD resolutions (see Supp. E for the top methods). Although larger inputs (regarded as Freestyle) may be preferable for accuracy, excessively large inputs reduce the practicality of detectors.</p><p>In addition to test image scales, the presence and degree of Test-Time Augmentation (TTA) make large differences in accuracy and inference time. When using TTA, participants should report its details (including scales of multi-scale testing) and results without TTA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.7">Evaluation Metrics</head><p>We mainly use the COCO metrics <ref type="bibr" target="#b37">[39,</ref><ref type="bibr" target="#b38">40]</ref> to evaluate the performance of detectors on each dataset. We provide data format converters for WOD <ref type="bibr" target="#b2">4</ref> and M109s 5 . The COCO-style AP (CAP) for a dataset d is calculated as</p><formula xml:id="formula_0">CAP d = 1 |T | ? t?T 1 |C d | ? c?C d AP t,c , where T = {0.5, 0.55, .</formula><p>.., 0.95} denotes the predefined 10 IoU thresholds, C d denotes categories in the dataset d, and AP t,c denotes Average Precision (AP) for an IoU threshold t and a category c. For detailed analysis, five additional AP metrics (averaged over categories) are evaluated. AP <ref type="bibr" target="#b48">50</ref> and AP 75 denote AP at single IoU thresholds of 0.5 and 0.75, respectively. AP S , AP M , and AP L are variants of CAP, where target objects are limited to small (area ? 32 2 ), medium (32 2 ? area ? 96 2 ), and large (96 2 ? area) objects, respectively. The area is measured using mask annotations for COCO and bounding box annotations for WOD and M109s.</p><p>As the primary metric for USB, we use the mean COCO-style AP (mCAP) averaged over all datasets D as mCAP = 1 |D| ? d?D CAP d . Since USB adopts the three datasets described in Sec. 3.3, mCAP = (CAP COCO + CAP WOD + CAP M109s )/3. Similarly, we define five metrics from AP 50 , AP <ref type="bibr" target="#b74">75</ref> , AP S , AP M , and AP L by averaging them over the datasets.</p><p>The three COCO-style scale-wise metrics (AP S , AP M , and AP L ) are too coarse for detailed scale-wise analysis. They confuse objects of significantly different scales. For example, the absolute scale of a large object might be 100 or 1600. Thus, we introduce finer scale-wise metrics. We define the Absolute Scale AP (ASAP) and Relative Scale AP (RSAP) using exponential thresholds. ASAP partitions object scales based on absolute scales (0, 8, 16, 32, ..., 1024, ?), while RSAP partitions object scales based on relative scales (0, 1 256 , 1 128 , ..., 1 2 , 1). We call the partitions by their maximum scales. For ease of quantitative evaluation, we limit the number of detections per image to 100 across all categories <ref type="bibr" target="#b37">[39]</ref>. For qualitative evaluation, participants may raise the limit to 300 because 1% of images in the M109s 15test set contain more than 100 annotations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>Here, we present benchmark results and analysis on USB. See Supp. E for the details of the experimental settings and results, including additional analysis and ablation studies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Settings</head><p>We compared and analyzed 15 methods. With the ResNet-50-B <ref type="bibr" target="#b26">[28,</ref><ref type="bibr" target="#b28">30]</ref>   <ref type="bibr" target="#b73">[74]</ref>, and (12) DyHead <ref type="bibr" target="#b14">[16]</ref>. For a strong baseline, we trained (13) YOLOX-L <ref type="bibr" target="#b21">[23]</ref>, which adopts strong data augmentation. We designed two additional detectors for USOD by collecting methods for multi-scale object detection. (14) UniverseNet: ATSS [81] with SEPC (without iBN) <ref type="bibr" target="#b73">[74]</ref>, Res2Net-50-v1b <ref type="bibr" target="#b20">[22]</ref>, Deformable Convolutional Networks (DCN) <ref type="bibr" target="#b13">[15]</ref>, and multi-scale training. (15) UniverseNet-20.08: A variant of UniverseNet designed around August 2020 with GFL <ref type="bibr" target="#b36">[38]</ref>, SyncBN <ref type="bibr" target="#b50">[52]</ref>, iBN <ref type="bibr" target="#b73">[74]</ref>, and the light use of DCN <ref type="bibr" target="#b13">[15,</ref><ref type="bibr" target="#b73">74]</ref>       Relative Scale AP UniverseNet-20.08 YOLOX <ref type="figure">Figure 5</ref>: Relative Scale AP of strong baseline methods.</p><p>Our code is built on MMDetection <ref type="bibr" target="#b9">[11]</ref>. We trained models with Stochastic Gradient Descent (SGD) or AdamW <ref type="bibr" target="#b45">[47]</ref>. COCO models other than YOLOX <ref type="bibr" target="#b21">[23]</ref> were fine-tuned from ImageNet <ref type="bibr" target="#b54">[56]</ref> pre-trained backbones. We trained the models for WOD and M109s from the corresponding COCO pre-trained models (some COCO models from MMDetection <ref type="bibr" target="#b9">[11]</ref>). The default hyperparameters are listed in <ref type="table" target="#tab_9">Table 5</ref>. Test scales were determined within the Standard USB protocol, considering the typical aspect ratio of the images in each dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Benchmark Results on USB</head><p>Main results. We trained and evaluated the eight popular methods on USB. All the methods follow the Standard USB 1.0 protocol. The results are shown in <ref type="table" target="#tab_10">Table 6</ref>. Cascade R-CNN <ref type="bibr" target="#b7">[9]</ref> achieves the highest results in almost all metrics. The accuracy of DETR <ref type="bibr" target="#b8">[10]</ref> is low by a large margin. We show the correlation between mCAP and CAP on each dataset in <ref type="figure">Figure 3</ref>. Faster R-CNN <ref type="bibr" target="#b52">[54]</ref> is underestimated on COCO. Although Sparse R-CNN <ref type="bibr" target="#b62">[63]</ref> is much more accurate than RetinaNet <ref type="bibr" target="#b40">[42]</ref> on COCO, this is not true on the other datasets. These results show the limitation of benchmarking with COCO only. Backbones and necks. <ref type="table" target="#tab_11">Tables 7 and 8</ref> show the comparison results of the backbones and necks, respectively. Swin-T <ref type="bibr" target="#b43">[45]</ref> shows lower AP than ResNet-50-B <ref type="bibr" target="#b26">[28,</ref><ref type="bibr" target="#b28">30]</ref> on M109s. SEPC <ref type="bibr" target="#b73">[74]</ref> deteriorates WOD CAP.</p><p>Strong baselines. <ref type="table" target="#tab_13">Table 9</ref> shows the results of the three strong baselines. UniverseNet-20.08 achieves the highest mCAP of 52.1%. YOLOX-L <ref type="bibr" target="#b21">[23]</ref> shows better results on WOD and M109s, which contain many small objects, possibly due to better AP S . Scale-wise AP. We show RSAP on USB in <ref type="figure" target="#fig_1">Figures 4 and 5</ref>. Since the proposed metrics partition object scales evenly-spaced exponentially, we can confirm the continuous change. RSAP does not increase monotonically but rather decreases at relative scales greater than 1/4. We cannot find this weakness from the coarse COCO-style scale-wise AP in <ref type="table" target="#tab_10">Table 6</ref> etc. The difficulty of very large objects may be caused by truncation or unusual viewpoints <ref type="bibr" target="#b29">[31]</ref>. The results also show that different methods are good at different scales. We need further analysis in future research to develop methods that can detect both tiny and large objects. Details on each dataset. We show detailed results on each dataset in Supp. E. AP S on WOD is at most 12.0%, which is much lower than AP S on COCO. This highlights the limitation of COCO and current detectors. Adding SEPC <ref type="bibr" target="#b73">[74]</ref> to ATSS [81] decreases all metrics on WOD except for AP L . We found that this reduction does not occur at large test scales in higher USB evaluation protocols. Improvements by ATSS [81] on M109s are smaller than those on COCO and WOD due to the drop of face AP. We conjecture that this phenomenon comes from the domain differences discussed in Sec. 3.3 and prior work <ref type="bibr" target="#b48">[50]</ref>. Qualitative results. We show some qualitative results of the best detector (UniverseNet-20.08) in <ref type="figure">Figure 1</ref>. Although most detections are accurate, it still suffers from classification error, localization error, and missing detections of tiny vehicles and small manga faces.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions and Discussions</head><p>We introduced USB, a benchmark for universal-scale object detection. To resolve unfair comparisons in existing benchmarks, we established USB training/evaluation protocols. With the benchmark, we found weaknesses in existing methods to be addressed in future research.</p><p>There are several limitations to this work. (1) USB has imbalances in domains and categories because it depends on the existing datasets that have large scale variations. It will be an important direction to construct a well-balanced and more comprehensive benchmark that contains more domains and categories. The current computer vision community places a high value on state-of-the-art results. Thus, there is a large incentive to make unfair comparisons for overly accurate results, like DETR <ref type="bibr" target="#b8">[10]</ref> and EfficientDet <ref type="bibr" target="#b68">[69]</ref>. We need to create a system that emphasizes fair comparisons. To improve effectiveness in broad areas, creating a checklist that can be incorporated into author/reviewer guidelines is a promising future direction. We believe that our work is an important step toward realizing fair and inclusive research by connecting various experimental settings.</p><p>[77] Hang Xu, Chenhan Jiang, Dapeng Feng, Chaoqiang Ye, Rui Sun, and Xiaodan Liang. SPNAS-Noah: Single Cascade-RCNN with backbone architecture adaption for Waymo 2D detection. https://sites.google.com/view/cvpr20-scalability /wod-reports, Accessed on June 21, 2020.</p><p>[78] Shuo Yang, Ping Luo, Chen-Change Loy, and Xiaoou Tang. WIDER FACE: A face detection benchmark. In CVPR, 2016.</p><p>[79] Xuehui Yu, Yuqi Gong, Nan Jiang, Qixiang Ye, and Zhenjun Han. Scale match for tiny person detection. In WACV, 2020.</p><p>[80] Hongyi Zhang, Moustapha Cisse, Yann N. Dauphin, and David Lopez-Paz. mixup: Beyond empirical risk minimization. In ICLR, 2018.</p><p>[81] Shifeng Zhang, Cheng Chi, Yongqiang Yao, Zhen Lei, and Stan Z. Li. Bridging the gap between anchor-based and anchor-free detection via adaptive training sample selection.</p><p>In CVPR, 2020.</p><p>[82] Xiangyun Zhao, Samuel Schulter, Gaurav Sharma, Yi-Hsuan Tsai, Manmohan Chandraker, and Ying Wu. Object detection with a unified label space from multiple datasets.</p><p>In ECCV, 2020.</p><p>[83] Xingyi Zhou, Vladlen Koltun, and Philipp Kr?henb?hl. Simple multi-dataset detection.</p><p>In CVPR, 2022.</p><p>[84] Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, and Jifeng Dai. Deformable DETR: Deformable transformers for end-to-end object detection. In ICLR, 2021.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Material</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Discussions on Research Ethics</head><p>Limitations. In addition to the limitations described in the main text, this work has the following limitations.</p><p>(1) USB depends on datasets with many instances. Reliable scale-wise metrics for small datasets should be considered. (2) USB does not cover the resolution of recent smartphone cameras (e.g., 4000?3000). Such high-resolution images may encourage completely different methods. (3) USB, as well as UODB <ref type="bibr" target="#b74">[75]</ref>, has a large imbalance in the number of images. If participants train a unified detector <ref type="bibr" target="#b74">[75,</ref><ref type="bibr">83]</ref>, they will need strategies for dataset sampling <ref type="bibr">[83]</ref>.</p><p>Potential negative societal impacts. Improving the accuracy and universality of object detectors could improve the performance of autonomous weapons. To mitigate the risk, we could develop more detectors for entertainment to increase people's happiness and decrease their hatred. Besides, detectors might be misused for surveillance systems (e.g., as a part of person tracking methods). To mitigate the risk, the computer vision community will need to have discussions with national and international organizations to regulate them appropriately.</p><p>Existing assets. We used the assets listed in  Consent. See <ref type="bibr" target="#b1">[3]</ref> for M109s. For the other datasets, we could not find whether and how consent was obtained. It will be impossible to obtain consent from people recorded in datasets for autonomous driving such as WOD <ref type="bibr" target="#b61">[62]</ref>.</p><p>Privacy. Faces and license plates in WOD <ref type="bibr" target="#b61">[62]</ref> are blurred. COCO images may harm privacy because they probably contain personally identifiable information. However, COCO <ref type="bibr" target="#b38">[40]</ref> is so popular that the computer vision community cannot stop using it suddenly. This paper will be a step toward reducing the dependence on COCO.</p><p>Offensive contents. M109s covers various contents <ref type="bibr" target="#b1">[3]</ref>. This characteristic is useful to develop universal-scale object detectors. One of the authors checked many images of the three datasets with eyes and felt that some images in M109s may be considered offensive (e.g., violence in battle manga and nudity in romantic comedy). Thus, researchers should be careful how they use it. It is also valuable to develop methods to detect such scenes using the dataset. ) times longer than that for COCO. This is reasonable as a next-generation benchmark after COCO. Furthermore, the proposed protocols provide incentives to avoid computationally intensive settings <ref type="bibr" target="#b68">[69]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Details of Related Work B.1 Components for Multi-Scale Object Detection</head><p>Backbones and modules. Inception module <ref type="bibr" target="#b63">[64]</ref> arranges 1?1, 3?3, and 5?5 convolutions to cover multi-scale regions. Residual block <ref type="bibr" target="#b26">[28]</ref> adds multi-scale features from shortcut connections and 3?3 convolutions. ResNet-C and ResNet-D <ref type="bibr" target="#b28">[30]</ref> replace the first layer of ResNet with the deep stem (three 3?3 convolutions) <ref type="bibr" target="#b64">[65]</ref>. Res2Net module <ref type="bibr" target="#b20">[22]</ref> stacks 3?3 convolutions hierarchically to represent multi-scale features. Res2Net-v1b <ref type="bibr" target="#b20">[22]</ref> adopts deep stem with Res2Net module. Deformable convolution module in Deformable Convolutional Networks (DCN) <ref type="bibr" target="#b13">[15]</ref> adjusts receptive field adaptively by deforming the sampling locations of standard convolutions. These modules are mainly used in backbones. Necks. To combine and enhance backbones' representation, necks follow backbones. Feature Pyramid Networks (FPN) <ref type="bibr" target="#b39">[41]</ref> adopt top-down path and lateral connections like architectures for semantic segmentation. Scale-Equalizing Pyramid Convolution (SEPC) <ref type="bibr" target="#b73">[74]</ref> introduces pyramid convolution across feature maps with different resolutions and utilizes DCN to align the features. Dynamic Head (DyHead) <ref type="bibr" target="#b14">[16]</ref> improves SEPC with two types of attention mechanisms. Heads and training sample selection. Faster R-CNN <ref type="bibr" target="#b52">[54]</ref> spreads multi-scale anchors over a feature map. SSD <ref type="bibr" target="#b42">[44]</ref> spreads multi-scale anchors over multiple feature maps with different resolutions. Adaptive Training Sample Selection (ATSS) [81] eliminates the need for multi-scale anchors by dividing positive and negative samples according to object statistics across pyramid levels. Multi-scale training and testing. Traditionally, the image pyramid is an essential technique to handle multi-scale objects <ref type="bibr" target="#b53">[55]</ref>. Although recent detectors can output multi-scale objects from a single-scale input, many studies use multi-scale inputs to improve performance <ref type="bibr" target="#b40">[42,</ref><ref type="bibr" target="#b52">54,</ref><ref type="bibr" target="#b73">74,</ref><ref type="bibr">81]</ref>. In a popular implementation <ref type="bibr" target="#b9">[11]</ref>, multi-scale training randomly chooses a scale at each iteration for (training-time) data augmentation. Multi-scale testing infers multi-scale inputs and merges their outputs for Test-Time Augmentation (TTA). Scale Normalization for Image Pyramids (SNIP) <ref type="bibr" target="#b59">[60]</ref> limits the range of object scales at each image scale during training and testing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Scale-Wise Metrics</head><p>Many studies have introduced different scale-wise metrics <ref type="bibr" target="#b5">[7,</ref><ref type="bibr" target="#b16">18,</ref><ref type="bibr" target="#b29">31,</ref><ref type="bibr" target="#b38">40,</ref><ref type="bibr" target="#b54">56,</ref><ref type="bibr" target="#b72">73,</ref><ref type="bibr">79]</ref>. Unlike these studies, we introduce two types of finer scale-wise metrics based on the absolute scale and relative scale <ref type="bibr">[79]</ref>. More importantly, we evaluated them on the datasets that have extensive scale variations and many instances in multiple domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Details of Protocols</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1 Dataset Splits of Manga109-s</head><p>The Manga109-s dataset (87 volumes) is a subset of the full Manga109 dataset (109 volumes) <ref type="bibr" target="#b1">[3]</ref>. Unlike the full Manga109 dataset, the Manga109-s dataset can be used by commercial organizations. The dataset splits for the full Manga109 dataset used in prior work <ref type="bibr" target="#b48">[50]</ref> cannot be used for the Manga109-s dataset. We defined the Manga109-s dataset Volume Genre   <ref type="table" target="#tab_18">Table 11</ref>. Unlike alphabetical order splits used in the prior work <ref type="bibr" target="#b48">[50]</ref>, we selected the volumes carefully. The 15test set was selected to be well-balanced for reliable evaluation. Five volumes in the 15test set were selected from the 10 test volumes used in <ref type="bibr" target="#b48">[50]</ref> to enable partially direct comparison. All the authors of the 15test and 4val set are different from those of the 68train set to evaluate generalizability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 Number of Images</head><p>There are 118,287 images in COCO train2017, 5,000 in COCO val2017, 79,735 in WOD f0train, 20,190 in WOD f0val, 6,467 in M109s 68train, 399 in M109s 4val, and 1,289 in M109s 15test. Following prior work <ref type="bibr" target="#b48">[50]</ref>, we exclude M109s images without annotations because objects on irregular pages are not annotated. We selected the test splits from images with publicly available annotations to reduce labor for submissions. Participants should not fine-tune hyperparameters based on the test splits to prevent overfitting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3 Importance of Many Instances</head><p>Here, we highlight the importance of a larger number of instances than UODB <ref type="bibr" target="#b74">[75]</ref>. We show that if we introduced scale-wise metrics to UODB, the results would be unreliable. Water-color2k, one of the datasets adopted by UODB, has 6 classes and 27 bicycle instances <ref type="bibr" target="#b32">[34]</ref>. If we equally divided the dataset for training and evaluation and they had the same number of small, medium, and large bicycles, the average number of bicycles of a particular scale in the evaluation split would be 4.5. Since the 4.5 bicycles affect 1 6 of a scale-wise metric, a single error can change the results by 3.7%. Thus, randomness can easily reverse the ranking between methods, making the benchmark results unreliable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.4 Exceptions of Protocols</head><p>The rounding error of epochs between epoch-and iteration-based training can be ignored when calculating the maximum epochs. Small differences of eight pixels or less can be ignored when calculating the maximum resolutions. For example, DSSD513 <ref type="bibr" target="#b19">[21]</ref> will be compared in Mini USB.</p><p>The number of additional images loaded for multi-image data augmentation techniques (e.g., Between-Class Learning <ref type="bibr" target="#b71">[72]</ref>, mixup [80], RICAP <ref type="bibr" target="#b65">[66]</ref>, and Mosaic <ref type="bibr" target="#b3">[5]</ref>) can be ignored when calculating the maximum epochs. Even in that case, the time per epoch is considered according to another provision in Sec. 3.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.5 Constraints on Training Time</head><p>We do not adopt constraints on training time as the major constraints of the training protocols because they have the following issues.</p><p>? It is difficult to measure training time on unified hardware. ? It is complicated to measure training time, calculate allowable epochs, and set learning rate schedules for each model. ? It is difficult to compare with previous studies, which align the number of epochs. ? They will reduce the value of huge existing resources for standard training epochs (trained models, configuration files, and experimental results) provided by popular object detection libraries such as MMDetection <ref type="bibr" target="#b9">[11]</ref>. ? They overemphasize implementation optimization rather than the trial and error of novel methods. ? There are overlaps between the factors of training time and those of inference time.</p><p>The proposed constraints on training epochs are much easier to adopt and more reasonable. Furthermore, our protocols compensate for the shortcomings of the epoch constraints by defining the provisions for hyperparameter optimization and data augmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.6 Characteristics of Scale-Wise Metrics</head><p>ASAP and COCO-style scale-wise metrics are based on the absolute scale. It has a weakness that it changes with image resizing. To limit inference time and GPU memory consumption, and to ensure fair comparisons, input image scales are typically resized. If they are smaller than the original image scales, relative scales have direct effects on accuracy rather than absolute scales. Furthermore, objects with the same absolute scale in the original images may have different absolute scales in the input images. Fluctuating object scale thresholds is not desirable for scale-wise metrics.</p><p>In addition, ASAP is not suitable for evaluating accuracy for very large objects. It may be impossible to calculate ASAP for large absolute scales on some datasets. In the case of USB, we cannot calculate ASAP ? on COCO because the absolute scales of COCO objects are smaller than 1024 (we filled ASAP ? on COCO with zero in experiments). Furthermore, ASAP for large absolute scales may show unusual behavior. For example, in the evaluation of ASAP ? on M109s, all predictions larger than 1024 of absolute scales have larger IoUs than 0.5 with an object of image resolution size (1654?1170).   <ref type="bibr" target="#b20">[22]</ref>. PConv (Pyramid Convolution) and iBN (integrated Batch Normalization) are the components of SEPC <ref type="bibr" target="#b73">[74]</ref>. The DCN columns indicate where to apply DCN. "P": The PConv modules in the combined head of SEPC <ref type="bibr" target="#b73">[74]</ref>. "LC": The extra head of SEPC for localization and classification <ref type="bibr" target="#b73">[74]</ref>. "c3-c5": conv3_x, conv4_x, and conv5_x layers in ResNet-style backbones <ref type="bibr" target="#b26">[28]</ref>. "c5": conv5_x layers in ResNet-style backbones <ref type="bibr" target="#b26">[28]</ref>. ATSEPC: ATSS with SEPC (without iBN). MStrain: Multi-scale training. FPS: Frames per second on one V100 with mixed precision.</p><p>We prefer RSAP to ASAP due to the above-mentioned weaknesses of ASAP. Absolute scales may be important depending on whether and how participants resize images. In that case, RSAP and ASAP can be used complementarily.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Details of UniverseNets</head><p>For fast and accurate detectors for USOD, we designed UniverseNets. We adopted singlestage detectors for efficiency. We show the detailed architectures in <ref type="table" target="#tab_3">Table 12</ref>.</p><p>As a baseline model, we used the RetinaNet <ref type="bibr" target="#b40">[42]</ref> implemented in MMDetection <ref type="bibr" target="#b9">[11]</ref>. Specifically, the backbone is ResNet-50-B <ref type="bibr" target="#b28">[30]</ref> (a variant of ResNet-50 <ref type="bibr" target="#b26">[28]</ref>, also known as the PyTorch style). The neck is FPN <ref type="bibr" target="#b39">[41]</ref>. We used focal loss <ref type="bibr" target="#b40">[42]</ref>, single-scale training, and single-scale testing.</p><p>Built on the RetinaNet baseline, we designed UniverseNet by collecting human wisdom about multi-scale object detection as of May 2020. We used ATSS [81] and SEPC without iBN <ref type="bibr" target="#b73">[74]</ref> (hereafter referred to as ATSEPC). The backbone is Res2Net-50-v1b <ref type="bibr" target="#b20">[22]</ref>. We adopted Deformable Convolutional Networks (DCN) <ref type="bibr" target="#b13">[15]</ref> in the backbone and neck. We used multi-scale training. Unless otherwise stated, we used single-scale testing for efficiency.</p><p>By adding GFL <ref type="bibr" target="#b36">[38]</ref>, SyncBN <ref type="bibr" target="#b50">[52]</ref>, and iBN <ref type="bibr" target="#b73">[74]</ref>, we designed three variants of Uni-verseNet around August 2020. UniverseNet-20.08d heavily uses DCN <ref type="bibr" target="#b13">[15]</ref>. UniverseNet-20.08 speeds up inference (and training) by the light use of DCN <ref type="bibr" target="#b13">[15,</ref><ref type="bibr" target="#b73">74]</ref>. UniverseNet-20.08s further speeds up inference using the ResNet-50-C <ref type="bibr" target="#b28">[30]</ref> backbone.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Details of Experiments</head><p>Here, we show the details of experimental settings and results. See also the code to reproduce our settings including minor hyperparameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.1 Common Settings</head><p>We follow the learning rate schedules of MMDetection <ref type="bibr" target="#b9">[11]</ref>, which are similar to those of Detectron <ref type="bibr" target="#b24">[26]</ref>. Specifically, the learning rates are reduced by 10? in two predefined epochs. Epochs for the first learning rate decay, the second decay, and ending training are <ref type="bibr" target="#b6">(8,</ref><ref type="bibr" target="#b9">11,</ref><ref type="bibr" target="#b10">12)</ref> for the 1? schedule, <ref type="bibr" target="#b14">(16,</ref><ref type="bibr" target="#b20">22,</ref><ref type="bibr" target="#b22">24)</ref> for the 2? schedule, and <ref type="bibr" target="#b14">(16,</ref><ref type="bibr" target="#b17">19,</ref><ref type="bibr" target="#b18">20)</ref> for the 20e schedule. To avoid overfitting by small learning rates <ref type="bibr" target="#b58">[59]</ref>, the 20e schedule is reasonable. We mainly used the 1? schedule (12 epochs).</p><p>We mainly used ImageNet <ref type="bibr" target="#b54">[56]</ref> pre-trained backbones that are standard in MMDetection <ref type="bibr" target="#b9">[11]</ref>. Some pre-trained backbones not supported in MMDetection were downloaded from the repositories of Res2Net <ref type="bibr" target="#b20">[22]</ref> and Swin Transformer <ref type="bibr" target="#b43">[45]</ref>. We used the COCO pre-trained models of the MMDetection <ref type="bibr" target="#b9">[11]</ref> repository for several existing methods (Faster R-CNN <ref type="bibr" target="#b52">[54]</ref> with FPN <ref type="bibr" target="#b39">[41]</ref>, Cascade R-CNN <ref type="bibr" target="#b7">[9]</ref>, RetinaNet <ref type="bibr" target="#b40">[42]</ref>, ATSS [81], GFL <ref type="bibr" target="#b36">[38]</ref>, and Sparse R-CNN <ref type="bibr" target="#b62">[63]</ref>). We trained most models with mixed precision and 4 GPUs (? 4 images per GPU). We mainly used NVIDIA T4 GPUs on the Google Cloud Platform. All results on USB and all results of UniverseNets are single model results without ensemble. We could not train each object detector multiple times with different random seeds to report error bars because training object detectors is too computationally expensive.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.2 Settings for Specific Methods</head><p>Many recent detectors <ref type="bibr" target="#b8">[10,</ref><ref type="bibr" target="#b43">45]</ref> adopt the AdamW optimizer <ref type="bibr" target="#b45">[47]</ref>. Since AdamW does not necessarily give better results than SGD, we follow the optimizer settings of the official implementations and MMDetection <ref type="bibr" target="#b9">[11]</ref>. Specifically, for COCO and WOD, we used AdamW with an initial learning rate of 10 ?4 for DETR <ref type="bibr" target="#b8">[10]</ref>  <ref type="bibr" target="#b44">[46]</ref>).</p><p>We trained YOLOX-L <ref type="bibr" target="#b21">[23]</ref> with SGD and an initial learning rate of 2.5?10 ?3 for COCO and WOD. The test scale on COCO is 1024?1024. Since YOLOX models are trained from scratch <ref type="bibr" target="#b21">[23]</ref>, we trained a COCO model for 36 epochs (USB 2.0) such that it achieves similar AP to a model trained for 12 epochs from an ImageNet pre-trained model <ref type="bibr" target="#b58">[59]</ref>. We also trained a COCO model for 24 epochs (USB 1.0).</p><p>We used multi-scale training for YOLOX-L <ref type="bibr" target="#b21">[23]</ref> and UniverseNets. The range of shorter side pixels for most models is 480-960, following prior work <ref type="bibr" target="#b73">[74]</ref>. That for YOLOX-L <ref type="bibr" target="#b21">[23]</ref> on COCO is 512-1024, and that for UniverseNets on WOD is 640-1280. Since we do not have sufficient computational resources, these hyperparameters have room for improvement.</p><p>For comparison with state-of-the-art methods on COCO, we used the 2? schedule (24 epochs) for most models and the 20e schedule (20 epochs) for UniverseNet-20.08d due to overfitting with the 2? schedule. For comparison with state-of-the-art methods on WOD, we trained UniverseNet on the WOD full training set for 7 epochs. We used a learning rate of 10 ?3 for 6 epochs and 10 ?4 for the last epoch.  For comparison with state-of-the-art methods with TTA on COCO, we used soft voting with 13-scale testing and horizontal flipping following the original implementation of ATSS <ref type="bibr">[81]</ref>. Specifically, shorter side pixels are (400, 500, 600, 640, 700, 800, 900, 1000, 1100, 1200, 1300, 1400, 1800), while longer side pixels are their 1.667?. For the 13 test scales, target objects are limited to corresponding 13 predefined ranges ((96, ?), (96, ?), (64, ?), (64, ?), (64, ?), (0, ?), (0, ?), (0, ?), (0, 256), (0, 256), (0, 192), (0, 192), (0, 96)), where each tuple denotes the minimum and maximum absolute scales. We also evaluated 5-scale TTA because the above-mentioned ATSS-style TTA is slow. We picked (400, 600, 800, 1000, 1200) for shorter side pixels, and ((96, ?), (64, ?), (0, ?), (0, ?), (0, 256)) for absolute scale ranges.</p><p>For M109s, we used learning rates 8? those of COCO and WOD. The value is roughly tuned based on a preliminary experiment with the RetinaNet <ref type="bibr" target="#b40">[42]</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.3 Evaluation with Scale-Wise Metrics</head><p>We show RSAP and ASAP of popular baseline methods on USB in <ref type="figure" target="#fig_1">Figures 4 and 6</ref>, respectively. They do not increase monotonically but rather decrease at relative scales greater than 1/4 or absolute scales greater than 512. The difficulty of very large objects may be caused by truncation or unusual viewpoints <ref type="bibr" target="#b29">[31]</ref>. Except for the issues of ASAP ? discussed in Sec. C.6, ASAP shows similar changes to RSAP. We conjecture that this is because image resolutions do not change much in each dataset of USB. RSAP 1 64 and ASAP <ref type="bibr" target="#b14">16</ref> are less than 10%, which indicates the difficulty of tiny object detection <ref type="bibr">[79]</ref>. RetinaNet <ref type="bibr" target="#b40">[42]</ref> shows low AP for small objects, while Faster R-CNN <ref type="bibr" target="#b52">[54]</ref> with FPN <ref type="bibr" target="#b39">[41]</ref> shows low AP for large objects. These results are consistent with the benchmark results of previous work <ref type="bibr" target="#b30">[32]</ref>, which compares SSD <ref type="bibr" target="#b42">[44]</ref> with Faster R-CNN without FPN on COCO. For further analysis, it will be worth comparing the design choice of pyramid levels <ref type="bibr" target="#b40">[42,</ref><ref type="bibr">79]</ref>.</p><p>We show RSAP and ASAP of strong baseline methods on USB in <ref type="figure" target="#fig_4">Figures 5 and 7</ref>, respectively. UniverseNet-20.08 is more accurate for large objects, and YOLOX-L [23] is more accurate for small objects. We conjecture that the former is due to SEPC <ref type="bibr" target="#b73">[74]</ref> and DCN <ref type="bibr" target="#b13">[15]</ref> (see Sec. E.7 and E.8). For the latter, YOLOX may be biased toward small object detection due to Mosaic augmentation <ref type="bibr" target="#b3">[5]</ref> and the absence of pre-training <ref type="bibr" target="#b58">[59]</ref> on ImageNet that has larger objects than COCO <ref type="bibr" target="#b59">[60]</ref>.  <ref type="table" target="#tab_11">Table 17</ref>: Waymo Open Dataset Challenge 2020 2D detection <ref type="bibr" target="#b0">[2]</ref>.</p><p>ods in their tables without specifying the difference in training epochs. The compatibility of the USB training protocols resolves this disorder. We hope that many papers report results with the protocols for inclusive, healthy, and sustainable development of detectors.</p><p>To simulate the compatibility from Standard USB 3.0 to 1.0, we refer to the training log of the EfficientDet author. The AP of EfficientDet-D4 <ref type="bibr" target="#b68">[69]</ref> on COCO minival is 43.8% at 23 epoch <ref type="bibr" target="#b66">[67]</ref>. Although it could be improved by changing the learning rate schedule, EfficientDet's inference efficiency is not compatible with training efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.6 Comparison with State-of-the-Art</head><p>WOD. For comparison with state-of-the-art methods on WOD, we submitted the detection results of UniverseNet to the Waymo Open Dataset Challenge 2020 2D detection, a competition held at a CVPR 2020 workshop. The primary metric is AP/L2, a KITTI-style AP evaluated with LEVEL_2 objects <ref type="bibr" target="#b0">[2,</ref><ref type="bibr" target="#b61">62]</ref>. We used multi-scale testing with soft-NMS <ref type="bibr">[6]</ref>. The shorter side pixels of test scales are (960, 1600, 2240), including 8 pixels of padding. These scales enable utilizing SEPC <ref type="bibr" target="#b73">[74]</ref> (see Sec. E.8) and detecting small objects. <ref type="table" target="#tab_11">Table 17</ref> shows the top teams' results. UniverseNet achieves 67.42% AP/L2 without multi-stage detectors, ensembles, expert models, or heavy backbones, unlike other top methods. RW-TSDet <ref type="bibr" target="#b31">[33]</ref> overwhelms other multi-stage detectors, whereas UniverseNet overwhelms other single-stage detectors. These two methods used light backbones and large test scales <ref type="bibr" target="#b2">[4]</ref>. Interestingly, the maximum test scales are the same (3360?2240). We conjecture that this is not a coincidence but a convergence caused by searching the accuracy saturation point. Manga109-s. To the best of our knowledge, no prior work has reported detection results on the Manga109-s dataset (87 volumes). Although many settings differ, the state-of-the-art method on the full Manga109 dataset (109 volumes, non-public to commercial organizations) achieves 77.1-92.0% (mean: 84.2%) AP 50 on ten test volumes <ref type="bibr" target="#b48">[50]</ref>. The mean AP 50 of UniverseNet-20.08 on the 15test set (92.5%) is higher than those results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.7 Ablation Studies for UniverseNets</head><p>We show the results of ablation studies for UniverseNets on COCO in <ref type="table" target="#tab_12">Table 18</ref>. As shown in <ref type="table" target="#tab_12">Table 18a</ref>, ATSEPC (ATSS [81] with SEPC without iBN <ref type="bibr" target="#b73">[74]</ref>) outperforms ATSS by a large margin. The effectiveness of SEPC for ATSS is consistent with those for other detectors reported in the SEPC paper <ref type="bibr" target="#b73">[74]</ref>. As shown in <ref type="table" target="#tab_12">Table 18b</ref>, UniverseNet further improves AP metrics by ?5% by adopting Res2Net-v1b <ref type="bibr" target="#b20">[22]</ref>, DCN <ref type="bibr" target="#b13">[15]</ref>, and multi-scale training. As shown in <ref type="table" target="#tab_12">Table 18c</ref>, adopting GFL <ref type="bibr" target="#b36">[38]</ref> improves AP by 0.8%. There is room for improvement of AP S in the Quality Focal Loss of GFL <ref type="bibr" target="#b36">[38]</ref>. As shown in <ref type="table" target="#tab_12">Table 18d</ref>, UniverseNet-20.08d achieves 48.6% AP by making more use of BatchNorm (SyncBN <ref type="bibr" target="#b50">[52]</ref> and iBN <ref type="bibr" target="#b73">[74]</ref>).  <ref type="table" target="#tab_13">Table 19</ref>: UniverseNet-20.08 on Manga109-s 15test with different pre-trained models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.9 Evaluation with KITTI-Style AP</head><p>We evaluated the KITTI-style AP (KAP) on WOD. KAP is a metric used in benchmarks for autonomous driving <ref type="bibr" target="#b22">[24,</ref><ref type="bibr" target="#b61">62]</ref>. Using different IoU thresholds (0.7 for vehicles, and 0.5 for pedestrians and cyclists), KAP is calculated as KAP = (AP 0.7,veh. +AP 0.5,ped. +AP 0.5,cyc. )/3. The results of KAP are shown in <ref type="figure" target="#fig_5">Figure 8b</ref>. GFL <ref type="bibr" target="#b36">[38]</ref> and Cascade R-CNN <ref type="bibr" target="#b7">[9]</ref>, which focus on localization quality, are less effective for KAP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.10 Effects of COCO Pre-Training</head><p>To verify the effects of COCO pre-training, we trained UniverseNet-20.08 on M109s from different pre-trained models. <ref type="table" target="#tab_13">Table 19</ref> shows the results. COCO pre-training improves all the metrics, especially body AP.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Distributions of objects' relative scale</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>Relative Scale AP of popular baseline methods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>( 2 )</head><label>2</label><figDesc>The architectures and results of the 15 methods are still biased toward COCO due to development and pre-training on COCO. Less biased and more universal detectors should be developed in future research. (3) We could not train detectors with higher protocols than USB 1.0 due to limited resources. Although the compatibility enables comparison in low protocols, still only well-funded researchers can compare detectors in high protocols. Other efforts are also needed to ensure fairness and inclusion in research. See Supp. A for discussion on other limitations and research ethics.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 :</head><label>6</label><figDesc>Absolute Scale AP of popular baseline methods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 :</head><label>7</label><figDesc>Absolute Scale AP of strong baseline methods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 8 :</head><label>8</label><figDesc>Test scales vs. different AP metrics on WOD f0val. Pre-training AP AP 50 AP 75 AP S AP M AP L body face frame text ImageNet 68.9 92.2 73.3 19.9 42.6 75.8 64.3 47.6 93.0 70.7 COCO 1? 69.9 92.5 74.3 20.5 43.6 77.1 66.6 48.0 93.7 71.2 COCO 2? 69.8 92.3 74.0 20.5 43.4 77.0 66.5 47.8 93.8 71.2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Following [79], we consider two types of object scales. The absolute scale is calculated as ? wh, where w and h denote the object's width and height, respectively. The relative scale is calculated as</figDesc><table /><note>wh W H , where W and H denote the image's width and height, respectively.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Comparison of training protocols.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>USB training protocols. AHPO: Aggressive hyperparameter optimization.</figDesc><table><row><cell>Protocol</cell><cell cols="3">Max reso. Typical scale Reference</cell></row><row><cell cols="4">Standard USB 1,066,667 1333? 800 Popular in COCO [11, 26, 40]</cell></row><row><cell>Mini USB</cell><cell>262,144</cell><cell cols="2">512? 512 Popular in VOC [20, 44]</cell></row><row><cell>Micro USB</cell><cell>50,176</cell><cell cols="2">224? 224 Popular in ImageNet [28, 56]</cell></row><row><cell>Large USB</cell><cell cols="3">2,457,600 1920?1280 WOD front cameras [62]</cell></row><row><cell>Huge USB</cell><cell cols="3">7,526,400 3360?2240 WOD methods ([33], ours)</cell></row><row><cell>Freestyle</cell><cell>?</cell><cell>-</cell><cell>-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>USB evaluation protocols.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>. See Supp. D for the details of the methods and architectures used in UniverseNets.</figDesc><table><row><cell>Hyperparameters</cell><cell>COCO</cell><cell>WOD</cell><cell>M109s</cell><cell>Hyperparam. Common</cell></row><row><cell>Learning rate for multi-stage detectors Learning rate for single-stage detectors Test scale</cell><cell cols="3">0.02 0.01 1333?800 1248?832 1216?864 0.02 0.16 0.01 0.08</cell><cell>Epoch Batch size Momentum Weight decay 10 ?4 12 16 0.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 5 :</head><label>5</label><figDesc>Default hyperparameters. See Supp. E for exceptions. Correlation between mCAP and CAP on each dataset. Method mCAP AP 50 AP 75 APS APM APL COCO WOD M109s Faster R-CNN [54] 45.9 68.2 49.1 15.2 38.9 62.5 37.4 34.5 65.8 Cascade R-CNN [9] 48.1 68.5 51.5 15.6 41.3 65.9 40.3 36.4 67.6 Deform. DETR [84] 44.6 67.0 47.3 13.8 36.1 62.6 37.1 32.7 64.1 Sparse R-CNN [63] 44.6 65.4 46.9 14.4 35.8 63.0 37.9 32.8 63.1</figDesc><table><row><cell></cell><cell>40</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>36</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>67 68</cell><cell></cell><cell cols="3">Cascade R-CNN GFL</cell></row><row><cell>COCO CAP</cell><cell>37 38 39</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>WOD CAP</cell><cell>33 34 35</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>M109s CAP</cell><cell>64 65 66</cell><cell cols="4">Faster R-CNN RetinaNet ATSS Deformable DETR</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>63</cell><cell cols="3">Sparse R-CNN</cell><cell></cell></row><row><cell></cell><cell>44</cell><cell>45</cell><cell>46</cell><cell>47</cell><cell>48</cell><cell>49</cell><cell>44</cell><cell>45</cell><cell>46</cell><cell>47</cell><cell>48</cell><cell>49</cell><cell>44</cell><cell>45</cell><cell>46</cell><cell>47</cell><cell>48</cell><cell>49</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">mCAP</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">mCAP</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">mCAP</cell><cell></cell></row><row><cell cols="11">Figure 3: RetinaNet [42] 44.8 66.0 47.4 12.9 37.3 62.6 36.5 32.5 65.3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ATSS [81]</cell><cell cols="10">47.1 68.0 50.2 15.5 39.5 64.7 39.4 35.4 66.5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>GFL [38]</cell><cell cols="10">47.7 68.3 50.6 15.8 39.9 65.8 40.2 35.7 67.3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>DETR [10]</cell><cell cols="10">23.7 45.9 21.6 2.8 13.8 42.1 22.2 17.8 31.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 6 :</head><label>6</label><figDesc>Results of popular baseline methods.</figDesc><table /><note>Backbone mCAP AP 50 AP 75 APS APM APL COCO WOD M109s ResNet-50-B [30] 47.1 68.0 50.2 15.5 39.5 64.7 39.4 35.4 66.5 Swin-T [45] 49.0 70.6 52.0 17.2 41.8 67.2 43.7 37.2 66.2 ConvNeXt-T [46] 50.4 71.8 53.7 17.3 43.0 69.0 45.5 38.3 67.4</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 7 :</head><label>7</label><figDesc>FPN+DyHead [16] 49.4 69.8 52.9 16.8 43.0 67.8 43.3 37.1 67.9</figDesc><table /><note>ATSS [81] with different backbones.Neck mCAP AP 50 AP 75 APS APM APL COCO WOD M109s FPN [41] 47.1 68.0 50.2 15.5 39.5 64.7 39.4 35.4 66.5 FPN+SEPC [74] 48.1 68.5 51.2 15.5 40.5 66.8 42.1 35.0 67.1</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 8 :</head><label>8</label><figDesc>ATSS [81] with different necks. UniverseNet 51.4 72.1 55.1 18.4 45.0 70.7 46.7 38.6 68.9 UniverseNet-20.08 52.1 72.9 55.5 19.2 45.8 70.8 47.5 39.0 69.9</figDesc><table /><note>Method mCAP AP 50 AP 75 APS APM APL COCO WOD M109s YOLOX-L [23] 51.0 72.6 54.7 21.2 45.9 65.0 41.1 41.6 70.2</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 9 :</head><label>9</label><figDesc>Results of strong baseline methods.</figDesc><table><row><cell></cell><cell>70</cell><cell>Faster R-CNN</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>60</cell><cell>Cascade R-CNN</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Relative Scale AP</cell><cell>20 30 40 50</cell><cell cols="2">RetinaNet ATSS GFL DETR Deformable DETR Sparse R-CNN</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>10</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0</cell><cell>1/256 1/128</cell><cell>1/64</cell><cell>1/32</cell><cell>1/16</cell><cell>1/8</cell><cell>1/4</cell><cell>1/2</cell><cell>1/1</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Relative scale</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 10</head><label>10</label><figDesc></figDesc><table><row><cell>Asset</cell><cell>Version</cell><cell>URL</cell><cell>License</cell></row><row><cell>COCO [40]</cell><cell>2017</cell><cell>https://cocodataset.org/</cell><cell>Annotations: CC-BY 4.0; images: various licenses</cell></row><row><cell>WOD [62]</cell><cell>1.2</cell><cell>https://waymo.com/open/</cell><cell>Custom license</cell></row><row><cell cols="3">Manga109-s [3, 48] 2020.12.18 http://www.manga109.org/</cell><cell>Custom license</cell></row><row><cell>COCO API [39]</cell><cell>2.0</cell><cell>https://github.com/cocodataset/cocoapi</cell><cell>2-Clause BSD License</cell></row><row><cell>WOD (code)</cell><cell>-</cell><cell cols="2">https://github.com/waymo-research/waymo-open-dataset Apache License 2.0</cell></row><row><cell>Manga109 API</cell><cell>0.3.1</cell><cell>https://github.com/manga109/manga109api</cell><cell>MIT License</cell></row><row><cell cols="2">MMDetection [11] 2.25.0</cell><cell>https://github.com/open-mmlab/mmdetection</cell><cell>Apache License 2.0</cell></row></table><note>. See our codes for more details. Refer to the papers [3, 40, 62] and the URLs for how the datasets were collected.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 10 :</head><label>10</label><figDesc>Existing assets we used.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>Table 11 :</head><label>11</label><figDesc>Manga109-s dataset splits (87 volumes in total).</figDesc><table /><note>splits shown in</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_20"><head>Table 12 :</head><label>12</label><figDesc>Architectures of UniverseNets with a summary of ablation studies on COCO minival. See Sec. E.7 for step-by-step improvements.</figDesc><table><row><cell>All results are based on MMDe-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_21"><head></head><label></label><figDesc>, Deformable DETR [84], ATSS [81] with Swin-T [45], and ATSS [81] with ConvNeXt-T [46], and 2.5?10 ?5 for Sparse R-CNN [63]. The learning rate of the backbone is 10 ?5 for DETR [10] and Deformable DETR [84]. For training ATSS [81] with ConvNeXt-T [46], we used layer-wise learning rate decay and a stochastic depth rate of 0.2 (see</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_22"><head></head><label></label><figDesc>baseline model. For training ATSS [81] with different backbones (Swin-T [45] and ConvNeXt-T [46]), we used an initial learning rate of 4?10 ?4 , roughly tuned from choices {2?10 ?4 , 4?10 ?4 , 8?10 ?4 }.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">YOLOv4 was trained for 273 epochs<ref type="bibr" target="#b3">[5]</ref>, DETR for 500 epochs<ref type="bibr" target="#b8">[10]</ref>, EfficientDet-D6 for 300 epochs<ref type="bibr" target="#b68">[69]</ref>, and EfficientDet-D7x for 600 epochs<ref type="bibr" target="#b69">[70]</ref>. SpineNet uses a learning rate of 0.28<ref type="bibr" target="#b17">[19]</ref>, and YOLOv4 uses a searched learning rate of 0.00261<ref type="bibr" target="#b3">[5]</ref>. EfficientDet finely changes the image resolution from 512?512 to 1536?1536<ref type="bibr" target="#b68">[69]</ref>.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Aerial datasets contain abundant small objects but scarce large ones (seeTable 4in<ref type="bibr" target="#b15">[17]</ref>). WOD has larger scale variation by distance variation, where 1% of objects are larger than 1/4 of the image area.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Higher protocols can adapt the data transfer rate to lower protocols.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">https://github.com/shinya7y/WaymoCOCO 5 https://github.com/shinya7y/manga109api</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments. We are grateful to Dr. Hirokatsu Kataoka for helpful comments. We thank all contributors for the datasets and software libraries. The original image of <ref type="figure">Figure 1</ref> (left) is satellite office by Taiyo FUJII (CC BY 2.0).</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Waymo Open Dataset 2D detection leaderboard</title>
		<ptr target="https://waymo.com/open/challenges/2d-detection/" />
		<imprint>
			<date type="published" when="2020-06-18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Building a manga dataset &quot;Manga109&quot; with annotations for multimedia applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kiyoharu</forename><surname>Aizawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Azuma</forename><surname>Fujimoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Atsushi</forename><surname>Otsubo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toru</forename><surname>Ogawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusuke</forename><surname>Matsui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koki</forename><surname>Tsubota</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hikaru</forename><surname>Ikuta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE MultiMedia</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Shallow networks for high-accuracy road object-detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khalid</forename><surname>Ashraf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bichen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Forrest</forename><forename type="middle">N</forename><surname>Iandola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mattthew</forename><forename type="middle">W</forename><surname>Moskewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Keutzer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.01561</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">YOLOv4: Optimal speed and accuracy of object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Bochkovskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chien-Yao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong-Yuan Mark</forename><surname>Liao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.10934</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Soft-NMSimproving object detection with one line of code</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navaneeth</forename><surname>Bodla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharat</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rama</forename><surname>Chellappa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">TIDE: A general toolbox for identifying object detection errors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Bolya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Foley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Towards Universal Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaowei</forename><surname>Cai</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
		<respStmt>
			<orgName>UC San Diego</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Cascade R-CNN: Delving into high quality object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaowei</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nuno</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">End-to-end object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangmiao</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxiao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuyang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wansen</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiarui</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dazhi</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenchen</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianheng</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qijie</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Buyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.07155</idno>
		<title level="m">MMDetection: Open mmlab detection toolbox and benchmark</title>
		<editor>Jifeng Dai, Jingdong Wang, Jianping Shi, Wanli Ouyang, Chen Change Loy, and Dahua Lin</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">2nd place solution for Waymo Open Dataset challenge -2D object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sijia</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Runzhou</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihan</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuangzhuang</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Liao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.15507</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Rep-Points v2: Verification meets regression for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">RelationNet++: Bridging visual representations for object detection via transformer decoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangyun</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Deformable convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haozhi</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuwen</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Dynamic head: Unifying object detection heads with attentions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiyang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinpeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongdong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengchen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Object detection in aerial images: A large-scale benchmark and challenges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gui-Song</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">Ying</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Datcu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcello</forename><surname>Pelillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangpei</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Pedestrian detection: An evaluation of the state of the art</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Wojek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">SpineNet: Learning scale-permuted backbone for recognition and localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianzhi</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengchong</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Golnaz</forename><surname>Ghiasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Mark Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Ali Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">I</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
		<title level="m">The PASCAL Visual Object Classes challenge: A retrospective. IJCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Yang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ananth</forename><surname>Ranga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ambrish</forename><surname>Tyagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.06659</idno>
		<title level="m">DSSD : Deconvolutional single shot detector</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shang-Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin-Yu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Torr</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note>Res2Net: A new multi-scale backbone architecture. TPAMI</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songtao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.08430</idno>
		<title level="m">YOLOX: Exceeding YOLO series in 2021</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Are we ready for autonomous driving? the KITTI vision benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Simple copy-paste is a strong data augmentation method for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Golnaz</forename><surname>Ghiasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aravind</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ekin</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Piotr Doll?r, and Kaiming He</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilija</forename><surname>Radosavovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<ptr target="https://github.com/facebookresearch/detectron" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">LVIS: A dataset for large vocabulary instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agrim</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Rethinking ImageNet pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Bag of tricks for image classification with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Tong He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongyue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mu</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Diagnosing error in object detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yodsawalai</forename><surname>Chodpathumwan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qieyun</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Yang Song, Sergio Guadarrama, and Kevin Murphy. Speed/accuracy trade-offs for modern convolutional object detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vivek</forename><surname>Rathod</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anoop</forename><surname>Korattikara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alireza</forename><surname>Fathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zbigniew</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zehao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zehui</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiaofei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongkai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naiyan</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.01365</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>1st place solutions of Waymo Open Dataset challenge 2020 -2D object detection track</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Crossdomain weakly-supervised object detection through progressive domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naoto</forename><surname>Inoue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryosuke</forename><surname>Furuta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toshihiko</forename><surname>Yamasaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kiyoharu</forename><surname>Aizawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Driving in the matrix: Can virtual worlds replace human-generated annotations for real world tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Johnson-Roberson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Barto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rounak</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nittur</forename><surname>Sharath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Sridhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ram</forename><surname>Rosaen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vasudevan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICRA</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Probabilistic anchor assignment with IoU prediction for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hee Seok</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">The Open Images Dataset V4: Unified image classification, object detection, and visual relationship detection at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alina</forename><surname>Kuznetsova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hassan</forename><surname>Rom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Alldrin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jasper</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Krasin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordi</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shahab</forename><surname>Kamali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Popov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Malloci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Duerig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vittorio</forename><surname>Ferrari</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>IJCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Generalized Focal Loss: Learning qualified and distributed bounding boxes for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolin</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinhui</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Yang</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<ptr target="https://github.com/cocodataset/cocoapi" />
		<imprint>
			<date type="published" when="2020-11-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Microsoft COCO: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Kaiming He, Bharath Hariharan, and Serge Belongie. Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Kaiming He, and Piotr Doll?r. Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Deep learning for generic object detection: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Fieguth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinwang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matti</forename><surname>Pietik?inen</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">SSD: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Yang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Swin Transformer: Hierarchical vision transformer using shifted windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baining</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">A convnet for the 2020s</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanzi</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao-Yuan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusuke</forename><surname>Matsui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kota</forename><surname>Ito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuji</forename><surname>Aramaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Azuma</forename><surname>Fujimoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toru</forename><surname>Ogawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toshihiko</forename><surname>Yamasaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kiyoharu</forename><surname>Aizawa</surname></persName>
		</author>
		<title level="m">Sketch-based manga retrieval using Manga109 dataset. Multimedia Tools and Applications</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">A metric learning reality check</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Musgrave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ser-Nam</forename><surname>Lim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Object detection for comics using Manga109 annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toru</forename><surname>Ogawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Atsushi</forename><surname>Otsubo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rei</forename><surname>Narita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusuke</forename><surname>Matsui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toshihiko</forename><surname>Yamasaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kiyoharu</forename><surname>Aizawa</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.08670</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Unsupervised domain adaptation of object detectors: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Poojan</forename><surname>Oza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vishwanath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sindagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">S</forename><surname>Vibashan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vishal</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.13502</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">MegDet: A large mini-batch object detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tete</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuning</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">DetectoRS: Detecting objects with recursive feature pyramid and switchable atrous convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyuan</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.02334</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Neural network-based face detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Henry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shumeet</forename><surname>Rowley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeo</forename><surname>Baluja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kanade</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">S</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">C</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fei-Fei</surname></persName>
		</author>
		<title level="m">ImageNet Large Scale Visual Recognition Challenge. IJCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roy</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesse</forename><surname>Dodge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><forename type="middle">Etzioni</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">I</forename></persName>
		</author>
		<title level="m">Communications of the ACM</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Objects365: A large-scale, high-quality dataset for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Understanding the effects of pretraining for object detectors via eigenspectrum</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yosuke</forename><surname>Shinya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edgar</forename><surname>Simo-Serra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taiji</forename><surname>Suzuki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV Workshop on Neural Architects</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">An analysis of scale invariance in object detection -SNIP</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharat</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Revisiting the sibling head in object detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanglu</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Jonathon Shlens, Zhifeng Chen, and Dragomir Anguelov. Scalability in perception for autonomous driving: Waymo Open Dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henrik</forename><surname>Kretzschmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xerxes</forename><surname>Dotiwalla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aurelien</forename><surname>Chouard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijaysai</forename><surname>Patnaik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Tsui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuning</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Caine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiquan</forename><surname>Ngiam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksei</forename><surname>Timofeev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Ettinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Krivokon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amy</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Sparse R-CNN: Endto-end object detection with learnable proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peize</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rufeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenfeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masayoshi</forename><surname>Tomizuka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zehuan</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changhu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zbigniew</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Data augmentation using random image cropping and patching for deep CNNs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryo</forename><surname>Takahashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takashi</forename><surname>Matsubara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuniaki</forename><surname>Uehara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TCSVT</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">A comment about the training log of EfficientDet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<ptr target="https://github.com/google/automl/issues/380#issuecomment-800814557" />
		<imprint>
			<date type="published" when="2021-08-23" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Efficientnet</surname></persName>
		</author>
		<title level="m">Rethinking model scaling for convolutional neural networks. In ICML</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">EfficientDet: Scalable and efficient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruoming</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruoming</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Efficientdet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.09070v7</idno>
		<title level="m">Scalable and efficient object detection</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">FCOS: Fully convolutional onestage object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Between-class learning for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuji</forename><surname>Tokozume</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshitaka</forename><surname>Ushiku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tatsuya</forename><surname>Harada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">REVISE: A tool for measuring and mitigating bias in visual datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angelina</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvind</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Scaleequalizing pyramid convolution for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinjiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shilong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuoran</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Litong</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wayne</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Towards universal object detection by domain attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xudong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaowei</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dashan</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nuno</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<monogr>
				<title level="m">E.4 Details on Each Dataset Tables 13, 14, and 15 show the results on COCO, WOD, and M109s, respectively. Method AP AP 50 AP 75 AP S AP M AP L</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<monogr>
				<title level="m">Table 13: Results on COCO minival</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R-Cnn</forename><surname>Faster</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">54</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<monogr>
				<title level="m">Table 14: Results on WOD f0val</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<monogr>
				<title level="m">Method AP AP 50 AP 75 AP S AP M AP L body face frame text</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<monogr>
		<title level="m" type="main">Protocol Method Backbone DCN Epoch Max test</title>
		<idno>scale TTA FPS AP AP50 AP75 APS APM APL</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<monogr>
		<title level="m" type="main">State-of-the-art methods on COCO test-dev. We classify methods by the proposed protocols without compatibility</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
		</imprint>
	</monogr>
	<note>X in the Backbone column denotes ResNeXt [76</note>
</biblStruct>

<biblStruct xml:id="b83">
	<monogr>
		<title level="m" type="main">TTA: Test-time augmentation including horizontal flip and multi-scale testing (numbers denote scales). FPS values without and with parentheses were measured on V100 with mixed precision and other environments, respectively. We measured the FPS of GFL [38] models in our environment and estimated those of ATSS</title>
		<imprint/>
	</monogr>
	<note>Other methods&apos; settings are based on conference papers, their arXiv versions, and authors&apos; codes. Values shown in gray were estimated from descriptions in papers and codes. Some FPS values are from [38</note>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Although state-of-the-art detectors on the COCO benchmark were trained with various settings, the introduced divisions enable us to compare methods in each division. UniverseNet-20.08d achieves the highest AP (51.3%) in the Standard USB 1.0 protocol. Despite 12.5? fewer epochs, the speed-accuracy trade-offs of UniverseNets are comparable to those of Efficient-Det [69</title>
		<idno>AP AP 50 AP 75 AP S APM APL</idno>
	</analytic>
	<monogr>
		<title level="m">E.5 Rethinking COCO with USB Protocols We classify state-of-the-art methods on COCO test-dev (as of November 14, 2020) by the proposed protocols without compatibility. The results are shown in Table 16</title>
		<imprint/>
	</monogr>
	<note>] shows a better speed-accuracy trade-off than EfficientDet [69] in Standard USB 3.x. Comparisons across different divisions are difficult. Especially, long training is problematic because it can secretly increase AP without decreasing FPS. unlike large test scales. Nevertheless, the EfficientDet [69], YOLOv4 [5], and SpineNet [19] papers compare meth-Method</note>
</biblStruct>

<biblStruct xml:id="b85">
	<monogr>
				<idno>training. Method AP AP 50 AP 75 AP S APM APL</idno>
		<title level="m">AP improvements by</title>
		<imprint/>
	</monogr>
	<note>Res2Net-v1b [22], DCN [15</note>
</biblStruct>

<biblStruct xml:id="b86">
	<monogr>
				<title level="m">AP improvements by GFL</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<monogr>
		<title level="m" type="main">Replacing Res2Net-v1b backbone with ResNet-B [30] has the largest effects</title>
		<idno>FPS AP AP 50 AP 75 AP S APM APL</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<monogr>
				<idno>UniverseNet-20.08</idno>
		<title level="m">with different backbones. Table 18: Ablation studies on COCO minival</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<monogr>
				<idno>ATSS: 39.4%</idno>
		<title level="m">It is much more accurate than other models trained for 12 epochs using ResNet-50-level backbones</title>
		<imprint/>
	</monogr>
	<note>11, 81], GFL: 40.2% [11, 38]). On the other hand, the inference is not so fast (less than 20 FPS) due to the heavy use of DCN [15</note>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">Ablating the Res2Net-v1b backbone (replacing Res2Net-50-v1b [22] with ResNet-50-B [30]) has the largest effects. Res2Net-v1b improves AP by 2.8% and increases the inference time by 1.3?. To further investigate the effectiveness of backbones, we trained variants of UniverseNet-20.08 as shown in Table 18g. Although the Res2Net module [22] makes inference slower, the deep stem used in ResNet-50-C [30] and Res2Net-50-v1b [22] improves AP metrics with similar speeds</title>
	</analytic>
	<monogr>
		<title level="m">As shown in Table 18e, UniverseNet-20.08 is 1.4? faster than UniverseNet-20.08d at the cost of a ?1% AP drop</title>
		<imprint/>
	</monogr>
	<note>To further verify the effectiveness of each technique, we conducted ablation from UniverseNet-20.08 shown in Table 18f. UniverseNet-20.08s (the variant using the ResNet-50-C backbone) shows a good speed-accuracy trade-off by achieving 45.8% AP and over 30 FPS</note>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">Single-stage detectors require larger test scales than multi-stage detectors to achieve peak performance, probably because they cannot extract features from precisely localized region proposals. Although ATSEPC shows lower AP than ATSS at the default test scale (1248?832 in Standard USB), it outperforms ATSS at larger test scales (e.g., 1920?1280 in Large USB)</title>
	</analytic>
	<monogr>
		<title level="m">Effects of Test Scales We show the results on WOD at different test scales in Figure 8a</title>
		<imprint/>
	</monogr>
	<note>We conjecture that we should enlarge object scales in images to utilize SEPC. 74] because its DCN [15] enlarges effective receptive fields. SEPC and DCN prefer large objects empirically (Tables 18a, 18f, [15, 74]), and DCN [15] cannot increase the sampling points for objects smaller than the kernel size in principle. By utilizing the characteristics of SEPC and multi-scale training, UniverseNets achieve the highest AP in a wide range of test scales</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
