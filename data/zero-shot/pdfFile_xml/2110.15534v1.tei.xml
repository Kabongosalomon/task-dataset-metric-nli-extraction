<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Structure-aware Fine-tuning of Sequence-to-sequence Transformers for Transition-based AMR Parsing</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Zhou</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Harvard University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tahira</forename><surname>Naseem</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">IBM Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ram?n</forename><surname>Fernandez</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">IBM Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Astudillo</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Young-Suk</forename><surname>Lee</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">IBM Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Florian</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">IBM Research</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
							<email>roukos@us.ibm.com</email>
							<affiliation key="aff1">
								<orgName type="institution">IBM Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Structure-aware Fine-tuning of Sequence-to-sequence Transformers for Transition-based AMR Parsing</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T15:19+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Predicting linearized Abstract Meaning Representation (AMR) graphs using pre-trained sequence-to-sequence Transformer models has recently led to large improvements on AMR parsing benchmarks. These parsers are simple and avoid explicit modeling of structure but lack desirable properties such as graph well-formedness guarantees or built-in graph-sentence alignments. In this work we explore the integration of general pre-trained sequence-to-sequence language models and a structure-aware transition-based approach. We depart from a pointer-based transition system and propose a simplified transition set, designed to better exploit pre-trained language models for structured fine-tuning. We also explore modeling the parser state within the pre-trained encoder-decoder architecture and different vocabulary strategies for the same purpose. We provide a detailed comparison with recent progress in AMR parsing and show that the proposed parser retains the desirable properties of previous transition-based approaches, while being simpler and reaching the new parsing state of the art for AMR 2.0, without the need for graph re-categorization.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The task of Abstract Meaning Representation (AMR) parsing translates a natural sentence into a rooted directed acyclic graph capturing the semantics of the sentence, with nodes representing concepts and edges representing their relations <ref type="bibr" target="#b4">(Banarescu et al., 2013)</ref>. Recent works utilizing pretrained encoder-decoder language models show great improvements in AMR parsing results <ref type="bibr" target="#b36">(Xu et al., 2020;</ref><ref type="bibr" target="#b5">Bevilacqua et al., 2021)</ref>. These approaches avoid explicit modeling of the graph structure. Instead, they directly predict the linearized AMR graph treated as free text. While the use of pre-trained Transformer encoders is widely extended in AMR parsing, the use of pre-trained Transformer decoders is recent and has shown to be very effective, maintaining current state-of-the-art results <ref type="bibr" target="#b5">(Bevilacqua et al., 2021)</ref>.</p><p>These approaches however lack certain desirable properties. There are no structural guarantees of graph well-formedness, i.e. the model may predict strings that can not be decoded into valid graphs, and post-processing is required. Furthermore, predicting AMR linearizations ignores the implicit alignments between graph nodes and words, which provide a strong inductive bias and are useful for downstream AMR applications <ref type="bibr" target="#b20">(Mitra and Baral, 2016;</ref><ref type="bibr" target="#b16">Liu et al., 2018;</ref><ref type="bibr" target="#b33">Vlachos et al., 2018;</ref>.</p><p>On the other hand, transition-based AMR parsers <ref type="bibr" target="#b35">(Wang et al., 2015;</ref><ref type="bibr" target="#b2">Ballesteros and Al-Onaizan, 2017a;</ref><ref type="bibr" target="#b13">Astudillo et al., 2020;</ref><ref type="bibr" target="#b40">Zhou et al., 2021)</ref> operate over the tokens of the input sentence, generating the graph incrementally. They implicitly model graph structural constraints through transitions and yield alignments by construction, thus guaranteeing graph well-formedness. 1 However, it remains unclear whether explicit modeling of structure is still beneficial for AMR parsing in the presence of powerful pre-trained language models and their strong free text generation abilities.</p><p>In this work, we integrate pre-trained sequenceto-sequence (seq-to-seq) language models with the transition-based approach for AMR parsing, and explore to what degree they are complementary. To fully utilize the generation power of the pre-trained language models, we propose a transition system with a small set of basic actions -a generalization of the action-pointer transition system of <ref type="bibr" target="#b40">Zhou et al. (2021)</ref>. We use BART <ref type="bibr" target="#b14">(Lewis et al., 2019)</ref> as our pre-trained language model, since it has shown significant improvements in linearized AMR generation <ref type="bibr" target="#b5">(Bevilacqua et al., 2021)</ref>. Unlike previous approaches that directly fine-tune the model with linearized graphs, we modify the model structure to work with our transition system, and encode parser states in BART's attention mechanism <ref type="bibr" target="#b13">(Astudillo et al., 2020;</ref><ref type="bibr" target="#b40">Zhou et al., 2021)</ref>. We also explore different vocabulary strategies for action generation. These changes convert the pre-trained BART to a transition-based parser where graph constraints and alignments are internalized.</p><p>We provide a detailed comparison with topperforming AMR parsers and perform ablation experiments showing that our proposed transition system and BART modifications are both necessary to achieve strong performance. Although BART has great language generation capacity, it still benefits from parser state encoding with hard attention, and can efficiently learn structural output. Our model establishes a new state of the art for AMR 2.0 while maintaining graph well-formedness guarantees and producing built-in alignments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Intricacies of AMR Parsers</head><p>A frequent complaint about AMR parsers is that they involve combining many different techniques and hand-crafted rules, resulting in complex pipelines that are hard to analyze and generalize poorly. This situation has notably improved in the past few years but there are still two main sources of complexity present in almost all recent parsers: graph re-categorization and subgraph actions.</p><p>Graph re-categorization <ref type="bibr" target="#b34">(Wang and Xue, 2017;</ref><ref type="bibr" target="#b19">Lyu and Titov, 2018;</ref><ref type="bibr" target="#b37">Zhang et al., 2019a)</ref> normalizes the graph prior to learning. This includes joining certain subgraphs such as entities, dates and other constructs into single nodes, removing special types of nodes like polarity and normalizing propbank names. An example of common normalizations is displayed in <ref type="figure" target="#fig_0">Figure 1</ref>. Training and decoding of models using this technique happens in this re-categorized space. Re-categorized graphs are expanded to normal valid AMR graphs in a post-processing stage. The type and number of subgraphs normalized vary across implementations, but most high performing approaches <ref type="bibr" target="#b7">(Cai and Lam, 2020;</ref><ref type="bibr" target="#b5">Bevilacqua et al., 2021)</ref> utilize the re-categorization described in Appendix A.1 of <ref type="bibr" target="#b37">Zhang et al. (2019a)</ref>. This version requires of an external Named Entity Recognition (NER) system to anonymize named entities, both at train time and test time. It also makes use of look-up tables for nominalizations (e.g. English to England) and other hand-crafted rules. Graph re-categorization has been criticised for its lack of generalization to new domains, such as biomedical domain or even the AMR 3.0 corpus <ref type="bibr" target="#b5">(Bevilacqua et al., 2021)</ref>. Recent top performing systems e.g. <ref type="bibr" target="#b7">Cai and Lam (2020)</ref>; <ref type="bibr" target="#b5">Bevilacqua et al. (2021)</ref> also provide results without re-categorization, but this is shown to hurt performance notably on the AMR 2.0 corpus.</p><p>Subgraph actions <ref type="bibr" target="#b3">(Ballesteros and Al-Onaizan, 2017b</ref>) are used in transition-based systems and play a role similar to re-categorization. Instead of normalizing and reverting, transition-based parsers apply a subgraph action that generates an entire subgraph at once. This subgraph action coincides with many of the subgraphs collapsed in re-categorization. Subgraph actions bring however fewer external dependencies, since the parser learns to segment and identify subgraphs during training. They still suffer however from data sparsity since some subgraphs appear very few times. As in re-categorization, subgraph actions also make use of lookup tables for nominalization and similar constructs that hinder generalization. Furthermore, they create the problem of unattachable nodes. This was addressed in <ref type="bibr" target="#b40">Zhou et al. (2021)</ref> by ignoring subgraphs for a set of heuristically determined cases. Subgraph actions have been used in all transitionbased AMR systems <ref type="bibr" target="#b22">(Naseem et al., 2019;</ref><ref type="bibr" target="#b13">Astudillo et al., 2020;</ref><ref type="bibr" target="#b40">Zhou et al., 2021)</ref>.</p><p>Aside from NER, past AMR parsers have other external dependencies such as POS taggers <ref type="bibr" target="#b37">(Zhang et al., 2019a;</ref><ref type="bibr" target="#b7">Cai and Lam, 2020)</ref> and lemmatizers <ref type="bibr" target="#b7">(Cai and Lam, 2020;</ref><ref type="bibr" target="#b22">Naseem et al., 2019;</ref><ref type="bibr" target="#b13">Astudillo et al., 2020)</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">A Simplified Transition System</head><p>In this section we propose a transition system for AMR parsing designed with two objectives: maximize the use of strong pre-trained decoders such as BART, and minimize the complexity and dependencies of the transition system compared to previous approaches. Similarly to <ref type="bibr" target="#b40">Zhou et al. (2021)</ref>, we scan the sentence from left to right and use a token cursor to point to a source token at each step. Parser actions can either shift the cursor one token forward or generate any number of nodes and edges while the cursor points to the same token. The proposed set of actions is as follows:</p><p>SHIFT moves token cursor one word to the right.</p><p>&lt;string&gt; creates node of name &lt;string&gt;.</p><p>COPY creates node where the node name is the token under the current cursor position.</p><p>LA(j,LBL) creates an arc with label LBL from the last generated node to the node generated at the j th transition step.</p><p>RA(j,LBL) same as LA but with arc direction reversed.</p><p>ROOT declares the last predicted node as the root.</p><p>Unlike previous transition-based approaches, we do not use a reserved action, such as PRED <ref type="bibr" target="#b40">(Zhou et al., 2021)</ref> or CONFIRM (Ballesteros and Al-Onaizan, 2017b), to predict nodes; 2 instead we directly use the node name &lt;string&gt; as the action symbol generating that node. This opens the possibility of utilizing BART's target side pre-trained vocabulary. We avoid using any copy actions that involve copying from lemmatizer outputs or lookup tables. Our COPY action is limited to copying the lower cased word. We also eliminate the use of SUBGRAPH <ref type="bibr" target="#b40">(Zhou et al., 2021)</ref> or ENTITY (Ballesteros and Al-Onaizan, 2017b) actions producing multiple connected nodes simultaneously, as well as MERGE action creating spans of words. In previous approaches these actions were derived from alignments or hand-crafted. They thus did not cover all possible cases limiting the scalability of the approach. Finally, we discard the REDUCE action previously used to delete a source token. The effect can be achieved by simply using SHIFT without performing any other action. <ref type="figure" target="#fig_1">Figure 2</ref> shows an example sentence with an action sequence and the corresponding graph. This can be compared with the handling of verbalization and named entities in <ref type="figure" target="#fig_0">Figure 1</ref>.</p><p>To train a parser with a transition system, we need an action sequence for each training sentence that will produce the gold graph when executed. This action sequence then serves as the target for seq-to-seq models. A simple rule-based oracle algorithm creates these ground-truth sequences given a sentence, its AMR graph and node-to-word alignments. At each step, the oracle tries the following possibilities in the order of listing and performs the first one that applies:</p><p>1. Create next gold arc between last created node and previously created nodes;</p><p>2. Create next gold node aligned to token under cursor;</p><p>3. If not at sentence end, SHIFT source cursor;</p><p>4. Finish oracle.</p><p>If possible, nodes are generated by COPY and otherwise with &lt;string&gt; actions. Arcs are generated with LA and RA, connecting the nodes closer to the current node before the ones that are farther away. Note that the arcs are created by pointing to positions in the action history, where a graph node is represented by the action that creates it, following <ref type="bibr" target="#b40">Zhou et al. (2021)</ref>. Multiple nodes can be generated at a single source word before the cursor is moved by SHIFT. When multiple nodes are aligned to the same token, these nodes are generated in a predetermined topological order of graph traversal, interleaved by edge creation actions. ROOT is applied as soon as the root node is generated.</p><p>The above oracle circumvents the problem of unattachable nodes by avoiding the use of subgraph actions. This implies that the oracle will always produce a unique action sequence that fully recovers the gold graph as long as every node in the graph is aligned to some token. To guarantee that all nodes are aligned, we improve upon the alignment system from <ref type="bibr" target="#b22">Naseem et al. (2019)</ref>; Astudillo et al. <ref type="formula">(2020)</ref>, which aligns a large majority, but not all AMR nodes. 3 In order to do this, we apply a heuristic based on graph proximity to maintain local correspondences between graph nodes and sentence words. If a node is not aligned, we copy the alignment from its first child node, if existing, and otherwise the alignment from its first parent node. For example, in <ref type="figure" target="#fig_1">Figure 2</ref> if the node person was not provided with an alignment, our oracle would have assigned it to the aligned token of its child node employ-01. This is a recursive procedure -as long as there are some alignments to start with and the ground-truth graph is connected, all the nodes will get aligned.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Parsing Model</head><p>We build our model on top of the pre-trained seqto-seq Transformer, BART <ref type="bibr" target="#b14">(Lewis et al., 2019)</ref>. We modify its architecture to incorporate a pointer network and internalize parser states induced by our transition system, and fine-tune for sentenceto-action generation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Structure-aware Architecture</head><p>We adopt similar modifications on the Transformer architecture as in <ref type="bibr" target="#b40">Zhou et al. (2021)</ref> since our transition system is based on the same action-pointer mechanism. The modifications do not introduce new modules or extra parameters, which naturally fit our need to adapt BART into a transition-based parser with internal graph well-formedness.</p><p>In particular, the target actions are factored into two parts: bare action symbols (containing labels when presented) and pointer values for edges. We use the BART standard output for the former, and a pointer network for the latter. As the pointing happens on the actions history, essentially a selfattention mechanism, we re-purpose one decoder self-attention head as the pointer network. It is supervised with additional cross entropy loss during fine-tuning and decoded for building graph edges at inference.</p><p>We encode the monotonic action-source alignments induced by the parser state with hard attention, i.e. by masking some decoder cross-attention heads to only focus on aligned words. Since BART processes source sentences with subwords, we apply an additional average pooling layer on top of its encoder to return states of original source words, used for the decoder layers for our hard attention. At last, as the possible valid actions are constrained with transition rules and states at every step, we restrict the decoder output space via hard masking of the BART final softmax layer. For simplicity, we do not incorporate the <ref type="bibr">GNN-style (Li et al., 2019)</ref> step-wise decoder graph embedding technique in <ref type="bibr" target="#b40">Zhou et al. (2021)</ref> as their gain was shown to be modest.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Action Generation</head><p>According to how we treat the target-side vocabulary for action generation, we propose two variations of the model. The first one is to use a completely separate vocabulary for target actions, where the decoder input side and output side use stand-alone embeddings for actions, separate from the pre-trained BART subword embeddings. <ref type="bibr">4</ref> We denote this setup as our sep-voc model (abbreviated as StructBART-S).</p><p>However, this might not fully utilize the power of the pre-trained BART since it is an encoder-decoder model with a single vocabulary and all embeddings shared. Although our generation targets are action symbols, the node generating actions are closely related to natural words in their surface forms, which are what BART was pre-trained on. Therefore, we propose a second variation where we use a joint vocabulary for both the source tokens and target actions. Naively relying on the original BART subword vocabulary would end up splitting action symbols blindly, which is not desired as the structures such as alignments and edge pointers would be disrupted. Inspired by <ref type="bibr" target="#b5">Bevilacqua et al. (2021)</ref>, we add frequent node-creating actions to the vocabulary, in order to capture common AMR concepts intact, and split the remaining concepts with BART subword vocabulary. Non-node-creating actions such as SHIFT and COPY are added as-is to the BART vocabulary.</p><p>In this setup, a single node string can potentially be generated with multiple steps; we modify the arc transitions to always point to the beginning position of a node string for attachment. With joint vocabulary setup, the model could learn to generate unseen nodes with BART's subword vocabulary, eliminating potential out-of-vocabulary problems. We refer to this setup as our joint-voc model (abbreviated as StructBART-J).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Training and Inference</head><p>We load the pre-trained BART parameters except for the standalone vocabulary embeddings for sepvoc model and the extended embeddings for the joint-voc model. We then fine-tune the model with the updated structure-aware architectures on sentence-action pairs with addition of pointer loss.</p><p>For decoding, we use similar constrained beam search algorithm as in <ref type="bibr" target="#b40">Zhou et al. (2021)</ref>, but with our own transition set and rules. We run a state machine on the side to get parser states used by the model. Note that for our joint-voc model, we only allow subword split for node (&lt;string&gt;) actions. As our fine-tuned model is already structure-aware, the graph well-formedness is always guaranteed and no post-processing is needed to return valid graphs, unlike <ref type="bibr" target="#b36">Xu et al. (2020)</ref>; <ref type="bibr" target="#b5">Bevilacqua et al. (2021)</ref>. The only post-processing we use is to add wikification nodes as used in all previous parsers. 5</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experimental Setup</head><p>Datasets We evaluate our models on 3 AMR benchmark datasets, namely AMR 1.0 (LDC2014T12), AMR 2.0 (LDC2017T10), and AMR 3.0 (LDC2020T02). They have around 10K, 37K, and 56K sentence-AMR pairs for training, respectively. 6 Both AMR 2.0 and AMR 3.0 have wikification nodes but AMR 1.0 does not.</p><p>Evaluation We assess our models with SMATCH (F1) scores 7 . We also report the fine-grained evaluation metrics <ref type="bibr" target="#b10">(Damonte et al., 2016)</ref> to further investigate different aspects of parsing results, such as concept identification, entity recognition, re-entrancies, etc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Configuration</head><p>We follow the original BART configuration <ref type="bibr" target="#b14">(Lewis et al., 2019)</ref> and code. <ref type="bibr">8</ref> We use the large model configuration as default, and also the base model for ablation studies. The pointer network is always tied with one head of the decoder top layer, and the pointer loss is added to the model cross-entropy loss with 1:1 ratio for training. Transition alignments are used to mask cross-attention in 2 heads of all decoder layers.</p><p>For sep-voc model, we build separate embedding matrices for target actions from the training data for decoder input and output space. For joint-voc model, we add new embedding vectors for nonnode action symbols and node action strings with a default minimum frequency of 5 (only accounts for about one third of all nodes due to sparsity). <ref type="bibr">5</ref> We also do light cleaning of the decoded AMR when they are printed to penman format, such as removing reserved characters in node concepts and printing disconnected subgraphs. <ref type="bibr">6</ref>   Implementation Details Our models are trained with Adam optimizer with batch size 2048 tokens and gradient accumulation of 4 steps. Learning rate is 1e?4 with 4000 warm-up steps using the inversesqrt scheduling scheme <ref type="bibr" target="#b31">(Vaswani et al., 2017)</ref>. The hyper-parameters are fixed and not tuned for different models and datasets, as we found results are not sensitive within small ranges. We train sep-voc models for 100 epochs and joint-voc models for 40 epochs as the latter is found to converge faster. The best 5 checkpoints based on development set SMATCH from greedy decoding are averaged, and default beam size of 10 is used for decoding for our final parsing scores. We implement our model 9 with the FAIRSEQ toolkit <ref type="bibr" target="#b23">(Ott et al., 2019)</ref>. More details can be found in the Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Results</head><p>Main Results We present parsing performances of our model (StructBART) in comparison with previous approaches in <ref type="table">Table 1</ref>. For each model, we also list its features such as utilization of pre-   trained language models and graph simplification methods such as re-categorization. This gives a comprehensive overview of how systems compare in terms of complexity aside from performance. All recent systems rely on pre-trained language models, either as fixed features or through finetuning. The pre-trained BART is particularly beneficial due to its encoder-decoder structure. Among all the models, the graph linearization models <ref type="bibr" target="#b36">(Xu et al., 2020;</ref><ref type="bibr" target="#b5">Bevilacqua et al., 2021)</ref> have the least number of extra dependencies when not using graph re-categorization. Our model only requires aligned training data, a trait common to all transition-based approaches. This bears the advantage of producing reliable alignments at decoding time, which are useful for downstream tasks and as explanation of the graph constructing process.</p><p>Both our sep-voc and joint-voc model variations work well on all datasets. Without using extra silver data, our model achieves the SMATCH score of 84.2 ?0.1 on AMR 2.0, which is the same as the previous best model <ref type="bibr" target="#b5">(Bevilacqua et al., 2021)</ref> with 200K silver data. With the input of only 47K silver data (consisting of ?20K example sentences of propbank frames and randomly selected ?27K SQuAD-2.0 context sentences 10 ), we achieve the highest score of 84.7 ?0.1 for AMR 2.0. We also attain the high score of 81.7 ?0.2 on the smallest AMR 1.0 benchmark, and the second best score of 82.7 ?0.1 on the largest AMR 3.0 benchmark. Ensemble of the 3 models from the silver training further improves the performances to 84.9 for AMR 10 https://rajpurkar.github.io/SQuAD-explorer/.</p><p>2.0 and 83.1 for AMR 3.0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fine-grained Results</head><p>We further examine the fine-grained parsing results on AMR 2.0 in Table 2. We compare models not relying on extra data nor graph re-categorizationn since silver data sets differ across methods, and re-categorization comes with limitations outlined in Section 2. Our models achieve the highest scores across most of the categories, except for negation and wikification. The former may be due to alignment errors and the latter is solved as a separate post-processing step independent of the parser. Compared with the closely related model from <ref type="bibr" target="#b5">Bevilacqua et al. (2021)</ref> which also fine-tunes BART but directly on linearized graphs, we achieve significant gains on re-entrancy and SRL (:ARG-I arcs), proving our model generate AMR graphs more faithful to their topological structures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Analysis</head><p>Transition System <ref type="table" target="#tab_3">Table 3 and Table 4</ref> compare different transition systems used by recent transition-based AMR parsers with strong performances. Our proposed system has the smallest set of base actions, utilizes the action-side pointer mechanism for flexible edge creation as in <ref type="bibr" target="#b40">Zhou et al. (2021)</ref>, but does not rely on special treatment of certain subgraphs such as named entities and dates. This results in slightly longer action sequences compared to <ref type="bibr" target="#b40">Zhou et al. (2021)</ref>, but with almost 100% coverage 11 <ref type="table" target="#tab_4">(Table 4</ref>). Our transition system and oracle can always find action sequences with full recovery of the original AMR graph, regardless of graph topology and alignments.</p><p>To assess whether our proposed transition system helps integration with pre-trained BART, we train both the APT model from <ref type="bibr" target="#b40">Zhou et al. (2021)</ref> and our sep-voc model on the transition system of   <ref type="bibr" target="#b40">Zhou et al. (2021)</ref> and the one introduced in this work <ref type="table" target="#tab_3">(Table 3</ref> last 4 columns). The APT model, based on fixed RoBERTa features, does not benefit from the proposed transition system. However, our proposed model gains 0.6 on AMR 2.0 and 0.7 on AMR 3.0. This confirms the hypothesis that the proposed transitions are better able to exploit BART's powerful language generation ability.</p><p>Structural Alignment Modeling In <ref type="table" target="#tab_6">Table 5</ref>, we evaluate the effects of our structural modeling of parser states within BART during fine-tuning. Action-source alignments are natural byproduct of the parser state, providing structural information of where and how to generate the next graph component. Our default use of hard attention to encode such alignments works the best. We explore two other strategies for modeling alignments. One is to supervise cross-attention distributions for the same heads with inferred alignments during training, inspired by <ref type="bibr" target="#b29">Strubell et al. (2018)</ref>. The other is to directly add the aligned source contextual embeddings from the encoder top layer to the decoder input at every generation step. The former hurts the model performance, indicating the model is unable to learn the underlying transition logic to infer correct alignments, while the latter does equally well as our default model. These results justify the modeling of structural constraints, even when finetuning strong pre-trained models such as BART. We also ablate the use of COPY action in our transition system. The sep-voc model suffers but the joint-voc model is not affected. Without COPY action, the joint-voc model would rely more on BART's pre-trained subword embeddings to split node concepts more frequently, while the sepvocab model would need to learn to generate more rare concepts from scratch. This indicates that BART's strong generation power is fully used to tackle concept sparsity problems with its subwords.  Special Nodes in Joint-Voc In <ref type="figure">Figure 3</ref>, we show the joint-voc model performance with different sized (joint) vocabularies. The vocabulary size is controlled by specifying the minimum frequency of occurrence needed for an AMR concept to be added to the vocabulary. For instance, when the minimum frequency is 1, all 12475 AMR concepts from the training data are added onto the BART vocabulary. The number of added concepts decreases as we increase the minimum frequency threshold. On model performance side, we only observe ?0.2 score variations resulting from vocabulary expansion. More interestingly, the model can work equally well when no special concepts are added to the BART vocabulary (minimum node frequency is ?) -where all the node names are split and generated with BART subword tokens. Although our default setup uses frequency threshold of 5 in joint-voc expansion, following <ref type="bibr" target="#b5">Bevilacqua et al. (2021)</ref>, it seems unnecessary in terms of achieving good performance. This highlights the efficacy of utilizing the pre-trained BART's lan-guage generation power for AMR concepts even with relatively small annotated training datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pre-trained Parameters</head><p>We study the contribution of different pre-trained BART components in We also experiment with freezing BART parameters during training in the bottom part of Table 6. Our results of freezing the BART encoder are on similar levels of previous best RoBERTa feature based models, which is behind the full finetuning. Overall, full initialization from BART with structure-aware fine-tuning (#8) works the best.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Related Work</head><p>Using seq-to-seq to predict linearized graph sequences for parsing was proposed in <ref type="bibr" target="#b32">Vinyals et al. (2015)</ref> and is currently a very extended approach <ref type="bibr" target="#b30">(Van Noord and Bos, 2017;</ref><ref type="bibr" target="#b11">Ge et al., 2019;</ref><ref type="bibr" target="#b27">Rongali et al., 2020)</ref>. However, it is only recently with the rise of pre-trained Transformer decoders, that these techniques have become dominant in semantic parsing. <ref type="bibr" target="#b36">Xu et al. (2020)</ref> proposed custom multi-task pre-training and fine-tuning approach for conventional Transformer models <ref type="bibr" target="#b31">(Vaswani et al., 2017)</ref>. The massively pre-trained transformer BART <ref type="bibr" target="#b14">(Lewis et al., 2019)</ref> was used for executable semantic parsing in <ref type="bibr" target="#b9">Chen et al. (2020)</ref> and AMR parsing in <ref type="bibr" target="#b5">Bevilacqua et al. (2021)</ref>. The importance of strongly pre-trained decoders seems also justified as BART gains popularity in various semantic generation tasks <ref type="bibr" target="#b9">(Chen et al., 2020;</ref><ref type="bibr">Shi et al., 2020)</ref>. Our work aims at capitalizing on the outstanding performance shown by BART, while providing a more structured approach that guarantees well-formed graphs and yields other desirable sub-products such as alignments. We show that this is not only possible but also attains state-of-the art parsing results without graph re-categorization. Our analysis also shows that contrary to <ref type="bibr" target="#b36">Xu et al. (2020)</ref>, vocabulary sharing is not necessary for strong performance for our structural fine-tuning.</p><p>Encoding of the parser state into neural parsers has been undertaken in various works, including seq-to-seq RNN models <ref type="bibr" target="#b6">Buys and Blunsom, 2017)</ref>, encoder-only Transformers <ref type="bibr" target="#b0">(Ahmad et al., 2019)</ref>, seq-to-seq Transformers <ref type="bibr" target="#b13">(Astudillo et al., 2020;</ref><ref type="bibr" target="#b40">Zhou et al., 2021)</ref> and pre-trained language models <ref type="bibr" target="#b24">(Qian et al., 2021)</ref>. Here we explore the application of these approaches to pre-trained seq-to-seq Transformers. Borrowing ideas from <ref type="bibr" target="#b40">Zhou et al. (2021)</ref>, we encode alignment states into the pre-trained BART attention mechanism, and re-purpose its selfattention as a pointer network. We also rely on a minimal set of actions targeted to utilize BART's generation with desirable guarantees, such as no unattachable nodes and full recovery of all graphs. We are the first to explore transition-based parsing applied on fine-tuning strongly pre-trained seq-toseq models, and we demonstrate that parser state encoding is still important for performance, even when implemented inside of a powerful pre-trained decoder such as BART.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Conclusion</head><p>We explore the integration of pre-trained sequenceto-sequence language models and transition-based approaches for AMR parsing, with the purpose of retaining the high performance of the former and structural advantages of the latter. We show that both approaches are complementary, establishing the new state of the art for AMR 2.0. Our results indicate that instead of simply converting the structured data into unstructured sequences to fit the need of the pre-trained model, it is possible to effectively re-purpose a generic pre-trained model to a structure-aware one achieving strong performance. Similar principles can be applied to adapt other powerful pre-trained models such as T5 (Raffel et al., 2019) and GPT-2 <ref type="bibr" target="#b25">(Radford et al., 2019)</ref> for structured data predictions. It is worth exploring thoroughly the pros and cons of introducing structure to the model compared to removing structure from the data (linearization) in various scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Dataset Statistics</head><p>We list the dataset sizes of AMR benchmarks in <ref type="table" target="#tab_11">Table 7</ref>. The sizes increase with the release version number. AMR 2.0 is the most used by far. AMR 2.0 shares the same set of sentences for development and test data with AMR 1.0, but with revised annotations and wikification links. AMR 3.0 is released most recently, which is under-explored.</p><p>Our silver data originate from two sources. First, we use ?20K example sentences (?386K tokens) from the propbank frames included in the AMR 3.0 distribution. Second, we use randomly selected ?27K sentences (?620K tokens) from SQuAD 2.0 context sentences available from https://rajpurkar.github. io/SQuAD-explorer/.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Details of Model Structures and Number of Parameters</head><p>In <ref type="table">Table 8</ref>, we list the detailed model configuration and number of parameters of the official pre-trained BART models. Our fine-tuned StructBART is with different action vocabulary strategies which builds additional embedding vectors for certain action symbols. The numbers vary from training dataset. We list the detailed number of parameters of our fine-tuned model in <ref type="table">Table 9</ref>. The fine-tuned model only increases about 3%-8% more parameters for sep-voc model and 0.4%-1% more parameters for joint-voc model.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>AMR graph of the sentence I have no opinion on the New York Mets. Examples of subgraphs for entity anonymization, collapsing of verbalized nouns and removal of the polarity node and edge.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>From top to bottom: graph (solid lines), sentence (source), addressable action positions and action sequence (target) for the sentence Employees liked their Boston trip, aligned (dotted lines) to its AMR graph. Arccreating actions are displayed vertically due to space constraints. Words are repeated in grey to indicate the word under cursor for each action. The node Boston in dotted box is created by copying the token under cursor via COPY action at position 13. LA(1,ARG0) creates a left arc with label ARG0 from the top concept like-01 to the concept person at position 1. For the concept trip-03, LA(1,ARG0) is a co-reference (re-entrancy) to the concept person.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Fine-grained F1 scores on the AMR 2.0 test set, among models that do not use extra silver data and graph re-categorization. The model IDs are matched with those inTable 1for detailed model features. We report results with our single best model (selected on development data) for fair comparison.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Transition system comparison, including their effects on different parsing models. * we adopt the cited model without graph structure embedding to compare and run on our proposed oracle.</figDesc><table><row><cell>Transition System</cell><cell cols="2">Avg. #actions Oracle SMATCH</cell></row><row><cell>Astudillo et al. (2020)</cell><cell>76.2</cell><cell>98.0</cell></row><row><cell>Zhou et al. (2021)</cell><cell>41.6</cell><cell>98.9</cell></row><row><cell>Ours</cell><cell>45.6</cell><cell>99.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4</head><label>4</label><figDesc></figDesc><table><row><cell>: Average action sequence length and oracle</cell></row><row><cell>coverage on AMR 2.0 training data from different tran-</cell></row><row><cell>sition systems. Average source sentence length is 18.9.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>?0.1 84.2 ?0.1 No align. modeling 83.5 ?0.0 83.4 ?0.2 Align. soft supervision 82.9 ?0.0 83.0 ?0.0</figDesc><table><row><cell>Model</cell><cell>SMATCH (%)</cell></row><row><cell>Variation</cell><cell>sep-voc joint-voc</cell></row><row><cell cols="2">Ours (hard attention) 84.0 Align. add src emb. 83.9 ?0.0 84.1 ?0.0</cell></row><row><cell>No COPY action</cell><cell>83.1 ?0.1 84.1 ?0.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Ablation study of structure modeling with transition alignments. Results are on AMR 2.0 test data.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc>Effects of pre-trained BART parameters. Results are with our sep-voc model on AMR 2.0 data.</figDesc><table /><note>* failed to converge with a range of hyper-parameters.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6 .</head><label>6</label><figDesc>With our sep-voc model, we decompose the whole seq-to-seq Transformer into four components for BART initialization, i.e.</figDesc><table><row><cell>the source</cell></row><row><cell>embedding (mapped with BART shared embed-</cell></row><row><cell>ding), encoder, decoder, and the separate target</cell></row><row><cell>embedding (initialized with the average subword</cell></row><row><cell>embeddings from BART shared embedding). We</cell></row><row><cell>run both the StructBART base model and the Struct-</cell></row><row><cell>BART large model with different combinations of</cell></row><row><cell>parameter initialization, on the top part of Table 6.</cell></row><row><cell>We can see that a randomly initialized model of the</cell></row><row><cell>same size (#1) performs badly. There is an accu-</cell></row><row><cell>mulative effect of BART initialization in helping</cell></row><row><cell>the model performance, except that BART decoder</cell></row><row><cell>can not work alone well without its encoder (#4).</cell></row><row><cell>The encoder gives the largest performance gains</cell></row><row><cell>(#3 vs. #2, #6 vs. #5) of about 10 points. Adding</cell></row><row><cell>the decoder further gives around 1.4 points on top</cell></row><row><cell>(#7 vs. #6), justifying its importance as well.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 7 :</head><label>7</label><figDesc>Number of sentence-AMR instances in the AMR benchmark datasets.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">With the only exception being disconnected graphs, which happen infrequently in practice.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">PRED wraps the node name inside the action as PRED(&lt;string&gt;), and CONFIRM calls a subroutine to predict the AMR node and find the right propbank sense.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Roughly 1% of graphs contain about 1-2 unaligned nodes. Our proposed transition system makes better use of BART pre-trained decoder compared to previous transition-based approaches (see Section 6) while greatly simplifying the transition set. It also naturally produces node-to-word alignments via source token cursor in the meantime.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">In practice the separate embeddings are initialized with the average subword embeddings from the original BART vocabulary, which gave small gains over random initialization.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9">Code and model available at https://github.com/ IBM/transition-amr-parser.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="11">We can recover 100% of AMR 2.0 training graphs excluding 4 with notation errors. Imperfect SMATCH is due to ambiguities of our parser in recovering Penman notation.</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Implementation Details</head><p>We use the Adam optimizer with ? 1 = 0.9 and ? 2 = 0.98. Batch size is set to 2048 maximum number of tokens, and gradient is accumulated over 4 steps. The the learning rate schedule is the same as <ref type="bibr" target="#b31">Vaswani et al. (2017)</ref>, where we use the maximum learning rate of 1e?4 with 4000 warm-up steps. Dropout of rate 0.2 and label smoothing of rate 0.01 are used. These hyperparameters are fixed and not tuned for different models and datasets, as we found results are not   sensitive within small ranges. Without silver data, we train sep-voc models for 100 (AMR 1.0 &amp; 2.0) or 120 (AMR 3.0) epochs and joint-voc models for 40 epochs as the latter is found to converge faster. The best 5 (AMR 1.0 &amp; 2.0) or 10 (AMR 3.0) checkpoints among the last 40/30 epochs are selected based on development set SMATCH from greedy decoding and averaged over the model parameters as our final model. With the 50K silver data, we train both sep-voc and joint-voc models for 20 epochs and select the best 10 checkpoints for model parameter averaging. We use a default beam size of 10 for decoding for our final parsing scores. Our models are implemented with the FAIRSEQ toolkit <ref type="bibr" target="#b23">(Ott et al., 2019)</ref>, trained and tested on a single Nvidia Tesla V100 GPU with 32GB memory. We use fp16 mixed precision training whenever possible, with which training a large model on AMR 2.0 takes about 10 hours for sep-vocab models and 7 hours for joint-vocab models, and the time varies proportionally with data size for other datasets and with silver data.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">On difficulties of cross-lingual transfer with order differences: A case study on dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wasi</forename><surname>Ahmad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhisong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuezhe</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanyun</forename><surname>Peng</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1253</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2440" to="2452" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramon</forename><surname>Fernandez Astudillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.10669</idno>
		<title level="m">Tahira Naseem, Austin Blodgett, and Radu Florian. 2020. Transition-based parsing with stacktransformers</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">AMR parsing using stack-LSTMs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Al-Onaizan</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D17-1130</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1269" to="1275" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Al-Onaizan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.07755</idno>
		<title level="m">Amr parsing using stack-lstms</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Abstract meaning representation for sembanking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Banarescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Bonial</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Madalina</forename><surname>Georgescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kira</forename><surname>Griffitt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulf</forename><surname>Hermjakob</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martha</forename><surname>Palmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Schneider</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th linguistic annotation workshop and interoperability with discourse</title>
		<meeting>the 7th linguistic annotation workshop and interoperability with discourse</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="178" to="186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">One spring to rule them both: Symmetric amr semantic parsing and generation without a complex pipeline</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michele</forename><surname>Bevilacqua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rexhina</forename><surname>Blloshmi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Navigli</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Robust incremental neural semantic graph parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Buys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.07092</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">AMR parsing via graphsequence iterative inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wai</forename><surname>Lam</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.119</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1290" to="1301" />
		</imprint>
	</monogr>
	<note>Online. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Smatch: an evaluation metric for semantic feature structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Short Papers</publisher>
			<date type="published" when="2013" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="748" to="752" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Low-resource domain adaptation for compositional task-oriented semantic parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xilun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asish</forename><surname>Ghoshal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yashar</forename><surname>Mehdad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sonal</forename><surname>Gupta</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.03546</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">An incremental parser for abstract meaning representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Damonte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giorgio</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Satta</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.06111</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Modeling source syntax and semantics for neural amr parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donglai</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junhui</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhua</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shoushan</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4975" to="4981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Leveraging Abstract Meaning Representation for knowledge base question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavan</forename><surname>Kapanipathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ibrahim</forename><surname>Abdelaziz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srinivas</forename><surname>Ravishankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ram?n</forename><surname>Fernandez Astudillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristina</forename><surname>Cornelio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saswati</forename><surname>Dana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Achille</forename><surname>Fokoue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dinesh</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alfio</forename><surname>Gliozzo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sairam</forename><surname>Gurajada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hima</forename><surname>Karanam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naweed</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dinesh</forename><surname>Khandelwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Young-Suk</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunyao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francois</forename><surname>Luus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ndivhuwo</forename><surname>Makondo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nandana</forename><surname>Mihindukulasooriya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tahira</forename><surname>Naseem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Neelam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucian</forename><surname>Popa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Revanth Gangi</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Riegel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaetano</forename><surname>Rossiello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Udit</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G P Shrivatsa</forename><surname>Bhargav</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mo</forename><surname>Yu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.findings-acl.339</idno>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021</title>
		<meeting><address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="3884" to="3894" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Pushing the limits of amr parsing with self-learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Young-Suk</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramon</forename><surname>Fernandez Astudillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tahira</forename><surname>Naseem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Revanth</forename><surname>Gangi Reddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Florian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.10673</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal ; Abdelrahman Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ves</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.13461</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Michael Lingzhi Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander M</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rush</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.05256</idno>
		<title level="m">A hierarchy of graph neural networks based on learnable local features</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Flanigan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Thomson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Norman</forename><surname>Sadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah A</forename><surname>Smith</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.10399</idno>
		<title level="m">Toward abstractive summarization using semantic representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Encoder-decoder shift-reduce syntactic parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangming</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th International Conference on Parsing Technologies</title>
		<meeting>the 15th International Conference on Parsing Technologies<address><addrLine>Pisa, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="105" to="114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">A differentiable relaxation of graph segmentation and alignment for amr parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunchuan</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Titov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.12676</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">AMR parsing as graph prediction with latent alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunchuan</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Titov</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P18-1037</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="397" to="407" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Addressing a question answering challenge by combining statistical methods with inductive rule learning and reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arindam</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chitta</forename><surname>Baral</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">A semantics-aware transformer model of relation linking for knowledge base question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tahira</forename><surname>Naseem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srinivas</forename><surname>Ravishankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nandana</forename><surname>Mihindukulasooriya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ibrahim</forename><surname>Abdelaziz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Young-Suk</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavan</forename><surname>Kapanipathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alfio</forename><surname>Gliozzo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Gray</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>ACL</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Rewarding Smatch: Transition-based AMR parsing with reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tahira</forename><surname>Naseem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Florian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1451</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4586" to="4592" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.01038</idno>
		<title level="m">fairseq: A fast, extensible toolkit for sequence modeling</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Structural guidance for transformer language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tahira</forename><surname>Peng Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roger</forename><surname>Naseem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ram?n</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fernandez Astudillo</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.acl-long.289</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="3735" to="3745" />
		</imprint>
	</monogr>
	<note>Online. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">OpenAI blog</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter J</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.10683</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Don&apos;t parse, generate! a sequence to sequence architecture for task-oriented semantic parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subendhu</forename><surname>Rongali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Soldaini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emilio</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wael</forename><surname>Hamza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The Web Conference 2020</title>
		<meeting>The Web Conference 2020</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2962" to="2968" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiguo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henghui</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">Hanbo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.10309</idno>
		<title level="m">Cicero Nogueira dos Santos, and Bing Xiang. 2020. Learning contextual representations for semantic parsing with generation-augmented pre-training</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Linguistically-informed self-attention for semantic role labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emma</forename><surname>Strubell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Verga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Andor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.08199</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Neural semantic parsing by character-based translation: Experiments with abstract meaning representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rik</forename><surname>Van Noord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johan</forename><surname>Bos</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.09980</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Grammar as a foreign language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terry</forename><surname>Koo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2773" to="2781" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Guided neural language generation for abstractive summarization using abstract meaning representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Vlachos</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.09160</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Getting the most out of amr parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nianwen</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 conference on empirical methods in natural language processing</title>
		<meeting>the 2017 conference on empirical methods in natural language processing</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1257" to="1268" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A transition-based algorithm for amr parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nianwen</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameer</forename><surname>Pradhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="366" to="375" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Improving amr parsing with sequence-to-sequence pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongqin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junhui</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhua</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.01771</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Amr parsing as sequence-to-graph transduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xutai</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Duh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Van Durme</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.08704</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Broad-coverage semantic parsing as transduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xutai</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Duh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Van Durme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3784" to="3796" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Stack-based multi-layer attention for transition-based dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhirui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shujie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enhong</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D17-1175</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1677" to="1682" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Amr parsing with action-pointer transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tahira</forename><surname>Naseem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ram?n</forename><surname>Fernandez Astudillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Florian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="5585" to="5598" />
		</imprint>
	</monogr>
	<note>Online</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
