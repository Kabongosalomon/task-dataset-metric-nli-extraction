<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main"></title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T11:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Autonomous vehicles</term>
					<term>decision-making</term>
					<term>holistic transformer</term>
					<term>multiple cues</term>
					<term>trajectory prediction</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Trajectory prediction and behavioral decision-making are two important tasks for autonomous vehicles that require good understanding of the environmental context; behavioral decisions are better made by referring to the outputs of trajectory predictions. However, most current solutions perform these two tasks separately. Therefore, a joint neural network that combines multiple cues is proposed and named as the holistic transformer to predict trajectories and make behavioral decisions simultaneously. To better explore the intrinsic relationships between cues, the network uses existing knowledge and adopts three kinds of attention mechanisms: the sparse multi-head type for reducing noise impact, feature selection sparse type for optimally using partial prior knowledge, and multi-head with sigmoid activation type for optimally using posteriori knowledge. Compared with other trajectory prediction models, the proposed model has better comprehensive performance and good interpretability.</p><p>Perceptual noise robustness experiments demonstrate that the proposed model has good noise robustness. Thus, simultaneous trajectory prediction and behavioral decision-making combining multiple cues can reduce computational costs and enhance semantic relationships between scenes and agents.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>The emergence of autonomous vehicles promises to improve people's travel efficiency and safety. Compared with human drivers, autonomous vehicles are supposed to have a more comprehensive and accurate perception of the surrounding environment <ref type="bibr" target="#b0">[1]</ref>.</p><p>Based on social knowledge obtained by perception, autonomous vehicles can better infer the intentions of surrounding agents and decide the subsequent tactical maneuvers. Currently, autonomous vehicles perform reasonable path planning and motion control to realize the tracking of its expected trajectory. In this process, the intention inferencing of surrounding agents and the decisionmaking of the ego-vehicle are the two key modules that need to function correctly to realize automated driving <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref>.</p><p>In most autonomous driving systems, trajectory prediction and behavior decision-making of the ego-vehicle are applied as two partitioned modules using separate models <ref type="bibr" target="#b3">[4]</ref>. However, there is a strong correlation between trajectory prediction and decision- making as both require a full understanding of the surrounding environment, and the decision-making task requires trajectory prediction information. In fact, trajectory prediction by the ego-vehicle outputs a feasible trajectory that contains decision-making information. The combination of decision-making and trajectory prediction will better enable the network to share and learn complementary information. Furthermore, the decision-making task plays the role of regularization, which improves the performance of all tasks <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6]</ref>. As these tasks are shared, the speed of inferencing is thus improved, alleviating resource limitations currently hampering the performance of autonomous vehicles.</p><p>Generally, several features are extracted for trajectory prediction, including dynamic and kinematic features of agents, interaction features among agents, and interaction features between agents and the environment <ref type="bibr" target="#b6">[7]</ref>. Decision information is also considered, which requires the neural network to process multiple cues simultaneously. Both correlation and redundancy exist in these cues. The traditional approach for this simultaneous processing is to simply concatenate them for decoding. This approach has two disadvantages. First, when multiple cues are simply concatenated, the decoder often has difficulty making full use of the cues;</p><p>moreover, it is difficult to eliminate redundant information. Second, performing separate feature extraction procedures is not conducive to understanding the intrinsic relationships among cues <ref type="bibr" target="#b7">[8]</ref>. To better concatenate multiple cues, we propose a novel holistic transformer (HT) that selects different attention mechanisms under the conditions of no prior knowledge, partial prior knowledge and posteriori knowledge of decisions. Then, it efficiently selects the useful information from multiple cues and reduces learning difficulty.</p><p>The context representations of the surrounding environment are essential to scene understanding; these representations are the premise of trajectory prediction and decision-making. Traditional methods only select a few key features <ref type="bibr" target="#b8">[9]</ref>, making it difficult to solve the interaction profiles of multiple agents in complex scenarios. Hence, the accuracy is negatively affected by the multimodality of agent behaviors. To represent heterogeneous environment information, some deep learning methods rasterize scene and agent information into image-style inputs to represent lane lines, obstacles, and agents using different colors <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11]</ref>.</p><p>These inputs are fed to the backbone for encoding and to obtain a good contextual representation of the environment. However, rasterized images are prone to lossy encoding, and topological relationships between environmental elements are difficult to model explicitly. For example, with the current methods, if a lane of interest extends for a long distance, the rasterized images not only recognize and track it but also track other useless large areas. This not only wastes computational resources but also affects model presentation. To narrow the autonomous vehicle's cues to a region of interest (ROI), this study adopts the vector representation method, which applies vectorized encoding of scenes and agent trajectories and uses their various attributes as vector features <ref type="bibr" target="#b11">[12]</ref>. This method can be used to efficiently model the spatial topological relationships between scenes and agents and improve environmental factor encoding efficiency <ref type="bibr" target="#b12">[13]</ref>.</p><p>HT is a novel network that outputs trajectory predictions and behavior decisions simultaneously. The proposed model consists of an encoder, a feature selection network, and a decoder. The encoder is divided into agent encoding and scene encoding modules. Agents lacking prior knowledge are encoded using sparse multi-head self-attention to reduce the impact of perceptual noise. The learning lane graph representations for motion forecasting network (LaneGCN) <ref type="bibr" target="#b12">[13]</ref> is used to encode scene information. Subsequently, we provide a new feature selection network based on partial prior knowledge to model the multimodal interactions among agents and between agents and scenes using sparse attention. According to the posteriori knowledge of decision-making, sigmoid-based self-attention is adopted to fuse decision-making information to output multiple trajectories alongside their probabilities. Finally, decision-posterior knowledge attention visualization, perceptual noise robustness, and ablation comparison experiments are performed to demonstrate the advantages of the proposed model.</p><p>The main contributions of this study can be summarized as follows:</p><p>? A joint neural network (i.e., HT) for simultaneous trajectory prediction and behavioral decision-making based on vector representation, which efficiently models the spatio-temporal topological relationships between scenes and agents and enhances the semantic relationships among vectors ? An attention-weighted regularizer for trajectory prediction that uses the behavior decision task to improve performance and reduce computational costs ? An organically integrated system of attention mechanisms (i.e., sparse multi-head, sparse feature selection, and multihead with sigmoid) based on the characteristics of multiple cues.</p><p>The remainder of this paper is organized as follows. Section 2 provides a literature review of related works. Section 2 presents the proposed method, including the network structure and loss function. The experimental settings, results, and evaluations are described in Section 4. Finally, Section 5 presents concluding remarks and limitations and highlights the scope of future works.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORKS</head><p>Presently, there are many examples of trajectory prediction and behavioral decision-making studies, and this section examines the ones closely related to the proposed model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Traditional methods</head><p>Traditional prediction methods focus on dynamic and kinematic parameters of the vehicle. As the vehicle yaw rate and acceleration cannot change suddenly in a short period, the constant yaw rate and acceleration (CYRA) model has been adopted most frequently <ref type="bibr" target="#b13">[14]</ref>. However, for long-term prediction, CYRA's prediction performance is severely degraded as the assumption of constancy does not hold <ref type="bibr" target="#b6">[7]</ref>. To improve the accuracy of long-term prediction, many models have begun considering driving patterns and agent interactions. For example, Houenou et al. <ref type="bibr" target="#b8">[9]</ref> proposed a method of combining CYRA with maneuver recognition, which discretizes the solution space into multiple trajectories and selects the optimal one based on road topology. Considering the interaction between agents and the surrounding environment in long-term prediction, Xie et al. <ref type="bibr" target="#b14">[15]</ref> proposed a multi-interaction model for short-and long-horizon trajectory prediction. A hidden Markov model was adopted to identify driving maneuvers. Finally, the multiple interacting model combines short-horizon vehicle physical modeling with long-horizon maneuver recognition. Additionally, a Gaussian stochastic process <ref type="bibr" target="#b15">[16]</ref> with a hidden Markov model <ref type="bibr" target="#b16">[17]</ref> and a finite automatic state machine <ref type="bibr" target="#b17">[18]</ref> was applied to trajectory prediction. However, these methods adopt manual features, which poorly model complex scenes and interactions; thus, the multimodality of agent behaviors are not understood accurately.</p><p>Traditional autonomous vehicle decision-making methods aim to make safe and reasonable driving decisions based on the surrounding environment and the state of the ego-vehicle. Currently, owing to their good stability and practicability, finite automatic state machines are often used in trajectory decision-making systems <ref type="bibr" target="#b18">[19]</ref>. A finite automatic state machine is a mathematical model of discrete input and output systems consisting of a finite number of states. The current state accepts events and generates corresponding actions, causing state transitions. Decision trees are also applied for decision-making methods <ref type="bibr" target="#b19">[20]</ref>.</p><p>These methods suffer from problems like those of trajectory prediction in that complex rules must be formulated in the face of complex scenes, and the ability of scene understanding is poor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Deep learning methods</head><p>For trajectory prediction, there are three mainstream deep learning methods, according to the classification of scene representation.</p><p>The first method feeds raw input directly into the neural network and is suitable for simple scenarios, such as highways, expressways, etc. Many methods simply encode the distance between the ego-vehicle and the lane lines, but other important information is often omitted. When given simple road structures, these methods do well mainly by focusing on the interactions among agents. For example, Messaoud et al. <ref type="bibr" target="#b1">[2]</ref> proposed a long short-term memory (LSTM) encoder/decoder structure based on multi-head attention, which maps the agent position to an attention matrix and visualizes their relative importance around the ego-vehicle. Deo et al. <ref type="bibr" target="#b20">[21]</ref> proposed convolution social pooling, which uses social tensors to encode the motions of surrounding agents. Additionally, a graph convolution neural network (CNN) <ref type="bibr" target="#b21">[22]</ref> using generative adversarial imitation learning <ref type="bibr" target="#b22">[23]</ref> with an LSTM <ref type="bibr" target="#b23">[24]</ref> were applied to model various agent interactions. However, due to the lack of comprehensive scene models, these methods have difficulty dealing with complex scenarios.</p><p>To represent complex road topologies, some methods were inspired by the ability of CNNs to extract information from images. By establishing a bird's-eye view raster image as input, Visual Geometry Group and residual neural network (ResNet) backbones are used to encode the scenes <ref type="bibr" target="#b24">[25]</ref>. With these, the output of the image convolution is used to decode the trajectory of each agent. For example, Zhao et al. <ref type="bibr" target="#b25">[26]</ref> proposed multi-agent tensor fusion, encoded the scene information based on a grid graph, and embedded the multiagent spatio-temporal information into the scene tensor. Tung et al. <ref type="bibr" target="#b26">[27]</ref> proposed CoverNet, which defines the trajectory prediction problem as the classification of different trajectory sets, inputting the semantic map and state into the backbone and trajectory cluster generator, respectively. Doing so eliminates impossible trajectories from the cluster and selects the optimal one. Additionally, anchor trajectories based on the prior knowledge of motion constraints are used to alleviate the problem of multimodal predictions <ref type="bibr" target="#b27">[28]</ref>.</p><p>However, the performance of these methods is limited by the spatial resolution of the grid images, which are easily disturbed by irrelevant raster image regions.</p><p>Map information contains strong spatial topology data, which are important for prediction in complex scenarios. A vector representation map describes topological information and ignores useless information. Thus, when encountering a roundabout or a long straight road, it can efficiently model topological relationships. For example, VectorNet <ref type="bibr" target="#b11">[12]</ref> represents various road components in the form of vectors, which avoids the lossy rendering of grid maps and the convolutional encoding of intensive calculations. Each vector is connected through a graph CNN to model the connection relationship between each vector component. To capture complex road topologies and long-distance dependencies, LaneGCN <ref type="bibr" target="#b12">[13]</ref> was proposed in which global information is integrated through the fusion of scenes and agents in the ROI. Additionally, the distributed representations for graph-centric motion forecasting model <ref type="bibr" target="#b28">[29]</ref> and the temporal point-cloud network (TPCN) <ref type="bibr" target="#b29">[30]</ref> use similar methods for modeling. However, when these methods fuse agent and scene information, they do not carefully select the predicted features from prior knowledge, thus limiting the expressive ability of the model.</p><p>Currently, most deep learning decision-making methods do not include prediction tasks. They generally use CNNs to encode and learn surrounding environments using cameras and other sensors to make classification decisions <ref type="bibr" target="#b30">[31]</ref>. Additionally, deep reinforcement learning is popular <ref type="bibr" target="#b31">[32]</ref>; however, it often relies on experience from its reward function setting, and it has difficulty understanding the complex topology of map information, resulting in decision-jumping in unknown scenarios.</p><p>In summary, several models exist for simultaneous prediction and decision-making. However, if decision information and other prior knowledge can be used as additional branches to improve network performance, it would greatly improve the fit between predictions and decisions while reducing calculation costs and jointly improving accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">DEVELOPMENT OF PROPOSED MODEL</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Problem formulation</head><p>In this study, the proposed model input includes scene and agent information. Scene information refers to lane lines, static obstacles, traffic signs, and traffic lights. These items are often combined with high definition (HD) maps, Global Positioning System localization techniques, and other data sources to establish a complete scene. Additionally, agent information includes all agents in the ROI, such as vehicles, non-motor vehicles, and pedestrians.</p><p>The scene information set, ? , contains road, traffic light, and other ROI data for the ego-vehicle, and the agent information set, ? , includes the ego-vehicle, 0 A , and all perceived agents <ref type="bibr" target="#b0">1</ref>   Additionally, the position coordinate of lane vectors is used as input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 1</head><p>Meanings of the different lane vector adjacency matrices It should be noted that merge must be overlap, but not vice versa.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Y N</head><p>The objective here is to predict K trajectories,</p><formula xml:id="formula_0">? ? , , ,?, ? ? ? t k t k t k</formula><p>x y a ? , of all agents in the scene and the probability, p k , of each trajectory, where K is the number of motion modalities, k = 1, 2, ?, K, t = 1, 2, ?, t fut . Additionally, the goal of this study is to output the maneuver decision, ?m D , and the lane decision, ?l D , of all agents. The maneuver decision refers to the tactical intention of the agent for future actions. <ref type="table" target="#tab_2">Table 2</ref> shows the types of maneuver decisions, where priority indicates which decision is defined by the current agent on the premise of meeting multiple decision conditions. Among them, "1" has the highest priority, and "6" has the lowest. Yield's decision has higher priority to improve the security of the decision. Lane decision refers to the lane lines to be followed by the agent in the future. </p><formula xml:id="formula_1">Yield + Decrease speed D 3</formula><p>Owing to the limitations of the surrounding agents, traffic lights, and other factors, the agent needs to slow down but not stop.</p><formula xml:id="formula_2">Following F 4</formula><p>An agent is considered as Following when it follows other agents that are ahead along the heading angle.</p><formula xml:id="formula_3">Ignore I 5</formula><p>Under the premise of abiding by traffic rules, the agent can arbitrarily accelerate or decelerate according to the current state, which is considered as Ignore.</p><formula xml:id="formula_4">Unknown U 6</formula><p>Owing to the short duration of perception, Unknown is defined when the decision is difficult to determine. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Proposed model</head><p>As shown in <ref type="figure" target="#fig_3">Figure 2</ref>, the proposed model includes three parts: an encoder, a feature selection network, and a decoder. These are used to encode agent information and scene information sets, select and fuse multiple cues, and output decoding information.</p><p>The input of the encoder comes from the input representation module. Each part is described in detail below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Encoder</head><p>The encoder encodes the agents and lane vectors and outputs the encoded features for feature selection, cue extraction, and fusion. Because the agent information set is composed of time-series data, and the lane vector set is composed of graphs, there are differences in their encoding methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1.1">Agent encoding</head><p>Agent trajectory prediction is a type of multivariable time series (MTS) prediction. To encode the MTS, recurrent neural networks (RNNs) are often used to capture temporal features, and LSTMs are more widely used <ref type="bibr" target="#b32">[33]</ref>. However, owing to the absence of long-term dependency management, there are weaknesses in the extraction of long-term dependency relationships using RNNs, which can negatively affect trajectory prediction. Additionally, the autonomous vehicle perception module introduces noise to the input data due to severe weather, low-cost sensors, algorithm errors, etc. The traditional approach is to filter out high-frequency noise and smooth the vehicle trajectory. The disadvantage of this is that hand-crafted filtering is added, which may corrupt the original dynamics and kinematics of agents. Therefore, two key problems need to be solved in agent encoding: feature extraction across time steps and noise suppression.</p><p>The vanilla transformer eliminates convolution and extracts features across time steps using multi-head attention. It uses the full connection layer to extract features across multiple time steps. However, it lacks noise suppression. If any time steps contain too much noise, the data will have data-dependent aleatoric uncertainty. To suppress this uncertainty, we must pay less attention to the time steps. Therefore, we use information entropy to measure the uncertainty of each time step to improve its impact with low uncertainty. Inspired by <ref type="bibr" target="#b33">[34]</ref> and <ref type="bibr" target="#b34">[35]</ref>, we modified the vanilla transformer.</p><p>Because the transformer adopts scaled dot-product attention and requires global location representation, the positional embedding (PE) layer is used, and the feature dimension after input representation is dmodel,</p><formula xml:id="formula_5">2 / ( ,2 ) sin( / (2 ) ) model i d pos i hst PE pos t ? ,<label>(1)</label></formula><formula xml:id="formula_6">2 / ( ,2 1 ) cos( / (2 ) ) model i d pos i hst PE pos t ? ? ,<label>(2)</label></formula><p>where i = 1, 2, ?,</p><formula xml:id="formula_7">/ 2 model d ? ? ? ? .</formula><p>The standard scaled dot-product self-attention mechanism consists of query</p><formula xml:id="formula_8">L d ? ? ? Q , key L d ? ? ? K , and value L d ? ? ? V ,</formula><p>where L is the sequence length, L=t hst in the first sparse attention, and d is the feature dimension of h heads, where d model =hd. The input to the attention mechanism is recorded as XA, and Q, K, and V are obtained through linear layer transformation. The selfattention mechanism is described by the following formula:</p><formula xml:id="formula_9">( , , ) Softmax A d ? ? ? ? ? ? ? ? QK Q K V V .<label>(3)</label></formula><p>Let qi, ki, and vi be the vectors of the ith row of Q, K, and V, respectively, where i = 1, 2, ?, L. The attention value for the ith in the sequence is</p><formula xml:id="formula_10">( , ) ( , , ) ( , ) i j i j j i k k f A f ? ? ? q k q K V v q k ,<label>(4)</label></formula><p>where ( , ) exp(</p><formula xml:id="formula_11">/ ) i j i j f d ? ? q k q k . Note that ( , ) ( | ) ( , ) i j j i i k k f p f ? ? q k k q q k<label>(5)</label></formula><p>is a conditional probability density function,</p><formula xml:id="formula_12">( | ) 1 j i j p ? ? k q . Thus, ( | ) ( , , ) [ ] j i i p j A ? ? k q q K V v .<label>(6)</label></formula><p>Therefore, the scaled dot-product self-attention is the mathematical expectation of V under distribution ( | ) the neural network must learn. Thus, we look at the problem from the perspective of information entropy. When the information entropy is maximal, its uncertainty is also maximal. Information entropy ?(q) is defined as</p><formula xml:id="formula_13">( ) log q q q ? ? ? ? .<label>(7)</label></formula><p>Finding the maximum value of H(q) is equivalent to</p><formula xml:id="formula_14">1 1 min log s.t. 1 L i i i L i i q q q ? ? ? ? ? .<label>(8)</label></formula><p>1 i q ? ? , this problem can be considered a convex optimization problem. Thus, the solution satisfying the Karush-Kuhn-Tucker (KKT) condition is the optimal solution. Under equality constraints, the solution satisfying the KKT condition is found when the first partial derivative of the Lagrange function equals zero. The Lagrange function is given by</p><formula xml:id="formula_15">1 2 1 1 ( , ,..., , ) log 1 L L L i i i i i q q q q q q ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ,<label>(9)</label></formula><p>where ? is the Lagrange multiplier. Then, find the first-order partial derivative of <ref type="bibr">1 2</ref> ( , ,..., , ) n q q q ? ? with respect to <ref type="bibr">1 2</ref> , ,..., , n q q q ? and set the partial derivative equal to zero:</p><formula xml:id="formula_16">1 2 1 log log ... log 1 L L i i q q q q ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? .<label>(10)</label></formula><p>The problem can then be solved as 1</p><formula xml:id="formula_17">2 ... 1/ L q q q L ? ? ? ? , according to Eq. (10). That is, when q is uniformly distributed as (0, ) q U L ? , the uncertainty is the highest. When (0, ) q U L ? , the scaled dot-product self-attention becomes the mean value of V,</formula><p>which is redundant for residential input. Therefore, we keep the distribution, p, as far as possible from the distribution, q. Then, we use the Kullback-Leibler (KL) divergence to measure the distance between the distributions:</p><formula xml:id="formula_18">( || ) ( , ) ( ) log q KL q p q p q q p ? ? ? ? ? ? ,<label>(11)</label></formula><p>where ?(q, p) is the cross entropy. We substitute ( | ) 1/ </p><formula xml:id="formula_19">L L L i j i j i j j j j i k k L KL q p L f L L d d f ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? q k q k q k q k ,<label>(12)</label></formula><p>where ( || ) KL q p is bounded. Its lower bound is set by letting h(x) = -log x; then, the function is strictly convex, and</p><formula xml:id="formula_20">( | ) 1 j i q ? ? k q .</formula><p>According to the Jensen inequality,</p><formula xml:id="formula_21">1 1 1 ( , ) 1 ( || ) log log ln1 0 ( , ) ( , ) ( , ) L L i j i j j j i k k i k k f L KL q p f L f f ? ? ? ? ? ? ? ? ? ? ? ? q k q k q k q k ,<label>(13)</label></formula><p>and its upper bound is</p><formula xml:id="formula_22">1 1 1 1 1 1 1 1 ( || ) log exp log log exp max log max . L L L L L i j i j i j i j i j i j j j j j j j j KL q p L L L L L d d d d d d ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? q k q k q k q k q k q k<label>(14)</label></formula><p>Therefore, we choose the largest Nd sequence steps according to KL divergence to replace the original full self-attention.</p><p>However, the existence of exponential function in Eq. (12) leads to an unstable numerical overflow in the calculation of KL divergence. Thus, we use the approximate value instead:</p><formula xml:id="formula_23">1 1 max L i j i j j j M L d d ? ? ? ? ? ? ? ? ? ? ? ? ? ? q k q k .<label>(15)</label></formula><p>The substituted M can be understood as the difference between the dominant term and the mean. If the difference between them is small, then it can be ignored. During the continuous training process, the possibility is higher that this data is noise.</p><p>We calculate the M value of all q i , i = 1, 2, ?, L and choose </p><formula xml:id="formula_24">? ? ? ? ? ? ? ? Q K Q K V V .<label>(16)</label></formula><p>We then concatenate the outputs of the multi-head attention and add them to the inputs after normalization:</p><formula xml:id="formula_25">? ? ( , , ) s A A s A ? ? ? X X Q K V ,<label>(17)</label></formula><p>where ? is the normalization function. Then, the feedforward layer is adopted as</p><formula xml:id="formula_26">? ? 1 1 2 2 ReLU( ) out s A A A A A ? ? ? ? ? X X XW bW b ,<label>(18)</label></formula><p>where 4 1</p><formula xml:id="formula_27">model model d d A ? ? ? W , 4 1 model d A ? ? b , 4 2 model model d d A ? ? ? W , 2 model d A ? ? b</formula><p>, representing is the weight of linear layers.</p><p>After the sparse self-attention, to improve the influence of useful features again, we use the convolution-pooling unit. A shortcut is not used here to reduce further noise propagation. Max-pooling selects more significant convolution features to better suppress the influence of noise. After Nx sparse attention Conv1d blocks, the dynamic and kinematic features of the agent are encoded.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1.2">Lane vector encoding</head><p>LaneGCN is adopted to encode the lane vectors <ref type="bibr" target="#b12">[13]</ref>. Owing to the relatively high speeds of agents traveling long distances in a lane, convoluting only one predecessor or successor lane vector is insufficient. To capture the long-range dependencies, a dilated LaneConv for LaneGCN is used to focus more predecessors and successors at a multi-scale level:</p><formula xml:id="formula_28">? ? { , , , } 1 c c C k k out L f f i f i p f p s f s i r l m o c ? ? ? ? ? ? ? M M W M M W M M W + M M W ,<label>(19)</label></formula><p>where ? W is the weight of LaneConv, and k c is the cth dilation size. Normalization and rectified linear units are adopted after each </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Feature selection network</head><p>After encoding, the encoder obtains three cues: agent kinematics and static and dynamic lane vectors. When these cues are fused, the irrelevant features make neural network learning more difficult. Unlike the original input of the agent lacking prior knowledge, the agent often only pays attention to the lane and other agents within a certain range. Based on this prior knowledge, we can purposefully select scene elements requiring attention. Therefore, we propose a feature selection network that combines multiple cues.</p><p>The input of feature selection sparse attention is of the base and context types; that is, base pays attention to useful features in context under partial prior knowledge. First, the tensor-of-interest selector is used to select the set of interesting features according to the </p><formula xml:id="formula_29">? ? ? ? ? ? ? ? ? ? x x d d d d d X X</formula><p>x X x X <ref type="bibr" target="#b19">(20)</ref> where d i , d j is the coordinate of x i , x j , respectively, in the local coordinate system, and th ? is the threshold of the ROI. Then the first, second, and third items of the tuples in the set are fed to a linear layer and recorded as base X , ctx X , and bc D , respectively. We connect the context with the distance information, bc D , and combine it with base through the linear layer, activating it with the exponential linear unit (ELU) after normalization:</p><formula xml:id="formula_30">ELU( ( concat( , ) )) bc base b b ctx bc c ? ? ? ? X X W b X D W ,<label>(21)</label></formula><p>where Wb, Wc is the linear weight, and bb is the bias. When concat( Therefore, we adopt the gate selection mechanism to select the useful features in the context:</p><formula xml:id="formula_31">, ) 0 base b b ctx bc c ? ? ? X W b X D W ,</formula><formula xml:id="formula_32">bc ? ? ? = X W b ? ,<label>(22)</label></formula><formula xml:id="formula_33">( ) ( ) g g ? ? ? ? ? ? ? ? X W b W b ? ? ,<label>(23)</label></formula><formula xml:id="formula_34">ReLU( ( )) g b c ? ? ? ? X X X ,<label>(24)</label></formula><p>where W?, W?, and Wg are the linear weights, b?, b?, and bg are the biases, and ? is the Hadamard product. We then add and normalize the selected features to the original input:</p><formula xml:id="formula_35">, ( ) att b c base g ? ? ? X X X ,<label>(25)</label></formula><p>where ? indicates an index-add, and , ( , , ) Sigmoid</p><formula xml:id="formula_36">S A d ? ? ? ? ? ? ? ? QK Q K V V ,<label>(26)</label></formula><p>where the attention output, </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Decoder</head><p>The decoder consists of two parts: behavior decisions and trajectory predictions. X and is output through the linear layer to obtain the agent maneuver decision probability, . To improve the network's ability to explore the multimodality of agents, feature selection attention is adopted so that the feature selection sparse attention of the tensor-of-interest selector is removed. The base is out fuse X , and the context is the predicted value, , of each predicted trajectory is output through the SoftMax layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Loss function</head><p>The loss function consists of trajectory prediction and behavior decision losses. The trajectory prediction task predicts K trajectories and their corresponding probabilities. For any agent, let = arg max( ) ? pred traj k P to locate the trajectory with the greatest probability. To improve the ability of the model to explore the multimodality of an agent, the max-margin loss is adopted:</p><formula xml:id="formula_39">1 1?m ax(0, ) ( 1 )( 1) ? ? ? ? ? ? ? ? ? ? ? ? ? Ns k k ns ns M ns k k M p p J Ns K ,<label>(26)</label></formula><p>where ?k ns p is the probability of the kth modality of the nsth agent, and ? M is the margin.</p><p>To improve the confidence of the k * th trajectory, a pseudo label, ? ? k p , is set. That is, k * assigns a positive label, and ? ? k k are assigned negative labels. Then cross-entropy loss is adopted:</p><formula xml:id="formula_40">1 1 1l og 1 ? ? ? ? ? ? ? ? ? ? Ns K k c ns ns traj ns c C p p J Ns .<label>(27)</label></formula><p>Additionally, the error between predicted and the real trajectories is calculated using the smooth L1 loss: </p><formula xml:id="formula_41">2 1 , , 2 1 1 , 1?, if 1 1 2 1 1?, otherwise, 2 ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?</formula><p>where ns j a is the ground truth of the nsth agent at time j.</p><p>Behavior decision tasks are classified tasks, and cross-entropy loss is adopted: </p><formula xml:id="formula_43">1 1 1?( log (1 ) log(1 )) ( 1) ? ? ? ? ? ? ? ? ? ? ? Nl Ns l l l l lane nl ns C D D D D J Nl Ns ,<label>(29)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Training setting and evaluation metrics</head><p>As described, the length of the input feature sequence of agents was t hst =20. The 180 trajectory points of t fut = 30 and K = 6 modals in the future were predicted, and the agent lane and maneuver decisions were output. The feature dimension, dmodel, of the model was set to 128. In the sparse multi-head attention, h = 4 heads were selected. The largest Nd = 0.75L sequence steps were used according to KL divergence to replace the original full self-attention. The sparse multi-head attention block, Nx, the LaneGCN block, Ng, and the feature selection sparse attention block, Na, were set to three, four, and two, respectively. ? M was set to 0.2. The Adam optimizer was employed using a learning rate of 10 -3 and a mini-batch size of 32. Because the ground truth of the test set was not provided in the dataset, the trajectory prediction metrics of the model came from the test set, and other results came from the verification set.</p><p>For prediction performance evaluation, average displacement error (ADE) and final displacement error (FDE) were adopted. ADE is the average L2 distance between the prediction trajectory and the ground truth, and FDE is the L2 distance between endpoint of the prediction trajectory and the ground truth. In this study, minimum ADE (minADE) and minimum FDE (minFDE) at K = 1 and K = 6, respectively, were used as evaluation metrics to obtain the performance of the best predicted trajectory under single-and multi-modal cases. For decision performance evaluation, accuracy, precision, recall, and F1-score were used as basic model evaluation indicators. Furthermore, to better demonstrate the performance of the model in specific cases for maneuvering decisions, additional statistics were taken on yield decisions. For lane decisions, case-level recall was evaluated when at least one or more correct lane lines were identified.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Results and comparisons</head><p>To validate the proposed model, its results were compared to those of models proposed by other excellent studies conducted in recent years:</p><p>? Nearest-neighbor (NN) regression (Argoverse Baseline) <ref type="bibr" target="#b35">[36]</ref>: The baseline builds on NN and prunes the number of predicted trajectories based on how often they exit the drivable area.</p><p>? Target-driven trajectory (TNT) <ref type="bibr" target="#b27">[28]</ref>: A prediction framework that contains three training stages.</p><p>? DenseTNT <ref type="bibr" target="#b36">[37]</ref>: An anchor-free, end-to-end trajectory prediction model that directly outputs a set of trajectories from dense goal candidates.</p><p>? VectorNet <ref type="bibr" target="#b11">[12]</ref>: A hierarchical graph neural network that first exploits the spatial locality of individual road components represented by vectors, then it models the high-order interactions among all components.</p><p>? LaneGCN <ref type="bibr" target="#b12">[13]</ref>: A lane-graph CNN that captures the complex topology and long-range dependencies of the lane graph.</p><p>? Temporal point-cloud network (TPCN) <ref type="bibr" target="#b29">[30]</ref>: Extends ideas from point-cloud learning with dynamic temporal learning to capture both spatial and temporal information by splitting trajectory prediction into those same dimensions.</p><p>? Multi-modal transformer <ref type="bibr" target="#b37">[38]</ref>: A neural prediction framework based on the transformer structure that models the relationship among interacting agents and extract the attention of the target agent on map waypoints.</p><p>? Heatmap output for future motion estimation (HOME) <ref type="bibr" target="#b38">[39]</ref>: A framework that tackles the motion forecasting problem with an image output representing the probability distribution of the agent's future location.</p><p>? Scene transformer <ref type="bibr" target="#b39">[40]</ref>: A model using masking strategy as the query, enabling one to invoke a single model to predict agent behavior in many ways.</p><p>? HT: The model proposed in this study. <ref type="table" target="#tab_9">Table 3</ref> shows minADE and minFDE at K=1 and K=6, respectively. We see that the performance of the HT was slightly worse than that of scene transformer on K=6 minADE, and the proposed method performed best comprehensively with multiple performance metrics. LaneGCN and VectorNet are based on vector representation. Among them, VectorNet simply connects the features through a graph neural network. LaneGCN uses a feature pyramid network to encode the historical trajectory of agents, fuse lane features with agent features through the attention mechanism, and output the trajectory of agents alongside the predicted endpoint attention mechanism. HT is also based on vectorized input representation, and it showed great improvements in all evaluation metrics. Compared with the above two methods, HT learned the relationship between vectors more deeply, making learning, dissemination, and feature and information fusion between the agent and the road more efficient and comprehensive. Multi-modal and scene transformers are like vanilla methods. The multi-modal transformer trains stacked transformer blocks using a tailored region-based training strategy. The scene transformer realizes multi-input fusion by stacking transformer blocks across times, agents, and road graphs. The method based on the transformer performed better than other methods, indicating that the transformer did well with multi-source information extraction and fusion. Compared with the aforementioned two methods, decision-making auxiliary regularization was added to HT, which reduced the difficulty of model training.       <ref type="table" target="#tab_10">Table 4</ref> shows the results of maneuver decision classification on the Argoverse validation set. Generally, maneuver decisions "S" and "F" showed the best classification performance, and "N", "D," and "I" showed general classification performance. Notably, decisions "S", "N", and "D" all belonged to the yield class, in which classification errors are prone to occur. Therefore, it can be seen that the model had good classification performance (F1-score: 87.99%) when the metrics of the yield class were counted.</p><p>Similarly, lane decision performance can only reflect model performance, making it difficult to attribute to specific cases. Owing to the class imbalance between positive and negative lane decision classes (~1:100), decision accuracy had little significance.</p><p>Therefore, by calculating the recall of lane decisions at the case-level, it can be seen that the model output lane decision results more accurately (case-level lane decision recall: 89.98%).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Decision posterior knowledge attention visualization</head><p>In the feature selection network, the sigmoid activation function was used for the lane-scaled dot-product attention. Therefore,   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Perceptual noise robustness experiment</head><p>As described in Section 2, the input data used for trajectory prediction are vulnerable to noise, resulting in data-dependent aleatoric uncertainty, which lead to worsening trajectory prediction and behavioral decision-making performances. Therefore, we proposed a sparse multi-head attention for agent trajectory encoding. To verify its robustness to noise, we designed two experimental scenarios. The first simulated perceptual loss, in which one frame of data in the input data was replaced by the probability of p o , has a value of zero. The second scenario simulated perceptual noise, in which Gaussian noise was added to one frame of the input data with probability pn, where</p><formula xml:id="formula_44">~(0, / 100) n p v ?</formula><p>and v is the velocity of the agent. To better compare the vanilla and sparse multi-head attentions, the experimental comparison model involved replacement of the sparse multi-head attention with the vanilla multi-head attention. <ref type="table" target="#tab_11">Table 5</ref> shows the mean and variance values of the prediction results under the influence of different noise. The mean shows the accuracy of prediction, and the variance shows its stability. The smaller the mean and variance, the more accurate and stable the prediction result of the model. For ease of illustration, <ref type="figure" target="#fig_18">Figure 6</ref> shows the trajectory prediction performance degradation of the two comparison models under the influence of noise. According to <ref type="table" target="#tab_11">Table 5</ref> and <ref type="figure" target="#fig_18">Figure 6</ref>, the following conclusions can be drawn:  1) Because the original training data contained a lot of noise, the prediction error of sparse multi-head attention was smaller than that of the vanilla multi-head attention in the baseline model, and the prediction result was more stable. Under the influence of noise, the performance of both contrasting models decreased, and that of the minFDE increased with noise.</p><p>2) The sparse multi-head attention was significantly lower than the vanilla multi-head attention in terms of increment and increment ratio minFDE metrics. Additionally, when the noise was large, the mean increment ratio and sparse multi-head attention variance increment ratio of the two methods were similar, but the vanilla multi-head attention variance had a large increase. This is because there were a smaller number of data having large prediction errors in the vanilla multi-head attention model. This illustrates that sparse multi-head attention is more robust to perceptual noise.</p><p>3) By contrast, perceptual noise may be more influential than perceptual loss. This tells us that if there is obviously a large noise in the input data of one frame, the trajectory can be smoothed through preprocessing; otherwise, the frame can be directly eliminated to reduce the impact of noise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Comparison and ablation experiment</head><p>To verify the performance of each module of the proposed model, the following comparison and ablation experiments were conducted:</p><p>? No feature selection: The feature selection network was removed, and only lane-scaled dot-product attention was retained.</p><p>? Full feature selection: The tensor-of-interest selector in the feature selection sparse attention was removed.</p><p>? LSTM Encoder: The sparse multi-head attention in agent trajectory encoding was replaced with LSTM. ? HT: Full model proposed in this study. <ref type="table">Table 6</ref> shows the metrics of ablation in the 3-s prediction horizon using the Argoverse validation set. The following inferences were obtained based on these results: <ref type="table">Table 6</ref> Metrics of the ablation model in the 3 s prediction horizon for the argoverse validation set agents and lane lines were captured in the final decoding part; hence, performance dropped the most. If full attention feature selection is used, the performance is improved, but complex scenes may introduce useless information, which affects trajectory prediction performance. Therefore, guided by some prior knowledge, feature selection sparse attention can learn more accurate features.</p><p>2) LSTM encoder, vanilla attention, and the proposed model: The performance of the model similar to the vanilla transformer was better than that of LSTM, indicating that the multi-head attention can better extract various features across time steps.</p><p>The comparison between vanilla attention and the proposed model was not repeated. When only the lane decision was kept, the prediction performance of K = 6 was better than that of no decision. This shows that the introduction of agent behavior decision is more helpful for prediction in a single modality, whereas the decision information only contains one positive label. Lane decision is helpful for multi-modal trajectory prediction as there may be multiple lane for attention. The proposed model employs both decisions, which act as a regulator, improving model predictive performance in both uni-and multi-modals simultaneously.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3) No decision, no</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSIONS</head><p>Trajectory prediction and behavioral decision-making are often considered as two separate tasks, although they are closely related. In addition, simple cue connection poses difficulties in exploring the internal relationships between multiple cues. In this study, we proposed the HT joint neural network, which combines multiple cues for trajectory prediction and decision-making.</p><p>The network consists of an encoder, a feature selection network, and a decoder. The proposed network reduces the influence of perceptual noise using three attention mechanisms as well as organically selects and fuses features for decoding. On the Argoverse dataset, the comprehensive performance of the proposed model was better than those of other trajectory prediction models. The comparison and ablation experiment results show that the proposed model can better fuse multiple cues.</p><p>Additionally, HT successfully outputs behavior and lane decisions simultaneously for intelligent vehicle path planning and control. We demonstrated the visualization results of a lane-scaled dot-product attention matrix, proving that lane decision attention adequately mines the multimodality of agents and improves model interpretability. We also found that if there is an obviously large error in the input data of one frame, its trajectory can be smoothed through preprocessing; otherwise, it can be directly eliminated to reduce its impact. Lane decisions contribute to mining the multimodality of agents, and agent behavior decisions are helpful in single modal predictions. In addition, the proposed network has good practical applicability. The </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>. 1 .</head><label>1</label><figDesc>0, 1, 2, ?, Ns, where fa is the number of agent features, and thst is the sequence length. The local coordinate is established by taking the position of the ego-vehicle at t = 0 as the origin. The positive direction of the x-axis is defined as the ego-vehicle driving direction, and the y-axis is the direction in which the x-axis rotates 90? counterclockwise. The feature of the agent at time t is Note that t = -t hst +1, -t hst +2, ?, -1, 0, where , the delta x, y coordinate of the agent of time t and t-1, class is the type of agent, and flag t signifies whether the agent is perceived at time t. Additionally, the position coordinate of each agent at t = 0 is used as input.The scene information set, ? , is divided into a lane vector feature matrix, f M , and a lane vector adjacency matrix set, fl is the number of lane vector features, and Nl is the number of lane vectors. Lane vector basic information of the lane vector, turn l designates whether the lane vector turns left or right, traf l indicates whether the lane vector is restricted due to traffic lights or lane signs, and intersect l indicates that the lane vector is in a junction. The meaning of each matrix in the lane vector adjacency matrix set, a ? , is explained in Table 1 and Figure Pairwise indicates that the relationship appears in pairs with the same attribute, and uniqueness indicates whether the relationship occurs more than once for the lane vector.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>MMMMMM</head><label></label><figDesc>Lane vector upstream of the current lane vector according to direction. 1: 6 represent the first to sixth predecessorsLane vector downstream of the current lane vector according to direction. 1: 6 represent the first to sixth successors. N N r Along the direction of the lane vector and to its right side. N Y l Along the direction of the lane vector and to its left side. N Y m If there are multiple predecessors or successors of the current lane vector, then the lane vector and each predecessor or successor merge with each other. The agent has multiple inflow sources or multiple outflow choices for the lane vector. Y N o If there are other lane vectors except a single direct predecessor or successor within the range in which the lane vector distance is less than tho, the two lane vectors overlap with each other.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 1 .</head><label>1</label><figDesc>Schematic of vectorization feature extraction. For simplicity, only the adjacency relationships of vector 2 and 8 are shown in the figure.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 .</head><label>2</label><figDesc>Schematic of holistic transformer network framework. The network is mainly composed of the Encoder, Feature Selection Network, and Decoder. The input to the Encoder is shown by Input Representation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>. For each sequence step i, V is the same, and all uncertainties contained in the current sequence step are reflected in the probability density function ( | ) j i p k q . Thus, the smaller the uncertainty of ( | ) j i p k q , the better is the outcome. However, we cannot assert what the probability distribution of ( | ) j i p k q should be, nor should we assume what its probability distribution should be, as this is what</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>as the query in the sparse attention.Therefore, the full attention of Eq. (</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>LaneConv and linear layer. After Ng LaneGCN blocks, the static lane vector features, out SL X , are finally encoded. The static lane vector features, out SL X , describe the topological structure of the lane of interest, but it lacks the dynamic scene information of the agent on the scene. Dynamic scene information refers to the additional properties generated when an agent uses the scene. There are also differences in the maneuvers made by the agent in response to road blockages, non-right-of-lane-owner intrusion scenarios, and ordinary scenarios. Therefore, dynamic lane vector features are very important for agent trajectory prediction and decision-making. To extract the dynamic lane vector features, we use the static lane line features, out SL X , as the basis and integrate the information of each agent into the lane vectors through the feature selection sparse attention mechanism. Feature selection sparse attention is described in the next section. Then, the information of each lane line is transmitted again through LaneGCN so that the lane line has traffic flow and agent lane occupancy information. After Ng LaneGCN blocks, the dynamic lane vector features,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>X</head><label></label><figDesc>is recorded as the output of feature selection sparse attention. Then, after the feedforward layer (Eq. (18)), the fused output, , out b c X , is the base feature fused to the context feature.Through the aforementioned operations, the base selects features in the context of interest for feature fusion, which reduces the learning difficulty and enables the organic fusion of multiple cues. In the feature selection network, the base of the lane-feature selection sparse attention refers to agents, and the context refers to lane vectors. This attention is used to establish the interaction between the agent and the surrounding environment. The base of the agent feature selection sparse attention is the target agent, and the context is other agents. This attention is used to establish the interaction between different agents. Additionally, the outputs of the above two interaction feature selection sparse attentions, , att b c X , are taken out for subsequent agent-maneuver decision-making.After the interaction is established, to fully use the posterior knowledge of the lane decision, a lane-scaled dot-product attention is adopted. Notably, as there may be multiple or no lane lines in the attention layer, a sigmoid function is used instead of Eq. (3) for activation:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>sigmoid is taken out for subsequent lane decisions. After the attention, the agent features, out fuse X , are combined with multi-cues are used for decoding.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>LiDAR and cameras at a frequency of 10 Hz. The dataset was split into training, validation, and testing sets with 205,942, 39,472, and 78,143 sequences, respectively. The future trajectory of the test set was not provided, and there was no geographic overlap between sets. A 5-s period was selected to describe the trajectory of each vehicle: 2 s were used as historical input, and 3 s were used as the trajectory to be predicted.All experiments were performed on an Intel Core(R) i9-10920x CPU @ 3.50 GHz (Turbo 4.60 GHz), NVIDIA GeForce(R) RTX 3090 24-GB GPU with 32-GB of RAM running the Ubuntu 16.04 LTS edition. All program tasks were conducted on Python 3.7, and the deep learning framework was based on PyTorch.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 3 .</head><label>3</label><figDesc>Prediction instances from the bird's-eye view of Argoverse validation dataset. For simplicity, only the k * -th trajectory is displayed. The blue dot is the agent, the green dotted line is the historical trajectory of the agent, the red solid line is the predicted trajectory, and the yellow dotted line is the ground truth. The characters next to the blue dots represent the behavior decision classification results of the agent.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 3</head><label>3</label><figDesc>shows prediction instances from the bird's-eye view of the Argoverse validation dataset. It can be seen that the proposed model output accurate prediction results for all agents in and out of junction. InFigure 3(e), there were multiple agents entering the junction and choosing to turn left. The model in this case output accurate prediction results, and "N" indicates that the agent's decision was a lane-change. In the leftmost lane, the agent behind the lane-changing agent had not yet entered the junction, so the decision information output, "F," indicated following the agent in front. The decision-making output of the agent in the second left lane on the left was "I," indicating that the agent preferred not to change lanes.Figure 3 (f)shows a T-shaped junction scenario. Although the agent at the upper left in the figure did not enter the junction, the decision information output by the model was "N," indicating that it was about to change lanes. This shows that the model captured the interaction between the agent and the surrounding environment and output accurate and reasonable trajectory predictions and behavior decision-making results.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 4 .</head><label>4</label><figDesc>Multi-modal prediction instances from the bird's-eye view of Argoverse validation dataset. For simplicity, prediction results are shown for only one agent (blue dots). The green dots are other agents, the yellow dotted line is the ground truth, and the other six lanes are the predicted trajectories.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 4</head><label>4</label><figDesc>shows multi-modal prediction instances from the bird's-eye view of the Argoverse validation dataset.Figure 4 (a) and (b) show the possible future trajectories of the agent according to the current road topology and the interactions of surrounding agents. For example, in Figure 4 (a), the agent was located in the rightmost lane, and the model output not only four straight-ahead prediction trajectories, but it also supplied two right-turn prediction trajectories. When the agent in Figure 4 (b) drove out of the junction, the model provided a variety of possible trajectories, and the end points were located in two different lanes. Such prediction results reflect the model's inferencing power of agent motion under existing conditions, and it fully considered the results of the interaction of road topologies and agents.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 5 1 ) 2 )</head><label>512</label><figDesc>each agent corresponded to an attention weight for all lane lines with a value range of [0,1]. The value indicates the attention of the model to the target lane segment at the current time. To show the best interpretable results consistent with human intuition, shows the decision posterior knowledge attention results. From these, the following conclusions can be drawn: Figure 5 (a) and (b) are lane-changing cases in the out-of-junction scenario of the target agent. In Figure 5 (a), the target agent was about to change lanes to surpass the agent in front. The lane in front of the target agent and the one in the front-left of the target agent had high attention weights. In Figure 5 (b), the target agent was about to complete a lane change, and its lane attention had a high weight directly only to the target lane line. This is similar to the driver's attention focusing on the lane line during a lane change. In such a case, the driver will always observe the current and target lanes before lane changing. Then, the driver decides to change lanes. When the target agent crosses the lane line, the attention of the original lane decreases gradually, and the attention is finally completely focused on the target lane. Figure 5 (c), (d), (e), and (f) show the scenes of the agent in the junction.Figure 5 (c), (d), and (e) show agents about to enter the junction, which are divided into combinations of four and two modals based on the predicted trajectories, focusing on different lane lines. Notably, the model captured the attention of the target agent across multiple lane lines, and it output two feasible lane decisions: straight and turning. Notably, the output of the model was not the final decision information. The final decision must be limited by global routing, traffic regulations, and other information, which are not discussed in this study.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Figure 5 (</head><label>5</label><figDesc>f) shows that the agent determined the proper lane line and was about to leave the junction, focusing on the target lane, which is similar to the lane change maneuver.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Figure 5 .</head><label>5</label><figDesc>Schematic of the decision posterior knowledge attention visualization. The gray dot is the target agent, the gray dotted line is the ground truth trajectory, the black solid line is the predicted trajectory of K=6 modals, and the green dot is the surrounding agent. The other solid lines are the lane center lines. When the line is red, the lane line attention is the largest, and when it is blue, the lane line attention is the smallest.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Figure 6 .</head><label>6</label><figDesc>Schematic of trajectory prediction performance degradations of the two comparison models under the influence of noise. In the figure, minFDE with modal K = 6 is selected as the example, and the results of the baseline model are compared. Figures (a) and (b) show the minFDE increment, and (c) and (d) show the ratio of minFDE increment.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>??X</head><label></label><figDesc>Vanilla Attention: The sparse multi-head attention in agent trajectory encoding was replaced with the vanilla multi-head attention. No decision: All decision branches were replaced. ? No att lane X : Lane decision branches were replaced. : Agent behavior decision branches were replaced.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head>X</head><label></label><figDesc>, and the proposed model: Prediction performance of the proposed model was not optimal when K = 1. When the lane attention was removed and only the agent behavior decisions were kept, the model predicted the best performance when K = 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_21"><head></head><label></label><figDesc>proposed model can output the trajectories of multiple modals and their probabilities, thus providing a more accurate solution space for subsequent motion planning. At the same time, the proposed model can output behavioral decisions, reduce computational overheads, and alleviate the shortage of computing resources. Although the method proposed in this study performs well, there are some limitations. Presently, HT predictions are limited to 3 s. Longer-term trajectories should also be predicted. There is also an imbalance in the driving maneuver classes; the system lacks adaptive weights to compensate for each unbalanced loss function. Future research should be conducted on longer trajectory prediction tasks, and the results should be compared to the current ones. The class imbalanced loss function and its adaptive loss form should also be more closely examined. FUNDING DECLARATION OF COMPETING INTEREST CREDIT AUTHORSHIP CONTRIBUTION STATEMENT</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Holistic Transformer: A Joint Neural Network for Trajectory Prediction and Decision-Making of Autonomous Vehicles Hongyu Hu a , Qi Wang a , Zhengguang Zhang a , Zhengyi Li a and Zhenhai Gao a * a State Key Laboratory of Automotive Simulation and Control, Jilin University, Changchun, 130022 China. (e-mail: huhongyu@jlu.edu.cn, wqi19@mails.jlu.edu.cn, zgzhang21@mails.jlu.edu.cn, lizy21@mails.jlu.edu.cn, gaozh@jlu.edu.cn)</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Ns is the number of perceived agents.</figDesc><table><row><cell>2 { , ,..., } Ns A A A , where</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2</head><label>2</label><figDesc>Meanings of behavioral decisions Owing to the limitations of the surrounding agents, obstacles, global routing, and other factors, the heading angle changes greatly.</figDesc><table><row><cell>Decision</cell><cell>Symbol</cell><cell>Priority</cell><cell>Meaning</cell></row><row><cell>Yield + Stop</cell><cell>S</cell><cell>1</cell><cell>Stopped owing to the limitations of surrounding agents, traffic lights, etc.</cell></row><row><cell>Yield + Nudge / Lane change</cell><cell>N</cell><cell>2</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 3</head><label>3</label><figDesc>Metrics of each model in the 3 s prediction horizon on the Argoverse test set</figDesc><table><row><cell>Model</cell><cell>minADE</cell><cell>K=6</cell><cell>minFDE</cell><cell>minADE</cell><cell>K =1</cell><cell>minFDE</cell></row><row><cell>NN (baseline) [36]</cell><cell>1.713</cell><cell></cell><cell>3.287</cell><cell>3.455</cell><cell></cell><cell>7.883</cell></row><row><cell>VectorNet [12]</cell><cell>3.110</cell><cell></cell><cell>6.723</cell><cell>3.110</cell><cell></cell><cell>6.723</cell></row><row><cell>TNT [28]</cell><cell>0.910</cell><cell></cell><cell>1.446</cell><cell>2.174</cell><cell></cell><cell>4.959</cell></row><row><cell>HOME [39]</cell><cell>0.890</cell><cell></cell><cell>1.292</cell><cell>1.699</cell><cell></cell><cell>3.681</cell></row><row><cell>DenseTNT [37]</cell><cell>0.882</cell><cell></cell><cell>1.282</cell><cell>1.679</cell><cell></cell><cell>3.632</cell></row><row><cell>LaneGCN [13]</cell><cell>0.878</cell><cell></cell><cell>1.355</cell><cell>1.702</cell><cell></cell><cell>3.764</cell></row><row><cell>Multi-modal Transformer [38]</cell><cell>0.837</cell><cell></cell><cell>1.291</cell><cell>1.735</cell><cell></cell><cell>3.901</cell></row><row><cell>TPCN [30]</cell><cell>0.815</cell><cell></cell><cell>1.244</cell><cell>1.575</cell><cell></cell><cell>3.487</cell></row><row><cell>Scene Transformer [40]</cell><cell>0.803</cell><cell></cell><cell>1.232</cell><cell>1.811</cell><cell></cell><cell>4.055</cell></row><row><cell>HT (proposed)</cell><cell>0.811</cell><cell></cell><cell>1.223</cell><cell>1.571</cell><cell></cell><cell>3.435</cell></row><row><cell>(a)</cell><cell></cell><cell>(b)</cell><cell></cell><cell></cell><cell></cell><cell>(c)</cell></row><row><cell>(d)</cell><cell></cell><cell>(e)</cell><cell></cell><cell></cell><cell></cell><cell>(f)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 4</head><label>4</label><figDesc>Classification performance of each maneuver decision for the Argoverse validation dataset</figDesc><table><row><cell>Item</cell><cell></cell><cell>Precision (%)</cell><cell>Recall (%)</cell><cell>F1-Score (%)</cell></row><row><cell></cell><cell>S</cell><cell>88.07</cell><cell>80.76</cell><cell>84.26</cell></row><row><cell>Maneuver Decision</cell><cell>N D F</cell><cell>62.77 55.96 89.92</cell><cell>77.02 71.22 78.09</cell><cell>69.17 62.67 83.59</cell></row><row><cell></cell><cell>I</cell><cell>63.88</cell><cell>82.25</cell><cell>71.91</cell></row><row><cell cols="2">Yield Decision</cell><cell>86.05</cell><cell>90.01</cell><cell>87.99</cell></row><row><cell cols="2">Lane Decision</cell><cell>82.76</cell><cell>68.55</cell><cell>74.99</cell></row><row><cell cols="3">Maneuver Decision Accuracy (%)</cell><cell>78.57</cell><cell></cell></row><row><cell cols="3">Lane Decision Accuracy (%)</cell><cell>99.62</cell><cell></cell></row><row><cell cols="3">Case-Level Lane Decision Recall (%)</cell><cell>89.98</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 5</head><label>5</label><figDesc>Comparison of mean and variance values of prediction results under the influence of different noise</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Mean</cell><cell></cell><cell></cell><cell cols="2">Variance</cell><cell></cell></row><row><cell>Method</cell><cell>Item</cell><cell></cell><cell>K=1</cell><cell></cell><cell>K=6</cell><cell></cell><cell>K=1</cell><cell></cell><cell>K=6</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>minADE</cell><cell>minFDE</cell><cell>minADE</cell><cell>minFDE</cell><cell>minADE</cell><cell>minFDE</cell><cell>minADE</cell><cell>minFDE</cell></row><row><cell></cell><cell>Baseline</cell><cell></cell><cell>1.262</cell><cell>2.722</cell><cell>0.636</cell><cell>0.957</cell><cell>3.488</cell><cell>10.117</cell><cell>0.630</cell><cell>1.301</cell></row><row><cell></cell><cell></cell><cell>0.01</cell><cell>1.289</cell><cell>2.775</cell><cell>0.644</cell><cell>0.967</cell><cell>3.591</cell><cell>10.361</cell><cell>0.637</cell><cell>1.301</cell></row><row><cell></cell><cell>Case1:</cell><cell>0.03</cell><cell>1.357</cell><cell>2.911</cell><cell>0.661</cell><cell>0.996</cell><cell>3.953</cell><cell>11.262</cell><cell>0.671</cell><cell>1.389</cell></row><row><cell>Sparse Multi-</cell><cell>po</cell><cell>0.05</cell><cell>1.431</cell><cell>3.053</cell><cell>0.678</cell><cell>1.024</cell><cell>4.248</cell><cell>11.962</cell><cell>0.687</cell><cell>1.428</cell></row><row><cell>head</cell><cell></cell><cell>0.08</cell><cell>1.556</cell><cell>3.305</cell><cell>0.705</cell><cell>1.064</cell><cell>4.837</cell><cell>13.281</cell><cell>0.743</cell><cell>1.502</cell></row><row><cell>Attention</cell><cell></cell><cell>0.01</cell><cell>1.302</cell><cell>2.798</cell><cell>0.647</cell><cell>0.971</cell><cell>3.688</cell><cell>10.626</cell><cell>0.643</cell><cell>1.319</cell></row><row><cell></cell><cell>Case2:</cell><cell>0.03</cell><cell>1.391</cell><cell>2.965</cell><cell>0.668</cell><cell>1.001</cell><cell>4.092</cell><cell>11.565</cell><cell>0.683</cell><cell>1.408</cell></row><row><cell></cell><cell>pn</cell><cell>0.05</cell><cell>1.501</cell><cell>3.181</cell><cell>0.691</cell><cell>1.038</cell><cell>4.652</cell><cell>12.904</cell><cell>0.726</cell><cell>1.483</cell></row><row><cell></cell><cell></cell><cell>0.08</cell><cell>1.706</cell><cell>3.572</cell><cell>0.733</cell><cell>1.104</cell><cell>5.665</cell><cell>14.628</cell><cell>0.811</cell><cell>1.646</cell></row><row><cell></cell><cell>Baseline</cell><cell></cell><cell>1.429</cell><cell>3.020</cell><cell>0.716</cell><cell>1.011</cell><cell>3.939</cell><cell>11.228</cell><cell>0.721</cell><cell>1.485</cell></row><row><cell></cell><cell></cell><cell>0.01</cell><cell>1.474</cell><cell>3.104</cell><cell>0.727</cell><cell>1.028</cell><cell>4.111</cell><cell>11.621</cell><cell>0.737</cell><cell>1.512</cell></row><row><cell>Vanilla Multi-</cell><cell>Case1: po</cell><cell>0.03 0.05</cell><cell>1.573 1.667</cell><cell>3.293 3.467</cell><cell>0.749 0.770</cell><cell>1.060 1.097</cell><cell>4.604 5.027</cell><cell>12.792 13.658</cell><cell>0.782 0.818</cell><cell>1.614 1.675</cell></row><row><cell>head</cell><cell></cell><cell>0.08</cell><cell>1.811</cell><cell>3.746</cell><cell>0.802</cell><cell>1.148</cell><cell>5.715</cell><cell>15.202</cell><cell>0.927</cell><cell>1.956</cell></row><row><cell>Attention</cell><cell></cell><cell>0.01</cell><cell>1.486</cell><cell>3.126</cell><cell>0.729</cell><cell>1.030</cell><cell>4.197</cell><cell>11.831</cell><cell>0.746</cell><cell>1.539</cell></row><row><cell></cell><cell>Case2:</cell><cell>0.03</cell><cell>1.601</cell><cell>3.345</cell><cell>0.756</cell><cell>1.073</cell><cell>4.770</cell><cell>13.143</cell><cell>0.819</cell><cell>1.699</cell></row><row><cell></cell><cell>pn</cell><cell>0.05</cell><cell>1.739</cell><cell>3.607</cell><cell>0.788</cell><cell>1.124</cell><cell>5.462</cell><cell>14.670</cell><cell>0.896</cell><cell>1.866</cell></row><row><cell></cell><cell></cell><cell>0.08</cell><cell>1.933</cell><cell>3.965</cell><cell>0.837</cell><cell>1.203</cell><cell>6.338</cell><cell>16.491</cell><cell>1.043</cell><cell>2.240</cell></row><row><cell></cell><cell>(a)</cell><cell></cell><cell>(b)</cell><cell></cell><cell></cell><cell>(c)</cell><cell></cell><cell></cell><cell>(d)</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Metrics No feature selection Full feature selection LSTM Encoder Vanilla Attention No decision No att lane X No</head><label></label><figDesc>No feature selection vs. full feature selection: Without feature selection, only the interactions between agents and between</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>X</cell><cell>, att b c</cell><cell>HT (proposed)</cell></row><row><cell>K=1</cell><cell>minADE minFDE</cell><cell>1.623 3.622</cell><cell>1.377 2.977</cell><cell>1.435 3.061</cell><cell>1.429 3.020</cell><cell>1.250 2.691</cell><cell>1.236 2.669</cell><cell cols="2">1.289 2.727</cell><cell>1.262 2.722</cell></row><row><cell>K=6</cell><cell>minADE minFDE</cell><cell>0.762 1.193</cell><cell>0.732 1.089</cell><cell>0.721 1.027</cell><cell>0.716 1.011</cell><cell>0.668 0.990</cell><cell>0.658 0.976</cell><cell cols="2">0.659 0.972</cell><cell>0.636 0.957</cell></row><row><cell>1)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Self-driving cars: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Badue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Guidolini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">V</forename><surname>Carneiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert Syst. Appl</title>
		<imprint>
			<biblScope unit="volume">165</biblScope>
			<biblScope unit="page">113816</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Relational recurrent neural networks for vehicle trajectory prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Messaoud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Yahiaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">V</forename><surname>Blondet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Nashashibi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Intelligent Transportation Systems (ITSC)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1813" to="1818" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Driver lane change intention inference for intelligent vehicles: framework, survey, and challenges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Veh. Technol</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="page" from="4377" to="4390" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">An Intelligent Lane-Changing Behavior Prediction and Decision-Making Strategy for an Autonomous Vehicle</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Qie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Ind. Electron</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="page" from="2927" to="2937" />
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A Survey on Multi-Task Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Knowl. Data Eng</title>
		<imprint>
			<biblScope unit="page" from="1" to="20" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Multi-Task Learning for Dense Prediction Tasks: A Survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vandenhende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Georgoulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">V</forename><surname>Gansbeke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="page" from="1" to="20" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">How would surround vehicles move? A unified framework for maneuver classification and motion prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Deo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rangesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Trivedi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Intell. Vehicles</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="129" to="140" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Holistic LSTM for Pedestrian Trajectory Prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="3229" to="3239" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Vehicle trajectory prediction based on motion model and maneuver recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Houenou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bonnifait</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Cherfaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">F</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/RSJ International Conference on Intelligent Robots and Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="4363" to="4369" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">TrafficPredict: Trajectory Prediction for Heterogeneous Traffic-Agents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="6120" to="6127" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">CoverNet: Multimodal Behavior Prediction using Trajectory Sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">M</forename><surname>Tung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">G</forename><surname>Elena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B</forename><surname>Freddy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="14074" to="14083" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">VectorNet: Encoding HD Maps and Agent Dynamics From Vectorized Representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="11522" to="11530" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning Lane Graph Representations for Motion Forecasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="541" to="556" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Comparison and evaluation of advanced motion models for vehicle tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Schubert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Richter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wanielik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">11th International Conference on Information Fusion</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Vehicle Trajectory Prediction by Integrating Physics-and Maneuver-Based Approaches Using Interactive Multiple Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Qian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Ind. Electron</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="page" from="5999" to="6008" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Probabilistic analysis of dynamic scenes and collision risks assessment to improve driving safety</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Laugier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">E</forename><surname>Paromtchik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Perrollaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yoder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Intell. Trans. Syst. Mag</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="4" to="19" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Predictive maneuver evaluation for enhancement of Car-to-X mobility data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Firl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Stubing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Huss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Stiller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Intelligent Vehicles Symposium</title>
		<meeting>IEEE Intelligent Vehicles Symposium</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="558" to="564" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Maneuver recognition using probabilistic finite-state machines and fuzzy logic</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hulnhagen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Dengler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tamke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Dang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Breuel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Intelligent Vehicles Symposium</title>
		<meeting>IEEE Intelligent Vehicles Symposium</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="65" to="70" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A Multiple Attribute-based Decision Making model for autonomous vehicle in urban environment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Intelligent Vehicles Symposium</title>
		<meeting>IEEE Intelligent Vehicles Symposium</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="480" to="485" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A Robust Design of Hybrid Fuzzy Controller with Fuzzy Decision Tree for Autonomous Intelligent Parking System</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of American Control Conference</title>
		<meeting>American Control Conference</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="5282" to="5287" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Convolutional social pooling for vehicle trajectory prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Deo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Trivedi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition workshops</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1549" to="1557" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">SCALE-Net: Scalable Vehicle Trajectory Prediction Network under Random Number of Interacting Vehicles via Edge-enhanced Graph Convolutional Neural Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jeon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2095" to="2102" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Imitating driver behavior with generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kuefler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Morton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wheeler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kochenderfer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Intelligent Vehicles Symposium</title>
		<meeting>IEEE Intelligent Vehicles Symposium</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="204" to="211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Interaction-Aware Trajectory Prediction of Connected Vehicles using CNN-LSTM Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lv</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 46th Annual Conference of the IEEE Industrial Electronics Society</title>
		<meeting>The 46th Annual Conference of the IEEE Industrial Electronics Society</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5057" to="5062" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">TPNet: Trajectory Proposal Network for Motion Prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6797" to="6806" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Multi-Agent Tensor Fusion for Contextual Trajectory Prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Monfort</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="12118" to="12126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">CoverNet: Multimodal Behavior Prediction using Trajectory Sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">M</forename><surname>Tung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">G</forename><surname>Elena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B</forename><surname>Freddy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="14074" to="14083" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.08294</idno>
		<title level="m">TNT: Target-driveN Trajectory Prediction</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lanercnn</forename></persName>
		</author>
		<idno type="arXiv">arXiv:2101.06653</idno>
		<title level="m">Distributed Representations for Graph-Centric Motion Forecasting</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">TPCN: Temporal Point Cloud Networks for Motion Forecasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="11318" to="11327" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A Survey of Deep Learning Applications to Autonomous Vehicle Control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kuutti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bowden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Barber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fallah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Intell. Transport. Syst</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="712" to="733" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deep Reinforcement Learning for Autonomous Driving: A Survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">R</forename><surname>Kiran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sobh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Talpaert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Intell. Transport. Syst</title>
		<imprint>
			<biblScope unit="page" from="1" to="18" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Multistage attention network for multivariate time series prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">383</biblScope>
			<biblScope unit="page" from="122" to="137" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Informer: Beyond Efficient Transformer for Long Sequence Time-Series Forecasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="11106" to="11115" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Attention Is All You Need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Neural Information Processing Systems (NIPS)</title>
		<meeting>the Conference on Neural Information Processing Systems (NIPS)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Argoverse: 3D Tracking and Forecasting With Rich Maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lambert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sangkloy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8740" to="8749" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">DenseTNT: End-to-end Trajectory Prediction from Dense Goal Sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="15303" to="15312" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Multi-modal Motion Prediction with Transformer-based Neural Network for Autonomous Driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lv</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.06446</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">HOME: Heatmap Output for future Motion Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gilles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sabatini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tsishkou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Intelligent Transportation Systems Conference (ITSC)</title>
		<meeting>the IEEE International Intelligent Transportation Systems Conference (ITSC)</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="500" to="507" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ngiam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Caine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.08417v1</idno>
		<title level="m">Scene Transformer: A unified multi-task model for behavior prediction and planning</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
