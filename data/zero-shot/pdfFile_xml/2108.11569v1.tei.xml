<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ROBUST LONG-TAILED LEARNING UNDER LABEL NOISE A PREPRINT</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021-08-27">August 27, 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Wei</surname></persName>
							<email>weit@lamda.nju.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Nanjing University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang-Xin</forename><surname>Shi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Nanjing University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Wei</forename><surname>Tu</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">4Paradigm Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Feng</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Nanjing University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">ROBUST LONG-TAILED LEARNING UNDER LABEL NOISE A PREPRINT</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-08-27">August 27, 2021</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T16:40+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Long-tailed learning has attracted much attention recently, with the goal of improving generalisation for tail classes. Most existing works use supervised learning without considering the prevailing noise in the training dataset. To move long-tailed learning towards more realistic scenarios, this work investigates the label noise problem under long-tailed label distribution. We first observe the negative impact of noisy labels on the performance of existing methods, revealing the intrinsic challenges of this problem. As the most commonly used approach to cope with noisy labels in previous literature, we then find that the small-loss trick fails under long-tailed label distribution. The reason is that deep neural networks cannot distinguish correctly-labeled and mislabeled examples on tail classes. To overcome this limitation, we establish a new prototypical noise detection method by designing a distance-based metric that is resistant to label noise. Based on the above findings, we propose a robust framework, ROLT, that realizes noise detection for long-tailed learning, followed by soft pseudo-labeling via both label smoothing and diverse label guessing. Moreover, our framework can naturally leverage semi-supervised learning algorithms to further improve the generalisation. Extensive experiments on benchmark and real-world datasets demonstrate the superiority of our methods over existing baselines. In particular, our method outperforms DivideMix by 3% in test accuracy. Source code will be released soon. * equal contribution</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Classification problems in real-world typically exhibit a long-tailed label distribution, where most classes are associated with only a few examples, e.g., visual recognition <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3]</ref>, instance segmentation <ref type="bibr" target="#b3">[4]</ref>, and text categorization <ref type="bibr" target="#b4">[5]</ref>. Due to the paucity of training examples, generalisation for tail classes is challenging; moreover, na?ve learning on such data is susceptible to an undesirable bias towards head classes. Recently, long-tailed learning (LTL) has gained renewed interest in the context of deep neural networks <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13]</ref>. Two active strands of work involve normalisation of the classifier's weights, and modification of the underlying loss to account for different class penalties. Each of these strands is intuitive, and has been empirically shown to be effective <ref type="bibr" target="#b13">[14]</ref>.</p><p>The above-mentioned LTL methods with remarkable performance are mostly trained on clean datasets with high-quality human annotations. However, in real-world machine learning applications, annotating a large-scale dataset is costly and time-consuming. Some recent works resort to the large amount of web data as a source of supervision for training deep neural networks <ref type="bibr" target="#b14">[15]</ref>. While the existing works have shown advantages in various applications <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17]</ref>, web data is naturally class-imbalanced <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19]</ref> and accompanied with label noise <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b21">22]</ref>. As a result, it is crucial that deep neural networks can harvest noisy and class-imbalanced training data.</p><p>Although the LTL and noisy label problems have been extensively studied in previous literature, it is still poorly explored when the training dataset follows a long-tailed label distribution while contains label noise. We provide a simple visualization of the studied problem in <ref type="figure" target="#fig_0">Figure 1a</ref>. Without considering label noise, we show that LTL methods severely degrade their performance in experiments. To address this problem, a direct approach is to apply methods for learning with noisy labels to LTL. One of the most commonly used approaches for learning with noisy labels is DivideMix <ref type="bibr" target="#b17">[18]</ref>, which uses the small-loss criterion to detect label noise. However, we note that using such approach leads to unsatisfactory results in long-tailed label distribution, as shown in <ref type="figure" target="#fig_0">Figure 1b</ref> and 1c. Therefore, it remains a challenge to obtain models that can cope with LTL under label noise.</p><p>To achieve performance improvement, it is a natural idea to detect noisy data while accommodating class imbalance. It is known that a classifier trained on long-tailed data yields higher accuracy for head classes but hurts tail classes <ref type="bibr" target="#b8">[9]</ref>. Thus, to detect label noise, it is not trustworthy to use predictions and training losses produced by the biased classifier. Another commonly used approach for LTL is the nearest class mean (NCM) classifier that computes class prototypes and performs nearest neighbour search in embedding space <ref type="bibr" target="#b8">[9]</ref>. In <ref type="figure" target="#fig_0">Figure 1d</ref>, we test NCM by disentangling the representation and classifier learning. We first train a feature extractor, then compute class prototypes by polluting clean labels with different noise level. It depicts that the estimation of class prototypes is robust to label noise. This observation motivates us to explore the geometric information between examples and their class prototypes as a criterion for noise detection. After acquiring the prototypes, we design a class-independent noise detector by treating examples closed to their corresponding prototypes as clean, while others as noisy. Unlike learning from balanced datasets where noisy data can be removed from training <ref type="bibr" target="#b22">[23]</ref>, we claim that each example is significant, especially for tail classes. To this end, we introduce a new soft pseudo-labeling mechanism that uses both label smoothing and label guessing to guide the learning of networks. Thanks to the generality of our proposed noise detection method, we can also interpret noisy examples as unlabeled data and incorporate well-established semi-supervised learning techniques to further improve the generalisation.</p><p>Our main contributions are: (i) We study the problem of long-tailed learning under label noise, which is less explored and is a significant step towards real-world applications; (ii) We find that the commonly used small-loss trick fails in long-tailed learning. Thus, we establish a novel prototypical noise detection method that overcomes the limitations of small-loss trick; (iii) We propose a robust framework, ROLT. It realizes noise detection that is immune to label distribution, and compensates the problem of data scarcity for tail classes. Our framework can be built on top of semi-supervised learning methods without much extra overhead, leading to an improved approach ROLT+. The proposed methods achieve strong empirical performance on benchmark and real-world datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Our work is closely related to the following directions.</p><p>Long-Tailed Learning. Recently, many approaches have been proposed to cope with long-tailed learning. Most extant approaches can be categorized into three types by modifying (i) the inputs to a model by re-balancing the training data <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b24">25]</ref>; (ii) the outputs of a model, for example by post-hoc adjustment of the classifier <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b13">14]</ref>; and (iii) the internals of a model by modifying the loss function <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b28">29]</ref>. Each of the above methods are intuitive, and have shown strong empirical performance. However, these methods assume the training examples are correctly-labeled, which is often difficult to obtain in many real-world applications. Instead, we study a realistic problem to learn from long-tailed data under label noise. Although the presence of label noise in class-imbalanced dataset has also been mentioned in HAR <ref type="bibr" target="#b29">[30]</ref>, they only consider a specialized noise setup. In this work, we provide a more general simulation of label noise, as well as systematic studies for long-tailed learning methods. More importantly, many existing methods can be easily integrated into our framework, leading to noticeable performance improvement.</p><p>Label Noise Detection. Plenty of methods have been proposed to detect noisy examples <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b32">33]</ref>. Many works adopt the small-loss trick, which treats examples with small training losses as correctly-labeled. In particular,  MentorNet <ref type="bibr" target="#b30">[31]</ref> reweights samples with small loss so that noisy samples contribute less to the loss. Co-teaching <ref type="bibr" target="#b31">[32]</ref> trains two networks where each network selects small-loss samples in a mini-batch to train the other. DivideMix <ref type="bibr" target="#b17">[18]</ref> fits a Gaussian mixture model on per-sample loss distribution to divide the training data into clean set and noisy set. In addition, AUM <ref type="bibr" target="#b22">[23]</ref> introduces a margin statistic to identify noisy samples by measuring the average difference between the logit values for a sample's assigned class and its highest non-assigned class. The above methods only consider training datasets that are class-balanced, thus is not applicable for long-tailed label distribution. Recent work <ref type="bibr" target="#b18">[19]</ref> observes the real-world dataset with label noise also has imbalanced number of samples per-class. Nevertheless, they only inspect a particular setup, while we provide a systematic study of learning with noisy labels under various long-tailed scenarios. In contrast to previous works, we propose a class-independent prototypical noise detection method that works well in long-tailed learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Robust Long-Tailed Learning under Label Noise</head><p>In this section, we first introduce the problem setting. Then, we present our method for long-tailed learning under label noise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Problem Formulation</head><p>Given a training dataset D = {x i , y i } N i=1 , where x i is an instance feature vector and y i ? C = [K] = {1, . . . , K} is the class label assigned to it. We assume that training examples (x i , y i ), 1 ? i ? N consists of two types: i) a correctly-labeled example whose assigned label matches the ground-truth label, i.e., y i = y * i , where y * i denotes the ground-truth label of x i , ii) a mislabeled example whose assigned label does not match the ground-truth label, but the input matches one of the classes in C, i.e., y i = y * i and y * i ? C. The setting of long-tailed learning is where the class prior distribution P(y) is highly skewed, so that many tail labels have a very low probability of occurrence. Specifically, we define the imbalance ratio (IR) as ? = max y P(y)/ min y P(y).</p><p>In practice, since the data distribution is not known, Empirical Risk Minimization (ERM) uses the training data to achieve an empirical estimate of the data distribution. Typically, one minimizes the softmax cross-entropy as</p><formula xml:id="formula_0">(y, f (x)) = log y ?[K] e f y (x) ? f y (x) = log 1 + y =y e f y (x)?fy(x) ,<label>(1)</label></formula><p>where f y (x) denotes the predictive probability of model f on class y. This ubiquitous approach neglects the issue of class imbalance, and makes the model biased toward head classes. Moreover, it assumes training examples are correctly-labeled. In the following, we adapt the ERM to address these problems without introducing much extra training efforts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Class-Independent Prototypical Noise Detection</head><p>A popular method for noise detection is the small-loss trick, however, we find that this commonly used method does not fit well with long-tailed learning. The reason is that the training loss of an example can also be large because it belongs to the tail classes and the small-loss trick is not able to distinguish mislabeled examples from tail classes examples, as shown in <ref type="figure" target="#fig_1">Figure 2a</ref> and 2b. In contrast, we find that the estimate of class prototypes is robust to label noise. More importantly, clean examples tend to be clustered around their prototypes even when training with noisy labels. As a comparison with the small-loss trick, we demonstrate the distance distribution for both head and tail classes in <ref type="figure" target="#fig_1">Figure 2c</ref> and 2d, which motivates us to detect noisy data using class prototypes.</p><p>Considering the discrepancy of data distribution of each class, we inspect the distance statistics in a class-independent manner. Formally, we model clean examples of class k ? [K] as if they were distributed around prototype c k ? R D , and the likelihood of an example x belonging to class k decays exponentially with its distance from the prototype c k , i.e.,</p><formula xml:id="formula_1">P(x | c k ) ? e ?dist(c k ,x)</formula><p>, which is a common assumption about the data distribution <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b34">35]</ref>. Here, dist is a distance measure in the embedding space and is typically set to be the Euclidean distance. To separate clean examples from noisy data, we assume that, for training examples of class k, the distance statistics follow a mixture of two Gaussians <ref type="bibr" target="#b35">[36]</ref>, i.e.,</p><formula xml:id="formula_2">d ? 2 j=1 ? j N (? j , ? 2 j ), where d = dist(c k , x)</formula><p>, ?x ? D k and ? j denotes weight of the j-th component. Note that we have 2 j=1 ? j = 1. Without loss of generality, we assume ? 1 &lt; ? 2 . Since clean examples locate around the prototype while noisy examples spread out, we flag x as clean if and only if P(d | ? 1 , ? 1 ) &gt; P(d | ? 2 , ? 2 ). We thus perform class-independent noise detection by estimating the Gaussians' parameters from distance statistics. In particular, for each class k ? [K], we compute its prototype as the normalized average of the embeddings for training examples by</p><formula xml:id="formula_3">c k ? Normalize 1 |D k | xi?D k f ? (x i ) , D k = {x i | y i = k} ,<label>(2)</label></formula><p>where f ? (x) denotes the extracted feature representation of x. Based on c k , the distances between c k and examples of class k are obtained by</p><formula xml:id="formula_4">dist(c k , x i ) = c k ? f ? (x i ) 2 2 , ?x i ? D k .<label>(3)</label></formula><p>We then fit a two-component Gaussian mixture model to maximize the log-likelihood value by max</p><formula xml:id="formula_5">|D k | i=1 log( 2 j=1 ? j P(d i | ? j , ? j )), where d i = dist(c k , x i ) for x i ? D k .</formula><p>For simplicity, we denote the clean (noisy) data of class k as X k (S k ). Note that we have D k = X k S k . Therefore, we obtain a subset of clean examples by X = K k=1 X k and noisy examples by S = K k=1 S k . It is also verified that Gaussian mixture model can be used to distinguish clean and noisy data because of its flexibility in the sharpness of distribution in previous literature <ref type="bibr" target="#b17">[18]</ref>. Recall that D k may contain noisy labels, the estimate of c k in (2) is inaccurate and the split of D k = X k S k is problematic. To remedy this, we refine class prototypes using X k rather than D k , and acquire a new split of D k . By doing this, we believe that the obtained X k retains most of correctly-labeled examples of class k as well as less mislabeled examples. </p><formula xml:id="formula_6">2 for t = 1, . . . , T 0 do 3 Sample m 0 examples {(x i , y i )} m0 i=1 from D 4 w t+1 = w t ? ? 0gt , whereg t = 1 m0 m0 i=1 ? (w t ; x i ) 5 end // Robust Learning Stage: run SGD for T iterations 6 for t = 1, . . . , T do 7 X = ?, S = ? 8 for k = 1, . . . , K do 9</formula><p>Compute class prototype c k as in <ref type="formula" target="#formula_3">(2)</ref> 10 Compute distance between the prototype c k and each of x i ? D k as in (3) <ref type="bibr" target="#b10">11</ref> Fit GMM and divide D k into clean set X k and noisy set S k 12 X = X X k , S = S S k // collect clean and noisy examples of class k <ref type="bibr" target="#b12">13</ref> Refine class prototype c k ? Normalize 1</p><formula xml:id="formula_7">|X k | i?X k f ? (x i )</formula><p>// In practice, class prototype computed from X k is more accurate than that from D k 14 end <ref type="bibr" target="#b14">15</ref> Compute soft pseudo-labels? for x ? S as in (4) <ref type="bibr" target="#b15">16</ref> Compute stochastic gradient g t as g t =</p><formula xml:id="formula_8">|X | i=1 ?H(yi,f (xi))+ |S| j=1 ?H(?j ,f (xj )) |X |+|S| 17</formula><p>Update model parameters using g t and learning rate ? : w t+1 = w t ? ?g t 18 end</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Soft Pseudo-Labeling via Label Smoothing and Label Guessing</head><p>For each noisy example, we aims to refine its training label by generating a soft pseudo-label. A direct approach is to leverage the prediction of ERM model. However, the ERM is known to be biased toward head classes <ref type="bibr" target="#b36">[37]</ref>. Hence, refining noisy labels using the predictions of ERM may be sub-optimal for examples of tail classes. In contrast, the NCM classifier can yield balanced classification boundary <ref type="bibr" target="#b8">[9]</ref>. Specifically, we find that the NCM classifier produces much higher recall on tail classes than the ERM in experiments. By aggregating the predictive information from the ERM and NCM classifiers, we construct diverse soft pseudo-labels for detected noisy examples. To amend the misflag of noisy detector, we also take account of the original labels as a source of soft pseudo-labels. Moreover, since it is not impossible that both ERM and NCM classifiers produce incorrect predictions, we further remedy this by the label smoothing technique <ref type="bibr" target="#b36">[37]</ref>.</p><p>Put together, given the predictions? erm = arg max k f (x),? ncm = arg min k c k ? f ? (x) 2 , and original label y, we form the guessing label set G = {? erm ,? ncm , y} and generate soft pseudo-label? ? R K of x ? S as follows. For class k ? [K], we compute?</p><formula xml:id="formula_9">k = ? ? ? ??G P(? = y * ) ? I(? = k) if k ? G 1? ??G P(?=y * ) K?|G| otherwise.<label>(4)</label></formula><p>Here, I(?) is an indicator which returns 1 if the condition is true, otherwise 0. P(? = y * ) denotes the probability that y matches ground-truth label. The targets? erm and? ncm can be set equal to the model output, but using a running average is more effective which is known as temporal ensembling <ref type="bibr" target="#b37">[38]</ref> in semi-supervised learning. For ERM or NCM classifier, let z i (t) ? R K be the output logits vector (pre-softmax output) for example x i at iteration t of training, we update the momentum logits by</p><formula xml:id="formula_10">q i (t) = ?q i (t ? 1) + (1 ? ?)z i (t),<label>(5)</label></formula><p>where 0 ? ? &lt; 1 is the combination weight. For each iteration t, we then obtain? erm and? ncm using softmax of q i (t). Having acquired X , S, and soft pseudo-labels, we first compute the cross-entropy loss for clean examples using original training labels by</p><formula xml:id="formula_11">L X = 1 |X | i?X H(y i , f (x i )),<label>(6)</label></formula><p>where y i is the one-hot label vector for x i . For noisy examples, the loss function is computed by</p><formula xml:id="formula_12">L S = 1 |S| i?S H(? i , f (x i )),<label>(7)</label></formula><p>where</p><formula xml:id="formula_13">H(q, p) = ? K k=1 q k log exp p k K j=1</formula><p>exp pj is the cross-entropy between distributions q and p. Overall, the training objective is L = L X + L S . Details of the method are presented in Algorithm 1. Moreover, X and S can be viewed as labeled and unlabeled data respectively, and semi-supervised learning <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b17">18]</ref> can be leveraged to train networks, which is further validated in experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We now present experiments that confirm our main claims: (i) on benchmark datasets, we demonstrate the efficacy of our method by comparing with both methods for long-tailed learning and learning with noisy labels; (ii) on a real-world class-imbalanced noisy dataset, we compare the performance with many existing methods; (iii) we provide detailed studies for each proposed component in our framework and analyze their effectiveness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Simulating Noisy and Long-Tailed Datasets on CIFAR</head><p>Setting. We test ROLT on CIFAR-10 and CIFAR-100 under various imbalanced ratio ? and noise level ?. For each dataset, we first simulate the long-tailed dataset following the same setting as LDAM <ref type="bibr" target="#b6">[7]</ref>. The long-tailed imbalance follows an exponential decay in sample sizes across different classes. We then inject label noise according to the noise transition matrix <ref type="bibr" target="#b7">(8)</ref> to the long-tailed dataset to form the training set. In particular, we consider imbalance ratio to be ? ? {10, 50, 100} and noise level to be ? ? {0, 0.1, 0.2, 0.3, 0.4, 0.5}. Due to space constraints, we defer the results for ? = 0 and ? = 50 to the supplementary material.</p><p>Label Noise Generation. To generate noisy labels, the most basic idea is to utilize the noise transition matrix <ref type="bibr" target="#b39">[40]</ref>, denoting the probabilities that clean labels flip into noisy labels. Let Y denote the variable for the clean label,? the noisy label, and X the instance/feature, the transition matrix T (X = x) is defined as T ij (X) = P(? = j | Y = i, X = x). In this work, we present a new noise generation approach by setting T (X = x) according to the estimated class priors P(y), e.g., the empirical class frequencies in the training dataset. Formally, given the noise proportion ? ? [0, 1], we define</p><formula xml:id="formula_14">T ij (X) = P(? = j | Y = i, X = x) = 1 ? ? i = j Nj N ?Ni ? otherwise.<label>(8)</label></formula><p>Here, N denotes the total number of training examples and N j is frequency of class j. In contrast to commonly used uniform label noise, we believe that examples are more likely to be mislabeled as frequent ones in real-world situations.</p><p>Result. <ref type="table" target="#tab_2">Table 1</ref> and <ref type="table">Table 2</ref> summarize the results for CIFAR-10 and CIFAR-100. We compare our methods with several commonly used baselines for long-tailed learning and learning with noisy labels. As shown in the results, previous methods dreadfully degrade their performance as the noise level and imbalance ratio increase, while our methods retain robust performance. In particular, compared with ERM, ROLT improves the test accuracy by 8% on average. It can be observed that the improvement becomes more significant at high noise levels, benefiting from proposed noise detection and soft pseudo-labeling. Further application of Deferred Re-Weighting (DRW) <ref type="bibr" target="#b6">[7]</ref> enhances the performance by favoring the tail classes. Note that, ERM-DRW achieves even lower accuracy than LDAM <ref type="bibr" target="#b6">[7]</ref> in many cases, while ROLT-DRW outperforms LDAM-DRW by 5% on average. This clearly demonstrates the importance of correcting noisy labels in the training data. Intriguingly, our experiments reveal that NCM <ref type="bibr" target="#b8">[9]</ref>, which is mostly overlooked in previous literature on long-tailed learning, performs better than cRT <ref type="bibr" target="#b8">[9]</ref> in most cases, especially in scenarios with high noise levels. This also provides evidence for us to develop geometry-based noise detection method.</p><p>We further compare ROLT+ with DivideMix <ref type="bibr" target="#b17">[18]</ref>, one of the most popular methods for learning with noisy labels. We use the same experimental setups for these two methods. The results are given in <ref type="table">Table 2</ref>. It can be observed that DivideMix performs worse as the training dataset becomes more class-imbalanced. In contrast, our method ROLT+ achieves an improvement in test accuracy by 3% on average. This validates the superiority of our class-independent prototypical noise detector over the small-loss trick. In the supplementary material, we further show that DivideMix flags most example of tail classes as noisy, which is the main reason accounting for its failure.   <ref type="table">Table 2</ref>: Test accuracy (%) on CIFAR datasets with different imbalanced ratio and noise level.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Evaluation on Real-World Class-Imbalanced and Noisy Dataset</head><p>We test the performance of our method on a real-world dataset. WebVision <ref type="bibr" target="#b14">[15]</ref> contains 2.4 million images collected from Flickr and Google with real noisy and class-imbalanced data. Following previous literature, we train on a subset, mini WebVision, which contains the first 50 classes. In <ref type="table" target="#tab_4">Table 3</ref>, we report results comparing against state-of-the-art approaches, including D2L <ref type="bibr" target="#b16">[17]</ref>, MentorNet <ref type="bibr" target="#b30">[31]</ref>, Co-teaching <ref type="bibr" target="#b31">[32]</ref>, Iterative-CV <ref type="bibr" target="#b40">[41]</ref>, HAR <ref type="bibr" target="#b29">[30]</ref>, and DivideMix <ref type="bibr" target="#b17">[18]</ref>.   <ref type="figure">Figure 3</ref>: Per-class recall of ERM and NCM classifiers on CIFAR-10 and CIFAR-100 datasets. It can be clearly seen that NCM produces more balanced predictions than ERM across classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Further Analysis and Ablation Studies</head><p>We study the effectiveness of the two main modules of our method.</p><p>Efficacy of the noise detector. To further support our motivation, we compare the performance of the ERM and NCM classifiers in <ref type="figure">Figure 3</ref>. It can be seen that NCM produces more balanced recall across classes, while ERM tends to predict examples as head classes, resulting in low recall for tail classes. <ref type="figure" target="#fig_3">Figure 4</ref> shows the precision and recall of selected clean examples by our method. To better understand ROLT, we construct three groups of classes for CIFAR-100 by: many (more than 100 images), medium (20?100 images), and few (less than 20 images) shots; and CIFAR-10 by: many ({0, 1}), medium({2, . . . , 6}), and few ({7, 8, 9}) shots according to class indices. ROLT maintains high precision and recall, which validates the effectiveness of our method. This experiment is conducted under imbalance ratio ? = 100 and noise level ? = 0.3.</p><p>Efficacy of the soft pseudo-labeling. We investigate the effectiveness of soft pseudo-labeling by comparing it with two other methods, i.e., keep the noisy labels or rectify it via the ERM predictions. We report the results in <ref type="table">Table 4</ref> with respect to noise level ? ? {0.2, 0.5} and imbalance ratio ? = 100. We observe that ERM and soft pseudo-labeling significantly improve the performance by over 4% in test accuracy, and the improvement is more significant under high noise levels. Moreover, the soft pseudo-labeling outperforms its ERM counterpart in most cases, demonstrating that label smoothing and label guessing can provide diverse and informative supervision under imperfect training labels. We also investigate the effectiveness of learned representations with NCM for classification. It can be observed that NCM with soft labels outperforms the one using original noisy labels, which confirms that our soft pseudo-labeling facilitates representation learning.  <ref type="table">Table 4</ref>: Ablation studies on pseudo-labeling. Test accuracy on CIFAR-100 is reported.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Discussion and Limitations</head><p>One may be interested in combining the proposed method ROLT with other loss functions. In particular, we attempt to optimize LDAM loss <ref type="bibr" target="#b6">[7]</ref> during training and the results are reported in the supplementary material. Indeed, LDAM encourages the model to yield balanced classification boundaries. However, it slightly distort these boundaries when applied together with soft pseudo-labeling because too much focus has been put on tail classes. Our experimental finding suggests using the ERM predictions as pseudo-labels leading to more significant improvements.</p><p>Additionally, we admit that it is challenging to train networks that consistently performs well under various noise levels in long-tailed learning. Although ROLT can take both label noise and class imbalance into account, its improvement is less obvious in the event of training on a clean dataset. We report the results in the supplementary material due to limited space. This is because that the noise detector inevitably fits a two-component GMM and flags some examples as noisy, leading to loss of accurate supervision. We believe this concern can be alleviated by estimating the noise proportion in training data, which is another interesting research problem, and leave this for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We study the long-tailed learning under label noise and a robust framework is proposed to tackle this challenging problem. We reveal the failure of small-loss trick in long-tailed learning, and establish a prototypical noise detection method that is immune to label distribution. We provide systematic studies on benchmark and real-world datasets to verify the superiority of our methods by comparing to state-of-the-art methods in the strands of long-tailed learning and learning with noisy labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Broader Impact</head><p>This paper introduces a method to learning from noisy and long-tailed data. It can benefit the widespread use of "weakly-labeled" data <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19]</ref>, which are often cheap to acquire but have suffered from data quality issues. The proposed method is simple yet effective, which we believe will broadly benefit practitioners dealing with heavily imbalanced data in realistic applications.</p><p>In this work, we only extensively test our strategies on benchmark datasets. In many real-world applications such as autonomous driving, medical diagnosis, and healthcare, beyond being naturally noisy and imbalanced, the data may impose additional constraints on learning process and final models, e.g., being fair or private. We focus on standard accuracy as our measure and largely ignore other ethical issues in imbalanced data, especially in minor classes. As such, the risk of producing unfair or biased outputs reminds us to carry rigorous validations in critical, high-stakes applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Implementation Details</head><p>We develop our core algorithm in PyTorch.</p><p>Implementation details for CIFAR. We follow the simple data augmentation used in <ref type="bibr" target="#b41">[42]</ref> with only random crop and horizontal flip. For experiments of ROLT, we use ResNet-32 as the backbone network and train it using standard SGD with a momentum of 0.9, a weight decay of 2 ? 10 ?4 , a batch size of 128, and an initial learning rate of 0.1. The model is trained for 200 epochs. We perform noise detection and soft pseudo-labeling after a warm up period of 80 epochs, and anneal the learning rate by a factor of 100 at 160 and 180 epochs. For soft pseudo-labeling, we manually set the prior P(? = y * | x) to be 0.4, 0.2, 0.2 for the ERM, NCM, and original label. For experiments of ROLT+, we use the same settings as <ref type="bibr" target="#b17">[18]</ref>, which trains two 18-layer PreAct Resnet for 300 epochs. We train each model with 1 NVIDIA GeForce RTX 2070.</p><p>Implementation details for mini WebVision. Following previous work <ref type="bibr" target="#b17">[18]</ref>, we use two Inception-Resnet V2 for ROLT+. The model is trained for 100 epochs. We set the initial learning rate as 0.01, and reduce it by a factor of 10 after 50 epochs. The warm up period is 40 epochs. We train each model with 1 NVIDIA Tesla V100 GPUs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Additional Experimental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 Comparison with DivideMix with respect to Noise Detection</head><p>To further demonstrate our proposed noise detection that is tailored for long-tailed learning, we compare it with DivideMix and the results are shown in <ref type="figure">Figure 5</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Comparison with DivideMix on Balanced Datasets</head><p>We compare the performance of our method with DivideMix on balanced datasets with noise level ? ? {0.2, 0.5}. The results are reported in <ref type="table">Table 5</ref> and our method is comparable with DivideMix. This shows that the proposed prototypical noise detector also works well on balanced datasets. <ref type="bibr">CIFAR</ref>  <ref type="table">Table 5</ref>: Test accuracy (%) on class-balanced CIFAR datasets with different noise level.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3 Additional Results on CIFAR Datasets</head><p>We report the results on CIFAR-10 and CIFAR-100 with simulated imbalance ratio ? = 50 with noise level ? ? {0.1, 0.2, 0.3, 0.4, 0.5} in <ref type="table">Table 6</ref>. The performance of comparison methods is in line with that of ? = 10 and ? = 100 which are reported in the main text. This further justifies that our method can adapt to various class-imbalanced and noisy datasets.  <ref type="table">Table 6</ref>: Test accuracy (%) on CIFAR datasets with imbalance ratio ? = 50 and different noise level.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.4 Results on Clean CIFAR Datasets</head><p>Although our method is particularly designed for long-tailed learning with noisy labels, it is interesting to study its performance on clean datasets. We report the results in <ref type="table" target="#tab_9">Table 7</ref>. Intriguingly, ROLT consistently outperforms vanilla ERM in all cases, showing the benefit of the proposed soft pseudo-labeling approach. Additionally, our method achieves comparable performance with the popular baseline LDAM-DRW. In comparison with the HAR-DRW, which is also proposed to cope with class imbalance and label noise problems, our method improves the performance by over 2% on average. This validates the robustness of our method, which does not hurt the performance in the corner case.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.5 Results for Optimizing LDAM Loss</head><p>In the main text, we optimize the cross-entropy loss and report its performance for comparison. One may interested in if other loss functions can be integrated into our framework. To this end, we leverage the LDAM loss, which is particularly designed for long-tailed learning, and report the results in <ref type="table">Table 8</ref>. This indeed produces different results with the cross-entropy. It is known that LDAM can prevent the networks from being biased toward tail classes and yield balanced predictions. Therefore, it is reasonable to use predictions of the ERM for pseudo-labeling. By further applying the soft pseudo-labels, it puts much focus on tail classes and results in performance deterioration.   <ref type="table">Table 8</ref>: Ablation studies on pseudo-labeling based on models that optimize LDAM loss. Test accuracy on CIFAR-100 dataset with imbalance ratio ? = 100 is reported.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.6 The Impact of Label Noise on Representation and Classifier Learning</head><p>In <ref type="table" target="#tab_2">Table 9?12</ref>, we study the impact of label noise for two-stage long-tailed learning methods, i.e., Classifier Re-Training (cRT) and Nearest Classifier Mean (NCM), which disentangle the representation and classifier learning. In this setup, ? r and ? c are the noise level when performing representation and classifier learning, respectively.</p><p>We have the following observations from the results. In particular, when ? c = 0, the performance of both cRT and NCM drop significantly as ? r increases, revealing the negative impact of label noise on representation learning. With respect to classifier learning, it can be seen that cRT further suffers from inaccurate supervision. In contrast, NCM classifier retains high performance as ? c grows. The results validate our finding that NCM is more robust to label noise, which motivates us to investigate distance-based method for noise detection. Moreover, in order to improve the representation learning, one may remove noisy data or rectify noisy labels during training. In this work, we provide two ways of achieving this, by pseudo-labeling using either ERM predictions or soft pseudo-labels. Recall that, NCM computes the classification vectors for each class by taking the mean of all vectors belonging to that class. Thus, the classification accuracy is directly related to the feature representation quality. By observing considerable performance gains for NCM, it shows the effectiveness of our pseudo-labeling method for representation learning.     <ref type="table" target="#tab_2">Table 12</ref>: Accuracy (%) of NCM on CIFAR-100 with different imbalanced ratio ? and noise level ?.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>(a) Illustration of the studied problem setup. (b-c) Comparison of DivideMix and ROLT+ on datasets with imbalance ratio 100 and noise level 0.2. (d) We show the test accuracy of NCM classifier under different noise levels by disentangling representation and classifier learning.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>(a-b) Training losses for examples of head class and tail class, respectively. (c-d) Distance distribution between examples and their class prototype for head class and tail class, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Algorithm 1 :</head><label>1</label><figDesc>Robust Long-Tailed Learning under Label Noise (ROLT) 1 Input: training dataset {(x i , y i ) N i=1 }, initial learning rate ? 0 , number of warm-up iterations T 0 // Warm-up Stage: run SGD for T0 iterations</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Precision and Recall of selected clean examples by our method.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>and 6 .Figure 5 :Figure 6 :</head><label>656</label><figDesc>This experiment is conducted under imbalance ratio ? = 100 and noise level ? = 0.3. We partition classes into three splits, i.e., Many, Medium, and Few-shots, and report the accuracy (recall) of examples that are flagged as clean for each split. It can be observed that DivideMix only flags a small proportion of examples as clean on tail classes, even fewer than those in original data (the green dotted line in figures, about 70%). This also explains that, DivideMix trains networks that are biased towards head classes, thus leading to poor overall performance. In contrast, ROLT+ correctly flags more clean examples of tail classes than DivideMix, demonstrating the superiority of our prototypical noise detection method. Comparison of detection accuracy between ROLT+ and DivideMix on CIFAR-10 dataset. Comparison of detection accuracy between ROLT+ and DivideMix on CIFAR-100 dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>ERM 80.41 75.61 71.94 70.13 63.25 64.41 62.17 52.94 48.11 38.71 ERM-DRW 81.72 77.61 71.94 70.13 63.25 66.74 62.17 52.94 48.11 38.71 LDAM 84.59 82.37 77.48 71.41 60.30 71.46 66.26 58.34 46.64 36.66 LDAM-DRW 85.94 83.73 80.20 74.87 67.93 76.58 72.28 66.68 57.51 43.23 BBN 83.59 80.35 72.94 70.04 63.63 70.05 64.51 56.86 44.30 36.72 cRT 80.22 76.15 74.17 70.05 64.15 61.54 59.92 54.05 50.12 36.73 NCM 82.33 74.73 74.76 68.43 64.82 68.09 66.25 60.91 55.47 42.61 HAR-DRW 84.09 82.43 80.41 77.43 67.39 70.81 67.88 48.59 54.23 42.80 ROLT 85.68 85.43 83.50 80.92 78.96 73.02 71.20 66.53 57.86 48.98 ROLT-DRW 86.24 85.49 84.11 81.99 80.05 76.22 74.92 71.08 63.61 55.06 43.27 37.43 32.94 26.92 31.81 26.21 21.79 17.91 14.23 ERM-DRW 50.38 45.24 39.02 34.78 28.50 34.49 28.67 23.84 19.47 14.76 LDAM 51.77 48.14 43.27 36.66 29.62 34.77 29.70 25.04 19.72 14.19 LDAM-DRW 54.01 50.44 45.11 39.35 32.24 37.24 32.27 27.55 21.22 15.21 BBN 53.50 47.91 42.81 35.17 28.60 34.39 27.84 23.38 18.20 15.47 cRT 49.13 42.56 37.80 32.18 25.55 32.25 26.31 21.48 20.62 16.01 NCM 50.76 45.15 41.31 35.41 29.34 34.89 29.45 24.74 21.84 16.77</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">CIFAR-10</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Imbalance Ratio</cell><cell></cell><cell></cell><cell>10</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>100</cell><cell></cell><cell></cell></row><row><cell>Noise Level</cell><cell>0.1</cell><cell>0.2</cell><cell>0.3</cell><cell>0.4</cell><cell>0.5</cell><cell>0.1</cell><cell>0.2</cell><cell>0.3</cell><cell>0.4</cell><cell>0.5</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">CIFAR-100</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Imbalance Ratio</cell><cell></cell><cell></cell><cell>10</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>100</cell><cell></cell><cell></cell></row><row><cell>Noise Level</cell><cell>0.1</cell><cell>0.2</cell><cell>0.3</cell><cell>0.4</cell><cell>0.5</cell><cell>0.1</cell><cell>0.2</cell><cell>0.3</cell><cell>0.4</cell><cell>0.5</cell></row><row><cell>ERM</cell><cell>48.65</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>HAR-DRW 51.04 46.24 41.23 37.35 31.30 33.21 26.29 22.57 18.98 14.78 ROLT 54.11 51.00 47.42 44.63 38.64 35.21 30.97 27.60 24.73 20.14 ROLT-DRW 55.37 52.41 49.31 46.34 40.88 37.60 32.68 30.22 26.58 21.05</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>DivideMix Best 88.80 77.95 71.54 87.26 69.09 53.96 63.41 48.77 43.34 47.60 34.15 29.92 Last 88.77 77.95 69.90 87.04 68.17 53.25 62.44 48.29 42.80 47.23 33.25 29.59 ROLT+ Best 89.09 77.93 72.75 87.51 75.57 65.72 64.24 49.90 44.39 52.27 38.96 32.50 Last 88.91 76.75 72.16 86.71 74.93 64.65 63.86 48.85 43.98 51.77 37.92 31.89</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="2">CIFAR-10</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">CIFAR-100</cell><cell></cell><cell></cell></row><row><cell>Noise Level</cell><cell></cell><cell>0.2</cell><cell></cell><cell></cell><cell>0.5</cell><cell></cell><cell></cell><cell>0.2</cell><cell></cell><cell></cell><cell>0.5</cell><cell></cell></row><row><cell>Imbalance Ratio</cell><cell>10</cell><cell>50</cell><cell>100</cell><cell>10</cell><cell>50</cell><cell>100</cell><cell>10</cell><cell>50</cell><cell>100</cell><cell>10</cell><cell>50</cell><cell>100</cell></row></table><note>Test accuracy (%) on CIFAR datasets with different imbalanced ratio and noise level.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Accuracy (%) on mini WebVision and ImageNet validation sets.</figDesc><table><row><cell>5HFDOO</cell><cell>5HFDOO</cell><cell>5HFDOO</cell><cell>5HFDOO</cell></row><row><cell>&amp;ODVV,QGH[</cell><cell>&amp;ODVV,QGH[</cell><cell>&amp;ODVV,QGH[</cell><cell>&amp;ODVV,QGH[</cell></row><row><cell>(a) ERM on CIFAR-10</cell><cell>(b) NCM on CIFAR-10</cell><cell>(c) ERM on CIFAR-100</cell><cell>(d) NCM on CIFAR-100</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>34.92 13.07 33.61 31.11 21.05 5.63 20.41 NCM Soft Label 48.03 32.18 14.25 32.55 31.46 21.66 5.59 20.75 Linear Noisy Label 45.82 26.50 10.79 28.67 23.77 14.53 3.41 14.76 Linear ERM 50.62 31.55 11.64 32.46 32.80 17.05 2.30 18.58 Linear Soft Label 49.50 31.11 14.39 32.68 32.14 21.16 6.52 21.05 NCM Noisy Label 43.21 31.95 12.61 30.36 26.86 17.89 5.59 17.71 NCM ERM 43.53 33.21 11.07 30.52 26.83 19.45 5.52 18.27 NCM Soft Label 46.79 31.61 13.79 31.78 29.86 21.05 6.26 20.14</figDesc><table><row><cell cols="2">DRW Classifier Pseudo-Label</cell><cell cols="4">? = 0.2 Many Medium Few All Many Medium Few All ? = 0.5</cell></row><row><cell>Linear</cell><cell cols="2">Noisy Label 49.38 21.42</cell><cell>4.57 26.21 32.06</cell><cell>7.89</cell><cell>0.04 14.23</cell></row><row><cell>Linear</cell><cell>ERM</cell><cell>58.79 26.50</cell><cell cols="3">4.21 31.24 38.83 12.05 0.89 18.41</cell></row><row><cell>Linear</cell><cell>Soft Label</cell><cell>56.79 26.47</cell><cell cols="3">5.71 30.97 39.09 16.13 1.22 20.14</cell></row><row><cell>NCM</cell><cell cols="5">Noisy Label 44.09 32.03 12.00 30.52 26.86 17.89 5.59 17.71</cell></row><row><cell>NCM</cell><cell>ERM</cell><cell>49.06</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>63.60 58.69 55.85 43.38 35.10 30.27 25.11 19.49 16.97 ERM-DRW 71.99 65.76 58.69 55.85 43.38 37.74 32.63 27.19 21.43 17.52 LDAM 75.06 71.34 64.71 54.42 42.95 39.94 33.43 30.01 23.30 17.51 LDAM-DRW 79.01 76.41 71.83 62.22 48.88 42.88 36.60 33.12 25.91 19.48 BBN 72.86 68.01 60.49 52.89 46.22 39.40 36.48 26.89 21.08 16.77 cRT 69.22 65.02 60.64 51.90 43.26 35.70 30.23 24.37 19.90 17.47 NCM 72.37 69.60 65.26 56.78 49.68 38.91 33.49 28.85 23.91 19.01 HAR-DRW 72.12 67.44 60.73 63.04 52.35 38.46 28.86 29.33 22.06 16.75 ROLT 77.49 74.91 72.43 64.37 49.32 38.94 35.75 32.55 27.62 25.01 ROLT-DRW 80.46 78.31 76.36 69.64 54.34 40.85 38.10 34.75 29.24 26.17</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>CIFAR-10</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>CIFAR-100</cell><cell></cell><cell></cell></row><row><cell>Noise Level</cell><cell>0.1</cell><cell>0.2</cell><cell>0.3</cell><cell>0.4</cell><cell>0.5</cell><cell>0.1</cell><cell>0.2</cell><cell>0.3</cell><cell>0.4</cell><cell>0.5</cell></row><row><cell>ERM</cell><cell>69.33</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>77.38 71.83 56.31 44.15 38.88 ERM-DRW 87.71 80.58 76.33 57.68 46.71 41.90 LDAM 86.38 77.62 74.31 55.66 43.61 39.25 LDAM-DRW 87.29 81.25 78.78 57.21 47.30 42.93 BBN 87.83 81.19 78.87 58.08 45.62 40.09 cRT 86.78 77.30 71.18 56.62 43.01 39.44 NCM 88.14 82.75 79.59 56.05 45.13 41.73 HAR-DRW 87.81 79.82 75.99 56.89 43.34 40.78 ROLT 87.87 79.79 76.49 57.83 44.52 40.75 ROLT-DRW 87.87 82.21 79.62 58.35 46.18 42.54</figDesc><table><row><cell></cell><cell></cell><cell>CIFAR-10</cell><cell></cell><cell></cell><cell>CIFAR-100</cell><cell></cell></row><row><cell>Imbalance Ratio</cell><cell>10</cell><cell>50</cell><cell>100</cell><cell>10</cell><cell>50</cell><cell>100</cell></row><row><cell>ERM</cell><cell>86.75</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 7 :</head><label>7</label><figDesc>Test accuracy (%) on clean CIFAR datasets with different imbalanced ratio. .82 27.82 11.14 30.63 25.60 16.37 6.59 16.96 NCM ERM 58.53 30.11 12.46 34.83 42.57 16.50 4.41 22.36 NCM Soft Label 56.18 30.24 10.68 33.58 28.80 15.89 5.59 17.63 Linear Noisy Label 49.53 30.34 13.93 32.27 24.83 13.53 5.11 15.21 Linear ERM 54.41 34.00 19.61 36.91 39.34 20.08 7.04 23.30 Linear Soft Label 54.15 35.84 16.79 36.73 31.80 21.00 6.93 20.98 NCM Noisy Label 49.50 29.45 12.00 31.38 25.60 16.37 6.59 16.96 NCM ERM 56.09 32.39 13.75 35.23 41.00 17.89 4.74 22.43 NCM Soft Label 55.26 30.82 10.82 33.53 28.80 15.89 5.59 17.63</figDesc><table><row><cell cols="2">DRW Classifier Pseudo-Label</cell><cell cols="4">? = 0.2 Many Medium Few All Many Medium Few All ? = 0.5</cell></row><row><cell>Linear</cell><cell cols="2">Noisy Label 54.06 26.53</cell><cell>4.43 29.70 31.03</cell><cell>8.42</cell><cell>0.48 14.19</cell></row><row><cell>Linear</cell><cell>ERM</cell><cell>61.47 29.32</cell><cell cols="3">4.96 33.43 46.60 14.18 1.11 22.00</cell></row><row><cell>Linear</cell><cell>Soft Label</cell><cell>61.32 31.63</cell><cell cols="3">7.46 34.96 38.89 15.71 1.52 19.99</cell></row><row><cell>NCM</cell><cell cols="2">Noisy Label 49</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>?r 0 93.15 92.85 92.76 92.55 92.56 92.40 ?r 0 86.78 85.90 85.43 85.21 83.13 81.49 ?r 0 71.18 68.59 66.31 65.92 61.83 57.58 0.1 91.43 91.37 91.36 91.31 91.33 91.41 0.1 81.13 80.22 78.84 77.60 77.05 75.13 0.1 62.48 61.54 59.91 58.70 55.57 53.17 0.2 90.40 90.42 90.31 90.33 90.35 90.24 0.2 76.91 76.48 76.15 75.09 75.20 73.66 0.2 61.33 60.18 59.92 57.98 56.34 52.82 0.3 88.74 88.80 88.77 88.58 88.73 88.54 0.3 75.64 74.60 74.36 74.17 72.76 71.17 0.3 55.26 55.05 53.79 54.05 50.45 47.74 0.4 87.00 86.91 86.82 86.89 86.85 86.75 0.4 72.26 71.61 70.95 69.96 70.05 67.83 0.4 51.98 51.22 51.05 50.36 50.12 46.28 0.5 84.57 84.53 84.46 84.38 84.29 83.95 0.5 67.01 67.04 66.83 64.68 64.16 64.15 0.5 41.70 40.90 40.75 40.07 38.61 36.73</figDesc><table><row><cell></cell><cell></cell><cell>? = 1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>? = 10</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">? = 100</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>?c</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>?c</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>?c</cell><cell></cell><cell></cell><cell></cell></row><row><cell>0</cell><cell>0.1</cell><cell>0.2</cell><cell>0.3</cell><cell>0.4</cell><cell>0.5</cell><cell>0</cell><cell>0.1</cell><cell>0.2</cell><cell>0.3</cell><cell>0.4</cell><cell>0.5</cell><cell>0</cell><cell>0.1</cell><cell>0.2</cell><cell>0.3</cell><cell>0.4</cell><cell>0.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 9 :</head><label>9</label><figDesc>Accuracy (%) of cRT on CIFAR-10 with different imbalanced ratio ? and noise level ?. 92.75 92.69 92.67 92.55 92.54 ?r 0 88.14 88.08 87.97 87.89 87.72 87.45 ?r 0 79.59 79.64 79.67 79.64 79.57 78.63 0.1 91.29 91.29 91.28 91.31 91.25 91.24 0.1 82.23 82.33 82.09 82.05 81.91 81.91 0.1 68.21 68.09 67.06 66.19 65.35 64.53 0.2 90.20 90.24 90.23 90.26 90.31 90.24 0.2 75.27 75.02 74.73 74.37 73.82 73.25 0.2 66.80 66.59 66.25 65.98 64.95 63.70 0.3 88.51 88.51 88.48 88.55 88.53 88.53 0.3 74.99 75.01 74.98 74.76 74.52 74.09 0.3 61.68 61.22 61.06 60.91 60.04 59.19 0.4 86.77 86.80 86.78 86.80 86.79 86.76 0.4 70.45 69.75 69.40 69.07 68.43 67.97 0.4 56.57 56.46 56.21 55.92 55.47 54.60 0.5 83.78 83.78 83.78 83.77 83.79 83.77 0.5 66.16 65.82 65.62 65.40 65.07 64.82 0.5 44.66 44.08 43.98 43.18 43.10 42.61</figDesc><table><row><cell></cell><cell></cell><cell>? = 1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>? = 10</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">? = 100</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>?c</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>?c</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>?c</cell><cell></cell><cell></cell><cell></cell></row><row><cell>0</cell><cell>0.1</cell><cell>0.2</cell><cell>0.3</cell><cell>0.4</cell><cell>0.5</cell><cell>0</cell><cell>0.1</cell><cell>0.2</cell><cell>0.3</cell><cell>0.4</cell><cell>0.5</cell><cell>0</cell><cell>0.1</cell><cell>0.2</cell><cell>0.3</cell><cell>0.4</cell><cell>0.5</cell></row><row><cell>0 92.77</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>?r</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 10 :</head><label>10</label><figDesc>Accuracy (%) of NCM on CIFAR-10 with different imbalanced ratio ? and noise level ?. ?r 0 69.73 68.90 68.09 67.49 66.61 65.70 ?r 0 56.62 53.55 52.21 50.52 49.09 47.34 ?r 0 39.44 35.43 33.93 32.34 31.32 29.68 0.1 68.55 68.03 67.38 66.58 66.48 65.87 0.1 50.55 49.13 47.89 46.23 44.55 43.36 0.1 33.19 32.25 30.69 28.76 27.61 25.97 0.2 65.51 65.21 64.86 64.45 64.46 63.96 0.2 45.31 44.22 42.56 41.73 40.27 38.61 0.2 27.77 27.02 26.31 24.57 23.79 22.82 0.3 63.01 62.74 62.32 62.02 61.65 60.96 0.3 41.72 40.82 39.77 37.80 37.84 36.38 0.3 24.91 23.83 23.61 21.48 21.28 19.61 0.4 60.78 60.42 60.30 59.73 59.19 58.93 0.4 37.33 36.76 35.31 34.46 32.18 32.68 0.4 23.02 22.38 22.04 21.49 20.62 19.48 0.5 57.88 57.51 56.98 56.83 55.97 55.14 0.5 32.07 31.09 30.29 29.90 28.58 25.55 0.5 19.05 18.60 17.93 17.67 16.89 16.01</figDesc><table><row><cell></cell><cell></cell><cell>? = 1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>? = 10</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">? = 100</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>?c</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>?c</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>?c</cell><cell></cell><cell></cell><cell></cell></row><row><cell>0</cell><cell>0.1</cell><cell>0.2</cell><cell>0.3</cell><cell>0.4</cell><cell>0.5</cell><cell>0</cell><cell>0.1</cell><cell>0.2</cell><cell>0.3</cell><cell>0.4</cell><cell>0.5</cell><cell>0</cell><cell>0.1</cell><cell>0.2</cell><cell>0.3</cell><cell>0.4</cell><cell>0.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 11 :</head><label>11</label><figDesc>Accuracy (%) of cRT on CIFAR-100 with different imbalanced ratio ? and noise level ?. ?r 0 66.94 66.71 66.37 65.79 64.84 64.07 ?r 0 56.05 55.63 55.22 54.14 53.18 51.78 ?r 0 41.73 41.18 40.59 39.81 38.56 37.95 0.1 65.72 65.84 65.66 65.27 64.83 64.16 0.1 50.49 50.76 50.14 49.53 49.51 48.16 0.1 35.43 34.89 34.49 33.77 32.93 32.11 0.2 63.08 62.92 63.26 62.61 62.67 62.15 0.2 45.22 45.05 45.15 44.83 44.21 43.22 0.2 30.47 29.95 29.45 28.74 28.56 28.13 0.3 60.82 60.64 60.62 60.81 60.29 60.16 0.3 41.82 41.66 41.23 41.31 40.27 39.68 0.3 25.97 25.50 25.17 24.74 23.96 22.59 0.4 57.87 58.00 57.81 57.82 57.91 57.55 0.4 36.13 36.32 36.19 35.81 35.41 34.84 0.4 23.89 23.47 22.80 22.29 21.84 20.50 0.5 55.24 55.25 55.05 55.01 54.64 54.95 0.5 30.85 30.63 30.50 30.07 29.84 29.34 0.5 19.16 18.63 18.47 18.15 16.89 16.77</figDesc><table><row><cell></cell><cell></cell><cell>? = 1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>? = 10</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">? = 100</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>?c</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>?c</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>?c</cell><cell></cell><cell></cell><cell></cell></row><row><cell>0</cell><cell>0.1</cell><cell>0.2</cell><cell>0.3</cell><cell>0.4</cell><cell>0.5</cell><cell>0</cell><cell>0.1</cell><cell>0.2</cell><cell>0.3</cell><cell>0.4</cell><cell>0.5</cell><cell>0</cell><cell>0.1</cell><cell>0.2</cell><cell>0.3</cell><cell>0.4</cell><cell>0.5</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The inaturalist species classification and detection dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oisin</forename><forename type="middle">Mac</forename><surname>Grant Van Horn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Shepard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><forename type="middle">J</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8769" to="8778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Large-scale long-tailed recognition in an open world</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongqi</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohang</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiayun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boqing</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stella</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2537" to="2546" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Equalization loss for long-tailed object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingru</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changbao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Buyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanquan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changqing</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="11659" to="11668" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">LVIS: A dataset for large vocabulary instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agrim</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5356" to="5364" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Does tail label help for large-scale multi-label learning?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Feng</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transaction Neural Networks Learning Systems</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="2315" to="2324" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning to model the tail</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Xiong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martial</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7029" to="7039" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning imbalanced datasets with label-distribution-aware margin loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaidi</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrien</forename><surname>Gaidon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Ar?chiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengyu</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1565" to="1576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Class-balanced loss based on effective number of samples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglin</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9268" to="9277" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Jiashi Feng, and Yannis Kalantidis. Decoupling representation and classifier for long-tailed recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingyi</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicheng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Albert</forename><surname>Gordo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Distribution-balanced loss for multi-label classification in long-tailed datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingqiu</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">12349</biblScope>
			<biblScope unit="page" from="162" to="178" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Rethinking the value of labels for improving class-imbalanced learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuzhe</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Adversarial robustness under long-tailed distribution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingqiu</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Long-tailed recognition by routing diverse distribution-aware experts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xudong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongqi</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stella</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Long-tail learning via logit adjustment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><forename type="middle">Krishna</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sadeep</forename><surname>Jayasumana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankit</forename><surname>Singh Rawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Himanshu</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Veit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjiv</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Webvision database: Visual learning and understanding from web data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eirikur</forename><surname>Agustsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<idno>abs/1708.02862</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Exploiting privileged information from web data for image categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="437" to="452" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Dimensionality-driven learning with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingjun</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yisen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">E</forename><surname>Houle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuo</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarah</forename><forename type="middle">M</forename><surname>Erfani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu-Tao</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudanthi</forename><surname>Wijewickrema</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bailey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3355" to="3364" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Dividemix: Learning with noisy labels as semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junnan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Mopro: Webly supervised learning with momentum prototypes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junnan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hoi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">L_DMI: An information-theoretic noise-robust loss function</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yilun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuqing</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhou</forename><surname>Wang</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Dual T: reducing estimation error for transition matrix in label-noise learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tongliang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingming</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiankang</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masashi</forename><surname>Sugiyama</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Robust earlylearning: Hindering the memorization of noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaobo</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tongliang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nannan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zongyuan</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Identifying mislabeled data using the area under the margin ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoff</forename><surname>Pleiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ethan</forename><forename type="middle">R</forename><surname>Elenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Relay backpropagation for effective learning of deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhouchen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingming</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">9911</biblScope>
			<biblScope unit="page" from="467" to="482" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">BBN: bilateral-branch network with cumulative learning for long-tailed visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boyan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quan</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiu-Shen</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhao-Min</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9716" to="9725" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Long-tailed classification by keeping the good and removing the bad momentum causal effect</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaihua</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianqiang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Meta-weight-net: Learning an explicit mapping for sample weighting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lixuan</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanping</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zongben</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deyu</forename><surname>Meng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1917" to="1928" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Rethinking class-balanced methods for long-tailed visual recognition from a domain adaptation perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammad</forename><forename type="middle">Abdullah</forename><surname>Jamal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liqiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boqing</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="7610" to="7619" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Balanced metasoftmax for long-tailed visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cunjun</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shunan</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiyu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Heteroskedastic and imbalanced deep learning with adaptive regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaidi</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yining</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junwei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Arechiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrien</forename><surname>Gaidon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengyu</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Mentornet: Learning data-driven curriculum for very deep neural networks on corrupted labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengyuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2304" to="2313" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Co-teaching: Robust training of deep neural networks with extremely noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanming</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingrui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivor</forename><forename type="middle">W</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masashi</forename><surname>Sugiyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8536" to="8546" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">SELF: learning to filter noisy labels with self-ensembling</title>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<editor>Duc Tam Nguyen, Chaithanya Kumar Mummadi, Thi -Phuong -Nhung Ngo, Thi Hoai Phuong Nguyen, Laura Beggel, and Thomas Brox</editor>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Neighbourhood components analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Goldberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><forename type="middle">T</forename><surname>Roweis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="513" to="520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Distributional robustness loss for long-tail learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dvir</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gal</forename><surname>Chechik</surname></persName>
		</author>
		<idno>abs/2104.03066</idno>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A study of gaussian mixture models of color and texture features for image classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Haim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><forename type="middle">M</forename><surname>Permuter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Francos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jermyn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="695" to="706" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Improving calibration for long-tailed recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhisheng</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiequan</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Temporal ensembling for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Mixmatch: A holistic approach to semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avital</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Classification with noisy labels by importance reweighting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tongliang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="447" to="461" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Understanding and utilizing deep neural networks trained with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><forename type="middle">Ben</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangyong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengyu</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
