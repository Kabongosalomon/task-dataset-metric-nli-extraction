<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Rectifying the Shortcut Learning of Background for Few-Shot Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Luo</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Electronic Science</orgName>
								<address>
									<country>Technology of China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longhui</forename><surname>Wei</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Science and Technology of China</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangjian</forename><surname>Wen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Electronic Science</orgName>
								<address>
									<country>Technology of China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinrong</forename><surname>Yang</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">Huazhong University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingxi</forename><surname>Xie</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zenglin</forename><surname>Xu</surname></persName>
							<email>zenglin@gmail.com</email>
							<affiliation key="aff5">
								<orgName type="institution">Harbin Institute of Technology</orgName>
								<address>
									<settlement>Shenzhen</settlement>
								</address>
							</affiliation>
							<affiliation key="aff6">
								<orgName type="laboratory">Pengcheng Laboratory</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
							<affiliation key="aff4">
								<orgName type="institution">Xidian University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Rectifying the Shortcut Learning of Background for Few-Shot Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T08:56+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The category gap between training and evaluation has been characterised as one of the main obstacles to the success of Few-Shot Learning (FSL). In this paper, we for the first time empirically identify image background, common in realistic images, as a shortcut knowledge helpful for in-class classification but ungeneralizable beyond training categories in FSL. A novel framework, COSOC, is designed to tackle this problem by extracting foreground objects in images at both training and evaluation without any extra supervision. Extensive experiments carried on inductive FSL tasks demonstrate the effectiveness of our approaches.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Through observing a few samples at a glance, humans can accurately identify brand-new objects. This advantage comes from years of experiences accumulated by the human vision system. Inspired by such learning capabilities, Few-Shot Learning (FSL) is developed to tackle the problem of learning from limited data <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b53">54]</ref>. At training, FSL models absorb knowledge from a large-scale dataset; later at evaluation, the learned knowledge is leveraged to solve a series of downstream classification tasks, each of which contains very few support (training) images from brand-new categories.</p><p>The category gap between training and evaluation has been considered as one of the core issues in FSL <ref type="bibr" target="#b9">[10]</ref>. Intuitively, the prior knowledge of old categories learned at training may not be applicable to novel ones. <ref type="bibr" target="#b62">[63]</ref> consider solving this problem from a causal perspective. Their backdoor adjustment method, however, adjusts the prior knowledge in a black-box manner and cannot tell which specific prior knowledge is harmful and should be suppressed.</p><p>In this paper, we, for the first time, identify image background as one specific harmful source knowledge for FSL. Empirical studies in <ref type="bibr" target="#b56">[57]</ref> suggest that there exists spurious correlations between background and category of images (e.g., birds usually stand on branches, and shells often lie on the beaches; see <ref type="figure">Fig. 1</ref>), which serves as a shortcut knowledge for modern CNN-based vision systems to learn. It is further revealed that background knowledge has positive impact on the performance of in-class classification tasks. As illustrated in the simple example of <ref type="figure">Fig. 1</ref>, images from the same category are more likely to share similar background, making it possible for background knowledge to generalize from training to testing in common classification tasks. For FSL, however, the category gap produces brand-new foreground, background and their combinations at evaluation. The correlations learned at training thus may not be able to generalize and would probably mislead the predictions.  <ref type="figure">Figure 1</ref>: An illustrative example that demonstrates why background information is useful for regular classification but harmful for few-shot learning.</p><p>We take empirical investigations on the role of image foreground and background in FSL, revealing how image background drastically affects the learning and evaluation of FSL in a negative way.</p><p>Since the background is harmful, it would be good if we could force the model to concentrate on foreground objects at both training and evaluation, but this is not easy since we do not have any prior knowledge of the entity and position of the foreground objects in images. When humans are going to recognize foreground objects of images from the same class, they usually look for a shared local pattern that appears in the majority of images, and recognize patches with this pattern as foreground. This inspires us to design a novel framework, COSOC, to extract foreground of images for both training and evaluation of FSL by seeking shared patterns among images. The approach does not depend on any additional fine-grained supervisions such as bounding boxes or pixel-level labelings.</p><p>The procedure of foreground extraction of images in the training set is implemented before training. The corresponding algorithm, named Clustering-based Object Seeker (COS), first pre-trains a feature extractor on the training set using contrative learning, which has an outstanding performance, shown empirically in a later section, on the task of discriminating between ground-truth foreground objects. The feature extractor then maps random crops of images-candidates of foreground objects-into a well-shaped feature space. This is followed by runing a clustering algorithm on all of the features of the same class, imitating the procedure of seeking shared local patterns inspired by human behavior. Each cropped patch is then assigned a foreground score according to its distance to the nearest cluster centroid, for determining a sampling probability of that patch in the later formal training of FSL models. For evaluation, we develop Shared Object Concentrator (SOC), an algorithm that applies iterative feature matching within the support set, looking for one crop per image at one time that is most likely to be foreground. The sorted averaging features of obtained crops are further leveraged to match crops of query images so that foreground crops have higher matching scores. A weighted sum of matching scores are finally calculated as classification logits of each query sample. Compared to other potential foreground extracting algorithms such as saliency-based methods, our COS and SOC algorithms have additional capability of capturing shared, inter-image information, performing better in complicated, multi-object scenery. Our methods also have flexibility of dynamically assigning beliefs (probabilities) to all candidate foreground objects, relieving the risk of overconfidence.</p><p>Our contributions can be summarized as follows. i) By conducting empirical studies on the role of image foreground and background in FSL, we reveal that image background serves as a source of shortcut knowledge which harms the evaluation performance. ii) To solve this problem, we propose COSOC, a framework combining COS and SOC, which can draw the model's attention to image foreground at both training and evaluation. iii) Extensive experiments for non-transductive FSL tasks demonstrate the effectiveness of our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Works</head><p>Few-shot Image Classification. Plenty of previous work tackled few-shot learning in meta-learning framework <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b50">51]</ref>, where a model learns experience about how to solve few-shot learning tasks by tackling pseudo few-shot classification tasks constructed from the training set. Existing methods that ultilize meta-learning can be generally divided into three groups: (1) Optimization-based methods learn the experience of how to optimize the model given few training samples. This kind of methods either meta-learn a good model initialization point <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b70">71,</ref><ref type="bibr" target="#b20">21]</ref> or the whole optimization process <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b27">28]</ref> or both <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b36">37]</ref>. (2) Hallucination-based methods <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b67">68,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b37">38]</ref> learn to augment similar support samples in few-shot tasks, thus can greatly alleviate the low-shot problem. (3) Metric-based methods <ref type="bibr" target="#b53">[54,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b61">62,</ref><ref type="bibr" target="#b59">60]</ref> learn to map images into a metric feature space and classify query images by computing feature distances to support images. Among them, several recent works <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b63">64,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b9">10]</ref> intended to seek correspondence between images either by attention or meta-filter, in order to obtain a more reasonable similarity measure. Our SOC algorithm in one-shot setting is in spirit similar to these methods, in that we both apply pair-wise feature alignment between support and query images, implicitly removing backgrounds that are more likely to be dissimilar across images. SOC differs in multi-shot setting, where potentially useful shared inter-image information in support set exists and can be captured by our SOC algorithm.</p><p>The Influence of Background. A body of prior work studied the impact of image background on learning-based vision systems from different perspectives. <ref type="bibr" target="#b52">[53]</ref> showed initial evidence of the existence of background correlations and how it influences the predictions of vision models. <ref type="bibr" target="#b64">[65,</ref><ref type="bibr" target="#b43">44]</ref> analyzed background dependence for object detection. Another relevant work <ref type="bibr" target="#b3">[4]</ref> utilized camera traps for investigating how performance drops when adapting classifiers to unseen cameras with novel backgrounds. They explore the effect of class-independent background (i.e., background changes from training to testing while categories remain the same) on classification performance. Although the problem is also concerned with image background, no shortcut learning of background exists under this setting. This is because under each training camera trap, the classifier must distinguish different categories with background fixed, causing the background knowledge being not useful for predictions of training images. Instead, the learning signal during training pushes the classifier towards ignoring each specific background. The difficulties under this setting lie in the domain shift challenge-the classifier is confident to handle previously existing backgrounds, but lost in novel backgrounds. More recently, <ref type="bibr" target="#b56">[57]</ref> systematically explore the role of image background in modern deep-learning-based vision systems through well-designed experiments. The results give clear evidence on the existence of background correlations and identify it as a positive shortcut knowledge for models to learn. Our results, on the contrary, identify background correlations as a negative knowledge in the context of few-shot learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Contrastive</head><p>Learning. Recent success on contrastive learning of visual representations has greatly promoted the development of unsupervised learning <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b4">5]</ref>. The promising performance of contrastive learning relies on the instance-level discrimination loss which maximizes agreement between transformed views of the same image and minimizes agreement between transformed views of different images. Recently there have been some attempts <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b31">32]</ref> at integrating contrastive learning into the framework of FSL. Although achieving good results, these work struggle to have an in-depth understanding of why contrastive learning has positive effects on FSL. Our work takes a step forward, revealing the advantages of contrastive learning over supervised FSL models in identifying core objects of images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Empirical Investigation</head><p>Problem Definition. Few-shot learning consists of a training set D B and an evaluation set D v which share no overlapping classes. D B contains a large amount of labeled data and is usually used at first to train a backbone network f ? (?). After training, a set of N -way K-shot classification tasks</p><formula xml:id="formula_0">T = {(S ? , Q ? )} N T ? =1</formula><p>are constructed, each by first sampling N classes in D v and then sampling K and M images from each class to constitute S ? and Q ? , respectively. In each task ? , given the learned backbone f ? (?) and a small support set S ? = {(x ? k,n , y ? k,n )} K,N k,n=1 consisting of K images x ? k,n and corresponding labels y ? k,n from each of N classes, a few-shot classification algorithm is designed to classify M N images from the query set Q ? = {(x ? mn )} M,N m,n=1 . Preparation. To investigate the role of background and foreground in FSL, we need ground-truth image foreground for comparison. However, it is time-consuming to label the whole dataset. Thus we select only a subset D new = (D B , D v ) of miniImageNet <ref type="bibr" target="#b53">[54]</ref> and crop each image manually according to the largest rectangular bounding box that contains the foreground object. We denote the uncropped version of the subset as (D B -Ori, D v -Ori), and the cropped foreground version as (D B -FG, D v -FG). Two well-known FSL baselines are selected in our empirical studies: Cosine  Classifier (CC) <ref type="bibr" target="#b14">[15]</ref> and Prototypical Networks (PN) <ref type="bibr" target="#b48">[49]</ref>. See Appendix A for details of constructing D new and a formal introdcution of CC and PN. Category gap disables generalization of background knowledge. It can be first noticed that, under any condition, the performance is consistently and significantly improved if background is removed at the evaluation stage (switch from D v -Ori to D v -FG). The result implies that background at the evaluation stage in FSL is harmful. This is the opposite of that reported in <ref type="bibr" target="#b56">[57]</ref> which shows background helps improve on the performance of traditional classification task, where no category gap exists between training and evaluation. Thus we can infer that the class/distribution gap in FSL disables generalization of background knowledge and degrades performance.</p><formula xml:id="formula_1">PN CC D v -Ori D v -FG D v -Ori D v -FG D B -Ori D B -FG D B -Fuse</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">The Role of Foreground and Background in Few-Shot Image Classification</head><p>Removing background at training prevents shortcut learning. When only foreground is given at evaluation (D v -FG), the models trained with only foreground (D B -FG) perform much better than those trained with original images (D B -Ori). This indicates that models trained with original images may not pay enough attention to the foreground object that really matters for classification. Background information at training serves as a shortcut for models to learn and cannot generalize to brand-new classes. In contrast, models trained with only foreground "learn to compare" different objects-a desirable ability for reliable generalization to downstream few-shot learning tasks with out-of-domain classes.</p><p>Training with background helps models to handle complex scenes. When evaluating on D v -Ori, the models trained with original dataset D B -Ori are slightly better than those with foreground dataset D B -FG. We attribute this to a sort of domain shift: models trained with D B -FG never meet images with complex background and do not know how to handle it. In Appendix D.1 we further verify the assertion by showing evaluation accuracy of each class under the above two training situations. Note that since we apply random crop augmentation at training, domain shift does not exist if the models are instead trained on D B -Ori and evaluated on D v -FG.</p><p>Simple fusion sampling combines advantages of both sides. One may wish to cut off shortcut learning of background while maintaining adaptability of model to complex scenes. A simple solution may be fusion sampling: given an image as input, choose its foreground version with probability p, and its original version with probability 1 ? p. We simply set p equal to 0.5. We denote the dataset using this sampling strategy as D B -Fuse. As observed in <ref type="figure" target="#fig_3">Fig. 2(a)</ref>, models trained this way indeed combine advantages of both sides: achieving relatively good performance on both D v -Ori and D v -FG. In Appendix C, we compare the training curves of PN trained on three versions of datasets to further investigate the effectiveness of fusion sampling.</p><p>The above analysis provides new inspiration for how to improve FSL further: (1) Fusion sampling of foreground and original images could be applied to training.</p><p>(2) Since background information disturbs evaluation, it is needed to focus on foreground objects or assign image patches, that are more likely to be foreground, a larger weight for classification. Therefore, a foreground object identification mechanism is required at both training (for fusion sampling) and evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Contrastive Learning is Good at Identifying Objects</head><p>In this subsection, we reveal the potential of contrastive learning in identifying foreground objects, which we will use later for foreground extraction. Given one transformed view of one image, contrastive learning tends to distinguish another transformed view of that same image from thousands of views of other images. A more detailed introduction of contrastive learning is given in Appendix B. The two augmented views of the same image always cover the same object, but probably with different parts, sizes and color. To discriminate two augmented patches from thousands of other image patches, the model has to learn to identify the key discriminative information of the object under varying environment. In this manner, semantic relations among crops of images are explicitly modeled, thereby clustering semantically similar contents automatically. The features of different images are pushed away, while those of similar objects in different images are pulled closer. Thus it is reasonable to speculate that contrastive learning may enable models with better identification of centered foreground object.</p><p>To verify this, we train CC and contrastive learning models on the whole training set of miniImageNet (D B -Full) and compare their accuracy on D v -Ori and D v -FG. The contrastive learning method we use is Exemplar <ref type="bibr" target="#b68">[69]</ref>, a modified version of MoCo <ref type="bibr" target="#b17">[18]</ref>. <ref type="figure" target="#fig_3">Fig 2(b)</ref> shows that, while the evaluation accuracy of Exemplar on D v -Ori is slightly worse than that of CC, Exemplar performs much better when only foreground of images are given at evaluation, affirming that contrastive learning indeed has a better discriminative ability of single centered object. In Appendix D.2, we provide a more in-depth analysis of why contrastive learning has such properties and infer that the shape bias and viewpoint invariance may play an important role.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Rectifying the Shortcut Learning of Background</head><p>Given the analysis in the previous section, we wish to focus more on image foreground both at training and evaluation. Inspired by how humans recognise foreground objects, we propose COSOC, a framework ultilizing contrastive learning to draw the model's attention to the foreground objects of images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Clustering-based Object Seeker (COS) with Fusion Sampling for Training</head><p>Since contrastive learning is good at discriminating foreground objects, we utilize it to extract foreground objects before training. The first step is to pre-train a backbone f ? (?) on the training set D B using Exemplar <ref type="bibr" target="#b68">[69]</ref>. Then a clustering-based algorithm is used to extract "objects" identified by the pre-trained model. The basic idea is that features of foreground objects in images within one class extracted by contrastive learning models are similar, thereby can be identified via a clustering algorithm; see a simple example in <ref type="figure" target="#fig_8">Fig. 3</ref>. All images within the i-th class in D B form a set {x i n } N n=1 . For clarity, we omit the class index i in the following descriptions. The scheme of seeking foreground objects in one class is detailed as follows: 1) For each image x n , we randomly crop it L times to obtain L image patches {p n,m } L m=1 . Each image patch p n,m is then passed through the pre-trained model f ? and we get a normalized feature vector v n,m = f ? (pn,m)</p><formula xml:id="formula_2">||f ? (pn,m)||2 ? R d . 2)</formula><p>We run a clustering algorithm A on all features vectors of the class and obtain H clusters</p><formula xml:id="formula_3">{z j } H j=1 = A({v n,m } N,L n,m=1 ), where z j is the feature centroid of the j-th cluster. 3) We say an image x n ? z j , if there exists k ? [L] s.t. v n,k ? z j , where [L] = {1, 2, . . . , L}. Let l(z j ) = #{x|x?zj } N</formula><p>be the proportion of images in the class that belong to z j . If l(z j ) is small, then  <ref type="figure" target="#fig_8">Figure 3</ref>: Simplified schematic illustration of COS algorithm. We show how we obtain foreground objects from three exemplified images. The value under each crop denotes its foreground score.</p><p>the cluster z j is not representative for the whole class and is possibly background. Thus we remove all the clusters z with l(z) &lt; ?, where ? is a threshold that controls the generality of clusters. The remaining h clusters {z j } ? h j=?1 represent "objects" of the class that we are looking for. 4) The foreground score of image patch p n,m is defined as</p><formula xml:id="formula_4">s n,m = 1 ? min j?[h] ||v n,m ? z ?j || 2 /?, where ? = max n,m min j?[hc] ||v n,m ? z ?j || 2 is used to normalize the score into [0, 1]. Then top-k scores of each image x n are obtained as {s n,m } ? k m=?1 = Topk m?[L]</formula><p>(s n,m ). The corresponding patches {p n,m } ? k m=?1 are seen as possible crops of the foreground object in image x n , and the foreground scores {s n,m } ? k m=?1 as the confidence. We then use it as prior knowledge to rectify the shortcut learning of background for FSL models.</p><p>The training strategy resembles fusion sampling introduced before. For an image x n , the probability that we choose the original version is 1 ? max i? <ref type="bibr">[k]</ref> s n,?i , and the probability of choosing p n,?j from</p><formula xml:id="formula_5">top-k patches is (s n,?j / i?[k] s n,?i ) ? max i?[k]</formula><p>s n,?i . Then we adjust the chosen image patch and make sure that the least area proportion to the original image keeps as a constant. We use this strategy to train a backbone f ? (?) using a FSL algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Few-shot Evaluation with Shared Object Concentrator (SOC)</head><p>As discussed before, if the foreground crop of the image is used at evaluation, the performance of FSL model will be boosted by a large margin, serving as an upper bound of the model performance.</p><p>To approach this upper bound, we propose SOC algorithm to capture foreground objects by seeking shared contents among support images of the same class and query images. Step 1: Shared Content Searching within Each Class. For each image x k within one class c from support set S ? , we randomly crop it V times and obtain corresponding candidates {p k,n } n=1,..,V . Each patch p k,n is individually sent to the learned backbone f ? to obtain a normalized feature vector v k,n . Thus we have totally K ? V feature vectors within a class c. Our goal is to obtain a feature vector ? 1 that contains maximal shared information of all images in class c. Ideally, ? 1 represents the centroid of the most similar K image patches, each from one image, which can be formulated as</p><formula xml:id="formula_6">? 1 = 1 K K k=1 v k,?opt(k) ,<label>(1)</label></formula><formula xml:id="formula_7">? opt = arg max ??[K] [V ] 1?i&lt;j?K cos(v i,?(i) , v j,?(j) ),<label>(2)</label></formula><p>where cos(?, ?) denotes cosine similarity and [K] <ref type="bibr">[V ]</ref> denotes the set of functions that take [K] as domain and [V ] as range. While ? opt can be obtained by enumerating all possible combinations of image patches, the computation complexity of this brute-force method is O(V K ), which is computation prohibitive when V or K is large. Thus when the computation is not affordable, we turn to use a simplified method that leverages iterative optimization. Instead of seeking for the closest image patches, we directly optimize ? 1 so that the sum of minimum distance to patches of each image is minimized, i.e.,</p><formula xml:id="formula_8">? 1 = arg max ??R d K k=1 max n [cos(?, v k,n )],<label>(3)</label></formula><p>which can be achieved by iterative optimization algorithms. We apply SGD in our experiments. After optimization, we remove the patch of each image that is most similar to ? 1 , and obtain K ? (V ? 1) feature vectors. Then we repeatedly implement the above optimization process until no features are left, as shown in <ref type="figure" target="#fig_5">Fig. 4</ref>. We eventually obtain V sorted feature vectors {? n } V n=1 , which we use to represent the class c. As for the case where shot K = 1, there is no shared inter-image information inside class, so similar to the handling in PN and DeepEMD <ref type="bibr" target="#b63">[64]</ref> , we just skip step 1 and use the original V feature vectors.</p><p>Step 2: Feature Matching for Concentrating on Foreground Object of Query Images. Once the foreground class representations are identified, the next step is to use them to implicitly concentrate on foreground of query images by feature matching. For each image x in the query set Q ? , we also randomly crop it for V times and obtain V candidate features {? n } V n=1 . For each class c, we have V sorted representative feature vectors {? n } V n=1 obtained in step 1. We then match the most similar patches between query features and class features, i.e.,</p><formula xml:id="formula_9">s 1 = max 1?i,j?V [? j?1 cos(? i , ? j )],<label>(4)</label></formula><p>where ? ? 1 is an importance factor. Thus the weight ? j?1 decreases exponentially in index n ? 1, indicating a decreased belief of each vector representing foreground. Similarly, the two matched features are removed and the above process repeats until no features left. Finally, the score of x w.r.t. class c is obtained as a weighted sum of all similarities, i.e., S c = V n=1 ? n?1 s n , where ? ? 1 is another importance factor controlling the belief of each crop being foreground objects. In this way, features matched earlier-thus more likely to be foreground-will have higher contributions to the score. The predicted class of x is the one with the highest score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experiment Setup</head><p>Dataset. We adopt two benchmark datasets which are the most representative in few-shot learning. The first is miniImageNet <ref type="bibr" target="#b53">[54]</ref>, a small subset of ILSVRC-12 <ref type="bibr" target="#b44">[45]</ref> that contains 600 images within each of the 100 categories. The categories are split into 64, 16, 20 classes for training, validation and evaluation, respectively. The second dataset, tieredImageNet <ref type="bibr" target="#b41">[42]</ref>, is a much larger subset of ILSVRC-12 and is more challenging. It is constructed by choosing 34 super-classes with 608 categories. The super-classes are split into 20, 6, 8 super-classes which ensures separation between training and evaluation categories. The final dataset contains 351, 97, 160 classes for training, validation and evaluation, respectively. On both datasets, the input image size is 84 ? 84 for fair comparison.</p><p>Evaluation Protocols. We follow the 5-way 5-shot (1-shot) FSL evaluation setting. Specifically, 2000 tasks, each contains 15 testing images and 5 (1) training images per class, are randomly sampled from the evaluation set D v and the average classification accuracy is computed. This is repeated 5 times and the mean of the average accuracy with 95% confidence intervals is reported.</p><p>Implementation Details. The backbone we use throughout the article is ResNet-12, which is widely used in few-shot learning. We use Pytorch <ref type="bibr" target="#b38">[39]</ref> to implement all our experiments on two NVIDIA 1080Ti GPUs. We train the model using SGD with cosine learning rate schedule without restart to reduce the number of hyperparameters (Which epochs to decay the learning rate). The initial learning rate for training Exemplar is 0.1, and for CC is 0.005. The batch size for Exemplar, CC are 256 and 128, respectively. For miniImageNet, we train Exemplar for 150k iterations, and train CC for 6k iterations. For tieredImageNet, we train Exemplar for approximately 900k iterations, and train CC for 120k iterations. We choose k-means <ref type="bibr" target="#b32">[33]</ref> as the clustering algorithm for COS. The threshold ? is set to 0.5, and top 3 out of 30 features are chosen per image at the training stage. At the evaluation stage, we crop each image 7 times. The importance factors ? and ? are both set to 0.8. <ref type="table">Table 1</ref>: Ablative study on miniImageNet. All models are trained on the full training set of miniImageNet. Since the aim of SOC algorithm is to find foreground objects, it is unnecessary to evaluate SOC on the foreground dataset D v -FG. FT means finetuning from Exemplar used in COS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CC FT COS SOC</head><p>D  </p><formula xml:id="formula_10">v -Ori D v -FG 1-shot 5-</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Model Analysis</head><p>In this subsection, we show the effectiveness of each component of our method. Tab. 1 shows the ablation study conducted on miniImageNet.</p><p>On the effect of finetuning. Since a feature extractor is pre-trained using contrastive learning in COS, it may help accelerate convergence if we directly finetune from the pre-trained model instead of training from scratch. As shown in line 2-3 in Tab. 1, fintuning gives no improvement on the performance over training from scratch. Thus we adopt finetuning mainly for speeding up convergence (5? faster). Effectiveness of COS Algorithm. As observed in Tab. 1, When COS is applied on CC, the performance is improved on both versions of datasets. In <ref type="figure" target="#fig_6">Fig. 5</ref>, we show the curves of training and validation error of CC during training with and without COS. Both models are trained from scratch and validated on the full miniImageNet. We observe that CC sinks into overfitting: the training accuracy drops to zero, and validation accuracy stops improving before the end of the training. Meanwhile, the COS algorithm helps slow down convergence and prevent training accuracy from reaching zero. This makes validation accuracy comparable at first but higher at the end. Our COS algorithm weakens the "background shortcut" for learning, draws model's attention on foreground objects, and improves upon generalization.</p><p>Effectiveness of SOC Algorithm. The result in Tab. 1 shows that the SOC algorithm is the key to maximally exploit the potential of good object-discrimination ability. The performance even approaches the upper bound performance obtained by evaluating the model on the ground-truth foreground D v -FG. One potential unfairness in our SOC algorithm may lie in the use of multicropping, which could possibly lead to performance improvement for other approaches as well. We ablate this concern in Appendix G, as well as in the comparisons to other methods in the later subsections. Note that if we apply only the SOC algorithm on CC, the performance degrades. This indicates that COS and SOC are both necessary: COS provides the discrimination ability of foreground objects and SOC leverages it to maximally boost the performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Comparison to Saliency-based Foreground Extractors</head><p>There could be other possible ways of extracting foreground objects. A simple yet possibly strong baseline could be running saliency detection to extract the most salient region in an image, followed by cropping to obtain patches without background. We consider comparing with three classical unsupervised saliency methods-RBD <ref type="bibr" target="#b69">[70]</ref>, FT <ref type="bibr" target="#b0">[1]</ref> and MBD <ref type="bibr" target="#b65">[66]</ref>. The cropping threshold is specially tuned. For training, fusion sampling with probability 0.5 is used for unsupervised saliency methods. For evaluation, We replace the original images with crops obtained by unsupervised saliency methods directly for classification. Tab. 2 displays the comparisons of performance using different foreground extraction methods applied at training or evaluation. For fair comparison, all methods are trained from scratch, and all compared baselines are evaluated with multi-cropping (i.e. using the average of features obtained from multiple crops for classification)and tested on the same backbone (COS trained).</p><p>The results show that: (1) Our method performs consistently much better than the listed unsupervised saliency methods. (2) The performance of different unsupervised saliency methods varies. While RBD gives a small improvement, MBD and FT have negative effect on the performance. The performance severely depends on the effectiveness of unsupervised saliency methods, and is very sensitive to the cropping threshold. Intuitively speaking, saliency detection methods focus on noticeable objects in the image, and might fail when there is another irrelevant salient object in the image (e.g., a man is walking a dog. Dog is the label, but the man is of high saliency). On the contrary, our method focuses on shared objects across images in the same class, thereby avoiding this problem. In addition, our COS algorithm has the ability to dynamically assign foreground scores to different patches, which reduces the risk of overconfidence. One of our main contributions is paving a new way towards improving FSL by rectifying shortcut learning of background, which can be implemented using any effective methods. Given the upper bound with ground truth foreground, we believe there is room to improve and there can be other more effective approaches in the future.  <ref type="figure" target="#fig_7">Figure 6</ref>: Examples of objects obtained with COS from the training set of miniImageNet. The first row shows the original images;the second row shows the picked patch with the highest foreground score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>dalmatian guitar</head><p>Original images Found object <ref type="figure">Figure 7</ref>: Visualization examples of the SOC algorithm. The first row displays 5 images that belong to dalmatian and guitar classes respectively from evaluation set of miniImageNet. The second row shows image patches that are picked up from the first round of SOC algorithm. Our method succesfully puts focus on the shared contents/foreground.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Comparison to State-of-the-Arts</head><p>Tab. 3 presents 5-way 1-shot and 5-shot classification results on miniImageNet and tieredImageNet.</p><p>We compare with state-of-the-art few-shot learning methods. For fair comparison, we reimplement some methods, and evaluate them with multi-cropping. See Appendix G for a detailed study on the influence of multi-cropping. Our method achieves state-of-the-art performance under all settings except for 1-shot task on tieredImageNet, on which the performance of our method is slightly worse than CA, which uses WRN-28-10, a deeper backbone, as the feature extractor. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Visualization</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>Few-shot image classification benefits from increasingly more complex network and algorithm design, but little attention has been focused on image itself. In this paper, we reveal that image background serves as a source of harmful knowledge that few-shot learning models easily absorb in. This problem is tackled by our COSOC framework that can draw the model's attention to image foreground at both training and evaluation. Our method is only one possible solution, and future work may include exploring the potential of unsupervised segmentation or detection algorithms which may be a more reliable alternative of random cropping, or looking for a completely different but better algorithm customized for foreground extraction. Cosine Classifier (CC) and Prototypical Network (PN). In CC <ref type="bibr" target="#b14">[15]</ref>, the feature extractor f ? is trained together with a cosine-similarity based classifier under standard supervised way. The loss can be formally described as</p><formula xml:id="formula_11">L CC = ?E (x,y)?D B [log e cos(f ? (x),wy) C i=1 e cos(f ? (x),wi) ],<label>(5)</label></formula><p>where C denotes the number of classes in D B , cos(?, ?) denotes cosine similarity and w i ? R d denotes the learnable prototype for class i. To solve a following downstream few-shot classification task (S ? , Q ? ) ? T , CC adopts a non-parametric metric-based algorithm. Specifically, all images in (S ? , Q ? ) are mapped into features by the trained feature extractor f ? . Then all features from the same class c in S ? are averaged to form a prototype p c = 1 K (x,y)?S? 1 [y=c] f ? (x). Cosine similarity between query image and each prototype is then calculated to obtain score w.r.t. the corresponding class. In summary, the score for a test image x q w.r.t. class c can be written as</p><formula xml:id="formula_12">S c (x q ; S ? ) = log e cos(f ? (xq),pc) N i=1 e cos(f ? (xq),pi) ,<label>(6)</label></formula><p>and the predicted class for x q is the one with the highest score.</p><p>The difference between PN and CC is only at the training stage. PN follows meta-learning/episodic paradigm, in which a pseudo N -way K-shot classication task (S t , Q t ) is sampled from D B during each iteration t and is solved using the same algorithm as <ref type="bibr" target="#b5">(6)</ref>. The loss at iteration t is the average prediction loss of all test images and can be described as</p><formula xml:id="formula_13">L PN t = ? 1 |Q t | (x,y)?Qt S y (x; S t ).<label>(7)</label></formula><p>Implementation Details in Sec. 3. For all experiments in Sec. 3, we train CC and PN with ResNet-12 for 60 epochs. The initial learning rate is 0.1 with cosine decay schedule without restart. Random crop is used as data augmentation. The batch size for CC is 128 and for PN is 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Contrastive Learning</head><p>Contrastive learning tends to maximize the agreement between transformed views of the same image and minimize the agreement between transformed views of different images. Specifically, Let f ? (?) be a convolutional neural network with output feature space R d . Two augmented image patches from one image x are mapped by f ? (?), producing one query feature q, and one key feature k. Additionally, a queue containing thousands of negative features {v n } Q n=1 is produced using patches of other images. This queue can either be generated online using all images in the current batch <ref type="bibr" target="#b5">[6]</ref> or offline using stored features from last few epochs <ref type="bibr" target="#b17">[18]</ref>. Given q, contrastive learning aims to identify k in thousands of features {v n } Q n=1 , and can be formulated as:</p><formula xml:id="formula_14">L(q, k, {v n }) = ?log e sim(q,k)/? e sim(q,k)/? + Q j=1 e sim(q,vj )/? ,<label>(8)</label></formula><p>Where ? denotes a temperature parameter, sim(?, ?) a similarity measure. In Exemplar <ref type="bibr" target="#b68">[69]</ref>, all samples in {v n } Q n=1 that belong to the same class as q are removed in order to "preserve the unique information of each positive instance while utilizing the label information in a weak manner".  <ref type="table">Table 4</ref>: Comparisons of class-wise evaluation performance. The first row shows the training sets of which we compare different models. The second row shows the dataset we evaluate on. Each score denotes the difference of average accuracy of one class, e.g. a vs. b: (performance of a) -(performance of b). zero within 10 epochs. However, the validation error does not decrease to a relatively low value and remains high after convergence, reflecting severe overfitting phenomenon. On the contrary, PN with fusion samping converges much slower with a relatively lower validation error at the end. Apparently, shortcuts for PN on both D B -Ori and D B -FG exist and are suppressed by fusion sampling. In our paper we have showed that the shortcuts for dataset D B -Ori may be the statistical correlations between background and label and can be relieved by foreground concentration. However for dataset D B -FG the shortcut is not clear, and we speculate that appropriate amount of background information injects some noisy signals into the optimizatiton process which can help the model escape from local minima. We leave it for future work to further exploration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Shortcut Learning in PN</head><formula xml:id="formula_15">D B -FG vs. D B -Ori D B -FG vs. D B -Ori D B -Full: Exemplar vs CC D v -Ori D v -FG D v -FG</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Comparisons of Class-wise Evaluation Performance</head><p>Common few-shot evaluation focuses on the average performance of the whole evaluation set, which can not tell a method is why and in what aspect better than another one. To this end, we propose a more fine-grained class-wise evaluation protocol which displays average few-shot performance per class instead of single average performance.</p><p>We first visualize some images from each class of D v -Ori in <ref type="figure">Fig. 10</ref>. The classes are sorted by Signal-to-Full (SNF) ratio, which is the average ratio of foreground area over original area in each class. For instance, the class with highest SNF is bookshop. The images within this class always display a whole indoor scene, which can be almost fully recognised as foreground. In contrast, images from the class ant always contain large parts of background which are irrelevant with the category semantics, thus have low SNF. Although the SNF may not reflect the true complexity of background, we use it as an indicator and hope we could obtain some insights from the analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1 Domain Shift</head><p>We </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2 Shape Bias and View-Point Invariance of Contrastive Learning</head><p>The third column of Tab. 4 shows the class-wise performance difference between Exemplar and CC evaluated on D v -FG. We at first take a look at classes on which contrastive learning performs much better than CC: electric guitar, vase, ant, nematode, cuirass and mixing bowl. One observation is that the objects of each of these classes look similar in shape. Geirhos et al. <ref type="bibr" target="#b13">[14]</ref> point out that CNNs are strongly biased towards recognising textures rather than shapes, which is different from what humans do and is harmful for some downstream tasks. Thus we speculate that one of the reasons that contrastive learning is better than supervised models in some aspects is that contrastive learning prefers shape information more to recognising objects. To simply verify this, we hand draw shapes of some examples from the evaluation dataset; see <ref type="figure">Fig. 11</ref>. Then we calculate the similarity between features of original images and the shape image using different feature extractors. The results are shown in <ref type="figure">Fig. 11</ref>. As we can see, Exemplar recognises objects based on shape information more than the other two supervised methods. This is a conjecture more than a assertion. We leave it for future work to explore the shape bias of contrastive learning more deeply.</p><p>Next, let's have a look on the classes on which contrastive learning performs relatively poor: blackfooted ferret, golden retriever, king crab and malamute. It can be noticed that these classes all refer to animals that have different shapes under different view points. For example, dogs from the front and dogs from the side look totally different. The supervised loss pulls all views of one kind of animals closer, therefore enabling the model with the knowledge of dicriminating objects from different view points. On the contrary, contrastive learning pushes different images away, but only pulls patches of the same one image which has the same view point, thus has no prior of view point invariance. This suggests that contrastive learning can be further improved if view point invariance is injected into the learning process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.3 The Similarity between training Supervised Models with Foreground and training Models with Contrastive Learning</head><p>The second column and the third column of Tab. 4 are somehow similar, indicating that supervised models learned with foreground and learned with contrastive learning learn similar patterns of images. However, there are some classes that have distinct performance. For instance, the performance difference of contrastive learning on class electric guitar over CC is much higher than that of CC with D B -FG over D B -Ori. It is interesting to investigate what makes the difference between the representations learned by contrastive learning and supervised learning.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Additional Ablative Studies</head><p>In <ref type="figure" target="#fig_3">Fig. 12</ref>, we show how different values of ? and ? influence the performance of our model. ? and ? serve as importance factors in SOC, that express the belief of our firstly obtained foreground objects. As we can see, the performance of our model suffers from either excessively firm (small values) or weak (high values) belief. As ? and ? approach zero, it puts more attention on the first few detected objects, leading to increasing risk of wrong matchings of foreground objects; as ? and ? approach one, all weights of features tend to be the same, losing more emphasis on foreground objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F Detailed Performance in Sec. 3</head><p>We show detailed performance (both 1-shot and 5-shot) in Tab. 5 and Tab. 6. From the tables, we can see that 5-way 1-shot performance follows the same trend as 5-way 5-shot performance discussed in the main article.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G The Influence of Multi-cropping</head><p>For fair comparison and to better clarify the influence of our SOC algorithm, we include additional experiments about the influence of multi-cropping. We implemented several few-shot learning methods using multi-cropping during evaluation. Specifically, for all methods except DeepEMD, we average the feature vectors of 7 crops and use the resulted averaged feature for classification. For DeepEMD, we notice that they also report performance using multi-cropping during the evaluation stage, thus we follow the method in the original paper. We report the results in Tab. 7. As a reference of upper bound, we have also included the performance of using the ground truth foreground. We  denote multi-cropping as MC. The results show that multi-cropping can improve FSL models by 1-3 points, and the improvement tends to be marginal when the baseline performance becomes higher. Moreover, the improvement is smaller in 5-shot settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H Additional Visualization Results</head><p>In <ref type="figure" target="#fig_7">Fig. 13-16</ref> we display more visualization results of COS algorithm on four classes from the training set of miniImageNet. For each image, we show the top 3 out of 30 crops with the highest foreground scores. From the visualization results, we can conclude that: (1) our COS algorithm can reliably extract foreground regions from images, even if the foreground objects are very small or backgrounds are extremely noisy. (2) When there is an object in the image which is similar with the foreground object but comes from a distinct class, our COS algorithm can accurately distinguish them and focus on the right one, e.g. the last group of pictures in <ref type="figure" target="#fig_5">Fig. 14.</ref> (3) When multiple instances of foreground object exist in one picture, our COS algorithm can capture them simultaneously, distributing them in different crops, e.g. last few groups in <ref type="figure" target="#fig_8">Fig. 13</ref>. <ref type="figure" target="#fig_17">Fig. 17</ref> shows additional visulization results of SOC algorithms. Each small group of images display one 5-shot example from one class of the evaluation set of miniImageNet. Similar observations are presented, consistent with those in the main article.     </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 : 5 -</head><label>25</label><figDesc>way 5-shot FSL performance on different variants of training and evaluation datasets detailed in Sec. 3. (a) Empirical exploration of image foreground and background in FSL using two models: PN and CC. (b) Comparison between CC and Exemplar trained on the full training set of miniImageNet and evaluated on D v -Ori and D v -FG.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 2 (</head><label>2</label><figDesc>a) shows the average of 5-way 5-shot classification accuracy obtained by training CC and PN on D B -Ori and D B -FG, and evaluating on D v -Ori and D v -FG, respectively. See Appendix F for additional 5-way 1-shot experiments.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>The overall pipeline of step 1 in SOC. Points in one color represent features of crops from one image. The red points are ? 1 , ? 2 and ? 3 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Comparison of training and validation curves between CC with and without COS.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 6</head><label>6</label><figDesc>and 7 display visualization examples of the COS and SOC algorithms. See more examples in Appendix H. Thanks to the well-designed mechanism of capturing shared inter-image information, the COS and SOC algorithms are capable of locating foreground patches embodied in complicated, multi-object scenery.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>A Details of Section 3 Dataset</head><label>3</label><figDesc>Construction. We construct a subset D = (D B , D v ) of miniImageNet D-Full = (D B -Full, D v -Full). D B is created by randomly picking 100 out of 600 images from the first 27 categories of D B -Full; And D v is created by randomly picking 40 out of 600 images from all categories of D v -Full. We then crop each image in D such that the foreground object is tightly bounded. Some examples are displayed in Fig. 8.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 8 :Figure 9 :</head><label>89</label><figDesc>Examples of images of constructed datasets D. The first row shows images in D B which are original images of miniImageNet; and the second row illustrates corresponding cropped versions in D v in which only foreground objects are remained. Comparison of training and validation curves of PN trained under three different settings.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 9</head><label>9</label><figDesc>shows training and validation curves of PN trained on D B -Ori, D B -FG and D B -Fuse. It can be observed that the training errors of models trained on D B -Ori and D B -FG both decrease to</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 10 :Figure 11 :</head><label>1011</label><figDesc>first analyse the phenomenon of domain shift of few-shot models trained on D B -FG and evaluated on D B -FG. The first column in Tab. 4 displays class-wise performance difference between CC Illustrative examples of images in D v -Ori. The number under each class of images denotes Signal-to-Full ratio (SNF) ratio which is the average ratio of foreground area over original area in each class. Higher SNF approximately means less noise inside images. Shape similarity test. Each number denotes the feature similarity between the above image and its shape using correponding trained feature extractor. trained on D B -FG and D B -Ori. It can be seen that the worst-performance classes of model trained on D B -FG are those with low SNF and complex background. This indicates that the model trained on D B -FG fails to recognise objects taking up small space because they have never met such images during training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 12 :</head><label>12</label><figDesc>The effect of different values of ? and ?. The left figure shows the 5-way 1-shot accuracies, while the right figure shows the 5-way 5-shot accuracies with ? fixed as 0.8.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 13 :</head><label>13</label><figDesc>Visulization results of COS algorithm on class house finch.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 14 :</head><label>14</label><figDesc>Visulization results of COS algorithm on class Saluki.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 15 :</head><label>15</label><figDesc>Visulization results of COS algorithm on class ladybug.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Figure 16 :</head><label>16</label><figDesc>Visulization results of COS algorithm on class unicycle.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Figure 17 :</head><label>17</label><figDesc>Additional visualization results of the first step of SOC algorithm. In each group of images, we show a 5-shot example from one class.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>? 0.32 80.22 ? 0.24 66.69 ? 0.32 82.86 ? 0.19 64.76 ? 0.13 81.18 ? 0.21 71.13 ? 0.36 86.21 ? 0.15 65.05 ? 0.06 81.16 ? 0.17 71.36 ? 0.30 86.20 ? 0.14 64.41 ? 0.22 81.54 ? 0.28 --69.29 ? 0.12 84.94 ? 0.28 --</figDesc><table><row><cell>shot</cell><cell>1-shot</cell><cell>5-shot</cell></row><row><cell>62.67</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Comparisons with baselines of foreground extractors using saliency detection algorithms on miniImageNet. For fair comparison, all models in the right column at evaluation use multi-cropping. GT means evaluating with ground truth foreground. ? 0.35 82.79 ? 0.31 CC+RBD 63.24 ? 0.41 80.45 ? 0.37 COS+RBD 67.03 ? 0.52 82.57 ? 0.27 CC+MBD 61.50 ? 0.31 79.12 ? 0.32 COS+MBD 62.98 ? 0.45 79.56 ? 0.38 CC+FT 62.71 ? 0.11 80.06 ? 0.08 COS+FT 64.74 ? 0.28 80.74 ? 0.13 CC+COS 64.76 ? 0.13 81.18 ? 0.21 COSOC 69.28 ? 0.49 85.16 ? 0.42</figDesc><table><row><cell></cell><cell>Used for training</cell><cell></cell><cell></cell><cell>Used for evaluation</cell><cell></cell></row><row><cell>Method</cell><cell>1-shot</cell><cell>5-shot</cell><cell>Method</cell><cell>1-shot</cell><cell>5-shot</cell></row><row><cell>CC</cell><cell cols="2">62.67 ? 0.32 80.22 ? 0.24</cell><cell>COS</cell><cell>67.23</cell><cell></cell></row></table><note>- - - COS+GT 72.71 ? 0.57 87.43 ? 0.36</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Comparisons with state-of-the-art models on miniImageNet and tieredImageNet. The average inductive 5-way few-shot classification accuracies with 95 confidence interval are reported. * indicates methods evaluated using multi-cropping. ? 0.51 81.26 ? 0.<ref type="bibr" target="#b22">23</ref> 68.62 ? 0.27 83.74 ? 0.18 COSOC* (ours) ResNet-12 69.28 ? 0.49 85.16 ? 0.42 73.57 ? 0.43 87.57 ? 0.10</figDesc><table><row><cell>Model</cell><cell>backbone</cell><cell>miniImageNet 1-shot 5-shot</cell><cell cols="2">tieredImageNet 1-shot 5-shot</cell></row><row><cell>MetaOptNet [23]</cell><cell>ResNet-12</cell><cell cols="3">62.64 ? 0.82 78.63 ? 0.46 65.99 ? 0.72 81.56 ? 0.53</cell></row><row><cell>DC [29]</cell><cell>ResNet-12</cell><cell>62.53 ? 0.19 79.77 ? 0.19</cell><cell>-</cell><cell>-</cell></row><row><cell>CTM [26]</cell><cell>ResNet-18</cell><cell cols="3">64.12 ? 0.82 80.51 ? 0.13 68.41 ? 0.39 84.28 ? 1.73</cell></row><row><cell>CAM [20]</cell><cell>ResNet-12</cell><cell cols="3">63.85 ? 0.48 79.44 ? 0.34 69.89 ? 0.51 84.23 ? 0.37</cell></row><row><cell>AFHN [27]</cell><cell>ResNet-18</cell><cell>62.38 ? 0.72 78.16 ? 0.56</cell><cell>-</cell><cell>-</cell></row><row><cell>DSN [48]</cell><cell>ResNet-12</cell><cell cols="3">62.64 ? 0.66 78.83 ? 0.45 66.22 ? 0.75 82.79 ? 0.48</cell></row><row><cell>AM3+TRAML [24]</cell><cell>ResNet-12</cell><cell>67.10 ? 0.52 79.54 ? 0.60</cell><cell>-</cell><cell>-</cell></row><row><cell>Net-Cosine [30]</cell><cell>ResNet-12</cell><cell>63.85 ? 0.81 81.57 ? 0.56</cell><cell>-</cell><cell>-</cell></row><row><cell>CA [2]</cell><cell cols="4">WRN-28-10 65.92 ? 0.60 82.85 ? 0.55 74.40 ? 0.68 86.61 ? 0.59</cell></row><row><cell>MABAS [22]</cell><cell>ResNet-12</cell><cell>65.08 ? 0.86 82.70 ? 0.54</cell><cell>-</cell><cell>-</cell></row><row><cell>ConsNet [60]</cell><cell>ResNet-12</cell><cell>64.89 ? 0.23 79.95 ? 0.17</cell><cell>-</cell><cell>-</cell></row><row><cell>IEPT [67]</cell><cell>ResNet-12</cell><cell cols="3">67.05 ? 0.44 82.90 ? 0.30 72.24 ? 0.50 86.73 ? 0.34</cell></row><row><cell>MELR [11]</cell><cell>ResNet-12</cell><cell cols="3">67.40 ? 0.43 83.40 ? 0.28 72.14 ? 0.51 87.01 ? 0.35</cell></row><row><cell>IER-Distill [43]</cell><cell>ResNet-12</cell><cell cols="3">67.28 ? 0.80 84.78 ? 0.52 72.21 ? 0.90 87.08 ? 0.58</cell></row><row><cell>LDAMF [58]</cell><cell>ResNet-12</cell><cell cols="3">67.76 ? 0.46 82.71 ? 0.31 71.89 ? 0.52 85.96 ? 0.35</cell></row><row><cell>FRN [56]</cell><cell>ResNet-12</cell><cell cols="3">66.45 ? 0.19 82.83 ? 0.13 72.06 ? 0.22 86.89 ? 0.14</cell></row><row><cell>Baseline* [7]</cell><cell>ResNet-12</cell><cell>63.83 ? 0.67 81.38 ? 0.41</cell><cell>-</cell><cell>-</cell></row><row><cell>DeepEMD* [64]</cell><cell>ResNet-12</cell><cell cols="3">67.63 ? 0.46 83.47 ? 0.61 74.29 ? 0.32 86.98 ? 0.60</cell></row><row><cell>RFS-Distill* [52]</cell><cell>ResNet-12</cell><cell cols="3">65.02 ? 0.44 82.04 ? 0.38 71.52 ? 0.69 86.03 ? 0.49</cell></row><row><cell>FEAT* [61]</cell><cell>ResNet-12</cell><cell>68.03 ? 0.38 82.99 ? 0.31</cell><cell>-</cell><cell>-</cell></row><row><cell>Meta-baseline* [8]</cell><cell>ResNet-12</cell><cell>65.31</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>5-way few-shot performance of CC and PN with different variants of training and evaluation datasets. ? 0.27 62.73 ? 0.36 49.03 ? 0.28 66.75 ? 0.15 DB-FG 44.84 ? 0.20 60.85 ? 0.32 52.22 ? 0.35 68.65 ? 0.22 DB-Fuse 46.02 ? 0.18 62.91 ? 0.40 51.87 ? 0.39 68.98 ? 0.22 PN DB-Ori 40.57 ? 0.32 52.74 ? 0.11 44.24 ? 0.45 56.75 ? 0.34 DB-FG 40.25 ? 0.36 53.25 ? 0.33 46.93 ? 0.50 61.16 ? 0.35 DB-Fuse 45.25 ? 0.44 59.23 ? 0.28 50.72 ? 0.43 64.96 ? 0.20</figDesc><table><row><cell>Model training set</cell><cell>1-shot</cell><cell>Dv-Ori</cell><cell>5-shot</cell><cell>1-shot</cell><cell>Dv-FG</cell><cell>5-shot</cell></row><row><cell>DB-Ori</cell><cell>45.29</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>CC</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Comparisons of 5-way few-shot performance of CC and Exemplar trained on the full miniImageNet and evaluated on two versions of evaluation datasets. ? 0.32 80.22 ? 0.23 66.69 ? 0.32 82.86 ? 0.20 Exemplar 61.14 ? 0.14 78.13 ? 0.23 70.14 ? 0.12 85.12 ? 0.21</figDesc><table><row><cell>Model</cell><cell>1-shot</cell><cell>Dv-Ori</cell><cell>5-shot</cell><cell>1-shot</cell><cell>Dv-FG</cell><cell>5-shot</cell></row><row><cell>CC</cell><cell>62.67</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 :</head><label>7</label><figDesc>The influence of multi-cropping on miniImageNet. 1-shot (no MC ? MC) 5-shot (no MC ? MC) PN 60.19?63.97 75.50?78.90 Baseline 60.93?63.83 78.46?81.38 CC 62.67?64.41 80.22?82.74</figDesc><table><row><cell>Method</cell><cell></cell><cell></cell></row><row><cell>Meta-baseline</cell><cell>62.65?65.31</cell><cell>79.10?81.26</cell></row><row><cell>RFS-distill</cell><cell>63.00?65.02</cell><cell>79.63?82.04</cell></row><row><cell>FEAT</cell><cell>66.45?68.03</cell><cell>81.94?82.99</cell></row><row><cell>DeepEMD</cell><cell>66.61?67.63</cell><cell>82.02?83.47</cell></row><row><cell>S2M2_R</cell><cell>64.93?66.97</cell><cell>83.18?84.16</cell></row><row><cell>COS</cell><cell>65.05?67.23</cell><cell>81.16?82.79</cell></row><row><cell>COSOC</cell><cell>69.28(with MC)</cell><cell>85.16(with MC)</cell></row><row><cell>COS+groundtruth</cell><cell>71.36?72.71</cell><cell>86.20?87.43</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments and Disclosure of Funding</head><p>Special thanks to Qi Yong, who gives indispensable support on the spirit of this paper. We also thank Junran Peng for his help and fruitful discussions. This paper was partially supported by the National Key Research and Development Program of China (No. 2018AAA0100204), and a key program of fundamental research from Shenzhen Science and Technology Innovation Commission (No. JCYJ20200109113403826).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Frequency-tuned salient region detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radhakrishna</forename><surname>Achanta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheila</forename><forename type="middle">S</forename><surname>Hemami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><forename type="middle">J</forename><surname>Estrada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sabine</forename><surname>S?sstrunk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Associative alignment for few-shot image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arman</forename><surname>Afrasiyabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Fran?ois</forename><surname>Lalonde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Gagn?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Meta-learning with adaptive hyperparameters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungyong</forename><surname>Baik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myungsub</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janghoon</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heewon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoung Mu</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Recognition in terra incognita</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sara</forename><surname>Beery</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grant</forename><surname>Van Horn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Unsupervised learning of visual features by contrasting cluster assignments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A closer look at few-shot classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Yu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Cheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zsolt</forename><surname>Kira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Chiang Frank</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Meta-baseline: exploring simple meta-learning for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinbo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huijuan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Image deformation meta-networks for one-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zitian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanwei</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Xiong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martial</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Crosstransformers: spatially-aware few-shot transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankush</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">MELR: meta-learning via modeling episode-level relationships for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanyi</forename><surname>Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiwu</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songfang</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Model-agnostic meta-learning for fast adaptation of deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Contrastive prototype learning with augmented embeddings for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhao</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanyi</forename><surname>Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangzhen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiwu</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songfang</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">UAI</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Imagenet-trained cnns are biased towards texture; increasing shape bias improves accuracy and robustness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Geirhos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patricia</forename><surname>Rubisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claudio</forename><surname>Michaelis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Bethge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><forename type="middle">A</forename><surname>Wichmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wieland</forename><surname>Brendel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Dynamic few-shot visual learning without forgetting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spyros</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Bootstrap your own latent -A new approach to selfsupervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Bastien</forename><surname>Grill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florent</forename><surname>Altch?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Corentin</forename><surname>Tallec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><forename type="middle">H</forename><surname>Richemond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elena</forename><surname>Buchatskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaohan</forename><surname>Bernardo ?vila Pires</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><forename type="middle">Gheshlaghi</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bilal</forename><surname>Azar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Piot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R?mi</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Munos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Valko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Low-shot visual recognition by shrinking and hallucinating features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Meta-learning in neural networks: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Hospedales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antreas</forename><surname>Antoniou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Micaelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amos</forename><surname>Storkey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">TPAMI</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Cross attention network for few-shot classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruibing</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingpeng</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiguang</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xilin</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Task agnostic meta-learning for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammad</forename><forename type="middle">Abdullah</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamal</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guo-Jun</forename><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Model-agnostic boundary-adversarial sampling for test-time generalization in few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaekyeom</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyoungseok</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gunhee</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Meta-learning with differentiable convex optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kwonjoon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhransu</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avinash</forename><surname>Ravichandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Boosting few-shot learning with adaptive margin loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aoxue</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiran</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenguo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">One-shot learning of object categories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei-Fei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">TPAMI</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Finding task-relevant features for few-shot learning by category traversal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Dodge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Adversarial feature hallucination networks for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kunpeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Meta-sgd: Learning to learn quickly for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenguo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fengwei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.09835</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Dense classification and implanting for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lifchitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannis</forename><surname>Avrithis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvaine</forename><surname>Picard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><surname>Bursuc</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Negative margin matters: Understanding margin in few-shot classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning a few-shot embedding model with contrastive learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanwei</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengming</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siqian</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jilin</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Boosting few-shot classification with view-learnable contrastive learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxuan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangjian</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lili</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zenglin</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICME</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Some methods for classification and analysis of multivariate observations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Macqueen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the fifth Berkeley symposium on mathematical statistics and probability</title>
		<meeting>the fifth Berkeley symposium on mathematical statistics and probability</meeting>
		<imprint>
			<date type="published" when="1967" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Revisiting contrastive learning for few-shot classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orchid</forename><surname>Majumder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avinash</forename><surname>Ravichandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhransu</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marzia</forename><surname>Polito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Bhotika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Soatto</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.11058</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsendsuren</forename><surname>Munkhdalai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Yu</surname></persName>
		</author>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Spatial contrastive learning for few-shot classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yassine</forename><surname>Ouali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C?line</forename><surname>Hudelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myriam</forename><surname>Tami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECML/PKDD</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Meta-curvature</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eunbyung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junier</forename><forename type="middle">B</forename><surname>Oliva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Meta variance transfer: Learning to augment from the others</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seong-Jin</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungju</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji-Won</forename><surname>Baek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Insoo</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juhwan</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haebeom</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jae-Joon</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sung</forename><forename type="middle">Ju</forename><surname>Hwang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>K?pf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alykhan</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sasank</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benoit</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Meta-learning with implicit gradients</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aravind</forename><surname>Rajeswaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Kakade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Optimization as a model for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sachin</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Meta-learning for semi-supervised few-shot classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengye</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eleni</forename><surname>Triantafillou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sachin</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jake</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Exploring complementary strengths of invariant and equivariant representations for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salman</forename><surname>Mamshad Nayeem Rizve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fahad</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Shahbaz Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Rosenfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">K</forename><surname>Tsotsos</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.03305</idno>
		<title level="m">The elephant in the room</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">S</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei-Fei</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Oriol Vinyals, Razvan Pascanu, Simon Osindero, and Raia Hadsell. Meta-learning with latent embedding optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dushyant</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakub</forename><surname>Sygnowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Delta-encoder: an effective sample synthesis method for few-shot object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Karlinsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Shtok</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sivan</forename><surname>Harary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mattias</forename><surname>Marder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rog?rio</forename><surname>Schmidt Feris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raja</forename><surname>Giryes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Adaptive subspaces for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Koniusz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Nock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehrtash</forename><surname>Harandi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Prototypical networks for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jake</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Learning to compare: Relation network for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Flood</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongxin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">M</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Learning to learn: Introduction and overview</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Thrun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lorien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pratt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Learning to Learn</title>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Rethinking few-shot image classification: A good embedding is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Contextual priming for object detection. IJCV</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Matching networks for one shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Low-shot learning from imaginary data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Xiong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hebert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hariharan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Few-shot classification with feature map reconstruction networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davis</forename><surname>Wertheimer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luming</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Noise or signal: The role of image backgrounds in object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Logan</forename><surname>Engstrom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Ilyas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksander</forename><surname>Madry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Learning dynamic alignment via meta-filter for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengming</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengjie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jilin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feiyue</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyang</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanwei</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Metafun: Meta-learning with iterative functional updates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Francois</forename><surname>Ton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyunjik</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><forename type="middle">R</forename><surname>Kosiorek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee Whye</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Attentional constellation nets for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijian</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaijin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Few-shot learning via embedding adaptation with set-to-set functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hexiang</forename><surname>Han-Jia Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>De-Chuan Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Tapnet: Neural network augmented with task-adaptive projection for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Sung Whan Yoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaekyun</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Moon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Interventional few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongqi</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qianru</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xian-Sheng</forename><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Deepemd: Few-shot image classification with differentiable earth mover&apos;s distance and structured classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujun</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guosheng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">Local features and kernels for classification of texture and object categories: A comprehensive study. IJCV</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianguo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcin</forename><surname>Marszalek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svetlana</forename><surname>Lazebnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Minimum barrier salient object detection at 80 FPS</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><surname>Sclaroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><forename type="middle">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohui</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><forename type="middle">L</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radom?r</forename><surname>Mech</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">IEPT: instancelevel and episode-level pretext tasks for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manli</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianhong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiwu</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingyu</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songfang</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Metagan: An adversarial approach to few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruixiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zoubin</forename><surname>Ghahramani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqiu</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">What makes instance discrimination good for transfer learning? In ICLR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanxuan</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhirong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">H</forename><surname>Rynson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Saliency optimization from robust background detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wangjiang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Fast context adaptation via meta-learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luisa</forename><forename type="middle">M</forename><surname>Zintgraf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyriacos</forename><surname>Shiarlis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vitaly</forename><surname>Kurin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katja</forename><surname>Hofmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shimon</forename><surname>Whiteson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
