<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Few-Shot Segmentation via Cycle-Consistent Transformer</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gengwei</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Centre for Artificial Intelligence</orgName>
								<orgName type="laboratory">ReLER</orgName>
								<orgName type="institution">University of Technology Sydney</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoliang</forename><surname>Kang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Texas</orgName>
								<address>
									<settlement>Austin</settlement>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
							<email>yee.i.yang@gmail.com</email>
							<affiliation key="aff2">
								<orgName type="department">College of Computer Science and Technology</orgName>
								<orgName type="institution" key="instit1">CCAI</orgName>
								<orgName type="institution" key="instit2">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">Institute of Information Science</orgName>
								<orgName type="institution">Beijing Jiaotong University</orgName>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="department">Beijing Key Laboratory of Advanced Information Science and Network</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baidu</forename><surname>Research</surname></persName>
						</author>
						<title level="a" type="main">Few-Shot Segmentation via Cycle-Consistent Transformer</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T20:55+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Few-shot segmentation aims to train a segmentation model that can fast adapt to novel classes with few exemplars. The conventional training paradigm is to learn to make predictions on query images conditioned on the features from support images. Previous methods only utilized the semantic-level prototypes of support images as the conditional information. These methods cannot utilize all pixel-wise support information for the query predictions, which is however critical for the segmentation task. In this paper, we focus on utilizing pixel-wise relationships between support and query images to facilitate the few-shot segmentation task. We design a novel Cycle-Consistent TRansformer (CyCTR) module to aggregate pixel-wise support features into query ones. CyCTR performs cross-attention between features from different images, i.e. support and query images. We observe that there may exist unexpected irrelevant pixel-level support features. Directly performing cross-attention may aggregate these features from support to query and bias the query features. Thus, we propose using a novel cycle-consistent attention mechanism to filter out possible harmful support features and encourage query features to attend to the most informative pixels from support images. Experiments on all few-shot segmentation benchmarks demonstrate that our proposed CyCTR leads to remarkable improvement compared to previous state-of-the-art methods. Specifically, on Pascal-5 i and COCO-20 i datasets, we achieve 67.5% and 45.6% mIoU for 5-shot segmentation, outperforming previous state-of-the-art method by 5.6% and 7.1% respectively.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Recent years have witnessed great progress in semantic segmentation <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b46">47]</ref>. The success can be largely attributed to large amounts of annotated data <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b16">17]</ref>. However, labeling dense segmentation masks are very time-consuming <ref type="bibr" target="#b44">[45]</ref>. Semi-supervised segmentation <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b37">38]</ref> has been broadly explored to alleviate this problem, which assumes a large amount of unlabeled data is accessible. However, semi-supervised approaches may fail to generalize to novel classes with very few exemplars. In the extreme low data regime, few-shot segmentation <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b34">35]</ref> is introduced to train a segmentation model that can quickly adapt to novel categories. Most few-shot segmentation methods follow a learning-to-learn paradigm where predictions of query images are made conditioned on the features and annotations of support images. The key to the success of this training paradigm lies in how to effectively utilize the information provided by support images. Previous approaches extract semantic-level prototypes from support features and follow a metric learning <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b34">35]</ref> pipeline extending from PrototypicalNet <ref type="bibr" target="#b27">[28]</ref>. According to the granularity of utilizing support features, these methods can be categorized into two groups, as illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>: 1) Class-wise mean pooling <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b43">44]</ref>  <ref type="figure" target="#fig_0">(Figure 1(a)</ref>). Support features within regions of different categories are averaged to serve as prototypes to facilitate the classification of query pixels. 2) Clustering <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b40">41]</ref>  <ref type="figure" target="#fig_0">(Figure 1(b)</ref>). Recent works attempt to generate multiple prototypes via EM algorithm or K-means clustering <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b17">18]</ref>, in order to extract more abundant information from support images. These prototype-based methods need to "compress" support information into different prototypes (i.e. class-wise or cluster-wise), which may lead to various degrees of loss of beneficial support information and thus harm segmentation on query image. Rather than using prototypes to abstract the support information, <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b33">34]</ref>  <ref type="figure" target="#fig_0">(Figure 1(c)</ref>) propose to employ the attention mechanism to extract information from support foreground pixels for segmenting query. However, such methods ignore all the background support pixels that can be beneficial for segmenting query image, and incorrectly consider partial foreground support pixels that are quite different from the query ones, leading to sub-optimal results.</p><p>Background points Foreground points support image query image <ref type="figure">Figure 2</ref>: The motivation of our proposed method. Many pixel-level support features are quite different from the query ones, and thus may confuse the attention. We incorporate cycle-consistency into attention to filter such confusing support features. Note that the confusing support features may come from foreground and background.</p><p>In this paper, we focus on equipping each query pixel with relevant information from support images to facilitate the query pixel classification. Inspired by the transformer architecture <ref type="bibr" target="#b31">[32]</ref> which performs feature aggregation through attention, we design a novel Cycle-Consistent Transformer (CyCTR) module ( <ref type="figure" target="#fig_0">Figure 1(d)</ref>) to aggregate pixel-wise support features into query ones. Specifically, our CyCTR consists of two types of transformer blocks: the self-alignment block and the cross-alignment block. The selfalignment block is employed to encode the query image features by aggregating its relevant context information, while the cross-alignment aims to aggregate the pixel-wise features of support images into the pixel-wise features of query image. Different from self-alignment where Query 3 , Key and Value come from the same embedding, cross-alignment takes features from query images as Query, and those from support images as Key and Value. In this way, CyCTR provides abundant pixel-wise support information for pixel-wise features of query images to make predictions.</p><p>Moreover, we observe that due to the differences between support and query images, e.g., scale, color and scene, only a small proportion of support pixels can be beneficial for the segmentation of query image. In other words, in the support image, some pixel-level information may confuse the attention in the transformer. <ref type="figure">Figure 2</ref> provides a visual example of a support-query pair together with the label masks. The confusing support pixels may come from both foreground pixels and background pixels. For instance, point p 1 in the support image located in the plane afar, which is indicated as foreground by the support mask. However, the nearest point p 2 in the query image (i.e. p 2 has the largest feature similarity with p 1 ) belongs to a different category, i.e. background. That means, there exists no query pixel which has both high similarity and the same semantic label with p 1 . Thus, p 1 is likely to be harmful for segmenting "plane" and should be ignored when performing the attention. To overcome this issue, in CyCTR, we propose to equip the cross-alignment block with a novel cycle-consistent attention operation. Specifically, as shown in <ref type="figure">Figure 2</ref>, starting from the feature of one support pixel, we find its nearest neighbor in the query features. In turn, this nearest neighbor finds the most similar support feature. If the starting and the end support features come from the same category, a cycle-consistency relationship is established. We incorporate such an operation into attention to force query features only attend to cycle-consistent support features to extract information. In this way, the support pixels that are far away from query ones are not considered. Meanwhile, cycle-consistent attention enables us to more safely utilize the information from background support pixels, without introducing much bias into the query features.</p><p>In a nutshell, our contributions are summarized as follows: <ref type="formula" target="#formula_1">(1)</ref> We tackle few-shot segmentation from the perspective of providing each query pixel with relevant information from support images through pixel-wise alignment. <ref type="formula" target="#formula_3">(2)</ref> We propose a novel Cycle-Consistent TRansformer (CyCTR) to aggregate the pixel-wise support features into the query ones. In CyCTR, we observe that many support features may confuse the attention and bias pixel-level feature aggregation, and propose incorporating cycle-consistent operation into the attention to deal with this issue. 2 Related Work</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Few-Shot Segmentation</head><p>Few-shot segmentation <ref type="bibr" target="#b25">[26]</ref> is established to perform segmentation with very few exemplars. Recent approaches formulate few-shot segmentation from the view of metric learning <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b34">35]</ref>. For instance, <ref type="bibr" target="#b6">[7]</ref> first extends PrototypicalNet <ref type="bibr" target="#b27">[28]</ref> to perform few-shot segmentation. PANet <ref type="bibr" target="#b34">[35]</ref> simplifies the framework with an efficient prototype learning framework. SG-One <ref type="bibr" target="#b45">[46]</ref> leverage the cosine similarity map between the single support prototype and query features to guide the prediction. CANet <ref type="bibr" target="#b43">[44]</ref> replaces the cosine similarity with an additive alignment module and iteratively refines the network output. PFENet <ref type="bibr" target="#b29">[30]</ref> further designs an effective feature pyramid module and leverages a prior map to achieve better segmentation performance. Recently, <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b42">43]</ref> point out that only a single support prototype is insufficient to represent a given category. Therefore, they attempt to obtain multiple prototypes via EM algorithm to represent the support objects and the prototypes are compared with query image based on cosine similarity <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b40">41]</ref>. Besides, <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b33">34]</ref> attempt to use graph attention networks <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b39">40]</ref> to utilize all foreground support pixel features. However, they ignore all pixels in the background region by default. Besides, due to the large difference between support and query images, not all support pixels will benefit final query segmentation. Recently, some concurrent works propose to learn dense matching through Hypercorrelation Squeeze Networks <ref type="bibr" target="#b21">[22]</ref> or mining latent classes <ref type="bibr" target="#b41">[42]</ref> from the background region. Our work aims at mining information from the whole support image, but exploring to use the transformer architecture and from a different perspective, i.e., reducing the noise in the support pixel-level features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Transformer</head><p>Transformer and self-attention were firstly introduced in the fields of machine translation and natural language processing <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b31">32]</ref>, and are receiving increasing interests recently in the computer vision area. Previous works utilize self-attention as additional module on top of existing convolutional networks, e.g., Nonlocal <ref type="bibr" target="#b35">[36]</ref> and CCNet <ref type="bibr" target="#b13">[14]</ref>. ViT <ref type="bibr" target="#b7">[8]</ref> and its following work <ref type="bibr" target="#b30">[31]</ref> demonstrate the pure transformer architecture can achieve state-of-the-art for image recognition. On the other hand, DETR <ref type="bibr" target="#b2">[3]</ref> builds up an end-to-end framework with a transformer encoder-decoder on top of backbone networks for object detection. And its deformable vairents <ref type="bibr" target="#b50">[51]</ref> improves the performance and training efficiency. Besides, in natural language processing, a few works <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b26">27]</ref> have been introduced for long documents processing with sparse transformers. In these works, each Query token only attends to a pre-defined subset of Key positions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Cycle-consistency Learning</head><p>Our work is partially inspired by cycle-consistency learning <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b8">9]</ref> that is explored in various computer vision areas. For instance, in image translation, CycleGAN <ref type="bibr" target="#b49">[50]</ref> uses cycle-consistency to align image pairs. It is also effective in learning 3D correspondence <ref type="bibr" target="#b48">[49]</ref>, consistency between video frames <ref type="bibr" target="#b36">[37]</ref> and association between different domains <ref type="bibr" target="#b15">[16]</ref>. These works typically constructs cycle-consistency loss between aligned targets (e.g., images). However, the simple training loss cannot be directly applied to few-shot segmentation because the test categories are unseen from the training process and no finetuning is involved during testing. In this work, we incorporate the idea of cycle-consistency into transformer to eliminate the negative effect of confusing or irrelevant support pixels.</p><p>3 Methodology</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Problem Setting</head><p>Few-shot segmentation aims at training a segmentation model that can segment novel objects with very few annotated samples. Specifically, given dataset D train and D test with category set C train and C test respectively, where C train ? C test = ?, the model trained on D train is directly used to test on D test . In line with previous works <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b43">44]</ref>, episode training is adopted in this work for few-shot segmentation. Each episode is composed of k support images I s and a query image </p><formula xml:id="formula_0">I q to form a k-shot episode {{I s } k , I q },</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Revisiting of Transformer</head><p>Following the general form in <ref type="bibr" target="#b31">[32]</ref>, a transformer block is composed of alternating layers of multi-head attention (MHA) and multi-layer perceptron (MLP). LayerNorm (LN) <ref type="bibr" target="#b0">[1]</ref> and residual connection <ref type="bibr" target="#b11">[12]</ref> are applied at the end of each block. Specially, an attention layer is formulated as</p><formula xml:id="formula_1">Atten(Q, K, V ) = softmax( QK T ? d )V,<label>(1)</label></formula><p>where</p><formula xml:id="formula_2">[Q; K; V ] = [W q Z q ; W k Z kv ; W v Z kv ], in which Z q is the input Query sequence, Z kv is the input Key/Value sequence, W q , W k , W v ? R d?d denote the learnable parameters, d</formula><p>is the hidden dimension of the input sequences and we assume all sequences have the same dimension d by default.</p><p>For each Query element, the attention layer computes its similarities with all Key elements. Then the computed similarities are normalized via softmax, which are used to multiply the Value elements to achieve the aggregated outputs. When Z q = Z kv , it functions as self-attention mechanism.</p><p>The multi-head attention layer is an extention of attention layer, which performs h attention operations and concatenates consequences together. Specifically,</p><formula xml:id="formula_3">MHA(Q, K, V ) = [head 1 , ..., head h ],<label>(2)</label></formula><p>where</p><formula xml:id="formula_4">head m = Atten(Q m , K m , V m ) and the inputs [Q m , K m , V m ] are the m th group from [Q, K, V ] with dimension d/h.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Cycle-Consistent Transformer</head><p>Our framework is illustrated in <ref type="figure" target="#fig_1">Figure 3</ref>  . Each encoder of CyCTR consists of two transformers blocks, i.e., the self-alignment block for utilizing global context within the query feature map and the cross-alignment block for aggregate information from support images. In the cross-alignment block, we introduce the multi-head cycle-consistent attention (shown on the right, with the number of heads h = 1 for simplicity). The attention operation is guided by the cycle-consistency among query and support features.</p><p>Specifically, for the given query feature X q ? R Hq?Wq?d and support feature X s ? R Hs?Ws?d , we first flatten them into 1D sequences (with shape HW ? d) as inputs for transformer, in which a token is represented by the feature z ? R d at one pixel location. The self-alignment block only takes the flattened query feature as input. As context information of each pixel has been proved beneficial for segmentation <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b46">47]</ref>, we adopt the self-alignment block to pixel-wise features of query image to aggregate their global context information. We don't pass support images through the self-alignment block, as we mainly focus on the segmentation performance of query images. Passing through the support images which don't coordinate with the query mask may do harm to the self-alignment on query images.</p><p>In contrast, the cross-alignment block performs attention between query and support pixel-wise features to aggregate relevant support features into query ones. It takes the flattened query feature and a subset of support feature (the sampling procedure is discussed latter) with size N s ? H s W s as Key/Value sequence Z kv .</p><p>With these two blocks, it is expected to better encoder the query features to facilitate the subsequent pixel-wise classification. When stacking L encoders, the output of the previous encoder is fed into the self-alignment block. The outputs of self-alignment block and the sampled support features are then fed into the cross-alignment block.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Cycle-Consistent Attention</head><p>According to the aforementioned discussion, the pure pixel-level attention may be confused by excessive irrelevant support features. To alleviate this issue, as shown in <ref type="figure" target="#fig_1">Figure 3</ref>(b), a cycleconsistent attention operation is proposed. We first go through the proposed approach for 1-shot case for presentation simplicity and then discuss it in the multiple shot setting.</p><p>Formally, an affinity map A = QK T ? d , A ? R HqWq?Ns is first calculated to measure the correspondence between all query and support pixels. Then, for an arbitrary support pixel/token j (j ? {0, 1, ..., N s ? 1}, N s is the number of support pixels), its most similar query pixel/token i is obtained by</p><formula xml:id="formula_5">i = argmax i A (i,j) ,<label>(3)</label></formula><p>where i ? {0, 1, ..., H q W q ? 1} denotes the spatial index of query pixels. Since the query mask is not accessible, the label of query pixel i is unknown. However, we can in turn find its most similar support pixel j in the same way:</p><formula xml:id="formula_6">j = argmax j A (i ,j) .<label>(4)</label></formula><p>Given the sampled support label M s ? R Ns , cycle-consistency is satisfied if M s(j) = M s(j ) . Previous work <ref type="bibr" target="#b15">[16]</ref> attempts to encourage the feature similarity between cycle-consistent pixels to improve the model's generalization ability within the same set of categories. However, in few-shot segmentation, the goal is to enable the model to fast adapt to novel categories rather than making the model fit better to training categories. Thus, we incorporate the cycle-consistency into the attention operation to encourage the cycle-consistent cross-attention. First, by traversing all support tokens, an additive bias B ? R Ns is obtained by</p><formula xml:id="formula_7">B j = 0, ifM s(j) = M s(j ) ??, ifM s(j) = M s(j ) ,</formula><p>where j ? {0, 1, ..., N s }. Then, for a single query token Z q(i) ? R d at location i, the support information is aggregated by</p><formula xml:id="formula_8">CyCAtten(Q i , K i , V i ) = softmax(A (i) + B)V,<label>(5)</label></formula><p>where i ? {0, 1, ..., H q W q } and A is obtained by QK T ? d . In the forward process, B is element-wise added with the affinity A (i) for Z q(i) to aggregate support features. In this way, the attention weight for the cycle-inconsistent support features become zero, implying that these irrelevant information will not be considered. Besides, the cycle-consistent attention implicitly encourages the consistency between the most relevant query and support pixel-wise features through backpropagation. Note that our method aims at removing support pixels with certain inconsistency, rather than ensuring all support pixels to form cycle-consistency, which is impossible without knowing the query ground truth labels.</p><p>When performing self-attention in the self-alignment block, there may also exist the same issue, i.e. the query token may attend to irrelevant or even harmful features (especially when background is complex). According to our cycle-consistent attention, each query token should receive information from more consistent pixels than aggregating from all pixels. Due to the lack of query mask M q , it is impossible to establish the cycle-consistency among query pixels/tokens. Inspired by DeformableAttention <ref type="bibr" target="#b50">[51]</ref>, the consistent pixels can be obtained via a learnable way as ? = f (Q + Coord) and A = g(Q + Coord), where ? ? R HpWp?P is the predicted consistent pixels, in which each element ? ? R P in ? represents the relative offset from each pixel and P represents the number of pixels to aggregate. And A ? R HqWq?P is the attention weights. Coord ? R HqWq?d is the positional encoding <ref type="bibr" target="#b23">[24]</ref> to make the prediction be aware of absolute position, and f (?) and g(?) are two fully connected layers that predict the offsets 4 and attention weights. Therefore, the self-attention within the self-alignment transformer block is represented as</p><formula xml:id="formula_9">PredAtten(Q r , V r ) = P g softmax(A ) (r,g) V r+? (r,g) ,<label>(6)</label></formula><p>where r ? {0, 1, ..., H q W q } is the index of the flattened query feature, both Q and V are obtained by multiplying the flattened query feature with the learnable parameter.</p><p>Generally speaking, the cycle-consistent transformer effectively avoids the attention being biased by irrelevant features to benefit the training of few-shot segmentation.</p><p>Mask-guided sparse sampling and K-shot Setting: Our proposed cycle-consistency transformer can be easily extended to K-shot setting where K &gt; 1. When multiple support feature maps are provided, all support features are flattened and concatenated together as input. As the attention is performed at the pixel-level, the computation load will be high if the number of support pixels/tokens is large, which is usually the case under K-shot setting. In this work, we apply a simple maskguided sampling strategy to reduce the computation complexity and make our method more scalable. Concretely, given the k-shot support sequence Z s ? R kHsWs?d and the flattened support masks M s ? R kHsWs , the support pixels/tokens are obtained by uniformly sampling N f g tokens (N f g &lt;= Ns 2 , where N s ? kH s W s ) from the foreground regions and N s ? N f g tokens from the background regions in all support images. With a proper N s , the sampling operation reduces the computational complexity, and makes our algorithm more scalable with the increase of spatial size of support images. Additionally, this strategy helps balance the foreground-background ratio and also implicitly considers different sizes of various object regions in support images. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Overall Framework</head><p>Following previous works <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b43">44]</ref>, both query and support images are first feed into a shared backbone (e.g., ResNet <ref type="bibr" target="#b11">[12]</ref>) which is initialized with weights pretrained from ImageNet <ref type="bibr" target="#b24">[25]</ref> to obtain general image features. Similar to <ref type="bibr" target="#b29">[30]</ref>, middle-level query features (the concatenation of query features from the 3 rd and the 4 th blocks of ResNet) are processed by a 1?1 convolution to reduce the hidden dimension. The high-level query features (from the 5 th block) are used to generate a prior map (the prior map is generated by calculating the pixel-wise similarity between query and support features, details can be found in the supplementary materials) and then are concatenated with the middle-level query features. The average masked support feature is also concatenated to provide global support information. The concatenated features are processed by a 1?1 convolution. The output query features are then fed into our proposed CyCTR encoders. The output of CyCTR encoders is fed into a classifier to obtain the final segmentation results. The classifier consists of a 3?3 convolutional layer, a ReLU layer and a 1?1 convolutional layer. More details about our network structure can be found in the supplementary materials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Dataset and Evaluation Metric</head><p>We conduct experiments on two commonly used few-shot segmentation datasets, Pascal-5 i <ref type="bibr" target="#b9">[10]</ref> (which is combined with SBD <ref type="bibr" target="#b10">[11]</ref> dataset) and COCO-20 i <ref type="bibr" target="#b16">[17]</ref>, to evaluate our method. For Pascal-5 i , 20 classes are separated into 4 splits. For each split, 15 classes are used for training and 5 classes for test. At the test time, 1,000 pairs that belong to the testing classes are sampled from the validation set for evaluation. In COCO-20 i , we follow the data split settings in FWB <ref type="bibr" target="#b22">[23]</ref> to divide 80 classes evenly into 4 splits, 60 classes for training and test on 20 classes, and 5,000 validation pairs from the 20 classes are sampled for evaluation. Detailed data split settings can be found in the supplementary materials. Following common practice <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b45">46]</ref>, the mean intersection over union (mIoU) is adopted as the evaluation metric, which is the averaged value of IoU of all test classes. We also report the foreground-background IoU (FB-IoU) for comparison.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implementation Details</head><p>In our experiments, the training strategies follow the same setting in <ref type="bibr" target="#b29">[30]</ref>: training for 50 epochs on COCO-20 i and 200 epochs on Pascal-5 i . Images are resized and cropped to 473 ? 473 for both datasets and we use random rotation from ?10 ? to 10 ? as data augmentation. Besides, we use ImageNet <ref type="bibr" target="#b24">[25]</ref> pretrained ResNet <ref type="bibr" target="#b11">[12]</ref> as the backbone network and its parameters (including BatchNorms) are freezed. For the parameters except those in the transformer layers, we use the initial learning rate 2.5 ? 10 ?3 , momentum 0.9, weight decay 1 ? 10 ?4 and SGD optimizer with poly learning rate decay <ref type="bibr" target="#b3">[4]</ref>. The mini batch size on each gpu is set to 4. Experiments are carried out on Tesla V100 GPUs. For Pascal-5 i , one model is trained on a single GPU, while for COCO-20 i , one model is trained with 4 GPUs. We construct our baseline as follows: as stated in Section 3.4, the middle-level query features from backbone network are concatenated and merged with the global support feature and the prior map. This feature is processed by two residule blocks and input to the same classifier as our method. Dice loss <ref type="bibr" target="#b20">[21]</ref> is used as the training objective. Besides, the middle-level query feature is averaged using the ground truth and concatenated with support feature to predict the support segmentation map, which produces an auxiliary loss for aligning features.</p><p>The same settings are also used in our method except that we use our cycle-consistent transformer to process features rather than the residule blocks. For the proposed cycle-consistent transformer, we set the number of sampled support tokens N s to 600 for 1-shot and 5 ? 600 for 5-shot setting. The number of sampled tokens is obtained according to the averaged number of foreground pixels among Pascal-5 i training set. For the self-attention block, the number of points P is set to 9. For other hyper-parameters in transformer blocks, we use L = 2 transformer encoders. We set the hidden dimension of MLP layer to 3?256 and that of input to 256. The number of heads for all attention layers is set to 8 for Pascal-5 i and 1 for COCO-20 i . Parameters in the transformer blocks are optimized with AdamW <ref type="bibr" target="#b19">[20]</ref> optimizer following other transformer works <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b30">31]</ref>, with learning rate 1 ? 10 ?4 and weight decay 1 ? 10 ?2 . Besides, we use Dropout with the probability 0.1 in all attention layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Comparisons with State-of-the-Art Methods</head><p>In <ref type="table" target="#tab_1">Table 1</ref> and <ref type="table" target="#tab_2">Table 2</ref>, we compare our method with other state-of-the-art few-shot segmentation approaches on Pascal-5 i and COCO-20 i respectively. It can be seen that our approach achieves new <ref type="table">Table 3</ref>: Comparison with other methods using FB-IoU (%) on Pascal-5 i for 1-shot and 5-shot segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Backbone FB-IoU (%) 1-shot 5-shot A-MCG <ref type="bibr" target="#b12">[13]</ref> Res-101 61.2 62.2 DAN <ref type="bibr" target="#b33">[34]</ref> Res-101 71.9 72.3 PFENet <ref type="bibr" target="#b29">[30]</ref> Res-101 72.9 73.5 CyCTR (Ours)</p><p>Res-101 73.0 75.4</p><p>state-of-the-art performance on both Pascal-5 i and COCO-20 i . Specifically, on Pascal-5 i , to make fair comparisons with other methods, we report results with both ResNet-50 and ResNet-101. Our CyCTR achieves 64.0% mIoU with ResNet-50 backbone and 63.7% mIoU with ResNet-101 backbone for 1-shot segmentation, significantly outperforming previous state-ofthe-art results by 3.2% and 3.6% respectively. For 5-shot segmentation, our CyCTR can even surpass state-of-the art methods by 5.6% and 6.0% mIoU when using ResNet-50 and ResNet-101 backbones respectively. For COCO-20 i results in <ref type="table" target="#tab_2">Table 2</ref>, our method also outperforms other methods by a large margin due to the capability of the transformer to fit more complex data. Besides, <ref type="table">Table 3</ref> shows the comparison using FB-IoU on PASCAL-5 i for 1-shot and 5-shot segmentation, our method also obtains the state-of-the-art performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Ablation Studies</head><p>To provide a deeper understanding of our proposed method, we show ablation studies in this section. The experiments are performed on Pascal-5 i 1-shot setting with ResNet-50 as the backbone network, and results are reported in terms of mIoU. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.1">Component-Wise Ablations</head><p>We perform ablation studies regarding each component of our CyCTR in <ref type="table" target="#tab_3">Table 4</ref>. The first line is the result of our baseline, where we use two residual blocks to merge features as stated in Section 4.2. For all ablations in <ref type="table" target="#tab_3">Table 4</ref>, the hidden dimension is set to 128 and two transformer encoders are used. The mIoU results are averaged over four splits. Firstly, we only use the self-alignment block that only encodes query features. The support information in this case comes from the concatenated global support feature and the prior map used in <ref type="bibr" target="#b43">[44]</ref>. It can already bring decent results, showing that the transformer encoder is effective for modeling context for few-shot segmentation. Then, we utilize the cross-alignment block but only with the vanilla attention operation in Equation 1. The mIoU increases by 0.4%, indicating that pixel-level features from support can provide additional performance gain. By using our proposed cycle-consistent attention module, the performance can be further improved by a large margin, i.e. 0.6% mIoU compared to the vanilla attention. This result demonstrates our cycle-consistent attention's capability to suppress possible harmful information from support. Besides, we assume some background support features may also benefit the query segmentation and therefore use the cycle-consistent transformer to aggregate pixel-level information from background support features as well. Comparing the last two lines in <ref type="table" target="#tab_3">Table 4</ref>, we show that our way of utilizing beneficial background pixel-level support information brings 0.5% mIoU improvement, validating our assumption and the effectiveness of our proposed cycle-consistent attention operation.</p><p>Besides, one may be curious about whether the noise can also be removed by predicting the aggregation position like the way in Equation 6 for aggregating support features to query. Therefore, we use predicted aggregation instead of the cycle-consistent attention in the cross-alignment block, as denoted by CyCTR(pred) in <ref type="table" target="#tab_3">Table 4</ref>. It does benefit the few-shot segmentation by aggregating useful information from support but is 0.9% worse than the proposed cycle-consistent attention. The reason lies in the dramatically changing support images under few-shot segmentation testing. The cycle-consistency is better than the learnable way as it can globally consider the varying conditional information from both query and support. We can stack more encoders or increase the hidden dimension of encoders to increase its capacity and validate the effectiveness of our CyCTR. The results with different numbers of encoders (denoted as L) or hidden dimensions (denoted as d) are shown in <ref type="table" target="#tab_4">Table 5a</ref> and 5b. While increasing L or d within a certain range, CyCTR achieves better results. We chose L = 2 as our default choice for accuracy-efficiency trade-off.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.2">Effect of Model Capacity</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Qualitative results</head><p>In <ref type="figure" target="#fig_4">Figure 4</ref>, we show some qualitative results generated by our model on Pascal-5 i . Our cycleconsistent attention can improve the segmentation quality by suppressing possible harmful information from support. For instance, without cycle-consistency, the model misclassifies trousers as "cow" in the first row, baby's hair as "cat" in the second row, and a fraction of mountain as "car" in the third row, while our model rectifies these part as background. However, in the first row, our CyCTR still segments part of the trousers as "cow" and the right boundary of the segmentation mask is slightly worse than the model without cycle-consistency. The reason comes from the extreme differences between query and support, i.e. the support image shows a "cattle" but the query image contains a milk cow. The cycle-consistency may over-suppress the positive region in support images. Solving such issue may be a potential direction to investigate to improve our method further.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we design a CyCTR module to deal with the few-shot segmentation problem. Different from previous practices that either adopt semantic-level prototype(s) from support images or only use foreground support features to encode query features, our CyCTR utilizes all pixel-level support features and can effectively eliminate aggregating confusing and harmful support features with the proposed novel cycle-consistency attention. We conduct extensive experiments on two popular benchmarks, and our CyCTR outperforms previous state-of-the-art methods by a significant margin. We hope this work can motivate researchers to utilize pixel-level support features to design more effective algorithms to advance the few-shot segmentation research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A More Details</head><p>A.1 Implementation  The overall network architecture used in our experiments is shown in <ref type="figure" target="#fig_6">Figure 5</ref>. Following the common practice <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b43">44]</ref>, query and support image are first feed into a shared backbone network to obtain general image features. Similar to <ref type="bibr" target="#b29">[30]</ref>, the backbone network is pretrained on ImageNet <ref type="bibr" target="#b24">[25]</ref> and then completely kept fixed during few-shot segmentation training. Following <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b40">41]</ref>, we use dilated version of ResNet <ref type="bibr" target="#b11">[12]</ref> as the backbone network. Besides, middle-level features are processed by a 1x1 convolution to reduce the hidden dimension and high-level features are used to generate a prior map that concatenated with the middle-level feature. In details, the middle-level feature consists of the concatenation of features from the 3 rd and the 4 th block of ResNet (total 5 blocks including the stem block) with shape H ? W ? (512 + 1024) and is feed into a 1 ? 1 convolution to reduce the dimension to H ? W ? d, where d is the hidden dimension that can be adjusted in our experiments. The high-level feature (from the 5 th block of ResNet) with shape H ? W ? 2048 is used to generate the prior mask as in <ref type="bibr" target="#b29">[30]</ref>, which compute the pixel-wise similarity between the query and support high-level features and keep the maximum similarity at each pixel and normalize (using min-max normalization) the similarity map to the range of [0, 1]. To enable the pixel-wise comparison, we also concatenate the mask averaged support feature to both query and support feature and processed by a 1x1 convolution before inputting into the transformer. The final segmentation result is obtained by reshaping the output sequence back to spatial dimensions and predicted by a small convolution head that is consisted of one 3x3 convolution, one ReLU activation, and a 1x1 convolution. Dice loss <ref type="bibr" target="#b20">[21]</ref> is used as the training objective.</p><p>Baseline setup: For the baseline of our method, we use two residual blocks <ref type="bibr" target="#b11">[12]</ref> to merge the query feature. The support information comes from the concatenated support global feature and the prior map. During training, the foreground middle-level query feature from backbone network is averaged and concatenated with the middle-level support feature to predict the support mask for feature alignment. This auxiliary supervision is included in all of our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Dataset Settings</head><p>In this <ref type="table" target="#tab_5">Table 6</ref> and <ref type="table" target="#tab_6">Table 7</ref>, we provide the detailed split settings for datasets (Pascal 5 i and COCO-20 i ) used in our experiments, which follow the split settings proposed in <ref type="bibr" target="#b22">[23]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Split</head><p>Test classes PASCAL-5 0 aeroplane, bicycle, bird, boat, bottle PASCAL-5 1 bus, car, cat, chair, cow PASCAL-5 2 diningtable, dog, horse, motorbike, person PASCAL-5 3 potted plant, sheep, sofa, train, tv/monitor  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B More Visualizations</head><p>We provide more visualizations in <ref type="figure">Figure 6</ref>. We also provide the visualization of cycle-consistency relationships. In the first row, only a small part of the foreground region is activated while most foreground regions are valid in the second row. And in the second row, pixels on the "person" are shown in gray, which indicates that these pixels may have a negative impact on segmenting "cat".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Support</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Query</head><p>Ours without Cycle-consistency Ours Cycle-consistency <ref type="figure">Figure 6</ref>: More Qualitative results on Pascal-5 i . The cycle-consistency is visualized in the 2 ed column, in which red points are cycle-consistent foreground pixels, blue points are cycle-consistent background pixels, and gray points are cycle-inconsistent pixels. Best viewed in color and with zoom-in.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Different learning frameworks for few-shot segmentation, from the perspective of ways to utilize support information. (a) Class-wise mean pooling based method. (b) Clustering based method. (c) Foreground pixel attention method. (d) Our Cycle-Consistent TRansformer (CyCTR) framework that enables all beneficial support pixel-level features (foreground and background) to be considered.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>( 3 )</head><label>3</label><figDesc>Our CyCTR achieves state-ofthe-art results on two few-shot segmentation benchmarks, i.e., Pascal-5 i and COCO-20 i . Extensive experiments validate the effectiveness of each component in our CyCTR.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>(a). Generally, an encoder of our Cycle-Consistent TRansformer (CyCTR) consists of a self-alignment transformer block for encoding the query features and a cross-alignment transformer block to enable the query features to attend to the informative support features. The whole CyCTR module stacks L encoders.Cycle-Consistent Muti-head Attention (h=1)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Framework of our proposed Cycle-Consistent TRansformer (CyCTR)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Qualitative results on Pascal-5 i . From left to right, each column shows the examples of: Support image with mask region in red; Query image with ground truth mask region in blue; Result produced by the model without cycle-consistency in CyCTR; Result produced by our method.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>The network structure used in our experiments. The backbone network first extracts features for query and support images. To enable the pixel-wise comparison in transformer, the averaged foreground support feature is expanded and concatenated with both query and support features. Our Cycle-Consistent TRansformer (CyCTR) takes the flattened query and support features as well as the flattened support mask as input and produces the encoded query feature for prediction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Comparison with other state-of-the-art methods for 1-shot and 5-shot segmentation on PASCAL-5 i using the mIoU (%) evaluation metric. Best results are shown in bold.</figDesc><table><row><cell>Method</cell><cell>Backbone</cell><cell>5 0</cell><cell>5 1</cell><cell>1-shot 5 2</cell><cell>5 3</cell><cell>Mean</cell><cell>5 0</cell><cell>5 1</cell><cell>5-shot 5 2</cell><cell>5 3</cell><cell>Mean</cell></row><row><cell>PANet [35]</cell><cell></cell><cell cols="4">42.3 58.0 51.1 41.2</cell><cell>48.1</cell><cell cols="4">51.8 64.6 59.8 46.5</cell><cell>55.7</cell></row><row><cell>FWB [23] SG-One [46]</cell><cell>Vgg-16</cell><cell cols="4">47.0 59.6 52.6 48.3 40.2 58.4 48.4 38.4</cell><cell>51.9 46.3</cell><cell cols="4">50.9 62.9 56.5 50.1 41.9 58.6 48.6 39.4</cell><cell>55.1 47.1</cell></row><row><cell>RPMM [41]</cell><cell></cell><cell cols="4">47.1 65.8 50.6 48.5</cell><cell>53.0</cell><cell cols="4">50.0 66.5 51.9 47.6</cell><cell>54.0</cell></row><row><cell>CANet [44]</cell><cell></cell><cell cols="4">52.5 65.9 51.3 51.9</cell><cell>55.4</cell><cell cols="4">55.5 67.8 51.9 53.2</cell><cell>57.1</cell></row><row><cell>PGNet [43]</cell><cell></cell><cell cols="4">56.0 66.9 50.6 50.4</cell><cell>56.0</cell><cell cols="4">57.7 68.7 52.9 54.6</cell><cell>58.5</cell></row><row><cell>RPMM [41]</cell><cell>Res-50</cell><cell cols="4">55.2 66.9 52.6 50.7</cell><cell>56.3</cell><cell cols="4">56.3 67.3 54.5 51.0</cell><cell>57.3</cell></row><row><cell>PPNet [18]</cell><cell></cell><cell cols="4">47.8 58.8 53.8 45.6</cell><cell>51.5</cell><cell cols="4">58.4 67.8 64.9 56.7</cell><cell>62.0</cell></row><row><cell>PFENet [30]</cell><cell></cell><cell cols="4">61.7 69.5 55.4 56.3</cell><cell>60.8</cell><cell cols="4">63.1 70.7 55.8 57.9</cell><cell>61.9</cell></row><row><cell>CyCTR (Ours)</cell><cell>Res-50</cell><cell cols="4">65.7 71.0 59.5 59.7</cell><cell>64.0</cell><cell cols="4">69.3 73.5 63.8 63.5</cell><cell>67.5</cell></row><row><cell>FWB [23]</cell><cell></cell><cell cols="4">51.3 64.5 56.7 52.2</cell><cell>56.2</cell><cell cols="4">54.9 67.4 62.2 55.3</cell><cell>59.9</cell></row><row><cell>DAN [34]</cell><cell>Res-101</cell><cell cols="4">54.7 68.6 57.8 51.6</cell><cell>58.2</cell><cell cols="4">57.9 69.0 60.1 54.9</cell><cell>60.5</cell></row><row><cell>PFENet [30]</cell><cell></cell><cell cols="4">60.5 69.4 54.4 55.9</cell><cell>60.1</cell><cell cols="4">62.8 70.4 54.9 57.6</cell><cell>61.4</cell></row><row><cell>CyCTR (Ours)</cell><cell>Res-101</cell><cell cols="4">67.2 71.1 57.6 59.0</cell><cell>63.7</cell><cell cols="4">71.0 75.0 58.5 65.0</cell><cell>67.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Comparison with other state-of-the-art methods for 1-shot and 5-shot segmentation on COCO-20 i using the mIoU (%) evaluation metric. Best results are shown in bold.</figDesc><table><row><cell>Method</cell><cell>Backbone</cell><cell>20 0</cell><cell>20 1</cell><cell>1-shot 20 2</cell><cell cols="3">20 3 Mean 20 0</cell><cell>20 1</cell><cell>5-shot 20 2</cell><cell>20 3 Mean</cell></row><row><cell>FWB [23]</cell><cell>Res-101</cell><cell cols="4">19.9 18.0 21.0 28.9</cell><cell>21.2</cell><cell cols="3">19.1 21.5 23.9 30.1</cell><cell>23.7</cell></row><row><cell>PPNet [18]</cell><cell>Res-50</cell><cell cols="4">28.1 30.8 29.5 27.7</cell><cell>29.0</cell><cell cols="3">39.0 40.8 37.1 37.3</cell><cell>38.5</cell></row><row><cell>RPMM [41]</cell><cell>Res-50</cell><cell cols="4">29.5 36.8 29.0 27.0</cell><cell>30.6</cell><cell cols="3">33.8 42.0 33.0 33.3</cell><cell>35.5</cell></row><row><cell>PFENet [30]</cell><cell>Res-101</cell><cell cols="4">34.3 33.0 32.3 30.1</cell><cell>32.4</cell><cell cols="3">38.5 38.6 38.2 34.3</cell><cell>37.4</cell></row><row><cell>CyCTR (Ours)</cell><cell>Res-50</cell><cell cols="4">38.9 43.0 39.6 39.8</cell><cell>40.3</cell><cell cols="3">41.1 48.9 45.2 47.0</cell><cell>45.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Ablation studies that validate the effectiveness of each component in our Cycle-Consistent TRansformer. The first result is obtained by our baseline (see Section 4.2 for details).</figDesc><table><row><cell>self-alignment cross-alignment CyCTR (pred) CyCTR (fg. only) CyCTR mIoU (%)</cell></row><row><cell>59.3</cell></row><row><cell>62.5</cell></row><row><cell>62.9</cell></row><row><cell>62.6</cell></row><row><cell>63.0</cell></row><row><cell>63.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table><row><cell cols="4">Effect of varying (a) number of encoders</cell></row><row><cell cols="4">L and (b) hidden dimensions d. When varying L,</cell></row><row><cell cols="4">d is fixed to 128; while varying d, L is fixed to 2.</cell></row><row><cell cols="2">#Encoder mIoU (%)</cell><cell cols="2">#Dim mIoU (%)</cell></row><row><cell>1</cell><cell>62.4</cell><cell>128</cell><cell>63.5</cell></row><row><cell>2</cell><cell>63.5</cell><cell>256</cell><cell>64.0</cell></row><row><cell>3</cell><cell>63.7</cell><cell>384</cell><cell>63.9</cell></row><row><cell>(a)</cell><cell></cell><cell>(b)</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Data split for PASCAL-5 i , which follows the 4-fold cross-validation. Each row contains 5 classes for test and the rest 15 classes in the PASCAL dataset are used for training. Person, Airplane, Boat, Park meter, Dog, Elephant, Backpack, Suitcase, Sports ball, Skateboard, W. glass, Spoon, Sandwich, Hot dog, Chair, D. table, Mouse, Microwave, Fridge, Scissors, COCO-20 1 Bicycle, Bus, T.light, Bench, Horse, Bear, Umbrella, Frisbee, Kite, Surfboard, Cup, Bowl, Orange, Pizza, Couch, Toilet, Remote, Oven, Book, Teddy, COCO-20 2 Car, Train, Fire H., Bird, Sheep, Zebra, Handbag, Skis, B. bat, T. racket, Fork, Banana, Broccoli, Donut, P. plant, TV, Keyboard, Toaster, Clock, Hairdrier, COCO-20 3 Motorcycle, Truck, Stop, Cat, Cow, Giraffe, Tie, Snowboard, B. glove, Bottle, Knife, Apple, Carrot, Cake, Bed, Laptop, Cellphone, Sink, Vase, Toothbrush,</figDesc><table><row><cell>Split</cell><cell>Test classes</cell></row><row><cell>COCO-20 0</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 :</head><label>7</label><figDesc>Data split for COCO-20 i , which follows the 4-fold cross-validation. Each row contains 20 classes for test and the rest classes in the COCO dataset are used for training.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">To distinguish from the phrase "query" in few-shot segmentation, we use "Query" with capitalization to note the query sequence in the transformer.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">The offsets are predicted as 2d coordinates and transformed into 1d coordinates.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><forename type="middle">Lei</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><forename type="middle">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">Layer normalization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iz</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arman</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Longformer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.05150</idno>
		<title level="m">The long-document transformer</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">End-to-end object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="213" to="229" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="834" to="848" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Generating long sequences with sparse transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.10509</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><forename type="middle">Toutanova</forename><surname>Bert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Few-shot semantic segmentation with prototype learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanqing</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<title level="m">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Temporal cycle-consistency learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Debidatta</forename><surname>Dwibedi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusuf</forename><surname>Aytar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1801" to="1810" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">I</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="303" to="338" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Simultaneous detection and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Bharath Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Arbel?ez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="297" to="312" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition (CVPR)</title>
		<meeting>the IEEE conference on computer vision and pattern recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Attentionbased multi-context guiding for few-shot semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengwan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chiliang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yadong</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">M</forename><surname>Cees</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Snoek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for the Advancement of Artificial Intelligence (AAAI)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="8441" to="8448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Ccnet: Criss-cross attention for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zilong</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lichao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyu</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="603" to="612" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Weaklysupervised semantic segmentation network with deep seeded region growing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zilong</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7014" to="7023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Pixellevel cycle association: A new perspective for domain adaptive semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoliang</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yueting</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision (ECCV)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Songyang Zhang, and Xuming He. Part-aware prototype network for few-shot semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongfei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyi</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="142" to="158" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05101</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">Decoupled weight decay regularization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">V-net: Fully convolutional neural networks for volumetric medical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fausto</forename><surname>Milletari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nassir</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seyed-Ahmad</forename><surname>Ahmadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 fourth international conference on 3D vision (3DV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="565" to="571" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Hypercorrelation squeeze for few-shot segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juhong</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahyun</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minsu</forename><surname>Cho</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.01538</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Feature weighting and boosting for few-shot segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khoi</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sinisa</forename><surname>Todorovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="622" to="631" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Image transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dustin</forename><surname>Tran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4055" to="4064" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Irfan Essa, and Byron Boots. One-shot learning for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amirreza</forename><surname>Shaban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shray</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference (BMVC)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Sparsebert: Rethinking the importance analysis in self-attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahui</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaozhe</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenguo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James T</forename><surname>Kwok</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.12871</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Prototypical networks for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jake</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning to compare: Relation network for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Flood</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongxin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">M</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1199" to="1208" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Prior guided feature enrichment network for few-shot segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Training data-efficient image transformers &amp; distillation through attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.12877</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Neural Information Processing Systems</title>
		<meeting>the 31st International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6000" to="6010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petar</forename><surname>Veli?kovi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10903</idno>
		<title level="m">Graph attention networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Xianbin Cao, and Xiantong Zhen. Few-shot semantic segmentation with democratic attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haochen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xudong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutao</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yandan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Panet: Few-shot image semantic segmentation with prototype alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaixin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Hao Liew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingtian</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daquan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9197" to="9206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7794" to="7803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Learning correspondence from the cycleconsistency of time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Allan</forename><surname>Jabri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2566" to="2576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Object region mining with adversarial erasing: A simple classification to semantic segmentation approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1568" to="1576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Revisiting dilated convolution: A simple approach for weakly-and semi-supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaxin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honghui</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zequn</forename><surname>Jie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7268" to="7277" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Bidirectional graph reasoning network for panoptic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gengwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiajun</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9080" to="9089" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Prototype mixture models for few-shot semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boyu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bohao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbin</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixiang</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="763" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Mining latent classes for few-shot segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihe</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinghuan</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Gao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.15402</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Pyramid graph networks with connection attentions for region-based one-shot semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guosheng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fayao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiushuang</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingyao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9587" to="9595" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Canet: Class-agnostic segmentation networks with iterative refinement and attentive few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guosheng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fayao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5217" to="5226" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Interactive object segmentation with inside-outside guidance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Hao Liew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shikui</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="12234" to="12244" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Sg-one: Similarity guidance network for one-shot semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Cybernetics</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="3855" to="3865" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Scene parsing through ade20k dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Puig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adela</forename><surname>Barriuso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="633" to="641" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Learning dense correspondence via 3d-guided cycle consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinghui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Krahenbuhl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Aubry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="117" to="126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Unpaired image-to-image translation using cycle-consistent adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2223" to="2232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Deformable detr: Deformable transformers for end-to-end object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xizhou</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijie</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lewei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.04159</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
