<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Gumbel-Attention for Multi-modal Machine Translation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengbo</forename><surname>Liu</surname></persName>
							<email>liupengbo.work@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Harbin Institute of Technology</orgName>
								<address>
									<settlement>Harbin</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hailong</forename><surname>Cao</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Harbin Institute of Technology</orgName>
								<address>
									<settlement>Harbin</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiejun</forename><surname>Zhao</surname></persName>
							<email>tjzhao@mtlab.hit.edu.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Harbin Institute of Technology</orgName>
								<address>
									<settlement>Harbin</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Gumbel-Attention for Multi-modal Machine Translation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T07:53+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Multi-modal machine translation (MMT) improves translation quality by introducing visual information. However, the existing MMT model ignores the problem that the image will bring information irrelevant to the text, causing much noise to the model and affecting the translation quality. This paper proposes a novel Gumbel-Attention for multi-modal machine translation, which selects the text-related parts of the image features. Specifically, different from the previous attention-based method, we first use a differentiable method to select the image information and automatically remove the useless parts of the image features. Experiments prove that our method retains the image features related to the text, and the remaining parts help the MMT model generates better translations.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Multi-modal machine translation (MMT) is a novel research field of machine translation, which considers text information and uses visual information. Recent research explores various methods based on the seq2seq network for MMT. <ref type="bibr" target="#b7">Huang et al. (2016)</ref>transform and make the image features as one of the steps in the encoder as text in order to make it possible to attend to both the text and the image while decoding. <ref type="bibr" target="#b17">Zhou et al. (2018)</ref> adopts the mechanism of visual attention to jointly optimize the shared visual language embedding and model, which links visual semantics with corresponding text semantics. Recently, <ref type="bibr" target="#b14">Yao and Wan (2020)</ref> models text modal and vision modal from the perspective of a graph network based on Transformer. Previous methods integrate multi-modal information in multi-modal machine translation, and has made great progress. Despite their success, the current studies did not exploit how to automatically select the valuable information to the text in the image mixed with noise. The noise in the image three people flying a kite on a small hill at dusk. drei personen lassen in der d?mmerung auf einem kleinen h?gel einen drachen steigen. three people flying a kite on a small hill at dusk <ref type="figure">Figure 1</ref>: An example of useless information in the Multi30k dataset. The above one is the original picture, and the bottom is the remaining part after masking the parts irrelevant to the text in the picture.</p><p>will not help understand the context and affect the performance of the model. <ref type="figure">Figure 1</ref>, more than two-thirds of the original image is masked, and the remaining part can still fully express the semantics of the text. Even if we only keep the area around the entities in the picture (such as "three people", "kites"), we can explain all the content in the text. Namely, most of the content in the image is not related to the text in some scenarios, and a selecting method is essential.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>As shown in</head><p>In order to solve the above issues, we propose a novel attention mechanism using Gumbel-Sigmoid <ref type="bibr" target="#b6">(Geng et al., 2020)</ref> to automatically select meaningful information in the image, called Gumebl-Attention. To the best of our knowledge, this is the first attempt to perform denoising with selecting method during training among multimodal machine translation models. We also design a loss function to constrain the visual representation, expecting the image-aware text representation to be semantically similar to the text representation. It further ensures that parts irrelevant to the text in the image are removed. Experiments show that our method achieves or nears the state-of-the-art performance on the three test datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>arXiv:2103.08862v2 [cs.CL] 24 Jul 2022 2 Approach</head><p>Our model is based on the structure of Transformer and incorporates visual modal information. In this section, we will elaborate on our proposed Gumbel-Attention MMT model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Gumbel-Attention</head><p>In the encoder block of multi-modal machine translation, we propose Gumbel-Attention select the regions in the image that are helpful for understanding the text semantics instead of directly using the entire picture as in the previous work.</p><p>Selecting and Gumbel-Sigmoid. Selecting relevant content in the picture is a question of choosing a few elements in some candidate sets. The usual approach is to normalize them using the softmax function first and then select the candidate elements according to the probability. This approach is also a standard method for classification tasks. However, the image selection is an intermediate step of the entire model in this task, which should be differentiable, and parameters can be updated through backpropagation. <ref type="bibr" target="#b6">Geng et al. (2020)</ref> proposed Gumbel-Sigmoid to select a subset of elements from all elements, helping contribute to the meaning of the sentence by paying more attention to content words. The implement of Gumbel-Sigmoid is similar to Gumbel-Softmax <ref type="bibr" target="#b9">(Jang et al., 2017)</ref> by adding Gumbel noise in the sigmoid function:</p><formula xml:id="formula_0">Gumbel-Sigmoid (E s ) = sigmoid E s + G ? G /? = exp ((E s + G ) /? ) exp ((E s + G ) /? ) + exp (G /? ) ,</formula><p>(1) where the input E s is a matrix, and G , G are two independent noises called Gumbel noise. We can obtain a differentiable sample by Gumbel-Sigmoid. ? ? (0, ?) is a hyperparameter controlling the distribution tendency of sampling results. When ? is smaller(such as 0.1), the sampling result tends to be closer to a real one-hot vector. In contrast, the sampling result will be more similar in each dimension when ? becomes larger. Similarly, in the spatial features of the image, a similar method can also be used to select whether each feature is retained.</p><p>Gumbel-Attention. In order to map visual information to the semantic space of the text, the usual approach is to use the attention mechanism. In our work, we introduce an improved method for attention mechanism using Gumbel-Sigmoid, called Gumbel-Attention. The Gumbel-Attention mechanism can be regarded as a selection of the part of the image related to the text, which is a differentiable, discretely distributed approximate sampling. Similar attention mechanisms <ref type="bibr" target="#b16">(Zheng et al., 2020)</ref> have recently appeared in other fields and have achieved success. The selecting for each element in the score matrix in the attention is as follows:</p><formula xml:id="formula_1">? ij = G-S ? ? ? x text i W Q x image j W K T ? d model ? ? ? , (2) where G-S is is the abbreviation of Gumbel- Attention, x text i is the i th text features, x image j is the j th image spatial features, d model is di- mension of model, and W Q ? R dtext?d model , W K ? R d image ?d model is trainable parameter ma- trices.</formula><p>The score-matrix consists of the sampling results of each word and all regional features of the image. Then, we will get image-aware text representation using score matrix:</p><formula xml:id="formula_2">v i = n j=1? ij x text j W V ,<label>(3)</label></formula><p>where v i is i th image-aware text representation which calculate the weighted sum and only use the  <ref type="bibr" target="#b2">(Calixto et al., 2017)</ref> 36.5 55.0 ----Fusion-conv <ref type="bibr" target="#b1">(Caglayan et al., 2017)</ref> 37.0 57.0 29.8 51.2 25.1 46.0 Trg-mul <ref type="bibr" target="#b1">(Caglayan et al., 2017)</ref> 37.8 55.7 30.7 52.2 26.4 47.4 Latent Variable MMT <ref type="bibr" target="#b3">(Calixto et al., 2019)</ref> 37.7 56.0 30.1 49.9 25.5 44.8 Deliberation networks <ref type="bibr" target="#b8">(Ive et al., 2019)</ref> 38.0 55.6 ----Multi-modal Transformer <ref type="bibr" target="#b14">(Yao and Wan, 2020)</ref>  image features related to the current word. To enhance the selecting accuracy of Gumbel-Attention, we also use multiple heads to improve ability of Gumbel-Attention to filter image features, just like the attention in vanilla transformer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Gated Fusion</head><p>To merge the output of the two modalities, we introduce a gating mechanism to control the fusion of h image and h text refering to recent work <ref type="bibr" target="#b15">(Zhang et al., 2020)</ref>:</p><formula xml:id="formula_3">? = sigmoid W h image + U h text , H = h text + ?h image ,<label>(4)</label></formula><p>where W and U are trainable parameters. The final output H is directly fed to the vanilla transformer decoder. The encoder of the model is shown in <ref type="figure" target="#fig_0">Figure 2</ref>, and the decoder is the same as the vanilla transformer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Multi-modal Similarity Loss</head><p>We propose a loss function to make the text and image expression more similar, which is also a supplement to image denoising:</p><formula xml:id="formula_4">L sim = max(0, 1 ? cosine(h image , h text ) ? m),<label>(5)</label></formula><p>where m is a hyperparameter that controls the degree of similarity margin 1 , and we use cosine similarity to measure the similarity of two vectors. The loss function of the entire model is as follows:</p><formula xml:id="formula_5">L total = L origin + ?L sim ,<label>(6)</label></formula><p>1 m should be a number from -1 to 1, 0-0.5 is suggested. In our experiment, we choose 0.3 as the value of margin.</p><p>where ? is a hyperparameter that controls the proportion of L sim . L total is obtained by the weighted sum of the multi-modal similarity loss and the original loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiment</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Datasets</head><p>Following previous work, we conduct experiments on the widely used Multil30k <ref type="bibr" target="#b5">(Elliott et al., 2016)</ref> dataset to investigate the effectiveness of our proposed model. The dataset contains 29,000 instances for training, 1,014 instances for validation, and 1,000 instances for testing(Test2016).</p><p>We also evaluate our model on the WMT17 test set(Test2017) and the MSCOCO test, which contain 1,000 and 461 instances, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Setup</head><p>Our model is implemented based on OpenNMT-py toolbox <ref type="bibr" target="#b10">(Klein et al., 2017)</ref>. Due to the small size of the training corpus, our encoder and decoder only use four layers, the number of attention heads is 4, and the input and output dimensions are both 128. We adopt Adam with ? 1 = 0.9, ? 2 = 0.98 to optimize our model. Spatial features are extracted from the VGG19 network as the visual representation. The feature dimension is 7 ? 7 ? 512 , which represents the spatial information in the image. The text representation is the randomly initialized word embedding. Finally, we evaluate the translation quality using BLEU <ref type="bibr" target="#b12">(Papineni et al., 2002)</ref> and METEOR (Denkowski and Lavie, 2011).  better than most existing models, except for Trgmul <ref type="bibr" target="#b1">(Caglayan et al., 2017)</ref> in meteor. One possible reason is that the result of Trg-mul comes from the system on the latest technology WMT2017 test set, which has been selected based on METEOR. An obvious finding is that our model surpasses the textonly Transformer by above 1.5 bleu points. In fact, the Gumbel-Attention-based method has only a minor modification on the vanilla Transformer, which proves the effectiveness of our model. Moreover, we draw two important conclusions: First, compared with the Multi-modal Transformer, the main improvement of our method is the Gumbel-Sigmoid operation in the score matrix and the multi-modal similarity loss. The results of Test2016 show that this method of information selected based on Gumbel-Sigmoid is indeed effective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Results</head><p>Second, our method is more straightforward while achieving better results than the transformerbased two-stage model Deliberation networks <ref type="bibr" target="#b8">(Ive et al., 2019)</ref>. This result shows that our method is effective. However, the model structure is also as simple as possible compared to other methods, conducive to reproducibility, and easy to apply to other tasks.</p><p>Overall, our model maintains the best or nearbest performance on the three test sets. Therefore, we reconfirmed the effectiveness and universality of Gumbel-Attention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Ablation Study</head><p>To investigate the effectiveness of different modules in Gumbel-Attention MMT, we further compare our method with the variants in <ref type="table" target="#tab_3">Table 2</ref>.</p><p>replace with vanilla-attention. In this variant, we replace Gumbel-Attention with vanillaattention, which performs a weighted sum of similarity between text and image information instead of selecting. This method can also make reasonable use of picture information, so the performance is improved compared with the text-only base-line. Additionally, the Gumbel-Attention model and vanilla-attention-based model have a gap of more than one bleu on Test2016, which demonstrates the influence of image noise on the MMT task.</p><p>replace with random image. With reference to Multi-modal Transformer, we conducted random image replacement experiments. However, different from the previous conclusion, the result of row 4 shows that using random image to replace the original image in the Gumbel-Attention model is still better than text-only. This suggests that the random image as a regularization item improves the model effect, similar to the effect of random noise on the model <ref type="bibr" target="#b0">(Bishop, 1995)</ref>.</p><p>shared parameters. In this variant, we encode text representation and image-aware text representation with the shared encoder, which can reduce many parameters in the model. Although there is a decline compared with the original model, it is still an improvement over the text-only Transformer. We can obtain the gain brought by the image information with a nominal training cost through the method of sharing parameters.</p><p>w/o multi-modal gated fusion. Instead of multi-modal gated fusion, we directly sum the outputs of two independent encoders. The result in row 6 shows that the multi-modal features fusion method based on the gate is a more suitable method than direct addition.</p><p>w/o multi-modal similarity loss We delete the multi-modal similarity loss in the loss function. The results of row 7 show that the performance does not drop much compared with the original model. This result shows that image-aware representation is semantically close to text representation even without additional loss function constraints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>This paper has proposed Gumbel-Attention for multi-modal machine translation, aiming to reduce irrelevant noise in pictures by differentiable methods to select the image information. Our experiment results demonstrate the effectiveness of our model. We also introduced multi-modal similarity loss to restrict other image representation and text representation to be more similar.</p><p>In future work, we would like to apply Gumbel Attention to other multi-modal tasks and investigate the theoretical interpretability of our method.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>The Encoder of Our Proposed Model</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1</head><label>1</label><figDesc>shows the results of all methods on three test sets. Our Gumbel-Attention MMT model is</figDesc><table><row><cell></cell><cell cols="2">BLEU METEOR</cell></row><row><cell>Gumbel-Attention Model</cell><cell>39.2</cell><cell>57.8</cell></row><row><cell>Text-only Transformer</cell><cell>37.8</cell><cell>55.3</cell></row><row><cell>-replace with vanilla-attention</cell><cell>38.6</cell><cell>55.9</cell></row><row><cell>-replace with random image</cell><cell>38.3</cell><cell>55.6</cell></row><row><cell>-shared parameters</cell><cell>38.5</cell><cell>56.0</cell></row><row><cell>-w/o multi-modal gated fusion</cell><cell>38.9</cell><cell>56.2</cell></row><row><cell cols="2">-w/o multi-modal similarity loss 39.0</cell><cell>57.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Ablation study of our model on Test2016</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Training with noise is equivalent to tikhonov regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">M</forename><surname>Bishop</surname></persName>
		</author>
		<idno type="DOI">10.1162/neco.1995.7.1.108</idno>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="108" to="116" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">LIUM-CVC submissions for WMT17 multimodal translation task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ozan</forename><surname>Caglayan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Walid</forename><surname>Aransa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrien</forename><surname>Bardet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mercedes</forename><surname>Garc?a-Mart?nez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lo?c</forename><surname>Barrault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Masana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luis</forename><surname>Herranz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joost</forename><surname>Van De Weijer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>Association for Computational Linguistics</publisher>
			<biblScope unit="page" from="432" to="439" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Doubly-attentive decoder for multi-modal neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iacer</forename><surname>Calixto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Campbell</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P17-1175</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1913" to="1924" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Latent variable model for multi-modal translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iacer</forename><surname>Calixto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Rios</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wilker</forename><surname>Aziz</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/p19-1642</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Conference of the Association for Computational Linguistics</title>
		<meeting>the 57th Conference of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6392" to="6405" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Meteor 1.3: Automatic metric for reliable optimization and evaluation of machine translation systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alon</forename><surname>Denkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lavie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Sixth Workshop on Statistical Machine Translation</title>
		<meeting>the Sixth Workshop on Statistical Machine Translation</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="85" to="91" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Multi30k: Multilingual englishgerman image descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Desmond</forename><surname>Elliott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stella</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khalil</forename><surname>Sima&amp;apos;an</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucia</forename><surname>Specia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th Workshop on Vision and Language. Association for Computer Linguistics</title>
		<meeting>the 5th Workshop on Vision and Language. Association for Computer Linguistics</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">How does selective mechanism improve self-attention networks?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinwei</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longyue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaopeng</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2986" to="2995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Attention-based multimodal neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Po-Yao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frederick</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sz-Rung</forename><surname>Shiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Conference on Machine Translation</title>
		<meeting>the First Conference on Machine Translation</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="639" to="645" />
		</imprint>
		<respStmt>
			<orgName>Association for Computer Linguistics</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Distilling translations with visual awareness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julia</forename><surname>Ive</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranava</forename><surname>Madhyastha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucia</forename><surname>Specia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Conference of the Association for Computational Linguistics</title>
		<meeting>the 57th Conference of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6525" to="6538" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Categorical reparameterization with gumbel-softmax</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shixiang</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">5th International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Opennmt: Open-source toolkit for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuntian</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Senellart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<biblScope unit="page" from="67" to="72" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Jing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 40th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Multimodal transformer for multimodal machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaowei</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojun</forename><surname>Wan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4346" to="4350" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Neural machine translation with universal visual representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuosheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kehai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masao</forename><surname>Utiyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eiichiro</forename><surname>Sumita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zuchao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">8th International Conference on Learning Representations. OpenReview.net</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Hierarchical gumbel attention network for text-based person search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kecheng</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng-Jun</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
		<idno type="DOI">10.1145/3394171.3413864</idno>
	</analytic>
	<monogr>
		<title level="m">MM &apos;20: The 28th ACM International Conference on Multimedia, Virtual Event</title>
		<meeting><address><addrLine>Seattle, WA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2020-10-12" />
			<biblScope unit="page" from="3441" to="3449" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A visual attention grounding neural model for multimodal machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingyang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Runxiang</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><forename type="middle">Jae</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3643" to="3653" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
