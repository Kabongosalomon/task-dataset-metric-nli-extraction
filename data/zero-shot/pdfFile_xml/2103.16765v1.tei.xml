<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Prototypical Cross-domain Self-supervised Learning for Few-shot Unsupervised Domain Adaptation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Yue</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">UC Berkeley</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zangwei</forename><surname>Zheng</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Nanjing University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanghang</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">UC Berkeley</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Gao</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">UC Berkeley</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Keutzer</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">UC Berkeley</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><forename type="middle">Sangiovanni</forename><surname>Vincentelli</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">UC Berkeley</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Prototypical Cross-domain Self-supervised Learning for Few-shot Unsupervised Domain Adaptation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T20:49+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Unsupervised Domain Adaptation (UDA) transfers predictive models from a fully-labeled source domain to an unlabeled target domain. In some applications, however, it is expensive even to collect labels in the source domain, making most previous works impractical. To cope with this problem, recent work performed instance-wise cross-domain self-supervised learning, followed by an additional fine-tuning stage. However, the instance-wise selfsupervised learning only learns and aligns low-level discriminative features. In this paper, we propose an endto-end Prototypical Cross-domain Self-Supervised Learning (PCS) framework for Few-shot Unsupervised Domain Adaptation (FUDA) 1 . PCS not only performs cross-domain low-level feature alignment, but it also encodes and aligns semantic structures in the shared embedding space across domains. Our framework captures category-wise semantic structures of the data by in-domain prototypical contrastive learning; and performs feature alignment through cross-domain prototypical self-supervision. Compared with state-of-the-art methods, PCS improves the mean classification accuracy over different domain pairs on FUDA by 10.5%, 3.5%, 9.0%, and 13.2% on Office, Office-Home, VisDA-2017, and DomainNet, respectively.   </p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Deep Learning has achieved remarkable performance in various computer vision tasks, such as image classification <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b31">32]</ref> and semantic segmentation <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b77">78,</ref><ref type="bibr" target="#b7">8]</ref>. Despite high accuracy, deep neural networks trained on specific datasets often fail to generalize to new domains owing to the domain shift problem <ref type="bibr" target="#b66">[67,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b67">68]</ref>. Unsupervised domain adaptation (UDA) transfers predictive models from a fully-labeled source domain to an unlabeled target domain. Although it is challenging with no label information in the Previous method source prototype unlabeled source target prototype unlabeled target labeled source <ref type="figure">Figure 1</ref>: We address the task of few-shot unsupervised domain adaptation. Top: Existing domain-classifier based methods align source and target distributions but fail to extract discriminative features due to lack of labeled data. Bottom: Our method estimates prototypes for in-domain and cross-domain self-supervised learning to extract domain-aligned discriminative features.</p><p>target domain, many UDA methods <ref type="bibr" target="#b67">[68,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b17">18]</ref> could achieve high accuracy on the target domain using the abundant explicit supervision in source domain, together with the unlabeled target samples for domain alignment.</p><p>In some real-world applications, however, providing large-scale annotations even in the source domain is often challenging due to the high cost and difficulty of annotation. Taking medical imaging for instance, each image of the Diabetic Retinopathy dataset <ref type="bibr" target="#b27">[28]</ref> is annotated by a panel of 7 or 8 U.S. board-certified ophthalmologists, with a total group of 54 doctors. Thus practically it is too stringent to assume the availability of source data with abundant labels.</p><p>In this paper, to cope with the labeling costs of the source domain, we instead consider a few-shot unsupervised domain adaptation (FUDA) setting, where only an extremely small fraction of source samples are labeled, while all the rest source and target samples remain unlabeled. Most state-of-the-art UDA methods align source and target features by minimizing some form of distribution distances <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b63">64,</ref><ref type="bibr" target="#b17">18]</ref>, and learn the discriminative representation by minimizing the supervision loss on fullylabeled source domain data. In FUDA, however, since we have a very limited number of labeled source samples, it is much harder to learn discriminative features in the source domain, not to mention in the target domain.</p><p>Several recent papers <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b72">73]</ref> on selfsupervised learning (SSL) present promising representation learning results on images from a single domain and <ref type="bibr" target="#b38">[39]</ref> further extended to perform SSL across two domains for better domain adaptation performance. Despite the improved performance, the instance-based method in <ref type="bibr" target="#b38">[39]</ref> has some fundamental weaknesses. First, the semantic structure of the data is not encoded by the learned structure. This is because the in-domain self-supervision in <ref type="bibr" target="#b38">[39]</ref> treats two instances as negative pairs as long as they are from different samples, regardless of the semantic similarity. Consequently, many instances sharing the same semantic are undesirably pushed apart in the feature space. Second, the cross-domain instance-to-instance matching in <ref type="bibr" target="#b38">[39]</ref> is very sensitive to abnormal samples. Imagine a case where the embeddings of source and target samples are far apart (i.e. the domain gap is large) and one abnormal source sample is mapped closer to all target samples than any other source sample. Then the method in <ref type="bibr" target="#b38">[39]</ref> would match all target samples to the same source sample (cf. <ref type="figure" target="#fig_4">Figure 3)</ref>. For a given sample, the matched sample in the other domain may change drastically during the training procedure, making the optimization harder to converge. Third, the two-stage pipeline (i.e. SSL followed by domain adaptation) is complicated and experiments show that the optimal DA methods for different datasets are different. As a result, the training is rather cumbersome and it is unclear how to choose the optimal DA method in the second stage for different datasets.</p><p>In this paper, we propose Prototypical Cross-domain Self-supervised learning, a novel single-stage framework for FUDA that unifies representation learning and domain alignment with few-shot labeled source samples. PCS contains three major components to learn both discriminative and domain-invariant features. First, PCS performs indomain prototypical self-supervision to implicitly encode the semantic structure of data into the embedding space. This is motivated by <ref type="bibr" target="#b40">[41]</ref>, but we further leverage the known semantic information of the task and learn better semantic structure in each domain. Second, PCS performs crossdomain instance-to-prototype matching to transfer knowledge from source to the target in a more robust manner. Instead of instance-to-instance matching, matching a sample to a prototype (i.e. representative embedding for a group of instances that are semantically similar) is more robust to abnormal instances in the other domain and makes the optimization converge faster and more smoothly. Third, PCS unifies prototype learning with cosine classifier and update cosine classifier adaptively with source and target prototypes. transfers from source prototypes to target prototypes for better performance on the target domain. In order to further mitigate the effect of cross-domain mismatching, we perform entropy maximization to obtain a more diversified output. We show that together with entropy minimization, this is equivalent to maximizing the mutual information (MI) between input image and the network prediction.</p><p>To summarize, our contributions are three-fold:</p><p>? We propose a novel Prototypical Cross-domain Selfsupervised learning framework (PCS) for few-shot unsupervised Domain Adaptation. ? We propose to leverage prototypes to perform better semantic structure learning, discriminative feature learning, and cross-domain alignment in a unified, unsupervised and adaptive manner. ? While it is hard to choose the optimal domain adaptation method in the complex two-stage framework <ref type="bibr" target="#b38">[39]</ref>, PCS can be easily trained in an end-to-end matter, and outperforms all state-of-the-art methods by a large margin across multiple benchmark datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Domain Adaptation. Unsupervised Domain Adaptation (UDA) <ref type="bibr" target="#b23">[24]</ref> addresses the problem of transferring knowledge from a fully-labeled source domain to an unlabeled target domain. Most UDA methods have focused on feature distribution alignment. Discrepancy-based methods explicitly compute the Maximum Mean Discrepancy (MMD) <ref type="bibr" target="#b25">[26]</ref> between the source and the target to align the two domains <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b68">69,</ref><ref type="bibr" target="#b45">46]</ref>. Long et al. <ref type="bibr" target="#b46">[47]</ref> proposed to align the joint distributions using the Joint MMD criterion. Sun et al. <ref type="bibr" target="#b63">[64]</ref> and Zhuo et al. <ref type="bibr" target="#b79">[80]</ref> further proposed to align second-order statistics of source and target features. With the development of Generative Adversarial Networks <ref type="bibr" target="#b22">[23]</ref>, additional papers proposed to perform domain alignment in the feature space with adversarial learning <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b67">68,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b73">74,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b61">62]</ref>. Recently, image translation methods, e.g. <ref type="bibr" target="#b78">[79,</ref><ref type="bibr" target="#b41">42]</ref> have been adopted to further improve domain adaptation by performing pixel-level alignment <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b75">76,</ref><ref type="bibr" target="#b60">61,</ref><ref type="bibr" target="#b62">63]</ref>. Instead of explicit feature alignment, Saito et al. <ref type="bibr" target="#b59">[60]</ref> proposed minimax entropy for adaptation. While these methods have full supervision on the source domain, similar to <ref type="bibr" target="#b38">[39]</ref>, we focus on a new adaptation setting with few source labels. Self-supervised Learning. Self-supervised learning (SSL) is a subset of unsupervised learning methods where supervision is automatically generated from the data <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b76">77,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b70">71]</ref>. One of the most common strategies for   SSL is handcrafting auxiliary pretext tasks predicting future, missing or contextual information. In particular, image colorization <ref type="bibr" target="#b76">[77,</ref><ref type="bibr" target="#b39">40]</ref>, patch location prediction <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14]</ref>, image jigsaw puzzle <ref type="bibr" target="#b50">[51]</ref>, image inpainting <ref type="bibr" target="#b53">[54]</ref> and geometric transformations <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b15">16]</ref> have been shown to be helpful. Currently, contrastive learning <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b65">66,</ref><ref type="bibr" target="#b48">49]</ref> has achieved state-of-the-art performance on representation learning <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b10">11]</ref>. Most contrastive methods are instance-wise, aiming to learn an embedding space where samples from the same instance are pulled closer and samples from different instances are pushed apart <ref type="bibr" target="#b72">[73,</ref><ref type="bibr" target="#b9">10]</ref>. Recently, contrastive learning based on prototypes has shown promising results in representation learning <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b18">19]</ref>.</p><formula xml:id="formula_0">v F / 7 x + C t F 1 k H G p U m C S L g 5 F q c C Q 4 L w A P O S a U R B T S w j V 3 G b F d E w 0 o W B r q t g S v O U v r 5 J O o + 5 d 1 B v 3 l 7 X m T V F H G Z 2 g U 3 S O P H S F m u g O t V A b U a T Q M 3 p F b 0 7 q v D j v z s d i t O Q U O 8 f o D 5 z P H 4 N 9 k f 4 = &lt; / l a t e x i t &gt; L cls &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " d u 7 7 z h O R j S o 3 T W U u Q 5 r H x S + D 4 f M = " &gt; A A A C A 3 i c b V D L S s N A F J 3 U V 6 2 v q O B C N 4 N F c F W S u t B l q R s X L l q w D 2 h D m E w n 7 d C Z J M x M h B I C b v w V N y K K u H X n F 7 h z 4 7 c 4 S b v Q 1 g M D Z 8 6 5 l 3 v v 8 S J G p b K s L 6 O w t L y y u l Z c L 2 1 s b m 3 v m L t 7 b R n G A p M W D l k o u h 6 S h N G A t B R V j H Q j Q R D 3 G O l 4 4 8 v M 7 9 w S I W k Y 3 K h J R B y O h g H 1 K U Z K S 6 5 5 2 O d I j T B i y X X q J v l H 8 A Q z m a a u W b Y q V g 6 4 S O w Z K d c O m t / 0 q f 7 R c M 3 P / i D E M S e B w g x J 2 b O t S D k J E o p i R t J S P 5 Y k Q n i M h q S n a Y A 4 k U 6 S 3 5 D C E 6 0 M o B 8 K / Q I F c / V 3 R 4 K 4 l B P u 6 c p s S T n v Z e J / X i 9 W / o W T 0 C C K F Q n w d J A f M 6 h C m A U C B 1 Q Q r N h E E 4 Q F 1 b t C P E I C Y a V j K + k Q 7 P m T F 0 m 7 W r H P K t W m T q M O p i i C I 3 A M T o E N z k E N X I E G a A E M 7 s A D e A Y v x r 3 x a L w a b 9 P S g j H r 2 Q d / Y L z / A D / 0 n E A = &lt; / l a t e x i t &gt; unlabeled unlabeled f t &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " n J y X j J w U 2 8 p R K l Z q Q / w u M b + 6 M 5 U = " &gt; A A A B 8 3 i c b V C 7 T s M w F L 0 p r x J e B U Y W i w q J q U r K A A u i g o W x S P Q h N a F y X K e 1 6 j i R 7 S B V U X + D h Q E E r H w H O w v i b 3 D a D t B y J E t H 5 9 y r e 3 y C h D O l H e f b K i w t r 6 y u F d f t j c 2 t 7 Z 3 S 7 l 5 T x a k k t E F i H s t 2 g B X l T N C G Z p r T d i I p j g J O W 8 H w K v d b 9 1 Q q F o t b P U q o H + G + Y C E j W B v J 8 y K s B 0 G Y h e M 7 3 S 2 V n Y o z A V o</formula><formula xml:id="formula_1">v b N b 2 N u v m y j R D G o s E p F u B t S A 4 A p q y F F A M 9 Z A Z S C g E Q y v J 3 7 j A b T h k b r D U Q w d S f u K h 5 x R t N L 9 s I 3 w h O m p B K r M u F s o e i V v C n e R + B k p k g z V b u G r 3 Y t Y I k E h E 9 S Y l u / F 2 E m p R s 4 E j P P t x E B M 2 Z D 2 o W W p o h J M J 5 1 e P X a P r d J z w 0 j b U u h O 1 d 8 T K Z X G j G R g O y X F g Z n 3 J u J / X i v B 8 L K T c h U n C I r N F o W J c D F y J x G</formula><p>Self-supervised Learning for Domain Adaptation. Selfsupervision-based methods incorporate SSL losses into the original task network <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21]</ref>. Reconstruction was first utilized as self-supervised task in some early works <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21]</ref>, in which source and target share the same encoder to extract domain-invariant features. To capture both domain-specific and shared properties, Bousmalis et al. <ref type="bibr" target="#b4">[5]</ref> explicitly extracts image representations into two spaces, one private for each domain and one shared across domains. In <ref type="bibr" target="#b5">[6]</ref>, solving jigsaw puzzle <ref type="bibr" target="#b50">[51]</ref> was leveraged as a self-supervision task to solve domain adaptation and generalization. Sun et al. <ref type="bibr" target="#b64">[65]</ref> further proposed to perform adaptation by jointly learning multiple self-supervision tasks. The feature encoder is shared by both source and target images, and the extracted features are then fed into different self-supervision task heads. Recently, based on instance discrimination <ref type="bibr" target="#b72">[73]</ref>, Kim et al. <ref type="bibr" target="#b38">[39]</ref> proposed a cross-domain SSL approach for adaptation with few source labels. SSL has also been incorporated for adaptation in other fields, including point cloud recognition <ref type="bibr" target="#b0">[1]</ref>, medical imaging <ref type="bibr" target="#b32">[33]</ref>, action segmentation <ref type="bibr" target="#b8">[9]</ref>, robotics <ref type="bibr" target="#b33">[34]</ref>, facial tracking <ref type="bibr" target="#b74">[75]</ref>, etc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Approach</head><p>In few-shot unsupervised domain adaptation, we are given a very limited number of labeled source images</p><formula xml:id="formula_2">D s = {(x s i , y s i )} Ns i=1 , as well as unlabeled source images D su = {(x su i )} Nsu i=1 .</formula><p>In the target domain, we are only given unlabeled target images</p><formula xml:id="formula_3">D tu = {(x tu i )} Ntu i=1</formula><p>. The goal is to train a model on D s , D su , and D tu ; and evaluate on D tu .</p><p>The base model consists of a feature encoder F , a 2 normalization layer, which outputs a normalized feature vector f ? R d and a cosine similarity-based classifier C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">In-domain Prototypical Contrastive Learning</head><p>We learn a shared feature encoder F that extracts discriminative features in both domains. Instance Discrimination <ref type="bibr" target="#b72">[73]</ref> is employed in <ref type="bibr" target="#b38">[39]</ref> to learn discriminative features. As an instance-wise contrastive learning method, it results in an embedding space where all instances are well separated. Despite promising results, instance discrimination has a fundamental weakness: the semantic structure of the data is not encoded by the learned representations. This is because two instances are treated as negative pairs as long as they are from different samples, regardless of their semantics. For a single domain, ProtoNCE <ref type="bibr" target="#b40">[41]</ref> is proposed to learn semantic structure of the data by performing iterative clustering and representation learning. The goal is to drive features within the same cluster to become more aggregated and features in different clusters further apart.</p><p>However, naively applying ProtoNCE to D s ? D su ? D tu in our domain adaptation setting would cause potential problems. Primarily due to the domain shift, images of different classes from different domains can be incorrectly aggregated into the same cluster, and images of the same class from different domains can be mapped into clusters that are far apart. To mitigate these problems, we propose to perform prototypical contrastive learning separately in D s ? D su and in D tu . This aims to prevent the incorrect clustering of images across domains and indiscriminative feature learning.</p><p>Specifically, two memory banks V s and V t are maintained for source and target respectively:</p><formula xml:id="formula_4">V s = [v s 1 , ? ? ? , v s (Ns+Nsu) ], V t = [v t 1 , ? ? ? , v t Ntu ],<label>(1)</label></formula><p>where v i is the stored feature vector of x i , initialized with f i and updated with a momentum m after each batch:</p><formula xml:id="formula_5">v i ? mv i + (1 ? m)f i .<label>(2)</label></formula><p>In order for in-domain prototypical contrastive learning, kmeans clustering is performed on V s and V t to get source clusters C s = {C </p><formula xml:id="formula_6">u s j = 1 |C (s) j | v s i ?C (s) j v s i .</formula><p>We explain only on the source domain for succinct notation, all operations are performed on target similarly.</p><p>During training, with the feature encoder F , we compute a feature vector f s i = F (x s i ). To perform in-domain prototypical contrastive learning, we compute the similarity distribution vector between f s i and {? s j } k j=1 as P s i = [P s i,1 , P s i,2 , . . . , P s i,k ], with</p><formula xml:id="formula_7">P s i,j = exp(? s j ? f s i /?) k r=1 exp(? s r ? f s i /?) ,<label>(3)</label></formula><p>where ? is a temperature value determining the level of concentration. Then the in-domain prototypical contrastive loss can be written as:</p><formula xml:id="formula_8">L PC = Ns+Nsu i=1 L CE (P s i , c s (i))+ Ntu i=1 L CE (P t i , c t (i)) (4)</formula><p>where c s (?) and c t (?) return the cluster index of the instance. Due to the randomness in clustering, we perform kmeans on the samples M times with different number of clusters {k m } M m=1 . Moreover, in the FUDA setting, since the number of classes n c is known, we set k m = n c for most m. The overall loss for in-domain self-supervision is:</p><formula xml:id="formula_9">L InSelf = 1 M M m=1 L (m) PC<label>(5)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Cross-domain Instance-Prototype SSL</head><p>In order to explicitly enforce learning domain-aligned and more discriminative features in both source and target domains, we perform cross-domain instance-prototype selfsupervised learning. Many previous works focus on domain alignment via discrepancy minimization or adversarial learning. However, these methods provide inferior performance or have unstable training. Moreover, most of them focus on distribution matching, without considering semantic similarity matching across domains. Instance-instance matching <ref type="bibr" target="#b38">[39]</ref> is proposed to match an instance i to another instance j in the other domain with the most similar representation. However, due to the domain gap, instances can be easily mapped to instances of different classes in the other domain. In some cases, if an outlier in one domain is extremely close to the other domain, it will be matched to all the instances in the other domain, as illustrated in <ref type="figure" target="#fig_4">Figure 3</ref>.</p><p>Instead, our method discovers positive matching as well as negative matchings between instance and cluster prototypes in different domains. To find a matching for an instance i, we perform entropy minimization on the similarity distribution vector between its representation, e.g. f s i and the centroids of the other domain, e.g. {? t j } k j=1 . Specifically, given feature vector f s i in the source domain, and centroids {? t j } k j=1 in the target domain, we first compute the similarity distribution vector P s )t</p><formula xml:id="formula_10">i = [P s )t i,1 , . . . , P s )t i,k ], in which P s )t i,j = exp(? t j ? f s i /? ) k r=1 exp(? t r ? f s i /? ) .<label>(6)</label></formula><p>Then we minimize the entropy of P s )t i , which is:</p><formula xml:id="formula_11">H(P s )t i ) = ? k j=1 P s )t i,j log P s )t i,j .<label>(7)</label></formula><p>Similarly, we can compute H(P t )s i ), and the final loss for cross-domain instance-prototype SSL is:</p><formula xml:id="formula_12">L CrossSelf = Ns+Nsu i=1 H(P s )t i ) + Ntu i=1 H(P t )s i ) (8)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Adaptive Prototypical Classifier Learning</head><p>The goal of this section is to learn a better domainaligned, discriminative feature encoder F and more importantly, a cosine classifier C that could achieve high accuracy on the target domain.</p><p>The cosine classifier C consists of weight vectors W = [w 1 , w 2 , . . . , w nc ], where n c denotes the total number of classes, and a temperature T . The output of C, <ref type="bibr" target="#b0">1</ref> T W T f is fed into a softmax layer ? to obtain the final probabilistic output p(x) = ?( 1 T W T f ). With the availability of the labeled set D s , it is straightforward to train F and C with a standard cross-entropy loss for classification:</p><formula xml:id="formula_13">L cls = E (x,y)?Ds L CE (p(x), y)<label>(9)</label></formula><p>However, since D s is quite small under FUDA setting, only training with L cls is hard to get a C with high performance on the target.</p><p>Adaptive Prototype-Classifier Update (APCU) Note that for C to classify samples correctly, the direction of a weight vector w i needs to be representative of features of the corresponding class i. This indicates that the meaning of w i coincide with the ideal cluster prototype of class i. We propose to use an estimate of the ideal cluster prototypes to update W. Yet the computed {? s j } and {? t j } cannot be naively used for this purpose, not only because the correspondence between {w i } and {? j } is unknown, but also the k-means result may contain very impure clusters, leading to non-representative prototypes.</p><p>We use the few-shot labeled data as well as samples with high-confident predictions to estimate the prototype for each class. Formally, we define D tu . Then the estimate of w i from source and target domain can be computed as:</p><formula xml:id="formula_14">w s i = 1 |D (i) s + | x?D (i) s + V s (x);? t i = 1 |D (i) tu | x?D (i) tu V t (x) (10) where D (i) s + = D (i) s ? D (i)</formula><p>su and V (x) returns the representation in memory bank corresponding to x.</p><p>With only few labeled samples in source, it is hard to learn a representative prototype shared across domains. Instead of directly employing a global prototype for a class i, we further propose to update w i in an domain adaptive manner, with? s i during early training stage and with? t i at later stage. This is because that? s i is more robust in early training stage due to the few labeled source samples, while? t i would be more representative later for target domain to get better adaptation performance. Specifically, we use |D (i) tu | to determine whether? t i is robust to use:</p><formula xml:id="formula_15">w i = unit(? s i ) if |D (i) tu | &lt; t w unit(? t i ) otherwise (11)</formula><p>where unit(?) normalizes the input vector and t w is a threshold hyper-parameter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mutual Information Maximization</head><p>In order for the above unified prototype-classifier learning paradigm to work well, the network is desired to have enough confident predictions, e.g. |D (i) | &gt; t w , for all classes to get robust? s i and? t i for i = 1, . . . , n c . First, to promote the network to have diversified outputs over the dataset, we maximize the entropy of expected network prediction H(E x?D [p(y|x; ?)]), where ? denotes learnable parameters in both F and C, and D = D s ?D su ?D tu . Second, in order to get high-confident prediction for each sample, we leverage entropy minimization on the network output which has shown efficacy in label-scarce scenarios <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b3">4]</ref>. These two desired behaviors turn out to be equivalent to maximizing the mutual information between input and output:</p><formula xml:id="formula_16">I(y; x) = H(p 0 ) ? E x [H(p(y|x; ?))],<label>(12)</label></formula><p>where the prior distribution p 0 is given by E x [p(y|x; ?)], and the detailed derivation is presented in the supplementary material. We can get the objective as:</p><formula xml:id="formula_17">L MIM = ?I(y; x)<label>(13)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">PCS Learning for FUDA</head><p>The PCS learning framework performs in-domain prototypical contrastive learning, cross-domain instanceprototype self-supervised learning, and unified adaptive prototype-classifier learning. Together with APCU in Eq. 11, the overall learning objective is:   than Office, which consists of 4 domains (Art, Clipart, Product, Real) in 65 classes. Following <ref type="bibr" target="#b38">[39]</ref>, we look into the settings with 3% and 6% labeled source images per class, which means each class has 2 to 4 labeled images on average. VisDA-2017 <ref type="bibr" target="#b55">[56]</ref> is a challenging simulation-to-real dataset containing over 280K images across 12 classes. We validate our model on settings with 0.1% and 1% labeled source images per class as suggested in <ref type="bibr" target="#b38">[39]</ref>. Domain-Net <ref type="bibr" target="#b54">[55]</ref> is a large-scale domain adaptation benchmark. Since some domains and classes are noisy, we follow <ref type="bibr" target="#b59">[60]</ref> and use a subset containing four domains (Clipart, Real, Painting, Sketch) with 126 classes. We show results on settings with 1-shot and 3-shots source labels on this dataset. Implementation Details. We use ResNet-101 <ref type="bibr" target="#b29">[30]</ref> (for DomainNet) and ResNet-50 (for other datasets) pre-trained on ImageNet <ref type="bibr" target="#b56">[57]</ref> as our backbones. To enable a fair comparison with <ref type="bibr" target="#b38">[39]</ref>, we replaced the last FC layer with a 512-D randomly initialized linear layer. L2-normalizing are performed on the output features. We use k-means GPU implementation in faiss <ref type="bibr" target="#b36">[37]</ref> for efficient clustering. We use SGD with momentum of 0.9, a learning rate of 0.01, a batch size of 64. More implementation details can be found in the supplementary material.</p><formula xml:id="formula_18">L P CS = L cls + ? in ? L InSelf + ? cross ? L CrossSelf + ? mim ? L MIM<label>(</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Results on FUDA</head><p>Baselines. SO is a model only trained using the labeled source images. CDAN <ref type="bibr" target="#b44">[45]</ref> and MDDIA <ref type="bibr" target="#b34">[35]</ref> are both state-of-the-art methods in UDA with a domain classifier to perform domain alignment. MME <ref type="bibr" target="#b59">[60]</ref> minimizes the conditional entropy of unlabeled target data with respect to the feature extractor and maximizes it with respect to the classifier. CAN <ref type="bibr" target="#b37">[38]</ref> uses clustering information to contrast discrepancy of source and target domain. CDS [39] is a instance-based cross-domain self-supervised pre-training, which can be used for other domain adaptation methods and form two-stage methods, such as CDS / CDAN and CDS / MME. We re-implement CDS into an end-to-end version by adding losses in two stage together and tuning the weight for different losses. We also investigate the one-stage version of the methods above (CDS + CDAN, CDS + MME). Following <ref type="bibr" target="#b38">[39]</ref>, entropy minimization (ENT) on source is  added to previous DA methods to obtain better baseline performance. We compare the proposed PCS with state-of-the-art methods on FUDA (adaptation with few source labels). Ex-   <ref type="table" target="#tab_1">Table 1</ref>, 3, 4, and 5, respectively. We can see that PCS outperforms the best state-of-the-arts in all the benchmarks, with large improvements: 10.5% and 3.4% on Office, 4.3% and 4.2% on Office-Home, 9.0% and 0.7% on VisDA, 13.2% and 3.6% on DomainNet. If we look at the result of each domain pair in each dataset (e.g. D ? A in Office), PCS outperforms previous best in 47 out of 52 settings. Finally, as the number of labeled samples decreases, PCS shows larger performance improvements against the previous best methods, which demonstrates PCS is extremely beneficial in label-scarce adaptation scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation Study and Analysis</head><p>Next, we investigate the effectiveness of each component in PCS on Office. <ref type="table" target="#tab_2">Table 2</ref> shows that adding each component contributes to the final results without any performance degradation. Comparing the last row in <ref type="table" target="#tab_2">Table 2</ref> and <ref type="table" target="#tab_1">Table 1</ref>, we can see even without MIM, PCS still outperforms all previous methods.</p><p>We plot the learned features with t-SNE <ref type="bibr" target="#b47">[48]</ref> on the DSLR-to-Amazon setting in Office, and Real-to-Clipart in Office-Home respectively in left and right of <ref type="figure" target="#fig_6">Figure 4</ref>. In the top row, the color represents the class of each sample; while in the bottom row, cyan represents source samples and red represents target samples. Compared to ImageNet pre-training and CDS, it qualitatively shows that PCS well clusters samples with the same class in the feature space; thus, PCS favors more discriminative features. Also, the features from PCS are more closely aggregated than Ima-geNet pre-training and CDS, which demonstrates that PCS learns a better semantic structure of the datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Sample Efficiency</head><p>We compare our method with other state-of-the-art methods on Office dataset (DSLR as source and Amazon as target) with a varying number of source labels. From <ref type="figure" target="#fig_7">Figure 5</ref>, we can see that PCS outperforms all SOTA methods in all settings with different number of labeled samples. Moreover, our method is very label-efficient: a) For 1-shot image per class (31 labeled source images in total), PCS can achieve 76.1% accuracy. b) For the fully-labeled setting (498 labeled source images in total), PCS can achieve 77.4% accuracy. c) With 94% less labeled source images, the performance degradation of our method is only 1.3%.</p><p>In short, with less labeled source data, PCS outperforms other works by a larger margin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we investigated Few-shot Unsupervised Domain Adaptation where only few labeled samples are available in the source domain, and no labeled samples in the target domain. We proposed a novel Prototypical Crossdomain Self-supervised learning (PCS) framework that performs both in-domain and cross-domain prototypical selfsupervised learning, as well as adaptive prototpe-classifier learning. We perform extensive experiments on multiple benchmark datasets, which demonstrates the superiority of PCS over previous best methods. PCS sets a new state of the art for Few-shot Unsupervised Domain Adaptation. Appendix A. Proof of Equation <ref type="formula" target="#formula_4">(13)</ref> As mentioned in Section 3.2 of the main paper, in order for the prototype-classifier learning paradigm to work well, the network is desired to have enough confident predictions for all classes to get robust? s i and? t i . First, to promote the network to have diversified outputs, we propose to maximize the entropy of expected network prediction H(E x [p(y|x; ?)]). Second, to get high-confident prediction for each sample, we perform entropy minimization on the network output. So the overall objective is:</p><formula xml:id="formula_19">max H(E x [p(y|x; ?)]) ? E x [H(p(y|x; ?))].<label>(15)</label></formula><p>Now we show that this objective equals maximizing the mutual information between input and output, i.e. I(y; x):</p><formula xml:id="formula_20">H(E x [p(y|x; ?)]) ? E x [H(p(y|x; ?))]<label>(16)</label></formula><formula xml:id="formula_21">=E x L i=1 p(y i |x) log p(y i |x) ? L i=1 E x [p(y i |x)] log E x [p(y i |x)]]<label>(17)</label></formula><formula xml:id="formula_22">=E x L i=1 p(y i |x) log p(y i |x) ? E x L i=1 p(y i |x) log E x [p(y i |x)]<label>(18)</label></formula><formula xml:id="formula_23">=E x L i=1 p(y i |x) log p(y i |x) E x [p(y i |x)]<label>(19)</label></formula><formula xml:id="formula_24">=E x p(y|x) log p(y|x) E x [p(y|x)] dy (20) = p(x) dx p(y|x) log p(y|x) p(x)p(y|x) dx dy (21) = p(x) dx p(y|x) log p(y|x) p(y) dy<label>(22)</label></formula><p>= p(y, x) log p(y, x) p(y)p(x) dy dx = I(y; x)</p><p>In addition, we estimate H(E x [p(y|x; ?)]) with x?D p(y|x; ?) logp 0 , wherep 0 is a moving average of p(y|x; ?).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Additional Datasets Details</head><p>Overall statistics of the datasets and the number of labeled source examples used in our experiments can be found in <ref type="table" target="#tab_6">Table 6</ref>. For Office <ref type="bibr" target="#b58">[59]</ref>, Office-Home <ref type="bibr" target="#b69">[70]</ref> and VisDA <ref type="bibr" target="#b55">[56]</ref>, we follow the same setting in <ref type="bibr" target="#b38">[39]</ref>, randomly sampling labeled images from the source domain and ensure that each class has at least one labeled example. For DomainNet <ref type="bibr" target="#b54">[55]</ref>, we use the same split files as <ref type="bibr" target="#b59">[60]</ref> and further select 1-shot and 3-shots labeled samples in the training set for each class.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Additional Implementation Details</head><p>We implemented our model in PyTorch <ref type="bibr" target="#b52">[53]</ref>. We choose batch size of 64 for both source and target in self-supervised learning and batch size of 32 for the classification loss. The learning rate ratio between linear layer and convolution layer is set to 1 : 0.1. We use SGD with weight decay rate 5e ?4 . For Office and Office-Home, we adaptively  set temperature ? according to <ref type="bibr" target="#b40">[41]</ref>. For VisDA and Do-mainNet, we fix ? to be 0.1 for more stable training. We set temperature ? to be 0.1 in all experiments. We choose hyper-parameters ? in and ? cross ? {1, 0.5}, and the weight ? mim ? {0.05, 0.01}. As for parameters m (momentum for memory bank update) and M (number of k-means in L InSelf ), we set m = 0.5 and M = 20.</p><p>We use spherical k-means for clustering and set half of the number of clusters in k-means to be the number of the classes n c , and the rest to be 2n c . We compute the weight for cosine classifier only using source images for the first 5 epochs and set t w to be around half of the average number of images per class. New prototypes (i.e. centroids of clusters and weights of cosine classifier) are computed per epoch for both self-supervised learning and classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Stability Analysis of PCS</head><p>To show the performance stability of PCS, we conduct multiple runs with three different random seeds. <ref type="table" target="#tab_7">Table 7</ref> reports the averaged accuracy and standard deviation of the three runs on the 1-shot and 3-shots settings of Office. <ref type="figure" target="#fig_8">Figure 6</ref> shows adaptation accuracy vs. training epochs using cosine classifier <ref type="figure" target="#fig_8">(Figure 6a</ref>) and weighted kNN classifier <ref type="figure" target="#fig_8">(Figure 6b</ref>). From the plots, we have the following observations. (1) The target accuracy of PCS increases more steadily and robustly compared to other methods.</p><p>In <ref type="figure" target="#fig_8">Figure 6a</ref>, CDS starts decreasing at Epoch 3. In <ref type="figure" target="#fig_8">Figure 6b</ref>, CDS and CDS+ENT starts decreasing at Epoch 1; while CDS+ENT+MME decreases from the beginning of training. In contrast, the performance of PCS increases smoothly until the end of training. (2) PCS converges much faster than other methods. We can see in <ref type="figure" target="#fig_8">Figure 6a</ref> that PCS plateaus at around Epoch 3, while CDS+ENT and CDS+ENT+MME reaches the best performance at Epoch 9 and 10.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Quantitative Feature Analysis</head><p>To quantitatively compare the quality of learned features with different approaches, we perform classification with weighted k-nearest neighbor (kNN) classifier proposed by Wu et al. <ref type="bibr" target="#b72">[73]</ref> in a cross-domain manner. Specifically, given a test image x t , we first compute its normalized feature f t = F (x t ), and then compare it again embeddings of all source images in the source memory bank V s using cosine similarity s i = cos(f t , v s i ). The top k nearest neighbors in the source domain, N k , would be used to make the final prediction with weighted voting. Specifically, class c would get weight w c = i?N k ? i ? 1(c i = c), in which ? i is the contributing weight of neighbor v s i defined as ? i = exp(s i /? ). We set ? = 0.07 and k = 200 as in <ref type="bibr" target="#b72">[73]</ref>.</p><p>We perform the above cross-domain kNN classification on models trained with 1) only cross-domain self- supervised learning methods, and 2) Few-shot Unsupervised Domain Adaptation methods, with the results shown in <ref type="table" target="#tab_8">Table 8</ref> and <ref type="table" target="#tab_9">Table 9</ref>, respectively. From the results, we can see that both the proposed cross-domain prototypical self-supervised learning method and the whole PCS framework outperforms previous approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Prototype Quality Comparison</head><p>To further compare how well source and target are aligned, we provide more t-SNE <ref type="bibr" target="#b47">[48]</ref> visualizations on Office (D?A) and Office-Home (Rw?Cl) in <ref type="figure">Figure 7a</ref>     cles for source and crosses for target, best view after zooming in). In bottom rows of both figures, the number of a prototype represents its class index, and color represent the domain of the prototype (Cyan for source, Red for target, and Black for prototype weight of the classifier). As we can see from <ref type="figure">Figure 7</ref>, for each class, the prototypes of source, target and the weight vector of classifier get more aggregated with PCS than other methods, which demonstrates that PCS could better align source and target representations for each category.</p><p>In a well-learned feature embedding space, prototypes of different classes should be far / different from each other. To quantitatively measure the similarity of the learned prototypes, we compute the sum of cosine similarities between all pairs of prototypes. From the results shown in <ref type="table" target="#tab_1">Table 10</ref>,   we can see that the prototypes learned with PCS have the least similarities, indicating that PCS learns an embedding space with better semantic structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Image Retrieval Results</head><p>We present cross-domain image retrieval results in <ref type="figure" target="#fig_10">Figure 8</ref>. Given a query feature f q in the target domain, we measure the pairwise cosine similarity between f q and all features in the source domain. The source images with the most similar features as f q are returned as the top retrieval results. We compare image retrieval results of PCS with CDS in <ref type="figure" target="#fig_10">Figure 8</ref>. As shown in <ref type="figure" target="#fig_10">Figure 8</ref>, features from model trained with CDS are biased to some wrong attributes, e.g. color, texture and other visual clues; and quantitatively similar features do not correspond to semantically similar images in different domains. In contrast, we can see that PCS could extract features that are more discriminative and semantically meaningful across domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H. More Ablation Study Results</head><p>In this section, we provide more ablation study results. Ablation experiments similar to <ref type="table" target="#tab_2">Table 2</ref> in the main paper are performed on Office-Home, with results shown in <ref type="table" target="#tab_1">Table 11</ref>. As we can see in the table, adding each component contributes to the final adaptation accuracy without any performance degradation, which demonstrates the effectiveness of all components in our PCS framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. Performance Comparison with UDA Methods using Full Source Labels</head><p>We have shown the superiority of PCS in label-scarce setting (FUDA), and we further conduct experiments with fully-labeled source domain (UDA). The performance comparison with other UDA methods on Office and Office-Home are presented in <ref type="table" target="#tab_1">Table 13</ref> and <ref type="table" target="#tab_1">Table 12</ref>, respectively. We can see that PCS achieves the best results even with fully-labeled source, which demonstrates that the proposed PCS could potentially be applied to a wider range of domain adaptation settings.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>1 &lt;s 2 &lt;t 1 &lt;t 2 &lt; l a t e x i t s h a 1 _ b a s e 6 4 =&lt; l a t e x i t s h a 1 _&lt; l a t e x i t s h a 1 _ b a s e 6 4 =</head><label>121214114</label><figDesc>l a t e x i t s h a 1 _ b a s e 6 4 = " x / M n b J + z o b X k v o / A v L M 3 p L W l L s A = " &gt; A A A B 9 X i c b V D L T g I x F L 3 j E / G F u n T T Q E x Y k R l c 4 J L o x i U m 8 k h g I J 3 S g Y Z O Z 9 J 2 M G T C f 5 g Y F x r j 1 u 9 w 6 8 7 o x 9 g B F g q e p M n J O f f m n h 4 v 4 k x p 2 / 6 0 1 t Y 3 N r e 2 M z v Z 3 b 3 9 g 8 P c 0 X F D h b E k t E 5 C H s q W h x X l T N C 6 Z p r T V i Q p D j x O m 9 7 o K v W b Y y o V C 8 W t n k T U D f B A M J 8 R r I 3 U 7 Q R Y D z 0 / G U 9 7 T l f 1 c g W 7 Z M + A V o m z I I V q v v j 9 V X l / q P V y H 5 1 + S O K A C k 0 4 V q r t 2 J F 2 E y w 1 I 5 x O s 5 1 Y 0 Q i T E R 7 Q t q E C B 1 S 5 y S z 1 F J 0 Z p Y / 8 U J o n N J q p v z c S H C g 1 C T w z m a Z U y 1 4 q / u e 1 Y + 1 f u A k T U a y p I P N D f s y R D l F a A e o z S Y n m E 0 M w k c x k R W S I J S b a F J U 1 J T j L X 1 4 l j X L J O S + V b 0 w b l z B H B k 4 h D 0 V w o A J V u I Y a 1 I G A h H t 4 g m f r z n q 0 X q z X + e i a t d g 5 g T + w 3 n 4 A 5 w K W f w = = &lt; / l a t e x i t &gt; v l a t e x i t s h a 1 _ b a s e 6 4 = " o 9 a j 6 m + V U 5 V V 7 6 k i V 8 / o U L q 0 U q 0 = " &gt; A A A B 9 X i c b V D L T g I x F L 3 j E / G F u n T T Q E x Y k R l c 4 J L o x i U m 8 k h g I J 3 S g Y Z O Z 9 J 2 M G T C f 5 g Y F x r j 1 u 9 w 6 8 7 o x 9 g B F g q e p M n J O f f m n h 4 v 4 k x p 2 / 6 0 1 t Y 3 N r e 2 M z v Z 3 b 3 9 g 8 P c 0 X F D h b E k t E 5 C H s q W h x X l T N C 6 Z p r T V i Q p D j x O m 9 7 o K v W b Y y o V C 8 W t n k T U D f B A M J 8 R r I 3 U 7 Q R Y D z 0 / G U 9 7 5 a 7 q 5 Q p 2 y Z 4 B r R J n Q Q r V f P H 7 q / L + U O v l P j r 9 k M Q B F Z p w r F T b s S P t J l h q R j i d Z j u x o h E m I z y g b U M F D q h y k 1 n q K T o z S h / 5 o T R P a D R T f 2 8 k O F B q E n h m M k 2 p l r 1 U / M 9 r x 9 q / c B M m o l h T Q e a H / J g j H a K 0 A t R n k h L N J 4 Z g I p n J i s g Q S 0 y 0 K S p r S n C W v 7 x K G u W S c 1 4 q 3 5 g 2 L m G O D J x C H o r g Q A W q c A 0 1 q A M B C f f w B M / W n f V o v V i v 8 9 E 1 a 7 F z A n 9 g v f 0 A 6 I i W g A = = &lt; / l a t e x i t &gt; v s n &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 0 i p o G M v C e 5 w H z i B f 1 n i i N M L n u V w = " &gt; A A A B + X i c b V D L S s N A F L 3 x W e s r 6 t J N a B G 6 K k l d 1 G X R j c s K 9 g F t D J P p p B 0 6 m Y S Z S a G E f I l u X C j i 1 q 9 w 6 0 7 0 Y 5 y 0 X W j r g Y H D O f d y z x w / Z l Q q 2 / 4 0 1 t Y 3 N r e 2 C z v F 3 b 3 9 g 0 P z 6 L g t o 0 R g 0 s I R i 0 T X R 5 I w y k l L U c V I N x Y E h T 4 j H X 9 8 l f u d C R G S R v x W T W P i h m j I a U A x U l r y T L M f I j X y g 3 S S e S n P 7 q R n l u 2 q P Y O 1 S p w F K T d K l e + v + v t 9 0 z M / + o M I J y H h C j M k Z c + x Y + W m S C i K G c m K / U S S G O E x G p K e p h y F R L r p L H l m n W l l Y A W R 0 I 8 r a 6 b + 3 k h R K O U 0 9 P V k n l M u e 7 n 4 n 9 d L V H D h p p T H i S I c z w 8 F C b N U Z O U 1 W A M q C F Z s q g n C g u q s F h 4 h g b D S Z R V 1 C c 7 y l 1 d J u 1 Z 1 z q u 1 G 9 3 G J c x R g F M o Q Q U c q E M D r q E J L c A w g Q d 4 g m c j N R 6 N F + N 1 P r p m L H Z O 4 A + M t x + K W Z f 5 &lt; / l a t e x i t &gt; t e x i t s h a 1 _ b a s e 6 4 = " 4 y 0 g Z H T 3 L X t 0 1 O k v O S U V t d m J I e k = " &gt; A A A B + X i c b V D L S s N A F L 3 x W e s r 6 t J N a B G 6 K k l d 1 G X R j c s K 9 g F t D J P p p B 0 6 m Y S Z S a G E f I l u X C j i 1 q 9 w 6 0 7 0 Y 5 y 0 X W j r g Y H D O f d y z x w / Z l Q q 2 / 4 0 1 t Y 3 N r e 2 C z v F 3 b 3 9 g 0 P z 6 L g t o 0 R g 0 s I R i 0 T X R 5 I w y k l L U c V I N x Y E h T 4 j H X 9 8 l f u d C R G S R v x W T W P i h m j I a U A x U l r y T L M f I j X y g 3 S S e S n P 7 p R n l u 2 q P Y O 1 S p w F K T d K l e + v + v t 9 0 z M / + o M I J y H h C j M k Z c + x Y + W m S C i K G c m K / U S S G O E x G p K e p h y F R L r p L H l m n W l l Y A W R 0 I 8 r a 6 b + 3 k h R K O U 0 9 P V k n l M u e 7 n 4 n 9 d L V H D h p p T H i S I c z w 8 F C b N U Z O U 1 W A M q C F Z s q g n C g u q s F h 4 h g b D S Z R V 1 C c 7 y l 1 d J u 1 Z 1 z q u 1 G 9 3 G J c x R g F M o Q Q U c q E M D r q E J L c A w g Q d 4 g m c j N R 6 N F + N 1 P r p m L H Z O 4 A + M t x + L 3 Z f 6 &lt; / l a t e x i t &gt; v l a t e x i t s h a 1 _ b a s e 6 4 = " 5 h T S o u I f c T D m q 4 E c t T V 4 7 T L n r k U = " &gt; A A A B + X i c b V C 7 S g N B F J 3 1 G e N r 1 d J m S B B S h d 1 Y x D J o Y x n B P C B Z l 9 n J b D J k 9 s H M 3 U B Y 9 k u 0 s V D E 1 q + w t R P 9 G G e T F J p 4 Y O B w z r 3 c M 8 e L B V d g W Z / G 2 v r G 5 t Z 2 Y a e 4 u 7 d / c G g e H b d V l E j K W j Q S k e x 6 R D H B Q 9 Y C D o J 1 Y 8 l I 4 A n W 8 c Z X u d + Z M K l 4 F N 7 C N G Z O Q I Y h 9 z k l o C X X N P s B g Z H n p 5 P M T e 3 s D l y z b F W t G f A q s R e k 3 C h V v r / q 7 / d N 1 / z o D y K a B C w E K o h S P d u K w U m J B E 4 F y 4 r 9 R L G Y 0 D E Z s p 6 m I Q m Y c t J Z 8 g y f a W W A / U j q F w K e q b 8 3 U h I o N Q 0 8 P Z n n V M t e L v 7 n 9 R L w L 5 y U h 3 E C L K T z Q 3 4 i M E Q 4 r w E P u G Q U x F Q T Q i X X W T E d E U k o 6 L K K u g R 7 + c u r p F 2 r 2 u f V 2 o 1 u 4 x L N U U C n q I Q q y E Z 1 1 E D X q I l a i K I J e k B P 6 N l I j U f j x X i d j 6 4 Z i 5 0 T 9 A f G 2 w 8 u s p e 9 &lt; / l a t e x i t &gt; v " o l X N t E x 3 u e r n q 3 O 0 + l X i h r w X N U s = " &gt; A A A B + X i c b V D L S s N A F J 3 4 r P U V d e k m t A h d l a Q u 6 r L o x m U F + 4 A 2 h s l 0 0 g 6 d T M L M T a G E f I l u X C j i 1 q 9 w 6 0 7 0 Y 5 y 0 X W j r g Y H D O f d y z x w / 5 k y B b X 8 a a + s b m 1 v b h Z 3 i 7 t 7 + w a F 5 d N x W U S I J b Z G I R 7 L r Y 0 U 5 E 7 Q F D D j t x p L i 0 O e 0 4 4 + v c r 8 z o V K x S N z C N K Z u i I e C B Y x g 0 J J n m v 0 Q w 8 g P 0 k n m p b X s D j y z b F f t G a x V 4 i x I u V G q f H / V 3 + + b n v n R H 0 Q k C a k A w r F S P c e O w U 2 x B E Y 4 z Y r 9 R N E Y k z E e 0 p 6 m A o d U u e k s e W a d a W V g B Z H U T 4 A 1 U 3 9 v p D h U a h r 6 e j L P q Z a 9 X P z P 6 y U Q X L g p E 3 E C V J D 5 o S D h F k R W X o M 1 Y J I S 4 F N N M J F M Z 7 X I C E t M Q J d V 1 C U 4 y 1 9 e J e 1 a 1 T m v 1 m 5 0 G 5 d o j g I 6 R S V U Q Q 6 q o w a 6 R k 3 U Q g R N 0 A N 6 Q s 9 G a j w a L 8 b r f H T N W O y c o D 8 w 3 n 4 A M D m X v g = = &lt; / l a t e x i t &gt; b a s e 6 4 = " W S G Z z J H / 2 b B 8 h l x O s P P L j X A Hi c U = " &gt; A A A C U H i c d V H L T h s x F L 0 T C o U p 0 L R d s r G g l V i N 5 t F C u g M q V V 2 C R A A p E y K P 5 0 6 w 4 v G M b A 9 q N M q n 8 C / 8 A B t 2 / Y O u y 4 Z F E T i J U g G i R 7 J 8 d M 6 9 9 v V x U g q u j e / / c h p z r + Y X X i 8 u u W + W V 1 b f N t + 9 P 9 J F p R i 2 W S E K d Z J Q j Y J L b B t u B J 6 U C m m e C D x O B t / G / v E 5 K s 0 L e W i G J X Z z 2 p c 8 4 4 w a K / W a / T j B P p d 1 k l O j + M + R 6 x I S W 3 6 W Z H W c V 6 N e c K r j + L k Y z s T z t D A v + I N T 7 b o x y v T f u b 3 m h u + F f i u K f O J 7 0 Z e t r 1 H L k u 0 g j D 5 v k c D z J 9 j Y + X j 5 / f e f i 9 3 9 X v M 6 T g t W 5 S g N E 1 T r T u C X p l t T Z T g T O H L j S m N J 2 Y D 2 s W O p p D n q b j 0 J Z E Q + W S U l W a H s k o Z M 1 M c d N c 2 1 H u a J r R y P r Z 9 7 Y / E l r 1 O Z r N W t u S w r g 5 J N L 8 o q Q U x B x u m S l C t k R g w t o U x x O y t h Z 1 R R Z u w f u D a E 2 U v J / 8 l R 6 A W R F x 7 Y N P Z g i k V Y g 3 X Y h A C 2 Y Q d + w D 6 0 g c E V 3 M B f u H O u n V v n vu F M S 2 c 7 f I A n a L g P K T O 4 V A = = &lt; / l a t e x i t &gt; " b i R S 6 r M I 8 H x s h e J 1 v 4 z P b e A z 0 g M = " &gt; A A A C U H i c b Z H P b h M x E M Z n Q 1 v K 0 j 8 B j l w s 2 k o 9 R b t J E 8 q t g I R 6 L B J p K 2 X T y O u d T a 1 4 v S t 7 t i J a 5 V F 4 F 1 6 A S 2 + 8 A W e 4 c C g C J 1 E K T R n J 8 q f f N 7 Z H n + N C S U t B 8 N W r P V h Z X X u 4 / s h / v L G 5 t V 1 / 8 v T U 5 q U R 2 B W 5 y s 1 5 z C 0 q q b F L k h S e F w Z 5 F i s 8 i 0 d v p / 7 Z F R o r c / 2 B x g X 2 M z 7 U M p W C k 0 O D + j C K c S h 1 F W e c j P w 4 8 X 3 G I q c v 4 7 S K s n I y C C 8 o i p Z h c w G v k p z s f X 9 0 Q b 4 f o U 5 u 7 x 3 U d 8 J G M C s W N N q d d q v d c a L 5 q t U 5 P G A L a + d o 9 / O 7 b 9 8 / v T 4 Z 1 K + j J B d l h p q E 4 t b 2 w q C g f s U N S a F w 4 k e l x Y K L E R 9 i z 0 n N M 7 T 9 a h b I h O 0 5 k r A 0 N 2 5 p Y j P 6 7 4 m K Z 9 a O s 9 h 1 T s e 2 y 9 4 U / s / r l Z Q e 9 i u p i 5 J Q i / l D a a k Y 5 W y a L k u k Q U F q 7 A Q X R r p Z m b j k h g t y f + C 7 E I K / I S y L R Q i n z U b Y a j T f u z T e w L z W 4 T m 8 g H 0 I 4 S U c w T G c Q B c E f I E f c A O / v G v v p / e 7 5 s 1 b F z s 8 g z t V 8 / 8 A H a S 4 S g = = &lt; / l a t e x i t &gt; t e x i t s h a 1 _ b a s e 6 4 = " p 3 u q b 3 y 4 A q Q X C G Y r 9 W 4 5 / a T Z c T M = " &gt; A A A B 9 X i c b V D L T g I x F L 2 D L 8 Q X 6 t J N A z F h R W Z w g U u i G 5 e Y y C O B g X R K B x o 6 n U n b 0 Z A J / 2 F i X G i M W 7 / D r T u j H 2 M H W C h 4 k i Y n 5 9 y b e 3 q 8 i D O l b f v T y q y t b 2 x u Z b d z O 7 t 7 + w f 5 w 6 O m C m N J a I O E P J R t D y v K m a A N z T S n 7 U h S H H i c t r z x Z e q 3 b q l U L B Q 3 e h J R N 8 B D w X x G s D Z S r x t g P f L 8 x J / 2 W U / 1 8 0 W 7 b M + A V o m z I M V a o f T 9 V X 1 / q P f z H 9 1 B S O K A C k 0 4 V q r j 2 J F 2 E y w 1 I 5 x O c 9 1 Y 0 Q i T M R 7 S j q E C B 1 S 5 y S z 1 F J 0 a Z Y D 8 U J o n N J qp v z c S H C g 1 C T w z m a Z U y 1 4 q / u d 1 Y u 2 f u w k T U a y p I P N D f s y R D l F a A R o w S Y n m E 0 M w k c x k R W S E J S b a F J U z J T j L X 1 4 l z U r Z O S t X r k 0 b F z B H F k 6 g A C V w o A o 1 u I I 6 N I C A h H t 4 g m f r z n q 0 X q z X + W j G W u w c w x 9 Y b z 8 j 0 Z a n &lt; / l a t e x i t &gt; f t j &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " t / W H p r e N Z C q P I Q 9 4 T P 5 y I 0 D A g r U = " &gt; A A A B 9 X i c b V C 7 S g N B F L 0 b X z G + o p Y 2 i 0 E Q h L A b C y 2 D N p Y R z A P y Y n Y y m 4 y Z n V 1 m 7 i p h y R f 4 A z Y W S r D 1 X + z 8 E H t n k x S a e G D g c M 6 9 3 D P H i w T X 6 D h f V m Z l d W 1 9 I 7 u Z 2 9 r e 2 d 3 L 7 x / U d B g r y q o 0 F K F q e E Q z w S W r I k f B G p F i J P A E q 3 v D 6 9 S v P z C l e S j v c B S x d k D 6 k v u c E j R S p x U Q H H h + 4 o + 7 9 x 3 s 5 g t O 0 Z n C X i b u n B T K Z 9 + T J 1 a I K t 3 8 Z 6 s X 0 j h g E q k g W j d d J 8 J 2 Q h R y K t g 4 1 4 o 1 i w g d k j 5 r G i p J w H Q 7 m a Y e 2 y d G 6 d l + q M y T a E / V 3 x s J C b Q e B Z 6 Z T F P q R S 8 V / / O a M f q X 7 Y T L K E Y m 6 e y Q H w s b Q z u t w O 5 x x S i K k S G E K m 6 y 2 n R A F K F o i s q Z E t z F L y + T W q n o n h d L t 6 a N K 5 g h C 0 d w D K f g w g W U 4 Q Y q U A U K C p 7 h F d 6 s R + v F m l j v s 9 G M N d 8 5 h D + w P n 4 A t 7 C W V A = = &lt; / l a t e x i t &gt;? s &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 3 m m f o + q p w z / 9 F 1 X v B 6 m Z g j y m d E E = " &gt; A A A B 9 X i c b V D L S g M x F L 1 T X 7 W + q i 4 V C R b B V Z m p C 1 0 W 3 b h s w T 6 g M y 2 Z N N O G Z j J D k l H K 0 K X / 4 M a F I m 7 d 9 j v c + Q 3 + h O l j o a 0 H A o d z 7 u W e H D / m T G n b / r I y K 6 t r 6 x v Z z d z W 9 s 7 u X n 7 / o K 6 i R B J a I x G P Z N P H i n I m a E 0 z z W k z l h S H P q c N f 3 A z 8 R v 3 V C o W i T s 9 j K k X 4 p 5 g A S N Y G 6 n t h l j 3 / S B 1 w 2 T U V p 1 8 w S 7 a U 6 B l 4 s x J o X w 8 r n 4 / n o w r n f y n 2 4 1 I E l K h C c d K t R w 7 1 l 6 K p W a E 0 1 H O T R S N M R n g H m 0 Z K n B I l Z d O U 4 / Q m V G 6 K I i k e U K j q f p 7 I 8 W h U s P Q N 5 O T l G r R m 4 j / e a 1 E B 1 d e y k S c a C r I 7 F C Q c K Q j N K k A d Z m k R P O h I Z h I Z r I i 0 s c S E 2 2 K y p k S n M U v L 5 N 6 q e h c F E t V 0 8 Y 1 z J C F I z i F c 3 D g E s p w C x W o A Q E J T / A C r 9 a D 9 W y 9 W e + z 0 Y w 1 3 z m E P 7 A + f g D 7 V p a G &lt; / l a t e x i t &gt; f s i &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " p 3 u q b 3 y 4 A q Q X C G Y r 9 W 4 5 / a T Z c T M = " &gt; A A A B 9 X i c b V D L T g I x F L 2 D L 8 Q X 6 t J N A z F h R W Z w g U u i G 5 e Y y C O B g X R K B x o 6 n U n b 0 Z A J / 2 F i X G i M W 7 / D r T u j H 2 M H W C h 4 k i Y n 5 9 y b e 3 q 8 i D O l b f v T y q y t b 2 x u Z b d z O 7 t 7 + w f 5 w 6 O m C m N J a I O E P J R t D y v K m a A N z T S n 7 U h S H H i c t r z x Z e q 3 b q l U L B Q 3 e h J R N 8 B D w X x G s D Z S r x t g P f L 8 x J / 2 W U / 1 8 0 W 7 b M + A V o m z I M V a o f T 9 V X 1 / q P f z H 9 1 B S O K A C k 0 4 V q r j 2 J F 2 E y w 1 I 5 x O c 9 1 Y 0 Q i T M R 7 S j q E C B 1 S 5 y S z 1 F J 0 a Z Y D 8 U J o n N J q p v z c S H C g 1 C T w z m a Z U y 1 4 q / u d 1 Y u 2 f u w k T U a y p I P N D f s y R D l F a A R o w S Y n m E 0 M w k c x k R W S E J S b a F J U z J T j L X 1 4 l z U r Z O S t X r k 0 b F z B H F k 6 g A C V w o A o 1 u I I 6 N I C A h H t 4 g m f r z n q 0 X q z X + W j G W u w c w x 9 Y b z 8 j 0 Z a n &lt; / l a t e x i t &gt; f t j &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " t / W H p r e N Z C q P I Q 9 4 T P 5 y I 0 D A g r U = " &gt; A A A B 9 X i c b V C 7 S g N B F L 0 b X z G + o p Y 2 i 0 E Q h L A b C y 2 D N p Y R z A P y Y n Y y m 4 y Z n V 1 m 7 i p h y R f 4 A z Y W S r D 1 X + z 8 E H t n k x S a e G D g c M 6 9 3 D P H i w T X 6 D h f V m Z l d W 1 9 I 7 u Z 2 9 r e 2 d 3 L 7 x / U d B g r y q o 0 F K F q e E Q z w S W r I k f B G p F i J P A E q 3 v D 6 9 S v P z C l e S j v c B S x d k D 6 k v u c E j R S p x U Q H H h + 4 o + 7 9 x 3 s 5 g t O 0 Z n C X i b u n B T K Z 9 + T J 1 a I K t 3 8 Z 6 s X 0 j h g E q k g W j d d J 8 J 2 Q h R y K t g 4 1 4 o 1 i w g d k j 5 r G i p J w H Q 7 m a Y e 2 y d G 6 d l + q M y T a E / V 3 x s J C b Q e B Z 6 Z T F P q R S 8 V / / O a M f q X 7 Y T L K E Y m 6 e y Q H w s b Q z u t w O 5 x x S i K k S G E K m 6 y 2 n R A F K F o i s q Z E t z F L y + T W q n o n h d L t 6 a N K 5 g h C 0 d w D K f g w g W U 4 Q Y q U A U K C p 7 h F d 6 s R + v F m l j v s 9 G M N d 8 5 h D + w P n 4 A t 7 C W V A = = &lt; / l a t e x i t &gt; ? t &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " l C P B I A r o / F 6 L R W R h J 1 v K F g m f 6 b E = " &gt; A A A B 9 X i c b V D L S g M x F L 1 T X 7 W + q i 7 d B I v g q s x U Q Z d F N y 4 r 2 A e 0 0 5 J J M 2 1 o k h m S j F K G / o c b F 4 q 4 9 V / c + T d m 2 l l o 6 4 H A 4 Z x 7 u S c n i D n T x n W / n c L a + s b m V n G 7 t L O 7 t 3 9 Q P j x q 6 S h R h D Z J x C P V C b C m n E n a N M x w 2 o k V x S L g t B 1 M b j O / / U i V Z p F 8 M N O Y + g K P J A s Z w c Z K / Z 7 A Z h y E a U 8 k s 7 4 Z l C t u 1 Z 0 D r R I v J x X I 0 R i U v 3 r D i C S C S k M 4 1 r r r u b H x U 6 w M I 5 z O S r 1 E 0 x i T C R 7 R r q U S C 6 r 9 d J 5 6 h s 6 s M k R h p O y T B s 3 V 3 x s p F l p P R W A n s 5 R 6 2 c v E / 7 x u Y s J r P 2 U y T g y V Z H E o T D g y E c o q Q E O m K D F 8 a g k m i t m s i I y x w s T Y o k q 2 B G / 5 y 6 u k V a t 6 F 9 X a / W W l f p P X U Y Q T O I V z 8 O A K 6 n A H D W g C A Q X P 8 A p v z p P z 4 r w 7 H 4 v R g p P v H M M f O J 8 / D + q S 4 A = = &lt; / l a t e x i t &gt; k-means &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 3 r K g N U r 1 i K G e U u Y 7 0 r D H 4 f t X Y r E = " &gt; A A A B 9 X i c b V D L S g N B E J z 1 G e M r 6 t H L Y h C 8 G H a j o M e g F 4 8 R z A O S G G Y n v c m Q m d l l p l c N S / 7 D i w d F v P o v 3 v w b J 8 k e N L G g o a j q p r s r i A U 3 6 H n f z t L y y u r a e m 4 j v 7 m 1 v b N b 2 N u v m y j R D G o s E p F u B t S A 4 A p q y F F A M 9 Z A Z S C g E Q y v J 3 7 j A b T h k b r D U Q w d S f u K h 5 x R t N L 9 s I 3 w h O m p B K r M u F s o e i V v C n e R + B k p k g z V b u G r 3 Y t Y I k E h E 9 S Y l u / F 2 E m p R s 4 E j P P t x E B M 2 Z D 2 o W W p o h J M J 5 1 e P X a P r d J z w 0 j b U u h O 1 d 8 T K Z X G j G R g O y X F g Z n 3 J u J / X i v B 8 L K T c h U n C I r N F o W J c D F y J x G 4 P a 6 B o R h Z Q p n m 9 l a X D a i m D G 1 Q e R u C P / / y I q m X S / 5 Z q X x 7 X q x c Z X H k y C E 5 I i f E J x e k Q m 5 I l d Q I I 5 o 8 k 1 f y 5 j w 6 L 8 6 7 8 z F r X X K y m Q P y B 8 7 n D / Y Y k s 8 = &lt; / l a t e x i t &gt; p s &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " / 2 Q N w W + f B O g F q H e w + V 0 h j F c q 2 0 w = " &gt; A A A B 8 3 i c b V D L S g M x F L 1 T X 7 W + q i 7 d B I v g q s x U Q Z d F N y 4 r 2 A d 0 x p J J M 2 1 o J h O S j F C G / o Y b F 4 q 4 9 W f c + T d m 2 l l o 6 4 H A 4 Z x 7 u S c n l J x p 4 7 r f T m l t f W N z q 7 x d 2 d n d 2 z + o H h 5 1 d J I q Q t s k 4 Y n q h V h T z g R t G 2 Y 4 7 U l F c R x y 2 g 0 n t 7 n f f a J K s 0 Q 8 m K m k Q Y x H g k W M Y G M l 3 4 + x G Y d R J m e P e l C t u X V 3 D r R K v I L U o E B r U P 3 y h w l J Y y o M 4 V j r v u d K E 2 R Y G U Y 4 n V X 8 V F O J y Q S P a N 9 S g W O q g 2 y e e Y b O r D J E U a L s E w b N 1 d 8 b G Y 6 1 n s a h n c w z 6 m U v F / / z + q m J r o O M C Z k a K s j i U J R y Z B K U F 4 C G T F F i + N Q S T B S z W R E Z Y 4 W J s T V V b A n e 8 p d X S a d R 9 y 7 q j f v L W v O m q K M M J 3 A K 5 + D B F T T h D l r Q B g I S n u E V 3 p z U e X H e n Y / F a M k p d o 7 h D 5 z P H 4 H 5 k f 0 = &lt; / l a t e x i t &gt; p t &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 9 A Q Q v Z 6 e 1 m 3 U / r l T B B 4 G 7 R m z T u o = " &gt; A A A B 8 3 i c b V D L S g M x F M 3 U V 6 2 v q k s 3 w S K 4 K j N V 0 G X R j c s K 9 g G d s W T S T B u a y Y T k j l C G / o Y b F 4 q 4 9 W f c + T d m 2 l l o 6 4 H A 4 Z x 7 u S c n V I I b c N 1 v p 7 S 2 v r G 5 V d 6 u 7 O z u 7 R 9 U D 4 8 6 J k k 1 Z W 2 a i E T 3 Q m K Y 4 J K 1 g Y N g P a U Z i U P B u u H k N v e 7 T 0 w b n s g H m C o W x G Q k e c Q p A S v 5 f k x g H E a Z m j 3 C o F p z 6 + 4 c e J V 4 B a m h A q 1 B 9 c s f J j S N m Q Q q i D F 9 z 1 U Q Z E Q D p 4 L N K n 5 q m C J 0 Q k a s b 6 k k M T N B N s 8 8 w 2 d W G e I o 0 f Z J w H P 1 9 0 Z G Y m O m c W g n 8 4 x m 2 c</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>k 7 o y U L z 7 s 8 + T 1 y 6 5 3 S 5 9 e L y Z p R I U m H C v V c Z 1 E + x m W m h F O x 7 a X K p p g M s R 9 2 j F U 4 I g q P 5 t k H q M j o / R Q G E v z h E Y T 9 f d G h i O l R l F g J v O M a t 7 L x f + 8 T q r D M z 9 j I k k 1 F W R 6 K E w 5 0 j H K C 0 A 9 J i n R f G Q I J p K Z r I g M s M R E m 5 p s U 4 I 7 / + V F 0 q x W 3 J N K 9 c Y p 1 y 5 h i i I c w C E c g w u n U I N r q E M D C C T w A E / w b K X W o / V i v U 1 H C 9 Z s Z x / + w H r / A d O + l T E = &lt; / l a t e x i t &gt; f s &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " U + a V s 4 P z s N G O 2 n 5 I q l m n 6 H s + p X o = " &gt; A A A B 8 3 i c b V C 7 T s M w F L 0 p r x J e B U Y W i w q J q U r K A A u i g o W x S P Q h N a F y X K e 1 6 j i R 7 S B V U X + D h Q E E r H w H O w v i b 3 D a D t B y J E t H 5 9 y r e 3 y C h D O l H e f b K i w t r 6 y u F d f t j c 2 t 7 Z 3 S 7 l 5 T x a k k t E F i H s t 2 g B X l T N C G Z p r T d i I p j g J O W 8 H w K v d b 9 1 Q q F o t b P U q o H + G + Y C E j W B v J 8 y K s B 0 G Y h e M 7 1 S 2 V n Y o z A V o k 7 o y U L z 7 s 8 + T 1 y 6 5 3 S 5 9 e L y Z p R I U m H C v V c Z 1 E + x m W m h F O x 7 a X K p p g M s R 9 2 j F U 4 I g q P 5 t k H q M j o / R Q G E v z h E Y T 9 f d G h i O l R l F g J v O M a t 7 L x f + 8 T q r D M z 9 j I k k 1 F W R 6 K E w 5 0 j H K C 0 A 9 J i n R f G Q I J p K Z r I g M s M R E m 5 p s U 4 I 7 / + V F 0 q x W 3 J N K 9 c Y p 1 y 5 h i i I c w C E c g w u n U I N r q E M D C C T w A E / w b K X W o / V i v U 1 H C 9 Z s Z x / + w H r / A d I 6 l T A = &lt; / l a t e x i t &gt; k-means &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 3 r K g N U r 1 i K G e U u Y 7 0 r D H 4 f t X Y r E = " &gt; A A A B 9 X i c b V D L S g N B E J z 1 G e M r 6 t H L Y h C 8 G H a j o M e g F 4 8 R z A O S G G Y n v c m Q m d l l p l c N S / 7 D i w d F v P o v 3 v w b J 8 k e N L G g o a j q p r s r i A U 3 6 H n f z t L y y u r a e m 4 j v 7 m 1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>4 P a 6 B o R h Z Q p n m 9 l a X D a i m D G 1 Q e R u C P / / y I q m X S / 5 Z q X x 7 X q x c Z X H k y C E 5 I i f E J x e k Q m 5 I l d Q I I 5 o 8 k 1 f y 5 j w 6 L 8 6 7 8 z F r X X K y m Q P y B 8 7 n D / Y Y k s 8 = &lt; / l a t e x i t &gt; L s MIM &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 4 t j t H Y F j a m l 1 O M D h 0 f 9 z l j i 0 C c 4 = " &gt; A A A C A 3 i c b Z C 7 S g N B F I Z n v c Z 4 W 7 X T Z k g Q U o X d W M Q y a K N g I I K 5 Q L I u s 5 P Z Z M j s h Z l Z I S w L N j 6 F v Y 2 F I r Y + g K 2 d 6 M M 4 u 0 m h i T 8 M f P z n H O a c 3 w k Z F d I w P r W F x a X l l d X c W n 5 9 Y 3 N r W 9 / Z b Y k g 4 p g 0 c c A C 3 n G Q I I z 6 p C m p Z K Q T c o I 8 h 5 G 2 M z p N 6 + 0 b w g U N / C s 5 D o n l o Y F P X Y q R V J a t 7 / c 8 J I c Y s f g i s T P m X l w / r y f X w t a L R t n I B O f B n E K x V i h 9 f 1 X f 7 x u 2 / t H r B z j y i C 8 x Q 0 J 0 T S O U V o y 4 p J i R J N + L B A k R H q E B 6 S r 0 k U e E F W c 3 J P B Q O X 3 o B l w 9 X 8 L M / T 0 R I 0 + I s e e o z n R L M V t L z f 9 q 3 U i 6 x 1 Z M / T C S x M e T j 9 y I Q R n A N B D Y p 5 x g y c Y K E O Z U 7 Q r x E H G E p Y o t r 0 I w Z 0 + e h 1 a l b B 6 V K 5 c q j R M w U Q 4 c g A I o A R N U Q Q 2 c g Q Z o A g x u w Q N 4 A s / a n f a o v W i v k 9 Y F b T q z B / 5 I e / s B w Z q b 9 Q = = &lt; / l a t e x i t &gt; L t MIM &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " v f F Y 0 s b a d e N v Q / G W z N W z 1 Z M 1 r B I = " &gt; A A A C A 3 i c b Z C 7 S g N B F I Z n v c Z 4 W 7 X T Z k g Q U o X d W M Q y a K N g I I K 5 Q L I u s 5 P Z Z M j s h Z l Z I S w L N j 6 F v Y 2 F I r Y + g K 2 d 6 M M 4 u 0 m h i T 8 M f P z n H O a c 3 w k Z F d I w P r W F x a X l l d X c W n 5 9 Y 3 N r W 9 / Z b Y k g 4 p g 0 c c A C 3 n G Q I I z 6 p C m p Z K Q T c o I 8 h 5 G 2 M z p N 6 + 0 b w g U N / C s 5 D o n l o Y F P X Y q R V J a t 7 / c 8 J I c Y s f g i s T P m X l w / r y f X 0 t a L R t n I B O f B n E K x V i h 9 f 1 X f 7 x u 2 / t H r B z j y i C 8 x Q 0 J 0 T S O U V o y 4 p J i R J N + L B A k R H q E B 6 S r 0 k U e E F W c 3 J P B Q O X 3 o B l w 9 X 8 L M / T 0 R I 0 + I s e e o z n R L M V t L z f 9 q 3 U i 6 x 1 Z M / T C S x M e T j 9 y I Q R n A N B D Y p 5 x g y c Y K E O Z U 7 Q r x E H G E p Y o t r 0 I w Z 0 + e h 1 a l b B 6 V K 5 c q j R M w U Q 4 c g A I o A R N U Q Q 2 c g Q Z o A g x u w Q N 4 A s / a n f a o v W i v k 9 Y F b T q z B / 5 I e / s B w x 6 b 9 g = = &lt; / l a t e x i t &gt; t e x i t s h a 1 _ b a s e 6 4 = " D R j W h 6 0 c 2 F k D e Q p 1 + d 7 z f g r j 2 t k = " &gt; A A A C T H i c b V C 7 S g N B F J 2 N 7 / i K W t o M C Y J V 2 I 2 F l k E b S w W j Q j Y s s 7 N 3 4 + D s 7 D J z V w 1 L P s A v 8 I O s b C x s x K + w s V B E c J K I a P T A w O G c + 5 o T Z l I Y d N 0 n p z Q x O T U 9 M z t X n l 9 Y X F q u r K w e m z T X H F o 8 l a k + D Z k B K R S 0 U K C E 0 0 w D S 0 I J J + H 5 3 s A / u Q B t R K q O s J d B J 2 F d J W L B G V o p q H A / h K 5 Q R Z g w 1 O K q X 6 b U t / Q s j I v L f u D 5 / m + h M R I u o h T N u F f 4 K F S P q o D 3 y z 6 o 6 H t m U K m 5 d X c I + p d 4 X 6 T W r P r V 6 5 v b x 4 O g 8 u B H K c 8 T U M g l M 6 b t u R l 2 C q Z R c A l 2 e m 4 g Y / y c d a F t q W I J m E 4 x D K N P N 6 w S 0 T j V 9 i m k Q / V n R 8 E S Y 3 p J a C s H t 5 t x b y D + 5 7 V z j H c 6 h V B Z j q D 4 a F G c S 4 o p H S R L I 6 G B o + x Z w r g W 9 l b K z 5 h m H G 3 + Z R u C N / 7 l v + S 4 U f e 2 6 o 1 D r 9 b c J S P M k n V S J Z v E I 9 u k S f b J A W k R T u 7 I M 3 k l b 8 6 9 8 + K 8 O x + j 0 p L z 1 b N G f q E 0 / Q m 7 x L d 8 &lt; / l a t e x i t &gt; L InSelf + L CrossSelf &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " / N W r p i t m X 2 m a b S T b + z / 4 E 9 3 L N u 0 = " &gt; A A A C I 3 i c b Z D L S s N A F I Y n 9 V b r L S q 4 c R M s g i C U p C 4 U V 6 X d K L h o 0 V 6 g D W E y n b R D J 5 M w M x F K y L u 4 c e l b i B s X S n H j Q p / F S e p C 2 / 4 w 8 P H / 5 z D n H D e k R E j T / N R y S 8 s r q 2 v 5 9 c L G 5 t b 2 j r 6 7 1 x J B x B F u o o A G v O N C g S l h u C m J p L g T c g x 9 l + K 2 O 6 q l e f s e c 0 E C d i f H I b Z 9 O G D E I w h K Z T n 6 Z c + H c o g g j W 8 S J 2 P u x 9 f s F l M v O V 2 U 1 X g g R B Y 7 e t E s m Z m M e b B + o V g 5 a H y T p + p z 3 d E n v X 6 A I h 8 z i S g U o m u Z o b R j y C V B F C e F X i R w C N E I D n B X I Y M + F n a c 7 Z g Y x 8 r p G 1 7 A 1 W P S y N y / H T H 0 h R j 7 r q p M J x W z W W o u y r q R 9 C 7 s m L A w k p i h 6 U d e R A 0 Z G O n B j D 7 h G E k 6 V g A R J 2 p W A w 0 h h 0 i q s x b U E a z Z l e e h V S 5 Z Z 6 V y w y p W q m C q P D g E R + A E W O A c V M A V q I M m Q O A B v I A 3 8 K 4 9 a q / a R P u Y l u a 0 3 5 5 9 8 E / a 1 w / q T q n 9 &lt; / l a t e x i t &gt; An overview of the PCS framework. In-domain and cross-domain self-supervision are performed between normalized feature vectors f and prototypes ? computed by clustering vectors v in memory banks. Features with confident predictions (p) are used to adaptively update classifier vectors w. MI maximization and classification loss are further used to extract discriminative features.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>k } and similarly C t with normalized source prototypes {? s j } k j=1 and normalized target prototypes {? t j } k j=1 . Specifically, ? s j =</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Comparison of cross-domain instance-instance (I-I) matching [39] (left) and our cross-domain instance-prototype (I-P) matching (right). Left: I-I incorrectly matches all orange samples to the same blue sample. Right: I-P robustly matches samples to the correct prototypes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>=</head><label></label><figDesc>{x|(x, y) ? D s , y = i} and denote by D (i) su and D (i) tu the set of samples with high-confident label i in source and target, respectively. With p(x) = [p(x) 1 , . . . , p(x) nc ], D (i) su = {x|x ? D su , p(x) i &gt; t}, where t is a confidence threshold; and similarly for D (i)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 :</head><label>4</label><figDesc>t-SNE visualization of ours and baselines on Office (left) and Office-Home (right). Top row: Coloring represents the class of each sample. Features with PCS are more discriminative than the ones with other methods. Bottom row: Cyan represents source features and Red represents target features. Feature from PCS are better-aligned between domains compared to other methods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 :</head><label>5</label><figDesc>Sample efficiency comparison from DSLR to Amazon in Office dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>bFigure 6 :</head><label>6</label><figDesc>Target Acc. with Weighted kNN vs. training epochs Stability of Target Accuracy during training procedure.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>PCS (Ours) CDS ImageNet Pre-trained a Office (D?A with 1-shot labeled source per class) PCS (Ours) CDS ImageNet Pre-trained b Office-Home (Rw?Cl with 3% labeled source per class) Figure 7: t-SNE visualization of ours and baselines on Office (a) and Office-Home (b). Top row: Coloring represents the class of each sample, and shape represents domain (circle for source and cross for target). Features with PCS are more discriminative than the ones with other methods. Bottom row: each number represents a centroid for corresponding class. Cyan represents centroids of source images based on ground truth and Red for target. Black represents prototypes of the classifier. Centroids from PCS are better-aligned between domains compared to other methods. (Zoom in for more details).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 8 :</head><label>8</label><figDesc>and 7b, comparing ImageNet Pre-training, CDS [39] and PCS. Specifically, we plot representations for all samples (top in Image retrieval examples of the closest cross-domain neighbors using CDS (a) and PCS (b) in Office-Home (Target: Real, Source: Art).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Adaptation accuracy (%) comparison on 1-shot and 3-shots per class on the Office dataset.</figDesc><table><row><cell>Method</cell><cell></cell><cell></cell><cell cols="3">Office: Target Acc. on 1-shot / 3-shots</cell><cell></cell><cell></cell></row><row><cell></cell><cell>A?D</cell><cell>A?W</cell><cell>D?A</cell><cell>D?W</cell><cell>W?A</cell><cell>W?D</cell><cell>Avg</cell></row><row><cell>SO</cell><cell cols="5">27.5 / 49.2 28.7 / 46.3 40.9 / 55.3 65.2 / 85.5 41.1 / 53.8</cell><cell>62.0 / 86.1</cell><cell>44.2 / 62.7</cell></row><row><cell>MME [60]</cell><cell cols="5">21.5 / 51.0 12.2 / 54.6 23.1 / 60.2 60.9 / 89.7 14.0 / 52.3</cell><cell>62.4 / 91.4</cell><cell>32.3 / 66.5</cell></row><row><cell>CDAN [45]</cell><cell>11.2 / 43.7</cell><cell>6.2 / 50.1</cell><cell>9.1 / 65.1</cell><cell cols="2">54.8 / 91.6 10.4 / 57.0</cell><cell>41.6 / 89.8</cell><cell>22.2 / 66.2</cell></row><row><cell>SPL [72]</cell><cell>12.0 / 77.1</cell><cell>7.7 / 80.3</cell><cell>7.3 / 74.2</cell><cell>7.2 / 93.5</cell><cell>7.2 / 64.4</cell><cell>10.2 / 91.6</cell><cell>8.6 / 80.1</cell></row><row><cell>CAN [38]</cell><cell cols="5">25.3 / 48.6 26.4 / 45.3 23.9 / 41.2 69.4 / 78.2 21.2 / 39.3</cell><cell>67.3 / 82.3</cell><cell>38.9 / 55.8</cell></row><row><cell>MDDIA [35]</cell><cell cols="5">45.0 / 62.9 54.5 / 65.4 55.6 / 67.9 84.4 / 93.3 53.4 / 70.3</cell><cell>79.5 / 93.2</cell><cell>62.1 / 75.5</cell></row><row><cell>CDS [39]</cell><cell cols="5">33.3 / 57.0 35.2 / 58.6 52.0 / 67.6 59.0 / 86.0 46.5 / 65.7</cell><cell>57.4 / 81.3</cell><cell>47.2 / 69.3</cell></row><row><cell>DANN + ENT [18]</cell><cell cols="5">32.5 / 57.6 37.2 / 54.1 36.9 / 54.1 70.1 / 87.4 43.0 / 51.4</cell><cell>58.8 / 89.4</cell><cell>46.4 / 65.7</cell></row><row><cell>MME + ENT</cell><cell cols="5">37.6 / 69.5 42.5 / 68.3 48.6 / 66.7 73.5 / 89.8 47.2 / 63.2</cell><cell>62.4 / 95.4</cell><cell>52.0 / 74.1</cell></row><row><cell>CDAN + ENT</cell><cell cols="5">31.5 / 68.3 26.4 / 71.8 39.1 / 57.3 70.4 / 88.2 37.5 / 61.5</cell><cell>61.9 / 93.8</cell><cell>44.5 / 73.5</cell></row><row><cell>CDS + ENT</cell><cell cols="5">40.4 / 61.2 44.7 / 66.7 66.4 / 73.1 71.6 / 90.6 58.6 / 71.8</cell><cell>69.3 / 86.1</cell><cell>58.5 / 74.9</cell></row><row><cell>CDS + MME + ENT</cell><cell cols="5">39.4 / 61.6 43.6 / 66.3 66.0 / 74.5 75.7 / 92.1 53.1 / 73.0</cell><cell>70.9 / 90.6</cell><cell>58.5 / 76.3</cell></row><row><cell cols="6">CDS + CDAN + ENT 52.6 / 65.1 55.2 / 68.8 65.7 / 71.2 76.6 / 88.1 59.7 / 71.0</cell><cell>73.3 / 87.3</cell><cell>63.9 / 75.3</cell></row><row><cell>CDS / MME + ENT  ?</cell><cell cols="5">55.4 / 75.7 57.2 / 77.2 62.8 / 69.7 84.9 / 92.1 62.6 / 69.9</cell><cell>77.7 / 95.4</cell><cell>65.3 / 80.0</cell></row><row><cell cols="6">CDS / CDAN + ENT  ? 53.8 / 78.1 65.6 / 79.8 59.5 / 70.7 83.0 / 93.2 57.4 / 64.5</cell><cell>77.1 / 97.4</cell><cell>66.1 / 80.6</cell></row><row><cell>PCS (Ours)</cell><cell cols="5">60.2 / 78.2 69.8 / 82.9 76.1 / 76.4 90.6 / 94.1 71.2 / 76.3</cell><cell>91.8 / 96.0</cell><cell>76.6 / 84.0</cell></row><row><cell>Improvement</cell><cell>+4.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>8 / +0.1 +4.2 / +3.1 +9.7 / +1.9 +5.7 / +0.9 +8.6 / +3.3 +14.1 / -1.4 +10.5 / +3.4 ? Two-stage training results reported in [39].</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Performance contribution of each part in PCS framework on Office. / 49.2 28.7 / 46.3 40.9 / 55.3 65.2 / 85.5 41.1 / 53.8 62.0 / 86.1 44.2 / 62.7 +L InSelf 39.0 / 55.6 38.6 / 55.1 47.2 / 68.5 71.7 / 89.4 50.9 / 68.4 65.1 / 90.6 52.1 / 71.3 +L CrossSelf 47.2 / 71.1 52.7 / 70.6 59.0 / 75.5 76.4 / 90.3 58.5 / 74.1 66.9 / 91.8 60.1 / 78.9 +L MIM 52.8 / 73.5 57.5 / 71.2 67.2 / 76.3 78.9 / 91.4 64.2 / 74.3 68.7 / 92.2 64.9 / 79.8 +APCU (PCS) 60.2 / 78.2 69.8 / 82.9 76.1 / 76.4 90.6 / 94.1 71.2 / 76.3 91.8 / 96.0 76.6 / 84.0 PCS w/o MIM 59.0 / 75.9 58.6 / 76.5 76.2 / 76.4 87.8 / 93.2 68.7 / 74.7 89.8 / 95.0 73.5 / 82.0</figDesc><table><row><cell>Method</cell><cell></cell><cell></cell><cell cols="3">Office: Target Acc. on 1-shot / 3-shots</cell><cell></cell></row><row><cell></cell><cell>A?D</cell><cell>A?W</cell><cell>D?A</cell><cell>D?W</cell><cell>W?A</cell><cell>W?D</cell><cell>Avg</cell></row><row><cell>L cls</cell><cell>27.5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Adaptation accuracy (%) comparison on 3% and 6% labeled samples per class on the Office-Home dataset. Ar ?Cl Ar ?Pr Ar ?Rw Cl ?Ar Cl ?Pr Cl ?Rw Pr ?Ar Pr ?Cl Pr ?Rw Rw ?Ar Rw ?Cl Rw ?Pr Avg</figDesc><table><row><cell>Method</cell></row></table><note>? Two-stage training results reported in [39].</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Adaptation accuracy (%) comparison on 0.1% and 1% labeled samples per class on the VisDA-2017 dataset.</figDesc><table><row><cell>Method</cell><cell cols="2">VisDA: Target Acc. (%) 0.1% Labeled 1% Labeled</cell></row><row><cell>SO</cell><cell>47.9</cell><cell>51.4</cell></row><row><cell>MME [60]</cell><cell>55.6</cell><cell>69.4</cell></row><row><cell>CDAN [45]</cell><cell>58.0</cell><cell>61.5</cell></row><row><cell>MDDIA [35]</cell><cell>68.9</cell><cell>71.3</cell></row><row><cell>CAN [38]</cell><cell>51.3</cell><cell>57.2</cell></row><row><cell>CDS [39]</cell><cell>34.2</cell><cell>67.5</cell></row><row><cell>DANN + ENT [18]</cell><cell>44.5</cell><cell>50.2</cell></row><row><cell>MME + ENT</cell><cell>54.0</cell><cell>66.1</cell></row><row><cell>CDAN + ENT</cell><cell>57.7</cell><cell>58.1</cell></row><row><cell>CDS + ENT</cell><cell>49.8</cell><cell>75.3</cell></row><row><cell>CDS + ENT + MME</cell><cell>60.0</cell><cell>78.3</cell></row><row><cell>CDS / MME + ENT  ?</cell><cell>62.5</cell><cell>69.4</cell></row><row><cell>CDS / CDAN + ENT  ?</cell><cell>69.0</cell><cell>69.1</cell></row><row><cell>PCS (Ours)</cell><cell>78.0</cell><cell>79.0</cell></row><row><cell>Improvement</cell><cell>+9.0</cell><cell>+0.7</cell></row><row><cell cols="2">? Two-stage training results reported in [39].</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Adaptation accuracy (%) comparison on 1-shot and 3shots per class on the DomainNet dataset.</figDesc><table><row><cell>Method</cell><cell>R)C</cell><cell>R)P</cell><cell cols="5">DomainNet: Target Acc. (%) R)S P)C P)R C)S S)P</cell><cell>Avg</cell></row><row><cell></cell><cell></cell><cell cols="3">1-shot labeled source</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>SO</cell><cell>18.4</cell><cell>30.6</cell><cell>16.7</cell><cell>16.2</cell><cell>28.9</cell><cell cols="2">12.7 10.5</cell><cell>19.1</cell></row><row><cell>MME [60]</cell><cell>13.8</cell><cell>29.2</cell><cell>9.7</cell><cell>16.0</cell><cell>26.0</cell><cell cols="2">13.4 14.4</cell><cell>17.5</cell></row><row><cell>CDAN [45]</cell><cell>16.0</cell><cell>25.7</cell><cell>12.9</cell><cell>12.6</cell><cell>19.5</cell><cell>7.2</cell><cell>8.0</cell><cell>14.6</cell></row><row><cell>MDDIA [35]</cell><cell>18.0</cell><cell>30.6</cell><cell>15.9</cell><cell>15.4</cell><cell>27.4</cell><cell>9.3</cell><cell>10.2</cell><cell>18.1</cell></row><row><cell>CAN [38]</cell><cell>18.3</cell><cell>22.1</cell><cell>16.7</cell><cell>13.2</cell><cell>23.9</cell><cell cols="2">11.1 12.1</cell><cell>16.8</cell></row><row><cell>CDS [39]</cell><cell>16.7</cell><cell>24.4</cell><cell>11.1</cell><cell>14.1</cell><cell>15.9</cell><cell cols="2">13.4 19.0</cell><cell>16.4</cell></row><row><cell>CDS + ENT</cell><cell>21.7</cell><cell>30.1</cell><cell>18.2</cell><cell>17.4</cell><cell>20.5</cell><cell cols="2">18.6 22.7</cell><cell>21.5</cell></row><row><cell>CDS + MME + ENT</cell><cell>21.2</cell><cell>28.8</cell><cell>15.5</cell><cell>15.8</cell><cell>17.6</cell><cell cols="2">19.0 20.7</cell><cell>19.8</cell></row><row><cell>PCS (Ours)</cell><cell>39.0</cell><cell>51.7</cell><cell>39.8</cell><cell>26.4</cell><cell>38.8</cell><cell cols="2">23.7 23.6</cell><cell>34.7</cell></row><row><cell>Improvement</cell><cell cols="8">+17.3 +21.1 +21.6 +9.0 +9.9 +4.7 +0.9 +13.2</cell></row><row><cell></cell><cell></cell><cell cols="3">3-shots labeled source</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>SO</cell><cell>30.2</cell><cell>44.2</cell><cell>25.7</cell><cell>24.6</cell><cell>49.8</cell><cell cols="2">24.2 23.2</cell><cell>31.7</cell></row><row><cell>MME [60]</cell><cell>22.8</cell><cell>46.5</cell><cell>14.5</cell><cell>25.1</cell><cell>50.0</cell><cell cols="2">20.1 24.9</cell><cell>29.1</cell></row><row><cell>CDAN [45]</cell><cell>30.0</cell><cell>40.1</cell><cell>21.7</cell><cell>21.4</cell><cell>40.8</cell><cell cols="2">17.1 19.7</cell><cell>27.3</cell></row><row><cell>MDDIA [35]</cell><cell>41.4</cell><cell>50.7</cell><cell>37.4</cell><cell>31.4</cell><cell>52.9</cell><cell cols="2">23.1 24.1</cell><cell>37.3</cell></row><row><cell>CAN [38]</cell><cell>28.1</cell><cell>33.5</cell><cell>25</cell><cell>24.7</cell><cell>46.9</cell><cell cols="2">23.3 20.1</cell><cell>28.8</cell></row><row><cell>CDS [39]</cell><cell>35.0</cell><cell>43.8</cell><cell>36.7</cell><cell>34.1</cell><cell>36.8</cell><cell cols="2">31.1 34.5</cell><cell>36.0</cell></row><row><cell>CDS + ENT</cell><cell>44.5</cell><cell>52.2</cell><cell>40.9</cell><cell>40.0</cell><cell>47.2</cell><cell cols="2">33.0 40.1</cell><cell>42.5</cell></row><row><cell>CDS + MME + ENT</cell><cell>43.8</cell><cell>54.9</cell><cell>41.1</cell><cell>38.9</cell><cell>45.9</cell><cell cols="2">32.8 38.7</cell><cell>42.3</cell></row><row><cell>PCS (Ours)</cell><cell>45.2</cell><cell>59.1</cell><cell>41.9</cell><cell>41.0</cell><cell>66.6</cell><cell cols="2">31.9 37.4</cell><cell>46.1</cell></row><row><cell>Improvement</cell><cell>+0.7</cell><cell>+6.9</cell><cell cols="5">+0.8 +1.0 +13.7 -0.9 -2.7</cell><cell>+3.6</cell></row><row><cell cols="9">tensive experiments are conducted on Office, Office-Home,</cell></row><row><cell cols="9">VisDA-2017 and DomainNet, with the results presented in</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Dataset statistics and labeled source used</figDesc><table><row><cell>Dataset</cell><cell>Domain</cell><cell># total image</cell><cell># labeled images</cell><cell># classes</cell></row><row><cell>Office [59]</cell><cell>Amazon (A) DSLR (D) Webcam (W)</cell><cell>2817 498 795</cell><cell>1-shot and 3-shots labeled source</cell><cell>31</cell></row><row><cell></cell><cell>Art (Ar)</cell><cell>2427</cell><cell></cell><cell></cell></row><row><cell>Office-Home [70]</cell><cell>Clipart (Cl) Product (Pr)</cell><cell>4365 4439</cell><cell>3% and 6% labeled source</cell><cell>65</cell></row><row><cell></cell><cell>Real (Rw)</cell><cell>4357</cell><cell></cell><cell></cell></row><row><cell>VisDA [56]</cell><cell>Synthetic (Syn) Real (Rw)</cell><cell>152K 55K</cell><cell>0.1% and 1% labeled source</cell><cell>12</cell></row><row><cell></cell><cell>Clipart (C)</cell><cell>18703</cell><cell></cell><cell></cell></row><row><cell>DomainNet [55]</cell><cell>Painting (P) Real (R)</cell><cell>31502 70358</cell><cell>1-shot and 3-shots labeled source</cell><cell>126</cell></row><row><cell></cell><cell>Sketch (S)</cell><cell>24582</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 :</head><label>7</label><figDesc>Averaged accuracy and standard deviation of PCS on three runs of 1-shot and 3-shots on Office dataset. 2?1.9 69.8?0.8 76.1?0.4 90.6?0.8 71.2?1.0 91.8?1.9 3-shots 78.2?1.8 82.9?1.1 76.4?0.5 94.1?0.1 76.3?0.7 96.0?0.7</figDesc><table><row><cell>Labeled Source</cell><cell>A?D</cell><cell>A?W</cell><cell>D?A</cell><cell>D?W</cell><cell>W?A</cell><cell>W?D</cell></row><row><cell>1-shot</cell><cell>60.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 8 :</head><label>8</label><figDesc>Accuracy of cross-domain weighted kNN with different SSL methods.</figDesc><table><row><cell>Method</cell><cell cols="2">D?A Rw?Cl</cell></row><row><cell>ImageNet pre-train</cell><cell>62.5</cell><cell>40.6</cell></row><row><cell>ID [73]</cell><cell>70.3</cell><cell>51.9</cell></row><row><cell>CDS [39]</cell><cell>72.5</cell><cell>53.7</cell></row><row><cell>protoNCE [41]</cell><cell>72.3</cell><cell>49.3</cell></row><row><cell>L InSelf + L CrossSelf</cell><cell>75.5</cell><cell>55.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 9 :</head><label>9</label><figDesc>Accuracy of cross-domain weighted kNN with different FUDA methods. both Figures), as well as the prototypes (normalized average representation) for each class. In top rows of both figures, the color of a sample represents its class, and samples from different domains are represented by different shapes (cir-</figDesc><table><row><cell>Method</cell><cell cols="2">D?A (1-shot) Rw?Cl (3%)</cell></row><row><cell>CDS [39]</cell><cell>72.3</cell><cell>57.6</cell></row><row><cell>CDS + ENT</cell><cell>72.8</cell><cell>58.6</cell></row><row><cell>CDS + MME + ENT</cell><cell>60.8</cell><cell>59.2</cell></row><row><cell>PCS (Ours)</cell><cell>76.0</cell><cell>59.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 10 :</head><label>10</label><figDesc>Sum of pair-wise cosine-similarity between prototypes in Office and Office-Home.</figDesc><table><row><cell>Method</cell><cell cols="2">D?A (1-shot) Rw?Pr (3%)</cell></row><row><cell>SO</cell><cell>0.44</cell><cell>-0.71</cell></row><row><cell>CDS [39]</cell><cell>0.43</cell><cell>-0.71</cell></row><row><cell>PCS w/o APCU</cell><cell>-53.3</cell><cell>-22.8</cell></row><row><cell>PCS (Ours)</cell><cell>-58.4</cell><cell>-26.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 11 :</head><label>11</label><figDesc>Performance contribution of each part in PCS framework on Office-Home. Method Office-Home: Target Acc. Ar ?Cl Ar ?Pr Ar ?Rw Cl ?Ar Cl ?Pr Cl ?Rw Pr ?Ar Pr ?Cl Pr ?Rw Rw ?Ar Rw ?Cl Rw ?Pr Avg</figDesc><table><row><cell>3% labeled source</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 12 :</head><label>12</label><figDesc>Adaptation accuracy (%) comparison on fully-labeled setting on the Office-Home dataset.</figDesc><table><row><cell>Method</cell><cell cols="13">Ar?Cl Ar?Pr Ar?Rw Cl?Ar Cl?Pr Cl?Rw Pr?Ar Pr?Cl Pr?Rw Rw?Ar Rw?Cl Rw?Pr Avg</cell></row><row><cell>SO</cell><cell>34.9</cell><cell>50.0</cell><cell>58.0</cell><cell>37.4</cell><cell>41.9</cell><cell>46.2</cell><cell>38.5</cell><cell>31.2</cell><cell>60.4</cell><cell>53.9</cell><cell>41.2</cell><cell>59.9</cell><cell>46.1</cell></row><row><cell>DANN [18]</cell><cell>45.6</cell><cell>59.3</cell><cell>70.1</cell><cell>47.0</cell><cell>58.5</cell><cell>60.9</cell><cell>46.1</cell><cell>43.7</cell><cell>68.5</cell><cell>63.2</cell><cell>51.8</cell><cell>76.8</cell><cell>57.6</cell></row><row><cell>CDAN [45]</cell><cell>50.7</cell><cell>70.6</cell><cell>76.0</cell><cell>57.6</cell><cell>70.0</cell><cell>70.0</cell><cell>57.4</cell><cell>50.9</cell><cell>77.3</cell><cell>70.9</cell><cell>56.7</cell><cell>81.6</cell><cell>65.8</cell></row><row><cell>MMDIA [35]</cell><cell>56.2</cell><cell>77.9</cell><cell>79.2</cell><cell>64.4</cell><cell>73.1</cell><cell>74.4</cell><cell>64.2</cell><cell>54.2</cell><cell>79.9</cell><cell>71.2</cell><cell>58.1</cell><cell>83.1</cell><cell>69.5</cell></row><row><cell>MME [60]</cell><cell>54.2</cell><cell>72.8</cell><cell>78.3</cell><cell>57.9</cell><cell>70.2</cell><cell>71.8</cell><cell>58.5</cell><cell>52.9</cell><cell>77.9</cell><cell>72.7</cell><cell>58.1</cell><cell>81.8</cell><cell>67.3</cell></row><row><cell>CDS / MME [39]</cell><cell>56.9</cell><cell>73.3</cell><cell>76.5</cell><cell>62.8</cell><cell>73.1</cell><cell>71.1</cell><cell>63.0</cell><cell>57.9</cell><cell>79.4</cell><cell>72.5</cell><cell>62.5</cell><cell>83.0</cell><cell>69.3</cell></row><row><cell>PCS (Ours)</cell><cell>55.8</cell><cell>76.9</cell><cell>80.3</cell><cell>67.9</cell><cell>74.0</cell><cell>75.7</cell><cell>67.0</cell><cell>52.9</cell><cell>81.0</cell><cell>74.5</cell><cell>58.3</cell><cell>82.8</cell><cell>70.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 13 :</head><label>13</label><figDesc>Adaptation accuracy (%) comparison on fully-labeled setting on the Office dataset.</figDesc><table><row><cell>Method</cell><cell cols="7">A?D A?W D?A D?W W?A W?D Avg</cell></row><row><cell>SO</cell><cell>68.9</cell><cell>68.4</cell><cell>62.5</cell><cell>96.7</cell><cell>60.7</cell><cell>99.3</cell><cell>76.1</cell></row><row><cell>DANN [18]</cell><cell>79.7</cell><cell>82</cell><cell>68.2</cell><cell>96.9</cell><cell>67.4</cell><cell>99.1</cell><cell>82.2</cell></row><row><cell>CDAN [45]</cell><cell>92.9</cell><cell>94.1</cell><cell>71</cell><cell>98.6</cell><cell>69.3</cell><cell>100</cell><cell>87.7</cell></row><row><cell>MMDIA [35]</cell><cell>92.1</cell><cell>90.3</cell><cell>75.3</cell><cell>98.7</cell><cell>74.9</cell><cell>99.8</cell><cell>88.8</cell></row><row><cell>MME [60]</cell><cell>88.8</cell><cell>87.3</cell><cell>69.2</cell><cell>98.7</cell><cell>65.6</cell><cell>100</cell><cell>84.9</cell></row><row><cell>CDS + MME [39]</cell><cell>86.9</cell><cell>88.3</cell><cell>75.9</cell><cell>98.6</cell><cell>73.3</cell><cell>100</cell><cell>87.1</cell></row><row><cell>PCS (Ours)</cell><cell>94.6</cell><cell>92.1</cell><cell>77.4</cell><cell>97.7</cell><cell>77.0</cell><cell>99.8</cell><cell>89.8</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Selfsupervised learning for domain adaptation on point-clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haggai</forename><surname>Idan Achituve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gal</forename><surname>Maron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chechik</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.12641,2020.3</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Self-labelling via simultaneous clustering and representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuki</forename><forename type="middle">M</forename><surname>Asano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Rupprecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<idno>2020. 3</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning representations by maximizing mutual information across views</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devon</forename><surname>Hjelm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Buchwalter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="15535" to="15545" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Mixmatch: A holistic approach to semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avital</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><forename type="middle">A</forename><surname>Raffel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5049" to="5059" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Unsupervised pixellevel domain adaptation with generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantinos</forename><surname>Bousmalis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Dohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Domain generalization by solving jigsaw puzzles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fabio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio D&amp;apos;</forename><surname>Carlucci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvia</forename><surname>Innocente</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barbara</forename><surname>Bucci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tatiana</forename><surname>Caputo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tommasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2229" to="2238" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Unsupervised learning of visual features by contrasting cluster assignments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="834" to="848" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Action segmentation with joint selfsupervised temporal domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min-Hung</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baopu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingze</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ghassan</forename><surname>Al-Regib</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zsolt</forename><surname>Kira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9454" to="9463" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.05709</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Big self-supervised models are strong semi-supervised learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.10029,2020.3</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.04297,2020.3</idno>
		<title level="m">Improved baselines with momentum contrastive learning</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Unsupervised visual representation learning by context prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Multi-task selfsupervised visual learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2051" to="2060" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Decaf: A deep convolutional activation feature for generic visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="647" to="655" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Discriminative unsupervised feature learning with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jost</forename><forename type="middle">Tobias</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="766" to="774" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation by backpropagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaroslav</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1180" to="1189" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Domain-adversarial training of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaroslav</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeniya</forename><surname>Ustinova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hana</forename><surname>Ajakan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Germain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fran?ois</forename><surname>Laviolette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Marchand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">16</biblScope>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Metric-guided prototype learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vivien</forename><surname>Sainte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fare</forename><surname>Garnot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Loic</forename><surname>Landrieu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.03047</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Domain generalization for object recognition with multi-task autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammad</forename><surname>Ghifary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengjie</forename><surname>Bastiaan Kleijn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Balduzzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2551" to="2559" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep reconstructionclassification networks for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammad</forename><surname>Ghifary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengjie</forename><surname>Bastiaan Kleijn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Balduzzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="597" to="613" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Unsupervised representation learning by predicting image rotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spyros</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Praveer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.07728</idno>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Domain adaptation for object recognition: An unsupervised approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raghuraman</forename><surname>Gopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruonan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rama</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 international conference on computer vision</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="999" to="1006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Semi-supervised learning by entropy minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yves</forename><surname>Grandvalet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="529" to="536" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A kernel two-sample test</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Karsten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Borgwardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Rasch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Sch?lkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="723" to="773" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Bootstrap your own latent: A new approach to self-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Bastien</forename><surname>Grill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florent</forename><surname>Altch?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Corentin</forename><surname>Tallec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pierre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elena</forename><surname>Richemond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Buchatskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernardo</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaohan</forename><forename type="middle">Daniel</forename><surname>Avila Pires</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><forename type="middle">Gheshlaghi</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Azar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.07733</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Development and validation of a deep learning algorithm for detection of diabetic retinopathy in retinal fundus photographs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lily</forename><surname>Varun Gulshan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Coram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Stumpe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arunachalam</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhashini</forename><surname>Narayanaswamy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kasumi</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Widner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jorge</forename><surname>Madams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cuadros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Jama</title>
		<imprint>
			<biblScope unit="volume">316</biblScope>
			<biblScope unit="issue">22</biblScope>
			<biblScope unit="page" from="2402" to="2410" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Cycada: Cycle-consistent adversarial domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1989" to="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4700" to="4708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Self-supervised domain adaptation for patientspecific, real-time tissue tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sontje</forename><surname>Ihler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Kuhnke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max-Heinrich</forename><surname>Laves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Ortmaier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="54" to="64" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Self-supervised sim-to-real adaptation for visual robotic manipulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rae</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusuf</forename><surname>Aytar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Khosid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxiang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jackie</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Lampe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantinos</forename><surname>Bousmalis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesco</forename><surname>Nori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2718" to="2724" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Implicit class-conditioned domain alignment for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qicheng</forename><surname>Lao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><surname>Matwin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Havaei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Self-supervised visual feature learning with deep neural networks: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longlong</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingli</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="issue">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Billionscale similarity search with gpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.08734</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Contrastive adaptation network for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoliang</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Cross-domain self-supervised learning for domain adaptation with few source labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donghyun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuniaki</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tae-Hyun</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bryan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><surname>Plummer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Sclaroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Saenko</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.08264</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Learning representations for automatic colorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gustav</forename><surname>Larsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Shakhnarovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="577" to="593" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junnan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hoi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.04966</idno>
		<title level="m">Prototypical contrastive learning of unsupervised representations</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Coupled generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oncel</forename><surname>Tuzel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="469" to="477" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Learning transferable features with deep adaptation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Jordan</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="97" to="105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Conditional adversarial domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangjie</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael I Jordan</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation with residual transfer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael I Jordan</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="136" to="144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Deep transfer learning with joint adaptation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael I Jordan</forename></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2208" to="2217" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">14</biblScope>
			<date type="published" when="2008-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Self-supervised learning of pretext-invariant representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6707" to="6717" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Image to image translation for domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zak</forename><surname>Murez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soheil</forename><surname>Kolouri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Kriegman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ravi</forename><surname>Ramamoorthi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyungnam</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4500" to="4509" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Unsupervised learning of visual representations by solving jigsaw puzzles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Noroozi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Favaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Representation learning with contrastive predictive coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yazhe</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03748</idno>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Context encoders: Feature learning by inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepak</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Krahenbuhl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2536" to="2544" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Moment matching for multi-source domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingchao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qinxun</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xide</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zijun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingchao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Usman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neela</forename><surname>Kaushik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dequan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Visda</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.06924</idno>
		<title level="m">The visual domain adaptation challenge</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">From source to target and back: symmetric bi-directional adaptive gan</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Russo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><forename type="middle">M</forename><surname>Carlucci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tatiana</forename><surname>Tommasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barbara</forename><surname>Caputo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8099" to="8108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Adapting visual category models to new domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Kulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Fritz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Semi-supervised domain adaptation via minimax entropy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuniaki</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donghyun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><surname>Sclaroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Generate to adapt: Aligning domains using generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Swami</forename><surname>Sankaranarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yogesh</forename><surname>Balaji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Carlos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rama</forename><surname>Castillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8503" to="8512" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Wasserstein distance guided representation learning for domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanru</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weinan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.01217</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Learning from simulated and unsupervised images through adversarial training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oncel</forename><surname>Tuzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Susskind</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenda</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Russell</forename><surname>Webb</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2107" to="2116" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Correlation alignment for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baochen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Domain Adaptation in Computer Vision Applications</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="153" to="171" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">Unsupervised domain adaptation through self-supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.11825</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.05849</idno>
		<title level="m">Contrastive multiview coding</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Unbiased look at dataset bias</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR 2011</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1521" to="1528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Adversarial discriminative domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="7167" to="7176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">Deep domain confusion: Maximizing for domain invariance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.3474</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Deep hashing network for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hemanth</forename><surname>Venkateswara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><surname>Eusebio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shayok</forename><surname>Chakraborty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sethuraman</forename><surname>Panchanathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanchen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Lasenby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">J</forename><surname>Kusner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.01089</idno>
		<title level="m">Pre-training by completing point clouds</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation via structured prediction based selective pseudolabeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toby</forename><surname>Breckon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="6243" to="6250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Unsupervised feature learning via non-parametric instance discrimination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhirong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Stella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Learning semantic representations for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoan</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zibin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuan</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5423" to="5432" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Self-supervised adaptation of high-fidelity face models for monocular performance tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jae</forename><surname>Shin Yoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takaaki</forename><surname>Shiratori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-I</forename><surname>Shoou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyun</forename><forename type="middle">Soo</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4601" to="4609" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Domain randomization and pyramid consistency: Simulation-to-real generalization without accessing target domain data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sicheng</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>Sangiovanni-Vincentelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Keutzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boqing</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2100" to="2110" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Colorful image colorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2881" to="2890" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Unpaired image-to-image translation using cycleconsistent adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2223" to="2232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Deep unsupervised convolutional domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junbao</forename><surname>Zhuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuhui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weigang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingming</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM international conference on Multimedia</title>
		<meeting>the 25th ACM international conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="261" to="269" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
