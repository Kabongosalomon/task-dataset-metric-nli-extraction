<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Oriented Response Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanzhao</forename><surname>Zhou</surname></persName>
							<email>zhouyanzhao215@mails.ucas.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixiang</forename><surname>Ye</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Qiu</surname></persName>
							<email>qiang.qiu@duke.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">Duke University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbin</forename><surname>Jiao</surname></persName>
							<email>jiaojb@ucas.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Oriented Response Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T21:36+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Deep Convolution Neural Networks (DCNNs) are capable of learning unprecedentedly effective image representations. However, their ability in handling significant local and global image rotations remains limited. In this paper, we propose Active Rotating Filters (ARFs) that actively rotate during convolution and produce feature maps with location and orientation explicitly encoded. An ARF acts as a virtual filter bank containing the filter itself and its multiple unmaterialised rotated versions. During backpropagation, an ARF is collectively updated using errors from all its rotated versions. DCNNs using ARFs, referred to as Oriented Response Networks (ORNs), can produce within-class rotation-invariant deep features while maintaining inter-class discrimination for classification tasks. The oriented response produced by ORNs can also be used for image and object orientation estimation tasks. Over multiple state-of-the-art DCNN architectures, such as VGG, ResNet, and STN, we consistently observe that replacing regular filters with the proposed ARFs leads to significant reduction in the number of network parameters and improvement in classification performance. We report the best results on several commonly used benchmarks 1 .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The problem of orientation information encoding has been extensively investigated in hand-crafted features, e.g., Gabor features <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b16">17]</ref>, HOG <ref type="bibr" target="#b8">[9]</ref>, and SIFT <ref type="bibr" target="#b30">[31]</ref>. In Deep Convolution Neural Networks (DCNNs), the inherent properties of convolution and pooling alleviate the effect of local transitions and warps; however, lacking the capability to handle large image rotation limits DCNN's performance in many visual tasks including object boundary detection <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b31">32]</ref>, multi-oriented object detection <ref type="bibr" target="#b5">[6]</ref>, and image classification <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b22">23]</ref>. <ref type="bibr">Figure</ref> 1. An ARF is a filter of the size W ? W ? N , and viewed as N-directional points on a W ? W grid. The form of the ARF enables it to effectively define relative rotations, e.g., the head rotation of a bird about its body. An ARF actively rotates during convolution; thus it acts as a virtual filter bank containing the canonical filter itself and its multiple unmaterialised rotated versions. In this example, the location and orientation of birds in different postures are captured by the ARF and explicitly encoded into a feature map.</p><p>Due to the lack of ability in fully understanding rotations, the most straightforward way for DCNN to decrease its loss is "learning by rote". The visualization of convolutional filters <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b46">47]</ref> indicates that different rotated versions of one identical image structure are often redundantly learned in low-level, middle-level, and relatively high-level filters, such as those in the VGG-16 model trained on ImageNet <ref type="bibr" target="#b9">[10]</ref>. When object parts rotate relatively to objects themselves, e.g., bird's head to its body, it requires learning multiple combinations of each orientation-distinct component with more convolutional filters. In such cases, the network could give up understanding the concept of the whole object and tend to use a discriminative part of it to make the final decisions <ref type="bibr" target="#b47">[48]</ref>. The learning-by-rote strategy needs a larger number of parameters to generate orientationredundant filters, significantly increasing both the training time and the risk of network over-fitting. Besides, the training data is not sufficiently utilized since the limited instances are implicitly split into subsets, which could increase the possibility of filter under-fitting. To alleviate such a problem, data augmentation, e.g., rotating each training sample into multi-oriented versions, is often used. Data augmentation improves the learning performance by extending the training set. However, it usually requires more network parameters and higher training cost.</p><p>In this paper, we propose Active Rotating Filters (ARFs) and leverage Oriented Response Convolution (ORConv) to generate feature maps with orientation channels that explicitly encode the location and orientation information of discriminative patterns. Compared to conventional filters, ARFs have an extra dimension to define the arrangement of oriented structures. During the convolution, each ARF rotates and produces feature maps to capture the response of receptive fields from multiple orientations, as shown in <ref type="figure">Fig. 1</ref>. The feature maps with orientation channels carry the oriented response along with the hierarchical network to produce high-level representations, endowing DCNNs the capability of capturing global/local rotations and the generalization ability for rotated samples never seen before.</p><p>Instead of introducing extra functional modules or new network topologies, our method implements the prior knowledge of rotation to the most basic element of DCNNs, i.e., the convolution operator. Thus, it can be naturally fused with modern DCNN architectures, upgrading them to more expressive and compact Oriented Response Networks (ORNs). With the orientation information that ORNs produce, we can either apply SIFT-like feature alignment to achieve rotation invariance or perform image/object orientation estimation. The contributions of this paper are summarized as follows:</p><p>? We specified Active Rotating Filters and Oriented Response Convolution, improved the most fundamental module of DCNN and endowed DCNN the capability of explicitly encoding hierarchical orientation information. We further applied such orientation information to rotation-invariant image classification and object orientation estimation. ? We upgraded successful DCNNs including VGG, ResNet, TI-Pooling and STN to ORNs, achieving state-of-the-art performance with significantly fewer network parameters on popular benchmarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Hand-crafted features.</head><p>Orientation information has been explicitly encoded in classical hand-crafted features including Weber's Law descriptor <ref type="bibr" target="#b4">[5]</ref>, Gabor features <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b16">17]</ref>, SIFT <ref type="bibr" target="#b30">[31]</ref>, and LBP <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b0">1]</ref>. SIFT descriptor <ref type="bibr" target="#b30">[31]</ref> and its modification with affine-local regions <ref type="bibr" target="#b24">[25]</ref> find the dominant orientation of a feature point, according to which statistics of local gradient directions of image intensities are accumulated to give a summarizing description of local image structures. With dominant orientation based feature alignment, SIFT achieves invariance to rotation and robustness to moderate perspective transforms <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b11">12]</ref>. Starting from the gray values of a circularly symmetric neighbor set of pixels in a local neighborhood, LBP derives an operator that is by definition invariant against any monotonic transformation of the gray scale <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b0">1]</ref>. Rotation invariance is achieved by minimizing the LBP code value using the bit cyclic shift. Other representative descriptors including CF-HOG <ref type="bibr" target="#b38">[39]</ref> that uses orientation alignment and RI-HOG <ref type="bibr" target="#b29">[30]</ref> that leverages radial gradient transform to be rotation invariant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Deep Convolutional Neural Networks.</head><p>Deep Convolution Neural Networks have the capability of processing transforms including moderate transitions, scale changes, and small rotations. Such capability is endowed with the inherent properties of convolutional operations, redundant convolutional filters, and hierarchical spatial pooling <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b19">20]</ref>. More general pooling operations <ref type="bibr" target="#b25">[26]</ref> permit to consider invariance to local deformation that however does not correspond to specific prior knowledge.</p><p>Data augmentation. Given rich, and often redundant, convolutional filters, data augmentation can be used to achieve local/global transform invariance <ref type="bibr" target="#b42">[43]</ref>. Despite the effectiveness of data augmentation, the main drawback lies in that learning all the possible transformations of augmented data usually requires more network parameters, which significantly increases the training cost and the risk of over-fitting. Most recent TI-Pooling <ref type="bibr" target="#b22">[23]</ref> alleviates the drawbacks by using parallel network architectures for the considered transform set and applying the transform invariant pooling operator on their outputs before the top layer. The essence of TI-Pooling comprises multi-instance learning and weight sharing which help to find the most optimal canonical instance of the input images for training, as well as reducing the redundancy in learned networks. Nevertheless, with built-in data augmentation, TI-Pooling requires significantly more training and testing cost than a standard DCNN.</p><p>Spatial Transform Network. Representatively, the spatial transformer network (STN) <ref type="bibr" target="#b19">[20]</ref> introduces an additional network module that can manipulate the feature maps according to the transform matrix estimated with a localisation sub-CNN. STN contributes a general framework for spatial transform, but the problem about how to precisely estimate the complex transform parameters by CNN remains not being well-solved <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b33">34]</ref>. In <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b35">36]</ref>, the Convolutional Restricted Boltzmann Machine (C-RBM) induces transformation-aware filters, i.e., it yields filters that have a notion with which specific image transformation they are used. From the view of group theory, Cohen et al. <ref type="bibr" target="#b7">[8]</ref> justified that the spatial transform of images could be reflected in both feature maps and filters, providing a theoretical foundation for our work. Most recent works  <ref type="figure">Figure 2</ref>. An ARF F is clockwise rotated by ? to yield its rotated variant F ? in two steps: coordinate rotation and orientation spin. <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b12">13]</ref> have tried rotating conventional filters to perform rotation-invariant texture and image classification; however, without upgrading conventional filters to multi-oriented filters with orientation channels, their capability about capturing hierarchical and fine-detailed orientation information remains limited.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Oriented Response Networks</head><p>Oriented Response Networks (ORNs) are deep convolutional neural networks using Active Rotating Filters (ARFs). An ARF is a filter that actively rotates during convolution to produce a feature map with multiple orientation channels. Thus, an ARF acts as a virtual filter bank with only one filter being materialized and learned. With ARFs, ORNs require significantly fewer network parameters with negligible computation overhead and enable explicitly hierarchical orientation information encoding.</p><p>In what follows, we address three problems in adopting ARFs in DCNN. First, we construct a two-step technique to efficiently rotate an ARF based on the circular shift property of Fourier Transform. Second, we describe convolutions that use ARFs to produce feature maps with location and orientation explicitly encoded. Third, we show how all rotated versions of an ARF contribute to its learning during the back-propagation update stage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Active Rotating Filters</head><p>An Active Rotating Filter (ARF) is a filter of the size W ? W ? N that actively rotates N ? 1 times during convolution to produce a feature map of N orientation channels, <ref type="figure">Fig. 2</ref>. Therefore, an ARF F can be virtually viewed as a bank of N filters (N ?W ?W ?N ), where only the canonical filter F itself is materialized and to be learned, and the remaining N ?1 filters are its unmaterialized copies. The n-th filter in such a filter bank, n ? [1, N ? 1], is obtained by clockwise rotating F by 2?n</p><p>N . An ARF contains N orientation channels and is viewed as N -directional points on a W ? W grid. Each element in an ARF F can be accessed with</p><formula xml:id="formula_0">? ? F ij (n) where 0 ? |i|, |j| ? W ?1 2 , 0 ? n ? N ? 1, i, j, n ? N.</formula><p>An ARF F is clockwise rotated by ? to yield its rotated variant F ? through the following two steps, coordinate rotation and orientation spin.</p><p>Coordinate Rotation. An ARF rotates around the origin O, <ref type="figure">Fig. 2</ref>, and the point at</p><formula xml:id="formula_1">(p, q) in F ? is calcu- lated from four neighbors around (p , q ) in F, ( p q ) = ( p q ) cos(?) sin(?) ?sin(?) cos(?) , using bilinear interpolation ???? F ?,pq = (1 ? ?)(1 ? ?) ? ? ? F uv + (1 ? ?)? ? ??? ? F u,v+1 + ?(1 ? ?) ? ??? ? F u+1,v + ?? ? ????? ? F u+1,v+1 ,<label>(1)</label></formula><formula xml:id="formula_2">where u = p , v = q , ? = p ? u, ? = q ? v.</formula><p>Note that points outside the inscribed circle are padded with 0. Orientation Spin. As discussed, an ARF can be viewed as N -directional points on a grid. Each N -directional point ???? F ?,pq is the N -points uniform sampling of a desired oriented response F ?,pq (?), which is a continuous periodic function of angle ? with period 2?. After the coordinates rotation, it still requires a clockwise spin by ? to yield ? ?? ? F ?,pq , which is, in fact, the quantization of F ?,pq (? ? ?), <ref type="figure">Fig. 2</ref>. Therefore, such spin procedure can be efficiently tackled in Fourier domain by using the circular shift property of Discrete Fourier Transforms (DFT),</p><formula xml:id="formula_3">X(k) ? DFT{ ???? F ?,pq (n) } = N ?1 n=0 ???? F ?,pq (n) e ?jk 2?n N , k=0,1,...,N ?1,<label>(2)</label></formula><formula xml:id="formula_4">? ?? ? F ?,pq (n) ? IDFT{X(k)e ?jk? } = 1 N N ?1 k=0 X(k)e jk( 2?n N ??) , n=0,1,...,N ?1.<label>(3)</label></formula><p>To smoothly process all rotation angles, ARFs require a considerable amount of orientation channels. In practice, thanks to the orientation 'interpolation' by multi-layer pooling operations, we can use a limited amount of orientations to guarantee the accuracy. The successful practice of DCNNs, e.g., VGG <ref type="bibr" target="#b37">[38]</ref> and ResNet <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19]</ref>, shows that the stacks of multiple small filters are more expressive and parameters-efficient than large filters. Moreover, when using the combination of small filters and a limited number of orientation channels, the computational complexity of rotating ARF can be further reduced, since both the coordinate rotation and the orientation spin can be calculated by the circular shift operator and implemented via high-efficient memory mapping under reasonable approximations. Take a 3 ? 3 ? 8 ARFF as an example, calculations of its ? clockwise rotated versionF ? are formulated as</p><formula xml:id="formula_5">? ??? ? F ?, i = ? ?????????? ? F (i?k) mod N , i?I, ? ? F ? (n) = ?? F ? ((n?k) mod N ) , n=0,1,...,N ?1,<label>(4)</label></formula><p>where ?k ? N, ? = k 2? N , N = 8 and I = 7 0 1 6 2 5 4 3</p><p>is a mapping table that defines the index of each surrounding element, which means</p><formula xml:id="formula_6">? ? ? F 0 ? ? ? ? F 0,1 , ? ? ? F 1 ? ? ? ? F 1,1 , ? ? ? F 2 ? ? ? ? F 1,0 , ? ? ? F 3 ? ? ?? ? F 1,?1 and so on.</formula><p>Given the above, we use 1 ? 1 and 3 ? 3 ARFs with 4 and 8 orientation channels in most experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Oriented Response Convolution</head><p>An ARF actively rotates N ? 1 times during convolution to produce a feature map of N orientation channels, and such feature map explicitly encodes both location and orientation information. As an ARF is defined as the size W ? W ? N , both an ARF F and an N -channel feature map M can be viewed as N -directional points on a grid. With ARF, we define the Oriented Response Convolution over F and M, denoted asM = ORConv(F, M). The output feature mapM consists of N orientation channels and the k-th channel is computed as</p><formula xml:id="formula_7">M (k) = N ?1 n=0 F (n) ? k * M (n) , ? k = k 2? N , k=0,...,N ?1, (5) where F ? k is the clockwise ? k -rotated version of F, F (n) ? k</formula><p>and M (n) are the n-th orientation channel of F ? k and M respectively. According to <ref type="bibr" target="#b4">(5)</ref>, the k-th orientation channel of the output feature mapM is generated by ? k rotated versions of the materialised ARF. It means that in each oriented response convolution, the ARF proactively captures image response in multiple directions and explicitly encodes its location and orientation into a single feature map with multiple orientation channels, visualized in <ref type="figure" target="#fig_1">Fig. 3</ref>. (5) also demonstrates that each orientation channel of the ARF contributes to the final convolutional response respectively, endowing ORNs the capability of capturing richer and more fine-detailed patterns than a regular CNN. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Updating Filters</head><p>During the back-propagation, error signals ? (k) of all rotated versions of the ARF are aligned to ? (k) ?? k using <ref type="formula" target="#formula_1">(1)</ref> and <ref type="formula" target="#formula_3">(2)</ref>, and aggregated to update the materialised ARF,</p><formula xml:id="formula_8">? (k) = ?L ?F ? k , ? k = k 2? N , k=0,1,...,N ?1, F ? F ? ? N ?1 0 ? (k) ?? k ,<label>(6)</label></formula><p>where L stands for training loss and ? for learning rate. An ARF acts as a virtual filter bank containing the materialized canonical filter itself and unmaterialised rotated versions. According to <ref type="bibr" target="#b5">(6)</ref>, the back-propagation collectively updates the materialised filter only, so that training errors of appearance-like but orientation-distinct samples are aggregated. In low-level layers, such collective updating  contributes more significantly, as in a single image there exist many appearance-like but orientation-distinct patches that can be exploited. The collective updating also helps when only limited training samples are given. One example of a collectively updated ARF is shown in <ref type="figure" target="#fig_3">Fig. 4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Rotation Invariant Feature Encoding</head><p>Feature maps in ORNs are not rotation-invariant as orientation information are encoded instead of being discarded. When within-class rotation-invariance is required, we introduce two strategies, ORAlign and ORPooling, at the top layer of ORNs. For simplicity, we choose a DCNN architecture, where the size of a feature map gradually shrinks to 1 ? 1 ? N . N is the number of orientation channels. Each feature map of the last ORConv layer has a receptive field of image size and stands for the oriented response of high-level representative patterns.</p><p>The first strategy is the ORAlign. Without loss of generality, let us denote the i-th feature map of the last ???? M{i} is an N dimension tensor records the response from different directions, with which we perform SIFT-like alignment to achieve rotation robustness. This is done by first calculating the dominant orientation (the orientation with the strongest response) as <ref type="bibr">(d)</ref> and spin the feature by ?D 2? N , <ref type="figure" target="#fig_1">Fig. 3</ref>. The second strategy is the ORPooling, which is done via simply pooling a ???? M{i} to a scalar max( ???? M{i} (j) ), 0 &lt; j &lt; N ? 1. This strategy reduces the feature dimension but loses feature arrangement information.</p><formula xml:id="formula_9">D = argmax d ???? M{i}</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>ORNs are evaluated on three benchmarks. In Sec. <ref type="bibr" target="#b3">4</ref>  [24] to validate its generalization ability on rotation. In Sec. 4.2, on a weakly-supervised orientation estimate task, the vast potential of directly taking advantage of the orientation information extracted by ORNs is demonstrated. In Sec. 4.3, we upgrade the VGG <ref type="bibr" target="#b37">[38]</ref>, ResNet <ref type="bibr" target="#b17">[18]</ref>, and the WideResNet <ref type="bibr" target="#b44">[45]</ref> to ORNs, and train them on CIFAR10 and CIFAR100 <ref type="bibr" target="#b21">[22]</ref>, showing the state-of-the-art performance on the natural image classification task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Rotation Invariance</head><p>Rotated MNIST. We randomly rotate each sample in the MNIST dataset <ref type="bibr" target="#b28">[29]</ref> between [0, 2?] to yield MNIST-rot. To assess the effect of data augmentation on different models, we further rotate each sample in the MNIST-rot training set to eight directions with 45-degree intervals, which means that the training set is augmented eightfold. The augmented data set is identified as MNIST-rot+.</p><p>We set up a baseline CNN with four convolutional layers and multiple 3x3 filters, <ref type="figure" target="#fig_5">Fig. 5</ref>. With the baseline CNN, we generate different ORNs, as well as configuring the STNs <ref type="bibr" target="#b19">[20]</ref> and the TI-Pooling network <ref type="bibr" target="#b22">[23]</ref>   ARFs are more expressive than conventional filters, the number of ARFs in each layer is decreased to one-eighth of those in the baseline. Corresponding to the strategies proposed in Sec. 3.4, we use ORAlign, ORPooling or none of them to encode rotation-invariant features. The network topologies are shown in <ref type="figure" target="#fig_5">Fig. 5</ref>.</p><p>In network training, we use the same hyper-parameters as TI-Pooling <ref type="bibr" target="#b22">[23]</ref>, i.e., 200 training epochs using the turning-free convergent adadelta algorithm <ref type="bibr" target="#b45">[46]</ref>, 128 batch size, and 0.5 dropout rate for the fully-connected layer. For each dataset, we randomly selected 10,000 samples from the training set for validation and the remaining 50,000 samples for training. The best model selected by 5-fold cross-validation is then applied to the test set, and the final results are presented in Tab. 1.</p><p>The second column of Tab. 1 shows that ORN keeps high training efficiency. The ORN-4 (4 orientation channels) uses only 50% training time while ORN-8 uses similar training time with the baseline CNN. In contrast, TIPooling increases the time by about eight times as each sample is augmented to 8 orientations. From the third to the last column of Tab. 1, it can be seen that ORNs can use significantly fewer network parameters (7.95%-31.4%) to consistently improve the performance. Even on the original dataset without sample rotations, it achieves 22% error rate decrease (0.57% vs 0.73%), as the digit curvatures are well modeled by ORN. Compared with the data augmentation strategy (baseline CNN on rot+), ORN (on rot) not only reduces network parameters and training cost but also achieves significant lower error rate (1.37% vs 2.19%).</p><p>Tab. 1 also shows that different rotation-invariant encoding strategies have different advantages. ORPooling can further compress the feature dimension and network parameters, while ORAlign retains the complete feature structure thus achieves higher performance. Even without rotation-invariant encoding, ORNs outperforms the baseline on the rot and rot+, because ARFs can explicitly capture the response in different directions so that a pattern and its rotated versions can be encoded in the same feature map with orientation channels, <ref type="figure" target="#fig_1">Fig. 3</ref>. It also can be seen in <ref type="figure" target="#fig_6">Fig. 6(c)</ref> that the t-SNE <ref type="bibr" target="#b41">[42]</ref> 2D mapping of features produced by ORN-8(None) constitutes clear clusters.</p><p>In Tab. 1, the state-of-the-art spatial transform network, STN, has minor improvement on the rot while slightly increasing the number of parameters. The visualization of calibrated images shows that it often outputs wrong transform parameters. This validates our previous viewpoint: the conventional CNN used in STN lacks the capability to precisely estimate rotation parameters. In Sec. 4.2, we will show that ORN can better solve such a problem.</p><p>The last column of Tab. 1 presents the results of crossgeneralization evaluation that trains models on the MNISToriginal and tests them on the MNIST-rot. ORNs show impressing performance with 71% improvement over the Method Error(%) ScatNet-2 <ref type="bibr" target="#b2">[3]</ref> 7.48 PCANet-2 <ref type="bibr" target="#b3">[4]</ref> 7.37 TIRBM <ref type="bibr" target="#b39">[40]</ref> 4.2 CNN 4.34 ORN-8(ORAlign) 2.25 TIPooling(with augmentation) <ref type="bibr" target="#b22">[23]</ref> 1.93 OR-TIPooling(with augmentation) 1.54 <ref type="table">Table 2</ref>. Classification error rates on the MNIST-rot-12k. baseline. <ref type="figure" target="#fig_6">Fig. 6(d)</ref> shows that ORN-8(ORAlign) produces much clearer feature distribution in manifold than other networks. An interesting experiment comes from the digit class '6' and '9'. It can be seen in <ref type="figure" target="#fig_7">Fig. 7</ref> that both CNN and STN have large within-class differences as the same digit with different angles produce various radii. Moreover, features generated by CNN and STN have apparently 180 o symmetrical distribution, which means that they can barely tell the difference between upside-down 6 and 9. In contrast, ORN-8(ORAlign) generates within-class rotation-invariant deep features, while maintaining inter-class discrimination.</p><p>Rotated Small Sample Set. A smaller dataset can better test the generalization capability of a learning model. We consider the MNIST-rot-12k dataset <ref type="bibr" target="#b23">[24]</ref> which contains 12,000 training samples and 50,000 test samples from the MNIST-rot dataset. Among them, 2000 training samples are used as the validation set and the remaining 10,000 samples as the training set.</p><p>In the dataset, we test the ORN-8 model that uses 8orientation ARFs and an ORAlign operator. We also test the OR-TIPooling network, which is constructed by upgrading its parallel CNNs to ORN-8(None)s. The reason why we do not use ORAlign or ORPooling is that TIPooling itself has the invariant encoding operator. Tab. 2 shows that ORN can decrease the state-of-the-art error rate from 4.2% to 2.25% using only 31% network parameters of the baseline CNN. Combined with TIPooling, ORN further decreases the error rate to 1.54%, achieving state-of-the-art performance,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Std Error(%) STN <ref type="bibr" target="#b19">[20]</ref> 0.745 3.38 OR-STN(ORAlign) 0.749 3.61 OR-STN 0.397 2.43 <ref type="table">Table 3</ref>. Orientation estimation performance. The second column describes the standard deviation of calibrated orientations and the third column describes the classification error rates. which shows that ORNs have good generalization capability for such reduced training sample cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Orientation Estimation</head><p>ORN is evaluated on the weakly image orientation estimation problem, using the STN <ref type="bibr" target="#b19">[20]</ref> as the baseline. The training images have only class labels but lack orientation annotation, which is estimated during learning. We upgrade the localisation sub-network of STN from a conventional CNN to ORN by converting Conv layers to ORConv layers which use ARFs with eight orientation channels. The STN model is simplified to process rotation only, which means that its localisation network estimates only a rotation parameter.</p><p>STN, OR-STN and OR-STN(ORAlign) are trained on the MNIST-half-rot dataset which is built by randomly rotating each sample in the MNIST dataset in the range [? ? 2 , ? 2 ] (half the circle). All the networks use hyper-parameters as Sec. 4.1 and are trained by only 80 epochs to make the localisation sub-network converge. The orientation estimation results are presented in Tab. 3, the rotationrectified images are shown in <ref type="figure" target="#fig_8">Fig. 8</ref>, and angle statistics of rotation-rectified images are shown in <ref type="figure" target="#fig_9">Fig. 9</ref>. It can be seen in <ref type="figure" target="#fig_8">Fig. 8(b)</ref> that STN cannot effectively handle the large-angle rotation problem, because the localisation sub-network itself is a conventional CNN, lacking the ability to explicitly process significant rotation. When upgrading the localisation network of STN to ORN (without ORAlign), it can be seen in <ref type="figure" target="#fig_8">Fig. 8(d)</ref> that most digit orientations are correctly estimated. In <ref type="figure" target="#fig_9">Fig. 9(b)</ref>, it can be seen that the OR-STN(ORAlign) performs even worse than the baseline on orientation estimation, because after the feature alignment, features become rotation-invariant and thus lose orientation information. Tab. 3 shows that upgrading localisation sub-network to ORN significantly improves the performance. Such experiments validate that the ARFs can capture the orientation information of   <ref type="table">Table 4</ref>. Results on the natural image classification benchmark.</p><p>In the second column, k is the widening factor corresponding to the number of filters in each layer. discriminative patterns and explicitly encode them into feature maps with orientation channels, which are effective for image orientation estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Natural Image Classification</head><p>Although most objects in natural scene images are upright, rotations could exist in small and/or medium scales (from edges to object parts). It is interesting to validate whether ORNs are effective to handle such partial object rotation or not. CIFAR-10 and CIFAR-100 datasets <ref type="bibr" target="#b21">[22]</ref> consist of 32x32 real-world object images drawn from 10 and 100 classes split into 50,000 training and 10,000 testing images. Three DCNNs including VGG <ref type="bibr" target="#b37">[38]</ref>, ResNet <ref type="bibr" target="#b17">[18]</ref> and WideResNet <ref type="bibr" target="#b44">[45]</ref>, are used as baselines on these datasets. Promoting the baselines to ORNs is done by converting each Conv layer to an ORConv layer that uses ARFs with eight orientation channels, and using an additional ORAlign layer to encode rotation invariant representations.</p><p>Following the V2 settings of WideResNet <ref type="bibr" target="#b44">[45]</ref>, image classification results, Tab. 4, show that ORNs consistently improved baselines with much fewer parameters. For example, OR-VGG uses about 50% parameters of the baseline to achieve better results. OR-WideResNet-40-  <ref type="figure">Figure 10</ref>. Sample images that contain rotated objects/parts falsely classified by the ResNet but correctly recognized by the proposed ORNs in CIFAR10.</p><p>2 (without dropout) uses only 12% parameters (4.5M vs 36.5M) to outperform the state-of-the-art WideResNet-28-10 (with dropout) on CIFAR10. OR-WideResNet-28-5 (with dropout) uses about 50% parameters of the baselines yet significantly improve the state-of-the-arts on both CIFAR10 and CIFAR100. The top-3 improved classes of CIFAR10 are frog (31% higher than baseline ResNet), bird (30.7%) and deer (27.3%), which happen to involve significant local and/or global rotations, <ref type="figure">Fig. 10</ref>. This further demonstrates the capability of ORN to process local and global image rotations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>In this paper, we proposed a simple but effective strategy to explicitly encode hierarchical orientation information of discriminative patterns and handle the global/local rotation problem. The primary contribution is designing Active Rotating Filters (ARFs), as well as upgrading the stateof-the-art DCNN architectures, e.g., VGG, ResNet, STN, and TI-Pooling, to Oriented Response Networks (ORNs). Experimentally, ORNs outperform the baseline DCNNs while using significantly fewer (12%-50%) network parameters, which indicates that the usage of model-level rotation prior is a key factor in training compact and effective deep networks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Example feature maps produced by one ARF at each layer of an ORN trained on the rotated MNIST dataset, with digit '4' in different rotations as the inputs (one network layer per row, one input per column). The right-most column magnifies sample regions in feature maps. It clearly shows that a feature map explicitly encodes position and orientation. At the second layer, an image is extended to an omnidirectional map to fit ORConv. At the second-to-last (ORConv4) layer, deep features are observed in similar values but in different orientations, which demonstrates that orientation information is extracted by ORNs. The last (ORAlign) layer performs SIFT-like alignment to enable rotation-invariance (Best viewed zooming on screen).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>A 31 ? 31 ? 16 ARF learned from a texture dataset. It is shown in the N-directional points form (left) and further visualized as one orientation channel per image (right). The ARF clearly defines a texture pattern through a combination of multi-oriented edges (Best viewed zooming on screen).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>ORConv layer as ???? M{i} and each oriented response in it as ???? M{i} (n) , 0 ? n ? N ? 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 .</head><label>5</label><figDesc>Comparison of network topologies.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 .</head><label>6</label><figDesc>Visualization of features in cross-generalization evaluation, corresponding to the last column of Tab. 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 .</head><label>7</label><figDesc>(a) CNN (b) STN(affine) (c) ORN-8(ORAlign) Visualization of features encoding of digit class '6' and '9' from MNIST-rot. Each point (r, ?) corresponds to a sample where radius r is the 1-D tSNE feature mapping, and ? is the angle of the sample. ORN-8(ORAlign) produces withinclass rotation-invariant deep features while maintaining inter-class discrimination. (Best viewed in color.)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 .</head><label>8</label><figDesc>STN(affine) (c) OR-STN(ORAlign) (d) OR-STN Orientation estimation. (a) is a mini-batch of samples from MNIST-half-rot and (b)-(d) are their rotation-rectified results.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 9 .</head><label>9</label><figDesc>Distributions of samples' orientations after rotation-rectification.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>for comparison. STNs are created by inserting a Spatial Transformer with affine or rotation transform to the entry of the baseline CNN. TIPooling network is constructed by duplicating the baseline CNN eight times to capture different augmented rotated versions of inputs, and a transform-invariant pooling layer before the output layer. ORNs are built by upgrading each convolution layer in the baseline CNN to Oriented Results on the MNIST variants. The second column describes the average training time of an epoch on the original training set (with a NVIDIA Tesla K80 GPU). The third column describes the percentage of parameters of each model about the baseline CNN.The fourth to sixth columns describe the error rates on the original, the rot, and the rot+ datasets. The last column describes the error rates achieved on the rot testing set (with random rotation) by models trained on the original training set (without rotation). TIPooling requires augmented data; thus some experiments are not permitted. The error rate of TIPooling on the original MNIST dataset is under augmentation, with the superscript ? to show its difference with others.</figDesc><table><row><cell>Response Convolution layer using Active Rotating Filters</cell></row><row><cell>(ARFs) with 4 or 8 orientation channels. Considering that</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Source code is publicly available at yzhou.work/ORN</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>The authors are very grateful for support by NSFC grant 61671427, BMSTC grant Z161100001616005, STIFCAS grant CXJJ-16Q218, and NSF.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Face description with local binary patterns: Application to face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ahonen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hadid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pietik?inen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2037" to="2041" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">On the use of SIFT features for face authentication</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bicego</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lagorio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Grosso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tistarelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page">35</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Invariant scattering convolution networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mallat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1872" to="1886" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Pcanet: A simple deep learning baseline for image classification?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Processing</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="5017" to="5032" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">WLD: A robust local image descriptor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pietik?inen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1705" to="1720" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Rifd-cnn: Rotation-invariant and fisher discriminative convolutional neural networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Fast and accurate deep network learning by exponential linear units (elus). CoRR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Clevert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<idno>abs/1511.07289</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Group equivariant convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2990" to="2999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Histograms of oriented gradients for human detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="886" to="893" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Visualizing higher-layer features of a deep network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<idno>1341</idno>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
		<respStmt>
			<orgName>University of Montreal</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Multi-view stereo for community photo collections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Goesele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Snavely</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Curless</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hoppe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Seitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Learning rotation invariant convolutional filters for texture classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Volpi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tuia</surname></persName>
		</author>
		<idno>abs/1604.06720</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Rotation-invariant texture classification using modified gabor filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">M</forename><surname>Haley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">S</forename><surname>Manjunath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICIP</title>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="262" to="265" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Oriented edge forests for boundary detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hallman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Fowlkes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1732" to="1740" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Rotation-invariant and scale-invariant gabor features for texture image retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image Vision Comput</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1474" to="1481" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno>abs/1512.03385</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
			<biblScope unit="volume">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Identity mappings in deep residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="630" to="645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Spatial transformer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Transformation equivariant boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Kivinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICANN</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">5</biblScope>
		</imprint>
		<respStmt>
			<orgName>Department of Computer Science, University of Toronto</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">TI-POOLING: transformation-invariant pooling for feature learning in convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Savinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Buhmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
		<idno>abs/1604.06318</idno>
		<imprint>
			<date type="published" when="2005" />
			<publisher>CoRR</publisher>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">An empirical evaluation of deep architectures on problems with many factors of variation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Semi-local affine parts for object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Generalizing pooling functions in convolutional neural networks: Mixed, gated, and tree</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">W</forename><surname>Gallagher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="464" to="472" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deeplysupervised nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">W</forename><surname>Gallagher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Network In Network. ICLR</title>
		<imprint>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Handwritten digit recognition: benchmarking of state-ofthe-art techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Nakashima</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sako</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fujisawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2271" to="2285" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Rotation-invariant HOG descriptors using fourier analysis in polar and spherical coordinates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Skibbe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Blein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Palme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">106</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="342" to="364" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Object recognition from local scale-invariant features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1150" to="1157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Convolutional oriented boundaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Maninis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">A</forename><surname>Arbel?ez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J V</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="580" to="596" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Multiresolution gray-scale and rotation invariant texture classification with local binary patterns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ojala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pietik?inen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>M?enp??</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="971" to="987" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Unsupervised representation learning with deep convolutional generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<idno>abs/1511.06434</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Evaluation of pooling operations in convolutional architectures for object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Scherer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Behnke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICANN</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="92" to="101" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Learning rotation-aware features: From invariant priors to equivariant descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2050" to="2057" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Rotation, scaling and deformation invariant scattering for texture discrimination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sifre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mallat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1233" to="1240" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno>abs/1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
			<biblScope unit="volume">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Circular fourier-hog features for rotation invariant object detection in biomedical images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Skibbe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Reisert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISBI</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="450" to="453" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Learning invariant representations with local transformations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">K</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<idno>abs/1505.00387</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Visualizing Data using t-SNE</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">The Art of Data Augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Van Dyk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-L</forename><surname>Meng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computational and Graphical Statistics</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="50" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Flip-rotate-pooling convolution and split dropout on convolution neural networks for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kong</surname></persName>
		</author>
		<idno>abs/1507.08754</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Wide residual networks. CoRR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
		<idno>abs/1605.07146</idno>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">ADADELTA: an adaptive learning rate method. CoRR, abs/1212</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">5701</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Visualizing and understanding convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="818" to="833" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<title level="m">Learning Deep Features for Discriminative Localization. CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
