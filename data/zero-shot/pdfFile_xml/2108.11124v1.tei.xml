<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Virtual Event, Australia. 2021. Inductive Matrix Completion Using Graph Autoencoder</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Virtual Event, Australia. ACM</publisher>
				<availability status="unknown"><p>Copyright Virtual Event, Australia. ACM</p>
				</availability>
				<date>November 1-5, 2021. November 1-5, 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Shen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuheng</forename><surname>Zhang</surname></persName>
							<email>zhangchuheng123@live.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Tian</surname></persName>
							<email>tianyun@shanghaitech.edu.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zeng</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanchun</forename><surname>Dou</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Xu</surname></persName>
							<email>xlxu@ieee.org</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Shen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuheng</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Tian</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zeng</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaonan</forename><surname>He</surname></persName>
							<email>xiaonan.cs@gmail.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanchun</forename><surname>Dou</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Xu</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Nanjing University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">ShanghaiTech University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">Tsinghua University Xiaonan He</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution">Beihang University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="institution">Nanjing University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff6">
								<orgName type="department">Nanjing University of Information Science and Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Virtual Event, Australia. 2021. Inductive Matrix Completion Using Graph Autoencoder</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 30th ACM Int&apos;l Conf. on Information and Knowledge Management (CIKM &apos;21)</title>
						<meeting>the 30th ACM Int&apos;l Conf. on Information and Knowledge Management (CIKM &apos;21) <address><addrLine>New York, NY, USA</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Virtual Event, Australia. ACM</publisher>
							<biblScope unit="volume">10</biblScope>
							<date type="published">November 1-5, 2021. November 1-5, 2021</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3459637.3482266</idno>
					<note>* The first two authors contribute equally to this work. ? Corresponding author. 1 https://github.com/swtheing/IMC-GAE ACM ISBN 978-1-4503-8446-9/21/11. . . $15.00</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T04:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recently, the graph neural network (GNN) has shown great power in matrix completion by formulating a rating matrix as a bipartite graph and then predicting the link between the corresponding user and item nodes. The majority of GNN-based matrix completion methods are based on Graph Autoencoder (GAE), which considers the one-hot index as input, maps a user (or item) index to a learnable embedding, applies a GNN to learn the node-specific representations based on these learnable embeddings and finally aggregates the representations of the target users and its corresponding item nodes to predict missing links. However, without node content (i.e., side information) for training, the user (or item) specific representation can not be learned in the inductive setting, that is, a model trained on one group of users (or items) cannot adapt to new users (or items). To this end, we propose an inductive matrix completion method using GAE (IMC-GAE), which utilizes the GAE to learn both the user-specific (or item-specific) representation for personalized recommendation and local graph patterns for inductive matrix completion. Specifically, we design two informative node features and employ a layer-wise node dropout scheme in GAE to learn local graph patterns which can be generalized to unseen data. The main contribution of our paper is the capability to efficiently learn local graph patterns in GAE, with good scalability and superior expressiveness compared to previous GNN-based matrix completion methods. Furthermore, extensive experiments demonstrate that our model achieves state-of-the-art performance on several matrix completion benchmarks. Our official code is publicly available 1 .</p><p>? Mathematics of computing ? Graph algorithms; ? Computing methodologies ? Neural networks. KEYWORDS matrix completion, graph neural networks, GAE-based model, inductive learning, recommender system ACM Reference Format:</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Matrix completion (MC) <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b16">17]</ref> is one of the most important problems in modern recommender systems, using past user-item interactions to predict future user ratings or purchases. Specially, given a partially observed user-item historical rating matrix whose entries represent the ratings of users with items, MC is to predict the missing entries (unobserved or future potential ratings) in the matrix based on the observed ones. The most common paradigm of MC is to factorize the rating matrix into the product of low-dimensional latent embeddings of rows (users) and columns (items), and then predict the missing entries based on these latent embeddings. Traditional matrix completion methods <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b4">5]</ref> have achieved great successes in the past. However, these methods mainly learn the latent user (or item) representation yet largely neglect an explicit encoding of the collaborative signal to reveal the behavioral similarity between users <ref type="bibr" target="#b24">[24]</ref>. These signals are crucial for predicting the missing rating in the rating matrix, but hard to be exploited, since they are hidden in user-item interactions <ref type="bibr" target="#b7">[8]</ref>.</p><p>Recently, many works <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b26">26,</ref><ref type="bibr" target="#b28">28]</ref> have studied using a GNN to distill collaborative signals from the user-item interaction graph. Specially, matrix completion is formulated as link prediction, where the rating matrix is formulated as a bipartite graph, with users (or items) as nodes and observed ratings/interactions as links. The goal of GNN-based matrix completion methods is to predict the potential or missing links connecting any pair of nodes in this graph. Graph GAE-based models IGMC IMC-GAE (ours) <ref type="table">Table 1</ref>: We compare the GNN-based matrix methods from different aspects: 1) whether they learn node-specific representations for personalized recommendation (denoted as Specific), 2) whether they learn local graph patterns (denoted as Local), (3) whether they are efficient matrix completion methods (denoted as Efficient), <ref type="bibr" target="#b3">(4)</ref> whether they are inductive matrix completion methods (denoted as Inductive)</p><formula xml:id="formula_0">Specific ? ? ? Local ? ? ? Efficient ? ? ? Inductive ? ? ?</formula><p>Autoencoder (GAE) <ref type="bibr" target="#b11">[12]</ref> is a popular GNN-based link prediction method, where a GNN is first applied to the entire network to learn node-specific representations. Then the representations of the target nodes are aggregated to predict the target link. Many GNN-based matrix completion methods directly apply GAE to the rating graph to predict potential ratings such as GC-MC and NMTR <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b5">6]</ref>. By exploiting the structure of the bipartite user-item graph, the node-specific representations learned by GAE, which represents user-specific preferences or item attributes, are more expressive than the patterns learned by the traditional matrix completion methods for personalized recommendation. Despite its effectiveness, there remain two main challenges to apply GAE-based matrix completion to real recommender systems. The first challenge stems from a key observation from real-world scenarios: There are a large number of users or items in a real recommender system that have few historical ratings. This requires a model to predict potential ratings in a sparse rating matrix. However, GAE-based models usually fail in this situation since there are a few historical ratings in a sparse rating matrix for GAE-based models to train node-specific representations for personalized recommendation <ref type="bibr" target="#b29">[29]</ref>. The second challenge is applying the GAE-based models to real recommender systems for the large-scale recommendation. In real recommender systems, new users (or items) are emerging that are not exposed to the model during training. This requires that the model to be inductive, i.e., the model trained on a group of users (or items) can adapt to new groups. However, previous GAE-based models are all transductive models so that the learned node representations cannot be generalized to users (or items) unseen during training <ref type="bibr" target="#b28">[28]</ref>.</p><p>The following question arises: Can we have a GAE-based model that can not only guarantee good performance on a sparse rating matrix but also enable inductive learning? In fact, using GAE to simultaneously satisfy the two requirements for matrix completion is a non-trivial challenge when high-quality user (or item) features are unavailable. The one-hot node indices (together with learnable node-specific embeddings) in the GAE-based model give a maximum capacity for learning distinct user preferences (or item attributes) from historical ratings. On the other side, learning distinct user preferences (or item attributes) in GAE also requires adequate rating samples from the rating matrix. Accordingly, without adequate rating samples in a sparse rating matrix, it is hard for GAE to obtain satisfactory performance. Moreover, for unseen nodes from a new rating matrix, GAE lacks the representations of them, and therefore cannot predict the potential ratings in a new rating matrix, which makes inductive learning impossible. To overcome these two challenges, Zhang and Chen <ref type="bibr" target="#b28">[28]</ref> propose an inductive matrix completion based on GNN (IGMC). To predict a potential link (i.e., rating), it first extracts a 1-hop subgraph around the target link and then relabels the node w.r.t the distance to the target nodes. Finally, a GNN is applied to each subgraph to learn the local graph patterns that can be generalized to an unseen graph. By learning local graph patterns, IGMC has a better performance on the sparse rating matrix and enables inductive matrix completion. However, extracting subgraphs in both training and inference processes is time-consuming for the real recommendation. Moreover, the performance degradation on the dense rating matrix in IGMC also hinder us from applying it to real recommender systems.</p><p>In this paper, we propose an inductive matrix completion method using GAE (IMC-GAE) that achieves efficient and inductive learning for matrix completion, and meanwhile obtain good performance on both sparse and dense rating matrices. As summarized in <ref type="table">Table 1</ref>, IMC-GAE combines the advantages of both the GAE-based models and IGMC together, which uses GAE to learn both node-specific representation for personalized recommendation, and local graph patterns for inductive matrix completion. Specially, we incorporate two informative node features into IMC-GAE to represent two types of user-item interactions and design a layer-wise node dropout scheme in GAE to learn local graph patterns for inductive matrix completion.</p><p>In summary, this work makes the following main contributions:</p><p>? (Sec. 3.1) To better understand local graph patterns, we conduct a quantitative analysis on five real datasets. Based on this quantitative analysis, we have multiple observations that reveal the properties of local graph patterns in matrix completion. It motivates us to design our model, IMC-GAE. ? (Sec. 3.2) We design two informative features, the identical feature and the role-aware feature, for the model to learn the expressive graph patterns. Moreover, these graph patterns can be easily generalized to unseen graphs. ? (Sec. 3.5) We design a layer-wise node dropout schema that drops out more nodes in the higher layers. With the layer-wise node dropout, link representation in our model contains more node information in a 1-hop local graph around the target link. Accordingly, our model is able to learn local graph patterns associated with the target link, which enhances the capability of the inductive learning of our model. ? (Sec. 5) To illustrate the effectiveness of the proposed IMC-GAE, we conduct empirical studies on five benchmark datasets. Extensive results demonstrate the state-of-the-art performance of IMC-GAE and its effectiveness in learning both local graph patterns and node-specific representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORKS</head><p>In this section, we will briefly review existing works on GAE-based matrix completion methods and inductive matrix completion methods based on GNN, which are most relevant with this work. Here, we highlight their differences to IMC-GAE, and illustrate how we combine the advantages of them to build a more effective model for real recommendation. The rating matrix is formulated as a bipartite user-item graph, in which the nodes represent users (or items) and the links represent the corresponding ratings. In addition, the input features of each node in this graph consist of the identical feature, the role-aware feature, and the one-hot index feature. In addition, the encoder of our model has multiple layers (e.g., Layer 1) with multiple rating-subgraph (e.g., Rating 1). As stacking more layers, the node dropout probability increases, which is referred to as layer-wise node dropout. The model aggregated the latent embedding which is learned by one-hot index feature and structure embedding of a node which is learned by role-aware feature and identical feature in all layers by the weighted sum operator. At last, we reconstruct the links by a bilinear decoder. In this way, the output of our model contains the information of both latent link representation and structure representation</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">GAE-based matrix completion</head><p>The majority of GNN-based matrix completion methods is based on Graph Autoencoder (GAE) <ref type="bibr" target="#b11">[12]</ref>, which applies a GNN to the entire network to learn a representation for each node. The representations of the user and item nodes are aggregated to predict potential ratings. For example, Monti et al. <ref type="bibr" target="#b17">[18]</ref> propose a multi-graph CNN model to extract user and item latent features from their nearestneighbor networks. Berg et al. <ref type="bibr" target="#b1">[2]</ref> propose graph convolutional matrix completion (GC-MC) which uses one-hot encoding of node IDs as initial node features, learns specific node representations by applying a GNN-encoder to the bipartite user-item graph, and reconstructs the rating links by a GNN-decoder. To the best of our knowledge, our method is the first inductive GAE-based matrix completion method that achieves a good performance in both sparse and dense rating matrices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Inductive GNN-based matrix completion methods</head><p>There are mainly two types of GNN-based matrix completion methods that are applicable to inductive settings. One is attempting to handle inductive matrix completion without using node content, such as IGMC <ref type="bibr" target="#b28">[28]</ref>. IGMC first extracts enclosing subgraphs around target links, then relabels the nodes in subgraphs according to their distances to the source and target nodes, and finally applies a GNN to each subgraph to learn a link representation for link prediction. IGMC applies GNN to those enclosing subgraphs to learn local graph patterns, which can easily generalize to the users (or items) unseen during training. Moreover, local graph patterns help IGMC obtain a better performance than the GAE-based models on the sparse rating matrices. However, applying IGMC to real recommender systems yields two crucial challenges. First of all, IGMC replaces nodes' one-hot index embedding with local structure features, which does not capture diverse user preferences and item attributes for personalized recommendation. Second, IGMC extracts subgraphs around target links during both the training and inference process, which is time-consuming for large-scale recommendation. In contrast, IMC-GAE maintains the ability to give a node-specific representation, which is important in personalized recommendation for the users with historical ratings. In addition, instead of extracting subgraphs and relabeling each node, we incorporate two informative features into the input features of each node and design a layer-wise node dropout scheme in IMC-GAE to help the GAE to learn local graph patterns. By using GAE to learn local graph patterns, the inference process of IMC-GAE becomes efficient and inductive. Another previous inductive GNN-based matrix completion methods are content-based models; such as PinSage <ref type="bibr" target="#b25">[25]</ref>, which uses node content as initial node features. Although being inductive and successful in real recommender systems, content-based models </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">METHOD</head><p>As aforementioned, matrix completion has been formulated as the link prediction problem on a bipartite user-item graph in recent GNN-based matrix completion methods. Specially, we consider a matrix completion that deals with a rating matrix of shape ? , where is the number of users and is the number of items. Some entries in this matrix exist and other entries are missing. Existing entry is a historical rating from a user to an item . The task of matrix completion is to predict the value of missing entries. GNN-based matrix completion views the matrix as a bipartite graph and predicts the missing links in this graph. In this section, we first present some findings on multiple real-world datasets, which reveal the properties of local graph patterns in both sparse and dense rating matrices. Based on these observations, we then elaborate on how the proposed learning algorithm, IMC-GAE, integrates the GAE-based model and IGMC to obtain a more effective model for real recommender systems. Then, we show an overview of IMC-GAE in <ref type="figure" target="#fig_0">Figure 1</ref>. Specially, IMC-GAE is a GAEbased model consisting of three major components: 1) embedding layer whose input features consist of the one-hot index of nodes, the identical feature and the role-aware feature, 2) relational GCN encoder, 3) a bilinear decoder that combines the representations of target nodes to reconstruct links representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Understanding local graph patterns</head><p>In the previous works, some handcrafted heuristics in a local graph around the target link (i.e., local graph patterns) are designed for link prediction on graphs <ref type="bibr" target="#b12">[13]</ref>. IGMC first adopts the labeling trick in GNN-based matrix completion that automatically learns suitable graph patterns from the local graph. These local graph patterns can be easily generalized to new local graphs or unseen links. To develop a better understanding of local graph patterns in matrix completion, we do a quantitative data exploration on five real-world datasets, the density of which ranges from less than 0.0001 to 0.063. In particular, we examine the Pearson's correlation coefficient (PCC) between the true ratings and four heuristic scores <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b27">27]</ref>: average user rating (AUR), average item rating (AIR), most common rating between source nodes and target nodes (MCR) and a simple collaborative signal (SCF) in five datasets. Specially, we find a user node that has the most common neighbors with the source node as guider in SCF. The link prediction in SCF is based on the rating that guider rates the target item node. From <ref type="table" target="#tab_0">Table 2</ref>, we can extract multiple findings,</p><p>? The PCCs between the true ratings and four heuristic scores in five datasets are all positive, which indicates that the true ratings are correlated with these four heuristic scores in each dataset. Furthermore, it suggests that local graph patterns are effective to predict the missing ratings in matrix completion. ? The PCCs between the true ratings and four heuristic scores are all smaller than 6.0, which indicates that a single local graph pattern is not enough to predict the missing ratings. It suggests that the model needs to learn more complex local graph patterns from rating matrix or specific node representations for personalized recommendation to obtain better performance. ? Among the four heuristic scores, AUR and AIR are simple statistics that only depend on one type of user-item interaction (i.e., the interactions with the target user or the interactions with the target item), while MCR is a statistic depending on these two types of interactions. We find that the performance of MCR is more stable across different datasets than that of AUR (or AIR) . It suggests that MCR is effective in both sparse and dense rating matrices. Moreover, stable local graph patterns like MCR are effective across different datasets, which makes inductive matrix completion possible. Furthermore, SCF considers all the interactions with the target nodes and their neighbors within 1-hop, which outperforms MCR on all datasets. It suggests that local graph patterns which consider more user-item interactions may be more powerful.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Input features</head><p>Motivated by our earlier findings, we now introduce two input features, identical feature and role-aware feature for the GAE-based model to learn local graph patterns. The identical feature is an identical index, which helps GNN model aggregate one-hop user-item interactions (user-to-item interaction or item-to-user interaction) by message passing function. It aims to represent some simple local heuristics scores such as AIR or AUR, which have been demonstrated to be effective to predict potential ratings in the above quantitative analysis. To model two-hop user-item interactions, we design the second structure feature, the role-aware feature, using two extra indexes to distinguish user and item in the input space of the model. It helps the model distinguish user nodes with item nodes, and therefore distinguish the interactions from user to item with the interactions from item to user. Furthermore, after the user-item interactions around the target link are aggregated by the message passing function, the model can distinguish the user-item interactions from the 1-hop neighbors with the user-item interactions from 2-hop neighbors. By distinguishing these two types of user-item interactions, the model is capable of learning more complicated and powerful local graph patterns such as the aforementioned MCR or SCF. Furthermore, the model needs more expressive patterns for personalized recommendation. Accordingly, we incorporate the onehot index into the input space of IMC-GAE, which is same as previous GAE-based models that learns specific node representations for personalized recommendation. Altogether, we adopt two informative features and one-hot index feature in IMC-GAE, which aims to help GAE learn structure link representation and latent link representation, respectively. The structure link representation represents local graph patterns around the target link, and the latent link representation represents the user-specific preference to the item.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">GNN encoder on heterogeneous graph</head><p>In our paper, matrix completion is formulated as the link prediction problem on a heterogeneous graph. In the heterogeneous graph, rating edges of the same type are collected into a rating subgraph (e.g., if the graph consists of four types of ratings, there are four rating subgraphs). Correspondingly, each rating subgraph contains a copy of all the nodes. Then IMC-GAE applies a node-level GNN encoder to these subgraphs that learn a distinct representation for each node in each subgraph. There are three components in our GNN encoder: 1) embedding layer, 2) message passing layer, and 3) accumulation layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Embedding layer.</head><p>In each rating subgraph, the representation of each node consists of three different embeddings (identical node embedding , role-aware embedding , and rating embedding ). We assume that there are rating types in the rating matrix so that we have rating subgraphs in our model. With three different embeddings in each rating subgraphs, each node has 3 ? embeddings in IMC-GAE. In order to reduce the number of parameters while allowing for more robust pattern learning, we use the same identical node embedding and role-aware embedding in each rating subgraph. Therefore, there are + 2 embeddings to represent a node in rating subgraphs. Moreover, we concentrate (denoted by ( )) these three embeddings (denoted by , , ) in embedding layer, which is the output of the embedding layer,</p><formula xml:id="formula_1">0 [ ] = ( [ ], [ ], [ ]),<label>(1)</label></formula><p>where 0 [ ] denotes node 's embedding vector in -th rating subgraph. The node embedding vectors are the input of message passing layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Message passing layer.</head><p>In IMC-GAE, we adopt a traditional GCN message passing layer to do local graph convolution, which has the following form:</p><formula xml:id="formula_2">+1 [ ] = ?? ?N ( ) 1 ?? |N ( )| ? |N ( )| [ ]<label>(2)</label></formula><p>where +1 [ ] denotes node 's feature vector at layer +1 in the -th rating subgraph. In addition, we chose symmetric normalization as the degree normalization factor in our message passing layer, where the |N ( )| represents the number of neighbors of node in the -th rating subgraph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.3">Accumulation layer.</head><p>In each -th rating subgraph, we stack message passage layer with ReLU activations <ref type="bibr" target="#b0">[1]</ref> between two layers. Following <ref type="bibr" target="#b7">[8]</ref>, node 's feature vectors from different layers are weighted sum as its final representation ? [ ] in the -th rating subgraph,</p><formula xml:id="formula_3">? [ ] = ?? 0? ? 1 + 1 1 [ ]<label>(3)</label></formula><p>Then we accumulate all node 's final representation ? [ ] from all rating subgraphs into a single vector representation by sum operator,</p><formula xml:id="formula_4">?[ ] = ?? ? ? [ ]<label>(4)</label></formula><p>To obtain the final representation of user or item node, we transform the intermediate output ?[ ] by a linear operator,</p><formula xml:id="formula_5">[ ] = ?( ?[ ])<label>(5)</label></formula><p>The parameter matrix of user nodes is the same as that of item nodes, which because the model is trained without side information of the nodes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Bilinear decoder</head><p>In IMC-GAE, following <ref type="bibr" target="#b1">[2]</ref>, we use a bilinear decoder to reconstruct links in the user-item graph and treat each rating level as a separate class. Given the final representation [ ] of user and [ ] of item , we use billinear operator to produce the final link representation [ , ] in the -th rating subgraph,</p><formula xml:id="formula_6">[ , ] = [ ] [ ],<label>(6)</label></formula><p>where is a learnable parameter matrix. Thus, we can estimate the final rating score as,</p><formula xml:id="formula_7">[ , ] = ?? ? (e(i, j)),<label>(7)</label></formula><p>where e(i, j) is the vector that concentrate the final link representations of user and item on all rating subgraph, and the is the softmax probability on -th dimension of e(i, j) vector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Layer-wise node dropout</head><p>The layer-wise node dropout is inspired by node dropout from <ref type="bibr" target="#b1">[2]</ref>, aiming to help model grasp patterns in local graphs which can be better generalized to unobserved ratings. In previous works, GAE-based models always adopt a node dropout scheme which randomly drops out all outgoing messages of a particular node with a probability . However, our method adopts different node dropout probabilities in different layers, which we call it layer-wise node dropout. Specially, layer-wise node dropout is = 0 ? , where is the node dropout probability in the -th layser, 0 is the initial node dropout probability, and is the hyperparameter.</p><p>In our paper, layer-wise node dropout facilitates node representation learning due to the following two reasons. The first reason is the same as <ref type="bibr" target="#b1">[2]</ref>, which we adopt to overcome the over-smoothing problem in GNN representation and improve the generalization ability of our model. The second reason is to help the model learn local graph patterns which consider more user-item interactions in a 1-hop subgraph around the target link. As shown in <ref type="figure" target="#fig_1">Figure 2</ref>, the target nodes are node 0 and node 1. In the previous GAE-based models, the representations of node 0 and node 1 in the 2-th layer aggregate too much node information beyond the 1-hop subgraph around them (e.g., node 4, node 5, node 8 and node 9 in the example), which prevents the model learning graph patterns from the user-item interactions around target nodes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Model training</head><p>3.6.1 Loss function. We minimize the cross entropy loss (denoted by ) between the predictions and the ground truth ratings,</p><formula xml:id="formula_8">L = 1 |( , )|? , = 1| ?? ( , ):? , =1 ( [ , ],?[ , ]),<label>(8)</label></formula><p>where we use [ , ] and?[ , ] to denote the true rating and the predicted rating of ( , ), respectively, and the 0/1 matrix ? serves as a mask for unobserved ratings in rating matrix .</p><p>3.6.2 Node representation regularization. It is inspired by adjacent rating regularization in <ref type="bibr" target="#b28">[28]</ref>. Since each two rating types are comparable in matrix completion (e.g., ratings 5 is bigger than ratings 4 and ratings 4 is bigger than 3), we need to consider the magnitude of ratings. Accordingly, we propose node representation regularization to encourages the representation of each node in rating subgraph that adjacent to each other to have similar representations. Specially, we assume that the representation of the -th node in the -th rating subgraph is ? [ ], where 0 ? ? . Then, the NRR regularizer is,</p><formula xml:id="formula_9">L = ? ?? 0? &lt; ?? 0? ? (? [ ], ? +1 [ ]),<label>(9)</label></formula><p>where is cosine similarity between two vectors, and is the total number of users and items in the matrix. Finally, we combine these two loss functions to the final loss function,</p><formula xml:id="formula_10">L = L + L ,<label>(10)</label></formula><p>where is a hyperparameter that trade-off two losses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.7">Inductive learning</head><p>In IMC-GAE, the inductive link representation for unseen nodes has two parts, inductive structure representation which is learned from the identical feature and the role-aware feature, and inductive latent representation which is learned from one-hot index of the node. For the inductive structure representation, we just leverage message passing, propagating learned structure representation from neighbors to target nodes. For the inductive latent representation, we also first accumulates the latent presentation of the neighbors of the target nodes. However, there may exist some unseen nodes during training in their neighbors, which lacks the latent representation. In our method, we use the average latent representation of the other nodes to represent the unseen nodes in each rating subgraph,</p><formula xml:id="formula_11">[ ] = ?? ?I 1 |I| [ ],<label>(11)</label></formula><p>where [ ] is the initial latent representation of the -th node in -th rating subgraph (in equation 1) and I is a set of nodes which we have seen during training. It is a simple but effective method, which is demonstrated in the following experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">DISCUSSING LOCAL GRAPH PATTERNS</head><p>To shed more light on local graph patterns learning in GNN-based models, we provide a comparison with GAE-based matrix completion, IGMC, and IMC-GAE through a typical example in <ref type="figure" target="#fig_2">Figure 3</ref>.</p><p>Here, we assume the ratings in our example are within {1, -1} (like, denoted by bold black line, and dislike, denoted by bold coffee line). The solid lines are observed ratings for training and dash lines are test ratings. In a train case, user 1 and 2 all like item 3, while they all dislike item 4. It indicates that user 1 may have a similar taste with user 2, which is a common local graph pattern in matrix completion. Furthermore, since user 2 dislikes item 5, we inference that user 1 may dislike item 5 based on the "similar taste" pattern. When trained with the existing rating between user 2 and item 4, IGMC first extracts the 1-hop local graph around user 2 and item 4, and relabels user 1, user 2, item 3, item 4 and item 5 as index 2, 0, 3, 1 and 3, respectively. Finally, the model applies a GNN to the local graph, where the new node labels are the input features of the model. Without introducing the user-item interactions beyond the 1-hop local graph around target nodes, the "similar taste" pattern is easily learned by the model. However, previous GAE-based models apply the GNN to the entire graph for learning graph patterns. Accordingly, when trained with the existing rating between user 2 and item 4, the representations of user 2 and item 4 are aggregated with many node embeddings beyond 1-hop local graph around user 2 and item 4, which makes the model hardly focus on the interactions in this local graph, and fails to learn "similar taste" pattern. To solve this problem, IMC-GAE designs the layer-wise node dropout scheme for the GAE-based model to avoid aggregating too many embeddings of the nodes beyond the 1-hop local graph into the target nodes representation. Although the target node representations are still aggregated a few node representations beyond the local graph, the model is capable of learning the "similar taste" pattern between user 1 and 2. Furthermore, if given a new graph in <ref type="figure" target="#fig_2">Figure 3</ref> which has the same graph structure as the original graph, previous GAE-based models need to be retrained to inference the missing rating between user 8 and item 9. However, with labeling trick in IGMC and inductive structure representation in IMC-GAE, the models learn the "similar taste" pattern into structural link representation, which can be generalized to the new graph.</p><p>Despite the effectiveness of local graph patterns learning in IGMC, the labeling trick introduces extra computational complexity. The reason is that for every rating ( , ) to predict, IGMC needs to relabel the graph according to ( , ). The same node will be labeled differently depending on the target link and will be given a different node representation by the GNN when it appears in different links' labeled graphs. This is different from previous GAEbased models and IMC-GAE, where we do not relabel the graph and each node only has a single embedding vector. For a graph with nodes and ratings to predict, the GAE-based model needs to apply the GNN O ( ) times to compute an embedding for each node, while IGMC needs to apply the GNN O ( ) times for all ratings. We compare local graph patterns learning in IMC-GAE with that in GAE and IGMC in two cases. In the first case, the model is to infer the link between node 1 and node 5 in original graph. In the second case, the model is to infer the link between node 6 and node 0 in a new graph.</p><p>When ? , IGMC has worse time complexity than GAE-based models, which is not suitable for real recommendation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTS</head><p>We perform experiments on five datasets to evaluate our proposed method. We aim to answer the following research questions:</p><p>? RQ1: How does IMC-GAE perform compared with state-of-theart matrix completion methods when facing both sparse and dense rating matrices? ? RQ2: How does the different hyper-parameter settings (e.g., depth of layer, weighted layer combination, and node representation regularization (NRR)) affect IMC-GAE? ? RQ3: How does the local graph patterns learning in IMC-GAE benefit from two informative features and layer-wise node dropout scheme respectively? ? RQ4: How does the IMC-GAE perform on few-shot or even unseen users (or items) as compared with GAE-based models and IGMC?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Datasets description</head><p>To evaluate the effectiveness of IMC-GAE, we conduct experiments on five common matrix completion datasets, Flixster <ref type="bibr" target="#b9">[10]</ref>, Douban datasets <ref type="bibr" target="#b13">[14]</ref>, YahooMusic <ref type="bibr" target="#b3">[4]</ref>, MovieLens-100K <ref type="bibr" target="#b14">[15]</ref> and MovieLens-1M <ref type="bibr" target="#b14">[15]</ref>, which are publicly accessible and vary in terms of domain, size, and sparsity. Moreover, Flixster, Douban, and Ya-hooMusic are preprocessed subsets of the original datasets provided by <ref type="bibr" target="#b18">[19]</ref>. These datasets contain sub rating matrix of only 3000 users and 3000 items, which we consider as sparse rating matrices in real recommendation. The MovieLens-100K and MovieLens-1M are widely used datasets for evaluating many recommender tasks, which we consider as dense rating matrices in real recommendation. For ML-100k, we train and evaluate on canonical u1.base/u1.test train/test split. For ML-1M, we randomly split into 90% and 10% train/test sets. For the Flixster, Douban, and YahooMusic, we use the splits provided by <ref type="bibr" target="#b18">[19]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Experimental Settings</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Baselines.</head><p>To demonstrate the effectiveness, we compare our proposed IMC-GAE with the following methods:</p><p>? Traditional methods. matrix completion (MC) <ref type="bibr" target="#b2">[3]</ref>, inductive matrix completion (IMC) <ref type="bibr" target="#b8">[9]</ref>, geometric matrix completion (GMC)   <ref type="bibr" target="#b10">[11]</ref>, PMF <ref type="bibr" target="#b15">[16]</ref>, I-RBM <ref type="bibr" target="#b21">[21]</ref>, NNMF <ref type="bibr" target="#b4">[5]</ref>, I-AutoRec <ref type="bibr" target="#b22">[22]</ref> and CF-NADE <ref type="bibr" target="#b30">[30]</ref> are traditional matrix competion methods, which use the user-item ratings (or interactions) only as the target value of their objective function. ? GAE-based methods. sRGCNN <ref type="bibr" target="#b17">[18]</ref>, NMTR <ref type="bibr" target="#b5">[6]</ref>, GC-MC <ref type="bibr" target="#b1">[2]</ref> are GAE-based matrix completion methods, which use one-hot index as the initial feature of each node. ? IGMC. IGMC <ref type="bibr" target="#b28">[28]</ref> is an inductive matrix completion method, which learns local graph patterns to generalize to new local graphs for inductive learning.</p><p>? Content-based GNN methods. Content-based matrix completion methods are inductive GNN-based methods adopting side information as initial features of each node, which includes Pin-Sage <ref type="bibr" target="#b25">[25]</ref> and IGC-MC <ref type="bibr" target="#b1">[2]</ref>. PinSage is originally used to predict related pins and is adapted to predicting ratings here. IGC-MC is a content-based GC-MC method, which uses the content features instead of the one-hot encoding of node IDs as its input features. ? Other GNN methods. GRALS <ref type="bibr" target="#b19">[20]</ref> is a graph regularized matrix completion algorithm and F-EAE <ref type="bibr" target="#b6">[7]</ref> uses exchangeable matrix layers to perform inductive matrix completion without using content.</p><p>In addition, given different datasets, we compare IMC-GAE with different baseline methods under RMSE in <ref type="table" target="#tab_1">Table 3</ref>. The RMSE is a common evaluation metric in matrix completion <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b28">28]</ref>. The baseline results are taken from <ref type="bibr" target="#b28">[28]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Performance comparison (RQ1)</head><p>We start by comparing our proposed IMC-GAE with baselines on five benchmark datasets and then explore how the combination of the local graph patterns learning and specific node representations improves the performance in matrix completion. For Flixster, Douban, and Yahoomusic, we compare our proposed model with GRALS, sRGCNN, GC-MC, F-EAE, PinSage, IGC-MC and IGMC. We show the result in <ref type="table" target="#tab_1">Table 3</ref>. Our model achieves the smallest RMSEs on Douban and YahooMusic datasets, but slightly worse than IGMC on the Flixster dataset. Furthermore, as a GAE-based model, our method outperforms significantly all the GAE-based baselines (sRGCNN and GC-MC), which highlights the successful designs (two informative features, layer-wise dropout scheme) of our model.</p><p>For ML-100k, we compare IMC-GAE with MC, IMC, as well as GRALS, sRGCNN, GC-MC, F-EAE, PinSage, NMTR and IGMC. For ML-1M, besides the baselines GC-MC, F-EAE, PinSage, NMTR and IGMC, we further include PMF, I-RBM, NNMF, I-AutoRec, and CF-NADE. Our model achieves the smallest RMSEs on these datasets without using any content, significantly outperforming all the compared baselines, regardless of whether they are GAE-based models.</p><p>Altogether, our model outperforms all GAE-based models on all datasets. It demonstrates that the local graph patterns learned in our model truly help model inference the missing ratings in both sparse and dense rating matrices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Study of IMC-GAE (RQ2)</head><p>As the GNN encoder plays a pivotal role in IMC-GAE, we investigate its impact on the performance. We start by exploring the influence of   <ref type="table" target="#tab_4">Table 5</ref> summarizes the experimental results, wherein IMC-GAE-indicates the model with embedding propagation layers, and similar notations for others. By analyzing <ref type="table" target="#tab_4">Table 5</ref>, we have the following observations:</p><p>? Increasing the depth of IMC-GAE substantially enhances the performance of the model. Clearly, IMC-GAE-2 achieves consistent improvement over IMC-GAE-1 across all the board, which considers the 1-hop neighbors only. We attribute the improvement to the effective modeling of local graph structure: structure features and layer-wise node dropout help model grasp effective patterns from local graph around target nodes. ? When further stacking propagation layer on the top of IMC-GAE-2, we find that IMC-GAE-3 leads to performance degradation on ML-100k, but performance improvement on Douban and Ya-hooMusic. This might be caused by the deeper layer with a higher node dropout probability might introduce noise in latent link representation. More specifically, the deeper layers (e.g., the third layer in IMC-GAE-3) lose the original graph connectivity, which makes the model fail to learn the latent link representation from the neighbors. Moreover, the marginal improvements on the other two datasets verify that the local graph patterns beyond 1-hop neighbors still improve the performance of the model in the sparse rating matrix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.2">Effect of weighted layer combination and NRR.</head><p>Different from prior works <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b28">28]</ref>, we adopt the weighted sum operator for layer combination instead of sum or concentration operator and NRR to encourages node representation in rating subgraph that adjacent to each other to have similar parameter matrice. <ref type="table" target="#tab_5">Table 6</ref> shows the result of the ablation experiments. From the ablation experiments, we have the following observations. First, the weighted sum combination shows a performance improvement over the sum or  concentration operator in both sparse and dense rating matrices. This might be because that sum or concentration operator does not assign lower importance to the node representations in deeper layers, which introduces more noise into the representation of the nodes. Second, we can see that disabling NRR results in the performance drop on all three datasets. It demonstrates that NRR is an effective way to regularize the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Study of the local graph patterns learning (RQ3)</head><p>5.5.1 Performance Comparison. In this section, we attempt to understand how the identical feature, the role-aware feature, and the layer-wise node dropout scheme affect local graph patterns learning in IMC-GAE, and how local graph patterns learning affects the performance of IMC-GAE in both sparse and dense rating matrices. Towards this end, we compare the performance of original IMC-GAE with IMC-GAE trained with only identical feature, IMC-GAE trained with only role-aware feature, IMC-GAE with normal node dropout on the three datasets. From the result shown in <ref type="table" target="#tab_6">Table 7</ref>, we conclude the following findings:</p><p>? With only role-aware feature or only identical feature for training, IMC-GAE obtains a competitive performance with IMC-GAE. It demonstrates that local graph patterns learning in IMC-GAE is effective in both sparse and dense rating matrices. ? The IMC-GAE outperforms the other three baselines in all datasets, which demonstrates that each design in IMC-GAE for local graph patterns learning (i.e., role-aware feature, identical feature, and layer-wise node dropout scheme) is essential, which helps GAE learn a series of effective local graph patterns.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.2">Inference Time Comparison.</head><p>We compare the inference time of IMC-GAE with IGMC and GC-MC (a typical GAE-based model) on three datasets. Specially, we infer the 20% samples in each dataset and conduct the experiment on GN7 on Tencent Cloud, which is equipped with 4*Tesla T4. We repeat this experiment five times and report the average inference time of each model in <ref type="table" target="#tab_7">Table 8</ref>. The results show that the inference time of IMC-GAE is slightly longer than that of GAE but significantly shorter than that of IGMC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">Study of IMC-GAE on sparse data (RQ4)</head><p>To investigate the model performance on few-shot or even unseen users (or items), we test the model on dataset under different sparsity levels of the rating matrix <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b28">28]</ref>. Here we construct several sparse datasets by using 100%, 20%, 10%, 5%, 1% and 0.1% training ratings in Movielens-1M, and then compare the test RMSEs of our method with GC-MC and IGMC, a typical GAE-based model and an inductive GNN model. As shown in <ref type="figure" target="#fig_3">Figure 4</ref>, we have two observations,</p><p>? As the dataset becomes sparser, the performance of all the models suffer from a drop, but the drop rate of our model is much smaller compared with GC-MC. ? The performance of our model in the datasets with 100%, 20%, and 10% training ratings is better than IGMC, but worse in other datasets. From the observations, we find that the way we adopt GAE to learn local graph patterns that truly improves its inductive learning ability. However, the performance of IMC-GAE on three sparer datasets is worse than that of IGMC. It suggests that local graph patterns learned in IMC-GAE is not as good as those learned in IGMC which can be generalized to sparser datasets containing more few-shot or unseen users (or items).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>In this paper, we propose Inductive Matrix Completion using Graph Autoencoder (IMC-GAE), which uses GAE to learn both graph patterns for inductive matrix completion and specific node representations for personalized recommendation. Extensive experiments on real-world datasets demonstrate the rationality and effectiveness of the way IMC-GAE learns local graph patterns by GAE. This work represents an initial attempt to exploit local structural knowledge in GAE-based matrix completion, which is more suitable to be applied to real recommender systems.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Model Overview.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Layer-wise Node Dropout. In this subgraph extracted in ML-100k, red nodes indicates target nodes; blue nodes indicates the 1-hop neighbors of target nodes; white nodes indicates the 2-hop neighbors of target nodes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: We compare local graph patterns learning in IMC-GAE with that in GAE and IGMC in two cases. In the first case, the model is to infer the link between node 1 and node 5 in original graph. In the second case, the model is to infer the link between node 6 and node 0 in a new graph.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>ML-1M results under different sparsity ratios.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 2 :</head><label>2</label><figDesc>Quantitative Analysis on multiple datasets.</figDesc><table><row><cell>Dataset</cell><cell cols="2">density AUR</cell><cell>AIR</cell><cell>MCR</cell><cell>SCF</cell></row><row><cell cols="6">YahooMusic &lt; 0.0001 0.1915 0.0745 0.3585 0.4713</cell></row><row><cell>Flixster</cell><cell>0.0029</cell><cell cols="4">0.4705 0.1289 0.4362 0.5008</cell></row><row><cell>Douban</cell><cell>0.0152</cell><cell cols="4">0.3672 0.5033 0.4537 0.4735</cell></row><row><cell>ML-1M</cell><cell>0.0447</cell><cell cols="4">0.3771 0.4812 0.4151 0.5659</cell></row><row><cell>ML-100K</cell><cell>0.0630</cell><cell cols="4">0.3826 0.4177 0.3815 0.5006</cell></row></table><note>rely heavily on the rich content of each node, which is not easily accessible in most real recommender systems. In comparison, our model is inductive and does not rely on any node content.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell cols="4">: RMSE of different algorithms on Flixster, Douban</cell></row><row><cell>and YahooMusic.</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Model</cell><cell cols="3">Flixster Douban YahooMusic</cell></row><row><cell>IGC-MC</cell><cell>0.999</cell><cell>0.990</cell><cell>21.3</cell></row><row><cell>F-EAE</cell><cell>0.908</cell><cell>0.738</cell><cell>20.0</cell></row><row><cell>PinSage</cell><cell>0.954</cell><cell>0.739</cell><cell>22.9</cell></row><row><cell>IGMC</cell><cell>0.872</cell><cell>0.721</cell><cell>19.1</cell></row><row><cell>GRALS</cell><cell>1.245</cell><cell>0.883</cell><cell>38.0</cell></row><row><cell>sRGCNN</cell><cell>0.926</cell><cell>0.801</cell><cell>22.4</cell></row><row><cell>GC-MC</cell><cell>0.917</cell><cell>0.734</cell><cell>20.5</cell></row><row><cell>IMC-GAE (ours)</cell><cell>0.884</cell><cell>0.721</cell><cell>18.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table><row><cell cols="4">RMSE test results on MovieLens-100K (left) and</cell></row><row><cell cols="2">MovieLens-1M (right).</cell><cell></cell><cell></cell></row><row><cell>Model</cell><cell cols="2">ML-100K Model</cell><cell>ML-1M</cell></row><row><cell>F-EAE</cell><cell>0.920</cell><cell>F-EAE</cell><cell>0.860</cell></row><row><cell>PinSage</cell><cell>0.951</cell><cell>PinSage</cell><cell>0.906</cell></row><row><cell>IGMC</cell><cell>0.905</cell><cell>IGMC</cell><cell>0.857</cell></row><row><cell>MC</cell><cell>0.973</cell><cell>PMF</cell><cell>0.883</cell></row><row><cell>IMC</cell><cell>1.653</cell><cell>I-RBM</cell><cell>0.854</cell></row><row><cell>GMC</cell><cell>0.996</cell><cell>NNMF</cell><cell>0.843</cell></row><row><cell>GRALS</cell><cell>0.945</cell><cell>I-AutoRec</cell><cell>0.831</cell></row><row><cell>sRGCNN</cell><cell>0.929</cell><cell>CF-NADE</cell><cell>0.829</cell></row><row><cell>GC-MC</cell><cell>0.905</cell><cell>GC-MC</cell><cell>0.832</cell></row><row><cell>NMTR</cell><cell>0.911</cell><cell>NMTR</cell><cell>0.834</cell></row><row><cell>IMC-GAE (ours)</cell><cell>0.897</cell><cell>IMC-GAE(ours)</cell><cell>0.829</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>5.2.2 HyperparameterSettings. We implement our model based on DGL<ref type="bibr" target="#b23">[23]</ref> and use the Adam optimizer. We apply a grid search for hyperparameters, the number of layers is searched in {1, 2, ..., 5}, the in equation 10 is searched in {4 ?5 , 4 ?4 , 4 ?3 , 4 ?2 }, the embedding size of each vector in embedding layer is chosen from {90, 120, ..., 1800} and the embedding size of each vector in bilinear decoder is searched in {30, 40, ..., 80}. Besides, the initial node dropout probability is tuned in {0.1, 0.2, 0.3} and the decay ratio is tuned in {0.05, 0.1, 0.2}. All implementation codes can be found at https://github.com/swtheing/IMC-GAE.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Effect of layer numbers in GNN encoder</figDesc><table><row><cell></cell><cell cols="3">Douban YahooMusic ML-100k</cell></row><row><cell>IMC-GAE-1</cell><cell>0.728</cell><cell>18.803</cell><cell>0.900</cell></row><row><cell>IMC-GAE-2</cell><cell>0.725</cell><cell>18.793</cell><cell>0.897</cell></row><row><cell>IMC-GAE-3</cell><cell>0.722</cell><cell>18.702</cell><cell>0.897</cell></row><row><cell>IMC-GAE-4</cell><cell>0.723</cell><cell>20.343</cell><cell>0.901</cell></row><row><cell>IMC-GAE-5</cell><cell>0.721</cell><cell>18.785</cell><cell>0.897</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Effect of weighted layer combination and NRR Effect of Layer Numbers. To investigate whether IMC-GAE can benefit from multiple layers in the GNN encoder, we vary the model depth. In particular, we search the layer numbers in the range of {1, 2, 3, 4, 5}.</figDesc><table><row><cell></cell><cell cols="3">Douban YahooMusic ML-100k</cell></row><row><cell>IMC-GAE(Original)</cell><cell>0.721</cell><cell>18.7</cell><cell>0.897</cell></row><row><cell>IMC-GAE(no NRR)</cell><cell>0.722</cell><cell>18.8</cell><cell>0.900</cell></row><row><cell>IMC-GAE(Sum)</cell><cell>0.727</cell><cell>19.2</cell><cell>0.905</cell></row><row><cell>IMC-GAE(Concat)</cell><cell>0.723</cell><cell>18.9</cell><cell>0.903</cell></row><row><cell cols="4">layer numbers. We then study how the weighted layer combination</cell></row><row><cell cols="2">and NRR affect the performance.</cell><cell></cell><cell></cell></row><row><cell>5.4.1</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 :</head><label>7</label><figDesc>Ablation study on three datasets, where IMC-GAE-R indicates the IMC-GAE-R trained with only role-aware feature; IMC-GAE-I indicates IMC-GAE trained with only identical feature; IMC-GAE-OD indicates IMC-GAE trained with original node dropout scheme.</figDesc><table><row><cell>Model</cell><cell cols="3">Douban ML-100K ML-1M</cell></row><row><cell>IMC-GAE</cell><cell>0.721</cell><cell>0.897</cell><cell>0.829</cell></row><row><cell>IMC-GAE-R</cell><cell>0.734</cell><cell>0.912</cell><cell>0.868</cell></row><row><cell>IMC-GAE-I</cell><cell>0.738</cell><cell>0.924</cell><cell>0.912</cell></row><row><cell>IMC-GAE-OD</cell><cell>0.727</cell><cell>0.905</cell><cell>0.834</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 8 :</head><label>8</label><figDesc></figDesc><table><row><cell cols="4">Inference time (s) of IMC-GAE, IGMC and GCMC on</cell></row><row><cell cols="4">Douban, MovieLens-100K, and MovieLens-1M.</cell></row><row><cell>Model</cell><cell cols="3">Douban ML-100K ML-1M</cell></row><row><cell>IGMC</cell><cell>9.255</cell><cell>33.041</cell><cell>122.042</cell></row><row><cell>GC-MC</cell><cell>0.011</cell><cell>0.011</cell><cell>0.025</cell></row><row><cell>IMC-GAE(ours)</cell><cell>0.060</cell><cell>0.0382</cell><cell>0.067</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agarap</forename><surname>Abien Fred</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.08375</idno>
		<title level="m">Deep learning using rectified linear units (relu)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rianne</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">N</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02263</idno>
		<title level="m">Graph convolutional matrix completion</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="32" to="40" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Exact matrix completion via convex optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Emmanuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Cand?s</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Recht</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations of Computational mathematics</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">717</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The yahoo! music dataset and kdd-cup&apos;11</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gideon</forename><surname>Dror</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Koenigstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yehuda</forename><surname>Koren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Weimer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of KDD Cup</title>
		<meeting>KDD Cup</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="3" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karolina</forename><surname>Gintare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel M</forename><surname>Dziugaite</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Roy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06443</idno>
		<title level="m">Neural network matrix factorization</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Neural multi-task recommendation from multibehavior data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangning</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuli</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE 35th International Conference on Data Engineering (ICDE)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1554" to="1557" />
		</imprint>
	</monogr>
	<note>Tat-Seng Chua, and Depeng Jin</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Kevin Leyton-Brown, and Siamak Ravanbakhsh</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Hartford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devon</forename><surname>Graham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning. PMLR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1909" to="1918" />
		</imprint>
	</monogr>
	<note>Deep models of interactions across sets</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuan</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongdong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.02126</idno>
		<title level="m">LightGCN: Simplifying and Powering Graph Convolution Network for Recommendation</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prateek</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Inderjit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dhillon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1306.0626</idno>
		<title level="m">Provable inductive matrix completion</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A matrix factorization technique with trust propagation for recommendation in social networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohsen</forename><surname>Jamali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Ester</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the fourth ACM conference on Recommender systems</title>
		<meeting>the fourth ACM conference on Recommender systems</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="135" to="142" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vassilis</forename><surname>Kalofolias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Vandergheynst</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.1717</idno>
		<title level="m">Matrix completion on graphs</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.07308</idno>
		<title level="m">Variational graph auto-encoders</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The link-prediction problem for social networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Liben</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-</forename><surname>Nowell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Kleinberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American society for information science and technology</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="page" from="1019" to="1031" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Recommender systems with social regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dengyong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irwin</forename><surname>Michael R Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>King</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the fourth ACM international conference on Web search and data mining</title>
		<meeting>the fourth ACM international conference on Web search and data mining</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="287" to="296" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Movielens unplugged: experiences with an occasionally connected recommender system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Bradley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Istvan</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Albert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Shyong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><forename type="middle">A</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Konstan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Riedl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th international conference on Intelligent user interfaces</title>
		<meeting>the 8th international conference on Intelligent user interfaces</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="263" to="266" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Probabilistic matrix factorization. Advances in neural information processing systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andriy</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Russ R Salakhutdinov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="1257" to="1264" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Probabilistic matrix factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andriy</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Russ R Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1257" to="1264" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Geometric Matrix Completion with Recurrent Multi-Graph Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Neural Information Processing Systems</title>
		<meeting>the 31st International Conference on Neural Information Processing Systems<address><addrLine>Long Beach, California, USA; Red Hook, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Curran Associates Inc</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3700" to="3710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Geometric matrix completion with recurrent multi-graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Michael M Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bresson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.06803</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikhil</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hsiang-Fu</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pradeep</forename><surname>Ravikumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Inderjit S</forename><surname>Dhillon</surname></persName>
		</author>
		<title level="m">Collaborative Filtering with Graph Information: Consistency and Scalable Methods</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">In NIPS</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
			<publisher>Citeseer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Restricted Boltzmann machines for collaborative filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andriy</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th international conference on Machine learning</title>
		<meeting>the 24th international conference on Machine learning</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="791" to="798" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Autorec: Autoencoders meet collaborative filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suvash</forename><surname>Sedhain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><forename type="middle">Krishna</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Sanner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lexing</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th international conference on World Wide Web</title>
		<meeting>the 24th international conference on World Wide Web</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="111" to="112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minjie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingfan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Da</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quan</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Gai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihao</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mufei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinjing</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Ma</surname></persName>
		</author>
		<title level="m">Deep Graph Library: Towards Efficient and Scalable Deep Learning on Graphs</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Neural graph collaborative filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuli</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 42nd international ACM SIGIR conference on Research and development in Information Retrieval</title>
		<meeting>the 42nd international ACM SIGIR conference on Research and development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="165" to="174" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Graph convolutional neural networks for web-scale recommender systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rex</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruining</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pong</forename><surname>Eksombatchai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="974" to="983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Handling Missing Data with Graph Representation Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxuan</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaobai</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daisy</forename><forename type="middle">Yi</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mykel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Kochenderfer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zijun</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanqiao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.05455</idno>
		<title level="m">Graph Symbiosis Learning</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixin</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.12058</idno>
		<title level="m">Inductive matrix completion based on graph neural networks</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Revisiting Graph Neural Networks for Link Prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinglong</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Jin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.16103</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A neural autoregressive approach to collaborative filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bangsheng</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenkui</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanning</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning. PMLR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="764" to="773" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
