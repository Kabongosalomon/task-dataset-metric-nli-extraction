<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deeper, Broader and Artier Domain Generalization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Da</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Queen Mary University of London University of Edinburgh</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongxin</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Queen Mary University of London University of Edinburgh</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Zhe</forename><surname>Song</surname></persName>
							<email>yizhe.song@qmul.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="institution">Queen Mary University of London University of Edinburgh</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
							<email>t.hospedales@ed.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="institution">Queen Mary University of London University of Edinburgh</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Deeper, Broader and Artier Domain Generalization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T15:57+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The problem of domain generalization is to learn from multiple training domains, and extract a domain-agnostic model that can then be applied to an unseen domain. Domain generalization (DG) has a clear motivation in contexts where there are target domains with distinct characteristics, yet sparse data for training. For example recognition in sketch images, which are distinctly more abstract and rarer than photos. Nevertheless, DG methods have primarily been evaluated on photo-only benchmarks focusing on alleviating the dataset bias where both problems of domain distinctiveness and data sparsity can be minimal. We argue that these benchmarks are overly straightforward, and show that simple deep learning baselines perform surprisingly well on them.</p><p>In this paper, we make two main contributions: Firstly, we build upon the favorable domain shift-robust properties of deep learning methods, and develop a low-rank parameterized CNN model for end-to-end DG learning. Secondly, we develop a DG benchmark dataset covering photo, sketch, cartoon and painting domains. This is both more practically relevant, and harder (bigger domain shift) than existing benchmarks. The results show that our method outperforms existing DG alternatives, and our dataset provides a more significant DG challenge to drive future research.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Learning models that can bridge train-test domain-shift is a topical issue in computer vision and beyond. In vision this has been motivated recently by the observation of significant bias across popular datasets <ref type="bibr" target="#b26">[27]</ref>, and the poor performance of state-of-the-art models when applied across datasets. Existing approaches can broadly be categorized into domain adaptation (DA) methods, that use (un)labeled target data to adapt source model(s) to a specific target domain <ref type="bibr" target="#b22">[23]</ref>; and domain generalization (DG) approaches, that learn a domain agnostic model from multiple sources that can be applied to any target domain <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b9">10]</ref>. While DA has been more commonly studied, DG is the more valuable  <ref type="figure">Figure 1</ref>: Contrast between prior Caltech Office and VLCS datasets versus our new PACS dataset. The domain generalization task is to recognize categories in an unseen testing domain. PACS provides more diverse domains with bigger more challenging domain-shifts between them.</p><p>yet challenging setting, as it does not require acquisition of a large target domain set for off-line analysis to drive adaptation. Such data may not even exist if the target domain is sparse. Instead it aims to produce a more human-like model, where there is a deeper semantic sharing across different domains -a dog is a dog no matter if it is depicted in the form of a photo, cartoon, painting, or indeed, a sketch. The most popular existing DA/DG benchmarks define domains as photos of objects spanning different camera types <ref type="bibr" target="#b22">[23]</ref>, or datasets collected with different composition biases <ref type="bibr" target="#b26">[27]</ref>. While these benchmarks provide a good start, we argue that they are neither well motivated nor hard enough to drive the field. Motivation: The constituent domains/datasets in existing benchmarks are based upon conventional photos, albeit with different camera types or composition bias. However there exist enough photos, that one could in principle collect enough target domain-specific data to train a good model, or enough diverse data to cover all domains and minimize bias (thus negating the need for DA). A more compelling motivation is domains where the total available images is fundamentally constrained, such as for particular styles of art <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b28">29]</ref>, and sketches <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b25">26]</ref>. Compared to photos, there may simply not be enough examples of a given art style to train a good model, even if we are willing to spend the effort. Difficulty: The camera type and bias differences between domains in existing benchmarks are already partially bridged by contemporary Deep features <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b31">32]</ref>, thus questioning the need for DA or DG methods. In this paper, we show that multidomain deep learning provides a very simple but highly effective approach to DG that outperforms existing purposedesigned methods.</p><p>To address these limitations, we provide a harder and better motivated benchmark dataset PACS, consisting of images from photo (P), art painting (A), cartoon (C), and sketch (S) domains. This benchmark carries two important advancements over prior examples: (i) it extends the previously photo-only setting in DA/DG research, and uniquely includes domains that are maximally distinct from each other, spanning a wide spectrum of visual abstraction, from photos that are the least abstract to human sketches which are the most abstract; (ii) it is more reflective of a real-world task where a target domain (such as sketch) is intrinsically sparse, and so DG from a more abundant domain (such as photos) is really necessary. As illustrated qualitatively in <ref type="figure">Fig. 1</ref>, the benchmark is harder, as the domains are visually more distinct than in prior datasets. We explore these differences quantitatively in Sec. 4.2.</p><p>There have been a variety of prior approaches to DG based on SVM <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b29">30]</ref>, subspace learning <ref type="bibr" target="#b18">[19]</ref>, metric learning <ref type="bibr" target="#b6">[7]</ref>, and autoencoders <ref type="bibr" target="#b9">[10]</ref>. Despite their differences, most of these have looked at fixed shallow features. In this paper, we address the question of how end-to-end learning of deep features impacts the DG setting. Our deep learning approach trains on multiple source domains, and extracts both domain agnostic features (e.g., convolutional kernels), and classifier (e.g., final FC layer) for transfer to a new target domain. This approach can be seen as a deep multi-class generalization of the shallow binary Undo Bias method <ref type="bibr" target="#b11">[12]</ref>, which takes the form of a dynamically parameterized deep neural network <ref type="bibr" target="#b24">[25]</ref>. However, the resulting number of parameters grows linearly with the number of source domains (of which ultimately, we expect many for DG), increasing overfitting risk. To address this we develop a low-rank parameterized neural network which reduces the number of parameters. Furthermore the low-rank approach provides an additional route to knowledge sharing besides through explicit parameterization. In particular it has the further benefit of automatically modeling how related the different domains are (e.g., perhaps sketch is similar to cartoon; and cartoon is similar to painting), and also how the degree of sharing should vary at each layer of the CNN.</p><p>To summarize our contributions: Firstly, we highlight the weaknesses of existing methods (they lose to a simple deep learning baseline) and datasets (their domain shift is small). Second, we introduce a new, better motivated, and more challenging DG benchmark. Finally, we develop a novel DG method based on low-rank parameterized CNNs that shows favorable performance compared to prior work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>Domain Generalization Despite different methodological tools (SVM, subspace learning, autoencoders, etc), existing methods approach DG based on a few different intuitions. One is to project the data to a new domain invariant representation where the differences between training domains is minimized <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b9">10]</ref>, with the intuition that such a space will also be good for an unseen testing domain. Another intuition is to predict which known domain a testing sample seems most relevant to, and use that classifier <ref type="bibr" target="#b29">[30]</ref>. Finally, there is the idea of generating a domain agnostic classifier, for example by asserting that each training domain's classifier is the sum of a domain-specific and domain-agnostic weight vector <ref type="bibr" target="#b11">[12]</ref>. The resulting domainagnostic weight vector can then be extracted and applied to held out domains. Our approach lies in this latter category. However, prior work in this area has dealt with shallow, linear models only. We show how to extend this intuition to end-to-end learning in CNNs, while limiting the resulting parameter growth, and making the sharing structure richer than an unweighted sum.</p><p>There has been more extensive work on CNN models for domain adaptation, with methods developed for encouraging CNN layers to learn transferable features <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b16">17]</ref>. However, these studies have typically not addressed our domain generalization setting. Moreover, as analysis has shown that the transferability of different layers in CNNs varies significantly <ref type="bibr" target="#b31">[32]</ref>, these studies have had carefully hand designed the CNN sharing structure to address their particular DA problems. In our benchmark, this is harder, as the gaps between our more diverse domains are unknown and likely to be more variable. However, our low-rank modeling approach provides the benefit of automatically estimating both the per-domain and per-layer sharing strength.</p><p>Domain Generalization is also related to learning to learn. Learning to learn methods aim to learn not just specific concepts or skills, but learning algorithms or problem agnostic biases that improve generalization <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b7">8]</ref>. Similarly DG is to extract common knowledge from source domains that applies to unseen target domains. Thus our method can be seen as a simple learning to learn method for the DG setting. Different from few-shot learning <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b21">22]</ref>, DG is a zero-shot problem as performance is immediately evaluated on the target domain with no further learning. Neural Network Methods Our DG method is related to parameterized neural networks <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b24">25]</ref>, in that the parameters are set based on external metadata. In our case, based on a description of the current domain, rather than an instance <ref type="bibr" target="#b0">[1]</ref>, or additional sensor <ref type="bibr" target="#b24">[25]</ref>. It is also related to lowrank neural network models, typically used to compress <ref type="bibr" target="#b12">[13]</ref> and speed up <ref type="bibr" target="#b15">[16]</ref> CNNs, and have very recently been explored for cross-category CNN knowledge transfer <ref type="bibr" target="#b30">[31]</ref>. In our case we exploit this idea both for compression -but across rather than within domains <ref type="bibr" target="#b12">[13]</ref>, as well as for crossdomain (rather than cross-category <ref type="bibr" target="#b30">[31]</ref>) knowledge sharing. Different domains can share parameters via common latent factors. <ref type="bibr" target="#b1">[2]</ref> also addresses the DG setting, but learns shared parameters based on image reconstruction, whereas ours is learned via paramaterizing each domain's CNN. As a parameterized neural network, our approach also differs from all those other low-rank methods <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b30">31]</ref>, which have a fixed parameterization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Benchmarks and Datasets</head><p>DG Benchmarks The most popular DG benchmarks are: 'Office' <ref type="bibr" target="#b22">[23]</ref> (containing Amazon/Webcam/DSLR images), later extended to include a fourth Caltech 101 domain <ref type="bibr" target="#b10">[11]</ref> (OfficeCaltech) and Pascal 2007, LabelMe, Caltech, SUN09 (VLCS) <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b11">12]</ref>. The domains within Office relate to different camera types, and the others are created by the biases of different data collection procedures <ref type="bibr" target="#b26">[27]</ref>. Despite the famous analysis of dataset bias <ref type="bibr" target="#b26">[27]</ref> that motivated the creation of the VLCS benchmark, it was later shown that the domain shift is much smaller with recent deep features <ref type="bibr" target="#b3">[4]</ref>. Thus recent DG studies have used deep features <ref type="bibr" target="#b9">[10]</ref>, to obtain better results. Nevertheless, we show that a very simple baseline of fine-tuning deep features on multiple source domains performs comparably or better than prior DG methods. This motivates our design of a CNN-based DG method, as well as our new dataset <ref type="figure">(Fig 1)</ref> which has greater domain shift than the prior benchmarks. Our dataset draws on nonphotorealistic and abstract visual domains which provide a better motivated example of the sort of relatively sparse data domain where DG would be of practical value. Non-photorealistic Image Analysis Non-photorealistic image analysis is a growing subfield of computer vision that extends the conventional photo-only setting of vision research to include other visual depictions (often more abstract) such as paintings and sketches. Typical tasks include instance-level matching between sketch-photo <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b23">24]</ref>, and art-photo domains <ref type="bibr" target="#b2">[3]</ref>, and transferring of object recognizers trained on photos to detect objects in art <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b28">29]</ref>. Most prior work focuses on two domains (such as photo and painting <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b28">29]</ref>, or photo and sketch <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b23">24]</ref>). Studies have investigated simple 'blind' transfer between domains <ref type="bibr" target="#b4">[5]</ref>, learning cross-domain projections <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b2">3]</ref>, or engineering structured models for matching <ref type="bibr" target="#b28">[29]</ref>. Thus, in contrast to our DG setting, prior non-photorealistic analyses fall into either cross-domain instance matching, or domain adaptation settings. To create our benchmark, we aggregate multiple domains including paintings, cartoons and sketches, and define a comprehensive domain-generalization benchmark covering a wide spectrum of visual abstraction based upon these. Thus in contrast to prior DG benchmarks, our domain-shifts are bigger and more challenging.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head><p>Assume we observe S domains, and the ith domain con-</p><formula xml:id="formula_0">tains N i labeled instances {(x (i) j , y (i) j )} Ni j=1 where x (i)</formula><p>j is the input data (e.g., an image) for which we assume they are of the same size among all domains (e.g., all images are cropped into the same size), and y (i) j ? {1 . . . C} is the class label. We assume the label space is consistent across domains. The objective of DG is to learn a domain agnostic model which can be applied to unseen domains in the future. In contrast to domain adaptation, we can not access the labeled or unlabeled examples from those domains to which the model is eventually applied. So the model is supposed to extract the domain agnostic knowledge within the observed domains. In the training stage, we will minimize the empirical error for all observed domains,</p><formula xml:id="formula_1">argmin ?1,?2,...,? S 1 S S i=1 1 N i Ni j=1 (? (i) j , y (i) j )<label>(1)</label></formula><p>where is the loss function that measures the error between the predicted label? and the true label y, and prediction is carried out by a function?</p><formula xml:id="formula_2">(i) j = f (x (i) j |? i ) parameter- ized by ? i .</formula><p>A straightforward approach to finding a domain agnostic model is to assume ? * = ? 1 = ? 2 = ? ? ? = ? S , i.e., there exists a universal model ? * . Doing so we literally ignore the domain difference. Alternatively, Undo-Bias <ref type="bibr" target="#b11">[12]</ref> considers linear models, and assumes that the parameter (a D-dimensional vector when</p><formula xml:id="formula_3">x ? R D ) for the ith domain is in the form ? (i) = ? (0) + ? (i) ,</formula><p>where ? (0) can be seen as a domain agnostic model that benefits all domains, and ? (i) is a domain specific bias term. Conceptually, ? (0) can also serve as the classifier for any unseen domains. <ref type="bibr" target="#b11">[12]</ref> showed that (for linear models) ? (0) is better than the universal model ? * trained by argmin</p><formula xml:id="formula_4">? * 1 S S i=1 1 Ni Ni j=1 (? T * x (i) j , y (i) j )</formula><p>in terms of testing performance on unseen domains. However we show that for deep networks, a universal model f (x|? * ) is a strong baseline that requires improved methodology to beat.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Parameterized Neural Network for DG</head><p>To extend the idea of Undo-Bias <ref type="bibr" target="#b11">[12]</ref> into the neural network context, it is more convenient to think ? (i) is generated from a function g(z (i) |?) parameterized by ?. Here z (i) is a binary vector encoding of the ith domain with two properties: (i) it is of length S + 1 where S is the number of observed domains; (ii) it always has only two units activated (being one): the ith unit active for the ith domain and the last unit active for all domains. Formally, the objective function becomes,</p><formula xml:id="formula_5">argmin ? 1 S S i=1 1 N i Ni j=1 (? (i) j , y (i) j )<label>(2)</label></formula><p>where?</p><formula xml:id="formula_6">(i) j = f (x (i) j |? i ) = f (x (i) j |g(z (i) |?))</formula><p>. To reproduce Undo-Bias <ref type="bibr" target="#b11">[12]</ref>, we can stack all parameters in a column-wise fashion to form ?, i.e., ? = [? <ref type="bibr" target="#b0">(1)</ref> , ? (2) , . . . , ? (S) , ? (0) ], and choose the g(?) function to be linear mapping: g(z (i) |?) = ?z (i) . From linear to multi-linear The method as described so far generates the model parameter in the form of vector thus it is only suitable for single-out setting (univariate regression or binary classification). To generate higher order parameters, we use a multi-linear model, where ? is (3rd order or higher) tensor. E.g., to generate a weighting matrix for a fully-connected layer in neural network, we can use</p><formula xml:id="formula_7">W (i) FC = g(z (i) |W) = W ? 3 z (i)<label>(3)</label></formula><p>Here ? 3 is the inner product between tensor and vector along tensor's 3rd axis. For example if W is the weight matrix of size H ? C (i.e., the number of input neurons is H and the number of output neurons is C) then W is a H ? C ? (S + 1) tensor. If we need to generate the parameter for a convolutional layer of size D 1 ?D 2 ?F 1 ?F 2 (Height?Width?Depth? Filter Number), then we use:</p><formula xml:id="formula_8">W (i) CONV = g(z (i) |W) = W ? 5 z (i)<label>(4)</label></formula><p>where W is a 5th order tensor of size</p><formula xml:id="formula_9">D 1 ? D 2 ? F 1 ? F 2 ? (S + 1).</formula><p>Domain generalization Using one such parameter generating function per layer, we can dynamically generate the weights at every layer of a CNN based on the encoded vector of every domain. In this approach, knowledge sharing is realized through the last (bias) bit in the encoding of z. I.e., every weight tensor for a given domain is the sum of a domain specific tensor and a (shared) domain agnostic tensor. For generalization to an unseen domain, we apply the one-hot, bias-only, vector z * = [0, 0, . . . , 0, 1] to synthesize a domain agnostic CNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Low rank parameterized CNNs</head><p>The method as described so far has two limitations: (i) the required parameters to learn now grow linearly in the number of domains (which we eventually hope to be large to achieve good DG), and (ii) the sharing structure is very prescribed: every parameter is an equally weighted sum of its domain agnostic and domain-specific bias partners.</p><p>To alleviate these two issues, we place a structural constraint on W. Motivated by the well-known Tucker decomposition <ref type="bibr" target="#b27">[28]</ref>, we assume that the M -order tensor W is synthesized as:</p><formula xml:id="formula_10">W = G ? 1 U 1 ? ? ? ? M U M<label>(5)</label></formula><p>where G is a K 1 ? . . . K M sized low-rank core tensor, and U m are K m ? D m matrices (note that D M = S + 1). By controlling the ranks K 1 . . . K M we can effectively reduce the number of parameters to learn. By learn-</p><formula xml:id="formula_11">ing {G, U 1 . . . U M } instead of W, the number of param- eters is reduced from (D 1 ? ? ? ? ? D M ?1 ? (S + 1)) to (K 1 ? . . . K M ) + M ?1 m=1 D m ? K m + K M ? (S + 1)</formula><p>. Besides, U M produces a K M -dimensional dense vector that guides how to linearly combine the shared factors, which is much more informative than the original case of equally weighted sum.</p><p>Given a tensor W the Tucker problem can be solved via high-order singular value decomposition (HO-SVD) <ref type="bibr" target="#b14">[15]</ref>.</p><formula xml:id="formula_12">G = W ? 1 U T 1 ? ? ? ? M U T M<label>(6)</label></formula><p>where U n is the U matrix from the SVD of the the mode-n flattening of W. However, note that aside from (optionally) performing this once for initialization, we do not perform this costly HO-SVD operation during learning.</p><p>Inference and Learning To make predictions for a particular domain, we synthesize a concrete CNN by multiplying out the parameters {G, U 1 , . . . , U M } after that doing an inner product with the corresponding domain's z. This CNN can then be used to classify an input instance x.</p><p>Since our method does not introduce any non-differentiable functions, we can use standard back-propagation to learn {G, U 1 , . . . , U M } for every layer. For our model there are hyperparameters -Tucker rank [K 1 . . . K M ] -that can potentially be set at each layer. We sidestep the need to set all of these, by using the strategy of decomposing the stack of (ImageNet pre-trained) single domain models plus one agnostic domain model through Tucker decomposition, and then applying a reconstruction error threshold of = 10% for the HO-SVD in Eq 6. This effectively determines all rank values via one 'sharing strength' hyperparameter .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">New Domain Generalization Dataset: PACS</head><p>Our PACS DG dataset is created by intersecting the classes found in Caltech256 (Photo), Sketchy (Photo, Sketch) <ref type="bibr" target="#b23">[24]</ref>, TU-Berlin (Sketch) <ref type="bibr" target="#b5">[6]</ref> and Google Images (Art painting, Cartoon, Photo). Our dataset and code, together with latest results using alternative state-of-the-art base networks, can be found at: http://sketchx. eecs.qmul.ac.uk/. PACS: Our new benchmark includes 4 domains (Photo, Sketch, Cartoon, Painting), and 7 common categories 'dog', 'elephant', 'giraffe'. 'guitar', 'horse', 'house', 'person'. The total number of images is 9991.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Characterizing Benchmarks' Domain Shifts</head><p>We first perform a preliminary analysis to contrast the domain shift within our PACS dataset to that of prior popular datasets such as VLCS. We make this contrast from both a feature space and a classifier performance perspective. Feature Space Analysis Given the DG setting of training on source domains and applying to held out test domain(s), we measure the shift between source and target domains based on the Kullback-Leibler divergence as:</p><formula xml:id="formula_13">D shif t (D s , D t ) = 1 m?n n i m j ? i KLD(D s i ||D t j )</formula><p>, where n and m are the number of source and target domains, and ? i weights the i th source domain, to account for data imbalance. To encode each domain as a probability, we calculate the mean DECAF 7 representation over instances and then apply softmax normalization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Classifier Performance Analysis</head><p>We also compare the datasets by the margin between multiclass classification accuracy of within-domain learning, and a simple crossdomain baseline of training a CNN on all the source domains before testing on the held out target domain (as we shall see later, this baseline is very competitive). Assuming within-domain learning performance is an upper bound, then this difference indicates the space which a DG method has to make a contribution, and hence roughly reflects size of the domain-shift/difficulty of the DG task.</p><p>Results <ref type="figure" target="#fig_1">Fig. 2(a)</ref> shows the average domain-shift in terms of KLD across all choices of held out domain in our new PACS benchmark, compared with the VLCS benchmark <ref type="bibr" target="#b26">[27]</ref>. Clearly the domain shift is significantly higher in our new benchmark, as is visually intuitive from the illustrative examples in <ref type="figure">Fig. 1</ref>. To provide a qualitative summarization, we also show the distribution of features in our PACS compared to VLCS in <ref type="figure" target="#fig_1">Fig. 2(b,c)</ref> as visualized by a 2 dimensional t-SNE <ref type="bibr" target="#b17">[18]</ref> plot, where the features are categorized and colored by their associated domain. From this result, we can see that the VLCS data are generally hard to sepa-rate by domain, while our PACS data are much more separated by domain. This illustrates the greater degree of shift between the domains in PACS over VLCS.</p><p>We next explore the domain shifts from a model-, rather than feature-centric perspective. <ref type="figure" target="#fig_3">Fig. 3a</ref> summarizes the within-domain and across-domain performance for each domain within PACS and VLCS benchmarks. The average drop in performance due to cross-domain transfer is 20.2% for PACS versus 10.0% for VLCS. This shows that the scope for contribution of DG/DA in our PACS is double that of VLCS, and illustrates the greater relevance and challenge of the PACS benchmark.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Domain Generalization Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Datasets and Settings</head><p>We evaluate our proposed method on two datasets: VLCS, and our proposed PACS dataset. VLCS <ref type="bibr" target="#b26">[27]</ref> aggregates photos from Caltech, LabelMe, Pascal VOC 2007 and SUN09. It provides a 5-way multiclass benchmark on the five common classes: 'bird','car','chair','dog' and 'person'. Our PACS (described in Sec. 4.1) with 7 classes from Photo, Sketch, Cartoon, Painting domains. All results are evaluated by multi-class accuracy, following <ref type="bibr" target="#b9">[10]</ref>. We explore features including Classic SIFT features (for direct comparison with earlier work), DECAF pre-extracted deep features following <ref type="bibr" target="#b9">[10]</ref>, and E2E end-to-end CNN learning.</p><p>Settings: For our method in E2E configuration, we use the ImageNet pre-trained AlexNet CNN, fine-tuned with multi-domain learning on the training domains. On VLCS, we follow the train-test split strategy from <ref type="bibr" target="#b9">[10]</ref>. Our initial learning rate is 5e-5 and batch size is 64 for each training domain. We use the best performed model on validation to do the test after tuning the model for 25k iterations. On PACS, we split the images from training domains to 9 (train) : 1 (val) and test on the whole held-out domain. Recall that our model uses a 2-hot encoding of z to parameterize the CNN. The domain-specific vs agnostic 'prior' can be set by varying the ratio ? of the elements in the 2-hot coding. For training we use ? = 0.3, so z = {[0, 0, 0.3, 1], [0, 0.3, 0, 1], ...}. For DG testing we use z = [0, 0, 0, 1].</p><p>Baselines: We evaluate our contributions by comparison with number of alternatives including variants designed to reveal insights, and state of the art competitors: Ours-MLP: Our DG method applied to a 1 hidden layer multi-layer perception. For use with pre-extracted features. Ours-Full: Our full low-rank parameterized CNN trained end-to-end on images. SVM: Linear SVM, applied on the aggregation of data from all source domains. Deep-All: Pretrained Alexnet CNN <ref type="bibr" target="#b13">[14]</ref>, fine-tuned on the aggregation of all source domains. Undo-Bias: Modifies traditional SVM to include a domain-specific and global weight vector which can be extracted for DG <ref type="bibr" target="#b11">[12]</ref>. The original    Undo-Bias is a binary classifier (BC). We also implement a multi-class (MC) generalization. uDICA: A kernel based method learning a subspace to minimize the dissimilarity between domains [19] 1 . UML: Structural metric learning algorithm learn a low-bias distance metric for classification tasks <ref type="bibr" target="#b6">[7]</ref>. LRE-SVM: Exploits latent domains, and a nuclear-norm based regularizer on the likelihood matrix of exemplar-SVM <ref type="bibr" target="#b29">[30]</ref>. 1HNN: 1 hidden layer neural network. MTAE-1HNN: 1HNN with multi-task auto encoder <ref type="bibr" target="#b9">[10]</ref>. D-MTAE-1HNN: 1HNN with de-noising multi-task auto encoder <ref type="bibr" target="#b9">[10]</ref>. DSN: The domain separation network learns specific and shared models for the source and target domains <ref type="bibr" target="#b1">[2]</ref>. We re-purpose the original DSN from the domain adaptation to the DG task. Note that DSN is already shown to outperform the related <ref type="bibr" target="#b8">[9]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">VLCS Benchmark</head><p>Classic Benchmark -Binary Classification with Shallow Features Since our approach to extracting a domain invariant model is related to the intuition in Undo Bias <ref type="bibr" target="#b11">[12]</ref>, we first evaluate our methodology by performing a direct comparison against Undo Bias. We use the same 5376 di-1 Like <ref type="bibr" target="#b9">[10]</ref>, we found sDICA to be worse than uDICA, so excluded it.</p><p>mensional VLCS SIFT-BOW features 2 from <ref type="bibr" target="#b11">[12]</ref>, and compare Our-MLP using one RELU hidden layer with 4096 neurons. For direct comparison, we apply Our-MLP in a 1-vs-All manner as per Undo-Bias. The results in <ref type="table" target="#tab_2">Table 1</ref> show that without exploiting the benefit of end-to-end learning, our approach still performs favorably compared to Undo Bias. This is due to (i) our low-rank modeling of domain-specific and domain-agnostic knowledge, and (ii) the generalization of doing so in a multi-layer network.</p><p>Multi-class recognition with Deep Learning In this experiment we continue to analyze the VLCS benchmark, but from a multiclass classification perspective. We compare existing DG methods (Undo-Bias <ref type="bibr" target="#b11">[12]</ref>, UML <ref type="bibr" target="#b6">[7]</ref>, LRE-SVM <ref type="bibr" target="#b29">[30]</ref>, uDICA <ref type="bibr" target="#b18">[19]</ref>, MTAE+1HNN <ref type="bibr" target="#b9">[10]</ref>, D-MTAE+1HNN <ref type="bibr" target="#b9">[10]</ref>) against baselines (1HNN, SVM, Deep) and our methods Ours-MLP/Ours-Full. For the other methods besides Deep-All and Ours-Full, we follow <ref type="bibr" target="#b9">[10]</ref> and use pre-extracted DECAF 6 features 3 <ref type="bibr" target="#b3">[4]</ref>. For Deep and Ours-Full, we fine-tune the CNN on the source domains. From the results in <ref type="table" target="#tab_3">Table 2</ref>, we make the following ob-   From the results, we can see that each component helps: (i) 2HE-Last outperforms Tuning-Last, demonstrating the ability of our tensor weight generator to synthesize domain agnostic models for a multiclass classifier. (ii) 2HE+Decomp-Last outperforms 2HE-Last, demonstrating the value of our low-rank tensor modeling of the weight generator parameters. (iii) Ours-Full outperforms 2HE+Decomp-Last, demonstrating the value of performing these DG strategies at every layer of the network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Further Analysis</head><p>Learned Layer-wise Sharing Strength An interesting property of our approach is that, unlike some other deep learning methods <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b16">17]</ref> it does not require manual specification of the cross-domain sharing structure at each layer of the CNN; and unlike Undo Bias <ref type="bibr" target="#b11">[12]</ref> it can choose how to share more flexibly through the rank choice at each layer. We can observe the estimated sharing structure at each layer by performing Tucker decomposition to factorize the tuned model under a specified reconstruction error threshold ( = 0.001). The resulting domain-rank at each layer reveals the sharing strength. The rank per-layer for each held-out domain in PACS is shown in <ref type="figure" target="#fig_3">Fig. 3b</ref>. Here there are three training domains, so the maximum rank is 3 and the minimum rank is 1. Intuitively, the results show heavily shared Conv1-Conv3 layers, and low-sharing in FC6-FC8   Visualization To visualize the preferences of our multidomain network, we apply the DGN-AM <ref type="bibr" target="#b20">[21]</ref> method to synthesize the preferred input images for our model when parameterized (via the domain descriptor z) to one specific domain versus the abstract domain-agnostic factor. This visualization is imperfect because <ref type="bibr" target="#b20">[21]</ref> is trained using a photo-domain, and most of our domains are nonphotographic art. Nevertheless, from <ref type="figure">Fig. 4</ref> the synthesis for Photo domain seem to be the most concrete, while the Sketch/Cartoon/Painting domains are more abstract.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We presented a new dataset and deep learning-based method for domain generalization. Our PACS (Photo-Art-Cartoon-Sketch) dataset is aligned with a practical application of domain generalization, and we showed it has more challenging domain shift than prior datasets, making it suitable to drive the field in future. Our new domain generalization method integrates the idea of learning a domainagnostic classifier with a robust deep learning approach for end-to-end learning of domain generalization. The result performs comparably or better than prior approaches.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Evaluation of domain shift in different domain generalization benchmarks. Measuring domain-shift by within versus across domain accuracy. Left: Our PACS, Middle: VLCS. Right: Distribution of margins between within and across domain accuracy.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Per-layer rank (inverse sharing strength) after learning for different held out domains.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Cross-domain similarity (a) and learned sharing strength by layer (b).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>MLP Undo bias Ours-MLP Undo bias Ours-MLP Undo bias Ours-MLP Undo bias Ours-MLP</figDesc><table><row><cell cols="3">Bird Undo bias Ours-Caltech Unseen domain 12.08 10.89</cell><cell>63.80</cell><cell>Car</cell><cell>61.29</cell><cell>7.54</cell><cell>Chair</cell><cell>11.26</cell><cell>5.24</cell><cell>Dog</cell><cell>3.90</cell><cell>50.81</cell><cell>Person</cell><cell>48.48</cell></row><row><cell>LabelMe</cell><cell>33.08</cell><cell>28.35</cell><cell>69.22</cell><cell></cell><cell>74.07</cell><cell>5.34</cell><cell></cell><cell>3.68</cell><cell>1.66</cell><cell></cell><cell>2.06</cell><cell>64.85</cell><cell>67.00</cell></row><row><cell>Pascal</cell><cell>15.42</cell><cell>13.63</cell><cell>37.49</cell><cell></cell><cell>42.81</cell><cell>30.05</cell><cell></cell><cell>32.71</cell><cell>14.97</cell><cell></cell><cell>15.93</cell><cell>58.47</cell><cell>63.61</cell></row><row><cell>Sun</cell><cell>0.59</cell><cell>2.01</cell><cell>70.62</cell><cell></cell><cell>71.32</cell><cell>37.44</cell><cell></cell><cell>37.50</cell><cell>1.12</cell><cell></cell><cell>1.89</cell><cell>42.20</cell><cell>42.71</cell></row><row><cell>Mean AP %</cell><cell>15.29</cell><cell>13.72</cell><cell>60.28</cell><cell></cell><cell>62.37</cell><cell>20.09</cell><cell></cell><cell>21.29</cell><cell>5.75</cell><cell></cell><cell>5.94</cell><cell>54.08</cell><cell>55.45</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Comparison against Undo-Bias [12] on the VLCS benchmark using classic SIFT-BOW features, and our shallow model Ours-MLP. Average precision (%) and mean average precision (%) of binary 1-v-all classification in unseen domains.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Comparison of features and state of the art on the VLCS benchmark. Multi-class accuracy (%). Full using end-to-end learning. From the results inTable 3we make the observations: (i) uDICA and D-MTAE-1HNN are the best prior DG models, and DSN is also effective despite being designed for DA. While uDICA scores well overall, this is mostly due to very high performance on the photo domain. This is understandable as in that condition DICA uses unaltered DECAF 7 features tuned for photo recognition. It is also the least useful direction for DG, as photos are already abundant.(ii) As for the VLCS benchmark, Deep-ALL again performs well. (iii) However Ours-Full performs best overall by combining the robustness of a CNN architecture with an explicit DG mechanism. Fine-tunes the final FC layer, and uses our tensor weight generation (Eq. 3) based on 2-hot encoding for multidomain learning, before transferring the shared model component to the target. But without low rank factorisation. 2HE+Decomp-Last: Uses 2-hot encoding based weight synthesis, and low-rank decomposition of the final layer (Eq. 3). Ours-Full: Uses 2-hot encoding and low-rank modeling on every layer in the CNN.</figDesc><table><row><cell>servations: (i) Given the fixed DECAF 6 feature, most prior</cell></row><row><cell>DG methods improve on vanilla SVM, and D-MTAE [10] is</cell></row><row><cell>the best of these. (ii) Ours-MLP outperforms 1HNN, which</cell></row><row><cell>uses the same type of architecture and the same feature.</cell></row><row><cell>This margin is due to our low-rank domain-generalization</cell></row><row><cell>approach. (iii) The very simple baseline of fine-tuning a</cell></row><row><cell>deep model on the aggregation of source domains (Deep-</cell></row><row><cell>All) performs surprisingly well and actually outperforms all</cell></row><row><cell>the prior DG methods. (iii) Ours-Full outperforms Deep-All</cell></row><row><cell>slightly. This small margin is understandable. Our model</cell></row><row><cell>does have more parameters to learn than Deep-All, despite</cell></row><row><cell>the low rank; and the cost of doing this is not justified by the</cell></row><row><cell>relatively small domain gap between the VLCS datasets.</cell></row><row><cell>4.3.3 Our PACS benchmark</cell></row><row><cell>We compare baselines (SVM, 1HNN) and prior methods</cell></row><row><cell>(LRE-SVM [30], D-MTAE+1HNN [10], uDICA [19]) us-</cell></row><row><cell>ing DECAF 7 features against Deep-ALL, DSN [2] and</cell></row><row><cell>Ours-</cell></row></table><note>Ablation Study: To investigate the contributions of each components in our framework, we compare the following variants: Tuning-Last: Trains on all sources followed by di- rect application to the target. But fine-tunes the final FC layer only. 2HE-Last:</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>Visualization of the preferred images of output neurons 'horse', 'giraffe' and 'house' in the domains of the PACS dataset. Left: real images. Middle: synthesized images for PACS domains. Right: synthesized images for agnostic domain.</figDesc><table><row><cell></cell><cell>house</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>giraffe</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>horse</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Real image</cell><cell></cell><cell>Photo</cell><cell>Art painting</cell><cell>Cartoon</cell><cell>Sketch</cell><cell></cell><cell>Agnostic</cell><cell></cell></row><row><cell>Figure 4: Unseen domain</cell><cell cols="8">Image ? Deep Feature ? Classifier SVM 1HNN uDICA [19] LRE-SVM [30] D-MTAE+1HNN [10] Ours-MLP Deep-All DSN [2] Ours-Full Image ? E2E</cell></row><row><cell>Art painting</cell><cell>55.39 59.10</cell><cell>64.57</cell><cell>59.74</cell><cell>60.27</cell><cell>61.40</cell><cell>63.30</cell><cell>61.13</cell><cell>62.86</cell></row><row><cell>Cartoon</cell><cell>52.86 57.89</cell><cell>64.54</cell><cell>52.81</cell><cell>58.65</cell><cell>57.16</cell><cell>63.13</cell><cell>66.54</cell><cell>66.97</cell></row><row><cell>Photo</cell><cell>82.83 89.86</cell><cell>91.78</cell><cell>85.53</cell><cell>91.12</cell><cell>89.68</cell><cell>87.70</cell><cell>83.25</cell><cell>89.50</cell></row><row><cell>Sketch</cell><cell>43.89 50.31</cell><cell>51.12</cell><cell>37.89</cell><cell>47.86</cell><cell>50.38</cell><cell>54.07</cell><cell>58.58</cell><cell>57.51</cell></row><row><cell>Ave.%</cell><cell>58.74 64.29</cell><cell>68.00</cell><cell>58.99</cell><cell>64.48</cell><cell>64.65</cell><cell>67.05</cell><cell>67.37</cell><cell>69.21</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Evaluation % of classification on PACS. Multi-class accuracy (%).</figDesc><table><row><cell>Unseen domain</cell><cell cols="4">Ablation Study Tuning-Last 2HE-Last 2HE+Decom-Last Ours-Full</cell></row><row><cell>Art painting</cell><cell>59.79</cell><cell>59.20</cell><cell>62.71</cell><cell>62.86</cell></row><row><cell>Cartoon</cell><cell>56.22</cell><cell>55.50</cell><cell>52.69</cell><cell>66.97</cell></row><row><cell>Photo</cell><cell>86.79</cell><cell>87.33</cell><cell>88.84</cell><cell>89.50</cell></row><row><cell>Sketch</cell><cell>46.41</cell><cell>48.45</cell><cell>52.16</cell><cell>57.51</cell></row><row><cell>Ave.%</cell><cell>62.30</cell><cell>62.62</cell><cell>64.10</cell><cell>69.21</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Ablation study. Multi-class accuracy (%). layers. The middle layers Conv4 and Conv5 have different sharing strength according to which domains provide the source set. For example, in Conv 5, when Sketch is unseen, the other domains are relatively similar so can have greater sharing, compared to when Sketch is included as a seen domain. This is intuitive as Sketch is the most different from the other three domains. This flexible ability to determine sharing strength is a key property of our model.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">http://undoingbias.csail.mit.edu/ 3 http://www.cs.dartmouth.edu/?chenfang/proj_ page/FXR_iccv13/index.php</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning feed-forward one-shot learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bertinetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Valmadre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Domain separation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bousmalis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Trigeorgis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Face painting: querying art with photos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">J</forename><surname>Crowley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">M</forename><surname>Parkhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Decaf: A deep convolutional activation feature for generic visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The art of detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Z E J</forename><surname>Crowley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV Workshop on Computer Vision for Art Analysis</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">How do humans sketch objects?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Eitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Alexa</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>TOG</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Unbiased metric learning: On the utilization of multiple datasets and web images for softening bias</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">N</forename><surname>Rockmore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Model-agnostic metalearning for fast adaptation of deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation by backpropagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Domain generalization for object recognition with multi-task autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ghifary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Bastiaan Kleijn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Balduzzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Geodesic flow kernel for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Undoing the damage of dataset bias</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Malisiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Compression of deep convolutional neural networks for fast and low power mobile applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Shin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">A multilinear singular value decomposition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">D</forename><surname>Lathauwer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">D</forename><surname>Moor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Vandewalle</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
			<publisher>SIMAX</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Speeding-up convolutional neural networks using fine-tuned cp-decomposition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lebedev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rakhuba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Oseledets</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning transferable features with deep adaptation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Visualizing data using t-sne</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V D</forename><surname>Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>JMLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Domain generalization via invariant feature representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Muandet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Balduzzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Scholkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Munkhdalai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<title level="m">Meta networks. In ICML</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Synthesizing the preferred inputs for neurons in neural networks via deep generator networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Clune</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Optimization as a model for fewshot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Adapting visual category models to new domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fritz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">The sketchy database: learning to retrieve badly drawn bunnies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sangkloy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Burnell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>TOG</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sigaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Masson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Filliat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Stulp</surname></persName>
		</author>
		<title level="m">Gated networks: an inventory. arXiv</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep spatial-semantic attention for fine-grained sketchbased image retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-Z</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Unbiased look at dataset bias</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Some mathematical notes on three-mode factor analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">R</forename><surname>Tucker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychometrika</title>
		<imprint>
			<date type="published" when="1966" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning graphs to model visual objects across different depictive styles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Exploiting low-rank structure from latent domains for domain generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Deep multi-task representation learning: A tensor factorisation approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Clune</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lipson</surname></persName>
		</author>
		<title level="m">How transferable are features in deep neural networks? In NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Sketch me that shoe</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-Z</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Sketch-a-net: A deep neural network that beats humans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-Z</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
