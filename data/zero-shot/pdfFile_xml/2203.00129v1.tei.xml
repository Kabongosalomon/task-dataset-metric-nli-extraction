<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">BlazeNeo: Blazing fast polyp segmentation and neoplasm detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nguyen</forename><forename type="middle">S</forename><surname>An</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Hanoi University of Science and Technology</orgName>
								<address>
									<settlement>Hanoi</settlement>
									<country key="VN">Vietnam</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phan</forename><forename type="middle">N</forename><surname>Lan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Hanoi University of Science and Technology</orgName>
								<address>
									<settlement>Hanoi</settlement>
									<country key="VN">Vietnam</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dao</forename><forename type="middle">V</forename><surname>Hang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Hanoi Medical University</orgName>
								<address>
									<settlement>Hanoi</settlement>
									<country key="VN">Vietnam</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Institute of Gastroenterology and Hepatology</orgName>
								<address>
									<settlement>Hanoi</settlement>
									<country key="VN">Vietnam</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dao</forename><forename type="middle">V</forename><surname>Long</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Hanoi Medical University</orgName>
								<address>
									<settlement>Hanoi</settlement>
									<country key="VN">Vietnam</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Institute of Gastroenterology and Hepatology</orgName>
								<address>
									<settlement>Hanoi</settlement>
									<country key="VN">Vietnam</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tran</forename><forename type="middle">Q</forename><surname>Trung</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution" key="instit1">University of Medicine and Pharmacy</orgName>
								<orgName type="institution" key="instit2">Hue University</orgName>
								<address>
									<settlement>Hue</settlement>
									<country key="VN">Vietnam</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nguyen</forename><forename type="middle">T</forename><surname>Thuy</surname></persName>
							<affiliation key="aff4">
								<orgName type="department">Faculty of Information Technology</orgName>
								<orgName type="institution">National University of Agriculture</orgName>
								<address>
									<settlement>Hanoi</settlement>
									<country>Vietnam, Vietnam</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dinh</forename><forename type="middle">V</forename><surname>Sang</surname></persName>
							<email>sangdv@soict.hust.edu.vn.</email>
							<affiliation key="aff0">
								<orgName type="institution">Hanoi University of Science and Technology</orgName>
								<address>
									<settlement>Hanoi</settlement>
									<country key="VN">Vietnam</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">BlazeNeo: Blazing fast polyp segmentation and neoplasm detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1109/ACCESS.2021.DOI</idno>
					<note>Corresponding author: Dinh V. Sang</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T14:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In recent years, computer-aided automatic polyp segmentation and neoplasm detection have been an emerging topic in medical image analysis, providing valuable support to colonoscopy procedures. Attentions have been paid to improving the accuracy of polyp detection and segmentation. However, not much focus has been given to latency and throughput for performing these tasks on dedicated devices, which can be crucial for practical applications. This paper introduces a novel deep neural network architecture called BlazeNeo, for the task of polyp segmentation and neoplasm detection with an emphasis on compactness and speed while maintaining high accuracy. The model leverages the highly efficient HarDNet backbone alongside lightweight Receptive Field Blocks for computational efficiency, and an auxiliary training mechanism to take full advantage of the training data for the segmentation quality. Our experiments on a challenging dataset show that BlazeNeo achieves improvements in latency and model size while maintaining comparable accuracy against state-of-the-art methods. When deploying on the Jetson AGX Xavier edge device in INT8 precision, our BlazeNeo achieves over 155 fps while yielding the best accuracy among all compared methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Colorectal polyps, especially adenomas with high-grade dysplasia, carry high risks of progressing into colorectal cancer (CRC) <ref type="bibr" target="#b9">[10]</ref>, which claims over 640,000 lives each year <ref type="bibr" target="#b2">[3]</ref>. There are available procedures to screen and detect high-risk polyps in an early stage, increasing chances of successful treatment. Polyp detection and removal in colonoscopy are the most effective method to prevent colorectal cancer <ref type="bibr" target="#b13">[14]</ref>.</p><p>At the same time, practical factors such as overloading healthcare systems, low-quality endoscopy equipment, or personnel's lack of experience <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b20">[21]</ref> can severely limit the effectiveness of colonoscopy. A review by Leufkens et al. <ref type="bibr" target="#b21">[22]</ref> pointed out that 20 ? 47% of polyps might have been missed during colonoscopies. Several types of imageenhanced endoscopies and accessories have been proposed to alleviate these, yet they can be prohibitively expensive for practical applications, especially in poor medical facilities. On the other hand, computer-aided systems for colonoscopy have shown a lot of promise and have attracted many researchers in recent years. Several works have achieved very good performance on benchmark datasets <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b38">[39]</ref>.</p><p>Polyp segmentation is a subset of medical image analysis that has gained much attention recently. Traditional machine learning methods for solving the problem are mostly based on hand-crafted features <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b34">[35]</ref> to extract image information such as color, shape, and textures. Since polyps have very high intra-class diversity and low inter-class variation, such approaches are often limited in representing and detecting polyps. Deep neural networks, and especially U-Net <ref type="bibr" target="#b31">[32]</ref> have been the state-of-the-art methods for polyp segmentation in the last few years. These networks can learn highly abstract and complex features, allowing them to achieve good performances. At the same time, deep neural networks also come with a complexity trade-off, as models can be very large (up to several hundred million parameters) and cause high latency during inference.</p><p>Most of the existing research in endoscopy image analysis has focused on the polyp segmentation problem, in which lesion regions or polyps are segmented from the background pixels. Those works attempted to improve the learning models to provide accurate segmentation of polyps. However, the segmentation of polyps only does not provide information about the type of polyps, i.e., benign (noncancerous) or malignancy (presence of cancerous cells). In our seminal work <ref type="bibr" target="#b19">[20]</ref>, we have defined the problem of Polyp Segmentation and Neoplasm Detection (PSND), aiming at fine-grained segmentation of polyps. This can be considered as an extension of the polyp segmentation problem, providing richer semantic information for the segmented regions. Particularly, besides classifying image pixels as polyp or background, the proposed formulation further identifies each polyp pixel as non-neoplastic, neoplastic, or undefined. In general, non-neoplastic polyps are typically benign, while neoplastic polyps have a risk of developing cancer. Our newly developed UNet-based neural network architecture, NeoUNet, has obtained state-of-the-art performance in solving this problem in terms of accuracy. However, attention has not yet been paid to the model size and speed, which is challenging for practical deployment.</p><p>In this paper, we further improve on our previous works on Polyp Segmentation and Neoplasm Detection with the proposal of BlazeNeo, a novel deep neural network architecture with an efficient learning mechanism. Our main contributions are:</p><p>? A new deep neural network architecture called BlazeNeo for polyp segmentation and neoplasm detection, aiming at reducing the model size and therefore improving inference speed; ? An auxiliary training strategy to fully exploit training data for maintaining high accuracy while reducing model size; ? Extensive experiments on the newly collected NeoPolyp dataset and comparisons to existing models. Moreover, we measure model latency and throughput on dedicated hardware in a setting similar to real-life deployments of polyp segmentation and neoplasm detection. The rest of the paper is organized as follows. We provide a brief review of related works in Section II. Section III briefly describes the polyp segmentation and neoplasm detection problem as formulated in <ref type="bibr" target="#b19">[20]</ref>. The BlazeNeo architecture is presented in Section IV. Section V showcases our experimental studies. Finally, we conclude the paper and highlight future works in Section VII.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>In recent years, many computer vision tasks have seen massive improvements through the advancements of convolutional neural networks (CNNs). AlexNet <ref type="bibr" target="#b18">[19]</ref> and VGG <ref type="bibr" target="#b35">[36]</ref> are among the first successful CNNs for the image classification problem. However, these early models still suffer from degradation when increasing network depth. Many works have attempted to modify the network architectures to improve learning capability aiming at improving network performance. Skip connections, first introduced in ResNet <ref type="bibr" target="#b10">[11]</ref> in 2016, helped alleviate the degradation and smoothed out the loss landscape. ResNeXt <ref type="bibr" target="#b42">[43]</ref> combined the idea of skip connections with a multi-branch design first proposed by the authors of GoogLeNet <ref type="bibr" target="#b36">[37]</ref>. More recently, Tan et al. <ref type="bibr" target="#b37">[38]</ref> employed neural architecture search to produce EfficientNet, a family of neural networks with varying levels of the tradeoff between accuracy and latency. Meanwhile, HarDNet <ref type="bibr" target="#b3">[4]</ref> is a model highly focused on optimizing inference latency and memory traffic.</p><p>Many CNN architectures have been designed for the semantic image segmentation task, especially in medical images. Among the earliest was the work by Long et al. <ref type="bibr" target="#b26">[27]</ref>, who adopted several well-known architectures using transfer learning. In the same year, U-Net <ref type="bibr" target="#b31">[32]</ref> became a breakthrough model in medical imaging, achieving highly promising results for medical image segmentation. Later works such as UNet++ <ref type="bibr" target="#b46">[47]</ref>, DoubleUNet <ref type="bibr" target="#b16">[17]</ref> and Coupled U-Net <ref type="bibr" target="#b38">[39]</ref> further improved and alleviated limitations in U-Net. ColonSegNet <ref type="bibr" target="#b15">[16]</ref> was a lightweight encoder-decoder architecture that uses residual connections with squeeze and excitation network as the main component. ColonSegNet achieved a high inference speed but with sacrificing accuracy a lot. DDANet <ref type="bibr" target="#b39">[40]</ref> was another encoder-decoder design that leverages the strength of residual connection and squeezeand-excitation modules. DDANet incorporated a single encoder followed by two dual decoders. The first decoder is used for the segmentation mask, while the second one acts as an autoencoder model that reconstructs the grayscale image and helps strengthen the feature representation of the encoder. Attention-UNet <ref type="bibr" target="#b28">[29]</ref> proposed attention gates as a filter mechanism for selecting useful salient features. Development on non-UNet models in medical segmentation has also remained active. DeepLabV3 <ref type="bibr" target="#b5">[6]</ref> is a prominent architecture that utilizes atrous convolutions for dense feature extraction. Fan et al. <ref type="bibr" target="#b8">[9]</ref> enhanced an FCN-like model with parallel partial decoder and reverse attention to form PraNet, a network that achieved state-of-the-art performance on many benchmark datasets. HarDNet-MSEG <ref type="bibr" target="#b11">[12]</ref> employed an encoder-decoder structure with HarDNet as the encoder backbone, achieving good performance on the Kvasir-SEG dataset and very high inference speed. Meanwhile, TransFuse <ref type="bibr" target="#b45">[46]</ref> combined a CNN with the Transformer architecture using a fusion module called BiFusion.</p><p>Many deep learning methods are also specially designed for the polyp segmentation and detection problem. Qadir et al. <ref type="bibr" target="#b29">[30]</ref> proposed a framework that incorporates a CNN architecture for labeling segmentation masks for polyps. The framework allows doctors to receive pre-annotations from a model trained in a semi-supervised manner. In <ref type="bibr" target="#b32">[33]</ref>, the authors proposed a model with an Inception-ResNet backbone combined with several post-learning methods to enhance polyp detection accuracy. Shin et al. <ref type="bibr" target="#b33">[34]</ref> used conditional adversarial networks to generate abnormal samples for training polyp detection models. Liu et al. <ref type="bibr" target="#b23">[24]</ref> applied different CNN backbones including InceptionV3 <ref type="bibr" target="#b36">[37]</ref>, ResNet50 <ref type="bibr" target="#b10">[11]</ref> and VGG16 <ref type="bibr" target="#b35">[36]</ref> to the SSD framework, whose accuracy was much higher than other one-stage object detectors and comparable to the Faster-RCNN two-stage one. In <ref type="bibr" target="#b22">[23]</ref>, Li et al. compared the performance of eight state-of-the-art deep learning object detectors and demonstrated promising results in colonoscopy.</p><p>While there have been few works in the past several years concerning polyp neoplasm detection (such as that of Ribeiro et al. <ref type="bibr" target="#b30">[31]</ref>), it was only very recently that neoplasm detection has been incorporated into polyp segmentation. Phan et al. <ref type="bibr" target="#b19">[20]</ref> formally described the problem (denoted as PSND) and proposed NeoUNet, a UNet-based architecture that established the baseline for PSND. While NeoUNet strikes a balance between accuracy and inference speed, typical deployment scenarios for polyp segmentation systems require relatively low latency to ensure smooth operation. Moreover, such scenarios would have the neural network run on lightweight systems embedded within the colonoscopy device. Therefore, this paper focuses on building a network architecture optimized for speed and small in size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. POLYP SEGMENTATION AND NEOPLASM DETECTION</head><p>Polyp Segmentation and Neoplasm Detection (PSND) has been formulated as a type of fine-grained polyp segmentation problem <ref type="bibr" target="#b19">[20]</ref>. Besides segmentation of polyps, this formulation further classifies a polyp pixel into two classes: non-neoplastic or neoplastic. In medical image analysis, nonneoplastic polyps are considered benign, while neoplastic polyps may further progress with a higher risk of cancer. During a colonoscopy, the operator must decide the types of polyps, neoplasm or non-neoplastic, to consider an optimal management strategy, i.e., removal or resection in endoscopy procedure or biopsy and operation. Figures 1 depicts the PSND problem as a semantic extension of the polyp segmentation problem.</p><p>In practice, an "undefined" class is labeled if there is not enough information from the endoscopic image to categorize the risk of neoplasm. This undefined subgroup is not a specific class we want to predict since such predictions would not bring any insight for the endoscopist. Therefore, the model only needs to learn to discriminate between neoplastic and non-neoplastic polyps. However, the undefined class still gives supplementary information about polyp regions that can be exploited to help the model gain more representation power in the training phase. <ref type="figure">Figure 2</ref> depicts three versions of the proposed BlazeNeo model. The models are developed based on HarDNet-MSEG <ref type="bibr" target="#b11">[12]</ref>, with several improvements tailored for the PSND problem. First, we use a simplified version of RFB <ref type="bibr" target="#b24">[25]</ref> with smaller kernel sizes and apply different feature aggregation schemes. For the encoder layer, we keep the HarDNet-68 <ref type="bibr" target="#b3">[4]</ref> backbone as in HarDNet-MSEG.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. PROPOSED METHOD</head><p>The three BlazeNeo variants differ in how outputs from the model are generated and processed. Inspired by NeoUNet <ref type="bibr" target="#b19">[20]</ref>, Single-headed Binary BlazeNeo (SB-BlazeNeo) solves two binary segmentation tasks corresponding to neoplastic and non-neoplastic classes. The second variant, Singleheaded Trinary BlazeNeo (ST-BlazeNeo), predicts a trinary map for three classes in the neoplasm segmentation task. Finally, Multi-headed BlazeNeo (or BlazeNeo for short) uses two output branches. The main output branch is used for the neoplasm segmentation task, and the auxiliary branch is for the polyp segmentation task. The following subsections describe our design and motivations in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. LIGHTWEIGHT ENCODER: HARDNET</head><p>HarDNet (Harmonic Densely Connected Network) <ref type="bibr" target="#b3">[4]</ref> is an improvement over DenseNet <ref type="bibr" target="#b12">[13]</ref>. The primary goal in HarDNet's design is to lower latency by reducing memory traffic. The authors argued that the connection pattern of Dense Blocks, in which each layer has skip connections toward every proceeding layer in the block, causes ineffective memory access during runtime that severely hinders performance. HarDNet reduces the number of skip connections to form a pattern similar to the harmonic wave function, as well as scaling channel width according to a layer's influence level. We illustrate the difference between the two architectures in <ref type="figure" target="#fig_0">Figure 3</ref>.</p><p>Inside a HarDNet block, each layer is indexed from the input layer 0. A layer l receives a skip connection from layer l ? 2 n if 2 n divides l (n ? 0, l ? 2 n ? 0). Given the initial growth rate k and compression factor m, layer l's channel</p><formula xml:id="formula_0">width is equal to k ? m x , where x = max{? | l . . . 2 ? }.</formula><p>Chao et al. <ref type="bibr" target="#b3">[4]</ref> further proposed HarDNet-68 for smallobject detection problems. While most CNNs focus on stride-16 to enhance classification ability, HarDNet-68 distributes most of the layers on stride-8 to aid small-scale object detection, as shown in <ref type="figure" target="#fig_1">Figure 4</ref>. Experiments in <ref type="bibr" target="#b3">[4]</ref> show that HarDNet-68 is not only 30% faster than ResNet-50 <ref type="bibr" target="#b10">[11]</ref> but also more accurate than ResNet-101 <ref type="bibr" target="#b10">[11]</ref> when used as backbone for SSD <ref type="bibr" target="#b25">[26]</ref> in the object detection problem.</p><p>Our BlazeNeo also uses HarDNet-68 as the encoder backbone. For an input colonoscopy image I with size h ? w, five levels of features {f i , i = 1, 2, 3, 4, 5} with resolution [h/2 i?1 , w/2 i?1 ] are produced from the encoder. Wu et al. <ref type="bibr" target="#b40">[41]</ref> showed that low-level features (corresponding to f 1 and f 2 ) contribute less to performance while being computationally expensive due to their size. Therefore, our BlazeNeo discards f 1 and f 2 , using only the final three feature maps for the decoder module to accelerate its inference speed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. PARALLEL PARTIAL DECODER</head><p>In all three variants of our BlazeNeo, the decoder module consists of a Receptive Field Block (RFB) series. Three last feature maps of the encoder are independently passed through the RFB blocks and then fused by the feature aggregation blocks. FIGURE 1: PSND is an extension of polyp segmentation, which further discriminates whether a polyp is neoplastic or nonneoplastic. For a given input image shown on the top right corner, expected outputs for polyp segmentation and PSND are depicted on the middle and the bottom images on the right, respectively. The black color denotes background pixels, white color denotes polyp regions; green and red colors denote non-neoplastic and neoplastic polyps, respectively. FIGURE 2: Proposed architectures of our BlazeNeo: (a) Single-headed Binary BlazeNeo (SB-BlazeNeo) has one output branch that produces two binary segmentation maps corresponding to neoplastic and non-neoplastic classes; (b) Single-headed Trinary BlazeNeo (ST-BlazeNeo) also has one output branch that directly predicts a trinary segmentation map; (c) Multi-headed BlazeNeo (or BlazeNeo for short) contains two output branches that are responsible for the two tasks: neoplasm segmentation treated as the main task, and polyp segmentation treated as the auxiliary task. Both branches share the same architecture of the feature aggregation module, but they are trained separately without sharing their parameters. size is often fixed. Several studies have suggested different mechanisms to create more robust receptive fields, including the Inception block <ref type="bibr" target="#b36">[37]</ref>, ASPP block <ref type="bibr" target="#b4">[5]</ref>, and Deformable Convolution block <ref type="bibr" target="#b7">[8]</ref>. Conceptually, these proposals are  similar in that they all use multiple convolutional branches with different kernel sizes, merging the outputs to form adaptive receptive fields. However, they also have their own limitations. The Inception block samples all the kernels of each branch at their center, ignoring crucial edge details due to small sampling coverage. Meanwhile, the ASPP and Deformable Convolution blocks do not differentiate between different pixel positions, making it difficult for the model to focus on segmentation targets.</p><p>The Receptive Field Block (RFB) <ref type="bibr" target="#b24">[25]</ref> has been proposed to address these limitations. RFB also uses the multi-branch convolution approach, with improvements inspired by the human visual cortex. In addition, it highlights the relationship between receptive field size and eccentricity, where the distance to the center is proportional to the weight specified or the importance level.</p><p>The RFB module used in PraNet <ref type="bibr" target="#b8">[9]</ref> and HarDNet-MSEG <ref type="bibr" target="#b11">[12]</ref> is modified with a larger kernel size and larger dilation rate. As this version of RFB is more computationally expensive, our BlazeNeo instead uses a simplified version of RFB (see <ref type="figure" target="#fig_2">Figure 5</ref>) as proposed in <ref type="bibr" target="#b24">[25]</ref> for faster inference. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2) Feature Aggregation</head><p>A high polyp miss rate is often associated with small and flat polyps (whose perimeters are below 10mm) <ref type="bibr" target="#b17">[18]</ref>. In order to detect these polyps, it is important to obtain high-resolution features from multiple image scales. Feature fusion (or aggregation) is a well-studied technique to achieve this for CNNs, in which feature maps from different scales are fused to form a multi-scale feature map. <ref type="figure" target="#fig_4">Figure 6</ref> demonstrates four feature aggregation techniques, in order of increasing complexity <ref type="bibr" target="#b44">[45]</ref>. <ref type="figure" target="#fig_4">Figure 6a</ref> is an early aggregation scheme used by segmentation networks such as UNet <ref type="bibr" target="#b31">[32]</ref>, UNet++ <ref type="bibr" target="#b46">[47]</ref>, and Attention UNet <ref type="bibr" target="#b28">[29]</ref>. Given feature maps X 1 , X 2 , X 3 , higher-level feature maps are upsampled and fused with their adjacent low-level features by a long skip connection, gradually restoring the spatial information. Each fusion module includes a concatenation layer and a convolutional layer with kernel size 3 ? 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Long Skip Connection (LSC) illustrated in</head><p>LSC is a well-tested and straightforward technique, but it is not without limitations. Zhang et al. <ref type="bibr" target="#b44">[45]</ref> proposed three alterations to produce higher-quality features, as described below.</p><p>Iterative Deep Aggregation (IDA) depicted in Figure 6b produces finer feature maps by using multiple iterative convolutions for a single scale.  Inspired by DenseNet, Dense Iterative Aggregation (DIA) introduces dense skip connections to the iterative convolutions in IDA (see <ref type="figure" target="#fig_4">Figure 6c</ref>). This addition ensures maximum information flow and reduces overfitting.</p><p>Dense Hierarchical Aggregation (DHA) further enhances semantic information by re-combining the output with highlevel feature maps as shown in <ref type="figure" target="#fig_4">Figure 6d</ref>.</p><p>From these observations, in our BlazeNeo, we propose to apply feature aggregation to the outputs of RFB modules at different scales. We examine variants with each of the aforementioned aggregation schemes, namely BlazeNeo-LSC, BlazeNeo-IDA, BlazeNeo-DIA, and BlazeNeo-DHA, in section V.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. LOSS FUNCTION AND AUXILIARY TRAINING</head><p>We aim to exploit the information from data with the undefined labels for training in a way similar to <ref type="bibr" target="#b19">[20]</ref>. The intuition is that while these data do not provide information for deciding on neoplasm class, they still can provide some semantic meaning from the data for the segmentation.</p><p>To training BlazeNeo, we propose the loss function L total consisting of two components: a main loss L main associated with the main task of neoplasm segmentation, and an auxiliary loss L aux associated with the auxiliary task of polyp segmentation. The total loss can be expressed as follows:</p><formula xml:id="formula_1">L total = L main + L aux<label>(1)</label></formula><p>The main loss L main drives the model toward making accurate class-specific segmentation. The pixels with undefined labels are excluded when calculating L main . The auxiliary loss L aux drives the model toward making accurate foreground-background segmentation, in which all pixels are used for training.</p><p>To investigate this training strategy, we iterate through three different ways to incorporate it into BlazeNeo, which creates three different variants: Single-headed Binary BlazeNeo (SB-BlazeNeo), Single-headed Trinary BlazeNeo (ST-BlazeNeo), and Multi-headed BlazeNeo (the final version of BlazeNeo).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) Single-headed Binary BlazeNeo</head><p>Our first variant, SB-BlazeNeo <ref type="figure">(Figure 2a</ref>), uses the same loss formulation as the NeoUNet model proposed in <ref type="bibr" target="#b19">[20]</ref>. The final output layer produces two binary segmentation maps, one for the neoplastic class and another for the nonneoplastic class. Losses are calculated separately for each map and then averaged. The main loss is a combination of Binary Cross Entropy and Focal Tversky loss <ref type="bibr" target="#b0">[1]</ref> as follows:</p><formula xml:id="formula_2">L main = L neo main + L non main = L BCE (P neo main , G neo main ) + L F T (P neo main , G neo main ) + L BCE (P non main , G non main ) + L F T (P non main , G non main ) (2)</formula><p>where P neo main and P non main denote the prediction maps for the neoplastic and non-neoplastic classes, respectively; G neo main and G non main are ground truths. We choose the Focal Tversky loss for L main to alleviate class imbalance due to small amount of non-neoplastic polyp pixels as shown later in <ref type="figure">Figure 8</ref>.</p><p>The auxiliary loss is a combination of Binary Cross Entropy and Tversky loss, which uses an auxiliary polyp segmentation map inferred from the two binary class-specific maps:</p><formula xml:id="formula_3">L aux = L BCE (P polyp aux , G polyp aux ) + L T (P polyp aux , G polyp aux ) (3)</formula><p>where P polyp aux and G polyp aux denote the auxiliary polyp prediction map and the corresponding ground truth.</p><p>The auxiliary polyp prediction map P polyp aux is inferred using element-wise max:</p><formula xml:id="formula_4">P polyp aux = max(P neo main , P non main )<label>(4)</label></formula><p>2) Single-headed Trinary BlazeNeo</p><p>While the method using binary map <ref type="bibr" target="#b19">[20]</ref> yielded promising classification results, we found that a lighter model can benefit from imposing an additional constraint on the outputs. Specifically, when the model outputs two separate classspecific maps for one image, it may make both maps have high prediction values for the same pixel. Therefore, a class constraint is needed to ensure that one class is chosen for each pixel. In our second variant, ST-BlazeNeo <ref type="figure">(Figure 2b</ref>), we use a 3-channel output map P trinary main , denoting the probabilities for the neoplastic, non-neoplastic, and background class, respectively. A softmax activation is used on the channel dimension, meaning each pixel may only belong to one class.</p><p>The main loss is a combination of Categorical Cross Entropy and Focal Tversky loss as follows:</p><formula xml:id="formula_5">L main = L CCE (P trinary main , G trinary main ) + L F T (P trinary main , G trinary main )<label>(5)</label></formula><p>where G trinary main is the trinary ground truth. Similarly, the auxiliary loss is a combination of Categorical Cross Entropy and Tversky loss:</p><formula xml:id="formula_6">L aux = L CCE (P polyp aux , G polyp aux ) + L T (P polyp aux , G polyp aux ) (6)</formula><p>Here we apply element-wise max to the two channels of the map P trinary main , which correspond to the neoplastic and nonneoplastic classes, to produce the auxiliary polyp segmentation map P polyp aux .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3) Multi-headed BlazeNeo</head><p>For our final version of BlazeNeo, we further evolve the use of auxiliary loss by adding an auxiliary segmentation branch. Auxiliary training is the process of jointly learning a side or auxiliary task to enhance the main task's performance <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b43">[44]</ref>. This idea is similar to multi-task learning, except the auxiliary branch does not activate in the inference phase.</p><p>The auxiliary branch uses an identical (with separate weights) feature aggregation module but outputs a binary segmentation map instead of a 3-channel multi-class map. The main loss and auxiliary loss are calculated exactly as in Eq. (5) and Eq. (6), respectively. However, there is no conversion between multi-class and polyp segmentation maps as in Eq. (4). Instead, the main loss is calculated with the 3channel multi-class map, and the auxiliary loss is calculated with the output map from the auxiliary branch. Intuitively, we believe the model will benefit from adding an auxiliary network branch since it strengthens the supervised signal and betters the optimization process during training the model. In practice, most non-neoplastic polyps are small, and the endoscopists can immediately remove them without the need to capture images or take a biopsy for lesions less than 5mm for post-checking. Due to that reason, neoplastic polyps take up a majority of the polyps present in NeoPolyp (see <ref type="figure">Figure 8</ref>). The number of neoplastic, non-neoplastic, and undefined polyps are 5113, 3185, and 1031, respectively. However, if we look at the pixel-wise level as shown in <ref type="figure">Figure 8b</ref>, we can observe a strong data imbalance between the three classes. The number of neoplastic polyp pixels takes up to 80% among all polyp pixels in the dataset. Meanwhile, these numbers for non-neoplastic and undefined classes are 13% and 7%, respectively. This data imbalance, combined with the inherent challenges of PSND, creates a difficult benchmark for models to overcome.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. EXPERIMENT SETUP</head><p>Our experiments include several ablation studies to verify the effectiveness of each component in BlazeNeo, a comparison against several polyp segmentation methods, and benchmarks on the NVIDIA Jetson AGX Xavier developer kit 1 , which closely resembles real deployments. The Jetson device is configured to run at MAXN power mode.</p><p>We oversample non-neoplastic polyps to account for class imbalance in the NeoPolyp dataset, as addressed by <ref type="bibr" target="#b19">[20]</ref>. Images containing non-neoplastic polyps are duplicated such that P non ? P neo , where P non and P neo are the number of pixels containing in non-neoplastic and neoplastic polyps, respectively.</p><p>The models are trained using Stochastic Gradient Descent (SGD) with Nesterov momentum and an initial learning rate of 0.001. The learning rate is adjusted according to a combination of linear warmup and a cosine annealing schedule.</p><p>Images used for training are at 3 different scales: 256?256, 352 ? 352 and 512 ? 512.</p><p>During training, augmentations including random scaling, rotation, horizontal/vertical flip, motion blur, and color jittering are added to improve generality. These augmentations are performed on-the-fly with a probability of 0.5.</p><p>BlazeNeo and other baseline models are implemented in Python 3.7 using the PyTorch framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. EVALUATION METRICS</head><p>Commonly used metrics Dice score and IoU score are employed to measure the model's output quality. They are evaluated on three classes: neoplastic polyp, non-neoplastic polyp, and generic polyp (same as polyp segmentation). Dice and IoU are calculated pixel-wise on the entire test set (microaveraged). Equations <ref type="bibr" target="#b6">(7)</ref> and (8) describe how these metrics are calculated. We also evaluate each model's inference speed using the number of processed frames per second (FPS). This metric is measured by running each model with a batch size of 1 on 100 colonoscopy images. FPS is measured on a Google Colaboratory instance with an NVIDIA Tesla V100 GPU when not specified otherwise.</p><formula xml:id="formula_7">Dice c = 2 ? i?I u c i v c i i?I u c i + i?I v c i<label>(7)</label></formula><formula xml:id="formula_8">IoU c = i?I u c i v c i i?I u c i + i?I v c i ? i?I u c i v c i<label>(8)</label></formula><p>Finally, we log each model's number of parameters and floating-point operations (measured in GFLOPs) to evaluate their size and complexity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. RESULTS AND DISCUSSION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. ABLATION STUDY</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) The effectiveness of different output architectures</head><p>Firstly, we compare our three BlazeNeo variants shown in <ref type="figure">Figure 2</ref> to evaluate the effectiveness of different output architectures. <ref type="table" target="#tab_0">Table 1</ref> shows performance metrics for each variant on the NeoPolyp test set. Here we use the same DHA feature aggregation scheme for all three of BlazeNeo. The   final BlazeNeo model with multi heads achieves the best results on all metrics. Notably, this final variant outperforms the other two variants by over 6% in IoU score for the nonneoplastic class. This shows the effectiveness of the auxiliary branch, which helps anchor segmentation performance and makes use of undefined labels. We also see that ST-BlazeNeo outperforms SB-BlazeNeo, which justifies our use of trinary output in the final variant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2) The effectiveness of different feature aggregation schemes</head><p>This experiment investigates the use of four different feature aggregation schemes in BlazeNeo: Long Skip Connection (LSC), Iterative Deep Aggregation (IDA), Dense Iterative Aggregation (DIA), and Dense Hierarchical Aggregation (DHA). <ref type="table" target="#tab_1">Table 2</ref> shows performance metrics for each model variation on the NeoPolyp dataset. We can see that BlazeNeo-DHA produces the highest values of Dice and IoU on all classes. This is what we expected, as DHA is the most complex aggregation mechanism that preserves a lot of high-resolution features. However, this improvement comes at a cost as BlazeNeo-DHA is also the slowest variation at only 81.5 FPS, compared to the fastest BlazeNeo-LSC at 88.6 FPS. Interestingly, the nested IDA aggregation scheme performs worse than basic LSC in the segmentation task. This is alleviated in DIA and DHA with the use of skip connections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3) The effectiveness of auxiliary training</head><p>This experiment looks into the effectiveness of enabling and disabling auxiliary training for each variation of BlazeNeo, namely BlazeNeo-LSC, BlazeNeo-IDA, BlazeNeo-DIA, and BlazeNeo-DHA. <ref type="figure" target="#fig_8">Figure 9</ref> shows that auxiliary training generally improves output quality. In fact, without auxiliary segmentation learning, BlazeNeo-DIA and BlazeNeo-DHA achieve even lower accuracy than BlazeNeo-IDA. The non-neoplastic class benefits the most from auxiliary training, improving BlazeNeo-LSC, BlazeNeo-IDA, BlazeNeo-DIA, and BlazeNeo-DHA by 3%, 1.9%, 4%, and 4.9%, respectively. VOLUME 10, 2021  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4) The effectiveness of including undefined polyps</head><p>This experiment examines the effectiveness of using undefined neoplasm pixels via the auxiliary module. <ref type="table" target="#tab_2">Table 3</ref> shows performance metrics for BlazeNeo-DHA, the bestperforming BlazeNeo model, when trained on the NeoPolyp dataset (which contains undefined polyps) and the NeoPolyp-Clean dataset (which does not contain undefined polyps).</p><p>Results show that the addition of these undefined pixels leads to improvements across the board, especially for the nonneoplastic class.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. COMPARISON WITH STATE-OF-THE-ART MODELS 1) Quantitative comparison</head><p>We compare the performance of BlazeNeo-DHA, the bestperforming BlazeNeo model, with seven state-of-the-art models for the polyp segmentation and PSND problem: U-Net <ref type="bibr" target="#b31">[32]</ref>, ColonSegNet <ref type="bibr" target="#b15">[16]</ref>, DDANet <ref type="bibr" target="#b39">[40]</ref>, DoubleUNet <ref type="bibr" target="#b16">[17]</ref>, HarDNet-MSEG <ref type="bibr" target="#b11">[12]</ref>, PraNet <ref type="bibr" target="#b8">[9]</ref>, and NeoUNet <ref type="bibr" target="#b19">[20]</ref>. Except for NeoUNet, the models mentioned above do not handle undefined neoplasm pixels during training. Thus, in the interest of a fair comparison, we use the NeoPolyp-Clean dataset for this experiment, which does not contain undefined polyps. Results are shown in <ref type="table" target="#tab_3">Table 4</ref>.</p><p>We can see that while NeoUNet remains the most accurate model, BlazeNeo is a close second, with a difference of less than 1% on most accuracy metrics. At the same time, BlazeNeo is a much more lightweight and faster model. Compared to NeoUNet, BlazeNeo achieves higher FPS (81.5 versus 68.3), has half as many parameters <ref type="bibr">(17, 143, 324 versus 38, 288, 397)</ref>, and lower GFLOPs (11.06 versus 39.88). Notably, BlazeNeo-DHA is faster and more accurate than HarDNet-MSEG, while NeoUNet is slower than HarDNet-MSEG and U-Net. We attribute the speed improvement of BlazeNeo over HarDNet-MSEG to the different decoder designs, especially the use of the small RFB module.</p><p>Although achieving a high overall dice score, our BlazeNeo is not without limitations. The dice and IoU scores of BlazeNeo are still low for non-neoplastic polyps, and both of them are significantly reduced when the model is converted into the INT8 precision. The reason may be due to the small size of non-neoplastic polyps. When the weights are converted into integers ranging from -128 to 127 in the INT8 precision, the model seems to lose the capacity of representing detailed information and, therefore, is easier to miss small objects in the input images.</p><p>2) Qualitative comparison <ref type="figure" target="#fig_9">Figure 10</ref> shows examples outputs of BlazeNeo-DHA and other baseline models. Overall, BlazeNeo-DHA produces the most accurate segmentation and classification results for different types of polyps.</p><p>The first five rows of <ref type="figure" target="#fig_9">Figure 10</ref> contain "easier" polyp examples. BlazeNeo-DHA and NeoUNet perform quite well in these examples, with similarly high accuracy. Meanwhile, PraNet, HarDNet-MSEG, DoubleU-Net, DDANet, U-Net and ColonSegNet produce predictions with less uniformity, with some polyps containing both neoplastic and nonneoplastic regions. For larger polyps such as the 5th row, PraNet is unable to fully segment the area.</p><p>The final five rows in <ref type="figure" target="#fig_9">Figure 10</ref> represent more challenging examples, in which all models struggle to provide accurate segmentation masks. This is because non-neoplastic polyps are usually small in size and easier to be miss detected. BlazeNeo-DHA and NeoUNet produce fewer false positives in these situations, while U-Net and ColonSegNet create the most inaccurate masks.</p><p>The last two rows show two non-neoplastic polyps in BLI and FICE modes. Contributing factors for these struggles in enhanced color modes include their smaller proportions in the training dataset. Furthermore, the endoscopist put the camera scope close to the mucosa in these cases, making the polyps' surface look very clear and sharp such that one can even observe small dots that are glandular holes on the surface. Therefore, these polyps are easily confused with angiogenesis in neoplastic lesions, which is why all models failed in classifying them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. BENCHMARK FOR EMBEDDED DEVICE</head><p>In this experiment, we apply model compression techniques via the NVIDIA TensorRT 7.1 toolkit <ref type="bibr" target="#b27">[28]</ref> to BlazeNeo and the baseline models. The compressed models are then  benchmarked on the NVIDIA Jetson AGX Xavier developer kit, an embedded computation unit with an NVIDIA GPU specialized for edge AI deployments. This setup is more in-line with deployment scenarios for polyp segmentation and PSND models, i.e., embedded on-site into colonoscopy devices.</p><p>Three available precision modes are tested for compress-ing each model: FP32, FP16, and INT8. Each mode can be seen as a different trade-off level between accuracy and speed. FP32 precision applies techniques such as layer/tensor fusion while keeping parameters as 32-bit floating-point numbers. Hence, this mode provides some speed-up while minimizing accuracy degradation. FP16 precision converts suitable parameters to 16-bit floating-point numbers, greatly VOLUME 10, 2021 reducing model size and latency but is subject to more degradation. Finally, INT8 precision mode quantizes model parameters to 8-bit integers. This precision mode requires an additional calibration procedure to maintain model integrity, which attempts to replicate the original model's output on a small calibration dataset. Despite calibration, INT8 precision is susceptible to a lot more degradation. For this experiment, INT8 calibration is done on a randomized set of images. <ref type="table" target="#tab_4">Table 5</ref> shows performance metrics in different precisions for U-Net, PraNet, HarDNet-MSEG, NeoUNet, and BlazeNeo-DHA. We can see a stark difference in FPS when running models on the Jetson AGX Xavier compared to the Google Colab environment. At FP32 precision, the fastest model (BlazeNeo) achieves only 53.3 FPS on the device, while the slowest model in the Colab environment (PraNet) without any compression still achieves 55.6 FPS. On the other hand, Dice and IoU measures are hardly affected at this precision level. In fact, U-Net and HarDNet-MSEG see improvements up to 1.5% across all metrics after compression. DoubleU-Net and ColonSegNet achieve a slight improvement of about 0.5 ? 1% after compression. DDANet suffers a little from compression, dropping by about 1.5% in segmentation metrics. Finally, PraNet is most affected after compression, dropping by 4.6% in segmentation metrics.</p><p>At FP16 precision, latency for all models are vastly improved, the fastest being BlazeNeo (121.9 FPS) and HarDNet-MSEG (118.7 FPS). In addition, accuracy metrics for all models are within 0.01% of their FP32 counterparts. This further shows that large neural networks do not require high float precision to remain effective.</p><p>INT8 precision gives the most speed gain out of the three modes, but at great expense in terms of accuracy. BlazeNeo runs at 155.6 FPS in this mode, whereas HarDNet-MSEG is the second fastest model at 143.7 FPS. However, HarDNet-MSEG also suffers the largest drop in accuracy at about 8.1% on all metrics. This drop is a bit lower for PraNet (? 3.3%), NeoUNet (? 6%) and BlazeNeo (? 3.6%).</p><p>In every precision mode, BlazeNeo is consistently the fastest model while being a close second in terms of accuracy behind NeoUNet. BlazeNeo also displays its robustness to compression techniques, incurring significantly less degradation compared to models such as PraNet. Its small size also gives the proposed model advantage in long-term deployment, as energy usage and equipment wear become factors. In <ref type="figure" target="#fig_10">Figure 11</ref>, we visualize the performance of BlazeNeo in terms of speed (FPS) and accuracy (dice score on polyp segmentation task) compared to other existing methods.</p><p>It should be emphasized that high FPS is essential in real scenarios because it helps endoscopists operate smoother and easier to monitor and detect lesions. According to endoscopists' experience, the idea FPS for colonoscopy should be at least 60. In addition, the fast inference speed gives us the potential to deploy the model on even more lowcost devices with less computational power, such as NVIDIA Jetson TX1/TX2, while still satisfying the minimal required FPS. This is important because it allows us to deploy the application on a large scale to many medical facilities in remote areas where the economic conditions are difficult.</p><p>For further discussion, we compare the performance of the models quantitatively and qualitatively in different precisions. <ref type="table" target="#tab_5">Table 6</ref> shows the accuracy of our BlazeNeo-DHA models in different precisions. One can observe that the models with FP32 and FP16 precisions give the same result, which is lower than the original model by a mean margin of 2.32%. The TensorRT INT8 model yields the worst accuracy, which is lower than the original model by a mean margin of 5.75%.</p><p>With lower precision, a model can reduce latency, throughput and increase power efficiency. To compare the speed performance of the models in different precisions, we present detailed benchmarking results of them for 100 iterations on the Jetson AGX Xavier with two metrics: host latency and GPU compute time. Host latency is measured as the endto-end execution time from the CPU point of view, while GPU computing time is the actual working time for GPU calculation.</p><p>As shown in <ref type="table" target="#tab_6">Table 7</ref>, the model in INT8 precision has the shortest latency compared to all other models, less than one-third of the model in FP32 precision. However, for deep learning inference, we must consider the trade-off between accuracy and speed. Therefore, the model in FP16 precision is the best choice, which gives the best result in all precisions and has a median latency.</p><p>For a more intuitive understanding of the loss in accuracy of each compressed model, <ref type="figure" target="#fig_11">Figure 12</ref> illustrates the sample results of BlazeNeo-DHA models in different precisions. As we can observe, the prediction result of models in FP32 and FP16 precisions are almost the same in every example. In general, the INT8 compressed model loses the ability to segment smaller polyps compared to other modes. For segmented polyps, the neoplasm prediction remains constant for every compression mode in all examples. This demonstrates that BlazeNeo can be quite robust to model compression and can be deployed in high compression modes such as FP16 with confidence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. CONCLUSION</head><p>This paper has proposed BlazeNeo, a novel neural network architecture for the polyp segmentation and neoplasm detection problem, with an emphasis on speed and deployability. BlazeNeo is an extremely lightweight and fast neural network, thanks to the use of an efficient HarDNet backbone, a multi-level feature aggregation structure, and an auxiliary training module to take advantage of undefined labels. Our experiments show that BlazeNeo outperforms all other stateof-the-art models in terms of inference latency while providing competitive accuracy. We also show that BlazeNeo can be very robust against degradation caused by compression techniques. In general, the proposed model is highly suitable for lightweight deployments with real-time requirements. Our source code is available at https://github.com/tofuai/ neoplasm-segmentation.   In future works, we would like to exploit recent advance-ments in Transformer-based architectures to improve the VOLUME 10, 2021  performance of the models. Especially, lightweight Transformers such as SegFormer <ref type="bibr" target="#b41">[42]</ref> will be the primary focus since they ensure both high accuracy and high inference speed which are crucial factors for a real computer-aided system in colonoscopy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VIII. ACKNOWLEDGMENT</head><p>This work was funded by Vingroup Innovation Foundation (VINIF) under project code VINIF.2020.DA17.</p><p>PHAN N. LAN received his Engineering Degree in Information and Communication Technology at HUST in 2019. He is currently continuing his studies as a graduate student at HUST, majoring in Data Science and Artificial Intelligence. He is also a student member of the International Research Center for Artificial Intelligence (BK.AI). His research interests include machine learning, computer vision, and optimization techniques.</p><p>DAO V. HANG is a gastroenterologist and clinical lecturer at Hanoi Medical University and Institute of Gastroenterology and Hepatology. She also assumes different roles in the Vietnam Association of Gastroenterology (VNAGE), including organizing endoscopy training courses. Her current interest is the application of innovative technologies, such as image-enhanced endoscopy, smart apps, and artificial intelligence (AI) in endoscopy, microbiome, and GI motility. She believes technological solutions, among which AI is promising and feasible, can help solve the problems in limited-resources settings. She has experience participating in collaborative AI projects, where she led her team to recruit, label data, validate the product and implement clinical studies.</p><p>DAO V. LONG is a professor in Gastroenterology and Hepatology and is now the Vice President of VNAGE. He was the Vice Rector of Hanoi Medical University, Director of Hanoi Medical University hospital, and Head of the Gastroenterology Department, Bach Mai hospital. He has good collaborations with foreign specialists and has organized many workshops and training courses for Vietnamese GI doctors and endoscopists. His research focuses on interventional endoscopy and GI tract diseases. During clinical practice, he has realized the importance of technology, particularly artificial intelligence, in GI endoscopy and has become interested in researching its application to facilitate diagnostic accuracy and reduce human labor in the practice of clinical physicians and endoscopists.</p><p>TRAN Q. TRUNG is currently doing a research project in MD.PhD program at Greifswald University of Medicine, Germany. He is one of the young scientists who was selected globally to meet 70 Nobel Laureates at the 70th Lindau Nobel Meeting, where he can join a multidisciplinary network. He has been passionate about GI endoscopy since 2010, when he became a medical doctor. With experiences gained from advanced training in GI endoscopy in Vietnam, Japan, and Germany, and from a variety of international conferences in endoscopy, he is doing some novel and fruitful works for patients in Vietnam. The revolution of AI has recently attracted his great interest in applying to the field and ultimately bringing benefit to patients. NGUYEN T. THUY received her Ph.D. degree in Computer Science from Graz University of Technology, Austria in 2009. She is now an Associate Professor, Head of Department of Computer Science, Faculty of Information Technology, Vietnam National University of Agriculture (VNUA), Vietnam. She has more than ten years of research experience in computer vision, machine learning, pattern recognition. She is an author and co-author of more than 70 research papers and patents. She has been a principal investigator and key member of a number of research projects in computer vision, machine learning, and applications. DINH V. SANG received his Ph.D. degree in the Computer Science from Dorodnitsyn Computing Centre of the Russian Academy of Sciences (CCRAS) in 2013. He is currently working at the Department of Computer Science, School of Information and Communication Technology (SoICT), Hanoi University of Science and Technology (HUST), Vietnam. He is now the deputy managing director of the International Research Center for Artificial Intelligence (BK.AI) at HUST. His research interests include computer vision, machine learning, and deep learning. He has more than ten years of research experience in computer vision and machine learning and has published about 50 publications. He is the first NVIDIA deep learning institute (DLI) ambassador in Vietnam. <ref type="bibr">VOLUME 10, 2021</ref> </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>FIGURE 3 :</head><label>3</label><figDesc>Illustrations for DenseNet block and Harmonic DenseNet (HarDNet) block. Each of the layers is a 3 ? 3 convolution. The value on each layer denotes the number of output channels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>FIGURE 4 :</head><label>4</label><figDesc>An overview of HarDNet-68 architecture. Following each HarDNet block is a transitional Conv 1x1 layer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>FIGURE 5 :</head><label>5</label><figDesc>Structure of the Receptive Field Block (RFB).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>FIGURE 6 :</head><label>6</label><figDesc>Different feature aggregation schemes as shown in [45]. (a) Long skip connection (LSC); (b) Iterative Deep Aggregation (IDA); (c) Dense Iterative Aggregation (DIA); (d) Dense Hierarchical Aggregation (DHA).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>Figure 2cdescribes how BlazeNeo incorporates the auxiliary branch for training in our BlazeNeo.V. EXPERIMENTSA. BENCHMARK DATASETWe use the NeoPolyp dataset as introduced in<ref type="bibr" target="#b19">[20]</ref> to train and benchmark the proposed BlazeNeo. The dataset consists of 7,466 annotated endoscopic images captured directly during endoscopic recording and includes all fourlighting modes: WLI (White Light Imaging), FICE (Flexible spectral Imaging Color Enhancement), BLI (Blue Light Imaging), and LCI (Linked Color Imaging). NeoPolyp is split into a training set of 5,966 images and a test set of 1,500 images. Some examples of the NeoPolyp dataset are shown in Figure 7. For comparison with baseline models, we also use the NeoPolyp-Clean dataset, which does not contain any polyps with undefined class labels. This dataset consists of 5,277 training images and 1,353 test images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>FIGURE 7 :FIGURE 8 :</head><label>78</label><figDesc>Some examples from the NeoPolyp dataset. The first row displays original images from the dataset. The second row shows the ground truths for polyp segmentation. The last row shows the ground truths for neoplasm segmentation, where some polyps are undefined and marked by yellow color. From left to right, the color modes are WLI, BLI, LCI, FICE, and FICE, respectively.(a) Polyp-wise distribution (b) Pixel-wise distribution Data distribution of polyp class labels in the NeoPolyp dataset. In the pixel-wise distribution on the right, percentages are calculated on polyp pixels only (not including background pixels.) where i ? I denotes a prediction pixel within the entire test set. u c i = 1 if the model predicts pixel i to have class c, and 0 otherwise. Similarly, v c i = 1 if the ground truth map states that pixel i has class c, and 0 otherwise. For neoplastic and non-neoplastic class evaluation, I does not include undefined neoplasm pixels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>(a) Non-neoplastic class (b) Neoplastic class</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>FIGURE 9 :</head><label>9</label><figDesc>Dice scores and FPS for different BlazeNeo variations, with and without auxiliary training. Red triangles denote results with auxiliary training, while blue circles are those without auxiliary training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>FIGURE 10 :</head><label>10</label><figDesc>Qualitative comparison of the proposed method with other baseline methods: (a) image, (b) ground truth, (c) BlazeNeo (Ours), (d) NeoUNet, (e) PraNet, (f) HarDNet-MSEG, (g) UNet, (h) DoubleU-Net, (i) DDANet, and (j) ColonSegNet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>FIGURE 11 :</head><label>11</label><figDesc>Comparison between different models in different precisions. Red, green and blue markers denote INT8, FP16 and FP32 precisions, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>FIGURE 12 :</head><label>12</label><figDesc>Qualitative comparison of BlazeNeo-DHA models in different precisions: (a) image, (b) ground truth, (c) Pytorch, (d) TensorRT FP32, (e) TensorRT FP16, (f) TensorRT INT8.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE 1 :</head><label>1</label><figDesc>Performance metrics on the NeoPolyp test set for the three variants of BlazeNeo using the same DHA feature aggregation scheme</figDesc><table><row><cell>Model</cell><cell>Diceseg</cell><cell>IoUseg</cell><cell>Dicenon</cell><cell>IoUnon</cell><cell>Diceneo</cell><cell>IoUneo</cell></row><row><cell>SB-BlazeNeo</cell><cell>0.866</cell><cell>0.764</cell><cell>0.683</cell><cell>0.518</cell><cell>0.862</cell><cell>0.758</cell></row><row><cell>ST-BlazeNeo</cell><cell>0.874</cell><cell>0.777</cell><cell>0.671</cell><cell>0.505</cell><cell>0.865</cell><cell>0.762</cell></row><row><cell>BlazeNeo</cell><cell>0.901</cell><cell>0.820</cell><cell>0.728</cell><cell>0.572</cell><cell>0.888</cell><cell>0.800</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE 2 :</head><label>2</label><figDesc>Performance metrics on the NeoPolyp test set for BlazeNeo with different feature aggregation schemes</figDesc><table><row><cell>Method</cell><cell>Diceseg</cell><cell>IoUseg</cell><cell>Dicenon</cell><cell>IoUnon</cell><cell>Diceneo</cell><cell>IoUneo</cell><cell>FPS</cell></row><row><cell>BlazeNeo-LSC</cell><cell>0.897</cell><cell>0.814</cell><cell>0.709</cell><cell>0.550</cell><cell>0.884</cell><cell>0.792</cell><cell>88.6</cell></row><row><cell>BlazeNeo-IDA</cell><cell>0.890</cell><cell>0.803</cell><cell>0.715</cell><cell>0.557</cell><cell>0.887</cell><cell>0.798</cell><cell>84.6</cell></row><row><cell>BlazeNeo-DIA</cell><cell>0.897</cell><cell>0.814</cell><cell>0.719</cell><cell>0.562</cell><cell>0.888</cell><cell>0.800</cell><cell>84.0</cell></row><row><cell>BlazeNeo-DHA</cell><cell>0.901</cell><cell>0.820</cell><cell>0.728</cell><cell>0.572</cell><cell>0.888</cell><cell>0.800</cell><cell>81.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE 3 :</head><label>3</label><figDesc>Performance metrics for BlazeNeo-DHA when training on NeoPolyp and NeoPolyp-Clean, measured on the NeoPolyp test set</figDesc><table><row><cell cols="2">Training dataset Diceseg</cell><cell>IoUseg</cell><cell>Dicenon</cell><cell>IoUnon</cell><cell>Diceneo</cell><cell>IoUneo</cell></row><row><cell>NeoPolyp-Clean</cell><cell>0.900</cell><cell>0.818</cell><cell>0.714</cell><cell>0.555</cell><cell>0.884</cell><cell>0.792</cell></row><row><cell>NeoPolyp</cell><cell>0.901</cell><cell>0.820</cell><cell>0.728</cell><cell>0.572</cell><cell>0.888</cell><cell>0.799</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE 4 :</head><label>4</label><figDesc>Performance metrics of different models on the NeoPolyp-Clean test set</figDesc><table><row><cell>Method</cell><cell>Diceseg</cell><cell>IoUseg</cell><cell>Dicenon</cell><cell>IoUnon</cell><cell>Diceneo</cell><cell>IoUneo</cell><cell>FPS</cell><cell>Parameters</cell><cell>GFLOPs</cell></row><row><cell>ColonSegNet [16]</cell><cell>0.738</cell><cell>0.585</cell><cell>0.505</cell><cell>0.338</cell><cell>0.732</cell><cell>0.577</cell><cell>44.9</cell><cell>5,010,000</cell><cell>64.84</cell></row><row><cell>U-Net [32]</cell><cell>0.785</cell><cell>0.646</cell><cell>0.525</cell><cell>0.356</cell><cell>0.773</cell><cell>0.631</cell><cell cols="2">69.6 31,043,651</cell><cell>103.59</cell></row><row><cell>DDANet [40]</cell><cell>0.813</cell><cell>0.684</cell><cell>0.578</cell><cell>0.406</cell><cell>0.802</cell><cell>0.670</cell><cell>46.2</cell><cell>6,840,000</cell><cell>31.45</cell></row><row><cell>DoubleU-Net [17]</cell><cell>0.837</cell><cell>0.720</cell><cell>0.621</cell><cell>0.450</cell><cell>0.832</cell><cell>0.712</cell><cell cols="2">43.2 18,836,804</cell><cell>83.62</cell></row><row><cell>HarDNet-MSEG [12]</cell><cell>0.883</cell><cell>0.791</cell><cell>0.659</cell><cell>0.492</cell><cell>0.869</cell><cell>0.769</cell><cell>77.1</cell><cell>17,424,031</cell><cell>11.38</cell></row><row><cell>PraNet [9]</cell><cell>0.895</cell><cell>0.811</cell><cell>0.705</cell><cell>0.544</cell><cell>0.873</cell><cell>0.775</cell><cell cols="2">55.6 30,501,341</cell><cell>13.11</cell></row><row><cell>NeoUNet [20]</cell><cell>0.911</cell><cell>0.837</cell><cell>0.720</cell><cell>0.563</cell><cell>0.889</cell><cell>0.800</cell><cell cols="2">68.3 38,288,397</cell><cell>39.88</cell></row><row><cell>BlazeNeo-DHA (Ours)</cell><cell>0.904</cell><cell>0.825</cell><cell>0.717</cell><cell>0.559</cell><cell>0.885</cell><cell>0.792</cell><cell cols="2">81.5 17,143,324</cell><cell>11.06</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE 5 :</head><label>5</label><figDesc>Performance metrics of different models with FP16, FP32 and INT8 precision levels on NVIDIA Jetson AGX Xavier</figDesc><table><row><cell>Model</cell><cell>Precision</cell><cell>Diceseg</cell><cell>IoUseg</cell><cell>Dicenon</cell><cell>IoUnon</cell><cell>Diceneo</cell><cell>IoUneo</cell><cell>FPS</cell></row><row><cell>ColonSegNet@FP32</cell><cell>FP32</cell><cell>0.745</cell><cell>0.594</cell><cell>0.528</cell><cell>0.359</cell><cell>0.728</cell><cell>0.572</cell><cell>9.4</cell></row><row><cell>UNet@FP32</cell><cell>FP32</cell><cell>0.800</cell><cell>0.667</cell><cell>0.537</cell><cell>0.367</cell><cell>0.781</cell><cell>0.641</cell><cell>10.7</cell></row><row><cell>DDANet@FP32</cell><cell>FP32</cell><cell>0.799</cell><cell>0.665</cell><cell>0.557</cell><cell>0.386</cell><cell>0.776</cell><cell>0.635</cell><cell>16.6</cell></row><row><cell>DoubleU-Net@FP32</cell><cell>FP32</cell><cell>0.840</cell><cell>0.725</cell><cell>0.627</cell><cell>0.456</cell><cell>0.835</cell><cell>0.717</cell><cell>10.6</cell></row><row><cell>HarDNet-MSEG@FP32</cell><cell>FP32</cell><cell>0.891</cell><cell>0.804</cell><cell>0.685</cell><cell>0.521</cell><cell>0.871</cell><cell>0.771</cell><cell>52.2</cell></row><row><cell>PraNet@FP32</cell><cell>FP32</cell><cell>0.849</cell><cell>0.738</cell><cell>0.571</cell><cell>0.400</cell><cell>0.844</cell><cell>0.730</cell><cell>37.0</cell></row><row><cell>NeoUNet@FP32</cell><cell>FP32</cell><cell>0.909</cell><cell>0.832</cell><cell>0.725</cell><cell>0.568</cell><cell>0.893</cell><cell>0.806</cell><cell>25.4</cell></row><row><cell>BlazeNeo@FP32 (Ours)</cell><cell>FP32</cell><cell>0.906</cell><cell>0.828</cell><cell>0.721</cell><cell>0.563</cell><cell>0.887</cell><cell>0.796</cell><cell>53.3</cell></row><row><cell>ColonSegNet@FP16</cell><cell>FP16</cell><cell>0.746</cell><cell>0.594</cell><cell>0.528</cell><cell>0.359</cell><cell>0.728</cell><cell>0.572</cell><cell>26.3</cell></row><row><cell>UNet@FP16</cell><cell>FP16</cell><cell>0.800</cell><cell>0.666</cell><cell>0.537</cell><cell>0.367</cell><cell>0.781</cell><cell>0.641</cell><cell>35.5</cell></row><row><cell>DDANet@FP16</cell><cell>FP16</cell><cell>0.799</cell><cell>0.665</cell><cell>0.557</cell><cell>0.386</cell><cell>0.776</cell><cell>0.635</cell><cell>31.8</cell></row><row><cell>DoubleU-Net@FP16</cell><cell>FP16</cell><cell>0.840</cell><cell>0.725</cell><cell>0.627</cell><cell>0.456</cell><cell>0.835</cell><cell>0.717</cell><cell>22.9</cell></row><row><cell>HarDNet-MSEG@FP16</cell><cell>FP16</cell><cell>0.891</cell><cell>0.804</cell><cell>0.685</cell><cell>0.521</cell><cell>0.871</cell><cell>0.771</cell><cell>118.7</cell></row><row><cell>PraNet@FP16</cell><cell>FP16</cell><cell>0.850</cell><cell>0.740</cell><cell>0.572</cell><cell>0.401</cell><cell>0.845</cell><cell>0.731</cell><cell>95.1</cell></row><row><cell>NeoUNet@FP16</cell><cell>FP16</cell><cell>0.908</cell><cell>0.832</cell><cell>0.724</cell><cell>0.568</cell><cell>0.893</cell><cell>0.806</cell><cell>73.4</cell></row><row><cell>BlazeNeo@FP16 (Ours)</cell><cell>FP16</cell><cell>0.906</cell><cell>0.828</cell><cell>0.721</cell><cell>0.563</cell><cell>0.887</cell><cell>0.796</cell><cell>121.9</cell></row><row><cell>ColonSegNet@INT8</cell><cell>INT8</cell><cell>0.672</cell><cell>0.507</cell><cell>0.456</cell><cell>0.295</cell><cell>0.633</cell><cell>0.463</cell><cell>41.6</cell></row><row><cell>UNet@INT8</cell><cell>INT8</cell><cell>0.764</cell><cell>0.618</cell><cell>0.501</cell><cell>0.334</cell><cell>0.753</cell><cell>0.604</cell><cell>44.8</cell></row><row><cell>DDANet@INT8</cell><cell>INT8</cell><cell>0.770</cell><cell>0.626</cell><cell>0.492</cell><cell>0.326</cell><cell>0.748</cell><cell>0.598</cell><cell>46.4</cell></row><row><cell>DoubleU-Net@INT8</cell><cell>INT8</cell><cell>0.835</cell><cell>0.717</cell><cell>0.562</cell><cell>0.391</cell><cell>0.830</cell><cell>0.709</cell><cell>46.7</cell></row><row><cell>HarDNet-MSEG@INT8</cell><cell>INT8</cell><cell>0.810</cell><cell>0.680</cell><cell>0.594</cell><cell>0.422</cell><cell>0.799</cell><cell>0.665</cell><cell>143.7</cell></row><row><cell>PraNet@INT8</cell><cell>INT8</cell><cell>0.817</cell><cell>0.691</cell><cell>0.575</cell><cell>0.404</cell><cell>0.815</cell><cell>0.688</cell><cell>116.7</cell></row><row><cell>NeoUNet@INT8</cell><cell>INT8</cell><cell>0.848</cell><cell>0.736</cell><cell>0.638</cell><cell>0.468</cell><cell>0.848</cell><cell>0.737</cell><cell>100.3</cell></row><row><cell>BlazeNeo@INT8 (Ours)</cell><cell>INT8</cell><cell>0.870</cell><cell>0.770</cell><cell>0.678</cell><cell>0.513</cell><cell>0.857</cell><cell>0.750</cell><cell>155.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE 6 :</head><label>6</label><figDesc>Accuracy metrics on the NeoPolyp-Clean test set for BlazeNeo-DHA models in different precisions</figDesc><table><row><cell>Model</cell><cell>Diceseg</cell><cell>IoUseg</cell><cell>Dicenon</cell><cell>IoUnon</cell><cell>Diceneo</cell><cell>IoUneo</cell></row><row><cell>PyTorch (w/o compression)</cell><cell>0.904</cell><cell>0.825</cell><cell>0.717</cell><cell>0.559</cell><cell>0.885</cell><cell>0.792</cell></row><row><cell>TensorRT FP32</cell><cell>0.906</cell><cell>0.828</cell><cell>0.721</cell><cell>0.563</cell><cell>0.887</cell><cell>0.796</cell></row><row><cell>TensorRT FP16</cell><cell>0.906</cell><cell>0.828</cell><cell>0.721</cell><cell>0.563</cell><cell>0.887</cell><cell>0.796</cell></row><row><cell>TensorRT INT8</cell><cell>0.870</cell><cell>0.770</cell><cell>0.678</cell><cell>0.513</cell><cell>0.857</cell><cell>0.750</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE 7 :</head><label>7</label><figDesc>Latency metrics for BlazeNeo-DHA models in different precisions. The latency is measured on Jetson Xavier AGX with power mode MAXN</figDesc><table><row><cell>Precision</cell><cell>min</cell><cell cols="2">Host Latency (ms) max mean</cell><cell>median</cell><cell>min</cell><cell cols="2">GPU Compute (ms) max mean</cell><cell>median</cell></row><row><cell>TensorRT FP32</cell><cell cols="3">18.50 22.04 18.67</cell><cell>18.55</cell><cell cols="3">18.40 21.94 18.57</cell><cell>18.45</cell></row><row><cell>TensorRT FP16</cell><cell>8.10</cell><cell>10.24</cell><cell>8.20</cell><cell>8.15</cell><cell>7.99</cell><cell>10.13</cell><cell>8.09</cell><cell>8.04</cell></row><row><cell>TensorRT INT8</cell><cell>6.35</cell><cell>6.51</cell><cell>6.42</cell><cell>6.42</cell><cell>6.25</cell><cell>6.42</cell><cell>6.32</cell><cell>6.32</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">) Receptive Field BlockPolyps can appear in various scales on endoscopic images depending on their actual size, their distance to the colonoscopy camera, or the angle between them and the camera. This is a challenge for CNN architectures, in which the receptive field VOLUME 10, 2021</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">   VOLUME 10, 2021   </note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">   VOLUME 10, 2021   </note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://developer.nvidia.com/embedded/jetson-agx-xavier-developer-kit VOLUME 10, 2021</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8">   VOLUME 10, 2021   </note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10">   VOLUME 10, 2021   </note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="12">   VOLUME 10, 2021   </note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="14">   VOLUME 10, 2021   </note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="16">   VOLUME 10, 2021   </note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A novel focal tversky loss function with improved attention u-net for lesion segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Abraham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">M</forename><surname>Khan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 16th International Symposium on Biomedical Imaging</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="683" to="687" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Visibility map: a new method in evaluation quality of optical colonoscopy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Armin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Visser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chetty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dumas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Conlan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Grimpen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Salvado</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="396" to="404" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Comparative validation of polyp detection methods in video colonoscopy: Results from the miccai 2015 endoscopic vision challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bernal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Tajkbaksh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">J</forename><surname>S?nchez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">J</forename><surname>Matuszewski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Angermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Romain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rustad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Balasingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Pogorelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Debard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Maier-Hein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Speidel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Brandao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>C?rdova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>S?nchez-Montes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Gurudu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Fern?ndez-Esparrach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Histace</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1231" to="1249" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Hardnet: A low memory traffic network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Y</forename><surname>Kao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">S</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">L</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3552" to="3561" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="834" to="848" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Rethinking atrous convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<idno>abs/1706.05587</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Auxnet: Auxiliary tasks enhanced semantic segmentation for automated driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chennupati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sistu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yogamani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rawashdeh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.05808</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deformable convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="764" to="773" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Pranet: Parallel reverse attention network for polyp segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">P</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="263" to="273" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">High-grade dysplasia and invasive carcinoma in colorectal adenomas: a multivariate analysis of the impact of adenoma and patient characteristics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gschwantler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kriwanek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Langner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>G?ritzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schrutka-K?lbl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Brownstone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Feichtinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">European journal of gastroenterology &amp; hepatology</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="183" to="188" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Hardnet-mseg: A simple encoderdecoder polyp segmentation neural network that achieves over 0.9 mean dice and 86 fps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">L</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.07172</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4700" to="4708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Colorectal cancer screening: An updated review of the available options</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">A</forename><surname>Issa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Noureddine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">World journal of gastroenterology</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">28</biblScope>
			<biblScope unit="page">5086</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Automatic polyp detection in endoscope images using a hessian filter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Iwahori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Shinohara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hattori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Woodham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fukui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">K</forename><surname>Bhuyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kasugai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">MVA. pp</title>
		<imprint>
			<biblScope unit="page" from="21" to="24" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Real-time polyp detection, localization and segmentation in colonoscopy using deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">K</forename><surname>Tomar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">D</forename><surname>Johansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Johansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rittscher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Halvorsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ieee Access</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="40496" to="40510" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Doubleu-net: A deep convolutional neural network for medical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Johansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Halvorsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">D</forename><surname>Johansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G S</forename><surname>De Herrera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Gonz?lez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">C</forename><surname>Santosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Temesgen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kane</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">33rd IEEE International Symposium on Computer-Based Medical Systems</title>
		<editor>Soda, P.</editor>
		<meeting><address><addrLine>Rochester, MN, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="558" to="564" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Miss rate of colorectal neoplastic polyps and risk factors for missed polyps in consecutive colonoscopies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">S</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">S</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">I</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Intestinal research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">411</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Neounet: Towards accurate colon polyp segmentation and neoplasm detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">N</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">S</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">V</forename><surname>Hang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">V</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Q</forename><surname>Trung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">T</forename><surname>Thuy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">V</forename><surname>Sang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th International Symposium on Visual Computing</title>
		<meeting>the 16th International Symposium on Visual Computing</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">An adequate level of training for technical competence in screening and diagnostic colonoscopy: a prospective multicenter evaluation of the learning curve</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">K</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">O</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">M</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hwangbo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">H</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Gastrointestinal endoscopy</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="683" to="689" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Factors influencing the miss rate of polyps in a back-to-back colonoscopy study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Leufkens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Van Oijen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Vleggaar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Siersema</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Endoscopy</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">05</biblScope>
			<biblScope unit="page" from="470" to="475" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Colonoscopy polyp detection and classification: Dataset creation and comparative evaluations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Fathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rastogi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Plos one</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">255809</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Colonic polyp detection in endoscopic videos with single shot detection based deep convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="75058" to="75066" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Receptive field block net for accurate and fast object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="385" to="400" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Ssd: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="21" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<ptr target="https://developer.nvidia.com/tensorrt" />
		<title level="m">NVIDIA: Nvidia tensorrt</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2021" to="2029" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Oktay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schlemper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">L</forename><surname>Folgoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">C H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">P</forename><surname>Heinrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Misawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">G</forename><surname>Mcdonagh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">Y</forename><surname>Hammerla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kainz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Glocker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rueckert</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
	<note>Attention u-net: Learning where to look for the pancreas</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A framework with a fully convolutional neural network for semi-automatic colon polyp annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">A</forename><surname>Qadir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Solhusvik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bergsland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Aabakken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Balasingham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="169537" to="169547" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Exploring deep learning and transfer learning for colonic polyp classification. Computational and mathematical methods in medicine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ribeiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Uhl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wimmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>H?fner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical image computing and computer-assisted intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Automatic colon polyp detection using region based deep cnn and post learning approaches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">A</forename><surname>Qadir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Aabakken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bergsland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Balasingham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="40950" to="40962" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Abnormal colon polyp image synthesis using conditional adversarial networks for improved detection performance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">A</forename><surname>Qadir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Balasingham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="56007" to="56017" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Toward embedded detection of polyps in wce images for early diagnosis of colorectal cancer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Histace</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Romain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Granado</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Assisted Radiology and Surgery</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="283" to="293" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for largescale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations</title>
		<editor>Bengio, Y., LeCun, Y.</editor>
		<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-05-07" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Efficientnet: Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6105" to="6114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
		</author>
		<title level="m">Cu-net: Coupled u-nets. In: 29th British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Ddanet: Dual decoder attention network for automatic polyp segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">K</forename><surname>Tomar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">D</forename><surname>Johansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Johansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Halvorsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pattern Recognition. ICPR International Workshops and Challenges -Virtual Event</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">12668</biblScope>
			<biblScope unit="page" from="307" to="314" />
		</imprint>
	</monogr>
	<note>Proceedings, Part VIII</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Cascaded partial decoder for fast and accurate salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3907" to="3916" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Segformer: Simple and efficient design for semantic segmentation with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Anandkumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1492" to="1500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Auxiliary training: Towards accurate and robust models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="372" to="381" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Multi-scale object detection model with anchor free approach and center of gravity prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE 5th Information Technology and Mechatronics Engineering Conference (ITOEC)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="38" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Transfuse: Fusing transformers and cnns for medical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.08005</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Unet++: Redesigning skip connections to exploit multiscale features in image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M R</forename><surname>Siddiquee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Tajbakhsh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on medical imaging</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1856" to="1867" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">In 2018, he won the ERAMUS+ scholarship at Tampere University of Technology (TUT), Finland. Then, he was selected as one of the two Vietnamese students for participating in the Asia-Oceania Top University League (AOTULE) summer program</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nguyen S</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
		<respStmt>
			<orgName>at Bandung Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note>AN graduated the Global Engineering program in Information and Communication Technology, Hanoi University of Science and Technology (HUST)</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
